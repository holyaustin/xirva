[{"id": "1504.00154", "submitter": "Wenji Li", "authors": "Zhun Fan, Wenji Li, Xinye Cai, Huibiao Lin, Shuxiang Xie, Erik Goodman", "title": "A New Repair Operator for Multi-objective Evolutionary Algorithm in\n  Constrained Optimization Problems", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we design a set of multi-objective constrained optimization\nproblems (MCOPs) and propose a new repair operator to address them. The\nproposed repair operator is used to fix the solutions that violate the box\nconstraints. More specifically, it employs a reversed correction strategy that\ncan effectively avoid the population falling into local optimum. In addition,\nwe integrate the proposed repair operator into two classical multi-objective\nevolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator is\ncompared with other two kinds of commonly used repair operators on benchmark\nproblems CTPs and MCOPs. The experiment results demonstrate that our proposed\napproach is very effective in terms of convergence and diversity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 09:07:13 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Fan", "Zhun", ""], ["Li", "Wenji", ""], ["Cai", "Xinye", ""], ["Lin", "Huibiao", ""], ["Xie", "Shuxiang", ""], ["Goodman", "Erik", ""]]}, {"id": "1504.00641", "submitter": "Ankit Patel", "authors": "Ankit B. Patel, Tan Nguyen and Richard G. Baraniuk", "title": "A Probabilistic Theory of Deep Learning", "comments": "56 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "Rice University Electrical and Computer Engineering Dept. Technical\n  Report No 2015-1", "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 18:38:38 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Patel", "Ankit B.", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1504.00923", "submitter": "Fred Richardson", "authors": "Fred Richardson, Douglas Reynolds, Najim Dehak", "title": "A Unified Deep Neural Network for Speaker and Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned feature representations and sub-phoneme posteriors from Deep Neural\nNetworks (DNNs) have been used separately to produce significant performance\ngains for speaker and language recognition tasks. In this work we show how\nthese gains are possible using a single DNN for both speaker and language\nrecognition. The unified DNN approach is shown to yield substantial performance\nimprovements on the the 2013 Domain Adaptation Challenge speaker recognition\ntask (55% reduction in EER for the out-of-domain condition) and on the NIST\n2011 Language Recognition Evaluation (48% reduction in EER for the 30s test\ncondition).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:57:06 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Richardson", "Fred", ""], ["Reynolds", "Douglas", ""], ["Dehak", "Najim", ""]]}, {"id": "1504.00941", "submitter": "Quoc Le", "authors": "Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton", "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long term dependencies in recurrent networks is difficult due to\nvanishing and exploding gradients. To overcome this difficulty, researchers\nhave developed sophisticated optimization techniques and network architectures.\nIn this paper, we propose a simpler solution that use recurrent neural networks\ncomposed of rectified linear units. Key to our solution is the use of the\nidentity matrix or its scaled version to initialize the recurrent weight\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\ntwo toy problems involving long-range temporal structures, a large language\nmodeling problem and a benchmark speech recognition problem.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 21:22:52 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 22:39:18 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Le", "Quoc V.", ""], ["Jaitly", "Navdeep", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1504.01106", "submitter": "Lili Mou", "authors": "Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin", "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a tree-based convolutional neural network (TBCNN) for\ndiscriminative sentence modeling. Our models leverage either constituency trees\nor dependency trees of sentences. The tree-based convolution process extracts\nsentences' structural features, and these features are aggregated by max\npooling. Such architecture allows short propagation paths between the output\nlayer and underlying feature detectors, which enables effective structural\nfeature learning and extraction. We evaluate our models on two tasks: sentiment\nanalysis and question classification. In both experiments, TBCNN outperforms\nprevious state-of-the-art results, including existing neural networks and\ndedicated feature/rule engineering. We also make efforts to visualize the\ntree-based convolution process, shedding light on how our models work.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 10:18:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 07:30:08 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 17:16:32 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 12:23:16 GMT"}, {"version": "v5", "created": "Tue, 2 Jun 2015 05:56:06 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Mou", "Lili", ""], ["Peng", "Hao", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1504.01167", "submitter": "Can Eren Sezener", "authors": "Can Eren Sezener and Erhan Oztop", "title": "Heuristic algorithms for obtaining Polynomial Threshold Functions with\n  low densities", "comments": "This paper will appear in the 13th Cologne-Twente Workshop on Graphs\n  & Combinatorial Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present several heuristic algorithms, including a Genetic\nAlgorithm (GA), for obtaining polynomial threshold function (PTF)\nrepresentations of Boolean functions (BFs) with small number of monomials. We\ncompare these among each other and against the algorithm of Oztop via\ncomputational experiments. The results indicate that our heuristic algorithms\nfind more parsimonious representations compared to the those of non-heuristic\nand GA-based algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 23:07:24 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Sezener", "Can Eren", ""], ["Oztop", "Erhan", ""]]}, {"id": "1504.01482", "submitter": "William Chan", "authors": "William Chan, Ian Lane", "title": "Deep Recurrent Neural Networks for Acoustic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep Recurrent Neural Network (RNN) model for acoustic\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\nand the final DNN takes the context and models the posterior probabilities of\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\neval92 task or more than 8% relative improvement over the baseline DNN models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:12:14 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01483", "submitter": "William Chan", "authors": "William Chan and Nan Rosemary Ke and Ian Lane", "title": "Transferring Knowledge from a RNN to a DNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\non embedded systems with limited computational capacity. Traditionally, the\napproach for embedded platforms is to either train a small DNN directly, or to\ntrain a small DNN that learns the output distribution of a large DNN. In this\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\ndivergence against the small DNN. The small DNN trained on the soft RNN\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\ncompared to a baseline 4.54 WER or more than 13% relative improvement.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:15:44 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Chan", "William", ""], ["Ke", "Nan Rosemary", ""], ["Lane", "Ian", ""]]}, {"id": "1504.01575", "submitter": "Mathias Berglund", "authors": "Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo K\\\"arkk\\\"ainen,\n  Akos Vetek, Juha Karhunen", "title": "Bidirectional Recurrent Neural Networks as Generative Models -\n  Reconstructing Gaps in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional recurrent neural networks (RNN) are trained to predict both in\nthe positive and negative time directions simultaneously. They have not been\nused commonly in unsupervised tasks, because a probabilistic interpretation of\nthe model has been difficult. Recently, two different frameworks, GSN and NADE,\nprovide a connection between reconstruction and probabilistic modeling, which\nmakes the interpretation possible. As far as we know, neither GSN or NADE have\nbeen studied in the context of time series before. As an example of an\nunsupervised task, we study the problem of filling in gaps in high-dimensional\ntime series with complex dynamics. Although unidirectional RNNs have recently\nbeen trained successfully to model such time series, inference in the negative\ntime direction is non-trivial. We propose two probabilistic interpretations of\nbidirectional RNNs that can be used to reconstruct missing gaps efficiently.\nOur experiments on text data show that both proposed methods are much more\naccurate than unidirectional reconstructions, although a bit less accurate than\na computationally complex bidirectional Bayesian inference on the\nunidirectional RNN. We also provide results on music data for which the\nBayesian inference is computationally infeasible, demonstrating the scalability\nof the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 12:21:03 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 13:29:05 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 07:46:24 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""], ["Honkala", "Mikko", ""], ["K\u00e4rkk\u00e4inen", "Leo", ""], ["Vetek", "Akos", ""], ["Karhunen", "Juha", ""]]}, {"id": "1504.01989", "submitter": "Tyng-Luh Liu", "authors": "Jyh-Jing Hwang and Tyng-Luh Liu", "title": "Pixel-wise Deep Learning for Contour Detection", "comments": "2 pages. arXiv admin note: substantial text overlap with\n  arXiv:1412.6857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. In the experiment of\ncontour detection, we look into the effectiveness of combining per-pixel\nfeatures from different CNN layers and verify their performance on BSDS500.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 14:44:20 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1504.02351", "submitter": "Yongxin Yang", "authors": "Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas,\n  Stan Z. Li and Timothy Hospedales", "title": "When Face Recognition Meets with Deep Learning: an Evaluation of\n  Convolutional Neural Networks for Face Recognition", "comments": "7 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, in particular Convolutional Neural Network (CNN), has achieved\npromising results in face recognition recently. However, it remains an open\nquestion: why CNNs work well and how to design a 'good' architecture. The\nexisting works tend to focus on reporting CNN architectures that work well for\nface recognition rather than investigate the reason. In this work, we conduct\nan extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a\ncommon ground to make our work easily reproducible. Specifically, we use public\ndatabase LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing\nCNNs trained on private databases. We propose three CNN architectures which are\nthe first reported architectures trained using LFW data. This paper\nquantitatively compares the architectures of CNNs and evaluate the effect of\ndifferent implementation choices. We identify several useful properties of\nCNN-FRS. For instance, the dimensionality of the learned features can be\nsignificantly reduced without adverse effect on face recognition accuracy. In\naddition, traditional metric learning method exploiting CNN-learned features is\nevaluated. Experiments show two crucial factors to good CNN-FRS performance are\nthe fusion of multiple CNNs and metric learning. To make our work reproducible,\nsource code and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:27:49 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Hu", "Guosheng", ""], ["Yang", "Yongxin", ""], ["Yi", "Dong", ""], ["Kittler", "Josef", ""], ["Christmas", "William", ""], ["Li", "Stan Z.", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1504.02366", "submitter": "Dhagash Mehta", "authors": "Dhagash Mehta, Crina Grosan", "title": "A Collection of Challenging Optimization Problems in Science,\n  Engineering and Economics", "comments": "Accepted as an invited contribution to the special session on\n  Evolutionary Computation for Nonlinear Equation Systems at the 2015 IEEE\n  Congress on Evolutionary Computation (at Sendai International Center, Sendai,\n  Japan, from 25th to 28th May, 2015.)", "journal-ref": null, "doi": "10.1109/CEC.2015.7257223", "report-no": "ADP-15-9/T911", "categories": "cs.NA cs.MS cs.NE math.AG math.NA math.OC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function optimization and finding simultaneous solutions of a system of\nnonlinear equations (SNE) are two closely related and important optimization\nproblems. However, unlike in the case of function optimization in which one is\nrequired to find the global minimum and sometimes local minima, a database of\nchallenging SNEs where one is required to find stationary points (extrama and\nsaddle points) is not readily available. In this article, we initiate building\nsuch a database of important SNE (which also includes related function\noptimization problems), arising from Science, Engineering and Economics. After\nproviding a short review of the most commonly used mathematical and\ncomputational approaches to find solutions of such systems, we provide a\npreliminary list of challenging problems by writing the Mathematical\nformulation down, briefly explaning the origin and importance of the problem\nand giving a short account on the currently known results, for each of the\nproblems. We anticipate that this database will not only help benchmarking\nnovel numerical methods for solving SNEs and function optimization problems but\nalso will help advancing the corresponding research areas.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:31:25 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Mehta", "Dhagash", ""], ["Grosan", "Crina", ""]]}, {"id": "1504.02462", "submitter": "Suresh Venkatasubramanian", "authors": "Arnab Paul, Suresh Venkatasubramanian", "title": "A Group Theoretic Perspective on Unsupervised Deep Learning", "comments": "2-page version of arXiv:1412.6621 prepared for presentation at ICLR\n  2015 workshop as required by ICLR PC). This version has some minor formatting\n  changes as required by the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called {\\em pretraining}: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:39:05 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 22:03:36 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 06:05:52 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Paul", "Arnab", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1504.02590", "submitter": "Hassan Ismkhan", "authors": "Hassan Ismkhan, Kamran Zamanifar", "title": "Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic\n  Algorithm, Using Symmetric Travelling Salesman Problem", "comments": "arXiv admin note: text overlap with arXiv:1209.5339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Travelling Salesman Problem (TSP) is one of the most famous optimization\nproblems. The Genetic Algorithm (GA) is one of metaheuristics that have been\napplied to TSP. The Crossover and mutation operators are two important elements\nof GA. There are many TSP solver crossover operators. In this paper, we state\nimplementation of some recent TSP solver crossovers at first and then we use\neach of them in GA to solve some Symmetric TSP (STSP) instances and finally\ncompare their effects on speed and accuracy of presented GA.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 08:53:03 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Ismkhan", "Hassan", ""], ["Zamanifar", "Kamran", ""]]}, {"id": "1504.02644", "submitter": "Johannes Lengler", "authors": "Carola Doerr and Johannes Lengler", "title": "OneMax in Black-Box Models with Several Restrictions", "comments": "This is the full version of a paper accepted to GECCO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box complexity studies lower bounds for the efficiency of\ngeneral-purpose black-box optimization algorithms such as evolutionary\nalgorithms and other search heuristics. Different models exist, each one being\ndesigned to analyze a different aspect of typical heuristics such as the memory\nsize or the variation operators in use. While most of the previous works focus\non one particular such aspect, we consider in this work how the combination of\nseveral algorithmic restrictions influence the black-box complexity. Our\ntestbed are so-called OneMax functions, a classical set of test functions that\nis intimately related to classic coin-weighing problems and to the board game\nMastermind.\n  We analyze in particular the combined memory-restricted ranking-based\nblack-box complexity of OneMax for different memory sizes. While its isolated\nmemory-restricted as well as its ranking-based black-box complexity for bit\nstrings of length $n$ is only of order $n/\\log n$, the combined model does not\nallow for algorithms being faster than linear in $n$, as can be seen by\nstandard information-theoretic considerations. We show that this linear bound\nis indeed asymptotically tight. Similar results are obtained for other memory-\nand offspring-sizes. Our results also apply to the (Monte Carlo) complexity of\nOneMax in the recently introduced elitist model, in which only the best-so-far\nsolution can be kept in the memory. Finally, we also provide improved lower\nbounds for the complexity of OneMax in the regarded models.\n  Our result enlivens the quest for natural evolutionary algorithms optimizing\nOneMax in $o(n \\log n)$ iterations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 11:56:17 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 12:37:13 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Doerr", "Carola", ""], ["Lengler", "Johannes", ""]]}, {"id": "1504.02833", "submitter": "Alireza Goudarzi", "authors": "Jens B\\\"urger, Alireza Goudarzi, Darko Stefanovic, Christof Teuscher", "title": "Hierarchical Composition of Memristive Networks for Real-Time Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in materials science have led to physical instantiations of\nself-assembled networks of memristive devices and demonstrations of their\ncomputational capability through reservoir computing. Reservoir computing is an\napproach that takes advantage of collective system dynamics for real-time\ncomputing. A dynamical system, called a reservoir, is excited with a\ntime-varying signal and observations of its states are used to reconstruct a\ndesired output signal. However, such a monolithic assembly limits the\ncomputational power due to signal interdependency and the resulting correlated\nreadouts. Here, we introduce an approach that hierarchically composes a set of\ninterconnected memristive networks into a larger reservoir. We use signal\namplification and restoration to reduce reservoir state correlation, which\nimproves the feature extraction from the input signals. Using the same number\nof output signals, such a hierarchical composition of heterogeneous small\nnetworks outperforms monolithic memristive networks by at least 20% on waveform\ngeneration tasks. On the NARMA-10 task, we reduce the error by up to a factor\nof 2 compared to homogeneous reservoirs with sigmoidal neurons, whereas single\nmemristive networks are unable to produce the correct result. Hierarchical\ncomposition is key for solving more complex tasks with such novel nano-scale\nhardware.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 04:24:07 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 00:34:12 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["B\u00fcrger", "Jens", ""], ["Goudarzi", "Alireza", ""], ["Stefanovic", "Darko", ""], ["Teuscher", "Christof", ""]]}, {"id": "1504.02902", "submitter": "Alexander Kalmanovich", "authors": "Alexander Kalmanovich and Gal Chechik", "title": "Gradual Training Method for Denoising Auto Encoders", "comments": "arXiv admin note: substantial text overlap with arXiv:1412.6257", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked denoising auto encoders (DAEs) are well known to learn useful deep\nrepresentations, which can be used to improve supervised training by\ninitializing a deep network. We investigate a training scheme of a deep DAE,\nwhere DAE layers are gradually added and keep adapting as additional layers are\nadded. We show that in the regime of mid-sized datasets, this gradual training\nprovides a small but consistent improvement over stacked training in both\nreconstruction quality and classification error over stacked training on MNIST\nand CIFAR datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 17:51:41 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Kalmanovich", "Alexander", ""], ["Chechik", "Gal", ""]]}, {"id": "1504.02945", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Cocktail Party Source Separation via Complex Convolution\n  in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional deep neural networks (DNN) are state of the art in many\nengineering problems but have not yet addressed the issue of how to deal with\ncomplex spectrograms. Here, we use circular statistics to provide a convenient\nprobabilistic estimate of spectrogram phase in a complex convolutional DNN. In\na typical cocktail party source separation scenario, we trained a convolutional\nDNN to re-synthesize the complex spectrograms of two source speech signals\ngiven a complex spectrogram of the monaural mixture - a discriminative deep\ntransform (DT). We then used this complex convolutional DT to obtain\nprobabilistic estimates of the magnitude and phase components of the source\nspectrograms. Our separation results are on a par with equivalent binary-mask\nbased non-complex separation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 08:44:56 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1504.02972", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter", "title": "Computing trading strategies based on financial sentiment data using\n  evolutionary optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply evolutionary optimization techniques to compute\noptimal rule-based trading strategies based on financial sentiment data. The\nsentiment data was extracted from the social media service StockTwits to\naccommodate the level of bullishness or bearishness of the online trading\ncommunity towards certain stocks. Numerical results for all stocks from the Dow\nJones Industrial Average (DJIA) index are presented and a comparison to\nclassical risk-return portfolio selection is provided.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 13:39:29 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Hochreiter", "Ronald", ""]]}, {"id": "1504.03212", "submitter": "Carola Doerr", "authors": "Benjamin Doerr and Carola Doerr", "title": "Optimal Parameter Choices Through Self-Adjustment: Applying the 1/5-th\n  Rule in Discrete Settings", "comments": "This is the full version of a paper that is to appear at GECCO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While evolutionary algorithms are known to be very successful for a broad\nrange of applications, the algorithm designer is often left with many\nalgorithmic choices, for example, the size of the population, the mutation\nrates, and the crossover rates of the algorithm. These parameters are known to\nhave a crucial influence on the optimization time, and thus need to be chosen\ncarefully, a task that often requires substantial efforts. Moreover, the\noptimal parameters can change during the optimization process. It is therefore\nof great interest to design mechanisms that dynamically choose best-possible\nparameters. An example for such an update mechanism is the one-fifth success\nrule for step-size adaption in evolutionary strategies. While in continuous\ndomains this principle is well understood also from a mathematical point of\nview, no comparable theory is available for problems in discrete domains.\n  In this work we show that the one-fifth success rule can be effective also in\ndiscrete settings. We regard the $(1+(\\lambda,\\lambda))$~GA proposed in\n[Doerr/Doerr/Ebel: From black-box complexity to designing new genetic\nalgorithms, TCS 2015]. We prove that if its population size is chosen according\nto the one-fifth success rule then the expected optimization time on\n\\textsc{OneMax} is linear. This is better than what \\emph{any} static\npopulation size $\\lambda$ can achieve and is asymptotically optimal also among\nall adaptive parameter choices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 15:16:00 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""]]}, {"id": "1504.03641", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko and Nikos Komodakis", "title": "Learning to Compare Image Patches via Convolutional Neural Networks", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to learn directly from image data (i.e., without\nresorting to manually-designed features) a general similarity function for\ncomparing image patches, which is a task of fundamental importance for many\ncomputer vision problems. To encode such a function, we opt for a CNN-based\nmodel that is trained to account for a wide variety of changes in image\nappearance. To that end, we explore and study multiple neural network\narchitectures, which are specifically adapted to this task. We show that such\nan approach can significantly outperform the state-of-the-art on several\nproblems and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 17:53:51 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1504.04054", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan, Lawrence Carin", "title": "A Generative Model for Deep Convolutional Learning", "comments": "3 pages, 1 figure, ICLR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative model is developed for deep (multi-layered) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\n(refinement) probabilistic learning. Experimental results demonstrate powerful\ncapabilities of the model to learn multi-layer features from images, and\nexcellent classification results are obtained on the MNIST and Caltech 101\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 21:31:58 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1504.04216", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, P. I. Meskin", "title": "Genetic algorithm implementation for effective document subject search", "comments": "in Russian", "journal-ref": "Programmnye produkty i sistemy 4 (2014) 118-126", "doi": "10.15827/0236-235X.108.118-126", "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the software implementation of genetic algorithm for\nidentifying and selecting most relevant results received during sequentially\nexecuted subject search operations. Simulated evolutionary process generates\nsustainable and effective population of search queries, forms search pattern of\ndocuments or semantic core, creates relevant sets of required documents, allows\nautomatic classification of search results. The paper discusses the features of\nsubject search, justifies the use of a genetic algorithm, describes arguments\nof the fitness function and describes basic steps and parameters of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 13:05:19 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Ivanov", "V. K.", ""], ["Meskin", "P. I.", ""]]}, {"id": "1504.04421", "submitter": "Nikhil Padhye", "authors": "Nikhil Padhye and Pulkit Mittal and Kalyanmoy Deb", "title": "Feasibility Preserving Constraint-Handling Strategies for Real Parameter\n  Evolutionary Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Algorithms (EAs) are being routinely applied for a variety of\noptimization tasks, and real-parameter optimization in the presence of\nconstraints is one such important area. During constrained optimization EAs\noften create solutions that fall outside the feasible region; hence a viable\nconstraint- handling strategy is needed. This paper focuses on the class of\nconstraint-handling strategies that repair infeasible solutions by bringing\nthem back into the search space and explicitly preserve feasibility of the\nsolutions. Several existing constraint-handling strategies are studied, and two\nnew single parameter constraint-handling methodologies based on parent-centric\nand inverse parabolic probability (IP) distribution are proposed. The existing\nand newly proposed constraint-handling methods are first studied with PSO, DE,\nGAs, and simulation results on four scalable test-problems under different\nlocation settings of the optimum are presented. The newly proposed\nconstraint-handling methods exhibit robustness in terms of performance and also\nsucceed on search spaces comprising up-to 500 variables while locating the\noptimum within an error of 10$^{-10}$. The working principle of the IP based\nmethods is also demonstrated on (i) some generic constrained optimization\nproblems, and (ii) a classic `Weld' problem from structural design and\nmechanics. The successful performance of the proposed methods clearly exhibits\ntheir efficacy as a generic constrained-handling strategy for a wide range of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 01:55:23 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Padhye", "Nikhil", ""], ["Mittal", "Pulkit", ""], ["Deb", "Kalyanmoy", ""]]}, {"id": "1504.04658", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson, Gerard Roma, Mark D. Plumbley", "title": "Deep Karaoke: Extracting Vocals from Musical Mixtures Using a\n  Convolutional Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and extraction of singing voice from within musical mixtures\nis a key challenge in source separation and machine audition. Recently, deep\nneural networks (DNN) have been used to estimate 'ideal' binary masks for\ncarefully controlled cocktail party speech separation problems. However, it is\nnot yet known whether these methods are capable of generalizing to the\ndiscrimination of voice and non-voice in the context of musical mixtures. Here,\nwe trained a convolutional DNN (of around a billion parameters) to provide\nprobabilistic estimates of the ideal binary mask for separation of vocal sounds\nfrom real-world musical mixtures. We contrast our DNN results with more\ntraditional linear methods. Our approach may be useful for automatic removal of\nvocal sounds from musical mixtures for 'karaoke' type applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 23:07:17 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Simpson", "Andrew J. R.", ""], ["Roma", "Gerard", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1504.04756", "submitter": "James P. Crutchfield", "authors": "Sarah E. Marzen and Michael R. DeWeese and James P. Crutchfield", "title": "Time Resolution Dependence of Information Measures for Spiking Neurons:\n  Atoms, Scaling, and Universality", "comments": "20 pages, 6 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/trdctim.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE math.PR nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mutual information between stimulus and spike-train response is commonly\nused to monitor neural coding efficiency, but neuronal computation broadly\nconceived requires more refined and targeted information measures of\ninput-output joint processes. A first step towards that larger goal is to\ndevelop information measures for individual output processes, including\ninformation generation (entropy rate), stored information (statistical\ncomplexity), predictable information (excess entropy), and active information\naccumulation (bound information rate). We calculate these for spike trains\ngenerated by a variety of noise-driven integrate-and-fire neurons as a function\nof time resolution and for alternating renewal processes. We show that their\ntime-resolution dependence reveals coarse-grained structural properties of\ninterspike interval statistics; e.g., $\\tau$-entropy rates that diverge less\nquickly than the firing rate indicate interspike interval correlations. We also\nfind evidence that the excess entropy and regularized statistical complexity of\ndifferent types of integrate-and-fire neurons are universal in the\ncontinuous-time limit in the sense that they do not depend on mechanism\ndetails. This suggests a surprising simplicity in the spike trains generated by\nthese model neurons. Interestingly, neurons with gamma-distributed ISIs and\nneurons whose spike trains are alternating renewal processes do not fall into\nthe same universality class. These results lead to two conclusions. First, the\ndependence of information measures on time resolution reveals mechanistic\ndetails about spike train generation. Second, information measures can be used\nas model selection tools for analyzing spike train processes.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 20:14:19 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Marzen", "Sarah E.", ""], ["DeWeese", "Michael R.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1504.04788", "submitter": "Wenlin Chen", "authors": "Wenlin Chen and James T. Wilson and Stephen Tyree and Kilian Q.\n  Weinberger and Yixin Chen", "title": "Compressing Neural Networks with the Hashing Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep nets are increasingly used in applications suited for mobile devices,\na fundamental dilemma becomes apparent: the trend in deep learning is to grow\nmodels to absorb ever-increasing data set sizes; however mobile devices are\ndesigned with very little memory and cannot store such large models. We present\na novel network architecture, HashedNets, that exploits inherent redundancy in\nneural networks to achieve drastic reductions in model sizes. HashedNets uses a\nlow-cost hash function to randomly group connection weights into hash buckets,\nand all connections within the same hash bucket share a single parameter value.\nThese parameters are tuned to adjust to the HashedNets weight sharing\narchitecture with standard backprop during training. Our hashing procedure\nintroduces no additional memory overhead, and we demonstrate on several\nbenchmark data sets that HashedNets shrink the storage requirements of neural\nnetworks substantially while mostly preserving generalization performance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 04:24:15 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Chen", "Wenlin", ""], ["Wilson", "James T.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Yixin", ""]]}, {"id": "1504.04909", "submitter": "Jeff Clune Jeff Clune", "authors": "Jean-Baptiste Mouret, Jeff Clune", "title": "Illuminating search spaces by mapping elites", "comments": "Early draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fields use search algorithms, which automatically explore a search space\nto find high-performing solutions: chemists search through the space of\nmolecules to discover new drugs; engineers search for stronger, cheaper, safer\ndesigns, scientists search for models that best explain data, etc. The goal of\nsearch algorithms has traditionally been to return the single\nhighest-performing solution in a search space. Here we describe a new,\nfundamentally different type of algorithm that is more useful because it\nprovides a holistic view of how high-performing solutions are distributed\nthroughout a search space. It creates a map of high-performing solutions at\neach point in a space defined by dimensions of variation that a user gets to\nchoose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)\nalgorithm illuminates search spaces, allowing researchers to understand how\ninteresting attributes of solutions combine to affect performance, either\npositively or, equally of interest, negatively. For example, a drug company may\nwish to understand how performance changes as the size of molecules and their\ncost-to-produce vary. MAP-Elites produces a large diversity of high-performing,\nyet qualitatively different solutions, which can be more helpful than a single,\nhigh-performing solution. Interestingly, because MAP-Elites explores more of\nthe search space, it also tends to find a better overall solution than\nstate-of-the-art search algorithms. We demonstrate the benefits of this new\nalgorithm in three different problem domains ranging from producing modular\nneural networks to designing simulated and real soft robots. Because MAP-\nElites (1) illuminates the relationship between performance and dimensions of\ninterest in solutions, (2) returns a set of high-performing, yet diverse\nsolutions, and (3) improves finding a single, best solution, it will advance\nscience and engineering.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 01:17:00 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Mouret", "Jean-Baptiste", ""], ["Clune", "Jeff", ""]]}, {"id": "1504.04914", "submitter": "Peng Yang", "authors": "Ke Tang, Peng Yang, Xin Yao", "title": "Negatively Correlated Search", "comments": null, "journal-ref": "IEEE Journal on Selected Areas in Communications, Vol. 34, Issue\n  3, pp. 1-9, March 2016", "doi": "10.1109/JSAC.2016.2525458", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Algorithms (EAs) have been shown to be powerful tools for\ncomplex optimization problems, which are ubiquitous in both communication and\nbig data analytics. This paper presents a new EA, namely Negatively Correlated\nSearch (NCS), which maintains multiple individual search processes in parallel\nand models the search behaviors of individual search processes as probability\ndistributions. NCS explicitly promotes negatively correlated search behaviors\nby encouraging differences among the probability distributions (search\nbehaviors). By this means, individual search processes share information and\ncooperate with each other to search diverse regions of a search space, which\nmakes NCS a promising method for non-convex optimization. The cooperation\nscheme of NCS could also be regarded as a novel diversity preservation scheme\nthat, different from other existing schemes, directly promotes diversity at the\nlevel of search behaviors rather than merely trying to maintain diversity among\ncandidate solutions. Empirical studies showed that NCS is competitive to\nwell-established search methods in the sense that NCS achieved the best overall\nperformance on 20 multimodal (non-convex) continuous optimization problems. The\nadvantages of NCS over state-of-the-art approaches are also demonstrated with a\ncase study on the synthesis of unequally spaced linear antenna arrays.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 01:51:39 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 02:41:37 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Tang", "Ke", ""], ["Yang", "Peng", ""], ["Yao", "Xin", ""]]}, {"id": "1504.05070", "submitter": "Han Zhao", "authors": "Han Zhao, Zhengdong Lu, Pascal Poupart", "title": "Self-Adaptive Hierarchical Sentence Model", "comments": "8 pages, 7 figures, accepted as a full paper at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately model a sentence at varying stages (e.g.,\nword-phrase-sentence) plays a central role in natural language processing. As\nan effort towards this goal we propose a self-adaptive hierarchical sentence\nmodel (AdaSent). AdaSent effectively forms a hierarchy of representations from\nwords to phrases and then to sentences through recursive gated local\ncomposition of adjacent segments. We design a competitive mechanism (through\ngating networks) to allow the representations of the same sentence to be\nengaged in a particular learning task (e.g., classification), therefore\neffectively mitigating the gradient vanishing problem persistent in other\nrecursive models. Both qualitative and quantitative analysis shows that AdaSent\ncan automatically form and select the representations suitable for the task at\nhand during training, yielding superior classification performance over\ncompetitor models on 5 benchmark data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 14:26:41 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 17:12:56 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Zhao", "Han", ""], ["Lu", "Zhengdong", ""], ["Poupart", "Pascal", ""]]}, {"id": "1504.05095", "submitter": "Bassam AlKindy Mr.", "authors": "Bassam AlKindy, Christophe Guyeux, Jean-Fran\\c{c}ois Couchot, Michel\n  Salomon, Christian Parisod, and Jacques M. Bahi", "title": "Hybrid Genetic Algorithm and Lasso Test Approach for Inferring Well\n  Supported Phylogenetic Trees based on Subsets of Chloroplastic Core Genes", "comments": "15 pages, 7 figures, 2nd International Conference on Algorithms for\n  Computational Biology, AlCoB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of completely sequenced chloroplast genomes increases rapidly\nevery day, leading to the possibility to build large scale phylogenetic trees\nof plant species. Considering a subset of close plant species defined according\nto their chloroplasts, the phylogenetic tree that can be inferred by their core\ngenes is not necessarily well supported, due to the possible occurrence of\n\"problematic\" genes (i.e., homoplasy, incomplete lineage sorting, horizontal\ngene transfers, etc.) which may blur phylogenetic signal. However, a\ntrustworthy phylogenetic tree can still be obtained if the number of\nproblematic genes is low, the problem being to determine the largest subset of\ncore genes that produces the best supported tree. To discard problematic genes\nand due to the overwhelming number of possible combinations, we propose an\nhybrid approach that embeds both genetic algorithms and statistical tests.\nGiven a set of organisms, the result is a pipeline of many stages for the\nproduction of well supported phylogenetic trees. The proposal has been applied\nto different cases of plant families, leading to encouraging results for these\nfamilies.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 15:50:46 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["AlKindy", "Bassam", ""], ["Guyeux", "Christophe", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Salomon", "Michel", ""], ["Parisod", "Christian", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1504.05143", "submitter": "David Kappel", "authors": "David Kappel, Stefan Habenschuss, Robert Legenstein, Wolfgang Maass", "title": "Network Plasticity as Bayesian Inference", "comments": "33 pages, 5 figures, the supplement is available on the author's web\n  page http://www.igi.tugraz.at/kappel", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004485", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General results from statistical learning theory suggest to understand not\nonly brain computations, but also brain plasticity as probabilistic inference.\nBut a model for that has been missing. We propose that inherently stochastic\nfeatures of synaptic plasticity and spine motility enable cortical networks of\nneurons to carry out probabilistic inference by sampling from a posterior\ndistribution of network configurations. This model provides a viable\nalternative to existing models that propose convergence of parameters to\nmaximum likelihood values. It explains how priors on weight distributions and\nconnection probabilities can be merged optimally with learned experience, how\ncortical networks can generalize learned information so well to novel\nexperiences, and how they can compensate continuously for unforeseen\ndisturbances of the network. The resulting new theory of network plasticity\nexplains from a functional perspective a number of experimental data on\nstochastic aspects of synaptic plasticity that previously appeared to be quite\npuzzling.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 18:18:18 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Kappel", "David", ""], ["Habenschuss", "Stefan", ""], ["Legenstein", "Robert", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1504.05158", "submitter": "Piotr Szwed PhD", "authors": "Piotr Szwed and Wojciech Chmiel", "title": "Multi-swarm PSO algorithm for the Quadratic Assignment Problem: a\n  massive parallel implementation on the OpenCL platform", "comments": "2 tables 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-swarm PSO algorithm for the Quadratic Assignment\nProblem (QAP) implemented on OpenCL platform. Our work was motivated by results\nof time efficiency tests performed for single-swarm algorithm implementation\nthat showed clearly that the benefits of a parallel execution platform can be\nfully exploited, if the processed population is large. The described algorithm\ncan be executed in two modes: with independent swarms or with migration. We\ndiscuss the algorithm construction, as well as we report results of tests\nperformed on several problem instances from the QAPLIB library. During the\nexperiments the algorithm was configured to process large populations. This\nallowed us to collect statistical data related to values of goal function\nreached by individual particles. We use them to demonstrate on two test cases\nthat although single particles seem to behave chaotically during the\noptimization process, when the whole population is analyzed, the probability\nthat a particle will select a near-optimal solution grows.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 18:58:08 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Szwed", "Piotr", ""], ["Chmiel", "Wojciech", ""]]}, {"id": "1504.05619", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh and Shahryar Rahnamayan", "title": "Learning Opposites with Evolving Rules", "comments": "Accepted for publication in The 2015 IEEE International Conference on\n  Fuzzy Systems (FUZZ-IEEE 2015), August 2-5, 2015, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of opposition-based learning was introduced 10 years ago. Since then\na noteworthy group of researchers has used some notions of oppositeness to\nimprove existing optimization and learning algorithms. Among others,\nevolutionary algorithms, reinforcement agents, and neural networks have been\nreportedly extended into their opposition-based version to become faster and/or\nmore accurate. However, most works still use a simple notion of opposites,\nnamely linear (or type- I) opposition, that for each $x\\in[a,b]$ assigns its\nopposite as $\\breve{x}_I=a+b-x$. This, of course, is a very naive estimate of\nthe actual or true (non-linear) opposite $\\breve{x}_{II}$, which has been\ncalled type-II opposite in literature. In absence of any knowledge about a\nfunction $y=f(\\mathbf{x})$ that we need to approximate, there seems to be no\nalternative to the naivety of type-I opposition if one intents to utilize\noppositional concepts. But the question is if we can receive some level of\naccuracy increase and time savings by using the naive opposite estimate\n$\\breve{x}_I$ according to all reports in literature, what would we be able to\ngain, in terms of even higher accuracies and more reduction in computational\ncomplexity, if we would generate and employ true opposites? This work\nintroduces an approach to approximate type-II opposites using evolving fuzzy\nrules when we first perform opposition mining. We show with multiple examples\nthat learning true opposites is possible when we mine the opposites from the\ntraining data to subsequently approximate $\\breve{x}_{II}=f(\\mathbf{x},y)$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 22:16:17 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Tizhoosh", "Hamid R.", ""], ["Rahnamayan", "Shahryar", ""]]}, {"id": "1504.05766", "submitter": "Muharrem D\\\"u\\u{g}enci", "authors": "Muharrem D\\\"u\\u{g}enci", "title": "Honeybees-inspired heuristic algorithms for numerical optimisation", "comments": "17 pages, 3 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": "KU-IE-MD-001", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm intelligence is all about developing collective behaviours to solve\ncomplex, ill-structured and large-scale problems. Efficiency in collective\nbehaviours depends on how to harmonise the individual contributions so that a\ncomplementary collective effort can be achieved to offer a useful solution. The\nmain points in organising the harmony remains as managing the diversification\nand intensification actions appropriately, where the efficiency of collective\nbehaviours depends on blending these two actions appropriately. In this study,\ntwo swarm intelligence algorithms inspired of natural honeybee colonies have\nbeen overviewed with many respects and two new revisions and a hybrid version\nhave been studied to improve the efficiencies in solving numerical optimisation\nproblems, which are well-known hard benchmarks. Consequently, the revisions and\nespecially the hybrid algorithm proposed have outperformed the two original bee\nalgorithms in solving these very hard numerical optimisation benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 12:46:07 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["D\u00fc\u011fenci", "Muharrem", ""]]}, {"id": "1504.05767", "submitter": "Lorenz K. Muller", "authors": "Lorenz K. Muller and Giacomo Indiveri", "title": "Rounding Methods for Neural Networks with Low Resolution Synaptic\n  Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network algorithms simulated on standard computing platforms typically\nmake use of high resolution weights, with floating-point notation. However, for\ndedicated hardware implementations of such algorithms, fixed-point synaptic\nweights with low resolution are preferable. The basic approach of reducing the\nresolution of the weights in these algorithms by standard rounding methods\nincurs drastic losses in performance. To reduce the resolution further, in the\nextreme case even to binary weights, more advanced techniques are necessary. To\nthis end, we propose two methods for mapping neural network algorithms with\nhigh resolution weights to corresponding algorithms that work with low\nresolution weights and demonstrate that their performance is substantially\nbetter than standard rounding. We further use these methods to investigate the\nperformance of three common neural network algorithms under fixed memory size\nof the weight matrix with different weight resolutions. We show that dedicated\nhardware systems, whose technology dictates very low weight resolutions (be\nthey electronic or biological) could in principle implement the algorithms we\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 12:47:32 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Muller", "Lorenz K.", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1504.06063", "submitter": "Lin Ma", "authors": "Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li", "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence", "comments": "Accepted by ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose multimodal convolutional neural networks (m-CNNs)\nfor matching image and sentence. Our m-CNN provides an end-to-end framework\nwith convolutional architectures to exploit image representation, word\ncomposition, and the matching relations between the two modalities. More\nspecifically, it consists of one image CNN encoding the image content, and one\nmatching CNN learning the joint representation of image and sentence. The\nmatching CNN composes words to different semantic fragments and learns the\ninter-modal relations between image and the composed fragments at different\nlevels, thus fully exploit the matching relations between image and sentence.\nExperimental results on benchmark databases of bidirectional image and sentence\nretrieval demonstrate that the proposed m-CNNs can effectively capture the\ninformation necessary for image and sentence matching. Specifically, our\nproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and\nMicrosoft COCO databases achieve the state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 07:10:13 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 01:47:05 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 08:09:54 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2015 07:30:53 GMT"}, {"version": "v5", "created": "Sat, 29 Aug 2015 09:35:09 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ma", "Lin", ""], ["Lu", "Zhengdong", ""], ["Shang", "Lifeng", ""], ["Li", "Hang", ""]]}, {"id": "1504.06260", "submitter": "Jorge Perez Heredia", "authors": "Tiago Paix\\~ao and Jorge P\\'erez Heredia and Dirk Sudholt and Barbora\n  Trubenov\\'a", "title": "First Steps Towards a Runtime Comparison of Natural and Artificial\n  Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) form a popular optimisation paradigm inspired\nby natural evolution. In recent years the field of evolutionary computation has\ndeveloped a rigorous analytical theory to analyse their runtime on many\nillustrative problems. Here we apply this theory to a simple model of natural\nevolution. In the Strong Selection Weak Mutation (SSWM) evolutionary regime the\ntime between occurrence of new mutations is much longer than the time it takes\nfor a new beneficial mutation to take over the population. In this situation,\nthe population only contains copies of one genotype and evolution can be\nmodelled as a (1+1)-type process where the probability of accepting a new\ngenotype (improvements or worsenings) depends on the change in fitness.\n  We present an initial runtime analysis of SSWM, quantifying its performance\nfor various parameters and investigating differences to the (1+1)EA. We show\nthat SSWM can have a moderate advantage over the (1+1)EA at crossing fitness\nvalleys and study an example where SSWM outperforms the (1+1)EA by taking\nadvantage of information on the fitness gradient.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 17:13:31 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 12:26:24 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Paix\u00e3o", "Tiago", ""], ["Heredia", "Jorge P\u00e9rez", ""], ["Sudholt", "Dirk", ""], ["Trubenov\u00e1", "Barbora", ""]]}, {"id": "1504.06363", "submitter": "Frank Neumann", "authors": "Frank Neumann and Carsten Witt", "title": "On the Runtime of Randomized Local Search and Simple Evolutionary\n  Algorithms for Dynamic Makespan Scheduling", "comments": "Conference version appears at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms have been frequently used for dynamic optimization\nproblems. With this paper, we contribute to the theoretical understanding of\nthis research area. We present the first computational complexity analysis of\nevolutionary algorithms for a dynamic variant of a classical combinatorial\noptimization problem, namely makespan scheduling. We study the model of a\nstrong adversary which is allowed to change one job at regular intervals.\nFurthermore, we investigate the setting of random changes. Our results show\nthat randomized local search and a simple evolutionary algorithm are very\neffective in dynamically tracking changes made to the problem instance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 23:13:19 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Neumann", "Frank", ""], ["Witt", "Carsten", ""]]}, {"id": "1504.06580", "submitter": "Cicero dos Santos", "authors": "Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou", "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "comments": "Accepted as a long paper in the 53rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification is an important semantic processing task for which\nstate-ofthe-art systems still rely on costly handcrafted features. In this work\nwe tackle the relation classification task using a convolutional neural network\nthat performs classification by ranking (CR-CNN). We propose a new pairwise\nranking loss function that makes it easy to reduce the impact of artificial\nclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,\nwhich is designed for the task of classifying the relationship between two\nnominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art\nfor this dataset and achieve a F1 of 84.1 without using any costly handcrafted\nfeatures. Additionally, our experimental results show that: (1) our approach is\nmore effective than CNN followed by a softmax classifier; (2) omitting the\nrepresentation of the artificial class Other improves both precision and\nrecall; and (3) using only word embeddings as input features is enough to\nachieve state-of-the-art results if we consider only the text between the two\ntarget nominals.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 17:50:33 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 13:58:05 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1504.06859", "submitter": "Fernando Lobo", "authors": "Fernando G. Lobo and Mosab Bazargani", "title": "When Hillclimbers Beat Genetic Algorithms in Multimodal Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown in the past that a multistart hillclimbing strategy\ncompares favourably to a standard genetic algorithm with respect to solving\ninstances of the multimodal problem generator. We extend that work and verify\nif the utilization of diversity preservation techniques in the genetic\nalgorithm changes the outcome of the comparison. We do so under two scenarios:\n(1) when the goal is to find the global optimum, (2) when the goal is to find\nall optima.\n  A mathematical analysis is performed for the multistart hillclimbing\nalgorithm and a through empirical study is conducted for solving instances of\nthe multimodal problem generator with increasing number of optima, both with\nthe hillclimbing strategy as well as with genetic algorithms with niching.\nAlthough niching improves the performance of the genetic algorithm, it is still\ninferior to the multistart hillclimbing strategy on this class of problems.\n  An idealized niching strategy is also presented and it is argued that its\nperformance should be close to a lower bound of what any evolutionary algorithm\ncan do on this class of problems.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 18:13:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Lobo", "Fernando G.", ""], ["Bazargani", "Mosab", ""]]}, {"id": "1504.07225", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran", "title": "Correlational Neural Networks", "comments": "27 pages. To Appear in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common Representation Learning (CRL), wherein different descriptions (or\nviews) of the data are embedded in a common subspace, is receiving a lot of\nattention recently. Two popular paradigms here are Canonical Correlation\nAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA\nbased approaches learn a joint representation by maximizing correlation of the\nviews when projected to the common subspace. AE based methods learn a common\nrepresentation by minimizing the error of reconstructing the two views. Each of\nthese approaches has its own advantages and disadvantages. For example, while\nCCA based approaches outperform AE based approaches for the task of transfer\nlearning, they are not as scalable as the latter. In this work we propose an AE\nbased approach called Correlational Neural Network (CorrNet), that explicitly\nmaximizes correlation among the views when projected to the common subspace.\nThrough a series of experiments, we demonstrate that the proposed CorrNet is\nbetter than the above mentioned approaches with respect to its ability to learn\ncorrelated common representations. Further, we employ CorrNet for several cross\nlanguage tasks and show that the representations learned using CorrNet perform\nbetter than the ones learned using other state of the art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:51:34 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 20:34:28 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 19:14:05 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Chandar", "Sarath", ""], ["Khapra", "Mitesh M.", ""], ["Larochelle", "Hugo", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1504.07278", "submitter": "Vipul Arora", "authors": "Vipul Arora, Laxmidhar Behera and Ajay Pratap Yadav", "title": "Optimal Convergence Rate in Feed Forward Neural Networks using HJB\n  Equation", "comments": "9 pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A control theoretic approach is presented in this paper for both batch and\ninstantaneous updates of weights in feed-forward neural networks. The popular\nHamilton-Jacobi-Bellman (HJB) equation has been used to generate an optimal\nweight update law. The remarkable contribution in this paper is that closed\nform solutions for both optimal cost and weight update can be achieved for any\nfeed-forward network using HJB equation in a simple yet elegant manner. The\nproposed approach has been compared with some of the existing best performing\nlearning algorithms. It is found as expected that the proposed approach is\nfaster in convergence in terms of computational time. Some of the benchmark\ntest data such as 8-bit parity, breast cancer and credit approval, as well as\n2D Gabor function have been used to validate our claims. The paper also\ndiscusses issues related to global optimization. The limitations of popular\ndeterministic weight update laws are critiqued and the possibility of global\noptimization using HJB formulation is discussed. It is hoped that the proposed\nalgorithm will bring in a lot of interest in researchers working in developing\nfast learning algorithms and global optimization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 21:09:15 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Arora", "Vipul", ""], ["Behera", "Laxmidhar", ""], ["Yadav", "Ajay Pratap", ""]]}, {"id": "1504.07327", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Farhad Pouladi, Siamak Talebi", "title": "Toward Smart Power Grids: Communication Network Design for Power Grids\n  Synchronization", "comments": "This paper has been presented at the 27th International Power System\n  Conference in 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In smart power grids, keeping the synchronicity of generators and the\ncorresponding controls is of great importance. To do so, a simple model is\nemployed in terms of swing equation to represent the interactions among\ndynamics of generators and feedback control. In case of having a communication\nnetwork available, the control can be done based on the transmitted\nmeasurements by the communication network. The stability of system is denoted\nby the largest eigenvalue of the weighted sum of the Laplacian matrices of the\ncommunication infrastructure and power network. In this work, we use graph\ntheory to model the communication network as a graph problem. Then, Ant Colony\nSystem (ACS) is employed for optimum design of above graph for synchronization\nof power grids. Performance evaluation of the proposed method for the 39-bus\nNew England power system versus methods such as exhaustive search and Rayleigh\nquotient approximation indicates feasibility and effectiveness of our method\nfor even large scale smart power grids.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 01:51:15 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Pouladi", "Farhad", ""], ["Talebi", "Siamak", ""]]}, {"id": "1504.07329", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Hossein Nezamabadi-pour, Saeid Saryazdi and\n  Fereydoun Farrahi-Moghaddam", "title": "Combined A*-Ants Algorithm: A New Multi-Parameter Vehicle Navigation\n  Scheme", "comments": "This paper has been presented at the 16th Iranian Conference on\n  Electrical Engineering in 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a multi-parameter A*(A- star)-ants based algorithm is proposed\nin order to find the best optimized multi-parameter path between two desired\npoints in regions. This algorithm recognizes paths, according to user desired\nparameters using electronic maps. The proposed algorithm is a combination of A*\nand ants algorithm in which the proposed A* algorithm is the prologue to the\nsuggested ant based algorithm .In fact, this A* algorithm invigorates some\npaths pheromones in ants algorithm. As one of implementations of this method,\nthis algorithm was applied on a part of Kerman city, Iran as a multi-parameter\nvehicle navigator. It finds the best optimized multi-parameter direction\nbetween two desired junctions based on city traveler parameters. Comparison\nresults between the proposed method and ants algorithm demonstrates efficiency\nand lower cost function results of the proposed method versus ants algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 01:54:49 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Nezamabadi-pour", "Hossein", ""], ["Saryazdi", "Saeid", ""], ["Farrahi-Moghaddam", "Fereydoun", ""]]}, {"id": "1504.07395", "submitter": "Thanh-Le Ha", "authors": "Thanh-Le Ha, Jan Niehues, Alex Waibel", "title": "Lexical Translation Model Using a Deep Neural Network Architecture", "comments": null, "journal-ref": "Proceedings of the 11th International Workshop on Spoken Language\n  Translation (IWSLT 2014), page 223-229, Lake Tahoe - US, December 4th and\n  5th, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine the advantages of a model using global source\nsentence contexts, the Discriminative Word Lexicon, and neural networks. By\nusing deep neural networks instead of the linear maximum entropy model in the\nDiscriminative Word Lexicon models, we are able to leverage dependencies\nbetween different source words due to the non-linearity. Furthermore, the\nmodels for different target words can share parameters and therefore data\nsparsity problems are effectively reduced.\n  By using this approach in a state-of-the-art translation system, we can\nimprove the performance by up to 0.5 BLEU points for three different language\npairs on the TED translation task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 09:43:40 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Ha", "Thanh-Le", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1504.07571", "submitter": "Murat Okandan", "authors": "Murat Okandan", "title": "Can Machines Truly Think", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Can machines truly think? This question and its answer have many implications\nthat depend, in large part, on any number of assumptions underlying how the\nissue has been addressed or considered previously. A crucial question, and one\nthat is almost taken for granted, is the starting point for this discussion:\nCan \"thought\" be achieved or emulated by algorithmic procedures?\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 17:08:18 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Okandan", "Murat", ""]]}, {"id": "1504.07846", "submitter": "Christian Schulz", "authors": "Nitin Ahuja, Matthias Bender, Peter Sanders, Christian Schulz and\n  Andreas Wagner", "title": "Incorporating Road Networks into Territory Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of basic areas, the territory design problem asks to create a\npredefined number of territories, each containing at least one basic area, such\nthat an objective function is optimized. Desired properties of territories\noften include a reasonable balance, compact form, contiguity and small average\njourney times which are usually encoded in the objective function or formulated\nas constraints. We address the territory design problem by developing graph\ntheoretic models that also consider the underlying road network. The derived\ngraph models enable us to tackle the territory design problem by modifying\ngraph partitioning algorithms and mixed integer programming formulations so\nthat the objective of the planning problem is taken into account. We test and\ncompare the algorithms on several real world instances.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:11:29 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 12:40:01 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Ahuja", "Nitin", ""], ["Bender", "Matthias", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Wagner", "Andreas", ""]]}, {"id": "1504.08022", "submitter": "Hongyu Guo Ph.D", "authors": "Hongyu Guo, Xiaodan Zhu, Martin Renqiang Min", "title": "A Deep Learning Model for Structured Outputs with High-order Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications are associated with structured data, where not\nonly input but also output has interplay. However, typical classification and\nregression models often lack the ability of simultaneously exploring high-order\ninteraction within input and that within output. In this paper, we present a\ndeep learning model aiming to generate a powerful nonlinear functional mapping\nfrom structured input to structured output. More specifically, we propose to\nintegrate high-order hidden units, guided discriminative pretraining, and\nhigh-order auto-encoders for this purpose. We evaluate the model with three\ndatasets, and obtain state-of-the-art performances among competitive methods.\nOur current work focuses on structured output regression, which is a less\nexplored area, although the model can be extended to handle structured label\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 20:58:52 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Guo", "Hongyu", ""], ["Zhu", "Xiaodan", ""], ["Min", "Martin Renqiang", ""]]}, {"id": "1504.08117", "submitter": "Jun He", "authors": "Jun He and Guangming Lin", "title": "Average Convergence Rate of Evolutionary Algorithms", "comments": null, "journal-ref": "IEEE Transactions on Evolutionary Computation 20.2 (2016): 316-321", "doi": "10.1109/TEVC.2015.2444793", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary optimization, it is important to understand how fast\nevolutionary algorithms converge to the optimum per generation, or their\nconvergence rate. This paper proposes a new measure of the convergence rate,\ncalled average convergence rate. It is a normalised geometric mean of the\nreduction ratio of the fitness difference per generation. The calculation of\nthe average convergence rate is very simple and it is applicable for most\nevolutionary algorithms on both continuous and discrete optimization. A\ntheoretical study of the average convergence rate is conducted for discrete\noptimization. Lower bounds on the average convergence rate are derived. The\nlimit of the average convergence rate is analysed and then the asymptotic\naverage convergence rate is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 08:35:47 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 10:36:33 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2015 10:31:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["He", "Jun", ""], ["Lin", "Guangming", ""]]}, {"id": "1504.08168", "submitter": "Jan \\v{Z}egklitz", "authors": "Jan \\v{Z}egklitz and Petr Po\\v{s}\\'ik", "title": "Model Selection and Overfitting in Genetic Programming: Empirical Study\n  [Extended Version]", "comments": "8 pages, 12 figures, full paper for GECCO 2015 (accepted as poster,\n  this is the original paper submitted to the conference); added subtitle and\n  removed copyright text at the first page, fixed some typography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Programming has been very successful in solving a large area of\nproblems but its use as a machine learning algorithm has been limited so far.\nOne of the reasons is the problem of overfitting which cannot be solved or\nsuppresed as easily as in more traditional approaches. Another problem, closely\nrelated to overfitting, is the selection of the final model from the\npopulation.\n  In this article we present our research that addresses both problems:\noverfitting and model selection. We compare several ways of dealing with\novefitting, based on Random Sampling Technique (RST) and on using a validation\nset, all with an emphasis on model selection. We subject each approach to a\nthorough testing on artificial and real--world datasets and compare them with\nthe standard approach, which uses the full training data, as a baseline.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 11:12:52 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 14:29:34 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["\u017degklitz", "Jan", ""], ["Po\u0161\u00edk", "Petr", ""]]}, {"id": "1504.08215", "submitter": "Tapani Raiko", "authors": "Antti Rasmus, Harri Valpola, Tapani Raiko", "title": "Lateral Connections in Denoising Autoencoders Support Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a deep denoising autoencoder with lateral connections can be used\nas an auxiliary unsupervised learning task to support supervised learning. The\nproposed model is trained to minimize simultaneously the sum of supervised and\nunsupervised cost functions by back-propagation, avoiding the need for\nlayer-wise pretraining. It improves the state of the art significantly in the\npermutation-invariant MNIST classification task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:26:46 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Valpola", "Harri", ""], ["Raiko", "Tapani", ""]]}, {"id": "1504.08291", "submitter": "Raja Giryes", "authors": "Raja Giryes and Guillermo Sapiro and Alex M. Bronstein", "title": "Deep Neural Networks with Random Gaussian Weights: A Universal\n  Classification Strategy?", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2546221", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three important properties of a classification machinery are: (i) the system\npreserves the core information of the input data; (ii) the training examples\nconvey information about unseen data; and (iii) the system is able to treat\ndifferently points from different classes. In this work we show that these\nfundamental properties are satisfied by the architecture of deep neural\nnetworks. We formally prove that these networks with random Gaussian weights\nperform a distance-preserving embedding of the data, with a special treatment\nfor in-class and out-of-class data. Similar points at the input of the network\nare likely to have a similar output. The theoretical analysis of deep networks\nhere presented exploits tools used in the compressed sensing and dictionary\nlearning literature, thereby making a formal connection between these important\ntopics. The derived results allow drawing conclusions on the metric learning\nproperties of the network and their relation to its structure, as well as\nproviding bounds on the required size of the training set such that the\ntraining examples would represent faithfully the unseen data. The results are\nvalidated with state-of-the-art trained networks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 16:14:52 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 11:30:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 13:53:11 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2016 19:25:05 GMT"}, {"version": "v5", "created": "Mon, 14 Mar 2016 19:17:08 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex M.", ""]]}]