[{"id": "2103.00051", "submitter": "Andreas St\\\"ockel", "authors": "Andreas St\\\"ockel", "title": "Constructing Dampened LTI Systems Generating Polynomial Bases", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "CTN-TR-20210226-017", "categories": "eess.SY cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative derivation of the LTI system underlying the\nLegendre Delay Network (LDN). To this end, we first construct an LTI system\nthat generates the Legendre polynomials. We then dampen the system by\napproximating a windowed impulse response, using what we call a \"delay\nre-encoder\". The resulting LTI system is equivalent to the LDN system. This\ntechnique can be applied to arbitrary polynomial bases, although there\ntypically is no closed-form equation that describes the state-transition\nmatrix.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 21:14:19 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 18:36:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["St\u00f6ckel", "Andreas", ""]]}, {"id": "2103.00180", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash, Sharad Chitlangia, Aditya Ahuja, Ashwin Srinivasan", "title": "Incorporating Domain Knowledge into Deep Neural Networks", "comments": "Submitted to IJCAI-2021 Survey Track (6+2 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a survey of ways in which domain-knowledge has been included when\nconstructing models with neural networks. The inclusion of domain-knowledge is\nof special interest not just to constructing scientific assistants, but also,\nmany other areas that involve understanding data using human-machine\ncollaboration. In many such instances, machine-based model construction may\nbenefit significantly from being provided with human-knowledge of the domain\nencoded in a sufficiently precise form. This paper examines two broad\napproaches to encode such knowledge--as logical and numerical constraints--and\ndescribes techniques and results obtained in several sub-categories under each\nof these approaches.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:39:43 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 17:47:22 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Chitlangia", "Sharad", ""], ["Ahuja", "Aditya", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "2103.00201", "submitter": "Giambattista Gruosso", "authors": "Giulia Crocioni, Giambattista Gruosso, Danilo Pau, Davide Denaro,\n  Luigi Zambrano, Giuseppe di Giore", "title": "Characterization of Neural Networks Automatically Mapped on\n  Automotive-grade Microcontrollers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Nowadays, Neural Networks represent a major expectation for the realization\nof powerful Deep Learning algorithms, which can determine several physical\nsystems' behaviors and operations. Computational resources required for model,\ntraining, and running are large, especially when related to the amount of data\nthat Neural Networks typically need to generalize. The latest TinyML\ntechnologies allow integrating pre-trained models on embedded systems, allowing\nmaking computing at the edge faster, cheaper, and safer. Although these\ntechnologies originated in the consumer and industrial worlds, many sectors can\ngreatly benefit from them, such as the automotive industry. In this paper, we\npresent a framework for implementing Neural Network-based models on a family of\nautomotive Microcontrollers, showing their efficiency in two case studies\napplied to vehicles: intrusion detection on the Controller Area Network bus and\nresidual capacity estimation in Lithium-Ion batteries, widely used in Electric\nVehicles.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 12:16:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Crocioni", "Giulia", ""], ["Gruosso", "Giambattista", ""], ["Pau", "Danilo", ""], ["Denaro", "Davide", ""], ["Zambrano", "Luigi", ""], ["di Giore", "Giuseppe", ""]]}, {"id": "2103.00421", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif, Muhammad\n  Shafique", "title": "SparkXD: A Framework for Resilient and Energy-Efficient Spiking Neural\n  Network Inference using Approximate DRAM", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have the potential for achieving low energy\nconsumption due to their biologically sparse computation. Several studies have\nshown that the off-chip memory (DRAM) accesses are the most energy-consuming\noperations in SNN processing. However, state-of-the-art in SNN systems do not\noptimize the DRAM energy-per-access, thereby hindering achieving high\nenergy-efficiency. To substantially minimize the DRAM energy-per-access, a key\nknob is to reduce the DRAM supply voltage but this may lead to DRAM errors\n(i.e., the so-called approximate DRAM). Towards this, we propose SparkXD, a\nnovel framework that provides a comprehensive conjoint solution for resilient\nand energy-efficient SNN inference using low-power DRAMs subjected to\nvoltage-induced errors. The key mechanisms of SparkXD are: (1) improving the\nSNN error tolerance through fault-aware training that considers bit errors from\napproximate DRAM, (2) analyzing the error tolerance of the improved SNN model\nto find the maximum tolerable bit error rate (BER) that meets the targeted\naccuracy constraint, and (3) energy-efficient DRAM data mapping for the\nresilient SNN model that maps the weights in the appropriate DRAM location to\nminimize the DRAM access energy. Through these mechanisms, SparkXD mitigates\nthe negative impact of DRAM (approximation) errors, and provides the required\naccuracy. The experimental results show that, for a target accuracy within 1%\nof the baseline design (i.e., SNN without DRAM errors), SparkXD reduces the\nDRAM energy by ca. 40% on average across different network sizes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:12:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2103.00424", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Shafique", "title": "SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with\n  Continual and Unsupervised Learning Capabilities in Dynamic Environments", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised\nand continual learning capabilities because of their biological plausibility,\nbut their complexity still poses a serious research challenge to enable their\nenergy-efficient design for resource-constrained scenarios (like embedded\nsystems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for\nenergy-efficient SNNs with continual and unsupervised learning capabilities in\ndynamic environments, for both the training and inference phases. It is\nachieved through the following multiple diverse mechanisms: 1) reduction of\nneuronal operations, by replacing the inhibitory neurons with direct lateral\ninhibitions; 2) a memory- and energy-constrained SNN model search algorithm\nthat employs analytical models to estimate the memory footprint and energy\nconsumption of different candidate SNN models and selects a Pareto-optimal SNN\nmodel; and 3) a lightweight continual and unsupervised learning algorithm that\nemploys adaptive learning rates, adaptive membrane threshold potential, weight\ndecay, and reduction of spurious updates. Our experimental results show that,\nfor a network with 400 excitatory neurons, our SpikeDyn reduces the energy\nconsumption on average by 51% for training and by 37% for inference, as\ncompared to the state-of-the-art. Due to the improved learning algorithm,\nSpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art,\nfor classifying the most recently learned task, and by 8% on average for the\npreviously learned tasks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:26:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2103.00441", "submitter": "Frederic Jumelle", "authors": "Frederic Jumelle, Kelvin So, and Didan Deng", "title": "Individual risk profiling for portable devices using a neural network to\n  process the cognitive reactions and the emotional responses to a multivariate\n  situational risk assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we are presenting a novel method and system for\nneuropsychological performance testing that can establish a link between\ncognition and emotion. It comprises a portable device used to interact with a\ncloud service which stores user information under username and is logged into\nby the user through the portable device; the user information is directly\ncaptured through the device and is processed by artificial neural network; and\nthis tridimensional information comprises user cognitive reactions, user\nemotional responses and user chronometrics. The multivariate situational risk\nassessment is used to evaluate the performance of the subject by capturing the\n3 dimensions of each reaction to a series of 30 dichotomous questions\ndescribing various situations of daily life and challenging the user's\nknowledge, values, ethics, and principles. In industrial application, the\ntiming of this assessment will depend on the user's need to obtain a service\nfrom a provider such as opening a bank account, getting a mortgage or an\ninsurance policy, authenticating clearance at work or securing online payments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 09:55:27 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 10:18:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jumelle", "Frederic", ""], ["So", "Kelvin", ""], ["Deng", "Didan", ""]]}, {"id": "2103.00476", "submitter": "Shi Gu", "authors": "Shikuang Deng, Shi Gu", "title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are biology-inspired artificial neural\nnetworks (ANNs) that comprise of spiking neurons to process asynchronous\ndiscrete signals. While more efficient in power consumption and inference speed\non the neuromorphic hardware, SNNs are usually difficult to train directly from\nscratch with spikes due to the discreteness. As an alternative, many efforts\nhave been devoted to converting conventional ANNs into SNNs by copying the\nweights from ANNs and adjusting the spiking threshold potential of neurons in\nSNNs. Researchers have designed new SNN architectures and conversion algorithms\nto diminish the conversion error. However, an effective conversion should\naddress the difference between the SNN and ANN architectures with an efficient\napproximation \\DSK{of} the loss function, which is missing in the field. In\nthis work, we analyze the conversion error by recursive reduction to layer-wise\nsummation and propose a novel strategic pipeline that transfers the weights to\nthe target SNN by combining threshold balance and soft-reset mechanisms. This\npipeline enables almost no accuracy loss between the converted SNNs and\nconventional ANNs with only $\\sim1/10$ of the typical SNN simulation time. Our\nmethod is promising to get implanted onto embedded platforms with better\nsupport of SNNs with limited energy and memory.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:04:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Deng", "Shikuang", ""], ["Gu", "Shi", ""]]}, {"id": "2103.00480", "submitter": "Fergal Stapleton", "authors": "Fergal Stapleton and Edgar Galv\\'an", "title": "Semantic Neighborhood Ordering in Multi-objective Genetic Programming\n  based on Decomposition", "comments": "9 pages, 4 tables, 2 figures, added additional references, fixed\n  minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic diversity in Genetic Programming has proved to be highly beneficial\nin evolutionary search. We have witnessed a surge in the number of scientific\nworks in the area, starting first in discrete spaces and moving then to\ncontinuous spaces. The vast majority of these works, however, have focused\ntheir attention on single-objective genetic programming paradigms, with a few\nexceptions focusing on Evolutionary Multi-objective Optimization (EMO). The\nlatter works have used well-known robust algorithms, including the\nNon-dominated Sorting Genetic Algorithm II and the Strength Pareto Evolutionary\nAlgorithm, both heavily influenced by the notion of Pareto dominance. These\ninspiring works led us to make a step forward in EMO by considering\nMulti-objective Evolutionary Algorithms Based on Decomposition (MOEA/D). We\nshow, for the first time, how we can promote semantic diversity in MOEA/D in\nGenetic Programming.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:18:37 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 18:03:09 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Stapleton", "Fergal", ""], ["Galv\u00e1n", "Edgar", ""]]}, {"id": "2103.00542", "submitter": "Yuling Jiao", "authors": "Yuling Jiao, Yanming Lai, Xiliang Lu, Zhijian Yang", "title": "Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse\n  of Dimensionality on H\\\"older Class", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we construct neural networks with ReLU, sine and $2^x$ as\nactivation functions. For general continuous $f$ defined on $[0,1]^d$ with\ncontinuity modulus $\\omega_f(\\cdot)$, we construct ReLU-sine-$2^x$ networks\nthat enjoy an approximation rate\n$\\mathcal{O}(\\omega_f(\\sqrt{d})\\cdot2^{-M}+\\omega_{f}\\left(\\frac{\\sqrt{d}}{N}\\right))$,\nwhere $M,N\\in \\mathbb{N}^{+}$ denote the hyperparameters related to widths of\nthe networks. As a consequence, we can construct ReLU-sine-$2^x$ network with\nthe depth $5$ and width\n$\\max\\left\\{\\left\\lceil2d^{3/2}\\left(\\frac{3\\mu}{\\epsilon}\\right)^{1/{\\alpha}}\\right\\rceil,2\\left\\lceil\\log_2\\frac{3\\mu\nd^{\\alpha/2}}{2\\epsilon}\\right\\rceil+2\\right\\}$ that approximates $f\\in\n\\mathcal{H}_{\\mu}^{\\alpha}([0,1]^d)$ within a given tolerance $\\epsilon >0$\nmeasured in $L^p$ norm $p\\in[1,\\infty)$, where\n$\\mathcal{H}_{\\mu}^{\\alpha}([0,1]^d)$ denotes the H\\\"older continuous function\nclass defined on $[0,1]^d$ with order $\\alpha \\in (0,1]$ and constant $\\mu >\n0$. Therefore, the ReLU-sine-$2^x$ networks overcome the curse of\ndimensionality on $\\mathcal{H}_{\\mu}^{\\alpha}([0,1]^d)$. In addition to its\nsupper expressive power, functions implemented by ReLU-sine-$2^x$ networks are\n(generalized) differentiable, enabling us to apply SGD to train.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:57:42 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:12:13 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 16:23:23 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 12:29:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jiao", "Yuling", ""], ["Lai", "Yanming", ""], ["Lu", "Xiliang", ""], ["Yang", "Zhijian", ""]]}, {"id": "2103.00682", "submitter": "Daofu Guo", "authors": "Xiaodong Ren, Daofu Guo, Zhigang Ren, Yongsheng Liang, An Chen", "title": "Enhancing hierarchical surrogate-assisted evolutionary algorithm for\n  high-dimensional expensive optimization via random projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By remarkably reducing real fitness evaluations, surrogate-assisted\nevolutionary algorithms (SAEAs), especially hierarchical SAEAs, have been shown\nto be effective in solving computationally expensive optimization problems. The\nsuccess of hierarchical SAEAs mainly profits from the potential benefit of\ntheir global surrogate models known as \"blessing of uncertainty\" and the high\naccuracy of local models. However, their performance leaves room for\nimprovement on highdimensional problems since now it is still challenging to\nbuild accurate enough local models due to the huge solution space. Directing\nagainst this issue, this study proposes a new hierarchical SAEA by training\nlocal surrogate models with the help of the random projection technique.\nInstead of executing training in the original high-dimensional solution space,\nthe new algorithm first randomly projects training samples onto a set of\nlow-dimensional subspaces, then trains a surrogate model in each subspace, and\nfinally achieves evaluations of candidate solutions by averaging the resulting\nmodels. Experimental results on six benchmark functions of 100 and 200\ndimensions demonstrate that random projection can significantly improve the\naccuracy of local surrogate models and the new proposed hierarchical SAEA\npossesses an obvious edge over state-of-the-art SAEAs\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:36:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ren", "Xiaodong", ""], ["Guo", "Daofu", ""], ["Ren", "Zhigang", ""], ["Liang", "Yongsheng", ""], ["Chen", "An", ""]]}, {"id": "2103.00792", "submitter": "Namyong Park", "authors": "Namyong Park, MinHyeok Kim, Nguyen Xuan Hoai, R.I. (Bob) McKay,\n  Dong-Kyun Kim", "title": "Knowledge-Guided Dynamic Systems Modeling: A Case Study on Modeling\n  River Water Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling real-world phenomena is a focus of many science and engineering\nefforts, such as ecological modeling and financial forecasting, to name a few.\nBuilding an accurate model for complex and dynamic systems improves\nunderstanding of underlying processes and leads to resource efficiency. Towards\nthis goal, knowledge-driven modeling builds a model based on human expertise,\nyet is often suboptimal. At the opposite extreme, data-driven modeling learns a\nmodel directly from data, requiring extensive data and potentially generating\noverfitting. We focus on an intermediate approach, model revision, in which\nprior knowledge and data are combined to achieve the best of both worlds. In\nthis paper, we propose a genetic model revision framework based on\ntree-adjoining grammar (TAG) guided genetic programming (GP), using the TAG\nformalism and GP operators in an effective mechanism to incorporate prior\nknowledge and make data-driven revisions in a way that complies with prior\nknowledge. Our framework is designed to address the high computational cost of\nevolutionary modeling of complex systems. Via a case study on the challenging\nproblem of river water quality modeling, we show that the framework efficiently\nlearns an interpretable model, with higher modeling accuracy than existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:31:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Park", "Namyong", "", "Bob"], ["Kim", "MinHyeok", "", "Bob"], ["Hoai", "Nguyen Xuan", "", "Bob"], ["I.", "R.", "", "Bob"], ["McKay", "", ""], ["Kim", "Dong-Kyun", ""]]}, {"id": "2103.01045", "submitter": "Jeremy Bernstein", "authors": "Jeremy Bernstein and Yisong Yue", "title": "Computing the Information Content of Trained Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much information does a learning algorithm extract from the training data\nand store in a neural network's weights? Too much, and the network would\noverfit to the training data. Too little, and the network would not fit to\nanything at all. Na\\\"ively, the amount of information the network stores should\nscale in proportion to the number of trainable weights. This raises the\nquestion: how can neural networks with vastly more weights than training data\nstill generalise? A simple resolution to this conundrum is that the number of\nweights is usually a bad proxy for the actual amount of information stored. For\ninstance, typical weight vectors may be highly compressible. Then another\nquestion occurs: is it possible to compute the actual amount of information\nstored? This paper derives both a consistent estimator and a closed-form upper\nbound on the information content of infinitely wide neural networks. The\nderivation is based on an identification between neural information content and\nthe negative log probability of a Gaussian orthant. This identification yields\nbounds that analytically control the generalisation behaviour of the entire\nsolution space of infinitely wide networks. The bounds have a simple dependence\non both the network architecture and the training data. Corroborating the\nfindings of Valle-P\\'erez et al. (2019), who conducted a similar analysis using\napproximate Gaussian integration techniques, the bounds are found to be both\nnon-vacuous and correlated with the empirical generalisation behaviour at\nfinite width.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:38:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Bernstein", "Jeremy", ""], ["Yue", "Yisong", ""]]}, {"id": "2103.01071", "submitter": "Davide Evangelista", "authors": "A. Asperti, D. Evangelista, E. Loli Piccolomini", "title": "A survey on Variational Autoencoders from a GreenAI perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational AutoEncoders (VAEs) are powerful generative models that merge\nelements from statistics and information theory with the flexibility offered by\ndeep neural networks to efficiently solve the generation problem for high\ndimensional data. The key insight of VAEs is to learn the latent distribution\nof data in such a way that new meaningful samples can be generated from it.\nThis approach led to tremendous research and variations in the architectural\ndesign of VAEs, nourishing the recent field of research known as unsupervised\nrepresentation learning. In this article, we provide a comparative evaluation\nof some of the most successful, recent variations of VAEs. We particularly\nfocus the analysis on the energetic efficiency of the different models, in the\nspirit of the so called Green AI, aiming both to reduce the carbon footprint\nand the financial cost of generative techniques. For each architecture we\nprovide its mathematical formulation, the ideas underlying its design, a\ndetailed model description, a running implementation and quantitative results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:26:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Asperti", "A.", ""], ["Evangelista", "D.", ""], ["Piccolomini", "E. Loli", ""]]}, {"id": "2103.01118", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Deep Learning with a Classifier System: Initial Results", "comments": "arXiv admin note: text overlap with arXiv:1910.10579", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the first results from using a learning classifier\nsystem capable of performing adaptive computation with deep neural networks.\nIndividual classifiers within the population are composed of two neural\nnetworks. The first acts as a gating or guarding component, which enables the\nconditional computation of an associated deep neural network on a per instance\nbasis. Self-adaptive mutation is applied upon reproduction and prediction\nnetworks are refined with stochastic gradient descent during lifetime learning.\nThe use of fully-connected and convolutional layers are evaluated on\nhandwritten digit recognition tasks where evolution adapts (i) the gradient\ndescent learning rate applied to each layer (ii) the number of units within\neach layer, i.e., the number of fully-connected neurons and the number of\nconvolutional kernel filters (iii) the connectivity of each layer, i.e.,\nwhether each weight is active (iv) the weight magnitudes, enabling escape from\nlocal optima. The system automatically reduces the number of weights and units\nwhile maintaining performance after achieving a maximum prediction error.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:40:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "2103.01148", "submitter": "Alperen G\\\"ormez", "authors": "Alperen Gormez and Erdem Koyuncu", "title": "Class Means as an Early Exit Decision Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural networks with early exit mechanisms often need\nconsiderable amount of training and fine-tuning to achieve good performance\nwith low computational cost. We propose a novel early exit technique based on\nthe class means of samples. Unlike most existing schemes, our method does not\nrequire gradient-based training of internal classifiers. This makes our method\nparticularly useful for neural network training in low-power devices, as in\nwireless edge networks. In particular, given a fixed training time budget, our\nscheme achieves higher accuracy as compared to existing early exit mechanisms.\nMoreover, if there are no limitations on the training time budget, our method\ncan be combined with an existing early exit scheme to boost its performance,\nachieving a better trade-off between computational cost and network accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:31:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gormez", "Alperen", ""], ["Koyuncu", "Erdem", ""]]}, {"id": "2103.01205", "submitter": "Justin Terry", "authors": "J. K. Terry, Mario Jayakumar, Kusal De Alwis", "title": "Statistically Significant Stopping of Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:51:16 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 03:37:36 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 02:42:19 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Terry", "J. K.", ""], ["Jayakumar", "Mario", ""], ["De Alwis", "Kusal", ""]]}, {"id": "2103.01301", "submitter": "Iana Polonskaia", "authors": "Iana S. Polonskaia, Nikolay O. Nikitin, Ilia Revin, Pavel Vychuzhanin,\n  Anna V. Kalyuzhnaya", "title": "Multi-Objective Evolutionary Design of Composite Data-Driven Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multi-objective approach for the design of composite\ndata-driven mathematical models is proposed. It allows automating the\nidentification of graph-based heterogeneous pipelines that consist of different\nblocks: machine learning models, data preprocessing blocks, etc. The\nimplemented approach is based on a parameter-free genetic algorithm (GA) for\nmodel design called GPComp@Free. It is developed to be part of automated\nmachine learning solutions and to increase the efficiency of the modeling\npipeline automation. A set of experiments was conducted to verify the\ncorrectness and efficiency of the proposed approach and substantiate the\nselected solutions. The experimental results confirm that a multi-objective\napproach to the model design allows achieving better diversity and quality of\nobtained models. The implemented approach is available as a part of the\nopen-source AutoML framework FEDOT.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:45:24 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 21:49:55 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Polonskaia", "Iana S.", ""], ["Nikitin", "Nikolay O.", ""], ["Revin", "Ilia", ""], ["Vychuzhanin", "Pavel", ""], ["Kalyuzhnaya", "Anna V.", ""]]}, {"id": "2103.01578", "submitter": "Daiki Morinaga", "authors": "Daiki Morinaga, Kazuto Fukuchi, Jun Sakuma, and Youhei Akimoto", "title": "Convergence Rate of the (1+1)-Evolution Strategy with Success-Based\n  Step-Size Adaptation on Convex Quadratic Functions", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The (1+1)-evolution strategy (ES) with success-based step-size adaptation is\nanalyzed on a general convex quadratic function and its monotone\ntransformation, that is, $f(x) = g((x - x^*)^\\mathrm{T} H (x - x^*))$, where\n$g:\\mathbb{R}\\to\\mathbb{R}$ is a strictly increasing function, $H$ is a\npositive-definite symmetric matrix, and $x^* \\in \\mathbb{R}^d$ is the optimal\nsolution of $f$. The convergence rate, that is, the decrease rate of the\ndistance from a search point $m_t$ to the optimal solution $x^*$, is proven to\nbe in $O(\\exp( - L / \\mathrm{Tr}(H) ))$, where $L$ is the smallest eigenvalue\nof $H$ and $\\mathrm{Tr}(H)$ is the trace of $H$. This result generalizes the\nknown rate of $O(\\exp(- 1/d ))$ for the case of $H = I_{d}$ ($I_d$ is the\nidentity matrix of dimension $d$) and $O(\\exp(- 1/ (d\\cdot\\xi) ))$ for the case\nof $H = \\mathrm{diag}(\\xi \\cdot I_{d/2}, I_{d/2})$. To the best of our\nknowledge, this is the first study in which the convergence rate of the\n(1+1)-ES is derived explicitly and rigorously on a general convex quadratic\nfunction, which depicts the impact of the distribution of the eigenvalues in\nthe Hessian $H$ on the optimization and not only the impact of the condition\nnumber of $H$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:03:44 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 14:16:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Morinaga", "Daiki", ""], ["Fukuchi", "Kazuto", ""], ["Sakuma", "Jun", ""], ["Akimoto", "Youhei", ""]]}, {"id": "2103.01636", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu, Elena Mocanu, Tiago Pinto, Selima Curci,\n  Phuong H. Nguyen, Madeleine Gibescu, Damien Ernst, Zita A. Vale", "title": "Sparse Training Theory for Scalable and Efficient Agents", "comments": null, "journal-ref": "20th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2021)", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task for artificial intelligence is learning. Deep Neural\nNetworks have proven to cope perfectly with all learning paradigms, i.e.\nsupervised, unsupervised, and reinforcement learning. Nevertheless, traditional\ndeep learning approaches make use of cloud computing facilities and do not\nscale well to autonomous agents with low computational resources. Even in the\ncloud, they suffer from computational and memory limitations, and they cannot\nbe used to model adequately large physical worlds for agents which assume\nnetworks with billions of neurons. These issues are addressed in the last few\nyears by the emerging topic of sparse training, which trains sparse networks\nfrom scratch. This paper discusses sparse training state-of-the-art, its\nchallenges and limitations while introducing a couple of new theoretical\nresearch directions which has the potential of alleviating sparse training\nlimitations to push deep learning scalability well beyond its current\nboundaries. Nevertheless, the theoretical advancements impact in complex\nmulti-agents settings is discussed from a real-world perspective, using the\nsmart grid case study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:48:29 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Mocanu", "Elena", ""], ["Pinto", "Tiago", ""], ["Curci", "Selima", ""], ["Nguyen", "Phuong H.", ""], ["Gibescu", "Madeleine", ""], ["Ernst", "Damien", ""], ["Vale", "Zita A.", ""]]}, {"id": "2103.01670", "submitter": "John Cartlidge", "authors": "Zijian Shi, Yu Chen, John Cartlidge", "title": "The LOB Recreation Model: Predicting the Limit Order Book from TAQ\n  History Using an Ordinary Differential Equation Recurrent Neural Network", "comments": "12 pages, preprint accepted for publication in the 35th AAAI\n  Conference on Artificial Intelligence (AAAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.CE cs.NE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an order-driven financial market, the price of a financial asset is\ndiscovered through the interaction of orders - requests to buy or sell at a\nparticular price - that are posted to the public limit order book (LOB).\nTherefore, LOB data is extremely valuable for modelling market dynamics.\nHowever, LOB data is not freely accessible, which poses a challenge to market\nparticipants and researchers wishing to exploit this information. Fortunately,\ntrades and quotes (TAQ) data - orders arriving at the top of the LOB, and\ntrades executing in the market - are more readily available. In this paper, we\npresent the LOB recreation model, a first attempt from a deep learning\nperspective to recreate the top five price levels of the LOB for small-tick\nstocks using only TAQ data. Volumes of orders sitting deep in the LOB are\npredicted by combining outputs from: (1) a history compiler that uses a Gated\nRecurrent Unit (GRU) module to selectively compile prediction relevant quote\nhistory; (2) a market events simulator, which uses an Ordinary Differential\nEquation Recurrent Neural Network (ODE-RNN) to simulate the accumulation of net\norder arrivals; and (3) a weighting scheme to adaptively combine the\npredictions generated by (1) and (2). By the paradigm of transfer learning, the\nsource model trained on one stock can be fine-tuned to enable application to\nother financial assets of the same class with much lower demand on additional\ndata. Comprehensive experiments conducted on two real world intraday LOB\ndatasets demonstrate that the proposed model can efficiently recreate the LOB\nwith high accuracy using only TAQ data as input.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:07:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shi", "Zijian", ""], ["Chen", "Yu", ""], ["Cartlidge", "John", ""]]}, {"id": "2103.01730", "submitter": "Elvin Isufi", "authors": "Elvin Isufi and Gabriele Mazzola", "title": "Graph-Time Convolutional Neural Networks", "comments": "IEEE Data Science and Learning Workshop, Toronto, Canada, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Spatiotemporal data can be represented as a process over a graph, which\ncaptures their spatial relationships either explicitly or implicitly. How to\nleverage such a structure for learning representations is one of the key\nchallenges when working with graphs. In this paper, we represent the\nspatiotemporal relationships through product graphs and develop a first\nprinciple graph-time convolutional neural network (GTCNN). The GTCNN is a\ncompositional architecture with each layer comprising a graph-time\nconvolutional module, a graph-time pooling module, and a nonlinearity. We\ndevelop a graph-time convolutional filter by following the shift-and-sum\nprinciples of the convolutional operator to learn higher-level features over\nthe product graph. The product graph itself is parametric so that we can learn\nalso the spatiotemporal coupling from data. We develop a zero-pad pooling that\npreserves the spatial graph (the prior about the data) while reducing the\nnumber of active nodes and the parameters. Experimental results with synthetic\nand real data corroborate the different components and compare with baseline\nand state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:03:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Isufi", "Elvin", ""], ["Mazzola", "Gabriele", ""]]}, {"id": "2103.02076", "submitter": "Juan Carlos Seck Tuoh Mora", "authors": "Juan Carlos Seck-Tuoh-Mora, Norberto Hernandez-Romero, Pedro\n  Lagos-Eulogio, Joselito Medina-Marin, Nadia Samantha Zu\\~niga-Pe\\~na", "title": "A continuous-state cellular automata algorithm for global optimization", "comments": "39 pages, 13 figures and 28 tables. Submitted to Expert Systems With\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cellular automata are capable of developing complex behaviors based on simple\nlocal interactions between their elements. Some of these characteristics have\nbeen used to propose and improve meta-heuristics for global optimization;\nhowever, the properties offered by the evolution rules in cellular automata\nhave not yet been used directly in optimization tasks. Inspired by the\ncomplexity that various evolution rules of cellular automata can offer, the\ncontinuous-state cellular automata algorithm (CCAA) is proposed. In this way,\nthe CCAA takes advantage of different evolution rules to maintain a balance\nthat maximizes the exploration and exploitation properties in each iteration.\nThe efficiency of the CCAA is proven with 33 test problems widely used in the\nliterature, 4 engineering applications that were also used in recent\nliterature, and the design of adaptive infinite-impulse response (IIR) filters,\ntesting 10 full-order IIR reference functions. The numerical results prove its\ncompetitiveness in comparison with state-of-the-art algorithms. The source\ncodes of the CCAA are publicly available at\nhttps://github.com/juanseck/CCAA.git\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:01:58 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Seck-Tuoh-Mora", "Juan Carlos", ""], ["Hernandez-Romero", "Norberto", ""], ["Lagos-Eulogio", "Pedro", ""], ["Medina-Marin", "Joselito", ""], ["Zu\u00f1iga-Pe\u00f1a", "Nadia Samantha", ""]]}, {"id": "2103.02107", "submitter": "Yongsheng Liang", "authors": "Yongsheng Liang, Zhigang Ren, Lin Wang, Hanqing Liu, Wenhao Du", "title": "Surrogate-assisted cooperative signal optimization for large-scale\n  traffic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasonable setting of traffic signals can be very helpful in alleviating\ncongestion in urban traffic networks. Meta-heuristic optimization algorithms\nhave proved themselves to be able to find high-quality signal timing plans.\nHowever, they generally suffer from performance deterioration when solving\nlarge-scale traffic signal optimization problems due to the huge search space\nand limited computational budget. Directing against this issue, this study\nproposes a surrogate-assisted cooperative signal optimization (SCSO) method.\nDifferent from existing methods that directly deal with the entire traffic\nnetwork, SCSO first decomposes it into a set of tractable sub-networks, and\nthen achieves signal setting by cooperatively optimizing these sub-networks\nwith a surrogate-assisted optimizer. The decomposition operation significantly\nnarrows the search space of the whole traffic network, and the\nsurrogate-assisted optimizer greatly lowers the computational burden by\nreducing the number of expensive traffic simulations. By taking Newman fast\nalgorithm, radial basis function and a modified estimation of distribution\nalgorithm as decomposer, surrogate model and optimizer, respectively, this\nstudy develops a concrete SCSO algorithm. To evaluate its effectiveness and\nefficiency, a large-scale traffic network involving crossroads and T-junctions\nis generated based on a real traffic network. Comparison with several existing\nmeta-heuristic algorithms specially designed for traffic signal optimization\ndemonstrates the superiority of SCSO in reducing the average delay time of\nvehicles.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:03:57 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liang", "Yongsheng", ""], ["Ren", "Zhigang", ""], ["Wang", "Lin", ""], ["Liu", "Hanqing", ""], ["Du", "Wenhao", ""]]}, {"id": "2103.02137", "submitter": "Nadine Wirkuttis", "authors": "Nadine Wirkuttis and Jun Tani", "title": "Controlling the Sense of Agency in Dyadic Robot Interaction: An Active\n  Inference Approach", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigated how social interaction among robotic agents changes\ndynamically depending on individual sense of agency. In a set of simulation\nstudies, we examine dyadic imitative interactions of robots using a variational\nrecurrent neural network model. The model is based on the free energy principle\nsuch that interacting robots find themselves in a loop, attempting to predict\nand infer each other's actions using active inference. We examined how\nregulating the complexity term to minimize free energy during training\ndetermines the dynamic characteristics of networks and affects dyadic imitative\ninteractions. Our simulation results show that through softer regulation of the\ncomplexity term, a robot with stronger agency develops and dominates its\ncounterpart developed with weaker agency through tighter regulation. When two\nrobots are trained with equally soft regulation, both generate individual\nintended behavior patterns, ignoring each other. We argue that primary\nintersubjectivity does develop in dyadic robotic interactions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:38:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wirkuttis", "Nadine", ""], ["Tani", "Jun", ""]]}, {"id": "2103.02238", "submitter": "Imran Raza", "authors": "A. Shahid (COMSATS University Islamabad, Lahore Campus), I. Raza\n  (COMSATS University Islamabad, Lahore Campus), S. A. Hussain (COMSATS\n  University Islamabad, Lahore Campus)", "title": "EmoWrite: A Sentiment Analysis-Based Thought to Text Conversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain Computer Interface (BCI) helps in processing and extraction of useful\ninformation from the acquired brain signals having applications in diverse\nfields such as military, medicine, neuroscience, and rehabilitation. BCI has\nbeen used to support paralytic patients having speech impediments with severe\ndisabilities. To help paralytic patients communicate with ease, BCI based\nsystems convert silent speech (thoughts) to text. However, these systems have\nan inconvenient graphical user interface, high latency, limited typing speed,\nand low accuracy rate. Apart from these limitations, the existing systems do\nnot incorporate the inevitable factor of a patient's emotional states and\nsentiment analysis. The proposed system EmoWrite implements a dynamic keyboard\nwith contextualized appearance of characters reducing the traversal time and\nimproving the utilization of the screen space. The proposed system has been\nevaluated and compared with the existing systems for accuracy, convenience,\nsentimental analysis, and typing speed. This system results in 6.58 Words Per\nMinute (WPM) and 31.92 Characters Per Minute (CPM) with an accuracy of 90.36\npercent. EmoWrite also gives remarkable results when it comes to the\nintegration of emotional states. Its Information Transfer Rate (ITR) is also\nhigh as compared to other systems i.e., 87.55 bits per min with commands and\n72.52 bits per min for letters. Furthermore, it provides easy to use interface\nwith a latency of 2.685 sec.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:03:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shahid", "A.", "", "COMSATS University Islamabad, Lahore Campus"], ["Raza", "I.", "", "COMSATS University Islamabad, Lahore Campus"], ["Hussain", "S. A.", "", "COMSATS\n  University Islamabad, Lahore Campus"]]}, {"id": "2103.02339", "submitter": "Omar Chehab", "authors": "Omar Chehab, Alexandre Defossez, Jean-Christophe Loiseau, Alexandre\n  Gramfort, Jean-Remi King", "title": "Deep Recurrent Encoder: A scalable end-to-end network to model brain\n  signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how the brain responds to sensory inputs is challenging: brain\nrecordings are partial, noisy, and high dimensional; they vary across sessions\nand subjects and they capture highly nonlinear dynamics. These challenges have\nled the community to develop a variety of preprocessing and analytical (almost\nexclusively linear) methods, each designed to tackle one of these issues.\nInstead, we propose to address these challenges through a specific end-to-end\ndeep learning architecture, trained to predict the brain responses of multiple\nsubjects at once. We successfully test this approach on a large cohort of\nmagnetoencephalography (MEG) recordings acquired during a one-hour reading\ntask. Our Deep Recurrent Encoding (DRE) architecture reliably predicts MEG\nresponses to words with a three-fold improvement over classic linear methods.\nTo overcome the notorious issue of interpretability of deep learning, we\ndescribe a simple variable importance analysis. When applied to DRE, this\nmethod recovers the expected evoked responses to word length and word\nfrequency. The quantitative improvement of the present deep learning approach\npaves the way to better understand the nonlinear dynamics of brain activity\nfrom large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:39:17 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 07:59:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chehab", "Omar", ""], ["Defossez", "Alexandre", ""], ["Loiseau", "Jean-Christophe", ""], ["Gramfort", "Alexandre", ""], ["King", "Jean-Remi", ""]]}, {"id": "2103.02522", "submitter": "Graham Rowlands", "authors": "Graham E. Rowlands, Minh-Hai Nguyen, Guilhem J. Ribeill, Andrew P.\n  Wagner, Luke C. G. Govia, Wendson A. S. Barbosa, Daniel J. Gauthier, Thomas\n  A. Ohki", "title": "Reservoir Computing with Superconducting Electronics", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.supr-con cs.ET cs.NE physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidity and low power consumption of superconducting electronics makes\nthem an ideal substrate for physical reservoir computing, which commandeers the\ncomputational power inherent to the evolution of a dynamical system for the\npurposes of performing machine learning tasks. We focus on a subset of\nsuperconducting circuits that exhibit soliton-like dynamics in simple\ntransmission line geometries. With numerical simulations we demonstrate the\neffectiveness of these circuits in performing higher-order parity calculations\nand channel equalization at rates approaching 100 Gb/s. The availability of a\nproven superconducting logic scheme considerably simplifies the path to a fully\nintegrated reservoir computing platform and makes superconducting reservoirs an\nenticing substrate for high rate signal processing applications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:51:46 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Rowlands", "Graham E.", ""], ["Nguyen", "Minh-Hai", ""], ["Ribeill", "Guilhem J.", ""], ["Wagner", "Andrew P.", ""], ["Govia", "Luke C. G.", ""], ["Barbosa", "Wendson A. S.", ""], ["Gauthier", "Daniel J.", ""], ["Ohki", "Thomas A.", ""]]}, {"id": "2103.02751", "submitter": "Julien Dupeyroux", "authors": "Julien Dupeyroux", "title": "A toolbox for neuromorphic sensing in robotics", "comments": "7 pages, 3 figures, 3 tables, 7 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The third generation of artificial intelligence (AI) introduced by\nneuromorphic computing is revolutionizing the way robots and autonomous systems\ncan sense the world, process the information, and interact with their\nenvironment. The promises of high flexibility, energy efficiency, and\nrobustness of neuromorphic systems is widely supported by software tools for\nsimulating spiking neural networks, and hardware integration (neuromorphic\nprocessors). Yet, while efforts have been made on neuromorphic vision\n(event-based cameras), it is worth noting that most of the sensors available\nfor robotics remain inherently incompatible with neuromorphic computing, where\ninformation is encoded into spikes. To facilitate the use of traditional\nsensors, we need to convert the output signals into streams of spikes, i.e., a\nseries of events (+1, -1) along with their corresponding timestamps. In this\npaper, we propose a review of the coding algorithms from a robotics perspective\nand further supported by a benchmark to assess their performance. We also\nintroduce a ROS (Robot Operating System) toolbox to encode and decode input\nsignals coming from any type of sensor available on a robot. This initiative is\nmeant to stimulate and facilitate robotic integration of neuromorphic AI, with\nthe opportunity to adapt traditional off-the-shelf sensors to spiking neural\nnets within one of the most powerful robotic tools, ROS.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:22:05 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dupeyroux", "Julien", ""]]}, {"id": "2103.02881", "submitter": "Michele Piana", "authors": "Sabrina Guastavino, Michele Piana, Federico Benvenuto", "title": "Bad and good errors: value-weighted skill scores in deep ensemble\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to realize forecast verification.\nSpecifically, we introduce a strategy for assessing the severity of forecast\nerrors based on the evidence that, on the one hand, a false alarm just\nanticipating an occurring event is better than one in the middle of consecutive\nnon-occurring events, and that, on the other hand, a miss of an isolated event\nhas a worse impact than a miss of a single event, which is part of several\nconsecutive occurrences. Relying on this idea, we introduce a novel definition\nof confusion matrix and skill scores giving greater importance to the value of\nthe prediction rather than to its quality. Then, we introduce a deep ensemble\nlearning procedure for binary classification, in which the probabilistic\noutcomes of a neural network are clustered via optimization of these\nvalue-weighted skill scores. We finally show the performances of this approach\nin the case of three applications concerned with pollution, space weather and\nstock prize forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:05:13 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Guastavino", "Sabrina", ""], ["Piana", "Michele", ""], ["Benvenuto", "Federico", ""]]}, {"id": "2103.03055", "submitter": "Matej Gazda", "authors": "Matej Gazda, Jakub Gazda, Jan Plavka, Peter Drotar", "title": "Self-supervised deep convolutional neural network for chest X-ray\n  classification", "comments": "This work has been submitted to the IEEE transactions on medical\n  imaging for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest radiography is a relatively cheap, widely available medical procedure\nthat conveys key information for making diagnostic decisions. Chest X-rays are\nalmost always used in the diagnosis of respiratory diseases such as pneumonia\nor the recent COVID-19. In this paper, we propose a self-supervised deep neural\nnetwork that is pretrained on an unlabeled chest X-ray dataset. The learned\nrepresentations are transferred to downstream task - the classification of\nrespiratory diseases. The results obtained on four public datasets show that\nour approach yields competitive results without requiring large amounts of\nlabeled training data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:28:37 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 07:34:50 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gazda", "Matej", ""], ["Gazda", "Jakub", ""], ["Plavka", "Jan", ""], ["Drotar", "Peter", ""]]}, {"id": "2103.03060", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Moncef Gabbouj", "title": "BM3D vs 2-Layer ONN", "comments": "Submitted for review in ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite their recent success on image denoising, the need for deep and\ncomplex architectures still hinders the practical usage of CNNs. Older but\ncomputationally more efficient methods such as BM3D remain a popular choice,\nespecially in resource-constrained scenarios. In this study, we aim to find out\nwhether compact neural networks can learn to produce competitive results as\ncompared to BM3D for AWGN image denoising. To this end, we configure networks\nwith only two hidden layers and employ different neuron models and layer widths\nfor comparing the performance with BM3D across different AWGN noise levels. Our\nresults conclusively show that the recently proposed self-organized variant of\noperational neural networks based on a generative neuron model (Self-ONNs) is\nnot only a better choice as compared to CNNs, but also provide competitive\nresults as compared to BM3D and even significantly surpass it for high noise\nlevels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:37:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Yamac", "Mehmet", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2103.03108", "submitter": "Hamidreza Ghader", "authors": "Hamidreza Ghader", "title": "An empirical analysis of phrase-based and neural machine translation", "comments": "PhD thesis, University of Amsterdam, October 2020.\n  https://pure.uva.nl/ws/files/51388868/Thesis.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Two popular types of machine translation (MT) are phrase-based and neural\nmachine translation systems. Both of these types of systems are composed of\nmultiple complex models or layers. Each of these models and layers learns\ndifferent linguistic aspects of the source language. However, for some of these\nmodels and layers, it is not clear which linguistic phenomena are learned or\nhow this information is learned. For phrase-based MT systems, it is often clear\nwhat information is learned by each model, and the question is rather how this\ninformation is learned, especially for its phrase reordering model. For neural\nmachine translation systems, the situation is even more complex, since for many\ncases it is not exactly clear what information is learned and how it is\nlearned.\n  To shed light on what linguistic phenomena are captured by MT systems, we\nanalyze the behavior of important models in both phrase-based and neural MT\nsystems. We consider phrase reordering models from phrase-based MT systems to\ninvestigate which words from inside of a phrase have the biggest impact on\ndefining the phrase reordering behavior. Additionally, to contribute to the\ninterpretability of neural MT systems we study the behavior of the attention\nmodel, which is a key component in neural MT systems and the closest model in\nfunctionality to phrase reordering models in phrase-based systems. The\nattention model together with the encoder hidden state representations form the\nmain components to encode source side linguistic information in neural MT. To\nthis end, we also analyze the information captured in the encoder hidden state\nrepresentations of a neural MT system. We investigate the extent to which\nsyntactic and lexical-semantic information from the source side is captured by\nhidden state representations of different neural MT architectures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:28:28 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ghader", "Hamidreza", ""]]}, {"id": "2103.03359", "submitter": "Amol Kelkar", "authors": "Amol Kelkar", "title": "Cognitive Homeostatic Agents", "comments": "Accepted at AAMAS2021 Blue Sky Ideas Track", "journal-ref": "In Proc. of the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2021), Online, May 3-7, 2021, IFAAMAS, 5 pages", "doi": "10.5555/3461017.3461021", "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human brain has been used as an inspiration for building autonomous agents,\nbut it is not obvious what level of computational description of the brain one\nshould use. This has led to overly opinionated symbolic approaches and overly\nunstructured connectionist approaches. We propose that using homeostasis as the\ncomputational description provides a good compromise. Similar to how\nphysiological homeostasis is the regulation of certain homeostatic variables,\ncognition can be interpreted as the regulation of certain 'cognitive\nhomeostatic variables'. We present an outline of a Cognitive Homeostatic Agent,\nbuilt as a hierarchy of physiological and cognitive homeostatic subsystems and\ndescribe structures and processes to guide future exploration. We expect this\nto be a fruitful line of investigation towards building sophisticated\nartificial agents that can act flexibly in complex environments, and produce\nbehaviors indicating planning, thinking and feelings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 07:29:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kelkar", "Amol", ""]]}, {"id": "2103.03386", "submitter": "Daniel Filan", "authors": "Daniel Filan, Stephen Casper, Shlomi Hod, Cody Wild, Andrew Critch,\n  Stuart Russell", "title": "Clusterability in Neural Networks", "comments": "20 pages, 22 figures. arXiv admin note: text overlap with\n  arXiv:2003.04881", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learned weights of a neural network have often been considered devoid of\nscrutable internal structure. In this paper, however, we look for structure in\nthe form of clusterability: how well a network can be divided into groups of\nneurons with strong internal connectivity but weak external connectivity. We\nfind that a trained neural network is typically more clusterable than randomly\ninitialized networks, and often clusterable relative to random networks with\nthe same distribution of weights. We also exhibit novel methods to promote\nclusterability in neural network training, and find that in multi-layer\nperceptrons they lead to more clusterable networks with little reduction in\naccuracy. Understanding and controlling the clusterability of neural networks\nwill hopefully render their inner workings more interpretable to engineers by\nfacilitating partitioning into meaningful clusters.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 23:53:53 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Filan", "Daniel", ""], ["Casper", "Stephen", ""], ["Hod", "Shlomi", ""], ["Wild", "Cody", ""], ["Critch", "Andrew", ""], ["Russell", "Stuart", ""]]}, {"id": "2103.03482", "submitter": "William Wagner", "authors": "William Wagner, Anna \\'Zakowska, Clement Aladi, Joseph Santhosh", "title": "Pilot Investigation for a Comprehensive Taxonomy of Autonomous Entities", "comments": "15 pages, 4 figures, 7 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LO cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper documents an exploratory pilot study to define the term Autonomous\nEntity, and any characteristics that are required to identify or classify an\nAutonomous Entity. Our solution builds on previous work with regard to\nphilosophical and scientific classification methods but focuses on a novel\nDesign Science Research Methodology (DSRM) and model to help identify those\ncharacteristics which make any autonomous entity similar or different from\nothers. We have solved the problem of not having an existing term to define our\nlens by creating a new combinatorial term: \"Riskyishness\". We present a DSRM\nand instrument for initial investigation, as well as observational and\nstatistical descriptions of their use in the real world to solicit domain\nexpertise and statistical evidence. Further, we demonstrate a specific\napplication of the methodology by creating a second artifact - a tool to score\nexisting and future technologies based on Riskyishness. The first artifact also\nprovides a technique to disentangle miscellaneous existing technologies or add\ndimensions to the tools to capture future additions and paradigm shifts.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:51:40 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 07:33:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wagner", "William", ""], ["\u0179akowska", "Anna", ""], ["Aladi", "Clement", ""], ["Santhosh", "Joseph", ""]]}, {"id": "2103.03526", "submitter": "Hugo Siqueira Gomes", "authors": "Hugo Siqueira Gomes, Benjamin L\\'eger and Christian Gagn\\'e", "title": "Meta Learning Black-Box Population-Based Optimizers", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The no free lunch theorem states that no model is better suited to every\nproblem. A question that arises from this is how to design methods that propose\noptimizers tailored to specific problems achieving state-of-the-art\nperformance. This paper addresses this issue by proposing the use of\nmeta-learning to infer population-based black-box optimizers that can\nautomatically adapt to specific classes of problems. We suggest a general\nmodeling of population-based algorithms that result in Learning-to-Optimize\nPOMDP (LTO-POMDP), a meta-learning framework based on a specific partially\nobservable Markov decision process (POMDP). From that framework's formulation,\nwe propose to parameterize the algorithm using deep recurrent neural networks\nand use a meta-loss function based on stochastic algorithms' performance to\ntrain efficient data-driven optimizers over several related optimization tasks.\nThe learned optimizers' performance based on this implementation is assessed on\nvarious black-box optimization tasks and hyperparameter tuning of machine\nlearning models. Our results revealed that the meta-loss function encourages a\nlearned algorithm to alter its search behavior so that it can easily fit into a\nnew context. Thus, it allows better generalization and higher sample efficiency\nthan state-of-the-art generic optimization algorithms, such as the Covariance\nmatrix adaptation evolution strategy (CMA-ES).\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:13:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gomes", "Hugo Siqueira", ""], ["L\u00e9ger", "Benjamin", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "2103.03875", "submitter": "Chen Li", "authors": "Chen Li, JinZhe Jiang, YaQian Zhao, RenGang Li, EnDong Wang, Xin\n  Zhang, Kun Zhao", "title": "Genetic Algorithm based hyper-parameters optimization for transfer\n  Convolutional Neural Network", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter optimization is a challenging problem in developing deep\nneural networks. Decision of transfer layers and trainable layers is a major\ntask for design of the transfer convolutional neural networks (CNN).\nConventional transfer CNN models are usually manually designed based on\nintuition. In this paper, a genetic algorithm is applied to select trainable\nlayers of the transfer model. The filter criterion is constructed by accuracy\nand the counts of the trainable layers. The results show that the method is\ncompetent in this task. The system will converge with a precision of 97% in the\nclassification of Cats and Dogs datasets, in no more than 15 generations.\nMoreover, backward inference according the results of the genetic algorithm\nshows that our method can capture the gradient features in network layers,\nwhich plays a part on understanding of the transfer AI models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 07:38:01 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Li", "Chen", ""], ["Jiang", "JinZhe", ""], ["Zhao", "YaQian", ""], ["Li", "RenGang", ""], ["Wang", "EnDong", ""], ["Zhang", "Xin", ""], ["Zhao", "Kun", ""]]}, {"id": "2103.03901", "submitter": "Bleema Rosenfeld", "authors": "Bleema Rosenfeld, Bipin Rajendran, Osvaldo Simeone", "title": "Fast On-Device Adaptation for Spiking Neural Networks via\n  Online-Within-Online Meta-Learning", "comments": "Accepted for publication at DSLW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have recently gained popularity as machine\nlearning models for on-device edge intelligence for applications such as mobile\nhealthcare management and natural language processing due to their low power\nprofile. In such highly personalized use cases, it is important for the model\nto be able to adapt to the unique features of an individual with only a minimal\namount of training data. Meta-learning has been proposed as a way to train\nmodels that are geared towards quick adaptation to new tasks. The few existing\nmeta-learning solutions for SNNs operate offline and require some form of\nbackpropagation that is incompatible with the current neuromorphic\nedge-devices. In this paper, we propose an online-within-online meta-learning\nrule for SNNs termed OWOML-SNN, that enables lifelong learning on a stream of\ntasks, and relies on local, backprop-free, nested updates.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 04:28:49 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Rosenfeld", "Bleema", ""], ["Rajendran", "Bipin", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2103.03905", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Yan Wu, Alexandros Kalousis", "title": "Kanerva++: extending The Kanerva Machine with differentiable, locally\n  block allocated latent memory", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic and semantic memory are critical components of the human memory\nmodel. The theory of complementary learning systems (McClelland et al., 1995)\nsuggests that the compressed representation produced by a serial event\n(episodic memory) is later restructured to build a more generalized form of\nreusable knowledge (semantic memory). In this work we develop a new principled\nBayesian memory allocation scheme that bridges the gap between episodic and\nsemantic memory via a hierarchical latent variable model. We take inspiration\nfrom traditional heap allocation and extend the idea of locally contiguous\nmemory to the Kanerva Machine, enabling a novel differentiable block allocated\nlatent memory. In contrast to the Kanerva Machine, we simplify the process of\nmemory writing by treating it as a fully feed forward deterministic process,\nrelying on the stochasticity of the read key distribution to disperse\ninformation within the memory. We demonstrate that this allocation scheme\nimproves performance in memory conditional image generation, resulting in new\nstate-of-the-art conditional likelihood values on binarized MNIST (<=41.58\nnats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting\ncompetitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:40:40 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:38:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ramapuram", "Jason", ""], ["Wu", "Yan", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "2103.03995", "submitter": "Wei-Chang Yeh", "authors": "Wei-Chang Yeh, Yi-Ping Lin, Yun-Chia Liang, Chyh-Ming Lai", "title": "Convolution Neural Network Hyperparameter Optimization Using Simplified\n  Swarm Optimization", "comments": "There are 44 manuscript pages, 16 tables, and 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Among the machine learning approaches applied in computer vision,\nConvolutional Neural Network (CNN) is widely used in the field of image\nrecognition. However, although existing CNN models have been proven to be\nefficient, it is not easy to find a network architecture with better\nperformance. Some studies choose to optimize the network architecture, while\nothers chose to optimize the hyperparameters, such as the number and size of\nconvolutional kernels, convolutional strides, pooling size, etc. Most of them\nare designed manually, which requires relevant expertise and takes a lot of\ntime. Therefore, this study proposes the idea of applying Simplified Swarm\nOptimization (SSO) on the hyperparameter optimization of LeNet models while\nusing MNIST, Fashion MNIST, and Cifar10 as validation. The experimental results\nshow that the proposed algorithm has higher accuracy than the original LeNet\nmodel, and it only takes a very short time to find a better hyperparameter\nconfiguration after training. In addition, we also analyze the output shape of\nthe feature map after each layer, and surprisingly, the results were mostly\nrectangular. The contribution of the study is to provide users with a simpler\nway to get better results with the existing model., and this study can also be\napplied to other CNN architectures.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 00:23:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yeh", "Wei-Chang", ""], ["Lin", "Yi-Ping", ""], ["Liang", "Yun-Chia", ""], ["Lai", "Chyh-Ming", ""]]}, {"id": "2103.04112", "submitter": "Omid Gheibi", "authors": "Omid Gheibi, Danny Weyns, and Federico Quin", "title": "Applying Machine Learning in Self-Adaptive Systems: A Systematic\n  Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we witness a rapid increase in the use of machine learning in\nself-adaptive systems. Machine learning has been used for a variety of reasons,\nranging from learning a model of the environment of a system during operation\nto filtering large sets of possible configurations before analysing them. While\na body of work on the use of machine learning in self-adaptive systems exists,\nthere is currently no systematic overview of this area. Such overview is\nimportant for researchers to understand the state of the art and direct future\nresearch efforts. This paper reports the results of a systematic literature\nreview that aims at providing such an overview. We focus on self-adaptive\nsystems that are based on a traditional Monitor-Analyze-Plan-Execute feedback\nloop (MAPE). The research questions are centred on the problems that motivate\nthe use of machine learning in self-adaptive systems, the key engineering\naspects of learning in self-adaptation, and open challenges. The search\nresulted in 6709 papers, of which 109 were retained for data collection.\nAnalysis of the collected data shows that machine learning is mostly used for\nupdating adaptation rules and policies to improve system qualities, and\nmanaging resources to better balance qualities and resources. These problems\nare primarily solved using supervised and interactive learning with\nclassification, regression and reinforcement learning as the dominant methods.\nSurprisingly, unsupervised learning that naturally fits automation is only\napplied in a small number of studies. Key open challenges in this area include\nthe performance of learning, managing the effects of learning, and dealing with\nmore complex types of goals. From the insights derived from this systematic\nliterature review we outline an initial design process for applying machine\nlearning in self-adaptive systems that are based on MAPE feedback loops.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 13:45:59 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 15:09:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gheibi", "Omid", ""], ["Weyns", "Danny", ""], ["Quin", "Federico", ""]]}, {"id": "2103.04747", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel", "title": "Info-Evo: Using Information Geometry to Guide Evolutionary Program\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel optimization strategy, Info-Evo, is described, in which natural\ngradient search using nonparametric Fisher information is used to provide\nongoing guidance to an evolutionary learning algorithm, so that the\nevolutionary process preferentially moves in the directions identified as\n\"shortest paths\" according to the natural gradient. Some specifics regarding\nthe application of this approach to automated program learning are reviewed,\nincluding a strategy for integrating Info-Evo into the MOSES program learning\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 09:36:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Goertzel", "Ben", ""]]}, {"id": "2103.04748", "submitter": "Pouya Rezazadeh Kalehbasti", "authors": "Pouya Rezazadeh Kalehbasti, Michael D. Lepech, and Samarpreet Singh\n  Pandher", "title": "Augmenting High-dimensional Nonlinear Optimization with Conditional GANs", "comments": "9 pages, 5 figures, accepted at GECCO 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many mathematical optimization algorithms fail to sufficiently explore the\nsolution space of high-dimensional nonlinear optimization problems due to the\ncurse of dimensionality. This paper proposes generative models as a complement\nto optimization algorithms to improve performance in problems with high\ndimensionality. To demonstrate this method, a conditional generative\nadversarial network (C-GAN) is used to augment the solutions produced by a\ngenetic algorithm (GA) for a 311-dimensional nonconvex multi-objective\nmixed-integer nonlinear optimization. The C-GAN, composed of two networks with\nthree fully connected hidden layers, is trained on solutions generated by GA,\nand then given sets of desired labels (i.e., objective function values),\ngenerates complementary solutions corresponding to those labels. Six\nexperiments are conducted to evaluate the capabilities of the proposed method.\nThe generated complementary solutions are compared to the original solutions in\nterms of optimality and diversity. The generative model generates solutions\nwith objective functions up to 100% better, and with hypervolumes up to 100%\nhigher, than the original solutions. These findings show that a C-GAN with even\na simple training approach and architecture can, with a much shorter runtime,\nhighly improve the diversity and optimality of solutions found by an\noptimization algorithm for a high-dimensional nonlinear optimization problem.\n[Link to GitHub repository: https://github.com/PouyaREZ/GAN_GA]\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:07:33 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 04:01:36 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kalehbasti", "Pouya Rezazadeh", ""], ["Lepech", "Michael D.", ""], ["Pandher", "Samarpreet Singh", ""]]}, {"id": "2103.04751", "submitter": "Avijit Basak", "authors": "Avijit Basak", "title": "A Memory Optimized Data Structure for Binary Chromosomes in Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a memory-optimized metadata-based data structure for\nimplementation of binary chromosome in Genetic Algorithm. In GA different types\nof genotypes are used depending on the problem domain. Among these, binary\ngenotype is the most popular one for non-enumerated encoding owing to its\nrepresentational and computational simplicity. This paper proposes a\nmemory-optimized implementation approach of binary genotype. The approach\nimproves the memory utilization as well as capacity of retaining alleles.\nMathematical proof has been provided to establish the same.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:49:11 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Basak", "Avijit", ""]]}, {"id": "2103.04852", "submitter": "E K", "authors": "Eren Kurshan, Hai Li, Mingoo Seok, Yuan Xie", "title": "A Case for 3D Integrated System Design for Neuromorphic Computing & AI\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, artificial intelligence has found many applications\nareas in the society. As AI solutions have become more sophistication and the\nuse cases grew, they highlighted the need to address performance and energy\nefficiency challenges faced during the implementation process. To address these\nchallenges, there has been growing interest in neuromorphic chips. Neuromorphic\ncomputing relies on non von Neumann architectures as well as novel devices,\ncircuits and manufacturing technologies to mimic the human brain. Among such\ntechnologies, 3D integration is an important enabler for AI hardware and the\ncontinuation of the scaling laws. In this paper, we overview the unique\nopportunities 3D integration provides in neuromorphic chip design, discuss the\nemerging opportunities in next generation neuromorphic architectures and review\nthe obstacles. Neuromorphic architectures, which relied on the brain for\ninspiration and emulation purposes, face grand challenges due to the limited\nunderstanding of the functionality and the architecture of the human brain.\nYet, high-levels of investments are dedicated to develop neuromorphic chips. We\nargue that 3D integration not only provides strategic advantages to the\ncost-effective and flexible design of neuromorphic chips, it may provide design\nflexibility in incorporating advanced capabilities to further benefits the\ndesigns in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:50:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kurshan", "Eren", ""], ["Li", "Hai", ""], ["Seok", "Mingoo", ""], ["Xie", "Yuan", ""]]}, {"id": "2103.04862", "submitter": "Andrea Ponti", "authors": "Antonio Candelieri, Andrea Ponti, Francesco Archetti", "title": "Risk Aware Optimization of Water Sensor Placement", "comments": "9 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal sensor placement (SP) usually minimizes an impact measure, such as\nthe amount of contaminated water or the number of inhabitants affected before\ndetection. The common choice is to minimize the minimum detection time (MDT)\naveraged over a set of contamination events, with contaminant injected at a\ndifferent location. Given a SP, propagation is simulated through a hydraulic\nsoftware model of the network to obtain spatio-temporal concentrations and the\naverage MDT. Searching for an optimal SP is NP-hard: even for mid-size\nnetworks, efficient search methods are required, among which evolutionary\napproaches are often used. A bi-objective formalization is proposed: minimizing\nthe average MDT and its standard deviation, that is the risk to detect some\ncontamination event too late than the average MDT. We propose a data structure\n(sort of spatio-temporal heatmap) collecting simulation outcomes for every SP\nand particularly suitable for evolutionary optimization. Indeed, the proposed\ndata structure enabled a convergence analysis of a population-based algorithm,\nleading to the identification of indicators for detecting problem-specific\nconverge issues which could be generalized to other similar problems. We used\nPymoo, a recent Python framework flexible enough to incorporate our problem\nspecific termination criterion. Results on a benchmark and a real-world network\nare presented.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:12:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Candelieri", "Antonio", ""], ["Ponti", "Andrea", ""], ["Archetti", "Francesco", ""]]}, {"id": "2103.04909", "submitter": "Ramin Hasani", "authors": "Axel Brunnbauer, Luigi Berducci, Andreas Brandst\\\"atter, Mathias\n  Lechner, Ramin Hasani, Daniela Rus, Radu Grosu", "title": "Model-based versus Model-free Deep Reinforcement Learning for Autonomous\n  Racing Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the rich theoretical foundation of model-based deep reinforcement\nlearning (RL) agents, their effectiveness in real-world robotics-applications\nis less studied and understood. In this paper, we, therefore, investigate how\nsuch agents generalize to real-world autonomous-vehicle control-tasks, where\nadvanced model-free deep RL algorithms fail. In particular, we set up a series\nof time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional\nLiDAR sensors, on a set of test tracks with a gradual increase in their\ncomplexity. In this continuous-control setting, we show that model-based agents\ncapable of learning in imagination, substantially outperform model-free agents\nwith respect to performance, sample efficiency, successful task completion, and\ngeneralization. Moreover, we show that the generalization ability of\nmodel-based agents strongly depends on the observation-model choice. Finally,\nwe provide extensive empirical evidence for the effectiveness of model-based\nagents provided with long enough memory horizons in sim2real tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:15:23 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Brunnbauer", "Axel", ""], ["Berducci", "Luigi", ""], ["Brandst\u00e4tter", "Andreas", ""], ["Lechner", "Mathias", ""], ["Hasani", "Ramin", ""], ["Rus", "Daniela", ""], ["Grosu", "Radu", ""]]}, {"id": "2103.05129", "submitter": "Dirk Roose", "authors": "I.A. Negrin, D. Roose, E.L. Chagoyen, G. Lombaert", "title": "Biogeography-Based Optimization of RC structures including static\n  soil-structure interaction", "comments": "15 pages, 16 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A method to minimize the cost of the structural design of reinforced concrete\nstructures using Biogeography-Based Optimization, an evolutionary algorithm, is\npresented. SAP2000 is used as computational engine, taking into account\nmodelling aspects such as static soil-structure interaction (SSSI). The\noptimization problem is formulated to properly reflect an actual design\nproblem, limiting e.g. the size of reinforcement bars to commercially available\nsections. Strategies to reduce the computational cost of the optimization\nprocedure are proposed and an extensive parameter tuning was performed. The\nresulting tuned optimization algorithm allows to reduce the direct cost of the\nconstruction of a particular structure project with 21% compared to a design\nbased on traditional criteria. We also evaluate the effect on the cost of the\nsuperstructure when SSSI is takeninto account.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:48:04 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Negrin", "I. A.", ""], ["Roose", "D.", ""], ["Chagoyen", "E. L.", ""], ["Lombaert", "G.", ""]]}, {"id": "2103.05442", "submitter": "Martina Saletta", "authors": "Martina Saletta, Claudio Ferretti", "title": "Mining Program Properties From Neural Networks Trained on Source Code\n  Embeddings", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for mining different program\nfeatures by analysing the internal behaviour of a deep neural network trained\non source code. Using an unlabelled dataset of Java programs and three\ndifferent embedding strategies for the methods in the dataset, we train an\nautoencoder for each program embedding and then we test the emerging ability of\nthe internal neurons in autonomously building internal representations for\ndifferent program features. We defined three binary classification labelling\npolicies inspired by real programming issues, so to test the performance of\neach neuron in classifying programs accordingly to these classification rules,\nshowing that some neurons can actually detect different program properties. We\nalso analyse how the program representation chosen as input affects the\nperformance on the aforementioned tasks. On the other hand, we are interested\nin finding the overall most informative neurons in the network regardless of a\ngiven task. To this aim, we propose and evaluate two methods for ranking\nneurons independently of any property. Finally, we discuss how these ideas can\nbe applied in different settings for simplifying the programmers' work, for\ninstance if included in environments such as software repositories or code\neditors.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:25:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Saletta", "Martina", ""], ["Ferretti", "Claudio", ""]]}, {"id": "2103.05447", "submitter": "Amir Mosavi Prof", "authors": "Babak Lashkar-Ara, Niloofar Kalantari, Zohreh Sheikh Khozani, Amir\n  Mosavi", "title": "Machine Learning versus Mathematical Model to Estimate the Transverse\n  Shear Stress Distribution in a Rectangular Channel", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most important subjects of hydraulic engineering is the reliable\nestimation of the transverse distribution in the rectangular channel of bed and\nwall shear stresses. This study makes use of the Tsallis entropy, genetic\nprogramming (GP) and adaptive neuro-fuzzy inference system (ANFIS) methods to\nassess the shear stress distribution (SSD) in the rectangular channel. To\nevaluate the results of the Tsallis entropy, GP and ANFIS models, laboratory\nobservations were used in which shear stress was measured using an optimized\nPreston tube. This is then used to measure the SSD in various aspect ratios in\nthe rectangular channel. To investigate the shear stress percentage, 10 data\nseries with a total of 112 different data were used. The results of the\nsensitivity analysis show that the most influential parameter for the SSD in a\nsmooth rectangular channel is the dimensionless parameter B/H, Where the\ntransverse coordinate is B, and the flow depth is H. With the parameters (b/B),\n(B/H) for the bed and (z/H), (B/H) for the wall as inputs, the modeling of the\nGP was better than the other one. Based on the analysis, it can be concluded\nthat the use of GP and ANFIS algorithms is more effective in estimating shear\nstress in smooth rectangular channels than the Tsallis entropy-based equations.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 23:08:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lashkar-Ara", "Babak", ""], ["Kalantari", "Niloofar", ""], ["Khozani", "Zohreh Sheikh", ""], ["Mosavi", "Amir", ""]]}, {"id": "2103.05590", "submitter": "Mehdi Yadollahi", "authors": "Mohammad Mehdi Yadollahi, Farzaneh Shoeleh, Sajjad Dadkhah, Ali A.\n  Ghorbani", "title": "Robust Black-box Watermarking for Deep NeuralNetwork using Inverse\n  Document Frequency", "comments": "This manuscript is submitted to computer & security journal on Sep\n  26th. It has 13 pages, 8 figures and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning techniques are one of the most significant elements of any\nArtificial Intelligence (AI) services. Recently, these Machine Learning (ML)\nmethods, such as Deep Neural Networks (DNNs), presented exceptional achievement\nin implementing human-level capabilities for various predicaments, such as\nNatural Processing Language (NLP), voice recognition, and image processing,\netc. Training these models are expensive in terms of computational power and\nthe existence of enough labelled data. Thus, ML-based models such as DNNs\nestablish genuine business value and intellectual property (IP) for their\nowners. Therefore the trained models need to be protected from any adversary\nattacks such as illegal redistribution, reproducing, and derivation.\nWatermarking can be considered as an effective technique for securing a DNN\nmodel. However, so far, most of the watermarking algorithm focuses on\nwatermarking the DNN by adding noise to an image. To this end, we propose a\nframework for watermarking a DNN model designed for a textual domain. The\nwatermark generation scheme provides a secure watermarking method by combining\nTerm Frequency (TF) and Inverse Document Frequency (IDF) of a particular word.\nThe proposed embedding procedure takes place in the model's training time,\nmaking the watermark verification stage straightforward by sending the\nwatermarked document to the trained model. The experimental results show that\nwatermarked models have the same accuracy as the original ones. The proposed\nframework accurately verifies the ownership of all surrogate models without\nimpairing the performance. The proposed algorithm is robust against well-known\nattacks such as parameter pruning and brute force attack.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:56:04 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yadollahi", "Mohammad Mehdi", ""], ["Shoeleh", "Farzaneh", ""], ["Dadkhah", "Sajjad", ""], ["Ghorbani", "Ali A.", ""]]}, {"id": "2103.05636", "submitter": "Jack Kendall", "authors": "Jack Kendall", "title": "A Gradient Estimator for Time-Varying Electrical Networks with\n  Non-Linear Dissipation", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for extending the technique of equilibrium propagation\nfor estimating gradients in fixed-point neural networks to the more general\nsetting of directed, time-varying neural networks by modeling them as\nelectrical circuits. We use electrical circuit theory to construct a Lagrangian\ncapable of describing deep, directed neural networks modeled using nonlinear\ncapacitors and inductors, linear resistors and sources, and a special class of\nnonlinear dissipative elements called fractional memristors. We then derive an\nestimator for the gradient of the physical parameters of the network, such as\nsynapse conductances, with respect to an arbitrary loss function. This\nestimator is entirely local, in that it only depends on information locally\navailable to each synapse. We conclude by suggesting methods for extending\nthese results to networks of biologically plausible neurons, e.g.\nHodgkin-Huxley neurons.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:07:39 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kendall", "Jack", ""]]}, {"id": "2103.05683", "submitter": "Amey Hengle", "authors": "Amey Hengle, Atharva Kshirsagar, Shaily Desai and Manisha Marathe", "title": "Combining Context-Free and Contextualized Representations for Arabic\n  Sarcasm Detection and Sentiment Identification", "comments": "7 pages, 1 figure, The Sixth Arabic Natural Language Processing\n  Workshop. (WANLP 2021), held in conjunction with EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their inception, transformer-based language models have led to\nimpressive performance gains across multiple natural language processing tasks.\nFor Arabic, the current state-of-the-art results on most datasets are achieved\nby the AraBERT language model. Notwithstanding these recent advancements,\nsarcasm and sentiment detection persist to be challenging tasks in Arabic,\ngiven the language's rich morphology, linguistic disparity and dialectal\nvariations. This paper proffers team SPPU-AASM's submission for the WANLP\nArSarcasm shared-task 2021, which centers around the sarcasm and sentiment\npolarity detection of Arabic tweets. The study proposes a hybrid model,\ncombining sentence representations from AraBERT with static word vectors\ntrained on Arabic social media corpora. The proposed system achieves a\nF1-sarcastic score of 0.62 and a F-PN score of 0.715 for the sarcasm and\nsentiment detection tasks, respectively. Simulation results show that the\nproposed system outperforms multiple existing approaches for both the tasks,\nsuggesting that the amalgamation of context-free and context-dependent text\nrepresentations can help capture complementary facets of word meaning in\nArabic. The system ranked second and tenth in the respective sub-tasks of\nsarcasm detection and sentiment identification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:39:43 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Hengle", "Amey", ""], ["Kshirsagar", "Atharva", ""], ["Desai", "Shaily", ""], ["Marathe", "Manisha", ""]]}, {"id": "2103.05707", "submitter": "Anup Das", "authors": "Twisha Titirsha, Shihao Song, Anup Das, Jeffrey Krichmar, Nikil Dutt,\n  Nagarajan Kandasamy, Francky Catthoor", "title": "Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic\n  Hardware", "comments": "Accepted for publication in IEEE Transactions on Parallel and\n  Distributed Systems (TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing systems are embracing memristors to implement high\ndensity and low power synaptic storage as crossbar arrays in hardware. These\nsystems are energy efficient in executing Spiking Neural Networks (SNNs). We\nobserve that long bitlines and wordlines in a memristive crossbar are a major\nsource of parasitic voltage drops, which create current asymmetry. Through\ncircuit simulations, we show the significant endurance variation that results\nfrom this asymmetry. Therefore, if the critical memristors (ones with lower\nendurance) are overutilized, they may lead to a reduction of the crossbar's\nlifetime. We propose eSpine, a novel technique to improve lifetime by\nincorporating the endurance variation within each crossbar in mapping machine\nlearning workloads, ensuring that synapses with higher activation are always\nimplemented on memristors with higher endurance, and vice versa. eSpine works\nin two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to\npartition a workload into clusters of neurons and synapses, where each cluster\ncan fit in a crossbar. Second, it uses an instance of Particle Swarm\nOptimization (PSO) to map clusters to tiles, where the placement of synapses of\na cluster to memristors of a crossbar is performed by analyzing their\nactivation within the workload. We evaluate eSpine for a state-of-the-art\nneuromorphic hardware model with phase-change memory (PCM)-based memristors.\nUsing 10 SNN workloads, we demonstrate a significant improvement in the\neffective lifetime.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:43:28 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Titirsha", "Twisha", ""], ["Song", "Shihao", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey", ""], ["Dutt", "Nikil", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2103.05753", "submitter": "Bradly Alicea", "authors": "Bradly Alicea, Rishabh Chakrabarty, Akshara Gopi, Anson Lim, and Jesse\n  Parent", "title": "Embodied Continual Learning Across Developmental Time Via Developmental\n  Braitenberg Vehicles", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much to learn through synthesis of Developmental Biology, Cognitive\nScience and Computational Modeling. One lesson we can learn from this\nperspective is that the initialization of intelligent programs cannot solely\nrely on manipulation of numerous parameters. Our path forward is to present a\ndesign for developmentally-inspired learning agents based on the Braitenberg\nVehicle. Using these agents to exemplify artificial embodied intelligence, we\nmove closer to modeling embodied experience and morphogenetic growth as\ncomponents of cognitive developmental capacity. We consider various factors\nregarding biological and cognitive development which influence the generation\nof adult phenotypes and the contingency of available developmental pathways.\nThese mechanisms produce emergent connectivity with shifting weights and\nadaptive network topography, thus illustrating the importance of developmental\nprocesses in training neural networks. This approach provides a blueprint for\nadaptive agent behavior that might result from a developmental approach: namely\nby exploiting critical periods or growth and acquisition, an explicitly\nembodied network architecture, and a distinction between the assembly of neural\nnetworks and active learning on these networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:22:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Alicea", "Bradly", ""], ["Chakrabarty", "Rishabh", ""], ["Gopi", "Akshara", ""], ["Lim", "Anson", ""], ["Parent", "Jesse", ""]]}, {"id": "2103.05760", "submitter": "Tarik A. Rashid", "authors": "Hardi M. Mohammed, Zrar Kh. Abdul, Tarik A. Rashid, Abeer Alsadoon,\n  Nebojsa Bacanin", "title": "A New K means Grey Wolf Algorithm for Engineering Problems", "comments": "15 pages. World Journal of Engineering, 2021", "journal-ref": null, "doi": "10.1108/WJE-10-2020-0527", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose: The development of metaheuristic algorithms has increased by\nresearchers to use them extensively in the field of business, science, and\nengineering. One of the common metaheuristic optimization algorithms is called\nGrey Wolf Optimization (GWO). The algorithm works based on imitation of the\nwolves' searching and the process of attacking grey wolves. The main purpose of\nthis paper to overcome the GWO problem which is trapping into local optima.\n  Design or Methodology or Approach: In this paper, the K-means clustering\nalgorithm is used to enhance the performance of the original Grey Wolf\nOptimization by dividing the population into different parts. The proposed\nalgorithm is called K-means clustering Grey Wolf Optimization (KMGWO).\n  Findings: Results illustrate the efficiency of KMGWO is superior to GWO. To\nevaluate the performance of the KMGWO, KMGWO applied to solve 10 CEC2019\nbenchmark test functions. Results prove that KMGWO is better compared to GWO.\nKMGWO is also compared to Cat Swarm Optimization (CSO), Whale Optimization\nAlgorithm-Bat Algorithm (WOA-BAT), and WOA, so, KMGWO achieves the first rank\nin terms of performance. Statistical results proved that KMGWO achieved a\nhigher significant value compared to the compared algorithms. Also, the KMGWO\nis used to solve a pressure vessel design problem and it has outperformed\nresults.\n  Originality/value: Results prove that KMGWO is superior to GWO. KMGWO is also\ncompared to cat swarm optimization (CSO), whale optimization algorithm-bat\nalgorithm (WOA-BAT), WOA, and GWO so KMGWO achieved the first rank in terms of\nperformance. Also, the KMGWO is used to solve a classical engineering problem\nand it is superior\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:29:07 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mohammed", "Hardi M.", ""], ["Abdul", "Zrar Kh.", ""], ["Rashid", "Tarik A.", ""], ["Alsadoon", "Abeer", ""], ["Bacanin", "Nebojsa", ""]]}, {"id": "2103.05916", "submitter": "Dominique Vaufreydaz", "authors": "Louis Airale (M-PSI, PERCEPTION), Dominique Vaufreydaz (M-PSI), Xavier\n  Alameda-Pineda (PERCEPTION)", "title": "SocialInteractionGAN: Multi-person Interaction Sequence Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of human actions in social interactions has important applications\nin the design of social robots or artificial avatars. In this paper, we model\nhuman interaction generation as a discrete multi-sequence generation problem\nand present SocialInteractionGAN, a novel adversarial architecture for\nconditional interaction generation. Our model builds on a recurrent\nencoder-decoder generator network and a dual-stream discriminator. This\narchitecture allows the discriminator to jointly assess the realism of\ninteractions and that of individual action sequences. Within each stream a\nrecurrent network operating on short subsequences endows the output signal with\nlocal assessments, better guiding the forthcoming generation. Crucially,\ncontextual information on interacting participants is shared among agents and\nreinjected in both the generation and the discriminator evaluation processes.\nWe show that the proposed SocialInteractionGAN succeeds in producing high\nrealism action sequences of interacting people, comparing favorably to a\ndiversity of recurrent and convolutional discriminator baselines. Evaluations\nare conducted using modified Inception Score and Fr{\\'e}chet Inception Distance\nmetrics, that we specifically design for discrete sequential generated data.\nThe distribution of generated sequences is shown to approach closely that of\nreal data. In particular our model properly learns the dynamics of interaction\nsequences, while exploiting the full range of actions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:11:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Airale", "Louis", "", "M-PSI, PERCEPTION"], ["Vaufreydaz", "Dominique", "", "M-PSI"], ["Alameda-Pineda", "Xavier", "", "PERCEPTION"]]}, {"id": "2103.06123", "submitter": "Hiroshi Yamakawa", "authors": "Hiroshi Yamakawa", "title": "The whole brain architecture approach: Accelerating the development of\n  artificial general intelligence by referring to the brain", "comments": "28 pages, 10 figures, Preprint submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The vastness of the design space created by the combination of a large number\nof computational mechanisms, including machine learning, is an obstacle to\ncreating an artificial general intelligence (AGI). Brain-inspired AGI\ndevelopment, in other words, cutting down the design space to look more like a\nbiological brain, which is an existing model of a general intelligence, is a\npromising plan for solving this problem. However, it is difficult for an\nindividual to design a software program that corresponds to the entire brain\nbecause the neuroscientific data required to understand the architecture of the\nbrain are extensive and complicated. The whole-brain architecture approach\ndivides the brain-inspired AGI development process into the task of designing\nthe brain reference architecture (BRA) -- the flow of information and the\ndiagram of corresponding components -- and the task of developing each\ncomponent using the BRA. This is called BRA-driven development. Another\ndifficulty lies in the extraction of the operating principles necessary for\nreproducing the cognitive-behavioral function of the brain from neuroscience\ndata. Therefore, this study proposes the Structure-constrained Interface\nDecomposition (SCID) method, which is a hypothesis-building method for creating\na hypothetical component diagram consistent with neuroscientific findings. The\napplication of this approach has begun for building various regions of the\nbrain. Moving forward, we will examine methods of evaluating the biological\nplausibility of brain-inspired software. This evaluation will also be used to\nprioritize different computational mechanisms, which should be merged,\nassociated with the same regions of the brain.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 04:58:12 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yamakawa", "Hiroshi", ""]]}, {"id": "2103.06380", "submitter": "Ke Li Kl", "authors": "Jiangjiao Xu, Ke Li, and Mohammad Abusara", "title": "Multi-Objective Reinforcement Learning based Multi-Microgrid System\n  Optimisation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microgrids with energy storage systems and distributed renewable energy\nsources play a crucial role in reducing the consumption from traditional power\nsources and the emission of $CO_2$. Connecting multi microgrid to a\ndistribution power grid can facilitate a more robust and reliable operation to\nincrease the security and privacy of the system. The proposed model consists of\nthree layers, smart grid layer, independent system operator (ISO) layer and\npower grid layer. Each layer aims to maximise its benefit. To achieve these\nobjectives, an intelligent multi-microgrid energy management method is proposed\nbased on the multi-objective reinforcement learning (MORL) techniques, leading\nto a Pareto optimal set. A non-dominated solution is selected to implement a\nfair design in order not to favour any particular participant. The simulation\nresults demonstrate the performance of the MORL and verify the viability of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:01:22 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xu", "Jiangjiao", ""], ["Li", "Ke", ""], ["Abusara", "Mohammad", ""]]}, {"id": "2103.06382", "submitter": "Ke Li Kl", "authors": "Xinyu Shan, Ke Li", "title": "An Improved Two-Archive Evolutionary Algorithm for Constrained\n  Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained multi-objective optimization problems (CMOPs) are ubiquitous in\nreal-world engineering optimization scenarios. A key issue in constrained\nmulti-objective optimization is to strike a balance among convergence,\ndiversity and feasibility. A recently proposed two-archive evolutionary\nalgorithm for constrained multi-objective optimization (C-TAEA) has be shown as\na latest algorithm. However, due to its simple implementation of the\ncollaboration mechanism between its two co-evolving archives, C-TAEA is\nstruggling when solving problems whose \\textit{pseudo} Pareto-optimal front,\nwhich does not take constraints into consideration, dominates the\n\\textit{feasible} Pareto-optimal front. In this paper, we propose an improved\nversion C-TAEA, dubbed C-TAEA-II, featuring an improved update mechanism of two\nco-evolving archives and an adaptive mating selection mechanism to promote a\nbetter collaboration between co-evolving archives. Empirical results\ndemonstrate the competitiveness of the proposed C-TAEA-II in comparison with\nfive representative constrained evolutionary multi-objective optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:04:02 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Shan", "Xinyu", ""], ["Li", "Ke", ""]]}, {"id": "2103.06435", "submitter": "Kevin Frans", "authors": "Kevin Frans, Olaf Witkowski", "title": "Population-Based Evolution Optimizes a Meta-Learning Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning models, or models that learn to learn, have been a long-desired\ntarget for their ability to quickly solve new tasks. Traditional meta-learning\nmethods can require expensive inner and outer loops, thus there is demand for\nalgorithms that discover strong learners without explicitly searching for them.\nWe draw parallels to the study of evolvable genomes in evolutionary systems --\ngenomes with a strong capacity to adapt -- and propose that meta-learning and\nadaptive evolvability optimize for the same objective: high performance after a\nset of learning iterations. We argue that population-based evolutionary systems\nwith non-static fitness landscapes naturally bias towards high-evolvability\ngenomes, and therefore optimize for populations with strong learning ability.\nWe demonstrate this claim with a simple evolutionary algorithm,\nPopulation-Based Meta Learning (PBML), that consistently discovers genomes\nwhich display higher rates of improvement over generations, and can rapidly\nadapt to solve sparse fitness and robotic control tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:45:43 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Frans", "Kevin", ""], ["Witkowski", "Olaf", ""]]}, {"id": "2103.06460", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu", "title": "Emerging Paradigms of Neural Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterization of neural networks benefits the optimization and\ngeneralization yet brings cost in practice. Pruning is adopted as a\npost-processing solution to this problem, which aims to remove unnecessary\nparameters in a neural network with little performance compromised. It has been\nbroadly believed the resulted sparse neural network cannot be trained from\nscratch to comparable accuracy. However, several recent works (e.g., [Frankle\nand Carbin, 2019a]) challenge this belief by discovering random sparse networks\nwhich can be trained to match the performance with their dense counterpart.\nThis new pruning paradigm later inspires more new methods of pruning at\ninitialization. In spite of the encouraging progress, how to coordinate these\nnew pruning fashions with the traditional pruning has not been explored yet.\nThis survey seeks to bridge the gap by proposing a general pruning framework so\nthat the emerging pruning paradigms can be accommodated well with the\ntraditional one. With it, we systematically reflect the major differences and\nnew insights brought by these new pruning fashions, with representative works\ndiscussed at length. Finally, we summarize the open questions as worthy future\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:01:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "2103.06490", "submitter": "Parag Dutta", "authors": "Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir,\n  Ambedkar Dukkipati", "title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation", "comments": "Two of the authors had published similar manuscripts on arXiv. So\n  withdrawing this one. All further updations will be reflected at\n  arXiv:1911.00234", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate further such redundant examples\nchosen by an AL strategy. We show that A$\\mathbf{^2}$L is widely applicable by\nusing it in conjunction with several different AL strategies and NLP tasks. We\nempirically demonstrate that the proposed approach is further able to reduce\nthe data requirements of state-of-the-art AL strategies by an absolute\npercentage reduction of $\\approx\\mathbf{3-25\\%}$ on multiple NLP tasks while\nachieving the same performance with no additional computation overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:27:31 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:49:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hazra", "Rishi", ""], ["Dutta", "Parag", ""], ["Gupta", "Shubham", ""], ["Qaathir", "Mohammed Abdul", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "2103.06739", "submitter": "Alexander Hvatov", "authors": "Mikhail Maslyaev and Alexander Hvatov", "title": "Multi-objective discovery of PDE systems using evolutionary approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, the systems of partial differential equations (PDEs) are discovered\nfrom observational data in the single vector equation form. However, this\napproach restricts the application to the real cases, where, for example, the\nform of the external forcing is of interest. In the paper, a multi-objective\nco-evolution algorithm is described. The single equations within the system and\nthe system itself are evolved simultaneously to obtain the system. This\napproach allows discovering the systems with the form-independent equations. In\ncontrast to the single vector equation, a component-wise system is more\nsuitable for expert interpretation and, therefore, for applications. The\nexample of the two-dimensional Navier-Stokes equation is considered.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:37:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Maslyaev", "Mikhail", ""], ["Hvatov", "Alexander", ""]]}, {"id": "2103.06846", "submitter": "Nicolas Bredeche", "authors": "Paul Ecoffet, Nicolas Fontbonne, Jean-Baptiste Andr\\'e, Nicolas\n  Bredeche", "title": "Policy Search with Rare Significant Events: Choosing the Right Partner\n  to Cooperate with", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on a class of reinforcement learning problems where\nsignificant events are rare and limited to a single positive reward per\nepisode. A typical example is that of an agent who has to choose a partner to\ncooperate with, while a large number of partners are simply not interested in\ncooperating, regardless of what the agent has to offer. We address this problem\nin a continuous state and action space with two different kinds of search\nmethods: a gradient policy search method and a direct policy search method\nusing an evolution strategy. We show that when significant events are rare,\ngradient information is also scarce, making it difficult for policy gradient\nsearch methods to find an optimal policy, with or without a deep neural\narchitecture. On the other hand, we show that direct policy search methods are\ninvariant to the rarity of significant events, which is yet another\nconfirmation of the unique role evolutionary algorithms has to play as a\nreinforcement learning method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:14:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ecoffet", "Paul", ""], ["Fontbonne", "Nicolas", ""], ["Andr\u00e9", "Jean-Baptiste", ""], ["Bredeche", "Nicolas", ""]]}, {"id": "2103.07117", "submitter": "Aurora Saibene", "authors": "Aurora Saibene (1 and 2) and Francesca Gasparini (1 and 2) ((1)\n  University of Milano-Bicocca, Department of Informatics, Systems and\n  Communications, Multi Media Signal Processing Laboratory, (2) University of\n  Milano-Bicocca, NeuroMI)", "title": "GA for feature selection of EEG heterogeneous data", "comments": "submitted to Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The electroencephalographic (EEG) signals provide highly informative data on\nbrain activities and functions. However, their heterogeneity and high\ndimensionality may represent an obstacle for their interpretation. The\nintroduction of a priori knowledge seems the best option to mitigate high\ndimensionality problems, but could lose some information and patterns present\nin the data, while data heterogeneity remains an open issue that often makes\ngeneralization difficult. In this study, we propose a genetic algorithm (GA)\nfor feature selection that can be used with a supervised or unsupervised\napproach. Our proposal considers three different fitness functions without\nrelying on expert knowledge. Starting from two publicly available datasets on\ncognitive workload and motor movement/imagery, the EEG signals are processed,\nnormalized and their features computed in the time, frequency and\ntime-frequency domains. The feature vector selection is performed by applying\nour GA proposal and compared with two benchmarking techniques. The results show\nthat different combinations of our proposal achieve better results in respect\nto the benchmark in terms of overall performance and feature reduction.\nMoreover, the proposed GA, based on a novel fitness function here presented,\noutperforms the benchmark when the two different datasets considered are merged\ntogether, showing the effectiveness of our proposal on heterogeneous data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 07:27:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Saibene", "Aurora", "", "1 and 2"], ["Gasparini", "Francesca", "", "1 and 2"]]}, {"id": "2103.07173", "submitter": "Xuan Wu", "authors": "Xuan Wu, Linhan Jia, Xiuyi Zhang, Liang Chen, Yanchun Liang, You Zhou\n  and Chunguo Wu", "title": "Neural Architecture Search based on Cartesian Genetic Programming Coding\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) is a hot topic in the field of automated\nmachine learning (AutoML) and has begun to outperform human-designed\narchitectures on many machine learning tasks. Motivated by the natural\nrepresentation form of neural networks by the Cartesian genetic programming\n(CGP), we propose an evolutionary approach of NAS based on CGP, called CPGNAS,\nto solve sentence classification task. To evolve the architectures under the\nframework of CGP, the existing key operations are identified as the types of\nfunction nodes of CGP, and the evolutionary operations are designed based on\nevolutionary strategy (ES). The experimental results show that the searched\narchitecture can reach the accuracy of human-designed architectures. The\nablation tests identify the Attention function as the single key function node\nand the Convolution and Attention as the joint key function nodes. However, the\nlinear transformations along could keep the accuracy of evolved architectures\nover 70%, which is worthy of investigation in the future.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:51:03 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 00:27:24 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 02:06:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wu", "Xuan", ""], ["Jia", "Linhan", ""], ["Zhang", "Xiuyi", ""], ["Chen", "Liang", ""], ["Liang", "Yanchun", ""], ["Zhou", "You", ""], ["Wu", "Chunguo", ""]]}, {"id": "2103.07248", "submitter": "Francesco Fusco", "authors": "Francesco Fusco, Bradley Eck, Robert Gormally, Mark Purcell, Seshu\n  Tirupathi", "title": "Knowledge- and Data-driven Services for Energy Systems using Graph\n  Neural Networks", "comments": "Accepted for publication in proceedings of IEEE Conference of Big\n  Data 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transition away from carbon-based energy sources poses several challenges\nfor the operation of electricity distribution systems. Increasing shares of\ndistributed energy resources (e.g. renewable energy generators, electric\nvehicles) and internet-connected sensing and control devices (e.g. smart\nheating and cooling) require new tools to support accurate, datadriven decision\nmaking. Modelling the effect of such growing complexity in the electrical grid\nis possible in principle using state-of-the-art power-power flow models. In\npractice, the detailed information needed for these physical simulations may be\nunknown or prohibitively expensive to obtain. Hence, datadriven approaches to\npower systems modelling, including feedforward neural networks and\nauto-encoders, have been studied to leverage the increasing availability of\nsensor data, but have seen limited practical adoption due to lack of\ntransparency and inefficiencies on large-scale problems. Our work addresses\nthis gap by proposing a data- and knowledge-driven probabilistic graphical\nmodel for energy systems based on the framework of graph neural networks\n(GNNs). The model can explicitly factor in domain knowledge, in the form of\ngrid topology or physics constraints, thus resulting in sparser architectures\nand much smaller parameters dimensionality when compared with traditional\nmachine-learning models with similar accuracy. Results obtained from a\nreal-world smart-grid demonstration project show how the GNN was used to inform\ngrid congestion predictions and market bidding services for a distribution\nsystem operator participating in an energy flexibility market.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:00:01 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Fusco", "Francesco", ""], ["Eck", "Bradley", ""], ["Gormally", "Robert", ""], ["Purcell", "Mark", ""], ["Tirupathi", "Seshu", ""]]}, {"id": "2103.07279", "submitter": "Thomas Wendler", "authors": "Christina Bukas, Bailiang Jian, Luis F. Rodriguez Venegas, Francesca\n  De Benetti, Sebastian Ruehling, Anjany Sekuboyina, Jens Gempt, Jan S.\n  Kirschke, Marie Piraud, Johannes Oberreuter, Nassir Navab and Thomas Wendler", "title": "Patient-specific virtual spine straightening and vertebra inpainting: An\n  automatic framework for osteoplasty planning", "comments": "7 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symptomatic spinal vertebral compression fractures (VCFs) often require\nosteoplasty treatment. A cement-like material is injected into the bone to\nstabilize the fracture, restore the vertebral body height and alleviate pain.\nLeakage is a common complication and may occur due to too much cement being\ninjected. In this work, we propose an automated patient-specific framework that\ncan allow physicians to calculate an upper bound of cement for the injection\nand estimate the optimal outcome of osteoplasty. The framework uses the patient\nCT scan and the fractured vertebra label to build a virtual healthy spine using\na high-level approach. Firstly, the fractured spine is segmented with a\nthree-step Convolution Neural Network (CNN) architecture. Next, a per-vertebra\nrigid registration to a healthy spine atlas restores its curvature. Finally, a\nGAN-based inpainting approach replaces the fractured vertebra with an\nestimation of its original shape. Based on this outcome, we then estimate the\nmaximum amount of bone cement for injection. We evaluate our framework by\ncomparing the virtual vertebrae volumes of ten patients to their healthy\nequivalent and report an average error of 3.88$\\pm$7.63\\%. The presented\npipeline offers a first approach to a personalized automatic high-level\nframework for planning osteoplasty procedures.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:55:08 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:42:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bukas", "Christina", ""], ["Jian", "Bailiang", ""], ["Venegas", "Luis F. Rodriguez", ""], ["De Benetti", "Francesca", ""], ["Ruehling", "Sebastian", ""], ["Sekuboyina", "Anjany", ""], ["Gempt", "Jens", ""], ["Kirschke", "Jan S.", ""], ["Piraud", "Marie", ""], ["Oberreuter", "Johannes", ""], ["Navab", "Nassir", ""], ["Wendler", "Thomas", ""]]}, {"id": "2103.07303", "submitter": "Jingchao Peng", "authors": "Peng Jingchao, Zhao Haitao, Hu Zhengwei", "title": "Second-Order Component Analysis for Fault Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process monitoring based on neural networks is getting more and more\nattention. Compared with classical neural networks, high-order neural networks\nhave natural advantages in dealing with heteroscedastic data. However,\nhigh-order neural networks might bring the risk of overfitting and learning\nboth the key information from original data and noises or anomalies. Orthogonal\nconstraints can greatly reduce correlations between extracted features, thereby\nreducing the overfitting risk. This paper proposes a novel fault detection\nmethod called second-order component analysis (SCA). SCA rules out the\nheteroscedasticity of pro-cess data by optimizing a second-order autoencoder\nwith orthogonal constraints. In order to deal with this constrained\noptimization problem, a geometric conjugate gradient algorithm is adopted in\nthis paper, which performs geometric optimization on the combination of Stiefel\nmanifold and Euclidean manifold. Extensive experiments on the Tennessee-Eastman\nbenchmark pro-cess show that SCA outperforms PCA, KPCA, and autoencoder in\nmissed detection rate (MDR) and false alarm rate (FAR).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:25:37 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Jingchao", "Peng", ""], ["Haitao", "Zhao", ""], ["Zhengwei", "Hu", ""]]}, {"id": "2103.07356", "submitter": "Akira Taniguchi", "authors": "Akira Taniguchi, Ayako Fukawa, Hiroshi Yamakawa", "title": "Hippocampal formation-inspired probabilistic generative model", "comments": "Submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We constructed a hippocampal formation (HPF)-inspired probabilistic\ngenerative model (HPF-PGM) using the structure-constrained interface\ndecomposition method. By modeling brain regions with PGMs, this model is\npositioned as a module that can be integrated as a whole-brain PGM. We discuss\nthe relationship between simultaneous localization and mapping (SLAM) in\nrobotics and the findings of HPF in neuroscience. Furthermore, we survey the\nmodeling for HPF and various computational models, including brain-inspired\nSLAM, spatial concept formation, and deep generative models. The HPF-PGM is a\ncomputational model that is highly consistent with the anatomical structure and\nfunctions of the HPF, in contrast to typical conventional SLAM models. By\nreferencing the brain, we suggest the importance of the integration of\negocentric/allocentric information from the entorhinal cortex to the\nhippocampus and the use of discrete-event queues.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:46:52 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Taniguchi", "Akira", ""], ["Fukawa", "Ayako", ""], ["Yamakawa", "Hiroshi", ""]]}, {"id": "2103.07413", "submitter": "Daniel Brunner", "authors": "Nadezhda Semenova, Laurent Larger, and Daniel Brunner", "title": "The general aspects of noise in analogue hardware deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks unlocked a vast range of new applications by solving\ntasks of which many were previouslydeemed as reserved to higher human\nintelligence. One of the developments enabling this success was a boost\nincomputing power provided by special purpose hardware, such as graphic or\ntensor processing units. However,these do not leverage fundamental features of\nneural networks like parallelism and analog state variables.Instead, they\nemulate neural networks relying on computing power, which results in\nunsustainable energyconsumption and comparatively low speed. Fully parallel and\nanalogue hardware promises to overcomethese challenges, yet the impact of\nanalogue neuron noise and its propagation, i.e. accumulation,\nthreatensrendering such approaches inept. Here, we analyse for the first time\nthe propagation of noise in paralleldeep neural networks comprising noisy\nnonlinear neurons. We develop an analytical treatment for both,symmetric\nnetworks to highlight the underlying mechanisms, and networks trained with back\npropagation.We find that noise accumulation is generally bound, and adding\nadditional network layers does not worsenthe signal to noise ratio beyond this\nlimit. Most importantly, noise accumulation can be suppressed entirelywhen\nneuron activation functions have a slope smaller than unity. We therefore\ndeveloped the frameworkfor noise of deep neural networks implemented in analog\nsystems, and identify criteria allowing engineers todesign noise-resilient\nnovel neural network hardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:16:26 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Semenova", "Nadezhda", ""], ["Larger", "Laurent", ""], ["Brunner", "Daniel", ""]]}, {"id": "2103.07428", "submitter": "Giovanni Iacca Dr.", "authors": "Michela Lorandi, Leonardo Lucio Custode, Giovanni Iacca", "title": "Genetic Improvement of Routing Protocols for Delay Tolerant Networks", "comments": "To be published in ACM Transactions on Evolutionary Learning and\n  Optimization", "journal-ref": null, "doi": "10.1145/3453683", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Routing plays a fundamental role in network applications, but it is\nespecially challenging in Delay Tolerant Networks (DTNs). These are a kind of\nmobile ad hoc networks made of e.g. (possibly, unmanned) vehicles and humans\nwhere, despite a lack of continuous connectivity, data must be transmitted\nwhile the network conditions change due to the nodes' mobility. In these\ncontexts, routing is NP-hard and is usually solved by heuristic \"store and\nforward\" replication-based approaches, where multiple copies of the same\nmessage are moved and stored across nodes in the hope that at least one will\nreach its destination. Still, the existing routing protocols produce relatively\nlow delivery probabilities. Here, we genetically improve two routing protocols\nwidely adopted in DTNs, namely Epidemic and PRoPHET, in the attempt to optimize\ntheir delivery probability. First, we dissect them into their fundamental\ncomponents, i.e., functionalities such as checking if a node can transfer data,\nor sending messages to all connections. Then, we apply Genetic Improvement (GI)\nto manipulate these components as terminal nodes of evolving trees. We apply\nthis methodology, in silico, to six test cases of urban networks made of\nhundreds of nodes, and find that GI produces consistent gains in delivery\nprobability in four cases. We then verify if this improvement entails a\nworsening of other relevant network metrics, such as latency and buffer time.\nFinally, we compare the logics of the best evolved protocols with those of the\nbaseline protocols, and we discuss the generalizability of the results across\ntest cases.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:46:51 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lorandi", "Michela", ""], ["Custode", "Leonardo Lucio", ""], ["Iacca", "Giovanni", ""]]}, {"id": "2103.08143", "submitter": "Olga Lukyanova", "authors": "Oleg Nikitin and Olga Lukyanova and Alex Kunin", "title": "Constrained plasticity reserve as a natural way to control frequency and\n  weights in spiking neural networks", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Biological neurons have adaptive nature and perform complex computations\ninvolving the filtering of redundant information. However, most common neural\ncell models, including biologically plausible, such as Hodgkin-Huxley or\nIzhikevich, do not possess predictive dynamics on a single-cell level.\nMoreover, the modern rules of synaptic plasticity or interconnections weights\nadaptation also do not provide grounding for the ability of neurons to adapt to\nthe ever-changing input signal intensity. While natural neuron synaptic growth\nis precisely controlled and restricted by protein supply and recycling, weight\ncorrection rules such as widely used STDP are efficiently unlimited in change\nrate and scale. The present article introduces new mechanics of interconnection\nbetween neuron firing rate homeostasis and weight change through STDP growth\nbounded by abstract protein reserve, controlled by the intracellular\noptimization algorithm. We show how these cellular dynamics help neurons filter\nout the intense noise signals to help neurons keep a stable firing rate. We\nalso examine that such filtering does not affect the ability of neurons to\nrecognize the correlated inputs in unsupervised mode. Such an approach might be\nused in the machine learning domain to improve the robustness of AI systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:22:14 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 10:56:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Nikitin", "Oleg", ""], ["Lukyanova", "Olga", ""], ["Kunin", "Alex", ""]]}, {"id": "2103.08245", "submitter": "Germ\\'an Kruszewski", "authors": "Germ\\'an Kruszewski, Tomas Mikolov", "title": "Emergence of self-reproducing metabolisms as recursive algorithms in an\n  Artificial Chemistry", "comments": "arXiv admin note: text overlap with arXiv:2003.07916", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researching the conditions for the emergence of life -- not necessarily as it\nis, but as it could be -- is one of the main goals of Artificial Life.\nArtificial Chemistries are one of the most important tools in this endeavour,\nas they allow us to investigate the process by which metabolisms capable of\nself-reproduction and -- ultimately -- of evolving, might have emerged. While\nprevious work has shown promising results in this direction, it is still\nunclear which are the fundamental properties of a chemical system that enable\nemergent structures to arise. To this end, here we present an Artificial\nChemistry based on Combinatory Logic, a Turing-complete rewriting system, which\nrelies on a minimal set of possible reactions. Our experiments show that a\nsingle run of this chemistry starting from a tabula rasa state discovers with\nno external intervention a wide range of emergent structures, including\nautopoietic structures that maintain their organisation unchanged, others that\ngrow recursively, and most notably, patterns that reproduce themselves,\nduplicating their number on each cycle. All of these structures take the form\nof recursive algorithms that acquire basic constituents from the environment\nand decompose them in a process that is remarkably similar to biological\nmetabolisms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:55:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kruszewski", "Germ\u00e1n", ""], ["Mikolov", "Tomas", ""]]}, {"id": "2103.08277", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "Representation Theorem for Matrix Product States", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the universal representation capacity of the\nMatrix Product States (MPS) from the perspective of boolean functions and\ncontinuous functions. We show that MPS can accurately realize arbitrary boolean\nfunctions by providing a construction method of the corresponding MPS structure\nfor an arbitrarily given boolean gate. Moreover, we prove that the function\nspace of MPS with the scale-invariant sigmoidal activation is dense in the\nspace of continuous functions defined on a compact subspace of the\n$n$-dimensional real coordinate space $\\mathbb{R^{n}}$. We study the relation\nbetween MPS and neural networks and show that the MPS with a scale-invariant\nsigmoidal function is equivalent to a one-hidden-layer neural network equipped\nwith a kernel function. We construct the equivalent neural networks for several\nspecific MPS models and show that non-linear kernels such as the polynomial\nkernel which introduces the couplings between different components of the input\ninto the model appear naturally in the equivalent neural networks. At last, we\ndiscuss the realization of the Gaussian Process (GP) with infinitely wide MPS\nby studying their equivalent neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:06:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2103.08327", "submitter": "Saibal Majumder", "authors": "Saibal Majumder", "title": "Some Network Optimization Models under Diverse Uncertain Environments", "comments": "Thesis document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Network models provide an efficient way to represent many real life problems\nmathematically. In the last few decades, the field of network optimization has\nwitnessed an upsurge of interest among researchers and practitioners. The\nnetwork models considered in this thesis are broadly classified into four types\nincluding transportation problem, shortest path problem, minimum spanning tree\nproblem and maximum flow problem. Quite often, we come across situations, when\nthe decision parameters of network optimization problems are not precise and\ncharacterized by various forms of uncertainties arising from the factors, like\ninsufficient or incomplete data, lack of evidence, inappropriate judgements and\nrandomness. Considering the deterministic environment, there exist several\nstudies on network optimization problems. However, in the literature, not many\ninvestigations on single and multi objective network optimization problems are\nobserved under diverse uncertain frameworks. This thesis proposes seven\ndifferent network models under different uncertain paradigms. Here, the\nuncertain programming techniques used to formulate the uncertain network models\nare (i) expected value model, (ii) chance constrained model and (iii) dependent\nchance constrained model. Subsequently, the corresponding crisp equivalents of\nthe uncertain network models are solved using different solution methodologies.\nThe solution methodologies used in this thesis can be broadly categorized as\nclassical methods and evolutionary algorithms. The classical methods, used in\nthis thesis, are Dijkstra and Kruskal algorithms, modified rough Dijkstra\nalgorithm, global criterion method, epsilon constraint method and fuzzy\nprogramming method. Whereas, among the evolutionary algorithms, we have\nproposed the varying population genetic algorithm with indeterminate crossover\nand considered two multi objective evolutionary algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 13:48:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Majumder", "Saibal", ""]]}, {"id": "2103.08389", "submitter": "Nuno Louren\\c{c}o", "authors": "Jessica M\\'egane, Nuno Louren\\c{c}o, Penousal Machado", "title": "Probabilistic Grammatical Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical Evolution (GE) is one of the most popular Genetic Programming\n(GP) variants, and it has been used with success in several problem domains.\nSince the original proposal, many enhancements have been proposed to GE in\norder to address some of its main issues and improve its performance.\n  In this paper we propose Probabilistic Grammatical Evolution (PGE), which\nintroduces a new genotypic representation and new mapping mechanism for GE.\nSpecifically, we resort to a Probabilistic Context-Free Grammar (PCFG) where\nits probabilities are adapted during the evolutionary process, taking into\naccount the productions chosen to construct the fittest individual. The\ngenotype is a list of real values, where each value represents the likelihood\nof selecting a derivation rule. We evaluate the performance of PGE in two\nregression problems and compare it with GE and Structured Grammatical Evolution\n(SGE).\n  The results show that PGE has a a better performance than GE, with\nstatistically significant differences, and achieved similar performance when\ncomparing with SGE.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:54:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["M\u00e9gane", "Jessica", ""], ["Louren\u00e7o", "Nuno", ""], ["Machado", "Penousal", ""]]}, {"id": "2103.08406", "submitter": "Lance Williams", "authors": "Lance R. Williams", "title": "Increased Complexity and Fitness of Artificial Cells that Reproduce\n  Using Spatially Distributed Asynchronous Parallel Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication time is among the most important components of a bacterial cell's\nreproductive fitness. Paradoxically, larger cells replicate in less time than\nsmaller cells despite the fact that building a larger cell requires increased\nquantities of raw materials and energy. This feat is primarily accomplished by\nthe massive over expression of ribosomes, which permits translation of mRNA\ninto protein, the limiting step in reproduction, to occur at a scale that would\nbe impossible were it not for the use of parallel processing. In computer\nscience, spatial parallelism is the distribution of work across the nodes of a\ndistributed-memory multicomputer system. Despite the fact that a non-negligible\nfraction of artificial life research is grounded in formulations based on\nspatially parallel substrates, there have been no examples of artificial\norganisms that use spatial parallelism to replicate in less time than smaller\norganisms. This paper describes artificial cells defined using a\ncombinator-based artificial chemistry that replicate in less time than smaller\ncells. This is achieved by employing extra copies of programs implementing the\nlimiting steps in the process used by the cells to synthesize their component\nparts. Significant speedup is demonstrated, despite the increased complexity of\ncontrol and export processes necessitated by the use of a parallel replication\nstrategy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:33:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Williams", "Lance R.", ""]]}, {"id": "2103.08408", "submitter": "Mital Raithatha", "authors": "Mital Raithatha, Aizaz U. Chaudhry, Roshdy H.M. Hafez, John W.\n  Chinneck", "title": "A Fast Heuristic for Gateway Location in Wireless Backhaul of 5G\n  Ultra-Dense Networks", "comments": "Accepted Journal paper in IEEE access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 5G Ultra-Dense Networks, a distributed wireless backhaul is an attractive\nsolution for forwarding traffic to the core. The macro-cell coverage area is\ndivided into many small cells. A few of these cells are designated as gateways\nand are linked to the core by high-capacity fiber optic links. Each small cell\nis associated with one gateway and all small cells forward their traffic to\ntheir respective gateway through multi-hop mesh networks. We investigate the\ngateway location problem and show that finding near-optimal gateway locations\nimproves the backhaul network capacity. An exact p-median integer linear\nprogram is formulated for comparison with our novel K-GA heuristic that\ncombines a Genetic Algorithm (GA) with K-means clustering to find near-optimal\ngateway locations. We compare the performance of KGA with six other approaches\nin terms of average number of hops and backhaul network capacity at different\nnode densities through extensive Monte Carlo simulations. All approaches are\ntested in various user distribution scenarios, including uniform distribution,\nbivariate Gaussian distribution, and cluster distribution. In all cases K-GA\nprovides near-optimal results, achieving average number of hops and backhaul\nnetwork capacity within 2% of optimal while saving an average of 95% of the\nexecution time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:34:49 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Raithatha", "Mital", ""], ["Chaudhry", "Aizaz U.", ""], ["Hafez", "Roshdy H. M.", ""], ["Chinneck", "John W.", ""]]}, {"id": "2103.08663", "submitter": "Jim C. Visschers", "authors": "Jim C. Visschers, Dmitry Budker, Lykourgos Bougas", "title": "Rapid parameter estimation of discrete decaying signals using\n  autoencoder networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we demonstrate the use of neural networks for rapid extraction\nof signal parameters of discretely sampled signals. In particular, we use dense\nautoencoder networks to extract the parameters of interest from exponentially\ndecaying signals and decaying oscillations. By using a three-stage training\nmethod and careful choice of the neural network size, we are able to retrieve\nthe relevant signal parameters directly from the latent space of the\nautoencoder network at significantly improved rates compared to traditional\nalgorithmic signal-analysis approaches. We show that the achievable precision\nand accuracy of this method of analysis is similar to conventional\nalgorithm-based signal analysis methods, by demonstrating that the extracted\nsignal parameters are approaching their fundamental parameter estimation limit\nas provided by the Cram\\'er-Rao bound. Furthermore, we demonstrate that\nautoencoder networks are able to achieve signal analysis, and, hence, parameter\nextraction, at rates of 75 kHz, orders-of-magnitude faster than conventional\ntechniques with similar precision. Finally, we explore the limitations of our\napproach, demonstrating that analysis rates of $>$200 kHz are feasible with\nfurther optimization of the transfer rate between the data-acquisition system\nand data-analysis system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:56:53 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 11:02:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Visschers", "Jim C.", ""], ["Budker", "Dmitry", ""], ["Bougas", "Lykourgos", ""]]}, {"id": "2103.08668", "submitter": "Xun Jiao", "authors": "Dongning Ma, Jianmin Guo, Yu Jiang, Xun Jiao", "title": "HDTest: Differential Fuzz Testing of Brain-Inspired Hyperdimensional\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-inspired hyperdimensional computing (HDC) is an emerging computational\nparadigm that mimics brain cognition and leverages hyperdimensional vectors\nwith fully distributed holographic representation and (pseudo)randomness.\nCompared to other machine learning (ML) methods such as deep neural networks\n(DNNs), HDC offers several advantages including high energy efficiency, low\nlatency, and one-shot learning, making it a promising alternative candidate on\na wide range of applications. However, the reliability and robustness of HDC\nmodels have not been explored yet. In this paper, we design, implement, and\nevaluate HDTest to test HDC model by automatically exposing unexpected or\nincorrect behaviors under rare inputs. The core idea of HDTest is based on\nguided differential fuzz testing. Guided by the distance between query\nhypervector and reference hypervector in HDC, HDTest continuously mutates\noriginal inputs to generate new inputs that can trigger incorrect behaviors of\nHDC model. Compared to traditional ML testing methods, HDTest does not need to\nmanually label the original input. Using handwritten digit classification as an\nexample, we show that HDTest can generate thousands of adversarial inputs with\nnegligible perturbations that can successfully fool HDC models. On average,\nHDTest can generate around 400 adversarial inputs within one minute running on\na commodity computer. Finally, by using the HDTest-generated inputs to retrain\nHDC models, we can strengthen the robustness of HDC models. To the best of our\nknowledge, this paper presents the first effort in systematically testing this\nemerging brain-inspired computational model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 19:23:45 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ma", "Dongning", ""], ["Guo", "Jianmin", ""], ["Jiang", "Yu", ""], ["Jiao", "Xun", ""]]}, {"id": "2103.08878", "submitter": "Gabriel Silva", "authors": "Vivek Kurien George, Vikash Morar, Weiwei Yang, Jonathan Larson, Bryan\n  Tower, Shweti Mahajan, Arkin Gupta, Christopher White, Gabriel A. Silva", "title": "Learning without gradient descent encoded by the dynamics of a\n  neurobiological model", "comments": "Version 2 includes a new subsection 4.1 and associated table and\n  figure benchmarking our biologically-inspired neural network against a\n  traditional ANN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of state-of-the-art machine learning is essentially all based on\ndifferent variations of gradient descent algorithms that minimize some version\nof a cost or loss function. A fundamental limitation, however, is the need to\ntrain these systems in either supervised or unsupervised ways by exposing them\nto typically large numbers of training examples. Here, we introduce a\nfundamentally novel conceptual approach to machine learning that takes\nadvantage of a neurobiologically derived model of dynamic signaling,\nconstrained by the geometric structure of a network. We show that MNIST images\ncan be uniquely encoded and classified by the dynamics of geometric networks\nwith nearly state-of-the-art accuracy in an unsupervised way, and without the\nneed for any training.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:03:04 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 20:55:19 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["George", "Vivek Kurien", ""], ["Morar", "Vikash", ""], ["Yang", "Weiwei", ""], ["Larson", "Jonathan", ""], ["Tower", "Bryan", ""], ["Mahajan", "Shweti", ""], ["Gupta", "Arkin", ""], ["White", "Christopher", ""], ["Silva", "Gabriel A.", ""]]}, {"id": "2103.08953", "submitter": "Jeremie Laydevant", "authors": "J\\'er\\'emie Laydevant, Maxence Ernoult, Damien Querlioz, Julie\n  Grollier", "title": "Training Dynamical Binary Neural Networks with Equilibrium Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equilibrium Propagation (EP) is an algorithm intrinsically adapted to the\ntraining of physical networks, thanks to the local updates of weights given by\nthe internal dynamics of the system. However, the construction of such a\nhardware requires to make the algorithm compatible with existing neuromorphic\nCMOS technologies, which generally exploit digital communication between\nneurons and offer a limited amount of local memory. In this work, we\ndemonstrate that EP can train dynamical networks with binary activations and\nweights. We first train systems with binary weights and full-precision\nactivations, achieving an accuracy equivalent to that of full-precision models\ntrained by standard EP on MNIST, and losing only 1.9% accuracy on CIFAR-10 with\nequal architecture. We then extend our method to the training of models with\nbinary activations and weights on MNIST, achieving an accuracy within 1% of the\nfull-precision reference for fully connected architectures and reaching the\nfull-precision accuracy for convolutional architectures. Our extension of EP to\nbinary networks opens new solutions for on-chip learning and provides a compact\nframework for training BNNs end-to-end with the same circuitry as for\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:24:13 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 12:23:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Laydevant", "J\u00e9r\u00e9mie", ""], ["Ernoult", "Maxence", ""], ["Querlioz", "Damien", ""], ["Grollier", "Julie", ""]]}, {"id": "2103.09353", "submitter": "Alexander Edwards", "authors": "Alexander J. Edwards, Dhritiman Bhattacharya, Peng Zhou, Nathan R.\n  McDonald, Lisa Loomis, Clare D. Thiem, Jayasimha Atulasimha, Joseph S.\n  Friedman", "title": "Frustrated Arrays of Nanomagnets for Efficient Reservoir Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We simulated our nanomagnet reservoir computer (NMRC) design on benchmark\ntasks, demonstrating NMRC's high memory content and expressibility. In support\nof the feasibility of this method, we fabricated a frustrated nanomagnet\nreservoir layer. Using this structure, we describe a low-power, low-area system\nwith an area-energy-delay product $10^7$ lower than conventional RC systems,\nthat is therefore promising for size, weight, and power (SWaP) constrained\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 22:32:49 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Edwards", "Alexander J.", ""], ["Bhattacharya", "Dhritiman", ""], ["Zhou", "Peng", ""], ["McDonald", "Nathan R.", ""], ["Loomis", "Lisa", ""], ["Thiem", "Clare D.", ""], ["Atulasimha", "Jayasimha", ""], ["Friedman", "Joseph S.", ""]]}, {"id": "2103.09560", "submitter": "Fernando Martin-Rodriguez Ph.D.", "authors": "Fernando Martin-Rodriguez", "title": "Big Plastic Masses Detection using Sentinel 2 Images", "comments": "3 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This communication describes a preliminary research on detection of big\nmasses of plastic (marine litter) on the oceans and seas using EO (Earth\nObservation) satellite systems. Free images from the Sentinel 2 (Copernicus\nProject) platform are used. To develop a plastic recognizer, we start with an\nimage where we can find a big accumulation of \"nonfloating\" plastic: Almer\\'ia\ngreenhouses. We made a test using remote sensing differential indexes, but we\ngot much better results using all available wavelengths (thirteen frequency\nbands) and applying Neural Networks to that feature vector.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:45:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Martin-Rodriguez", "Fernando", ""]]}, {"id": "2103.09593", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty", "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots", "comments": "To be presented at NAACL-HLT 2021. Abstract also published in the\n  Rising Stars Track of the Workshop on Computational Approaches to Linguistic\n  Code-Switching (CALCS 2021)", "journal-ref": "2021.naacl-main.282", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual models have demonstrated impressive cross-lingual transfer\nperformance. However, test sets like XNLI are monolingual at the example level.\nIn multilingual communities, it is common for polyglots to code-mix when\nconversing with each other. Inspired by this phenomenon, we present two strong\nblack-box adversarial attacks (one word-level, one phrase-level) for\nmultilingual models that push their ability to handle code-mixed sentences to\nthe limit. The former uses bilingual dictionaries to propose perturbations and\ntranslations of the clean example for sense disambiguation. The latter directly\naligns the clean example with its translations before extracting phrases as\nperturbations. Our phrase-level attack has a success rate of 89.75% against\nXLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.\nFinally, we propose an efficient adversarial training scheme that trains in the\nsame number of steps as the original model and show that it improves model\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:20:53 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:30:27 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 02:02:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""]]}, {"id": "2103.10293", "submitter": "Or Sharir", "authors": "Or Sharir, Amnon Shashua and Giuseppe Carleo", "title": "Neural tensor contractions and the expressive power of deep neural\n  quantum states", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a direct connection between general tensor networks and deep\nfeed-forward artificial neural networks. The core of our results is the\nconstruction of neural-network layers that efficiently perform tensor\ncontractions, and that use commonly adopted non-linear activation functions.\nThe resulting deep networks feature a number of edges that closely matches the\ncontraction complexity of the tensor networks to be approximated. In the\ncontext of many-body quantum states, this result establishes that\nneural-network states have strictly the same or higher expressive power than\npractically usable variational tensor networks. As an example, we show that all\nmatrix product states can be efficiently written as neural-network states with\na number of edges polynomial in the bond dimension and depth logarithmic in the\nsystem size. The opposite instead does not hold true, and our results imply\nthat there exist quantum states that are not efficiently expressible in terms\nof matrix product states or practically usable PEPS, but that are instead\nefficiently expressible with neural network states.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:47:38 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 08:34:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sharir", "Or", ""], ["Shashua", "Amnon", ""], ["Carleo", "Giuseppe", ""]]}, {"id": "2103.10394", "submitter": "Dogan Corus", "authors": "Dogan Corus and Andrei Lissovoi and Pietro S. Oliveto and Carsten Witt", "title": "On Steady-State Evolutionary Algorithms and Selective Pressure: Why\n  Inverse Rank-Based Allocation of Reproductive Trials is Best", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the impact of the selective pressure for the global optimisation\ncapabilities of steady-state EAs. For the standard bimodal benchmark function\n\\twomax we rigorously prove that using uniform parent selection leads to\nexponential runtimes with high probability to locate both optima for the\nstandard ($\\mu$+1)~EA and ($\\mu$+1)~RLS with any polynomial population sizes.\nOn the other hand, we prove that selecting the worst individual as parent leads\nto efficient global optimisation with overwhelming probability for reasonable\npopulation sizes. Since always selecting the worst individual may have\ndetrimental effects for escaping from local optima, we consider the performance\nof stochastic parent selection operators with low selective pressure for a\nfunction class called \\textsc{TruncatedTwoMax} where one slope is shorter than\nthe other. An experimental analysis shows that the EAs equipped with inverse\ntournament selection, where the loser is selected for reproduction and small\ntournament sizes, globally optimise \\textsc{TwoMax} efficiently and effectively\nescape from local optima of \\textsc{TruncatedTwoMax} with high probability.\nThus they identify both optima efficiently while uniform (or stronger)\nselection fails in theory and in practice. We then show the power of inverse\nselection on function classes from the literature where populations are\nessential by providing rigorous proofs or experimental evidence that it\noutperforms uniform selection equipped with or without a restart strategy. We\nconclude the paper by confirming our theoretical insights with an empirical\nanalysis of the different selective pressures on standard benchmarks of the\nclassical MaxSat and Multidimensional Knapsack Problems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:27:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Corus", "Dogan", ""], ["Lissovoi", "Andrei", ""], ["Oliveto", "Pietro S.", ""], ["Witt", "Carsten", ""]]}, {"id": "2103.10592", "submitter": "Chankyu Lee", "authors": "Chankyu Lee, Adarsh Kumar Kosta and Kaushik Roy", "title": "Fusion-FlowNet: Energy-Efficient Optical Flow Estimation using Sensor\n  Fusion and Deep Fused Spiking-Analog Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard frame-based cameras that sample light intensity frames are heavily\nimpacted by motion blur for high-speed motion and fail to perceive scene\naccurately when the dynamic range is high. Event-based cameras, on the other\nhand, overcome these limitations by asynchronously detecting the variation in\nindividual pixel intensities. However, event cameras only provide information\nabout pixels in motion, leading to sparse data. Hence, estimating the overall\ndense behavior of pixels is difficult. To address such issues associated with\nthe sensors, we present Fusion-FlowNet, a sensor fusion framework for\nenergy-efficient optical flow estimation using both frame- and event-based\nsensors, leveraging their complementary characteristics. Our proposed network\narchitecture is also a fusion of Spiking Neural Networks (SNNs) and Analog\nNeural Networks (ANNs) where each network is designed to simultaneously process\nasynchronous event streams and regular frame-based images, respectively. Our\nnetwork is end-to-end trained using unsupervised learning to avoid expensive\nvideo annotations. The method generalizes well across distinct environments\n(rapid motion and challenging lighting conditions) and demonstrates\nstate-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event\nCamera (MVSEC) dataset. Furthermore, our network offers substantial savings in\nterms of the number of network parameters and computational energy cost.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:03:33 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lee", "Chankyu", ""], ["Kosta", "Adarsh Kumar", ""], ["Roy", "Kaushik", ""]]}, {"id": "2103.10736", "submitter": "Daniel Santiago Cuervo G\\'omez", "authors": "Santiago Cuervo, Miguel Melgarejo, Angie Blanco-Ca\\~non, Laura\n  Reyes-Fajardo, Sergio Rojas-Galeano", "title": "PAMELI: A Meta-Algorithm for Computationally Expensive Multi-Objective\n  Optimization Problems", "comments": "Submitted to IEEE Transactions on Evolutionary Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for multi-objective optimization of computationally\nexpensive problems. The proposed algorithm is based on solving a set of\nsurrogate problems defined by models of the real one, so that only solutions\nestimated to be approximately Pareto-optimal are evaluated using the real\nexpensive functions. Aside of the search for solutions, our algorithm also\nperforms a meta-search for optimal surrogate models and navigation strategies\nfor the optimization landscape, therefore adapting the search strategy for\nsolutions to the problem as new information about it is obtained. The\ncompetitiveness of our approach is demonstrated by an experimental comparison\nwith one state-of-the-art surrogate-assisted evolutionary algorithm on a set of\nbenchmark problems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:18:03 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 12:08:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cuervo", "Santiago", ""], ["Melgarejo", "Miguel", ""], ["Blanco-Ca\u00f1on", "Angie", ""], ["Reyes-Fajardo", "Laura", ""], ["Rojas-Galeano", "Sergio", ""]]}, {"id": "2103.10790", "submitter": "Adam Katona", "authors": "Adam Katona, Daniel W. Franks, James Alfred Walker", "title": "Quality Evolvability ES: Evolving Individuals With a Distribution of\n  Well Performing and Diverse Offspring", "comments": "2021 Conference on Artificial Life", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important lessons from the success of deep learning is that\nlearned representations tend to perform much better at any task compared to\nrepresentations we design by hand. Yet evolution of evolvability algorithms,\nwhich aim to automatically learn good genetic representations, have received\nrelatively little attention, perhaps because of the large amount of\ncomputational power they require. The recent method Evolvability ES allows\ndirect selection for evolvability with little computation. However, it can only\nbe used to solve problems where evolvability and task performance are aligned.\nWe propose Quality Evolvability ES, a method that simultaneously optimizes for\ntask performance and evolvability and without this restriction. Our proposed\napproach Quality Evolvability has similar motivation to Quality Diversity\nalgorithms, but with some important differences. While Quality Diversity aims\nto find an archive of diverse and well-performing, but potentially genetically\ndistant individuals, Quality Evolvability aims to find a single individual with\na diverse and well-performing distribution of offspring. By doing so Quality\nEvolvability is forced to discover more evolvable representations. We\ndemonstrate on robotic locomotion control tasks that Quality Evolvability ES,\nsimilarly to Quality Diversity methods, can learn faster than objective-based\nmethods and can handle deceptive problems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:22:22 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 11:28:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Katona", "Adam", ""], ["Franks", "Daniel W.", ""], ["Walker", "James Alfred", ""]]}, {"id": "2103.11154", "submitter": "Tao Li", "authors": "Tao Li, Lei Tan, Qinghua Tao, Yipeng Liu, Xiaolin Huang", "title": "Train Deep Neural Networks in 40-D Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are massive parameters in deep neural networks, the training\ncan actually proceed in a rather low-dimensional space. By investigating such\nlow-dimensional properties of the training trajectory, we propose a Dynamic\nLinear Dimensionality Reduction (DLDR), which dramatically reduces the\nparameter space to a variable subspace of significantly lower dimension. Since\nthere are only a few variables to optimize, second-order methods become\napplicable. Following this idea, we develop a quasi-Newton-based algorithm to\ntrain these variables obtained by DLDR, rather than the original parameters of\nneural networks. The experimental results strongly support the dimensionality\nreduction performance: for many standard neural networks, optimizing over only\n40 variables, one can achieve comparable performance against the regular\ntraining over thousands or even millions of parameters.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 10:48:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Tao", ""], ["Tan", "Lei", ""], ["Tao", "Qinghua", ""], ["Liu", "Yipeng", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2103.11177", "submitter": "Xianqi Chen", "authors": "Xianqi Chen (1 and 2), Xiaoyu Zhao (2), Zhiqiang Gong (2), Jun Zhang\n  (2), Weien Zhou (2), Xiaoqian Chen (2), Wen Yao (2) ((1) College of Aerospace\n  Science and Engineering, National University of Defense Technology, (2)\n  National Innovation Institute of Defense Technology, Chinese Academy of\n  Military Science)", "title": "A Deep Neural Network Surrogate Modeling Benchmark for Temperature Field\n  Prediction of Heat Source Layout", "comments": "31 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thermal issue is of great importance during layout design of heat source\ncomponents in systems engineering, especially for high functional-density\nproducts. Thermal analysis generally needs complex simulation, which leads to\nan unaffordable computational burden to layout optimization as it iteratively\nevaluates different schemes. Surrogate modeling is an effective way to\nalleviate computation complexity. However, temperature field prediction (TFP)\nwith complex heat source layout (HSL) input is an ultra-high dimensional\nnonlinear regression problem, which brings great difficulty to traditional\nregression models. The Deep neural network (DNN) regression method is a\nfeasible way for its good approximation performance. However, it faces great\nchallenges in both data preparation for sample diversity and uniformity in the\nlayout space with physical constraints, and proper DNN model selection and\ntraining for good generality, which necessitates efforts of both layout\ndesigner and DNN experts. To advance this cross-domain research, this paper\nproposes a DNN based HSL-TFP surrogate modeling task benchmark. With\nconsideration for engineering applicability, sample generation, dataset\nevaluation, DNN model, and surrogate performance metrics, are thoroughly\nstudied. Experiments are conducted with ten representative state-of-the-art DNN\nmodels. Detailed discussion on baseline results is provided and future\nprospects are analyzed for DNN based HSL-TFP tasks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 13:26:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Xianqi", "", "1 and 2"], ["Zhao", "Xiaoyu", ""], ["Gong", "Zhiqiang", ""], ["Zhang", "Jun", ""], ["Zhou", "Weien", ""], ["Chen", "Xiaoqian", ""], ["Yao", "Wen", ""]]}, {"id": "2103.11388", "submitter": "Antonios Liapis", "authors": "Konstantinos Sfikas and Antonios Liapis", "title": "Collaborative Agent Gameplay in the Pandemic Board Game", "comments": "11 pages", "journal-ref": "Proceedings of the Foundations of Digital Games Conference, 2020", "doi": "10.1145/3402942.3402943", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While artificial intelligence has been applied to control players' decisions\nin board games for over half a century, little attention is given to games with\nno player competition. Pandemic is an exemplar collaborative board game where\nall players coordinate to overcome challenges posed by events occurring during\nthe game's progression. This paper proposes an artificial agent which controls\nall players' actions and balances chances of winning versus risk of losing in\nthis highly stochastic environment. The agent applies a Rolling Horizon\nEvolutionary Algorithm on an abstraction of the game-state that lowers the\nbranching factor and simulates the game's stochasticity. Results show that the\nproposed algorithm can find winning strategies more consistently in different\ngames of varying difficulty. The impact of a number of state evaluation metrics\nis explored, balancing between optimistic strategies that favor winning and\npessimistic strategies that guard against losing.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:18:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sfikas", "Konstantinos", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.11614", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en and Jessica McBroom and Bryn Jeffries and Irena\n  Koprinska and Kalina Yacef", "title": "ast2vec: Utilizing Recursive Neural Encodings of Python Programs", "comments": "Under consideration at the Journal of Educational Datamining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Educational datamining involves the application of datamining techniques to\nstudent activity. However, in the context of computer programming, many\ndatamining techniques can not be applied because they expect vector-shaped\ninput whereas computer programs have the form of syntax trees. In this paper,\nwe present ast2vec, a neural network that maps Python syntax trees to vectors\nand back, thereby facilitating datamining on computer programs as well as the\ninterpretation of datamining results. Ast2vec has been trained on almost half a\nmillion programs of novice programmers and is designed to be applied across\nlearning tasks without re-training, meaning that users can apply it without any\nneed for (additional) deep learning. We demonstrate the generality of ast2vec\nin three settings: First, we provide example analyses using ast2vec on a\nclassroom-sized dataset, involving visualization, student motion analysis,\nclustering, and outlier detection, including two novel analyses, namely a\nprogress-variance-projection and a dynamical systems analysis. Second, we\nconsider the ability of ast2vec to recover the original syntax tree from its\nvector representation on the training data and two further large-scale\nprogramming datasets. Finally, we evaluate the predictive capability of a\nsimple linear regression on top of ast2vec, obtaining similar results to\ntechniques that work directly on syntax trees. We hope ast2vec can augment the\neducational datamining toolbelt by making analyses of computer programs easier,\nricher, and more efficient.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 06:53:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""], ["McBroom", "Jessica", ""], ["Jeffries", "Bryn", ""], ["Koprinska", "Irena", ""], ["Yacef", "Kalina", ""]]}, {"id": "2103.11715", "submitter": "Antonios Liapis", "authors": "Antonios Liapis, Hector P. Martinez, Julian Togelius and Georgios N.\n  Yannakakis", "title": "Transforming Exploratory Creativity with DeLeNoX", "comments": "8 pages", "journal-ref": "Proceedings of the Fourth International Conference on\n  Computational Creativity, 2013, pages 56-63", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeLeNoX (Deep Learning Novelty Explorer), a system that\nautonomously creates artifacts in constrained spaces according to its own\nevolving interestingness criterion. DeLeNoX proceeds in alternating phases of\nexploration and transformation. In the exploration phases, a version of novelty\nsearch augmented with constraint handling searches for maximally diverse\nartifacts using a given distance function. In the transformation phases, a deep\nlearning autoencoder learns to compress the variation between the found\nartifacts into a lower-dimensional space. The newly trained encoder is then\nused as the basis for a new distance function, transforming the criteria for\nthe next exploration phase. In the current paper, we apply DeLeNoX to the\ncreation of spaceships suitable for use in two-dimensional arcade-style\ncomputer games, a representative problem in procedural content generation in\ngames. We also situate DeLeNoX in relation to the distinction between\nexploratory and transformational creativity, and in relation to Schmidhuber's\ntheory of creativity through the drive for compression progress.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:39:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liapis", "Antonios", ""], ["Martinez", "Hector P.", ""], ["Togelius", "Julian", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2103.11726", "submitter": "Antonios Liapis", "authors": "Panagiotis Migkotzidis and Antonios Liapis", "title": "SuSketch: Surrogate Models of Gameplay as a Design Assistant", "comments": "To be published in IEEE Transactions on Games, 11 pages", "journal-ref": null, "doi": "10.1109/TG.2021.3068360", "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SuSketch, a design tool for first person shooter\nlevels. SuSketch provides the designer with gameplay predictions for two\ncompeting players of specific character classes. The interface allows the\ndesigner to work side-by-side with an artificially intelligent creator and to\nreceive varied types of feedback such as path information, predicted balance\nbetween players in a complete playthrough, or a predicted heatmap of the\nlocations of player deaths. The system also proactively designs alternatives to\nthe level and class pairing, and presents them to the designer as suggestions\nthat improve the predicted balance of the game. SuSketch offers a new way of\nintegrating machine learning into mixed-initiative co-creation tools, as a\nsurrogate of human play trained on a large corpus of artificial playtraces. A\nuser study with 16 game developers indicated that the tool was easy to use, but\nalso highlighted a need to make SuSketch more accessible and more explainable.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:05:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Migkotzidis", "Panagiotis", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.11746", "submitter": "Michael Lones", "authors": "Michael A. Lones", "title": "Evolving Continuous Optimisers from Scratch", "comments": "arXiv admin note: text overlap with arXiv:1910.00945", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work uses genetic programming to explore the space of continuous\noptimisers, with the goal of discovering novel ways of doing optimisation. In\norder to keep the search space broad, the optimisers are evolved from scratch\nusing Push, a Turing-complete, general-purpose, language. The resulting\noptimisers are found to be diverse, and explore their optimisation landscapes\nusing a variety of interesting, and sometimes unusual, strategies.\nSignificantly, when applied to problems that were not seen during training,\nmany of the evolved optimisers generalise well, and often outperform existing\noptimisers. This supports the idea that novel and effective forms of\noptimisation can be discovered in an automated manner. This paper also shows\nthat pools of evolved optimisers can be hybridised to further increase their\ngenerality, leading to optimisers that perform robustly over a broad variety of\nproblem types and sizes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:54:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lones", "Michael A.", ""]]}, {"id": "2103.12184", "submitter": "Jan Prosi", "authors": "Jan Prosi, Sina Khajehabdollahi, Emmanouil Giannakakis, Georg Martius\n  and Anna Levina", "title": "The dynamical regime and its importance for evolvability, task\n  performance and generalization", "comments": "8 Pages, 7 Figures, Artificial Life Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.MA nlin.AO q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has long been hypothesized that operating close to the critical state is\nbeneficial for natural and artificial systems. We test this hypothesis by\nevolving foraging agents controlled by neural networks that can change the\nsystem's dynamical regime throughout evolution. Surprisingly, we find that all\npopulations, regardless of their initial regime, evolve to be subcritical in\nsimple tasks and even strongly subcritical populations can reach comparable\nperformance. We hypothesize that the moderately subcritical regime combines the\nbenefits of generalizability and adaptability brought by closeness to\ncriticality with the stability of the dynamics characteristic for subcritical\nsystems. By a resilience analysis, we find that initially critical agents\nmaintain their fitness level even under environmental changes and degrade\nslowly with increasing perturbation strength. On the other hand, subcritical\nagents originally evolved to the same fitness, were often rendered utterly\ninadequate and degraded faster. We conclude that although the subcritical\nregime is preferable for a simple task, the optimal deviation from criticality\ndepends on the task difficulty: for harder tasks, agents evolve closer to\ncriticality. Furthermore, subcritical populations cannot find the path to\ndecrease their distance to criticality. In summary, our study suggests that\ninitializing models near criticality is important to find an optimal and\nflexible solution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 21:22:52 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Prosi", "Jan", ""], ["Khajehabdollahi", "Sina", ""], ["Giannakakis", "Emmanouil", ""], ["Martius", "Georg", ""], ["Levina", "Anna", ""]]}, {"id": "2103.12231", "submitter": "Anup Das", "authors": "Twisha Titirsha, Shihao Song, Adarsha Balaji, Anup Das", "title": "On the Role of System Software in Energy Management of Neuromorphic\n  Computing", "comments": "To appear in 18th Computer Frontiers 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing systems such as DYNAPs and Loihi have recently been\nintroduced to the computing community to improve performance and energy\nefficiency of machine learning programs, especially those that are implemented\nusing Spiking Neural Network (SNN). The role of a system software for\nneuromorphic systems is to cluster a large machine learning model (e.g., with\nmany neurons and synapses) and map these clusters to the computing resources of\nthe hardware. In this work, we formulate the energy consumption of a\nneuromorphic hardware, considering the power consumed by neurons and synapses,\nand the energy consumed in communicating spikes on the interconnect. Based on\nsuch formulation, we first evaluate the role of a system software in managing\nthe energy consumption of neuromorphic systems. Next, we formulate a simple\nheuristic-based mapping approach to place the neurons and synapses onto the\ncomputing resources to reduce energy consumption. We evaluate our approach with\n10 machine learning applications and demonstrate that the proposed mapping\napproach leads to a significant reduction of energy consumption of neuromorphic\ncomputing systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:31:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Titirsha", "Twisha", ""], ["Song", "Shihao", ""], ["Balaji", "Adarsha", ""], ["Das", "Anup", ""]]}, {"id": "2103.12476", "submitter": "Philipp Andelfinger", "authors": "Philipp Andelfinger", "title": "Differentiable Agent-Based Simulation for Gradient-Guided\n  Simulation-Based Optimization", "comments": "Accepted at the 2021 ACM SIGSIM Conference Conference on Principles\n  of Advanced Discrete Simulation (PADS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based optimization using agent-based models is typically carried\nout under the assumption that the gradient describing the sensitivity of the\nsimulation output to the input cannot be evaluated directly. To still apply\ngradient-based optimization methods, which efficiently steer the optimization\ntowards a local optimum, gradient estimation methods can be employed. However,\nmany simulation runs are needed to obtain accurate estimates if the input\ndimension is large. Automatic differentiation (AD) is a family of techniques to\ncompute gradients of general programs directly. Here, we explore the use of AD\nin the context of time-driven agent-based simulations. By substituting common\ndiscrete model elements such as conditional branching with smooth\napproximations, we obtain gradient information across discontinuities in the\nmodel logic. On the example of microscopic traffic models and an epidemics\nmodel, we study the fidelity and overhead of the differentiable models, as well\nas the convergence speed and solution quality achieved by gradient-based\noptimization compared to gradient-free methods. In traffic signal timing\noptimization problems with high input dimension, the gradient-based methods\nexhibit substantially superior performance. Finally, we demonstrate that the\napproach enables gradient-based training of neural network-controlled\nsimulation entities embedded in the model logic.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:58:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Andelfinger", "Philipp", ""]]}, {"id": "2103.12544", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Anupam Biswas and Sabuzima Nayak", "title": "deepBF: Malicious URL detection using Learned Bloom Filter and\n  Evolutionary Deep Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malicious URL detection is an emerging research area due to continuous\nmodernization of various systems, for instance, Edge Computing. In this\narticle, we present a novel malicious URL detection technique, called deepBF\n(deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we\npropose a learned Bloom Filter using 2-dimensional Bloom Filter. We\nexperimentally decide the best non-cryptography string hash function. Then, we\nderive a modified non-cryptography string hash function from the selected hash\nfunction for deepBF by introducing biases in the hashing method and compared\namong the string hash functions. The modified string hash function is compared\nto other variants of diverse non-cryptography string hash functions. It is also\ncompared with various filters, particularly, counting Bloom Filter, Kirsch\n\\textit{et al.}, and Cuckoo Filter using various use cases. The use cases\nunearth weakness and strength of the filters. Secondly, we propose a malicious\nURL detection mechanism using deepBF. We apply the evolutionary convolutional\nneural network to identify the malicious URLs. The evolutionary convolutional\nneural network is trained and tested with malicious URL datasets. The output is\ntested in deepBF for accuracy. We have achieved many conclusions from our\nexperimental evaluation and results and are able to reach various conclusive\ndecisions which are presented in the article.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:53:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Patgiri", "Ripon", ""], ["Biswas", "Anupam", ""], ["Nayak", "Sabuzima", ""]]}, {"id": "2103.12564", "submitter": "Huy Nguyen BSc", "authors": "Huy Le Nguyen, Dominique Chu", "title": "Linear Constraints Learning for Spiking Neurons", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Encoding information with precise spike timings using spike-coded neurons has\nbeen shown to be more computationally powerful than rate-coded approaches.\nHowever, most existing supervised learning algorithms for spiking neurons are\ncomplicated and offer poor time complexity. To address these limitations, we\npropose a supervised multi-spike learning algorithm which reduces the required\nnumber of training iterations. We achieve this by formulating a large number of\nweight updates as a linear constraint satisfaction problem, which can be solved\nefficiently. Experimental results show this method offers better efficiency\ncompared to existing algorithms on the MNIST dataset. Additionally, we provide\nexperimental results on the classification capacity of the LIF neuron model,\nrelative to several parameters of the system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:54:05 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Nguyen", "Huy Le", ""], ["Chu", "Dominique", ""]]}, {"id": "2103.12593", "submitter": "Bojian Yin", "authors": "Bojian Yin, Federico Corradi, Sander M. Bohte", "title": "Accurate and efficient time-domain classification with adaptive spiking\n  recurrent neural networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Inspired by more detailed modeling of biological neurons, Spiking neural\nnetworks (SNNs) have been investigated both as more biologically plausible and\npotentially more powerful models of neural computation, and also with the aim\nof extracting biological neurons' energy efficiency; the performance of such\nnetworks however has remained lacking compared to classical artificial neural\nnetworks (ANNs). Here, we demonstrate how a novel surrogate gradient combined\nwith recurrent networks of tunable and adaptive spiking neurons yields\nstate-of-the-art for SNNs on challenging benchmarks in the time-domain, like\nspeech and gesture recognition. This also exceeds the performance of standard\nclassical recurrent neural networks (RNNs) and approaches that of the best\nmodern ANNs. As these SNNs exhibit sparse spiking, we show that they\ntheoretically are one to three orders of magnitude more computationally\nefficient compared to RNNs with comparable performance. Together, this\npositions SNNs as an attractive solution for AI hardware implementations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:27:29 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Yin", "Bojian", ""], ["Corradi", "Federico", ""], ["Bohte", "Sander M.", ""]]}, {"id": "2103.12623", "submitter": "Pedro Filipe Gomes Ramos de Carvalho", "authors": "Pedro Carvalho, Nuno Louren\\c{c}o, Penousal Machado", "title": "Evolving Learning Rate Optimizers for Deep Neural Networks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) became popular due to their successful\napplication difficult problems such image and speech recognition. However, when\npractitioners want to design an ANN they need to undergo laborious process of\nselecting a set of parameters and topology. Currently, there are several\nstate-of-the art methods that allow for the automatic selection of some of\nthese aspects. Learning Rate optimizers are a set of such techniques that\nsearch for good values of learning rates. Whilst these techniques are effective\nand have yielded good results over the years, they are general solution i.e.\nthey do not consider the characteristics of a specific network.\n  We propose a framework called AutoLR to automatically design Learning Rate\nOptimizers. Two versions of the system are detailed. The first one, Dynamic\nAutoLR, evolves static and dynamic learning rate optimizers based on the\ncurrent epoch and the previous learning rate. The second version, Adaptive\nAutoLR, evolves adaptive optimizers that can fine tune the learning rate for\neach network eeight which makes them generally more effective. The results are\ncompetitive with the best state of the art methods, even outperforming them in\nsome scenarios. Furthermore, the system evolved a classifier, ADES, that\nappears to be novel and innovative since, to the best of our knowledge, it has\na structure that differs from state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:23:57 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Carvalho", "Pedro", ""], ["Louren\u00e7o", "Nuno", ""], ["Machado", "Penousal", ""]]}, {"id": "2103.12633", "submitter": "Sean-Kelly Palicki", "authors": "Sean-Kelly Palicki, R. Muhammad Atif Azad", "title": "GA-SVM for Evaluating Heroin Consumption Risk", "comments": "Genetic Algorithm, Feature Selection, Applied Computing, Psychology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There were over 70,000 drug overdose deaths in the USA in 2017. Almost half\nof those involved the use of Opioids such as Heroin. This research supports\nefforts to combat the Opioid Epidemic by further understanding factors that\nlead to Heroin consumption. Previous research has debated the cause of Heroin\naddiction, with some explaining the phenomenon as a transition from\nprescription Opioids, and others pointing to various psycho-social factors.\nThis research used self-reported information about personality, demographics\nand drug consumption behavior to predict Heroin consumption. By applying a\nSupport Vector Machine algorithm optimized with a Genetic Algorithm (GA-SVM\nHybrid) to simultaneously identify predictive features and model parameters,\nthis research produced several models that were more accurate in predicting\nHeroin use than those produced in previous studies. Although all factors had\npredictive power, these results showed that consumption of other drugs (both\nprescription and illicit) were stronger predictors of Heroin use than\npsycho-social factors. The use of prescription drugs as a strong predictor of\nHeroin use is an important though disturbing discovery but that can help combat\nHeroin use.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:41:30 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Palicki", "Sean-Kelly", ""], ["Azad", "R. Muhammad Atif", ""]]}, {"id": "2103.13080", "submitter": "Luo Chunjie", "authors": "Chunjie Luo, Jianfeng Zhan, Tianshu Hao, Lei Wang, Wanling Gao", "title": "Shift-and-Balance Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is an effective mechanism to improve the deep model capability.\nSqueeze-and-Excite (SE) introduces a light-weight attention branch to enhance\nthe network's representational power. The attention branch is gated using the\nSigmoid function and multiplied by the feature map's trunk branch. It is too\nsensitive to coordinate and balance the trunk and attention branches'\ncontributions. To control the attention branch's influence, we propose a new\nattention method, called Shift-and-Balance (SB). Different from\nSqueeze-and-Excite, the attention branch is regulated by the learned control\nfactor to control the balance, then added into the feature map's trunk branch.\nExperiments show that Shift-and-Balance attention significantly improves the\naccuracy compared to Squeeze-and-Excite when applied in more layers, increasing\nmore size and capacity of a network. Moreover, Shift-and-Balance attention\nachieves better or close accuracy compared to the state-of-art Dynamic\nConvolution.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:54:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""], ["Hao", "Tianshu", ""], ["Wang", "Lei", ""], ["Gao", "Wanling", ""]]}, {"id": "2103.13302", "submitter": "Sourav De", "authors": "Sourav De, Bo-Han Qiu, Wei-Xuan Bu, Md.Aftab Baig, Chung-Jun Su,\n  Yao-Jen Lee, and Darsen Lu", "title": "Neuromorphic Computing with Deeply Scaled Ferroelectric FinFET in\n  Presence of Process Variation, Device Aging and Flicker Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports a comprehensive study on the applicability of ultra-scaled\nferroelectric FinFETs with 6 nm thick hafnium zirconium oxide layer for\nneuromorphic computing in the presence of process variation, flicker noise, and\ndevice aging. An intricate study has been conducted about the impact of such\nvariations on the inference accuracy of pre-trained neural networks consisting\nof analog, quaternary (2-bit/cell) and binary synapse. A pre-trained neural\nnetwork with 97.5% inference accuracy on the MNIST dataset has been adopted as\nthe baseline. Process variation, flicker noise, and device aging\ncharacterization have been performed and a statistical model has been developed\nto capture all these effects during neural network simulation. Extrapolated\nretention above 10 years have been achieved for binary read-out procedure. We\nhave demonstrated that the impact of (1) retention degradation due to the oxide\nthickness scaling, (2) process variation, and (3) flicker noise can be abated\nin ferroelectric FinFET based binary neural networks, which exhibits superior\nperformance over quaternary and analog neural network, amidst all variations.\nThe performance of a neural network is the result of coalesced performance of\ndevice, architecture and algorithm. This research corroborates the\napplicability of deeply scaled ferroelectric FinFETs for non-von Neumann\ncomputing with proper combination of architecture and algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:24:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["De", "Sourav", ""], ["Qiu", "Bo-Han", ""], ["Bu", "Wei-Xuan", ""], ["Baig", "Md. Aftab", ""], ["Su", "Chung-Jun", ""], ["Lee", "Yao-Jen", ""], ["Lu", "Darsen", ""]]}, {"id": "2103.13567", "submitter": "Yiwen Guo", "authors": "Zhi Wang, Yiwen Guo, Wangmeng Zuo", "title": "Deepfake Forensics via An Adversarial Game", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the progress in AI-based facial forgery (i.e., deepfake), people are\nincreasingly concerned about its abuse. Albeit effort has been made for\ntraining classification (also known as deepfake detection) models to recognize\nsuch forgeries, existing models suffer from poor generalization to unseen\nforgery technologies and high sensitivity to changes in image/video quality. In\nthis paper, we advocate adversarial training for improving the generalization\nability to both unseen facial forgeries and unseen image/video qualities. We\nbelieve training with samples that are adversarially crafted to attack the\nclassification models improves the generalization ability considerably.\nConsidering that AI-based face manipulation often leads to high-frequency\nartifacts that can be easily spotted by models yet difficult to generalize, we\nfurther propose a new adversarial training method that attempts to blur out\nthese specific artifacts, by introducing pixel-wise Gaussian blurring models.\nWith adversarial training, the classification models are forced to learn more\ndiscriminative and generalizable features, and the effectiveness of our method\ncan be verified by plenty of empirical evidence. Our code will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 02:20:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wang", "Zhi", ""], ["Guo", "Yiwen", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2103.13578", "submitter": "Wentao Zhu", "authors": "Wentao Zhu and Yufang Huang and Daguang Xu and Zhen Qian and Wei Fan\n  and Xiaohui Xie", "title": "Test-Time Training for Deformable Multi-Scale Image Registration", "comments": "ICRA 2021; 8 pages, 4 figures, 2 big tables", "journal-ref": "ICRA 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Registration is a fundamental task in medical robotics and is often a crucial\nstep for many downstream tasks such as motion analysis, intra-operative\ntracking and image segmentation. Popular registration methods such as ANTs and\nNiftyReg optimize objective functions for each pair of images from scratch,\nwhich are time-consuming for 3D and sequential images with complex\ndeformations. Recently, deep learning-based registration approaches such as\nVoxelMorph have been emerging and achieve competitive performance. In this\nwork, we construct a test-time training for deep deformable image registration\nto improve the generalization ability of conventional learning-based\nregistration model. We design multi-scale deep networks to consecutively model\nthe residual deformations, which is effective for high variational\ndeformations. Extensive experiments validate the effectiveness of multi-scale\ndeep registration with test-time training based on Dice coefficient for image\nsegmentation and mean square error (MSE), normalized local cross-correlation\n(NLCC) for tissue dense tracking tasks. Two videos are in\nhttps://www.youtube.com/watch?v=NvLrCaqCiAE and\nhttps://www.youtube.com/watch?v=pEA6ZmtTNuQ\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:22:59 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhu", "Wentao", ""], ["Huang", "Yufang", ""], ["Xu", "Daguang", ""], ["Qian", "Zhen", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2103.13598", "submitter": "Yiwen Guo", "authors": "Yiwen Guo, Changshui Zhang", "title": "Recent Advances in Large Margin Learning", "comments": "Accepted by TPAMI, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper serves as a survey of recent advances in large margin training and\nits theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs)\nthat are probably the most prominent machine learning models for large-scale\ndata in the community over the past decade. We generalize the formulation of\nclassification margins from classical research to latest DNNs, summarize\ntheoretical connections between the margin, network generalization, and\nrobustness, and introduce recent efforts in enlarging the margins for DNNs\ncomprehensively. Since the viewpoint of different methods is discrepant, we\ncategorize them into groups for ease of comparison and discussion in the paper.\nHopefully, our discussions and overview inspire new research work in the\ncommunity that aim to improve the performance of DNNs, and we also point to\ndirections where the large margin principle can be verified to provide\ntheoretical evidence why certain regularizations for DNNs function well in\npractice. We managed to shorten the paper such that the crucial spirit of large\nmargin learning and related methods are better emphasized.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:12:00 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:41:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Guo", "Yiwen", ""], ["Zhang", "Changshui", ""]]}, {"id": "2103.13861", "submitter": "Briti Gangopadhyay", "authors": "Briti Gangopadhyay, Harshit Soora, Pallab Dasgupta", "title": "Hierarchical Program-Triggered Reinforcement Learning Agents For\n  Automated Driving", "comments": "The paper is under consideration in Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3096998", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in Reinforcement Learning (RL) combined with Deep Learning\n(DL) have demonstrated impressive performance in complex tasks, including\nautonomous driving. The use of RL agents in autonomous driving leads to a\nsmooth human-like driving experience, but the limited interpretability of Deep\nReinforcement Learning (DRL) creates a verification and certification\nbottleneck. Instead of relying on RL agents to learn complex tasks, we propose\nHPRL - Hierarchical Program-triggered Reinforcement Learning, which uses a\nhierarchy consisting of a structured program along with multiple RL agents,\neach trained to perform a relatively simple task. The focus of verification\nshifts to the master program under simple guarantees from the RL agents,\nleading to a significantly more interpretable and verifiable implementation as\ncompared to a complex RL agent. The evaluation of the framework is demonstrated\non different driving tasks, and NHTSA precrash scenarios using CARLA, an\nopen-source dynamic urban simulation environment.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:19:54 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Gangopadhyay", "Briti", ""], ["Soora", "Harshit", ""], ["Dasgupta", "Pallab", ""]]}, {"id": "2103.14372", "submitter": "Aymeric Vie", "authors": "Aymeric Vie", "title": "A Genetic Algorithm approach to Asymmetrical Blotto Games with\n  Heterogeneous Valuations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blotto Games are a popular model of multi-dimensional strategic resource\nallocation. Two players allocate resources in different battlefields in an\nauction setting. While competition with equal budgets is well understood,\nlittle is known about strategic behavior under asymmetry of resources. We\nintroduce a genetic algorithm, a search heuristic inspired from biological\nevolution, interpreted as social learning, to solve this problem. Most\nperformant strategies are combined to create more performant strategies.\nMutations allow the algorithm to efficiently scan the space of possible\nstrategies, and consider a wide diversity of deviations. We show that our\ngenetic algorithm converges to the analytical Nash equilibrium of the symmetric\nBlotto game. We present the solution concept it provides for asymmetrical\nBlotto games. It notably sees the emergence of \"guerilla warfare\" strategies,\nconsistent with empirical and experimental findings. The player with less\nresources learns to concentrate its resources to compensate for the asymmetry\nof competition. When players value battlefields heterogeneously, counter\nstrategies and bidding focus is obtained in equilibrium. These features are\nconsistent with empirical and experimental findings, and provide a learning\nfoundation for their existence.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:20:19 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Vie", "Aymeric", ""]]}, {"id": "2103.14379", "submitter": "Aymeric Vie", "authors": "Aymeric Vie", "title": "Evolutionary Strategies with Analogy Partitions in p-guessing Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.NE q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Keynesian Beauty Contests notably modeled by p-guessing games, players try\nto guess the average of guesses multiplied by p. Convergence of plays to Nash\nequilibrium has often been justified by agents' learning. However,\ninterrogations remain on the origin of reasoning types and equilibrium behavior\nwhen learning takes place in unstable environments. When successive values of p\ncan take values above and below 1, bounded rational agents may learn about\ntheir environment through simplified representations of the game, reasoning\nwith analogies and constructing expectations about the behavior of other\nplayers. We introduce an evolutionary process of learning to investigate the\ndynamics of learning and the resulting optimal strategies in unstable\np-guessing games environments with analogy partitions. As a validation of the\napproach, we first show that our genetic algorithm behaves consistently with\nprevious results in persistent environments, converging to the Nash\nequilibrium. We characterize strategic behavior in mixed regimes with unstable\nvalues of p. Varying the number of iterations given to the genetic algorithm to\nlearn about the game replicates the behavior of agents with different levels of\nreasoning of the level k approach. This evolutionary process hence proposes a\nlearning foundation for endogenizing existence and transitions between levels\nof reasoning in cognitive hierarchy models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:28:23 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Vie", "Aymeric", ""]]}, {"id": "2103.14493", "submitter": "Tian Huang", "authors": "Tian Huang, Tao Luo, Ming Yan, Joey Tianyi Zhou, Rick Goh", "title": "RCT: Resource Constrained Training for Edge AI", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks training on edge terminals is essential for edge AI\ncomputing, which needs to be adaptive to evolving environment. Quantised models\ncan efficiently run on edge devices, but existing training methods for these\ncompact models are designed to run on powerful servers with abundant memory and\nenergy budget. For example, quantisation-aware training (QAT) method involves\ntwo copies of model parameters, which is usually beyond the capacity of on-chip\nmemory in edge devices. Data movement between off-chip and on-chip memory is\nenergy demanding as well. The resource requirements are trivial for powerful\nservers, but critical for edge devices. To mitigate these issues, We propose\nResource Constrained Training (RCT). RCT only keeps a quantised model\nthroughout the training, so that the memory requirements for model parameters\nin training is reduced. It adjusts per-layer bitwidth dynamically in order to\nsave energy when a model can learn effectively with lower precision. We carry\nout experiments with representative models and tasks in image application and\nnatural language processing. Experiments show that RCT saves more than 86\\%\nenergy for General Matrix Multiply (GEMM) and saves more than 46\\% memory for\nmodel parameters, with limited accuracy loss. Comparing with QAT-based method,\nRCT saves about half of energy on moving model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:33:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Tian", ""], ["Luo", "Tao", ""], ["Yan", "Ming", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick", ""]]}, {"id": "2103.14624", "submitter": "Javier Del Ser Dr.", "authors": "Javier Del Ser, David Casillas-Perez, Laura Cornejo-Bueno, Luis\n  Prieto-Godino, Julia Sanz-Justo, Carlos Casanova-Mateo, Sancho Salcedo-Sanz", "title": "Randomization-based Machine Learning in Renewable Energy Prediction\n  Problems: Critical Literature Review, New Results and Perspectives", "comments": "88 pages, 14 figures, 12 tables. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization-based Machine Learning methods for prediction are currently a\nhot topic in Artificial Intelligence, due to their excellent performance in\nmany prediction problems, with a bounded computation time. The application of\nrandomization-based approaches to renewable energy prediction problems has been\nmassive in the last few years, including many different types of\nrandomization-based approaches, their hybridization with other techniques and\nalso the description of new versions of classical randomization-based\nalgorithms, including deep and ensemble approaches. In this paper we review the\nmost important characteristics of randomization-based machine learning\napproaches and their application to renewable energy prediction problems. We\ndescribe the most important methods and algorithms of this family of modeling\nmethods, and perform a critical literature review, examining prediction\nproblems related to solar, wind, marine/ocean and hydro-power renewable\nsources. We support our critical analysis with an extensive experimental study,\ncomprising real-world problems related to solar, wind and hydro-power energy,\nwhere randomization-based algorithms are found to achieve superior results at a\nsignificantly lower computational cost than other modeling counterparts. We end\nour survey with a prospect of the most important challenges and research\ndirections that remain open this field, along with an outlook motivating\nfurther research efforts in this exciting research field.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:38:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Del Ser", "Javier", ""], ["Casillas-Perez", "David", ""], ["Cornejo-Bueno", "Laura", ""], ["Prieto-Godino", "Luis", ""], ["Sanz-Justo", "Julia", ""], ["Casanova-Mateo", "Carlos", ""], ["Salcedo-Sanz", "Sancho", ""]]}, {"id": "2103.14633", "submitter": "Michael S. Ryoo", "authors": "Iretiayo Akinola, Anelia Angelova, Yao Lu, Yevgen Chebotar, Dmitry\n  Kalashnikov, Jacob Varley, Julian Ibarz, Michael S. Ryoo", "title": "Visionary: Vision architecture discovery for robot learning", "comments": null, "journal-ref": "ICRA 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vision-based architecture search algorithm for robot\nmanipulation learning, which discovers interactions between low dimension\naction inputs and high dimensional visual inputs. Our approach automatically\ndesigns architectures while training on the task - discovering novel ways of\ncombining and attending image feature representations with actions as well as\nfeatures from previous layers. The obtained new architectures demonstrate\nbetter task success rates, in some cases with a large margin, compared to a\nrecent high performing baseline. Our real robot experiments also confirm that\nit improves grasping performance by 6%. This is the first approach to\ndemonstrate a successful neural architecture search and attention connectivity\nsearch for a real-robot task.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:51:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Akinola", "Iretiayo", ""], ["Angelova", "Anelia", ""], ["Lu", "Yao", ""], ["Chebotar", "Yevgen", ""], ["Kalashnikov", "Dmitry", ""], ["Varley", "Jacob", ""], ["Ibarz", "Julian", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2103.14833", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, D. S. Dumina, N. A. Semenov", "title": "Determination of weight coefficients for additive fitness function of\n  genetic algorithm", "comments": "9 pages, in Russian", "journal-ref": "Software & Systems 2020, vol. 33, no. 1", "doi": "10.15827/0236-235X.129.047-053", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The paper presents a solution for the problem of choosing a method for\nanalytical determining of weight factors for a genetic algorithm additive\nfitness function. This algorithm is the basis for an evolutionary process,\nwhich forms a stable and effective query population in a search engine to\nobtain highly relevant results. The paper gives a formal description of an\nalgorithm fitness function, which is a weighted sum of three heterogeneous\ncriteria. The selected methods for analytical determining of weight factors are\ndescribed in detail. It is noted that expert assessment methods are impossible\nto use. The authors present a research methodology using the experimental\nresults from earlier in the discussed project \"Data Warehouse Support on the\nBase Intellectual Web Crawler and Evolutionary Model for Target Information\nSelection\". There is a description of an initial dataset with data ranges for\ncalculating weights. The calculation order is illustrated by examples. The\nresearch results in graphical form demonstrate the fitness function behavior\nduring the genetic algorithm operation using various weighting options.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:38:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ivanov", "V. K.", ""], ["Dumina", "D. S.", ""], ["Semenov", "N. A.", ""]]}, {"id": "2103.14886", "submitter": "Jenia Jitsev", "authors": "Marcel Aach, Jens Henrik Goebbert, Jenia Jitsev", "title": "Generalization over different cellular automata rules learned by a deep\n  feed-forward neural network", "comments": "Preprint. In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To test generalization ability of a class of deep neural networks, we\nrandomly generate a large number of different rule sets for 2-D cellular\nautomata (CA), based on John Conway's Game of Life. Using these rules, we\ncompute several trajectories for each CA instance. A deep convolutional\nencoder-decoder network with short and long range skip connections is trained\non various generated CA trajectories to predict the next CA state given its\nprevious states. Results show that the network is able to learn the rules of\nvarious, complex cellular automata and generalize to unseen configurations. To\nsome extent, the network shows generalization to rule sets and neighborhood\nsizes that were not seen during the training at all.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:12:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Aach", "Marcel", ""], ["Goebbert", "Jens Henrik", ""], ["Jitsev", "Jenia", ""]]}, {"id": "2103.15090", "submitter": "Antonios Liapis", "authors": "Konstantinos Sfikas and Antonios Liapis", "title": "Playing Against the Board: Rolling Horizon Evolutionary Algorithms\n  Against Pandemic", "comments": "Accepted to IEEE Transactions on Games, 11 pages, 7 figures. arXiv\n  admin note: text overlap with arXiv:2103.11388", "journal-ref": null, "doi": "10.1109/TG.2021.3069766", "report-no": null, "categories": "cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive board games have provided a rich and diverse testbed for\nartificial intelligence. This paper contends that collaborative board games\npose a different challenge to artificial intelligence as it must balance\nshort-term risk mitigation with long-term winning strategies. Collaborative\nboard games task all players to coordinate their different powers or pool their\nresources to overcome an escalating challenge posed by the board and a\nstochastic ruleset. This paper focuses on the exemplary collaborative board\ngame Pandemic and presents a rolling horizon evolutionary algorithm designed\nspecifically for this game. The complex way in which the Pandemic game state\nchanges in a stochastic but predictable way required a number of specially\ndesigned forward models, macro-action representations for decision-making, and\nrepair functions for the genetic operations of the evolutionary algorithm.\nVariants of the algorithm which explore optimistic versus pessimistic game\nstate evaluations, different mutation rates and event horizons are compared\nagainst a baseline hierarchical policy agent. Results show that an evolutionary\napproach via short-horizon rollouts can better account for the future dangers\nthat the board may introduce, and guard against them. Results highlight the\ntypes of challenges that collaborative board games pose to artificial\nintelligence, especially for handling multi-player collaboration interactions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:22:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sfikas", "Konstantinos", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.15138", "submitter": "Sarah Hamilton", "authors": "William Herzberg, Daniel B. Rowe, Andreas Hauptmann, and Sarah J.\n  Hamilton", "title": "Graph Convolutional Networks for Model-Based Learning in Nonlinear\n  Inverse Problems", "comments": "9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.NA cs.NE math.NA math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of model-based learned image reconstruction methods in medical\nimaging have been limited to uniform domains, such as pixelated images. If the\nunderlying model is solved on nonuniform meshes, arising from a finite element\nmethod typical for nonlinear inverse problems, interpolation and embeddings are\nneeded. To overcome this, we present a flexible framework to extend model-based\nlearning directly to nonuniform meshes, by interpreting the mesh as a graph and\nformulating our network architectures using graph convolutional neural\nnetworks. This gives rise to the proposed iterative Graph Convolutional\nNewton-type Method (GCNM), which includes the forward model in the solution of\nthe inverse problem, while all updates are directly computed by the network on\nthe problem specific mesh. We present results for Electrical Impedance\nTomography, a severely ill-posed nonlinear inverse problem that is frequently\nsolved via optimization-based methods, where the forward problem is solved by\nfinite element methods. Results for absolute EIT imaging are compared to\nstandard iterative methods as well as a graph residual network. We show that\nthe GCNM has strong generalizability to different domain shapes and meshes, out\nof distribution data as well as experimental data, from purely simulated\ntraining data and without transfer training.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 14:19:56 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 19:23:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Herzberg", "William", ""], ["Rowe", "Daniel B.", ""], ["Hauptmann", "Andreas", ""], ["Hamilton", "Sarah J.", ""]]}, {"id": "2103.15203", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Timothy Davis, Vijay Gadepally, Hayden Jananthan,\n  Lauren Milechin", "title": "Mathematics of Digital Hyperspace", "comments": "9 pages, 8 figures, 2 tables, accepted to GrAPL 2021. arXiv admin\n  note: text overlap with arXiv:1807.03165, arXiv:2004.01181, arXiv:1909.05631,\n  arXiv:1708.02937", "journal-ref": null, "doi": "10.1109/IPDPSW52791.2021.00048", "report-no": null, "categories": "cs.MS cs.DB cs.DM cs.NE math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media, e-commerce, streaming video, e-mail, cloud documents, web\npages, traffic flows, and network packets fill vast digital lakes, rivers, and\noceans that we each navigate daily. This digital hyperspace is an amorphous\nflow of data supported by continuous streams that stretch standard concepts of\ntype and dimension. The unstructured data of digital hyperspace can be\nelegantly represented, traversed, and transformed via the mathematics of\nhypergraphs, hypersparse matrices, and associative array algebra. This paper\nexplores a novel mathematical concept, the semilink, that combines pairs of\nsemirings to provide the essential operations for graph analytics, database\noperations, and machine learning. The GraphBLAS standard currently supports\nhypergraphs, hypersparse matrices, the mathematics required for semilinks, and\nseamlessly performs graph, network, and matrix operations. With the addition of\nkey based indices (such as pointers to strings) and semilinks, GraphBLAS can\nbecome a richer associative array algebra and be a plug-in replacement for\nspreadsheets, database tables, and data centric operating systems, enhancing\nthe navigation of unstructured data found in digital hyperspace.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 19:11:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kepner", "Jeremy", ""], ["Davis", "Timothy", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Milechin", "Lauren", ""]]}, {"id": "2103.15413", "submitter": "Toni Schneidereit", "authors": "Toni Schneidereit and Michael Breu{\\ss}", "title": "Collocation Polynomial Neural Forms and Domain Fragmentation for Initial\n  Value Problems", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several neural network approaches for solving differential equations employ\ntrial solutions with a feedforward neural network. There are different means to\nincorporate the trial solution in the construction, for instance one may\ninclude them directly in the cost function. Used within the corresponding\nneural network, the trial solutions define the so-called neural form. Such\nneural forms represent general, flexible tools by which one may solve various\ndifferential equations. In this article we consider time-dependent initial\nvalue problems, which require to set up the neural form framework adequately.\nThe neural forms presented up to now in the literature for such a setting can\nbe considered as first order polynomials. In this work we propose to extend the\npolynomial order of the neural forms. The novel collocation-type construction\nincludes several feedforward neural networks, one for each order. Additionally,\nwe propose the fragmentation of the computational domain into subdomains. The\nneural forms are solved on each subdomain, whereas the interfacing grid points\noverlap in order to provide initial values over the whole fragmentation. We\nillustrate in experiments that the combination of collocation neural forms of\nhigher order and the domain fragmentation allows to solve initial value\nproblems over large domains with high accuracy and reliability.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:19:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schneidereit", "Toni", ""], ["Breu\u00df", "Michael", ""]]}, {"id": "2103.15451", "submitter": "Antonios Liapis", "authors": "Daniel Karavolos, Antonios Liapis and Georgios N. Yannakakis", "title": "Pairing Character Classes in a Deathmatch Shooter Game via a\n  Deep-Learning Surrogate Model", "comments": "Proceedings of the FDG Workshop on Procedural Content Generation,\n  2018, 10 pages", "journal-ref": null, "doi": "10.1145/3235765.3235816", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a surrogate model of gameplay that learns the mapping\nbetween different game facets, and applies it to a generative system which\ndesigns new content in one of these facets. Focusing on the shooter game genre,\nthe paper explores how deep learning can help build a model which combines the\ngame level structure and the game's character class parameters as input and the\ngameplay outcomes as output. The model is trained on a large corpus of game\ndata from simulations with artificial agents in random sets of levels and class\nparameters. The model is then used to generate classes for specific levels and\nfor a desired game outcome, such as balanced matches of short duration.\nFindings in this paper show that the system can be expressive and can generate\nclasses for both computer generated and human authored levels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:34:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Karavolos", "Daniel", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2103.15546", "submitter": "Richard Allmendinger", "authors": "Richard Allmendinger and Joshua Knowles", "title": "Heterogeneous Objectives: State-of-the-Art and Future Research", "comments": "20 pages, submitted for consideration to the MACODA book project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiobjective optimization problems with heterogeneous objectives are\ndefined as those that possess significantly different types of objective\nfunction components (not just incommensurable in units or scale). For example,\nin a heterogeneous problem the objective function components may differ in\nformal computational complexity, practical evaluation effort (time, costs, or\nresources), determinism (stochastic vs deterministic), or some combination of\nall three. A particularly challenging variety of heterogeneity may occur by the\ncombination of a time-consuming laboratory-based objective with other\nobjectives that are evaluated using faster computer-based calculations. Perhaps\nmore commonly, all objectives may be evaluated computationally, but some may\nrequire a lengthy simulation process while others are computed from a\nrelatively simple closed-form calculation. In this chapter, we motivate the\nneed for more work on the topic of heterogeneous objectives (with reference to\nreal-world examples), expand on a basic taxonomy of heterogeneity types, and\nreview the state of the art in tackling these problems. We give special\nattention to heterogeneity in evaluation time (latency) as this requires\nsophisticated approaches. We also present original experimental work on\nestimating the amount of heterogeneity in evaluation time expected in\nmany-objective problems, given reasonable assumptions, and survey related\nresearch threads that could contribute to this area in future.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:30:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Allmendinger", "Richard", ""], ["Knowles", "Joshua", ""]]}, {"id": "2103.15547", "submitter": "Amir Mosavi Prof", "authors": "Hossein Moayedi, Amir Mosavi", "title": "Analyzing Uniaxial Compressive Strength of Concrete Using a Novel Satin\n  Bowerbird Optimizer", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": "10.31219/osf.io/5qmt7", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surmounting the complexities in analyzing the mechanical parameters of\nconcrete entails selecting an appropriate methodology. This study integrates an\nartificial neural network (ANN) with a novel metaheuristic technique, namely\nsatin bowerbird optimizer (SBO) for predicting uniaxial compressive strength\n(UCS) of concrete. For this purpose, the created hybrid is trained and tested\nusing a relatively large dataset collected from the published literature. Three\nother new algorithms, namely Henry gas solubility optimization (HGSO),\nsunflower optimization (SFO), and vortex search algorithm (VSA) are also used\nas benchmarks. After attaining a proper population size for all algorithms,\nUtilizing various accuracy indicators, it was shown that the proposed ANN-SBO\nnot only can excellently analyze the UCS behavior, but also outperforms all\nthree benchmark hybrids (i.e., ANN-HGSO, ANN-SFO, and ANN-VSA). In the\nprediction phase, the correlation indices of 0.87394, 0.87936, 0.95329, and\n0.95663, as well as mean absolute percentage errors of 15.9719, 15.3845,\n9.4970, and 8.0629%, calculated for the ANN-HGSO, ANN-SFO, ANN-VSA, and\nANN-SBO, respectively, manifested the best prediction performance for the\nproposed model. Also, the ANN-VSA achieved reliable results as well. In short,\nthe ANN-SBO can be used by engineers as an efficient non-destructive method for\npredicting the UCS of concrete.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:07:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Moayedi", "Hossein", ""], ["Mosavi", "Amir", ""]]}, {"id": "2103.15550", "submitter": "Ha Thanh Nguyen", "authors": "Ha-Thanh Nguyen, Le-Minh Nguyen", "title": "SCNN: Swarm Characteristic Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a powerful approach with good performance on many different\ntasks. However, these models often require massive computational resources. It\nis a worrying trend that we increasingly need models that work well on more\ncomplex problems. In this paper, we propose and verify the effectiveness and\nefficiency of SCNN, an innovative neural network inspired by the swarm concept.\nIn addition to introducing the relevant theories, our detailed experiments\nsuggest that fewer parameters may perform better than models with more\nparameters. Besides, our experiments show that SCNN needs less data than\ntraditional models. That could be an essential hint for problems where there is\nnot much data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 01:26:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Nguyen", "Ha-Thanh", ""], ["Nguyen", "Le-Minh", ""]]}, {"id": "2103.15552", "submitter": "Jamie Shelley Mr", "authors": "Jamie Nicholas Shelley, Optishell Consultancy", "title": "Energy Decay Network (EDeN)", "comments": null, "journal-ref": null, "doi": "10.31224/osf.io/dfyzn", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper and accompanying Python and C++ Framework is the product of the\nauthors perceived problems with narrow (Discrimination based) AI. (Artificial\nIntelligence) The Framework attempts to develop a genetic transfer of\nexperience through potential structural expressions using a common\nregulation/exchange value (energy) to create a model whereby neural\narchitecture and all unit processes are co-dependently developed by genetic and\nreal time signal processing influences; successful routes are defined by\nstability of the spike distribution per epoch which is influenced by\ngenetically encoded morphological development biases.These principles are aimed\ntowards creating a diverse and robust network that is capable of adapting to\ngeneral tasks by training within a simulation designed for transfer learning to\nother mediums at scale.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:17:59 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 22:48:26 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shelley", "Jamie Nicholas", ""], ["Consultancy", "Optishell", ""]]}, {"id": "2103.15553", "submitter": "Bradly Alicea", "authors": "Krishna Katyal, Jesse Parent, Bradly Alicea", "title": "Connectionism, Complexity, and Living Systems: a comparison of\n  Artificial and Biological Neural Networks", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Artificial Neural Networks (ANNs) have yielded impressive results in\nthe realm of simulated intelligent behavior, it is important to remember that\nthey are but sparse approximations of Biological Neural Networks (BNNs). We go\nbeyond comparison of ANNs and BNNs to introduce principles from BNNs that might\nguide the further development of ANNs as embodied neural models. These\nprinciples include representational complexity, complex network\nstructure/energetics, and robust function. We then consider these principles in\nways that might be implemented in the future development of ANNs. In\nconclusion, we consider the utility of this comparison, particularly in terms\nof building more robust and dynamic ANNs. This even includes constructing a\nmorphology and sensory apparatus to create an embodied ANN, which when\ncomplemented with the organizational and functional advantages of BNNs unlocks\nthe adaptive potential of lifelike networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 02:52:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Katyal", "Krishna", ""], ["Parent", "Jesse", ""], ["Alicea", "Bradly", ""]]}, {"id": "2103.15586", "submitter": "Valerio Carruba", "authors": "V. Carruba, S. Aljbaae, R. C. Domingos, W. Barletta", "title": "Artificial Neural Network classification of asteroids in the M1:2\n  mean-motion resonance with Mars", "comments": "11 pages, 10 figures, 1 table. Accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab914", "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks (ANN) have been successfully used in the last\nyears to identify patterns in astronomical images. The use of ANN in the field\nof asteroid dynamics has been, however, so far somewhat limited. In this work\nwe used for the first time ANN for the purpose of automatically identifying the\nbehaviour of asteroid orbits affected by the M1:2 mean-motion resonance with\nMars. Our model was able to perform well above 85% levels for identifying\nimages of asteroid resonant arguments in term of standard metrics like\naccuracy, precision and recall, allowing to identify the orbital type of all\nnumbered asteroids in the region. Using supervised machine learning methods,\noptimized through the use of genetic algorithms, we also predicted the orbital\nstatus of all multi-opposition asteroids in the area. We confirm that the M1:2\nresonance mainly affects the orbits of the Massalia, Nysa, and Vesta asteroid\nfamilies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:03:47 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Carruba", "V.", ""], ["Aljbaae", "S.", ""], ["Domingos", "R. C.", ""], ["Barletta", "W.", ""]]}, {"id": "2103.15608", "submitter": "Ajitabh Kumar", "authors": "Ajitabh Kumar", "title": "Hybrid Evolutionary Optimization Approach for Oilfield Well Control\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oilfield production optimization is challenging due to subsurface model\ncomplexity and associated non-linearity, large number of control parameters,\nlarge number of production scenarios, and subsurface uncertainties.\nOptimization involves time-consuming reservoir simulation studies to compare\ndifferent production scenarios and settings. This paper presents efficacy of\ntwo hybrid evolutionary optimization approaches for well control optimization\nof a waterflooding operation, and demonstrates their application using Olympus\nbenchmark. A simpler, weighted sum of cumulative fluid (WCF) is used as\nobjective function first, which is then replaced by net present value (NPV) of\ndiscounted cash-flow for comparison. Two popular evolutionary optimization\nalgorithms, genetic algorithm (GA) and particle swarm optimization (PSO), are\nfirst used in standalone mode to solve well control optimization problem. Next,\nboth GA and PSO methods are used with another popular optimization algorithm,\ncovariance matrix adaptation-evolution strategy (CMA-ES), in hybrid mode.\nHybrid optimization run is made by transferring the resulting population from\none algorithm to the next as its starting population for further improvement.\nApproximately four thousand simulation runs are needed for standalone GA and\nPSO methods to converge, while six thousand runs are needed in case of two\nhybrid optimization modes (GA-CMA-ES and PSO-CMA-ES). To reduce turn-around\ntime, commercial cloud computing is used and simulation workload is distributed\nusing parallel programming. GA and PSO algorithms have a good balance between\nexploratory and exploitative properties, thus are able identify regions of\ninterest. CMA-ES algorithm is able to further refine the solution using its\nexcellent exploitative properties. Thus, GA or PSO with CMA-ES in hybrid mode\nyields better optimization result as compared to standalone GA or PSO\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:36:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kumar", "Ajitabh", ""]]}, {"id": "2103.15624", "submitter": "Gabriel Kronberger", "authors": "Gabriel Kronberger and Fabricio Olivetti de Fran\\c{c}a and Bogdan\n  Burlacu and Christian Haider and Michael Kommenda", "title": "Shape-constrained Symbolic Regression -- Improving Extrapolation with\n  Prior Knowledge", "comments": null, "journal-ref": null, "doi": "10.1162/evco_a_00294", "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the addition of constraints on the function image and its\nderivatives for the incorporation of prior knowledge in symbolic regression.\nThe approach is called shape-constrained symbolic regression and allows us to\nenforce e.g. monotonicity of the function over selected inputs. The aim is to\nfind models which conform to expected behaviour and which have improved\nextrapolation capabilities. We demonstrate the feasibility of the idea and\npropose and compare two evolutionary algorithms for shape-constrained symbolic\nregression: i) an extension of tree-based genetic programming which discards\ninfeasible solutions in the selection step, and ii) a two population\nevolutionary algorithm that separates the feasible from the infeasible\nsolutions. In both algorithms we use interval arithmetic to approximate bounds\nfor models and their partial derivatives. The algorithms are tested on a set of\n19 synthetic and four real-world regression problems. Both algorithms are able\nto identify models which conform to shape constraints which is not the case for\nthe unmodified symbolic regression algorithms. However, the predictive accuracy\nof models with constraints is worse on the training set and the test set.\nShape-constrained polynomial regression produces the best results for the test\nset but also significantly larger models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:04:18 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 09:56:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kronberger", "Gabriel", ""], ["de Fran\u00e7a", "Fabricio Olivetti", ""], ["Burlacu", "Bogdan", ""], ["Haider", "Christian", ""], ["Kommenda", "Michael", ""]]}, {"id": "2103.15692", "submitter": "Samuel Schmidgall", "authors": "Samuel Schmidgall", "title": "Self-Constructing Neural Networks Through Random Mutation", "comments": "Accepted to ICLR 'A Roadmap to Never-Ending RL' (NERL) 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The search for neural architecture is producing many of the most exciting\nresults in artificial intelligence. It has increasingly become apparent that\ntask-specific neural architecture plays a crucial role for effectively solving\nproblems. This paper presents a simple method for learning neural architecture\nthrough random mutation. This method demonstrates 1) neural architecture may be\nlearned during the agent's lifetime, 2) neural architecture may be constructed\nover a single lifetime without any initial connections or neurons, and 3)\narchitectural modifications enable rapid adaptation to dynamic and novel task\nscenarios. Starting without any neurons or connections, this method constructs\na neural architecture capable of high-performance on several tasks. The\nlifelong learning capabilities of this method are demonstrated in an\nenvironment without episodic resets, even learning with constantly changing\nmorphology, limb disablement, and changing task goals all without losing\nlocomotion capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:27:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schmidgall", "Samuel", ""]]}, {"id": "2103.15960", "submitter": "Yannik Stradmann", "authors": "Yannik Stradmann, Sebastian Billaudelle, Oliver Breitwieser, Falk\n  Leonard Ebert, Arne Emmel, Dan Husmann, Joscha Ilmberger, Eric M\\\"uller,\n  Philipp Spilger, Johannes Weis, Johannes Schemmel", "title": "Demonstrating Analog Inference on the BrainScaleS-2 Mobile System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the BrainScaleS-2 mobile system as a compact analog inference\nengine based on the BrainScaleS-2 ASIC and demonstrate its capabilities at\nclassifying a medical electrocardiogram dataset. The analog network core of the\nASIC is utilized to perform the multiply-accumulate operations of a\nconvolutional deep neural network. We measure a total energy consumption of\n192uJ for the ASIC and achieve a classification time of 276us per\nelectrocardiographic patient sample. Patients with atrial fibrillation are\ncorrectly identified with a detection rate of 93.7(7)% at 14.0(10)% false\npositives. The system is directly applicable to edge inference applications due\nto its small size, power envelope and flexible I/O capabilities. Possible\nfuture applications can furthermore combine conventional machine learning\nlayers with online-learning in spiking neural networks on a single\nBrainScaleS-2 ASIC. The system has successfully participated and proven to\noperate reliably in the independently judged competition\n\"Pilotinnovationswettbewerb 'Energieeffizientes KI-System'\" of the German\nFederal Ministry of Education and Research (BMBF).\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:22:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Stradmann", "Yannik", ""], ["Billaudelle", "Sebastian", ""], ["Breitwieser", "Oliver", ""], ["Ebert", "Falk Leonard", ""], ["Emmel", "Arne", ""], ["Husmann", "Dan", ""], ["Ilmberger", "Joscha", ""], ["M\u00fcller", "Eric", ""], ["Spilger", "Philipp", ""], ["Weis", "Johannes", ""], ["Schemmel", "Johannes", ""]]}, {"id": "2103.15985", "submitter": "Youhei Akimoto", "authors": "Youhei Akimoto", "title": "Saddle Point Optimization with Approximate Minimization Oracle", "comments": "Accepted for GECCO 2021", "journal-ref": null, "doi": "10.1145/3449639.3459266", "report-no": null, "categories": "math.OC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major approach to saddle point optimization $\\min_x\\max_y f(x, y)$ is a\ngradient based approach as is popularized by generative adversarial networks\n(GANs). In contrast, we analyze an alternative approach relying only on an\noracle that solves a minimization problem approximately. Our approach locates\napproximate solutions $x'$ and $y'$ to $\\min_{x'}f(x', y)$ and $\\max_{y'}f(x,\ny')$ at a given point $(x, y)$ and updates $(x, y)$ toward these approximate\nsolutions $(x', y')$ with a learning rate $\\eta$. On locally strong\nconvex--concave smooth functions, we derive conditions on $\\eta$ to exhibit\nlinear convergence to a local saddle point, which reveals a possible\nshortcoming of recently developed robust adversarial reinforcement learning\nalgorithms. We develop a heuristic approach to adapt $\\eta$ derivative-free and\nimplement zero-order and first-order minimization algorithms. Numerical\nexperiments are conducted to show the tightness of the theoretical results as\nwell as the usefulness of the $\\eta$ adaptation mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 23:03:24 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 23:30:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Akimoto", "Youhei", ""]]}, {"id": "2103.16836", "submitter": "Hermann Courteille", "authors": "Hermann Courteille (LISTIC), A. Beno\\^it (LISTIC), N M\\'eger (LISTIC),\n  A Atto (LISTIC), D. Ienco (UMR TETIS)", "title": "Channel-Based Attention for LCC Using Sentinel-2 Time Series", "comments": null, "journal-ref": "International Geoscience and Remote Sensing Symposium (IGARSS),\n  Jul 2021, Brussels, Belgium", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are getting increasing attention to deal with\nLand Cover Classification (LCC) relying on Satellite Image Time Series (SITS).\nThough high performances can be achieved, the rationale of a prediction yielded\nby a DNN often remains unclear. An architecture expressing predictions with\nrespect to input channels is thus proposed in this paper. It relies on\nconvolutional layers and an attention mechanism weighting the importance of\neach channel in the final classification decision. The correlation between\nchannels is taken into account to set up shared kernels and lower model\ncomplexity. Experiments based on a Sentinel-2 SITS show promising results.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:24:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Courteille", "Hermann", "", "LISTIC"], ["Beno\u00eet", "A.", "", "LISTIC"], ["M\u00e9ger", "N", "", "LISTIC"], ["Atto", "A", "", "LISTIC"], ["Ienco", "D.", "", "UMR TETIS"]]}, {"id": "2103.16882", "submitter": "Giovanni Iacca Prof.", "authors": "Quintino Francesco Lotito, Leonardo Lucio Custode, Giovanni Iacca", "title": "A Signal-Centric Perspective on the Evolution of Symbolic Communication", "comments": "To be published in the proceedings of ACM Genetic and Evolutionary\n  Computation Conference (GECCO) 2021", "journal-ref": null, "doi": "10.1145/3449639.3459273", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evolution of symbolic communication is a longstanding open research\nquestion in biology. While some theories suggest that it originated from\nsub-symbolic communication (i.e., iconic or indexical), little experimental\nevidence exists on how organisms can actually evolve to define a shared set of\nsymbols with unique interpretable meaning, thus being capable of encoding and\ndecoding discrete information. Here, we use a simple synthetic model composed\nof sender and receiver agents controlled by Continuous-Time Recurrent Neural\nNetworks, which are optimized by means of neuro-evolution. We characterize\nsignal decoding as either regression or classification, with limited and\nunlimited signal amplitude. First, we show how this choice affects the\ncomplexity of the evolutionary search, and leads to different levels of\ngeneralization. We then assess the effect of noise, and test the evolved\nsignaling system in a referential game. In various settings, we observe agents\nevolving to share a dictionary of symbols, with each symbol spontaneously\nassociated to a 1-D unique signal. Finally, we analyze the constellation of\nsignals associated to the evolved signaling systems and note that in most cases\nthese resemble a Pulse Amplitude Modulation system.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:05:01 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Lotito", "Quintino Francesco", ""], ["Custode", "Leonardo Lucio", ""], ["Iacca", "Giovanni", ""]]}, {"id": "2103.16897", "submitter": "Giovanni Iacca Prof.", "authors": "Ahmed Hallawa, Anil Yaman, Giovanni Iacca, Gerd Ascheid", "title": "A Framework for Knowledge Integrated Evolutionary Algorithms", "comments": "Published in: Squillero G., Sim K. (eds) Applications of Evolutionary\n  Computation. EvoApplications 2017. Lecture Notes in Computer Science, vol\n  10199. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-319-55849-3_42", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main reasons for the success of Evolutionary Algorithms (EAs) is\ntheir general-purposeness, i.e., the fact that they can be applied\nstraightforwardly to a broad range of optimization problems, without any\nspecific prior knowledge. On the other hand, it has been shown that\nincorporating a priori knowledge, such as expert knowledge or empirical\nfindings, can significantly improve the performance of an EA. However,\nintegrating knowledge in EAs poses numerous challenges. It is often the case\nthat the features of the search space are unknown, hence any knowledge\nassociated with the search space properties can be hardly used. In addition, a\npriori knowledge is typically problem-specific and hard to generalize. In this\npaper, we propose a framework, called Knowledge Integrated Evolutionary\nAlgorithm (KIEA), which facilitates the integration of existing knowledge into\nEAs. Notably, the KIEA framework is EA-agnostic (i.e., it works with any\nevolutionary algorithm), problem-independent (i.e., it is not dedicated to a\nspecific type of problems), expandable (i.e., its knowledge base can grow over\ntime). Furthermore, the framework integrates knowledge while the EA is running,\nthus optimizing the use of the needed computational power. In the preliminary\nexperiments shown here, we observe that the KIEA framework produces in the\nworst case an 80% improvement on the converge time, w.r.t. the corresponding\n\"knowledge-free\" EA counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:30:11 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hallawa", "Ahmed", ""], ["Yaman", "Anil", ""], ["Iacca", "Giovanni", ""], ["Ascheid", "Gerd", ""]]}, {"id": "2103.17244", "submitter": "Natalia Berloff", "authors": "Nikita Stroev and Natalia G. Berloff", "title": "XY Neural Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mes-hall cond-mat.other cs.ET cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classical XY model is a lattice model of statistical mechanics notable\nfor its universality in the rich hierarchy of the optical, laser and condensed\nmatter systems. We show how to build complex structures for machine learning\nbased on the XY model's nonlinear blocks. The final target is to reproduce the\ndeep learning architectures, which can perform complicated tasks usually\nattributed to such architectures: speech recognition, visual processing, or\nother complex classification types with high quality. We developed the robust\nand transparent approach for the construction of such models, which has\nuniversal applicability (i.e. does not strongly connect to any particular\nphysical system), allows many possible extensions while at the same time\npreserving the simplicity of the methodology.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:47:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Stroev", "Nikita", ""], ["Berloff", "Natalia G.", ""]]}]