[{"id": "2002.00027", "submitter": "Marcos Eduardo Valle", "authors": "Marcos Eduardo Valle and Rodolfo Anibal Lobo", "title": "Hypercomplex-Valued Recurrent Correlation Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.12.034", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent correlation neural networks (RCNNs), introduced by Chiueh and\nGoodman as an improved version of the bipolar correlation-based Hopfield neural\nnetwork, can be used to implement high-capacity associative memories. In this\npaper, we extend the bipolar RCNNs for processing hypercomplex-valued data.\nPrecisely, we present the mathematical background for a broad class of\nhypercomplex-valued RCNNs. Then, we provide the necessary conditions which\nensure that a hypercomplex-valued RCNN always settles at an equilibrium using\neither synchronous or asynchronous update modes. Examples with bipolar,\ncomplex, hyperbolic, quaternion, and octonion-valued RCNNs are given to\nillustrate the theoretical results. Finally, computational experiments confirm\nthe potential application of hypercomplex-valued RCNNs as associative memories\ndesigned for the storage and recall of gray-scale images.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 19:14:19 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Valle", "Marcos Eduardo", ""], ["Lobo", "Rodolfo Anibal", ""]]}, {"id": "2002.00059", "submitter": "Santiago Gonzalez", "authors": "Santiago Gonzalez and Risto Miikkulainen", "title": "Optimizing Loss Functions Through Multivariate Taylor Polynomial\n  Parameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metalearning of deep neural network (DNN) architectures and hyperparameters\nhas become an increasingly important area of research. Loss functions are a\ntype of metaknowledge that is crucial to effective training of DNNs, however,\ntheir potential role in metalearning has not yet been fully explored. Whereas\nearly work focused on genetic programming (GP) on tree representations, this\npaper proposes continuous CMA-ES optimization of multivariate Taylor polynomial\nparameterizations. This approach, TaylorGLO, makes it possible to represent and\nsearch useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN\nbenchmark tasks, TaylorGLO finds new loss functions that outperform functions\npreviously discovered through GP, as well as the standard cross-entropy loss,\nin fewer generations. These functions serve to regularize the learning task by\ndiscouraging overfitting to the labels, which is particularly useful in tasks\nwhere limited training data is available. The results thus demonstrate that\nloss function optimization is a productive new avenue for metalearning.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 21:25:37 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 17:28:04 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 05:44:27 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 05:29:18 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gonzalez", "Santiago", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2002.00519", "submitter": "Ji-Hoon Jeong", "authors": "Ji-Hoon Jeong, Dae-Hyeok Lee, Hyung-Ju Ahn, and Seong-Whan Lee", "title": "Towards Brain-Computer Interfaces for Drone Swarm Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noninvasive brain-computer interface (BCI) decodes brain signals to\nunderstand user intention. Recent advances have been developed for the\nBCI-based drone control system as the demand for drone control increases.\nEspecially, drone swarm control based on brain signals could provide various\nindustries such as military service or industry disaster. This paper presents a\nprototype of a brain swarm interface system for a variety of scenarios using a\nvisual imagery paradigm. We designed the experimental environment that could\nacquire brain signals under a drone swarm control simulator environment.\nThrough the system, we collected the electroencephalogram (EEG) signals with\nrespect to four different scenarios. Seven subjects participated in our\nexperiment and evaluated classification performances using the basic machine\nlearning algorithm. The grand average classification accuracy is higher than\nthe chance level accuracy. Hence, we could confirm the feasibility of the drone\nswarm control system based on EEG signals for performing high-level tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 00:47:50 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Jeong", "Ji-Hoon", ""], ["Lee", "Dae-Hyeok", ""], ["Ahn", "Hyung-Ju", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2002.00539", "submitter": "C.-H. Huck Yang", "authors": "Haoling Zhang, Chao-Han Huck Yang, Hector Zenil, Narsis A. Kiani, Yue\n  Shen, Jesper N. Tegner", "title": "Evolving Neural Networks through a Reverse Encoding Tree", "comments": "Accepted to IEEE Congress on Evolutionary Computation (IEEE CEC)\n  2020. Lecture Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.SY eess.SY q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  NeuroEvolution is one of the most competitive evolutionary learning\nframeworks for designing novel neural networks for use in specific tasks, such\nas logic circuit design and digital gaming. However, the application of\nbenchmark methods such as the NeuroEvolution of Augmenting Topologies (NEAT)\nremains a challenge, in terms of their computational cost and search time\ninefficiency. This paper advances a method which incorporates a type of\ntopological edge coding, named Reverse Encoding Tree (RET), for evolving\nscalable neural networks efficiently. Using RET, two types of approaches --\nNEAT with Binary search encoding (Bi-NEAT) and NEAT with Golden-Section search\nencoding (GS-NEAT) -- have been designed to solve problems in benchmark\ncontinuous learning environments such as logic gates, Cartpole, and Lunar\nLander, and tested against classical NEAT and FS-NEAT as baselines.\nAdditionally, we conduct a robustness test to evaluate the resilience of the\nproposed NEAT algorithms. The results show that the two proposed strategies\ndeliver improved performance, characterized by (1) a higher accumulated reward\nwithin a finite number of time steps; (2) using fewer episodes to solve\nproblems in targeted environments, and (3) maintaining adaptive robustness\nunder noisy perturbations, which outperform the baselines in all tested cases.\nOur analysis also demonstrates that RET expends potential future research\ndirections in dynamic environments. Code is available from\nhttps://github.com/HaolingZHANG/ReverseEncodingTree.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 02:29:51 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 21:00:11 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhang", "Haoling", ""], ["Yang", "Chao-Han Huck", ""], ["Zenil", "Hector", ""], ["Kiani", "Narsis A.", ""], ["Shen", "Yue", ""], ["Tegner", "Jesper N.", ""]]}, {"id": "2002.00544", "submitter": "C.-H. Huck Yang", "authors": "Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco\n  Siniscalchi, Chin-Hui Lee", "title": "Tensor-to-Vector Regression for Multi-channel Speech Enhancement based\n  on Tensor-Train Network", "comments": "Accepted to ICASSP 2020. Update reproducible code", "journal-ref": "IEEE ICASSP 2020", "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a tensor-to-vector regression approach to multi-channel speech\nenhancement in order to address the issue of input size explosion and\nhidden-layer size expansion. The key idea is to cast the conventional deep\nneural network (DNN) based vector-to-vector regression formulation under a\ntensor-train network (TTN) framework. TTN is a recently emerged solution for\ncompact representation of deep models with fully connected hidden layers. Thus\nTTN maintains DNN's expressive power yet involves a much smaller amount of\ntrainable parameters. Furthermore, TTN can handle a multi-dimensional tensor\ninput by design, which exactly matches the desired setting in multi-channel\nspeech enhancement. We first provide a theoretical extension from DNN to TTN\nbased regression. Next, we show that TTN can attain speech enhancement quality\ncomparable with that for DNN but with much fewer parameters, e.g., a reduction\nfrom 27 million to only 5 million parameters is observed in a single-channel\nscenario. TTN also improves PESQ over DNN from 2.86 to 2.96 by slightly\nincreasing the number of trainable parameters. Finally, in 8-channel\nconditions, a PESQ of 3.12 is achieved using 20 million parameters for TTN,\nwhereas a DNN with 68 million parameters can only attain a PESQ of 3.06. Our\nimplementation is available online\nhttps://github.com/uwjunqi/Tensor-Train-Neural-Network.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 02:58:00 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Qi", "Jun", ""], ["Hu", "Hu", ""], ["Wang", "Yannan", ""], ["Yang", "Chao-Han Huck", ""], ["Siniscalchi", "Sabato Marco", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2002.00612", "submitter": "Sheng Xin Zhang", "authors": "Sheng Xin Zhang, Wing Shing Chan, Kit Sang Tang, Shao Yong Zheng", "title": "Adaptive strategy in differential evolution via explicit exploitation\n  and exploration controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When introducing new strategies to the existing one, two key issues should be\naddressed. One is to efficiently distribute computational resources so that the\nappropriate strategy dominates. The other is to remedy or even eliminate the\ndrawback of inappropriate strategies. Adaptation is a popular and efficient\nmethod for strategy adjustments and has been widely studied in the literature.\nExisting methods commonly involve the trials of multiple strategies and then\nreward better-performing one with more resources based on their previous\nperformance. As a result, it may not efficiently address those two key issues.\nOn the one hand, they are based on trial-and-error with inappropriate\nstrategies consuming resources. On the other hand, since multiple strategies\nare involved in the trial, the inappropriate strategies could mislead the\nsearch. In this paper, we propose an adaptive differential evolution (DE) with\nexplicit exploitation and exploration controls (Explicit adaptation DE, EaDE),\nwhich is the first attempt using offline knowledge to separate multiple\nstrategies to exempt the optimization from trial-and-error. EaDE divides the\nevolution process into several SCSS (Selective-candidate with similarity\nselection) generations and adaptive generations. Exploitation and exploration\nneeds are learned in the SCSS generations by a relatively balanced strategy.\nWhile in the adaptive generations, to meet these needs, two other alternative\nstrategies, an exploitative one or an explorative one is employed. Experimental\nstudies on 28 benchmark functions confirm the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:12:32 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Zhang", "Sheng Xin", ""], ["Chan", "Wing Shing", ""], ["Tang", "Kit Sang", ""], ["Zheng", "Shao Yong", ""]]}, {"id": "2002.00623", "submitter": "Iakov Karandashev M.", "authors": "Magomed Yu. Malsagov, Emil M. Khayrov, Maria M. Pushkareva, Iakov M.\n  Karandashev", "title": "Exponential discretization of weights of neural network connections in\n  pre-trained neural networks", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": "Optical Memory and Neural Networks (Inf. Opt.), V.28, N.4,\n  pp.262-270 (2019)", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce random access memory (RAM) requirements and to increase speed of\nrecognition algorithms we consider a weight discretization problem for trained\nneural networks. We show that an exponential discretization is preferable to a\nlinear discretization since it allows one to achieve the same accuracy when the\nnumber of bits is 1 or 2 less. The quality of the neural network VGG-16 is\nalready satisfactory (top5 accuracy 69%) in the case of 3 bit exponential\ndiscretization. The ResNet50 neural network shows top5 accuracy 84% at 4 bits.\nOther neural networks perform fairly well at 5 bits (top5 accuracies of\nXception, Inception-v3, and MobileNet-v2 top5 were 87%, 90%, and 77%,\nrespectively). At less number of bits, the accuracy decreases rapidly.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:41:24 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Malsagov", "Magomed Yu.", ""], ["Khayrov", "Emil M.", ""], ["Pushkareva", "Maria M.", ""], ["Karandashev", "Iakov M.", ""]]}, {"id": "2002.00721", "submitter": "Nikolai Zolotykh", "authors": "Evgeny Dolotov and Nikolai Zolotykh", "title": "Evolutionary algorithms for constructing an ensemble of decision trees", "comments": "7 pages, 2 tables AIST 2019, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most decision tree induction algorithms are based on a greedy top-down\nrecursive partitioning strategy for tree growth. In this paper, we propose\nseveral methods for induction of decision trees and their ensembles based on\nevolutionary algorithms. The main difference of our approach is using\nreal-valued vector representation of decision tree that allows to use a large\nnumber of different optimization algorithms, as well as optimize the whole tree\nor ensemble for avoiding local optima. Differential evolution and evolution\nstrategies were chosen as optimization algorithms, as they have good results in\nreinforcement learning problems. We test the predictive performance of this\nmethods using several public UCI data sets, and the proposed methods show\nbetter quality than classical methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 13:38:50 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Dolotov", "Evgeny", ""], ["Zolotykh", "Nikolai", ""]]}, {"id": "2002.00799", "submitter": "Youming Lei", "authors": "Youming Lei, Jian Hu and Jianpeng Ding", "title": "A hybrid model based on deep LSTM for predicting high-dimensional\n  chaotic systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid method combining the deep long short-term memory (LSTM)\nmodel with the inexact empirical model of dynamical systems to predict\nhigh-dimensional chaotic systems. The deep hierarchy is encoded into the LSTM\nby superimposing multiple recurrent neural network layers and the hybrid model\nis trained with the Adam optimization algorithm. The statistical results of the\nMackey-Glass system and the Kuramoto-Sivashinsky system are obtained under the\ncriteria of root mean square error (RMSE) and anomaly correlation coefficient\n(ACC) using the singe-layer LSTM, the multi-layer LSTM, and the corresponding\nhybrid method, respectively. The numerical results show that the proposed\nmethod can effectively avoid the rapid divergence of the multi-layer LSTM model\nwhen reconstructing chaotic attractors, and demonstrate the feasibility of the\ncombination of deep learning based on the gradient descent method and the\nempirical model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 06:47:44 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Lei", "Youming", ""], ["Hu", "Jian", ""], ["Ding", "Jianpeng", ""]]}, {"id": "2002.00811", "submitter": "Jan Drchal", "authors": "Jan Drchal and Jan Faigl and Petr V\\'a\\v{n}a", "title": "WiSM: Windowing Surrogate Model for Evaluation of Curvature-Constrained\n  Tours with Dubins vehicle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dubins tours represent a solution of the Dubins Traveling Salesman Problem\n(DTSP) that is a variant of the optimization routing problem to determine a\ncurvature-constrained shortest path to visit a set of locations such that the\npath is feasible for Dubins vehicle, which moves only forward and has a limited\nturning radius. The DTSP combines the NP-hard combinatorial optimization to\ndetermine the optimal sequence of visits to the locations, as in the regular\nTSP, with the continuous optimization of the heading angles at the locations,\nwhere the optimal heading values depend on the sequence of visits and vice\nversa. We address the computationally challenging DTSP by fast evaluation of\nthe sequence of visits by the proposed Windowing Surrogate Model (WiSM) which\nestimates the length of the optimal Dubins path connecting a sequence of\nlocations in a Dubins tour. The estimation is sped up by a regression model\ntrained using close to optimum solutions of small Dubins tours that are\ngeneralized for large-scale instances of the addressed DTSP utilizing the\nsliding window technique and a cache for already computed results. The reported\nresults support that the proposed WiSM enables a fast convergence of a\nrelatively simple evolutionary algorithm to high-quality solutions of the DTSP.\nWe show that with an increasing number of locations, our algorithm scales\nsignificantly better than other state-of-the-art DTSP solvers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:06:43 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Drchal", "Jan", ""], ["Faigl", "Jan", ""], ["V\u00e1\u0148a", "Petr", ""]]}, {"id": "2002.00860", "submitter": "Wolfgang Maass Prof.", "authors": "Christoph St\\\"ockl and Wolfgang Maass", "title": "Optimized spiking neurons classify images with high accuracy through\n  temporal coding with two spikes", "comments": "23 pages, 5 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-based neuromorphic hardware promises to reduce the energy consumption\nof image classification and other deep learning applications, particularly on\nmobile phones or other edge devices. However, direct training of deep spiking\nneural networks is difficult, and previous methods for converting trained\nartificial neural networks to spiking neurons were inefficient because the\nneurons had to emit too many spikes. We show that a substantially more\nefficient conversion arises when one optimizes the spiking neuron model for\nthat purpose, so that it not only matters for information transmission how many\nspikes a neuron emits, but also when it emits those spikes. This advances the\naccuracy that can be achieved for image classification with spiking neurons,\nand the resulting networks need on average just two spikes per neuron for\nclassifying an image. In addition, our new conversion method improves latency\nand throughput of the resulting spiking networks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 10:11:45 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 07:12:27 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 14:27:31 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 07:57:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["St\u00f6ckl", "Christoph", ""], ["Maass", "Wolfgang", ""]]}, {"id": "2002.00862", "submitter": "Joseph Friedman", "authors": "Wesley H. Brigner, Naimul Hassan, Xuan Hu, Christopher H. Bennett,\n  Felipe Garcia-Sanchez, Matthew J. Marinella, Jean Anne C. Incorvia, Joseph S.\n  Friedman", "title": "CMOS-Free Multilayer Perceptron Enabled by Four-Terminal MTJ Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.mes-hall cs.ET physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing promises revolutionary improvements over conventional\nsystems for applications that process unstructured information. To fully\nrealize this potential, neuromorphic systems should exploit the biomimetic\nbehavior of emerging nanodevices. In particular, exceptional opportunities are\nprovided by the non-volatility and analog capabilities of spintronic devices.\nWhile spintronic devices have previously been proposed that emulate neurons and\nsynapses, complementary metal-oxide-semiconductor (CMOS) devices are required\nto implement multilayer spintronic perceptron crossbars. This work therefore\nproposes a new spintronic neuron that enables purely spintronic multilayer\nperceptrons, eliminating the need for CMOS circuitry and simplifying\nfabrication.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:14:02 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Brigner", "Wesley H.", ""], ["Hassan", "Naimul", ""], ["Hu", "Xuan", ""], ["Bennett", "Christopher H.", ""], ["Garcia-Sanchez", "Felipe", ""], ["Marinella", "Matthew J.", ""], ["Incorvia", "Jean Anne C.", ""], ["Friedman", "Joseph S.", ""]]}, {"id": "2002.00876", "submitter": "Alexander M. Rush", "authors": "Alexander M. Rush", "title": "Torch-Struct: Deep Structured Prediction Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on structured prediction for NLP describes a rich collection\nof distributions and algorithms over sequences, segmentations, alignments, and\ntrees; however, these algorithms are difficult to utilize in deep learning\nframeworks. We introduce Torch-Struct, a library for structured prediction\ndesigned to take advantage of and integrate with vectorized,\nauto-differentiation based frameworks. Torch-Struct includes a broad collection\nof probabilistic structures accessed through a simple and flexible\ndistribution-based API that connects to any deep learning model. The library\nutilizes batched, vectorized operations and exploits auto-differentiation to\nproduce readable, fast, and testable code. Internally, we also include a number\nof general-purpose optimizations to provide cross-algorithm efficiency.\nExperiments show significant performance gains over fast baselines and\ncase-studies demonstrate the benefits of the library. Torch-Struct is available\nat https://github.com/harvardnlp/pytorch-struct.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:43:02 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Rush", "Alexander M.", ""]]}, {"id": "2002.00984", "submitter": "Jekan Thangavelautham", "authors": "Ravi teja Nallapu, Yinan Xu, Abraham Marquez, Tristan Schuler and\n  Jekan Thangavelautham", "title": "The Design of a Space-based Observation and Tracking System for\n  Interstellar Objects", "comments": "19 pages, 17 figures, AAS GNC Conferences 2020/Advances in\n  Astronautical Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent observation of interstellar objects, 1I/Oumuamua and 2I/Borisov\ncross the solar system opened new opportunities for planetary science and\nplanetary defense. As the first confirmed objects originating outside of the\nsolar system, there are myriads of origin questions to explore and discuss,\nincluding where they came from, how did they get here and what are they\ncomposed of. Besides, there is a need to be cognizant especially if such\ninterstellar objects pass by the Earth of potential dangers of impact.\nSpecifically, in the case of Oumuamua, which was detected after its perihelion,\npassed by the Earth at around 0.2 AU, with an estimated excess speed of 60 km/s\nrelative to the Earth. Without enough forewarning time, a collision with such\nhigh-speed objects can pose a catastrophic danger to all life Earth. Such\nchallenges underscore the importance of detection and exploration systems to\nstudy these interstellar visitors. The detection system can include a\nspacecraft constellation with zenith-pointing telescope spacecraft. After an\nevent is detected, a spacecraft swarm can be deployed from Earth to flyby past\nthe visitor. The flyby can then be designed to perform a proximity operation of\ninterest. This work aims to develop algorithms to design these swarm missions\nthrough the IDEAS (Integrated Design Engineering & Automation of Swarms)\narchitecture. Specifically, we develop automated algorithms to design an\nEarth-based detection constellation and a spacecraft swarm that generates\ndetailed surface maps of the visitor during the rendezvous, along with their\nheliocentric cruise trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 19:09:18 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Nallapu", "Ravi teja", ""], ["Xu", "Yinan", ""], ["Marquez", "Abraham", ""], ["Schuler", "Tristan", ""], ["Thangavelautham", "Jekan", ""]]}, {"id": "2002.01070", "submitter": "Jakob Bossek", "authors": "Jakob Bossek, Katrin Casel, Pascal Kerschke and Frank Neumann", "title": "The Node Weight Dependent Traveling Salesperson Problem: Approximation\n  Algorithms and Randomized Search Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important optimization problems in the area of vehicle routing can be\nseen as a variant of the classical Traveling Salesperson Problem (TSP). In the\narea of evolutionary computation, the traveling thief problem (TTP) has gained\nincreasing interest over the last 5 years. In this paper, we investigate the\neffect of weights on such problems, in the sense that the cost of traveling\nincreases with respect to the weights of nodes already visited during a tour.\nThis provides abstractions of important TSP variants such as the Traveling\nThief Problem and time dependent TSP variants, and allows to study precisely\nthe increase in difficulty caused by weight dependence. We provide a\n3.59-approximation for this weight dependent version of TSP with metric\ndistances and bounded positive weights. Furthermore, we conduct experimental\ninvestigations for simple randomized local search with classical mutation\noperators and two variants of the state-of-the-art evolutionary algorithm EAX\nadapted to the weighted TSP. Our results show the impact of the node weights on\nthe position of the nodes in the resulting tour.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 00:57:35 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Bossek", "Jakob", ""], ["Casel", "Katrin", ""], ["Kerschke", "Pascal", ""], ["Neumann", "Frank", ""]]}, {"id": "2002.01126", "submitter": "Jenifer Kalafatovich", "authors": "Jenifer Kalafatovich, Minji Lee", "title": "Neural Oscillations for Encoding and Decoding Declarative Memory using\n  EEG Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative memory has been studied for its relationship with remembering\ndaily life experiences. Previous studies reported changes in power spectra\nduring encoding phase related to behavioral performance, however decoding phase\nstill needs to be explored. This study investigates neural oscillations changes\nrelated to memory process. Participants were asked to perform a memory task for\nencoding and decoding phase while EEG signals were recorded. Results showed\nthat for encoding phase, there was a significant decrease of power in low beta,\nhigh beta bands over fronto-central area and a decrease in low beta, high beta\nand gamma bands over left temporal area related to successful subsequent memory\neffects. For decoding phase, only significant decreases of alpha power were\nobserved over fronto-central area. This finding showed relevance of beta and\nalpha band for encoding and decoding phase of a memory task respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:53:30 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Kalafatovich", "Jenifer", ""], ["Lee", "Minji", ""]]}, {"id": "2002.01406", "submitter": "Mihaela Dimovska", "authors": "Mihaela Dimovska, Travis Johnston, Catherine D. Schuman, J. Parker\n  Mitchell, Thomas E. Potok", "title": "Multi-Objective Optimization for Size and Resilience of Spiking Neural\n  Networks", "comments": "Will appear in proceedings of 2019 IEEE 10th Annual Ubiquitous\n  Computing, Electronics & Mobile Communication Conference (UEMCON). IEEE\n  Catalog Number: CFP19G31-USB ISBN: 978-1-7281-3884-8 pg. 431-438", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the connectivity mechanisms in the brain, neuromorphic computing\narchitectures model Spiking Neural Networks (SNNs) in silicon. As such,\nneuromorphic architectures are designed and developed with the goal of having\nsmall, low power chips that can perform control and machine learning tasks.\nHowever, the power consumption of the developed hardware can greatly depend on\nthe size of the network that is being evaluated on the chip. Furthermore, the\naccuracy of a trained SNN that is evaluated on chip can change due to voltage\nand current variations in the hardware that perturb the learned weights of the\nnetwork. While efforts are made on the hardware side to minimize those\nperturbations, a software based strategy to make the deployed networks more\nresilient can help further alleviate that issue. In this work, we study Spiking\nNeural Networks in two neuromorphic architecture implementations with the goal\nof decreasing their size, while at the same time increasing their resiliency to\nhardware faults. We leverage an evolutionary algorithm to train the SNNs and\npropose a multiobjective fitness function to optimize the size and resiliency\nof the SNN. We demonstrate that this strategy leads to well-performing,\nsmall-sized networks that are more resilient to hardware faults.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 16:58:25 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Dimovska", "Mihaela", ""], ["Johnston", "Travis", ""], ["Schuman", "Catherine D.", ""], ["Mitchell", "J. Parker", ""], ["Potok", "Thomas E.", ""]]}, {"id": "2002.01673", "submitter": "Hendrik Richter", "authors": "Maximilian Gerwien, Rick Vo{\\ss}winkel, and Hendrik Richter", "title": "Convergence analysis of particle swarm optimization using stochastic\n  Lyapunov functions and quantifier elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adds to the discussion about theoretical aspects of particle swarm\nstability by proposing to employ stochastic Lyapunov functions and to determine\nthe convergence set by quantifier elimination. We present a computational\nprocedure and show that this approach leads to reevaluation and extension of\npreviously know stability regions for PSO using a Lyapunov approach under\nstagnation assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 07:47:07 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Gerwien", "Maximilian", ""], ["Vo\u00dfwinkel", "Rick", ""], ["Richter", "Hendrik", ""]]}, {"id": "2002.01873", "submitter": "George De Ath", "authors": "George De Ath, Richard M. Everson, Jonathan E. Fieldsend, Alma A. M.\n  Rahat", "title": "$\\epsilon$-shotgun: $\\epsilon$-greedy Batch Bayesian Optimisation", "comments": "Genetic and Evolutionary Computation Conference 2020 (GECCO '20). 9\n  pages (main paper) + 11 pages (supplementary material). Code avaliable at\n  https://github.com/georgedeath/eshotgun", "journal-ref": null, "doi": "10.1145/3377930.3390154", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation is a popular, surrogate model-based approach for\noptimising expensive black-box functions. Given a surrogate model, the next\nlocation to expensively evaluate is chosen via maximisation of a cheap-to-query\nacquisition function. We present an $\\epsilon$-greedy procedure for Bayesian\noptimisation in batch settings in which the black-box function can be evaluated\nmultiple times in parallel. Our $\\epsilon$-shotgun algorithm leverages the\nmodel's prediction, uncertainty, and the approximated rate of change of the\nlandscape to determine the spread of batch solutions to be distributed around a\nputative location. The initial target location is selected either in an\nexploitative fashion on the mean prediction, or -- with probability $\\epsilon$\n-- from elsewhere in the design space. This results in locations that are more\ndensely sampled in regions where the function is changing rapidly and in\nlocations predicted to be good (i.e close to predicted optima), with more\nscattered samples in regions where the function is flatter and/or of poorer\nquality. We empirically evaluate the $\\epsilon$-shotgun methods on a range of\nsynthetic functions and two real-world problems, finding that they perform at\nleast as well as state-of-the-art batch methods and in many cases exceed their\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 17:24:39 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 15:25:31 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["De Ath", "George", ""], ["Everson", "Richard M.", ""], ["Fieldsend", "Jonathan E.", ""], ["Rahat", "Alma A. M.", ""]]}, {"id": "2002.01973", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "Exploring Maximum Entropy Distributions with Evolutionary Algorithms", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG cs.NE math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to evolve numerically the maximum entropy probability\ndistributions for a given set of constraints, which is a variational calculus\nproblem. An evolutionary algorithm can obtain approximations to some well-known\nanalytical results, but is even more flexible and can find distributions for\nwhich a closed formula cannot be readily stated. The numerical approach handles\ndistributions over finite intervals. We show that there are two ways of\nconducting the procedure: by direct optimization of the Lagrangian of the\nconstrained problem, or by optimizing the entropy among the subset of\ndistributions which fulfill the constraints. An incremental evolutionary\nstrategy easily obtains the uniform, the exponential, the Gaussian, the\nlog-normal, the Laplace, among other distributions, once the constrained\nproblem is solved with any of the two methods. Solutions for mixed (\"chimera\")\ndistributions can be also found. We explain why many of the distributions are\nsymmetrical and continuous, but some are not.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 19:52:05 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "2002.02184", "submitter": "Francisco Jesus Martinez-Murcia", "authors": "F.J. Martinez-Murcia, A. Ortiz, Marco A. Formoso, M. Lopez-Zamora,\n  J.L. Luque, A. Gim\\'enez", "title": "A Neural Approach to Ordinal Regression for the Preventive Assessment of\n  Developmental Dyslexia", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-030-61705-9_51", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developmental Dyslexia (DD) is a learning disability related to the\nacquisition of reading skills that affects about 5% of the population. DD can\nhave an enormous impact on the intellectual and personal development of\naffected children, so early detection is key to implementing preventive\nstrategies for teaching language. Research has shown that there may be\nbiological underpinnings to DD that affect phoneme processing, and hence these\nsymptoms may be identifiable before reading ability is acquired, allowing for\nearly intervention. In this paper we propose a new methodology to assess the\nrisk of DD before students learn to read. For this purpose, we propose a mixed\nneural model that calculates risk levels of dyslexia from tests that can be\ncompleted at the age of 5 years. Our method first trains an auto-encoder, and\nthen combines the trained encoder with an optimized ordinal regression neural\nnetwork devised to ensure consistency of predictions. Our experiments show that\nthe system is able to detect unaffected subjects two years before it can assess\nthe risk of DD based mainly on phonological processing, giving a specificity of\n0.969 and a correct rate of more than 0.92. In addition, the trained encoder\ncan be used to transform test results into an interpretable subject spatial\ndistribution that facilitates risk assessment and validates methodology.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 10:08:41 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:49:21 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Martinez-Murcia", "F. J.", ""], ["Ortiz", "A.", ""], ["Formoso", "Marco A.", ""], ["Lopez-Zamora", "M.", ""], ["Luque", "J. L.", ""], ["Gim\u00e9nez", "A.", ""]]}, {"id": "2002.02210", "submitter": "Javier Del Ser Dr.", "authors": "Ibai Lana, Javier J. Sanchez-Medina, Eleni I. Vlahogianni, Javier Del\n  Ser", "title": "From Data to Actions in Intelligent Transportation Systems: a\n  Prescription of Functional Requirements for Model Actionability", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Data Science permeate every field of Transportation Science and\nEngineering, resulting in developments in the transportation sector that {are}\ndata-driven. Nowadays, Intelligent Transportation Systems (ITS) could be\narguably approached as a ``story'' intensively producing and consuming large\namounts of data. A~diversity of sensing devices densely spread over the\ninfrastructure, vehicles or the travelers' personal devices act as sources of\ndata flows that are eventually fed {into} software running on automatic\ndevices, actuators or control systems producing, in~turn, complex information\nflows {among} users, traffic managers, data analysts, traffic modeling\nscientists, etc. These~information flows provide enormous opportunities to\nimprove model development and decision-making. This work aims to describe how\ndata, coming from diverse ITS sources, can be used to learn and adapt\ndata-driven models for efficiently operating ITS assets, systems and processes;\nin~other words, for data-based models to fully become \\emph{actionable}.\nGrounded in this described data modeling pipeline for ITS, we~define the\ncharacteristics, engineering requisites and challenges intrinsic to its three\ncompounding stages, namely, data fusion, adaptive learning and model\nevaluation. We~deliberately generalize model learning to be adaptive, since,\nin~the core of our paper is the firm conviction that most learners will have to\nadapt to the ever-changing phenomenon scenario underlying the majority of ITS\napplications. Finally, we~provide a prospect of current research lines within\nData Science that can bring notable advances to data-based ITS modeling, which\nwill eventually bridge the gap towards the practicality and actionability of\nsuch models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 12:02:30 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 14:33:07 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 15:17:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lana", "Ibai", ""], ["Sanchez-Medina", "Javier J.", ""], ["Vlahogianni", "Eleni I.", ""], ["Del Ser", "Javier", ""]]}, {"id": "2002.02342", "submitter": "Xiaoliang Luo", "authors": "Xiaoliang Luo, Brett D. Roads, Bradley C. Love", "title": "The Costs and Benefits of Goal-Directed Attention in Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People deploy top-down, goal-directed attention to accomplish tasks, such as\nfinding lost keys. By tuning the visual system to relevant information sources,\nobject recognition can become more efficient (a benefit) and more biased toward\nthe target (a potential cost). Motivated by selective attention in\ncategorisation models, we developed a goal-directed attention mechanism that\ncan process naturalistic (photographic) stimuli. Our attention mechanism can be\nincorporated into any existing deep convolutional neural network (DCNNs). The\nprocessing stages in DCNNs have been related to ventral visual stream. In that\nlight, our attentional mechanism incorporates top-down influences from\nprefrontal cortex (PFC) to support goal-directed behaviour. Akin to how\nattention weights in categorisation models warp representational spaces, we\nintroduce a layer of attention weights to the mid-level of a DCNN that amplify\nor attenuate activity to further a goal. We evaluated the attentional mechanism\nusing photographic stimuli, varying the attentional target. We found that\nincreasing goal-directed attention has benefits (increasing hit rates) and\ncosts (increasing false alarm rates). At a moderate level, attention improves\nsensitivity (i.e., increases $d^\\prime$) at only a moderate increase in bias\nfor tasks involving standard images, blended images, and natural adversarial\nimages chosen to fool DCNNs. These results suggest that goal-directed attention\ncan reconfigure general-purpose DCNNs to better suit the current task goal,\nmuch like PFC modulates activity along the ventral stream. In addition to being\nmore parsimonious and brain consistent, the mid-level attention approach\nperformed better than a standard machine learning approach for transfer\nlearning, namely retraining the final network layer to accommodate the new\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:42:00 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 20:21:42 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 11:25:26 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Luo", "Xiaoliang", ""], ["Roads", "Brett D.", ""], ["Love", "Bradley C.", ""]]}, {"id": "2002.02521", "submitter": "Benjamin Stevens", "authors": "Ben Stevens, Tim Colonius", "title": "Enhancement of shock-capturing methods via machine learning", "comments": "10 pages, 11 figures. Under review for TCFD", "journal-ref": null, "doi": "10.1007/s00162-020-00531-1", "report-no": null, "categories": "physics.comp-ph cs.LG cs.NA cs.NE math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning has been used to create data-driven\nsolutions to problems for which an algorithmic solution is intractable, as well\nas fine-tuning existing algorithms. This research applies machine learning to\nthe development of an improved finite-volume method for simulating PDEs with\ndiscontinuous solutions. Shock capturing methods make use of nonlinear\nswitching functions that are not guaranteed to be optimal. Because data can be\nused to learn nonlinear relationships, we train a neural network to improve the\nresults of a fifth-order WENO method. We post-process the outputs of the neural\nnetwork to guarantee that the method is consistent. The training data consists\nof the exact mapping between cell averages and interpolated values for a set of\nintegrable functions that represent waveforms we would expect to see while\nsimulating a PDE. We demonstrate our method on linear advection of a\ndiscontinuous function, the inviscid Burgers' equation, and the 1-D Euler\nequations. For the latter, we examine the Shu-Osher model problem for\nturbulence-shockwave interactions. We find that our method outperforms WENO in\nsimulations where the numerical solution becomes overly diffused due to\nnumerical viscosity.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 21:51:39 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stevens", "Ben", ""], ["Colonius", "Tim", ""]]}, {"id": "2002.02636", "submitter": "Daniel Herring", "authors": "Daniel Herring, Michael Kirley, Xin Yao", "title": "Dynamic Multi-objective Optimization of the Travelling Thief Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of detailed and complex optimisation problem formulations that\nreflect realistic scenarios is a burgeoning field of research. A growing body\nof work exists for the Travelling Thief Problem, including multi-objective\nformulations and comparisons of exact and approximate methods to solve it.\nHowever, as many realistic scenarios are non-static in time, dynamic\nformulations have yet to be considered for the TTP. Definition of dynamics\nwithin three areas of the TTP problem are addressed; in the city locations,\navailability map and item values. Based on the elucidation of solution\nconservation between initial sets and obtained non-dominated sets, we define a\nrange of initialisation mechanisms using solutions generated via solvers,\ngreedily and randomly. These are then deployed to seed the population after a\nchange and the performance in terms of hypervolume and spread is presented for\ncomparison. Across a range of problems with varying TSP-component and\nKP-component sizes, we observe interesting trends in line with existing\nconclusions; there is little benefit to using randomisation as a strategy for\ninitialisation of solution populations when the optimal TSP and KP component\nsolutions can be exploited. Whilst these separate optima don't guarantee good\nTTP solutions, when combined, provide better initial performance and therefore\nin some examined instances, provides the best response to dynamic changes. A\ncombined approach that mixes solution generation methods to provide a composite\npopulation in response to dynamic changes provides improved performance in some\ninstances for the different dynamic TTP formulations. Potential for further\ndevelopment of a more cooperative combined method are realised to more\ncohesively exploit known information about the problems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:33:05 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Herring", "Daniel", ""], ["Kirley", "Michael", ""], ["Yao", "Xin", ""]]}, {"id": "2002.02671", "submitter": "Ali Asadipour", "authors": "Efstratios Doukakis, Kurt Debattista, Thomas Bashford-Rogers, Amar\n  Dhokia, Ali Asadipour, Alan Chalmers and Carlo Harvey", "title": "Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual\n  Environments", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2898823", "report-no": null, "categories": "cs.GR cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Environments (VEs) provide the opportunity to simulate a wide range\nof applications, from training to entertainment, in a safe and controlled\nmanner. For applications which require realistic representations of real world\nenvironments, the VEs need to provide multiple, physically accurate sensory\nstimuli. However, simulating all the senses that comprise the human sensory\nsystem (HSS) is a task that requires significant computational resources. Since\nit is intractable to deliver all senses at the highest quality, we propose a\nresource distribution scheme in order to achieve an optimal perceptual\nexperience within the given computational budgets. This paper investigates\nresource balancing for multi-modal scenarios composed of aural, visual and\nolfactory stimuli. Three experimental studies were conducted. The first\nexperiment identified perceptual boundaries for olfactory computation. In the\nsecond experiment, participants (N=25) were asked, across a fixed number of\nbudgets (M=5), to identify what they perceived to be the best visual, acoustic\nand olfactory stimulus quality for a given computational budget. Results\ndemonstrate that participants tend to prioritise visual quality compared to\nother sensory stimuli. However, as the budget size is increased, users prefer a\nbalanced distribution of resources with an increased preference for having\nsmell impulses in the VE. Based on the collected data, a quality prediction\nmodel is proposed and its accuracy is validated against previously unused\nbudgets and an untested scenario in a third and final experiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 08:59:41 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Doukakis", "Efstratios", ""], ["Debattista", "Kurt", ""], ["Bashford-Rogers", "Thomas", ""], ["Dhokia", "Amar", ""], ["Asadipour", "Ali", ""], ["Chalmers", "Alan", ""], ["Harvey", "Carlo", ""]]}, {"id": "2002.02751", "submitter": "Amir Mosavi Prof", "authors": "Hossein Bonakdari, Isa Ebtehaj, Bahram Gharabaghi, Ali Sharifi, Amir\n  Mosavi", "title": "Prediction of Discharge Capacity of Labyrinth Weir with Gene Expression\n  Programming", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a model based on gene expression programming for\npredicting the discharge coefficient of triangular labyrinth weirs. The\nparameters influencing discharge coefficient prediction were first examined and\npresented as crest height ratio to the head over the crest of the weir, a crest\nlength of water to channel width, a crest length of water to the head over the\ncrest of the weir, Froude number and vertex angle dimensionless parameters.\nDifferent models were then presented using sensitivity analysis in order to\nexamine each of the dimensionless parameters presented in this study. In\naddition, an equation was presented through the use of nonlinear regression\n(NLR) for the purpose of comparison with GEP. The results of the studies\nconducted by using different statistical indexes indicated that GEP is more\ncapable than NLR. This is to the extent that GEP predicts the discharge\ncoefficient with an average relative error of approximately 2.5% in such a\nmanner that the predicted values have less than 5% relative error in the worst\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 00:48:27 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Bonakdari", "Hossein", ""], ["Ebtehaj", "Isa", ""], ["Gharabaghi", "Bahram", ""], ["Sharifi", "Ali", ""], ["Mosavi", "Amir", ""]]}, {"id": "2002.02807", "submitter": "Thomas Passer Jensen", "authors": "T. P. Jensen, S. Tata, A. J. Ijspeert, S. Tolu", "title": "Adaptive control for hindlimb locomotion in a simulated mouse through\n  temporal cerebellar learning", "comments": "To be published in NICE '20: Proceedings of the 8th Annual\n  Neuro-inspired Computational Elements Workshop. 8 pages, 13 figures", "journal-ref": null, "doi": "10.1145/3381755", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human beings and other vertebrates show remarkable performance and efficiency\nin locomotion, but the functioning of their biological control systems for\nlocomotion is still only partially understood. The basic patterns and timing\nfor locomotion are provided by a central pattern generator (CPG) in the spinal\ncord. The cerebellum is known to play an important role in adaptive locomotion.\nRecent studies have given insights into the error signals responsible for\ndriving the cerebellar adaptation in locomotion. However, the question of how\nthe cerebellar output influences the gait remains unanswered. We hypothesize\nthat the cerebellar correction is applied to the pattern formation part of the\nCPG. Here, a bio-inspired control system for adaptive locomotion of the\nmusculoskeletal system of the mouse is presented, where a cerebellar-like\nmodule adapts the step time by using the double support interlimb asymmetry as\na temporal teaching signal. The control system is tested on a simulated mouse\nin a split-belt treadmill setup similar to those used in experiments with real\nmice. The results show adaptive locomotion behavior in the interlimb parameters\nsimilar to that seen in humans and mice. The control system adaptively\ndecreases the double support asymmetry that occurs due to environmental\nperturbations in the split-belt protocol.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:25:21 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 09:00:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jensen", "T. P.", ""], ["Tata", "S.", ""], ["Ijspeert", "A. J.", ""], ["Tolu", "S.", ""]]}, {"id": "2002.02869", "submitter": "Jakub Tomczak", "authors": "Jakub M. Tomczak and Ewelina Weglarz-Tomczak and Agoston E. Eiben", "title": "Differential Evolution with Reversible Linear Transformations", "comments": "Code: https://github.com/jmtomczak", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential evolution (DE) is a well-known type of evolutionary algorithms\n(EA). Similarly to other EA variants it can suffer from small populations and\nloose diversity too quickly. This paper presents a new approach to mitigate\nthis issue: We propose to generate new candidate solutions by utilizing\nreversible linear transformation applied to a triplet of solutions from the\npopulation. In other words, the population is enlarged by using newly generated\nindividuals without evaluating their fitness. We assess our methods on three\nproblems: (i) benchmark function optimization, (ii) discovering parameter\nvalues of the gene repressilator system, (iii) learning neural networks. The\nempirical results indicate that the proposed approach outperforms vanilla DE\nand a version of DE with applying differential mutation three times on all\ntestbeds.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:05:54 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Weglarz-Tomczak", "Ewelina", ""], ["Eiben", "Agoston E.", ""]]}, {"id": "2002.02882", "submitter": "Thomas O'Leary-Roseberry", "authors": "Thomas O'Leary-Roseberry, Omar Ghattas", "title": "Ill-Posedness and Optimization Geometry for Nonlinear Neural Network\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we analyze the role nonlinear activation functions play at\nstationary points of dense neural network training problems. We consider a\ngeneric least squares loss function training formulation. We show that the\nnonlinear activation functions used in the network construction play a critical\nrole in classifying stationary points of the loss landscape. We show that for\nshallow dense networks, the nonlinear activation function determines the\nHessian nullspace in the vicinity of global minima (if they exist), and\ntherefore determines the ill-posedness of the training problem. Furthermore,\nfor shallow nonlinear networks we show that the zeros of the activation\nfunction and its derivatives can lead to spurious local minima, and discuss\nconditions for strict saddle points. We extend these results to deep dense\nneural networks, showing that the last activation function plays an important\nrole in classifying stationary points, due to how it shows up in the gradient\nfrom the chain rule.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:33:34 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["O'Leary-Roseberry", "Thomas", ""], ["Ghattas", "Omar", ""]]}, {"id": "2002.02912", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh", "title": "Universal Equivariant Multilayer Perceptrons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group invariant and equivariant Multilayer Perceptrons (MLP), also known as\nEquivariant Networks, have achieved remarkable success in learning on a variety\nof data structures, such as sequences, images, sets, and graphs. Using tools\nfrom group theory, this paper proves the universality of a broad class of\nequivariant MLPs with a single hidden layer. In particular, it is shown that\nhaving a hidden layer on which the group acts regularly is sufficient for\nuniversal equivariance (invariance). A corollary is unconditional universality\nof equivariant MLPs for Abelian groups, such as CNNs with a single hidden\nlayer. A second corollary is the universality of equivariant MLPs with a\nhigh-order hidden layer, where we give both group-agnostic bounds and means for\ncalculating group-specific bounds on the order of hidden layer that guarantees\nuniversal equivariance (invariance).\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:25:59 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 22:28:22 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Ravanbakhsh", "Siamak", ""]]}, {"id": "2002.02949", "submitter": "Priyadarshini Panda", "authors": "Timothy Foldy-Porto, Yeshwanth Venkatesha, and Priyadarshini Panda", "title": "Activation Density driven Energy-Efficient Pruning in Training", "comments": "8 pages, 5 figures, 4 tables (Accepted in ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning with suitable retraining can yield networks with\nconsiderably fewer parameters than the original with comparable degrees of\naccuracy. Typical pruning methods require large, fully trained networks as a\nstarting point from which they perform a time-intensive iterative pruning and\nretraining procedure to regain the original accuracy. We propose a novel\npruning method that prunes a network real-time during training, reducing the\noverall training time to achieve an efficient compressed network. We introduce\nan activation density based analysis to identify the optimal relative sizing or\ncompression for each layer of the network. Our method is architecture agnostic,\nallowing it to be employed on a wide variety of systems. For VGG-19 and\nResNet18 on CIFAR-10, CIFAR-100, and TinyImageNet, we obtain exceedingly sparse\nnetworks (up to $200 \\times$ reduction in parameters and over $60 \\times$\nreduction in inference compute operations in the best case) with accuracy\ncomparable to the baseline network. By reducing the network size periodically\nduring training, we achieve total training times that are shorter than those of\npreviously proposed pruning methods. Furthermore, training compressed networks\nat different epochs with our proposed method yields considerable reduction in\ntraining compute complexity ($1.6\\times$ to $3.2\\times$ lower) at near\niso-accuracy as compared to a baseline network trained entirely from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:34:31 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 12:16:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Foldy-Porto", "Timothy", ""], ["Venkatesha", "Yeshwanth", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2002.02962", "submitter": "Daniel Seemaier", "authors": "Merten Popp, Sebastian Schlag, Christian Schulz, Daniel Seemaier", "title": "Multilevel Acyclic Hypergraph Partitioning", "comments": "arXiv admin note: text overlap with arXiv:1710.01968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed acyclic hypergraph is a generalized concept of a directed acyclic\ngraph, where each hyperedge can contain an arbitrary number of tails and heads.\nDirected hypergraphs can be used to model data flow and execution dependencies\nin streaming applications. Thus, hypergraph partitioning algorithms can be used\nto obtain efficient parallelizations for multiprocessor architectures. However,\nan acyclicity constraint on the partition is necessary when mapping streaming\napplications to embedded multiprocessors due to resource restrictions on this\ntype of hardware. The acyclic hypergraph partitioning problem is to partition\nthe hypernodes of a directed acyclic hypergraph into a given number of blocks\nof roughly equal size such that the corresponding quotient graph is acyclic\nwhile minimizing an objective function on the partition.\n  Here, we contribute the first n-level algorithm for the acyclic hypergraph\npartitioning problem. Our focus is on acyclic hypergraphs where hyperedges can\nhave one head and arbitrary many tails. Based on this, we engineer a memetic\nalgorithm to further reduce communication cost, as well as to improve\nscheduling makespan on embedded multiprocessor architectures. Experiments\nindicate that our algorithm outperforms previous algorithms that focus on the\ndirected acyclic graph case which have previously been employed in the\napplication domain. Moreover, our experiments indicate that using the directed\nhypergraph model for this type of application yields a significantly smaller\nmakespan.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:03:58 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 11:36:31 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Popp", "Merten", ""], ["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""], ["Seemaier", "Daniel", ""]]}, {"id": "2002.02992", "submitter": "Michael Green", "authors": "Michael Cerny Green, Luvneesh Mugrai, Ahmed Khalifa and Julian\n  Togelius", "title": "Mario Level Generation From Mechanics Using Scene Stitching", "comments": "10 pages, 7 figures, submitted to Foundations of Digital Games\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a level generation method for Super Mario by stitching\ntogether pre-generated \"scenes\" that contain specific mechanics, using\nmechanic-sequences from agent playthroughs as input specifications. Given a\nsequence of mechanics, our system uses an FI-2Pop algorithm and a corpus of\nscenes to perform automated level authoring. The system outputs levels that\nhave a similar mechanical sequence to the target mechanic sequence but with a\ndifferent playthrough experience. We compare our system to a greedy method that\nselects scenes that maximize the target mechanics. Our system is able to\nmaximize the number of matched mechanics while reducing emergent mechanics\nusing the stitching process compared to the greedy approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 19:44:44 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Green", "Michael Cerny", ""], ["Mugrai", "Luvneesh", ""], ["Khalifa", "Ahmed", ""], ["Togelius", "Julian", ""]]}, {"id": "2002.03001", "submitter": "Guannan Zhang", "authors": "Jiaxin Zhang, Hoang Tran, Dan Lu, Guannan Zhang", "title": "A Novel Evolution Strategy with Directional Gaussian Smoothing for\n  Blackbox Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved evolution strategy (ES) using a novel nonlocal\ngradient operator for high-dimensional black-box optimization. Standard ES\nmethods with $d$-dimensional Gaussian smoothing suffer from the curse of\ndimensionality due to the high variance of Monte Carlo (MC) based gradient\nestimators. To control the variance, Gaussian smoothing is usually limited in a\nsmall region, so existing ES methods lack nonlocal exploration ability required\nfor escaping from local minima. We develop a nonlocal gradient operator with\ndirectional Gaussian smoothing (DGS) to address this challenge. The DGS\nconducts 1D nonlocal explorations along $d$ orthogonal directions in\n$\\mathbb{R}^d$, each of which defines a nonlocal directional derivative as a 1D\nintegral. We then use Gauss-Hermite quadrature, instead of MC sampling, to\nestimate the $d$ 1D integrals to ensure high accuracy (i.e., small variance).\nOur method enables effective nonlocal exploration to facilitate the global\nsearch in high-dimensional optimization. We demonstrate the superior\nperformance of our method in three sets of examples, including benchmark\nfunctions for global optimization, and real-world science and engineering\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 20:17:19 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 19:14:44 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Tran", "Hoang", ""], ["Lu", "Dan", ""], ["Zhang", "Guannan", ""]]}, {"id": "2002.03102", "submitter": "Anuraganand Sharma Dr", "authors": "Anuraganand Sharma", "title": "A Constraint Driven Solution Model for Discrete Domains with a Case\n  Study of Exam Timetabling Problems", "comments": "41 pages in double space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many science and engineering applications require finding solutions to\nplanning and optimization problems by satisfying a set of constraints. These\nconstraint problems (CPs) are typically NP-complete and can be formalized as\nconstraint satisfaction problems (CSPs) or constraint optimization problems\n(COPs). Evolutionary algorithms (EAs) are good solvers for optimization\nproblems ubiquitous in various problem domains, however traditional operators\nfor EAs are 'blind' to constraints or generally use problem dependent objective\nfunctions; as they do not exploit information from the constraints in search\nfor solutions. A variation of EA, Intelligent constraint handling evolutionary\nalgorithm (ICHEA), has been demonstrated to be a versatile constraints-guided\nEA for continuous constrained problems in our earlier works in (Sharma and\nSharma, 2012) where it extracts information from constraints and exploits it in\nthe evolutionary search to make the search more efficient. In this paper ICHEA\nhas been demonstrated to solve benchmark exam timetabling problems, a classic\nCOP. The presented approach demonstrates competitive results with other\nstate-of-the-art approaches in EAs in terms of quality of solutions. ICHEA\nfirst uses its inter-marriage crossover operator to satisfy all the given\nconstraints incrementally and then uses combination of traditional and enhanced\noperators to optimize the solution. Generally CPs solved by EAs are problem\ndependent penalty based fitness functions. We also proposed a generic\npreference based solution model that does not require a problem dependent\nfitness function, however currently it only works for mutually exclusive\nconstraints.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 06:53:38 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Sharma", "Anuraganand", ""]]}, {"id": "2002.03148", "submitter": "Ke Li Kl", "authors": "Ke Li, Zilin Xiang, Tao Chen, Shuo Wang, Kay Chen Tan", "title": "Understanding the Automated Parameter Optimization on Transfer Learning\n  for CPDP: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven defect prediction has become increasingly important in software\nengineering process. Since it is not uncommon that data from a software project\nis insufficient for training a reliable defect prediction model, transfer\nlearning that borrows data/knowledge from other projects to facilitate the\nmodel building at the current project, namely cross-project defect prediction\n(CPDP), is naturally plausible. Most CPDP techniques involve two major steps,\ni.e., transfer learning and classification, each of which has at least one\nparameter to be tuned to achieve their optimal performance. This practice fits\nwell with the purpose of automated parameter optimization. However, there is a\nlack of thorough understanding about what are the impacts of automated\nparameter optimization on various CPDP techniques. In this paper, we present\nthe first empirical study that looks into such impacts on 62 CPDP techniques,\n13 of which are chosen from the existing CPDP literature while the other 49\nones have not been explored before. We build defect prediction models over 20\nreal-world software projects that are of different scales and characteristics.\nOur findings demonstrate that: (1) Automated parameter optimization\nsubstantially improves the defect prediction performance of 77\\% CPDP\ntechniques with a manageable computational cost. Thus more efforts on this\naspect are required in future CPDP studies. (2) Transfer learning is of\nultimate importance in CPDP. Given a tight computational budget, it is more\ncost-effective to focus on optimizing the parameter configuration of transfer\nlearning algorithms (3) The research on CPDP is far from mature where it is\n\"not difficult\" to find a better alternative by making a combination of\nexisting transfer learning and classification techniques. This finding provides\nimportant insights about the future design of CPDP techniques.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 12:01:53 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Ke", ""], ["Xiang", "Zilin", ""], ["Chen", "Tao", ""], ["Wang", "Shuo", ""], ["Tan", "Kay Chen", ""]]}, {"id": "2002.03150", "submitter": "Ke Li Kl", "authors": "Xiaoran Ruan, Ke Li, Bilel Derbel, Arnaud Liefooghe", "title": "Surrogate Assisted Evolutionary Algorithm for Medium Scale Expensive\n  Multi-Objective Optimisation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a surrogate model of an objective function has shown to be effective\nto assist evolutionary algorithms (EAs) to solve real-world complex\noptimisation problems which involve either computationally expensive numerical\nsimulations or costly physical experiments. However, their effectiveness mostly\nfocuses on small-scale problems with less than 10 decision variables. The\nscalability of surrogate assisted EAs (SAEAs) have not been well studied yet.\nIn this paper, we propose a Gaussian process surrogate model assisted EA for\nmedium-scale expensive multi-objective optimisation problems with up to 50\ndecision variables. There are three distinctive features of our proposed SAEA.\nFirst, instead of using all decision variables in surrogate model building, we\nonly use those correlated ones to build the surrogate model for each objective\nfunction. Second, rather than directly optimising the surrogate objective\nfunctions, the original multi-objective optimisation problem is transformed to\na new one based on the surrogate models. Last but not the least, a subset\nselection method is developed to choose a couple of promising candidate\nsolutions for actual objective function evaluations thus to update the training\ndataset. The effectiveness of our proposed algorithm is validated on benchmark\nproblems with 10, 20, 50 variables, comparing with three state-of-the-art\nSAEAs.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 12:06:08 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ruan", "Xiaoran", ""], ["Li", "Ke", ""], ["Derbel", "Bilel", ""], ["Liefooghe", "Arnaud", ""]]}, {"id": "2002.03283", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Segmented Graph-Bert for Graph Instance Modeling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph instance representation learning, both the diverse graph instance\nsizes and the graph node orderless property have been the major obstacles that\nrender existing representation learning models fail to work. In this paper, we\nwill examine the effectiveness of GRAPH-BERT on graph instance representation\nlearning, which was designed for node representation learning tasks originally.\nTo adapt GRAPH-BERT to the new problem settings, we re-design it with a\nsegmented architecture instead, which is also named as SEG-BERT (Segmented\nGRAPH-BERT) for reference simplicity in this paper. SEG-BERT involves no\nnode-order-variant inputs or functional components anymore, and it can handle\nthe graph node orderless property naturally. What's more, SEG-BERT has a\nsegmented architecture and introduces three different strategies to unify the\ngraph instance sizes, i.e., full-input, padding/pruning and segment shifting,\nrespectively. SEG-BERT is pre-trainable in an unsupervised manner, which can be\nfurther transferred to new tasks directly or with necessary fine-tuning. We\nhave tested the effectiveness of SEG-BERT with experiments on seven graph\ninstance benchmark datasets, and SEG-BERT can out-perform the comparison\nmethods on six out of them with significant performance advantages.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 04:55:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "2002.03331", "submitter": "Xiruo Wang", "authors": "Xiruo Wang and Risto Miikkulainen", "title": "MDEA: Malware Detection with Evolutionary Adversarial Learning", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware detection have used machine learning to detect malware in programs.\nThese applications take in raw or processed binary data to neural network\nmodels to classify as benign or malicious files. Even though this approach has\nproven effective against dynamic changes, such as encrypting, obfuscating and\npacking techniques, it is vulnerable to specific evasion attacks where that\nsmall changes in the input data cause misclassification at test time. This\npaper proposes a new approach: MDEA, an Adversarial Malware Detection model\nuses evolutionary optimization to create attack samples to make the network\nrobust against evasion attacks. By retraining the model with the evolved\nmalware samples, its performance improves a significant margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 09:59:56 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 02:26:30 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Wang", "Xiruo", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2002.03401", "submitter": "Hamid Reza Boveiri", "authors": "Hamid Reza Boveiri, Raouf Khayami, Reza Javidan, Ali Reza MehdiZadeh", "title": "Medical Image Registration Using Deep Neural Networks: A Comprehensive\n  Review", "comments": "45 Pages, 39 Figures, 10 Tables, 2 Appendixes", "journal-ref": null, "doi": "10.1016/j.compeleceng.2020.106767", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-guided interventions are saving the lives of a large number of patients\nwhere the image registration problem should indeed be considered as the most\ncomplex and complicated issue to be tackled. On the other hand, the recently\nhuge progress in the field of machine learning made by the possibility of\nimplementing deep neural networks on the contemporary many-core GPUs opened up\na promising window to challenge with many medical applications, where the\nregistration is not an exception. In this paper, a comprehensive review on the\nstate-of-the-art literature known as medical image registration using deep\nneural networks is presented. The review is systematic and encompasses all the\nrelated works previously published in the field. Key concepts, statistical\nanalysis from different points of view, confiding challenges, novelties and\nmain contributions, key-enabling techniques, future directions and prospective\ntrends all are discussed and surveyed in details in this comprehensive review.\nThis review allows a deep understanding and insight for the readers active in\nthe field who are investigating the state-of-the-art and seeking to contribute\nthe future literature.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 17:22:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Boveiri", "Hamid Reza", ""], ["Khayami", "Raouf", ""], ["Javidan", "Reza", ""], ["MehdiZadeh", "Ali Reza", ""]]}, {"id": "2002.03427", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Graph Neural Distance Metric Learning with Graph-Bert", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph distance metric learning serves as the foundation for many graph\nlearning problems, e.g., graph clustering, graph classification and graph\nmatching. Existing research works on graph distance metric (or graph kernels)\nlearning fail to maintain the basic properties of such metrics, e.g.,\nnon-negative, identity of indiscernibles, symmetry and triangle inequality,\nrespectively. In this paper, we will introduce a new graph neural network based\ndistance metric learning approaches, namely GB-DISTANCE (GRAPH-BERT based\nNeural Distance). Solely based on the attention mechanism, GB-DISTANCE can\nlearn graph instance representations effectively based on a pre-trained\nGRAPH-BERT model. Different from the existing supervised/unsupervised metrics,\nGB-DISTANCE can be learned effectively in a semi-supervised manner. In\naddition, GB-DISTANCE can also maintain the distance metric basic properties\nmentioned above. Extensive experiments have been done on several benchmark\ngraph datasets, and the results demonstrate that GB-DISTANCE can out-perform\nthe existing baseline methods, especially the recent graph neural network model\nbased graph metrics, with a significant gap in computing the graph distance.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 18:58:31 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "2002.03432", "submitter": "Jeremy Bernstein", "authors": "Jeremy Bernstein, Arash Vahdat, Yisong Yue, Ming-Yu Liu", "title": "On the distance between two neural networks and the stability of\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper relates parameter distance to gradient breakdown for a broad class\nof nonlinear compositional functions. The analysis leads to a new distance\nfunction called deep relative trust and a descent lemma for neural networks.\nSince the resulting learning rule seems to require little to no learning rate\ntuning, it may unlock a simpler workflow for training deeper and more complex\nneural networks. The Python code used in this paper is here:\nhttps://github.com/jxbz/fromage.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 19:18:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 17:36:33 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 13:51:25 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Bernstein", "Jeremy", ""], ["Vahdat", "Arash", ""], ["Yue", "Yisong", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "2002.03620", "submitter": "Anil Yaman", "authors": "Anil Yaman, Giovanni Iacca, Decebal Constantin Mocanu, George\n  Fletcher, Mykola Pechenizkiy", "title": "Novelty Producing Synaptic Plasticity", "comments": null, "journal-ref": null, "doi": "10.1145/3377929.3389976", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A learning process with the plasticity property often requires reinforcement\nsignals to guide the process. However, in some tasks (e.g. maze-navigation), it\nis very difficult (or impossible) to measure the performance of an agent (i.e.\na fitness value) to provide reinforcements since the position of the goal is\nnot known. This requires finding the correct behavior among a vast number of\npossible behaviors without having the knowledge of the reinforcement signals.\nIn these cases, an exhaustive search may be needed. However, this might not be\nfeasible especially when optimizing artificial neural networks in continuous\ndomains. In this work, we introduce novelty producing synaptic plasticity\n(NPSP), where we evolve synaptic plasticity rules to produce as many novel\nbehaviors as possible to find the behavior that can solve the problem. We\nevaluate the NPSP on maze-navigation on deceptive maze environments that\nrequire complex actions and the achievement of subgoals to complete. Our\nresults show that the search heuristic used with the proposed NPSP is indeed\ncapable of producing much more novel behaviors in comparison with a random\nsearch taken as baseline.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 09:52:41 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Yaman", "Anil", ""], ["Iacca", "Giovanni", ""], ["Mocanu", "Decebal Constantin", ""], ["Fletcher", "George", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2002.03911", "submitter": "Alexander Ororbia", "authors": "Alexander Ororbia, Ankur Mali, Daniel Kifer, C. Lee Giles", "title": "Large-Scale Gradient-Free Deep Learning with Recursive Local\n  Representation Alignment", "comments": "Further revised submission -- main description of rec-LRA revamped\n  and architecture-agnostic pseudo-code moved to appendix with additional\n  results/derivation updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on large-scale datasets requires significant\nhardware resources whose costs (even on cloud platforms) put them out of reach\nof smaller organizations, groups, and individuals. Backpropagation, the\nworkhorse for training these networks, is an inherently sequential process that\nis difficult to parallelize. Furthermore, it requires researchers to\ncontinually develop various tricks, such as specialized weight initializations\nand activation functions, in order to ensure a stable parameter optimization.\nOur goal is to seek an effective, neuro-biologically-plausible alternative to\nbackprop that can be used to train deep networks. In this paper, we propose a\ngradient-free learning procedure, recursive local representation alignment, for\ntraining large-scale neural architectures. Experiments with residual networks\non CIFAR-10 and the large benchmark, ImageNet, show that our algorithm\ngeneralizes as well as backprop while converging sooner due to weight updates\nthat are parallelizable and computationally less demanding. This is empirical\nevidence that a backprop-free algorithm can scale up to larger datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:20:02 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 04:46:28 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 06:16:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ororbia", "Alexander", ""], ["Mali", "Ankur", ""], ["Kifer", "Daniel", ""], ["Giles", "C. Lee", ""]]}, {"id": "2002.04060", "submitter": "Behnam Asadi", "authors": "Behnam Asadi, Hui Jiang", "title": "On Approximation Capabilities of ReLU Activation and Softmax Output\n  Layer in Neural Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have extended the well-established universal approximator\ntheory to neural networks that use the unbounded ReLU activation function and a\nnonlinear softmax output layer. We have proved that a sufficiently large neural\nnetwork using the ReLU activation function can approximate any function in\n$L^1$ up to any arbitrary precision. Moreover, our theoretical results have\nshown that a large enough neural network using a nonlinear softmax output layer\ncan also approximate any indicator function in $L^1$, which is equivalent to\nmutually-exclusive class labels in any realistic multiple-class pattern\nclassification problems. To the best of our knowledge, this work is the first\ntheoretical justification for using the softmax output layers in neural\nnetworks for pattern classification.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 19:48:47 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Asadi", "Behnam", ""], ["Jiang", "Hui", ""]]}, {"id": "2002.04068", "submitter": "Otman Abdoun", "authors": "Myriem Alijo, Otman Abdoun, Mostafa Bachran, Amal Bergam", "title": "Optimization by Hybridization of a Genetic Algorithm with the PROMOTHEE\n  Method: Management of Multicriteria Localization", "comments": "18 pages", "journal-ref": "Journal Economic Computation and Economic Cybernetics Studies and\n  Research, Vol 52, N 3 , pp. 171-188, 2018", "doi": "10.24818/18423264/52.3.18.12", "report-no": null, "categories": "q-fin.GN cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decision to locate an economic activity of one or several countries is\nmade taking into account numerous parameters and criteria. Several studies have\nbeen carried out in this field, but they generally use information in a reduced\ncontext. The majority are based solely on parameters, using traditional methods\nwhich often lead to unsatisfactory solutions.This work consists in hybridizing\nthrough genetic algorithms, economic intelligence (EI) and multicriteria\nanalysis methods (MCA) to improve the decisions of territorial localization.\nThe purpose is to lead the company to locate its activity in the place that\nwould allow it a competitive advantage. This work also consists of identifying\nall the parameters that can influence the decision of the economic actors and\nequipping them with tools using all the national and international data\navailable to lead to a mapping of countries, regions or departments favorable\nto the location. Throughout our research, we have as a goal the realization of\na hybrid conceptual model of economic intelligence based on multicriteria on\nwith genetic algorithms in order to optimize the decisions of localization, in\nthis perspective we opted for the method of PROMETHEE (Preference Ranking\nOrganization for Method of Enrichment Evaluation), which has made it possible\nto obtain the best compromise between the various visions and various points of\nview.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 12:17:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Alijo", "Myriem", ""], ["Abdoun", "Otman", ""], ["Bachran", "Mostafa", ""], ["Bergam", "Amal", ""]]}, {"id": "2002.04099", "submitter": "Jonas Skackauskas Mr", "authors": "Jonas Skackauskas, Tatiana Kalganova, Ian Dear, Mani Janakram", "title": "Dynamic Impact for Ant Colony Optimization algorithm", "comments": "14 pages, 5 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an extension method for Ant Colony Optimization (ACO)\nalgorithm called Dynamic Impact. Dynamic Impact is designed to solve\nchallenging optimization problems that has nonlinear relationship between\nresource consumption and fitness in relation to other part of the optimized\nsolution. This proposed method is tested against complex real-world Microchip\nManufacturing Plant Production Floor Optimization (MMPPFO) problem, as well as\ntheoretical benchmark Multi-Dimensional Knapsack problem (MKP). MMPPFO is a\nnon-trivial optimization problem, due the nature of solution fitness value\ndependence on collection of wafer-lots without prioritization of any individual\nwafer-lot. Using Dynamic Impact on single objective optimization fitness value\nis improved by 33.2%. Furthermore, MKP benchmark instances of small complexity\nhave been solved to 100% success rate where high degree of solution sparseness\nis observed, and large instances have showed average gap improved by 4.26\ntimes. Algorithm implementation demonstrated superior performance across small\nand large datasets and sparse optimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:46:32 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Skackauskas", "Jonas", ""], ["Kalganova", "Tatiana", ""], ["Dear", "Ian", ""], ["Janakram", "Mani", ""]]}, {"id": "2002.04225", "submitter": "Jason Liang", "authors": "Jason Liang, Santiago Gonzalez, Hormoz Shahrzad, and Risto\n  Miikkulainen", "title": "Regularized Evolutionary Population-Based Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metalearning of deep neural network (DNN) architectures and hyperparameters\nhas become an increasingly important area of research. At the same time,\nnetwork regularization has been recognized as a crucial dimension to effective\ntraining of DNNs. However, the role of metalearning in establishing effective\nregularization has not yet been fully explored. There is recent evidence that\nloss-function optimization could play this role, however it is computationally\nimpractical as an outer loop to full training. This paper presents an algorithm\ncalled Evolutionary Population-Based Training (EPBT) that interleaves the\ntraining of a DNN's weights with the metalearning of loss functions. They are\nparameterized using multivariate Taylor expansions that EPBT can directly\noptimize. Such simultaneous adaptation of weights and loss functions can be\ndeceptive, and therefore EPBT uses a quality-diversity heuristic called Novelty\nPulsation as well as knowledge distillation to prevent overfitting during\ntraining. On the CIFAR-10 and SVHN image classification benchmarks, EPBT\nresults in faster, more accurate learning. The discovered hyperparameters adapt\nto the training process and serve to regularize the learning task by\ndiscouraging overfitting to the labels. EPBT thus demonstrates a practical\ninstantiation of regularization metalearning based on simultaneous training.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 06:28:13 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 20:16:37 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 08:52:37 GMT"}, {"version": "v4", "created": "Wed, 21 Jul 2021 04:04:51 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Liang", "Jason", ""], ["Gonzalez", "Santiago", ""], ["Shahrzad", "Hormoz", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2002.04289", "submitter": "Alo\\\"is Pourchot", "authors": "Alo\\\"is Pourchot, Alexis Ducarouge, Olivier Sigaud", "title": "To Share or Not To Share: A Comprehensive Appraisal of Weight-Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight-sharing (WS) has recently emerged as a paradigm to accelerate the\nautomated search for efficient neural architectures, a process dubbed Neural\nArchitecture Search (NAS). Although very appealing, this framework is not\nwithout drawbacks and several works have started to question its capabilities\non small hand-crafted benchmarks. In this paper, we take advantage of the\n\\nasbench dataset to challenge the efficiency of WS on a representative search\nspace. By comparing a SOTA WS approach to a plain random search we show that,\ndespite decent correlations between evaluations using weight-sharing and\nstandalone ones, WS is only rarely significantly helpful to NAS. In particular\nwe highlight the impact of the search space itself on the benefits.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 10:29:31 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:11:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pourchot", "Alo\u00efs", ""], ["Ducarouge", "Alexis", ""], ["Sigaud", "Olivier", ""]]}, {"id": "2002.04301", "submitter": "Yangzi Guo", "authors": "Yangzi Guo, Yiyuan She, Adrian Barbu", "title": "Network Pruning via Annealing and Direct Sparsity Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs) especially deep convolutional networks are\nvery popular these days and have been proved to successfully offer quite\nreliable solutions to many vision problems. However, the use of deep neural\nnetworks is widely impeded by their intensive computational and memory cost. In\nthis paper, we propose a novel efficient network pruning method that is\nsuitable for both non-structured and structured channel-level pruning. Our\nproposed method tightens a sparsity constraint by gradually removing network\nparameters or filter channels based on a criterion and a schedule. The\nattractive fact that the network size keeps dropping throughout the iterations\nmakes it suitable for the pruning of any untrained or pre-trained network.\nBecause our method uses a $L_0$ constraint instead of the $L_1$ penalty, it\ndoes not introduce any bias in the training parameters or filter channels.\nFurthermore, the $L_0$ constraint makes it easy to directly specify the desired\nsparsity level during the network pruning process. Finally, experimental\nvalidation on extensive synthetic and real vision datasets show that the\nproposed method obtains better or competitive performance compared to other\nstates of art network pruning methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 10:51:12 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 00:28:09 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 02:48:56 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Guo", "Yangzi", ""], ["She", "Yiyuan", ""], ["Barbu", "Adrian", ""]]}, {"id": "2002.04303", "submitter": "Jonatas Chagas", "authors": "Jonatas B. C. Chagas and Julian Blank and Markus Wagner and Marcone J.\n  F. Souza and Kalyanmoy Deb", "title": "A Non-Dominated Sorting Based Customized Random-Key Genetic Algorithm\n  for the Bi-Objective Traveling Thief Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to solve a bi-objective variant of the\nwell-studied Traveling Thief Problem (TTP). The TTP is a multi-component\nproblem that combines two classic combinatorial problems: Traveling Salesman\nProblem (TSP) and Knapsack Problem (KP). We address the BI-TTP, a bi-objective\nversion of the TTP, where the goal is to minimize the overall traveling time\nand to maximize the profit of the collected items. Our proposed method is based\non a biased-random key genetic algorithm with customizations addressing\nproblem-specific characteristics. We incorporate domain knowledge through a\ncombination of near-optimal solutions of each subproblem in the initial\npopulation and use a custom repair operator to avoid the evaluation of\ninfeasible solutions. The bi-objective aspect of the problem is addressed\nthrough an elite population extracted based on the non-dominated rank and\ncrowding distance. Furthermore, we provide a comprehensive study showing the\ninfluence of each parameter on the performance. Finally, we discuss the results\nof the BI-TTP competitions at EMO-2019 and GECCO-2019 conferences where our\nmethod has won first and second places, respectively, thus proving its ability\nto find high-quality solutions consistently.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 10:56:13 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 11:48:04 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 11:41:39 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chagas", "Jonatas B. C.", ""], ["Blank", "Julian", ""], ["Wagner", "Markus", ""], ["Souza", "Marcone J. F.", ""], ["Deb", "Kalyanmoy", ""]]}, {"id": "2002.04504", "submitter": "Julian Blank", "authors": "Julian Blank, Kalyanmoy Deb", "title": "pymoo: Multi-objective Optimization in Python", "comments": null, "journal-ref": "IEEE Access 8 (2020) 89497-89509", "doi": "10.1109/ACCESS.2020.2990567", "report-no": "COIN-2020001", "categories": "cs.NE cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Python has become the programming language of choice for research and\nindustry projects related to data science, machine learning, and deep learning.\nSince optimization is an inherent part of these research fields, more\noptimization related frameworks have arisen in the past few years. Only a few\nof them support optimization of multiple conflicting objectives at a time, but\ndo not provide comprehensive tools for a complete multi-objective optimization\ntask. To address this issue, we have developed pymoo, a multi-objective\noptimization framework in Python. We provide a guide to getting started with\nour framework by demonstrating the implementation of an exemplary constrained\nmulti-objective optimization scenario. Moreover, we give a high-level overview\nof the architecture of pymoo to show its capabilities followed by an\nexplanation of each module and its corresponding sub-modules. The\nimplementations in our framework are customizable and algorithms can be\nmodified/extended by supplying custom operators. Moreover, a variety of single,\nmulti and many-objective test problems are provided and gradients can be\nretrieved by automatic differentiation out of the box. Also, pymoo addresses\npractical needs, such as the parallelization of function evaluations, methods\nto visualize low and high-dimensional spaces, and tools for multi-criteria\ndecision making. For more information about pymoo, readers are encouraged to\nvisit: https://pymoo.org\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:04:24 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Blank", "Julian", ""], ["Deb", "Kalyanmoy", ""]]}, {"id": "2002.04634", "submitter": "Bruno Iochins Grisci", "authors": "Jonas da Silveira Bohrer, Bruno Iochins Grisci and Marcio Dorn", "title": "Neuroevolution of Neural Network Architectures Using CoDeepNEAT and\n  Keras", "comments": "The original work was presented by Jonas da Silveira Bohrer in\n  partial fulfillment of the requirements for the degree of Bachelor in\n  Computer Engineering at the Institute of Informatics of the Federal\n  University of Rio Grande do Sul (UFRGS), Brazil, with Marcio Dorn as advisor\n  and Bruno Iochins Grisci as co-advisor. The original text is available at\n  Lume: https://lume.ufrgs.br/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a huge field of study in computer science and statistics\ndedicated to the execution of computational tasks through algorithms that do\nnot require explicit instructions but instead rely on learning patterns from\ndata samples to automate inferences. A large portion of the work involved in a\nmachine learning project is to define the best type of algorithm to solve a\ngiven problem. Neural networks - especially deep neural networks - are the\npredominant type of solution in the field. However, the networks themselves can\nproduce very different results according to the architectural choices made for\nthem. Finding the optimal network topology and configurations for a given\nproblem is a challenge that requires domain knowledge and testing efforts due\nto a large number of parameters that need to be considered. The purpose of this\nwork is to propose an adapted implementation of a well-established evolutionary\ntechnique from the neuroevolution field that manages to automate the tasks of\ntopology and hyperparameter selection. It uses a popular and accessible machine\nlearning framework - Keras - as the back-end, presenting results and proposed\nchanges concerning the original algorithm. The implementation is available at\nGitHub (https://github.com/sbcblab/Keras-CoDeepNEAT) with documentation and\nexamples to reproduce the experiments performed for this work.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:03:34 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Bohrer", "Jonas da Silveira", ""], ["Grisci", "Bruno Iochins", ""], ["Dorn", "Marcio", ""]]}, {"id": "2002.04688", "submitter": "Jeremy Howard", "authors": "Jeremy Howard and Sylvain Gugger", "title": "fastai: A Layered API for Deep Learning", "comments": null, "journal-ref": "Information 2020, 11(2), 108", "doi": "10.3390/info11020108", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  fastai is a deep learning library which provides practitioners with\nhigh-level components that can quickly and easily provide state-of-the-art\nresults in standard deep learning domains, and provides researchers with\nlow-level components that can be mixed and matched to build new approaches. It\naims to do both things without substantial compromises in ease of use,\nflexibility, or performance. This is possible thanks to a carefully layered\narchitecture, which expresses common underlying patterns of many deep learning\nand data processing techniques in terms of decoupled abstractions. These\nabstractions can be expressed concisely and clearly by leveraging the dynamism\nof the underlying Python language and the flexibility of the PyTorch library.\nfastai includes: a new type dispatch system for Python along with a semantic\ntype hierarchy for tensors; a GPU-optimized computer vision library which can\nbe extended in pure Python; an optimizer which refactors out the common\nfunctionality of modern optimizers into two basic pieces, allowing optimization\nalgorithms to be implemented in 4-5 lines of code; a novel 2-way callback\nsystem that can access any part of the data, model, or optimizer and change it\nat any point during training; a new data block API; and much more. We have used\nthis library to successfully create a complete deep learning course, which we\nwere able to write more quickly than using previous approaches, and the code\nwas more clear. The library is already in wide use in research, industry, and\nteaching. NB: This paper covers fastai v2, which is currently in pre-release at\nhttp://dev.fast.ai/\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:16:48 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 18:17:51 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Howard", "Jeremy", ""], ["Gugger", "Sylvain", ""]]}, {"id": "2002.04806", "submitter": "Terrence Sejnowski", "authors": "Terrence J. Sejnowski", "title": "The Unreasonable Effectiveness of Deep Learning in Artificial\n  Intelligence", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences U.S.A. (2020)\n  https://www.pnas.org/content/early/2020/01/23/1907373117", "doi": "10.1073/pnas.1907373117", "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning networks have been trained to recognize speech, caption\nphotographs and translate text between languages at high levels of performance.\nAlthough applications of deep learning networks to real world problems have\nbecome ubiquitous, our understanding of why they are so effective is lacking.\nThese empirical results should not be possible according to sample complexity\nin statistics and non-convex optimization theory. However, paradoxes in the\ntraining and effectiveness of deep learning networks are being investigated and\ninsights are being found in the geometry of high-dimensional spaces. A\nmathematical theory of deep learning would illuminate how they function, allow\nus to assess the strengths and weaknesses of different network architectures\nand lead to major improvements. Deep learning has provided natural ways for\nhumans to communicate with digital devices and is foundational for building\nartificial general intelligence. Deep learning was inspired by the architecture\nof the cerebral cortex and insights into autonomy and general intelligence may\nbe found in other brain regions that are essential for planning and survival,\nbut major breakthroughs will be needed to achieve these goals.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 05:25:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sejnowski", "Terrence J.", ""]]}, {"id": "2002.04924", "submitter": "Mattias Nilsson", "authors": "Mattias Nilsson, Foteini Liwicki and Fredrik Sandin", "title": "Synaptic Integration of Spatiotemporal Features with a Dynamic\n  Neuromorphic Processor", "comments": "Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  2020, pp. 1-7", "doi": "10.1109/IJCNN48605.2020.9207210", "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neurons can perform spatiotemporal feature detection by nonlinear\nsynaptic and dendritic integration of presynaptic spike patterns.\nMulticompartment models of non-linear dendrites and related neuromorphic\ncircuit designs enable faithful imitation of such dynamic integration\nprocesses, but these approaches are also associated with a relatively high\ncomputing cost or circuit size. Here, we investigate synaptic integration of\nspatiotemporal spike patterns with multiple dynamic synapses on point-neurons\nin the DYNAP-SE neuromorphic processor, which offers a complementary\nresource-efficient, albeit less flexible, approach to feature detection. We\ninvestigate how previously proposed excitatory--inhibitory pairs of dynamic\nsynapses can be combined to integrate multiple inputs, and we generalize that\nconcept to a case in which one inhibitory synapse is combined with multiple\nexcitatory synapses. We characterize the resulting delayed excitatory\npostsynaptic potentials (EPSPs) by measuring and analyzing the membrane\npotentials of the neuromorphic neuronal circuits. We find that biologically\nrelevant EPSP delays, with variability of order 10 milliseconds per neuron, can\nbe realized in the proposed manner by selecting different synapse combinations,\nthanks to device mismatch. Based on these results, we demonstrate that a single\npoint-neuron with dynamic synapses in the DYNAP-SE can respond selectively to\npresynaptic spikes with a particular spatiotemporal structure, which enables,\nfor instance, visual feature tuning of single neurons.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:26:35 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:05:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nilsson", "Mattias", ""], ["Liwicki", "Foteini", ""], ["Sandin", "Fredrik", ""]]}, {"id": "2002.05202", "submitter": "Noam Shazeer", "authors": "Noam Shazeer", "title": "GLU Variants Improve Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gated Linear Units (arXiv:1612.08083) consist of the component-wise product\nof two linear projections, one of which is first passed through a sigmoid\nfunction. Variations on GLU are possible, using different nonlinear (or even\nlinear) functions in place of sigmoid. We test these variants in the\nfeed-forward sublayers of the Transformer (arXiv:1706.03762)\nsequence-to-sequence model, and find that some of them yield quality\nimprovements over the typically-used ReLU or GELU activations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 19:57:13 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Shazeer", "Noam", ""]]}, {"id": "2002.05368", "submitter": "Risto Miikkulainen", "authors": "Olivier Francon, Santiago Gonzalez, Babak Hodjat, Elliot Meyerson,\n  Risto Miikkulainen, Xin Qiu, and Hormoz Shahrzad", "title": "Effective Reinforcement Learning through Evolutionary Surrogate-Assisted\n  Prescription", "comments": null, "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO-2020)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is now significant historical data available on decision making in\norganizations, consisting of the decision problem, what decisions were made,\nand how desirable the outcomes were. Using this data, it is possible to learn a\nsurrogate model, and with that model, evolve a decision strategy that optimizes\nthe outcomes. This paper introduces a general such approach, called\nEvolutionary Surrogate-Assisted Prescription, or ESP. The surrogate is, for\nexample, a random forest or a neural network trained with gradient descent, and\nthe strategy is a neural network that is evolved to maximize the predictions of\nthe surrogate model. ESP is further extended in this paper to sequential\ndecision-making tasks, which makes it possible to evaluate the framework in\nreinforcement learning (RL) benchmarks. Because the majority of evaluations are\ndone on the surrogate, ESP is more sample efficient, has lower variance, and\nlower regret than standard RL approaches. Surprisingly, its solutions are also\nbetter because both the surrogate and the strategy network regularize the\ndecision-making behavior. ESP thus forms a promising foundation to decision\noptimization in real-world problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 06:59:26 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 03:27:46 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Francon", "Olivier", ""], ["Gonzalez", "Santiago", ""], ["Hodjat", "Babak", ""], ["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""], ["Qiu", "Xin", ""], ["Shahrzad", "Hormoz", ""]]}, {"id": "2002.05406", "submitter": "Josef Urban", "authors": "Jan Jakub\\r{u}v, Karel Chvalovsk\\'y, Miroslav Ol\\v{s}\\'ak, Bartosz\n  Piotrowski, Martin Suda, Josef Urban", "title": "ENIGMA Anonymous: Symbol-Independent Inference Guiding Machine (system\n  description)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO cs.NE cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an implementation of gradient boosting and neural guidance of\nsaturation-style automated theorem provers that does not depend on consistent\nsymbol names across problems. For the gradient-boosting guidance, we manually\ncreate abstracted features by considering arity-based encodings of formulas.\nFor the neural guidance, we use symbol-independent graph neural networks (GNNs)\nand their embedding of the terms and clauses. The two methods are efficiently\nimplemented in the E prover and its ENIGMA learning-guided framework.\n  To provide competitive real-time performance of the GNNs, we have developed a\nnew context-based approach to evaluation of generated clauses in E. Clauses are\nevaluated jointly in larger batches and with respect to a large number of\nalready selected clauses (context) by the GNN that estimates their collectively\nmost useful subset in several rounds of message passing. This means that\napproximative inference rounds done by the GNN are efficiently interleaved with\nprecise symbolic inference rounds done inside E. The methods are evaluated on\nthe MPTP large-theory benchmark and shown to achieve comparable real-time\nperformance to state-of-the-art symbol-based methods. The methods also show\nhigh complementarity, solving a large number of hard Mizar problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 09:44:38 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 13:59:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jakub\u016fv", "Jan", ""], ["Chvalovsk\u00fd", "Karel", ""], ["Ol\u0161\u00e1k", "Miroslav", ""], ["Piotrowski", "Bartosz", ""], ["Suda", "Martin", ""], ["Urban", "Josef", ""]]}, {"id": "2002.05421", "submitter": "Ryan Dougherty", "authors": "Ryan E. Dougherty", "title": "Genetic Algorithms for Redundancy in Interaction Testing", "comments": "Submitted to Genetic and Evolutionary Computation Conference 2020\n  (GECCO '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is imperative for testing to determine if the components within\nlarge-scale software systems operate functionally. Interaction testing involves\ndesigning a suite of tests, which guarantees to detect a fault if one exists\namong a small number of components interacting together. The cost of this\ntesting is typically modeled by the number of tests, and thus much effort has\nbeen taken in reducing this number. Here, we incorporate redundancy into the\nmodel, which allows for testing in non-deterministic environments. Existing\nalgorithms for constructing these test suites usually involve one \"fast\"\nalgorithm for generating most of the tests, and another \"slower\" algorithm to\n\"complete\" the test suite. We employ a genetic algorithm that generalizes these\napproaches that also incorporates redundancy by increasing the number of\nalgorithms chosen, which we call \"stages.\" By increasing the number of stages,\nwe show that not only can the number of tests be reduced compared to existing\ntechniques, but the computational time in generating them is also greatly\nreduced.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:16:46 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Dougherty", "Ryan E.", ""]]}, {"id": "2002.05484", "submitter": "Hong Wu", "authors": "Hong Wu, Jiahai Wang and Zizhen Zhang", "title": "MODRL/D-AM: Multiobjective Deep Reinforcement Learning Algorithm Using\n  Decomposition and Attention Model for Multiobjective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a deep reinforcement learning method is proposed to solve\nmultiobjective optimization problem. In this method, the multiobjective\noptimization problem is decomposed to a number of single-objective optimization\nsubproblems and all the subproblems are optimized in a collaborative manner.\nEach subproblem is modeled with a pointer network and the model is trained with\nreinforcement learning. However, when pointer network extracts the features of\nan instance, it ignores the underlying structure information of the input\nnodes. Thus, this paper proposes a multiobjective deep reinforcement learning\nmethod using decomposition and attention model to solve multiobjective\noptimization problem. In our method, each subproblem is solved by an attention\nmodel, which can exploit the structure features as well as node features of\ninput nodes. The experiment results on multiobjective travelling salesman\nproblem show the proposed algorithm achieves better performance compared with\nthe previous method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 12:59:39 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Wu", "Hong", ""], ["Wang", "Jiahai", ""], ["Zhang", "Zizhen", ""]]}, {"id": "2002.05521", "submitter": "Fabricio Breve", "authors": "B\\'arbara Ribeiro da Silva and Fabricio Aparecido Breve", "title": "Segmenta\\c{c}\\~ao de imagens utilizando competi\\c{c}\\~ao e\n  coopera\\c{c}\\~ao entre part\\'iculas", "comments": null, "journal-ref": "Interci\\^encia & Sociedade - Revista Eletr\\^onica. v.4, p.75 - 85,\n  2015", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an extension proposal of the semi-supervised learning\nmethod known as Particle Competition and Cooperation for carrying out tasks of\nimage segmentation. Preliminary results show that this is a promising approach.\n  Este artigo apresenta uma proposta de extens\\~ao do modelo de aprendizado\nsemi-supervisionado conhecido como Competi\\c{c}\\~ao e Coopera\\c{c}\\~ao entre\nPart\\'iculas para a realiza\\c{c}\\~ao de tarefas de segmenta\\c{c}\\~ao de\nimagens. Resultados preliminares mostram que esta \\'e uma abordagem promissora.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 03:02:00 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["da Silva", "B\u00e1rbara Ribeiro", ""], ["Breve", "Fabricio Aparecido", ""]]}, {"id": "2002.05640", "submitter": "Cem C. Tutum", "authors": "Cem C. Tutum and Risto Miikkulainen", "title": "Adapting to Unseen Environments through Explicit Representation of\n  Context", "comments": "7 pages, 7 figures, ALife2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to deploy autonomous agents to domains such as autonomous driving,\ninfrastructure management, health care, and finance, they must be able to adapt\nsafely to unseen situations. The current approach in constructing such agents\nis to try to include as much variation into training as possible, and then\ngeneralize within the possible variations. This paper proposes a principled\napproach where a context module is coevolved with a skill module. The context\nmodule recognizes the variation and modulates the skill module so that the\nentire system performs well in unseen situations. The approach is evaluated in\na challenging version of the Flappy Bird game where the effects of the actions\nvary over time. The Context+Skill approach leads to significantly more robust\nbehavior in environments with previously unseen effects. Such a principled\ngeneralization ability is essential in deploying autonomous agents in real\nworld tasks, and can serve as a foundation for continual learning as well.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:15:47 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 22:04:16 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Tutum", "Cem C.", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2002.05882", "submitter": "Pavel Golovinski", "authors": "P.A. Golovinski, S.A. Kolodyazhnyi", "title": "Gender Genetic Algorithm in the Dynamic Optimization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach to optimizing fast processes using a gender genetic\nalgorithm is described. Its difference from the more traditional genetic\nalgorithm it contains division the artificial population into two sexes. Male\nsubpopulations undergo large mutations and more strong selection compared to\nfemale individuals from another subset. This separation allows combining the\nrapid adaptability of the entire population to changes due to the variation of\nthe male subpopulation with fixation of adaptability in the female part. The\nadvantage of the effect of additional individual learning in the form of\nBoldwin effect in finding optimal solutions is observed in comparison with the\nusual gender genetic algorithm. As a promising application of the gender\ngenetic algorithm with the Boldwin effect, the dynamics of extinguishing\nnatural fires is pointed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 06:06:38 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Golovinski", "P. A.", ""], ["Kolodyazhnyi", "S. A.", ""]]}, {"id": "2002.05933", "submitter": "Juan-Pablo Ortega", "authors": "Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega", "title": "Approximation Bounds for Random Neural Networks and Reservoir Systems", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies approximation based on single-hidden-layer feedforward and\nrecurrent neural networks with randomly generated internal weights. These\nmethods, in which only the last layer of weights and a few hyperparameters are\noptimized, have been successfully applied in a wide range of static and dynamic\nlearning problems. Despite the popularity of this approach in empirical tasks,\nimportant theoretical questions regarding the relation between the unknown\nfunction, the weight distribution, and the approximation rate have remained\nopen. In this work it is proved that, as long as the unknown function,\nfunctional, or dynamical system is sufficiently regular, it is possible to draw\nthe internal weights of the random (recurrent) neural network from a generic\ndistribution (not depending on the unknown object) and quantify the error in\nterms of the number of neurons and the hyperparameters. In particular, this\nproves that echo state networks with randomly generated weights are capable of\napproximating a wide class of dynamical systems arbitrarily well and thus\nprovides the first mathematical explanation for their empirically observed\nsuccess at learning dynamical systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 09:43:28 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 09:14:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Gonon", "Lukas", ""], ["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "2002.06001", "submitter": "Fabricio Breve", "authors": "Fabricio Breve", "title": "Building Networks for Image Segmentation using Particle Competition and\n  Cooperation", "comments": null, "journal-ref": "BREVE, FA. Building Networks for Image Segmentation Using Particle\n  Competition and Cooperation. Lecture Notes in Computer Science. Cham:\n  Springer International Publishing AG, 2017. v.10404. p.217 - 231", "doi": "10.1007/978-3-319-62392-4_16", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle competition and cooperation (PCC) is a graph-based semi-supervised\nlearning approach. When PCC is applied to interactive image segmentation tasks,\npixels are converted into network nodes, and each node is connected to its\nk-nearest neighbors, according to the distance between a set of features\nextracted from the image. Building a proper network to feed PCC is crucial to\nachieve good segmentation results. However, some features may be more important\nthan others to identify the segments, depending on the characteristics of the\nimage to be segmented. In this paper, an index to evaluate candidate networks\nis proposed. Thus, building the network becomes a problem of optimizing some\nfeature weights based on the proposed index. Computer simulations are performed\non some real-world images from the Microsoft GrabCut database, and the\nsegmentation results related in this paper show the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 12:45:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Breve", "Fabricio", ""]]}, {"id": "2002.06036", "submitter": "Victor Adrian Jimenez", "authors": "Jorge Bustos, Victor A. Jimenez, Adrian Will", "title": "A comparison of different types of Niching Genetic Algorithms for\n  variable selection in solar radiation estimation", "comments": "10 pages, two columns, 9 figures, non-published paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection problems generally present more than a single solution\nand, sometimes, it is worth to find as many solutions as possible. The use of\nEvolutionary Algorithms applied to this kind of problem proves to be one of the\nbest methods to find optimal solutions. Moreover, there are variants designed\nto find all or almost all local optima, known as Niching Genetic Algorithms\n(NGA). There are several different NGA methods developed in order to achieve\nthis task. The present work compares the behavior of eight different niching\ntechniques, applied to a climatic database of four weather stations distributed\nin Tucuman, Argentina. The goal is to find different sets of input variables\nthat have been used as the input variable by the estimation method. Final\nresults were evaluated based on low estimation error and low dispersion error,\nas well as a high number of different results and low computational time. A\nsecond experiment was carried out to study the capability of the method to\nidentify critical variables. The best results were obtained with Deterministic\nCrowding. In contrast, Steady State Worst Among Most Similar and Probabilistic\nCrowding showed good results but longer processing times and less ability to\ndetermine the critical factors.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 13:52:04 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bustos", "Jorge", ""], ["Jimenez", "Victor A.", ""], ["Will", "Adrian", ""]]}, {"id": "2002.06095", "submitter": "Alina Patelli PhD", "authors": "Alina Patelli, Victoria Lush, Aniko Ekart, Elisabeth Ilie-Zudor", "title": "Traffic Modelling and Prediction via Symbolic Regression on Road Sensor\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous expansion of the urban traffic sensing infrastructure has led\nto a surge in the volume of widely available road related data. Consequently,\nincreasing effort is being dedicated to the creation of intelligent\ntransportation systems, where decisions on issues ranging from city-wide road\nmaintenance planning to improving the commuting experience are informed by\ncomputational models of urban traffic instead of being left entirely to humans.\nThe automation of traffic management has received substantial attention from\nthe research community, however, most approaches target highways, produce\npredictions valid for a limited time window or require expensive retraining of\navailable models in order to accurately forecast traffic at a new location. In\nthis article, we propose a novel and accurate traffic flow prediction method\nbased on symbolic regression enhanced with a lag operator. Our approach\nproduces robust models suitable for the intricacies of urban roads, much more\ndifficult to predict than highways. Additionally, there is no need to retrain\nthe model for a period of up to 9 weeks. Furthermore, the proposed method\ngenerates models that are transferable to other segments of the road network,\nsimilar to, yet geographically distinct from the ones they were initially\ntrained on. We demonstrate the achievement of these claims by conducting\nextensive experiments on data collected from the Darmstadt urban\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:03:04 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Patelli", "Alina", ""], ["Lush", "Victoria", ""], ["Ekart", "Aniko", ""], ["Ilie-Zudor", "Elisabeth", ""]]}, {"id": "2002.06199", "submitter": "Qianhui Liu", "authors": "Qianhui Liu, Haibo Ruan, Dong Xing, Huajin Tang, Gang Pan", "title": "Effective AER Object Classification Using Segmented\n  Probability-Maximization Learning in Spiking Neural Networks", "comments": "AAAI 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Address event representation (AER) cameras have recently attracted more\nattention due to the advantages of high temporal resolution and low power\nconsumption, compared with traditional frame-based cameras. Since AER cameras\nrecord the visual input as asynchronous discrete events, they are inherently\nsuitable to coordinate with the spiking neural network (SNN), which is\nbiologically plausible and energy-efficient on neuromorphic hardware. However,\nusing SNN to perform the AER object classification is still challenging, due to\nthe lack of effective learning algorithms for this new representation. To\ntackle this issue, we propose an AER object classification model using a novel\nsegmented probability-maximization (SPA) learning algorithm. Technically, 1)\nthe SPA learning algorithm iteratively maximizes the probability of the classes\nthat samples belong to, in order to improve the reliability of neuron responses\nand effectiveness of learning; 2) a peak detection (PD) mechanism is introduced\nin SPA to locate informative time points segment by segment, based on which\ninformation within the whole event stream can be fully utilized by the\nlearning. Extensive experimental results show that, compared to\nstate-of-the-art methods, not only our model is more effective, but also it\nrequires less information to reach a certain level of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 04:10:58 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Liu", "Qianhui", ""], ["Ruan", "Haibo", ""], ["Xing", "Dong", ""], ["Tang", "Huajin", ""], ["Pan", "Gang", ""]]}, {"id": "2002.06233", "submitter": "Vahid Ranjbar", "authors": "Morteza Rohanian, Mostafa Salehi, Ali Darzi, Vahid Ranjbar", "title": "Convolutional Neural Networks for Sentiment Analysis in Persian Social\n  Media", "comments": "in Farsi, Iranian Journal of Electrical and Computer Engineering\n  (IJECE), February 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the social media engagement on the rise, the resulting data can be used\nas a rich resource for analyzing and understanding different phenomena around\nus. A sentiment analysis system employs these data to find the attitude of\nsocial media users towards certain entities in a given document. In this paper\nwe propose a sentiment analysis method for Persian text using Convolutional\nNeural Network (CNN), a feedforward Artificial Neural Network, that categorize\nsentences into two and five classes (considering their intensity) by applying a\nlayer of convolution over input data through different filters. We evaluated\nthe method on three different datasets of Persian social media texts using Area\nunder Curve metric. The final results show the advantage of using CNN over\nearlier attempts at developing traditional machine learning methods for Persian\ntexts sentiment classification especially for short texts.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 19:52:39 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rohanian", "Morteza", ""], ["Salehi", "Mostafa", ""], ["Darzi", "Ali", ""], ["Ranjbar", "Vahid", ""]]}, {"id": "2002.06250", "submitter": "Md Navid Akbar", "authors": "Md Navid Akbar, Mathew Yarossi, Marc Martinez-Gost, Marc A. Sommer,\n  Moritz Dannhauer, Sumientra Rampersad, Dana Brooks, Eugene Tunik, Deniz\n  Erdo\\u{g}mu\\c{s}", "title": "Mapping Motor Cortex Stimulation to Muscle Responses: A Deep Neural\n  Network Modeling Approach", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network (DNN) that can reliably model muscle responses from\ncorresponding brain stimulation has the potential to increase knowledge of\ncoordinated motor control for numerous basic science and applied use cases.\nSuch cases include the understanding of abnormal movement patterns due to\nneurological injury from stroke, and stimulation based interventions for\nneurological recovery such as paired associative stimulation. In this work,\npotential DNN models are explored and the one with the minimum squared errors\nis recommended for the optimal performance of the M2M-Net, a network that maps\ntranscranial magnetic stimulation of the motor cortex to corresponding muscle\nresponses, using: a finite element simulation, an empirical neural response\nprofile, a convolutional autoencoder, a separate deep network mapper, and\nrecordings of multi-muscle activation. We discuss the rationale behind the\ndifferent modeling approaches and architectures, and contrast their results.\nAdditionally, to obtain a comparative insight of the trade-off between\ncomplexity and performance analysis, we explore different techniques, including\nthe extension of two classical information criteria for M2M-Net. Finally, we\nfind that the model analogous to mapping the motor cortex stimulation to a\ncombination of direct and synergistic connection to the muscles performs the\nbest, when the neural response profile is used at the input.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 21:24:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Akbar", "Md Navid", ""], ["Yarossi", "Mathew", ""], ["Martinez-Gost", "Marc", ""], ["Sommer", "Marc A.", ""], ["Dannhauer", "Moritz", ""], ["Rampersad", "Sumientra", ""], ["Brooks", "Dana", ""], ["Tunik", "Eugene", ""], ["Erdo\u011fmu\u015f", "Deniz", ""]]}, {"id": "2002.06413", "submitter": "Alexander Beasley", "authors": "Alexander E. Beasley, Mohammed-Salah Abdelouahab, Ren\\'e Lozi, Anna L.\n  Powell, and Andrew Adamatzky", "title": "Mem-fractive Properties of Mushrooms", "comments": "26 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristors close the loop for I-V characteristics of the traditional,\npassive, semi-conductor devices. Originally proposed in 1971, the hunt for the\nmemristor has been going ever since. The key feature of a memristor is that its\ncurrent resitance is a function of its previous resistance. As such, the\nbehaviour of the device is influenced by changing the way in which potential is\napplied across it. Ultimately, information can be encoded on memristors.\nBiological substrates have already been shown to exhibit some memristive\nproperties. However, many memristive devices are yet to be found. Here we show\nthat the fruit bodies of grey oyster fungi Pleurotus ostreatus exhibit\nmemristive behaviours. This paper presents the I-V characteristics of the\nmushrooms. By examination of the conducted current for a given voltage applied\nas a function of the previous voltage, it is shown that the mushroom is a\nmemristor. Our results demonstrate that nature continues to provide specimens\nthat hold these unique and valuable electrical characteristics and which have\nthe potential to advance the field of hybrid electronic systems.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 16:59:14 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:43:02 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Beasley", "Alexander E.", ""], ["Abdelouahab", "Mohammed-Salah", ""], ["Lozi", "Ren\u00e9", ""], ["Powell", "Anna L.", ""], ["Adamatzky", "Andrew", ""]]}, {"id": "2002.06701", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic\n  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF)\nfor Better Semantic Selection for Indian regional language-based image\ncaptioning and introduced a procedure where we used the existing translation\nand English crowd-sourced sentences for training. We have shown that this\narchitecture is a promising alternative source, where there is a crunch in\nresources. Our main contribution of this work is the development of deep\nlearning architectures for the Bengali language (is the fifth widely spoken\nlanguage in the world) with a completely different grammar and language\nattributes. We have shown that these are working well for complex applications\nlike language generation from image contexts and can diversify the\nrepresentation through introducing constraints, more extensive features, and\nunique feature spaces. We also established that we could achieve absolute\nprecision and diversity when we use smoothened semantic tensor with the\ntraditional LSTM and feature decomposition networks. With better learning\narchitecture, we succeeded in establishing an automated algorithm and\nassessment procedure that can help in the evaluation of competent applications\nwithout the requirement for expertise and human intervention.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 23:03:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2002.06766", "submitter": "Hirad Assimi", "authors": "Hirad Assimi, Oscar Harper, Yue Xie, Aneta Neumann, Frank Neumann", "title": "Evolutionary Bi-objective Optimization for the Dynamic\n  Chance-Constrained Knapsack Problem Based on Tail Bound Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world combinatorial optimization problems are often stochastic and\ndynamic. Therefore, it is essential to make optimal and reliable decisions with\na holistic approach. In this paper, we consider the dynamic chance-constrained\nknapsack problem where the weight of each item is stochastic, the capacity\nconstraint changes dynamically over time, and the objective is to maximize the\ntotal profit subject to the probability that total weight exceeds the capacity.\nWe make use of prominent tail inequalities such as Chebyshev's inequality, and\nChernoff bound to approximate the probabilistic constraint. Our key\ncontribution is to introduce an additional objective which estimates the\nminimal capacity bound for a given stochastic solution that still meets the\nchance constraint. This objective helps to cater for dynamic changes to the\nstochastic problem. We apply single- and multi-objective evolutionary\nalgorithms to the problem and show how bi-objective optimization can help to\ndeal with dynamic chance-constrained problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:36:15 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Assimi", "Hirad", ""], ["Harper", "Oscar", ""], ["Xie", "Yue", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""]]}, {"id": "2002.07221", "submitter": "Wei-Chang Yeh", "authors": "Wei-Chang Yeh", "title": "Convolutional Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) and deep learning (e.g., convolutional\nneural networks (CNNs)) are the two most famous algorithms in small and big\ndata, respectively. Nonetheless, smaller datasets may be very important,\ncostly, and not easy to obtain in a short time. This paper proposes a novel\nconvolutional SVM (CSVM) that has both advantages of CNN and SVM to improve the\naccuracy and effectiveness of mining smaller datasets. The proposed CSVM adapts\nthe convolution product from CNN to learn new information hidden deeply in the\ndatasets. In addition, it uses a modified simplified swarm optimization (SSO)\nto help train the CSVM to update classifiers, and then the traditional SVM is\nimplemented as the fitness for the SSO to estimate the accuracy. To evaluate\nthe performance of the proposed CSVM, experiments were conducted to test five\nwell-known benchmark databases for the classification problem. Numerical\nexperiments compared favorably with those obtained using SVM, 3-layer\nartificial NN (ANN), and 4-layer ANN. The results of these experiments verify\nthat the proposed CSVM with the proposed SSO can effectively increase\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:23:21 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yeh", "Wei-Chang", ""]]}, {"id": "2002.07224", "submitter": "Garrett Bingham", "authors": "Garrett Bingham, William Macke, and Risto Miikkulainen", "title": "Evolutionary Optimization of Deep Learning Activation Functions", "comments": "8 pages; 9 figures/tables; GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of activation function can have a large effect on the performance\nof a neural network. While there have been some attempts to hand-engineer novel\nactivation functions, the Rectified Linear Unit (ReLU) remains the most\ncommonly-used in practice. This paper shows that evolutionary algorithms can\ndiscover novel activation functions that outperform ReLU. A tree-based search\nspace of candidate activation functions is defined and explored with mutation,\ncrossover, and exhaustive search. Experiments on training wide residual\nnetworks on the CIFAR-10 and CIFAR-100 image datasets show that this approach\nis effective. Replacing ReLU with evolved activation functions results in\nstatistically significant increases in network accuracy. Optimal performance is\nachieved when evolution is allowed to customize activation functions to a\nparticular task; however, these novel activation functions are shown to\ngeneralize, achieving high performance across tasks. Evolutionary optimization\nof activation functions is therefore a promising new dimension of metalearning\nin neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 19:54:26 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 15:53:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bingham", "Garrett", ""], ["Macke", "William", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2002.07259", "submitter": "Mostafa ElAraby", "authors": "Mostafa ElAraby, Guy Wolf, Margarida Carvalho", "title": "Identifying Critical Neurons in ANN Architectures using Mixed Integer\n  Programming", "comments": "16 pages, 3 figures, 5 tables, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a mixed integer program (MIP) for assigning importance scores to\neach neuron in deep neural network architectures which is guided by the impact\nof their simultaneous pruning on the main learning task of the network. By\ncarefully devising the objective function of the MIP, we drive the solver to\nminimize the number of critical neurons (i.e., with high importance score) that\nneed to be kept for maintaining the overall accuracy of the trained neural\nnetwork. Further, the proposed formulation generalizes the recently considered\nlottery ticket optimization by identifying multiple \"lucky\" sub-networks\nresulting in optimized architecture that not only performs well on a single\ndataset, but also generalizes across multiple ones upon retraining of network\nweights. Finally, we present a scalable implementation of our method by\ndecoupling the importance scores across layers using auxiliary networks. We\ndemonstrate the ability of our formulation to prune neural networks with\nmarginal loss in accuracy and generalizability on popular datasets and\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:32:47 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 07:03:36 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 16:09:43 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 16:39:50 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["ElAraby", "Mostafa", ""], ["Wolf", "Guy", ""], ["Carvalho", "Margarida", ""]]}, {"id": "2002.07422", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Qiuchi Li, Lingyu Hua and Dawei Song", "title": "Assessing the Memory Ability of Recurrent Neural Networks", "comments": "Accepted by ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that Recurrent Neural Networks (RNNs) can remember, in their\nhidden layers, part of the semantic information expressed by a sequence (e.g.,\na sentence) that is being processed. Different types of recurrent units have\nbeen designed to enable RNNs to remember information over longer time spans.\nHowever, the memory abilities of different recurrent units are still\ntheoretically and empirically unclear, thus limiting the development of more\neffective and explainable RNNs. To tackle the problem, in this paper, we\nidentify and analyze the internal and external factors that affect the memory\nability of RNNs, and propose a Semantic Euclidean Space to represent the\nsemantics expressed by a sequence. Based on the Semantic Euclidean Space, a\nseries of evaluation indicators are defined to measure the memory abilities of\ndifferent recurrent units and analyze their limitations. These evaluation\nindicators also provide a useful guidance to select suitable sequence lengths\nfor different RNNs during training.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 08:07:23 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhang", "Cheng", ""], ["Li", "Qiuchi", ""], ["Hua", "Lingyu", ""], ["Song", "Dawei", ""]]}, {"id": "2002.07514", "submitter": "Andrea Asperti", "authors": "Andrea Asperti, Matteo Trentin", "title": "Balancing reconstruction error and Kullback-Leibler divergence in\n  Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the loss function of Variational Autoencoders there is a well known\ntension between two components: the reconstruction loss, improving the quality\nof the resulting images, and the Kullback-Leibler divergence, acting as a\nregularizer of the latent space. Correctly balancing these two components is a\ndelicate issue, easily resulting in poor generative behaviours. In a recent\nwork, Dai and Wipf obtained a sensible improvement by allowing the network to\nlearn the balancing factor during training, according to a suitable loss\nfunction. In this article, we show that learning can be replaced by a simple\ndeterministic computation, helping to understand the underlying mechanism, and\nresulting in a faster and more accurate behaviour. On typical datasets such as\nCifar and Celeba, our technique sensibly outperforms all previous VAE\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 12:22:31 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Asperti", "Andrea", ""], ["Trentin", "Matteo", ""]]}, {"id": "2002.07528", "submitter": "Piotr Kicki", "authors": "Piotr Kicki, Mete Ozay and Piotr Skrzypczy\\'nski", "title": "A Computationally Efficient Neural Network Invariant to the Action of\n  Symmetry Subgroups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to design a computationally efficient $G$-invariant\nneural network that approximates functions invariant to the action of a given\npermutation subgroup $G \\leq S_n$ of the symmetric group on input data. The key\nelement of the proposed network architecture is a new $G$-invariant\ntransformation module, which produces a $G$-invariant latent representation of\nthe input data. This latent representation is then processed with a multi-layer\nperceptron in the network. We prove the universality of the proposed\narchitecture, discuss its properties and highlight its computational and memory\nefficiency. Theoretical considerations are supported by numerical experiments\ninvolving different network configurations, which demonstrate the effectiveness\nand strong generalization properties of the proposed method in comparison to\nother $G$-invariant neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 12:50:56 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kicki", "Piotr", ""], ["Ozay", "Mete", ""], ["Skrzypczy\u0144ski", "Piotr", ""]]}, {"id": "2002.07534", "submitter": "Konstantinos Michmizos", "authors": "Praveenram Balachandar and Konstantinos P. Michmizos", "title": "A Spiking Neural Network Emulating the Structure of the Oculomotor\n  System Requires No Learning to Control a Biomimetic Robotic Head", "comments": "6 pages, 3 figures, IEEE International Conference on Biomedical\n  Robotics and Biomechatronics (BioRob) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic vision introduces requirements for real-time processing of\nfast-varying, noisy information in a continuously changing environment. In a\nreal-world environment, convenient assumptions, such as static camera systems\nand deep learning algorithms devouring high volumes of ideally slightly-varying\ndata are hard to survive. Leveraging on recent studies on the neural connectome\nassociated with eye movements, we designed a neuromorphic oculomotor controller\nand placed it at the heart of our in-house biomimetic robotic head prototype.\nThe controller is unique in the sense that (1) all data are encoded and\nprocessed by a spiking neural network (SNN), and (2) by mimicking the\nassociated brain areas' topology, the SNN is biologically interpretable and\nrequires no training to operate. Here, we report the robot's target tracking\nability, demonstrate that its eye kinematics are similar to those reported in\nhuman eye studies and show that a biologically-constrained learning, although\nnot required for the SNN's function, can be used to further refine its\nperformance. This work aligns with our ongoing effort to develop\nenergy-efficient neuromorphic SNNs and harness their emerging intelligence to\ncontrol biomimetic robots with versatility and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:03:06 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 17:48:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Balachandar", "Praveenram", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.07541", "submitter": "Neelesh Kumar", "authors": "Neelesh Kumar and Konstantinos P. Michmizos", "title": "Machine Learning for Motor Learning: EEG-based Continuous Assessment of\n  Cognitive Engagement for Adaptive Rehabilitation Robots", "comments": "6 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cognitive engagement (CE) is crucial for motor learning, it remains\nunderutilized in rehabilitation robots, partly because its assessment currently\nrelies on subjective and gross measurements taken intermittently. Here, we\npropose an end-to-end computational framework that assesses CE in real-time,\nusing electroencephalography (EEG) signals as objective measurements. The\nframework consists of i) a deep convolutional neural network (CNN) that\nextracts task-discriminative spatiotemporal EEG to predict the level of CE for\ntwo classes -- cognitively engaged vs. disengaged; and ii) a novel sliding\nwindow method that predicts continuous levels of CE in real-time. We evaluated\nour framework on 8 subjects using an in-house Go/No-Go experiment that adapted\nits gameplay parameters to induce cognitive fatigue. The proposed CNN had an\naverage leave-one-out accuracy of 88.13\\%. The CE prediction correlated well\nwith a commonly used behavioral metric based on self-reports taken every 5\nminutes ($\\rho$=0.93). Our results objectify CE in real-time and pave the way\nfor using CE as a rehabilitation parameter for tailoring robotic therapy to\neach patient's needs and skills.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:13:39 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:59:22 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kumar", "Neelesh", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.07922", "submitter": "Mehrdad Farahani", "authors": "Mehrdad Farahani, Marzieh Farahani, Mohammad Manthouri, Okyay Kaynak", "title": "Short-Term Traffic Flow Prediction Using Variational LSTM Networks", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic flow characteristics are one of the most critical decision-making and\ntraffic policing factors in a region. Awareness of the predicted status of the\ntraffic flow has prime importance in traffic management and traffic information\ndivisions. The purpose of this research is to suggest a forecasting model for\ntraffic flow by using deep learning techniques based on historical data in the\nIntelligent Transportation Systems area. The historical data collected from the\nCaltrans Performance Measurement Systems (PeMS) for six months in 2019. The\nproposed prediction model is a Variational Long Short-Term Memory Encoder in\nbrief VLSTM-E try to estimate the flow accurately in contrast to other\nconventional methods. VLSTM-E can provide more reliable short-term traffic flow\nby considering the distribution and missing values.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 23:22:31 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Farahani", "Mehrdad", ""], ["Farahani", "Marzieh", ""], ["Manthouri", "Mohammad", ""], ["Kaynak", "Okyay", ""]]}, {"id": "2002.08032", "submitter": "Jianhao Ding", "authors": "Jianhao Ding, Lansheng Han", "title": "A Fixed point view: A Model-Based Clustering Framework", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the inflation of the data, clustering analysis, as a branch of\nunsupervised learning, lacks unified understanding and application of its\nmathematical law. Based on the view of fixed point, this paper restates the\nmodel-based clustering and proposes a unified clustering framework. In order to\nfind fixed points as cluster centers, the framework iteratively constructs the\ncontraction map, which strongly reveals the convergence mechanism and\ninterconnections among algorithms. By specifying a contraction map, Gaussian\nmixture model (GMM) can be mapped to the framework as an application. We hope\nthe fixed point framework will help the design of future clustering algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 07:06:47 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Ding", "Jianhao", ""], ["Han", "Lansheng", ""]]}, {"id": "2002.08071", "submitter": "Stefano Massaroli", "authors": "Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita,\n  Hajime Asama", "title": "Dissecting Neural ODEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous deep learning architectures have recently re-emerged as Neural\nOrdinary Differential Equations (Neural ODEs). This infinite-depth approach\ntheoretically bridges the gap between deep learning and dynamical systems,\noffering a novel perspective. However, deciphering the inner working of these\nmodels is still an open challenge, as most applications apply them as generic\nblack-box modules. In this work we \"open the box\", further developing the\ncontinuous-depth formulation with the aim of clarifying the influence of\nseveral design choices on the underlying dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 09:14:46 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:22:32 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 06:46:43 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 14:40:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Massaroli", "Stefano", ""], ["Poli", "Michael", ""], ["Park", "Jinkyoo", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""]]}, {"id": "2002.08089", "submitter": "Kayode Adetunji", "authors": "Kayode Adetunji, Ivan Hofsajer, Ling Cheng", "title": "Optimal DG allocation and sizing in power system networks using\n  swarm-based algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed generation (DG) units are power generating plants that are very\nimportant to the architecture of present power system networks. The benefit of\nthe addition of these DG units is to increase the power supply to a network.\nHowever, the installation of these DG units can cause an adverse effect if not\nproperly allocated and/or sized. Therefore, there is a need to optimally\nallocate and size them to avoid cases such as voltage instability and expensive\ninvestment costs. In this paper, two swarm-based meta-heuristic algorithms,\nparticle swarm optimization (PSO) and whale optimization algorithm (WOA) were\ndeveloped to solve optimal placement and sizing of DG units in the quest for\ntransmission network planning. A supportive technique, loss sensitivity factors\n(LSF) was used to identify potential buses for optimal location of DG units.\nThe feasibility of the algorithms was confirmed on two IEEE bus test systems\n(14- and 30-bus). Comparison results showed that both algorithms produce good\nsolutions and they outperform each other in different metrics. The WOA real\npower loss reduction considering techno-economic factors in the IEEE 14-bus and\n30-bus test system are 6.14 MW and 10.77 MW, compared to the PSOs' 6.47 MW and\n11.73 MW respectively. The PSO has a more reduced total DG unit size in both\nbus systems with 133.45 MW and 82.44 MW compared to WOAs' 152.21 MW and 82.44\nMW respectively. The paper unveils the strengths and weaknesses of the PSO and\nthe WOA in the application of optimal sizing of DG units in transmission\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 09:58:28 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Adetunji", "Kayode", ""], ["Hofsajer", "Ivan", ""], ["Cheng", "Ling", ""]]}, {"id": "2002.08111", "submitter": "Sam Ringer", "authors": "Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, Jamie\n  Dougherty", "title": "Hierarchical Quantized Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite progress in training neural networks for lossy image compression,\ncurrent approaches fail to maintain both perceptual quality and abstract\nfeatures at very low bitrates. Encouraged by recent success in learning\ndiscrete representations with Vector Quantized Variational Autoencoders\n(VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors\nof compression. We show that the combination of stochastic quantization and\nhierarchical latent structure aids likelihood-based image compression. This\nleads us to introduce a novel objective for training hierarchical VQ-VAEs. Our\nresulting scheme produces a Markovian series of latent variables that\nreconstruct images of high-perceptual quality which retain semantically\nmeaningful features. We provide qualitative and quantitative evaluations on the\nCelebA and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:26:34 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 15:39:36 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 11:10:26 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Williams", "Will", ""], ["Ringer", "Sam", ""], ["Ash", "Tom", ""], ["Hughes", "John", ""], ["MacLeod", "David", ""], ["Dougherty", "Jamie", ""]]}, {"id": "2002.08118", "submitter": "Tony Duan", "authors": "Greg Yang, Tony Duan, J. Edward Hu, Hadi Salman, Ilya Razenshteyn,\n  Jerry Li", "title": "Randomized Smoothing of All Shapes and Sizes", "comments": "9 pages main text, 49 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized smoothing is the current state-of-the-art defense with provable\nrobustness against $\\ell_2$ adversarial attacks. Many works have devised new\nrandomized smoothing schemes for other metrics, such as $\\ell_1$ or\n$\\ell_\\infty$; however, substantial effort was needed to derive such new\nguarantees. This begs the question: can we find a general theory for randomized\nsmoothing?\n  We propose a novel framework for devising and analyzing randomized smoothing\nschemes, and validate its effectiveness in practice. Our theoretical\ncontributions are: (1) we show that for an appropriate notion of \"optimal\", the\noptimal smoothing distributions for any \"nice\" norms have level sets given by\nthe norm's *Wulff Crystal*; (2) we propose two novel and complementary methods\nfor deriving provably robust radii for any smoothing distribution; and, (3) we\nshow fundamental limits to current randomized smoothing techniques via the\ntheory of *Banach space cotypes*. By combining (1) and (2), we significantly\nimprove the state-of-the-art certified accuracy in $\\ell_1$ on standard\ndatasets. Meanwhile, we show using (3) that with only label statistics under\nrandom input perturbations, randomized smoothing cannot achieve nontrivial\ncertified accuracy against perturbations of $\\ell_p$-norm $\\Omega(\\min(1,\nd^{\\frac{1}{p} - \\frac{1}{2}}))$, when the input dimension $d$ is large. We\nprovide code in github.com/tonyduan/rs4a.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:41:09 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:33:48 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 06:59:55 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 05:27:53 GMT"}, {"version": "v5", "created": "Thu, 23 Jul 2020 21:20:51 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Yang", "Greg", ""], ["Duan", "Tony", ""], ["Hu", "J. Edward", ""], ["Salman", "Hadi", ""], ["Razenshteyn", "Ilya", ""], ["Li", "Jerry", ""]]}, {"id": "2002.08354", "submitter": "Konstantinos Michmizos", "authors": "Neelesh Kumar and Konstantinos P. Michmizos", "title": "Deep Learning of Movement Intent and Reaction Time for EEG-informed\n  Adaptation of Rehabilitation Robots", "comments": "6 pages, 3 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:2002.07541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mounting evidence suggests that adaptation is a crucial mechanism for\nrehabilitation robots in promoting motor learning. Yet, it is commonly based on\nrobot-derived movement kinematics, which is a rather subjective measurement of\nperformance, especially in the presence of a sensorimotor impairment. Here, we\npropose a deep convolutional neural network (CNN) that uses\nelectroencephalography (EEG) as an objective measurement of two kinematics\ncomponents that are typically used to assess motor learning and thereby\nadaptation: i) the intent to initiate a goal-directed movement, and ii) the\nreaction time (RT) of that movement. We evaluated our CNN on data acquired from\nan in-house experiment where 13 subjects moved a rehabilitation robotic arm in\nfour directions on a plane, in response to visual stimuli. Our CNN achieved\naverage test accuracies of 80.08% and 79.82% in a binary classification of the\nintent (intent vs. no intent) and RT (slow vs. fast), respectively. Our results\ndemonstrate how individual movement components implicated in distinct types of\nmotor learning can be predicted from synchronized EEG data acquired before the\nstart of the movement. Our approach can, therefore, inform robotic adaptation\nin real-time and has the potential to further improve one's ability to perform\nthe rehabilitation task.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:20:46 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kumar", "Neelesh", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.08468", "submitter": "Jae-Geun Yoon", "authors": "Jae-Geun Yoon and Minji Lee", "title": "Effective Correlates of Motor Imagery Performance based on Default Mode\n  Network in Resting-State", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery based brain-computer interfaces (MI-BCIs) allow the control of\ndevices and communication by imagining different muscle movements. However,\nmost studies have reported a problem of \"BCI-illiteracy\" that does not have\nenough performance to use MI-BCI. Therefore, understanding subjects with poor\nperformance and finding the cause of performance variation is still an\nimportant challenge. In this study, we proposed predictors of MI performance\nusing effective connectivity in resting-state EEG. As a result, the high and\nlow MI performance groups had a significant difference as 23% MI performance\ndifference. We also found that connection from right lateral parietal to left\nlateral parietal in resting-state EEG was correlated significantly with MI\nperformance (r = -0.37). These findings could help to understand BCI-illiteracy\nand to consider alternatives that are appropriate for the subject.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 04:22:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Yoon", "Jae-Geun", ""], ["Lee", "Minji", ""]]}, {"id": "2002.08539", "submitter": "Zhixin Liu", "authors": "Lei Gao, Mingxiang Chen, Qichang Chen, Ganzhong Luo, Nuoyi Zhu, Zhixin\n  Liu", "title": "Learn to Design the Heuristics for Vehicle Routing Problem", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to learn the local-search heuristics that\niteratively improves the solution of Vehicle Routing Problem (VRP). A\nlocal-search heuristics is composed of a destroy operator that destructs a\ncandidate solution, and a following repair operator that rebuilds the\ndestructed one into a new one. The proposed neural network, as trained through\nactor-critic framework, consists of an encoder in form of a modified version of\nGraph Attention Network where node embeddings and edge embeddings are\nintegrated, and a GRU-based decoder rendering a pair of destroy and repair\noperators. Experiment results show that it outperforms both the traditional\nheuristics algorithms and the existing neural combinatorial optimization for\nVRP on medium-scale data set, and is able to tackle the large-scale data set\n(e.g., over 400 nodes) which is a considerable challenge in this area.\nMoreover, the need for expertise and handcrafted heuristics design is\neliminated due to the fact that the proposed network learns to design the\nheuristics with a better performance. Our implementation is available online.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:39:02 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Gao", "Lei", ""], ["Chen", "Mingxiang", ""], ["Chen", "Qichang", ""], ["Luo", "Ganzhong", ""], ["Zhu", "Nuoyi", ""], ["Liu", "Zhixin", ""]]}, {"id": "2002.08645", "submitter": "Alberto Tonda", "authors": "Pietro Barbiero, Giovanni Squillero, Alberto Tonda", "title": "Uncovering Coresets for Classification With Multi-Objective Evolutionary\n  Algorithms", "comments": "9 pages, 3 figures, conference. Submitted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A coreset is a subset of the training set, using which a machine learning\nalgorithm obtains performances similar to what it would deliver if trained over\nthe whole original data. Coreset discovery is an active and open line of\nresearch as it allows improving training speed for the algorithms and may help\nhuman understanding the results. Building on previous works, a novel approach\nis presented: candidate corsets are iteratively optimized, adding and removing\nsamples. As there is an obvious trade-off between limiting training size and\nquality of the results, a multi-objective evolutionary algorithm is used to\nminimize simultaneously the number of points in the set and the classification\nerror. Experimental results on non-trivial benchmarks show that the proposed\napproach is able to deliver results that allow a classifier to obtain lower\nerror and better ability of generalizing on unseen data than state-of-the-art\ncoreset discovery techniques.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 09:59:56 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Barbiero", "Pietro", ""], ["Squillero", "Giovanni", ""], ["Tonda", "Alberto", ""]]}, {"id": "2002.08729", "submitter": "Ke Quan", "authors": "Ke Quan", "title": "Bimodal Distribution Removal and Genetic Algorithm in Neural Network for\n  Breast Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis of breast cancer has been well studied in the past. Multiple linear\nprogramming models have been devised to approximate the relationship between\ncell features and tumour malignancy. However, these models are less capable in\nhandling non-linear correlations. Neural networks instead are powerful in\nprocessing complex non-linear correlations. It is thus certainly beneficial to\napproach this cancer diagnosis problem with a model based on neural network.\nParticularly, introducing bias to neural network training process is deemed as\nan important means to increase training efficiency. Out of a number of popular\nproposed methods for introducing artificial bias, Bimodal Distribution Removal\n(BDR) presents ideal efficiency improvement results and fair simplicity in\nimplementation. However, this paper examines the effectiveness of BDR against\nthe target cancer diagnosis classification problem and shows that BDR process\nin fact negatively impacts classification performance. In addition, this paper\nalso explores genetic algorithm as an efficient tool for feature selection and\nproduced significantly better results comparing to baseline model that without\nany feature selection in place\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:51:40 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Quan", "Ke", ""]]}, {"id": "2002.08809", "submitter": "Guan-Horng Liu", "authors": "Guan-Horng Liu, Tianrong Chen and Evangelos A. Theodorou", "title": "DDPNOpt: Differential Dynamic Programming Neural Optimizer", "comments": "Accepted in International Conference on Learning Representations\n  (ICLR) 2021 as Spotlight", "journal-ref": "10th International Conference on Learning Representations (ICLR\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of Deep Neural Networks (DNNs) training as an optimal control\nproblem with nonlinear dynamical systems has received considerable attention\nrecently, yet the algorithmic development remains relatively limited. In this\nwork, we make an attempt along this line by reformulating the training\nprocedure from the trajectory optimization perspective. We first show that most\nwidely-used algorithms for training DNNs can be linked to the Differential\nDynamic Programming (DDP), a celebrated second-order method rooted in the\nApproximate Dynamic Programming. In this vein, we propose a new class of\noptimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and\nconvolution networks. DDPNOpt features layer-wise feedback policies which\nimprove convergence and reduce sensitivity to hyper-parameter over existing\nmethods. It outperforms other optimal-control inspired training methods in both\nconvergence and complexity, and is competitive against state-of-the-art first\nand second order methods. We also observe DDPNOpt has surprising benefit in\npreventing gradient vanishing. Our work opens up new avenues for principled\nalgorithmic design built upon the optimal control theory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 15:42:15 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 16:51:50 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 21:47:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liu", "Guan-Horng", ""], ["Chen", "Tianrong", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "2002.08856", "submitter": "Thomas Flynn", "authors": "Thomas Flynn, Kwang Min Yu, Abid Malik, Nicolas D'Imperio, Shinjae Yoo", "title": "Bounding the expected run-time of nonconvex optimization with early\n  stopping", "comments": "Camera ready version for UAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the convergence of stochastic gradient-based optimization\nalgorithms that use early stopping based on a validation function. The form of\nearly stopping we consider is that optimization terminates when the norm of the\ngradient of a validation function falls below a threshold. We derive conditions\nthat guarantee this stopping rule is well-defined, and provide bounds on the\nexpected number of iterations and gradient evaluations needed to meet this\ncriterion. The guarantee accounts for the distance between the training and\nvalidation sets, measured with the Wasserstein distance. We develop the\napproach in the general setting of a first-order optimization algorithm, with\npossibly biased update directions subject to a geometric drift condition. We\nthen derive bounds on the expected running time for early stopping variants of\nseveral algorithms, including stochastic gradient descent (SGD), decentralized\nSGD (DSGD), and the stochastic variance reduced gradient (SVRG) algorithm.\nFinally, we consider the generalization properties of the iterate returned by\nearly stopping.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:43:37 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 20:50:55 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 15:27:06 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2020 17:56:23 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Flynn", "Thomas", ""], ["Yu", "Kwang Min", ""], ["Malik", "Abid", ""], ["D'Imperio", "Nicolas", ""], ["Yoo", "Shinjae", ""]]}, {"id": "2002.08867", "submitter": "Cristian Ramirez-Atencia", "authors": "Cristian Ramirez-Atencia and Sanaz Mostaghim and David Camacho", "title": "sKPNSGA-II: Knee point based MOEA with self-adaptive angle for Mission\n  Planning Problems", "comments": "Submitted to Applied Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world and complex problems have usually many objective functions that\nhave to be optimized all at once. Over the last decades, Multi-Objective\nEvolutionary Algorithms (MOEAs) are designed to solve this kind of problems.\nNevertheless, some problems have many objectives which lead to a large number\nof non-dominated solutions obtained by the optimization algorithms. The large\nset of non-dominated solutions hinders the selection of the most appropriate\nsolution by the decision maker. This paper presents a new algorithm that has\nbeen designed to obtain the most significant solutions from the Pareto Optimal\nFrontier (POF). This approach is based on the cone-domination applied to MOEA,\nwhich can find the knee point solutions. In order to obtain the best cone\nangle, we propose a hypervolume-distribution metric, which is used to\nself-adapt the angle during the evolving process. This new algorithm has been\napplied to the real world application in Unmanned Air Vehicle (UAV) Mission\nPlanning Problem. The experimental results show a significant improvement of\nthe algorithm performance in terms of hypervolume, number of solutions, and\nalso the required number of generations to converge.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:07:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Ramirez-Atencia", "Cristian", ""], ["Mostaghim", "Sanaz", ""], ["Camacho", "David", ""]]}, {"id": "2002.09063", "submitter": "Dario  Izzo", "authors": "Dario Izzo and Ekin \\\"Ozt\\\"urk", "title": "Real-Time Optimal Guidance and Control for Interplanetary Transfers\n  Using Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the Earth-Venus mass-optimal interplanetary transfer of a\nlow-thrust spacecraft and show how the optimal guidance can be represented by\ndeep networks in a large portion of the state space and to a high degree of\naccuracy. Imitation (supervised) learning of optimal examples is used as a\nnetwork training paradigm. The resulting models are suitable for an on-board,\nreal-time, implementation of the optimal guidance and control system of the\nspacecraft and are called G&CNETs. A new general methodology called Backward\nGeneration of Optimal Examples is introduced and shown to be able to\nefficiently create all the optimal state action pairs necessary to train\nG&CNETs without solving optimal control problems. With respect to previous\nworks, we are able to produce datasets containing a few orders of magnitude\nmore optimal trajectories and obtain network performances compatible with real\nmissions requirements. Several schemes able to train representations of either\nthe optimal policy (thrust profile) or the value function (optimal mass) are\nproposed and tested. We find that both policy learning and value function\nlearning successfully and accurately learn the optimal thrust and that a\nspacecraft employing the learned thrust is able to reach the target conditions\norbit spending only 2 permil more propellant than in the corresponding\nmathematically optimal transfer. Moreover, the optimal propellant mass can be\npredicted (in case of value function learning) within an error well within 1%.\nAll G&CNETs produced are tested during simulations of interplanetary transfers\nwith respect to their ability to reach the target conditions optimally starting\nfrom nominal and off-nominal conditions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 23:37:43 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Izzo", "Dario", ""], ["\u00d6zt\u00fcrk", "Ekin", ""]]}, {"id": "2002.09106", "submitter": "Mehdi Neshat", "authors": "Mehdi Neshat, Meysam Majidi Nezhad, Ehsan Abbasnejad, Lina Bertling\n  Tjernberg, Davide Astiaso Garcia, Bradley Alexander, Markus Wagner", "title": "An Evolutionary Deep Learning Method for Short-term Wind Speed\n  Prediction: A Case Study of the Lillgrund Offshore Wind Farm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate short-term wind speed forecasting is essential for large-scale\nintegration of wind power generation. However, the seasonal and stochastic\ncharacteristics of wind speed make forecasting a challenging task. This study\nuses a new hybrid evolutionary approach that uses a popular evolutionary search\nalgorithm, CMA-ES, to tune the hyper-parameters of two Long short-term\nmemory(LSTM) ANN models for wind prediction. The proposed hybrid approach is\ntrained on data gathered from an offshore wind turbine installed in a Swedish\nwind farm located in the Baltic Sea. Two forecasting horizons including\nten-minutes ahead (absolute short term) and one-hour ahead (short term) are\nconsidered in our experiments. Our experimental results indicate that the new\napproach is superior to five other applied machine learning models, i.e.,\npolynomial neural network (PNN), feed-forward neural network (FNN), nonlinear\nautoregressive neural network (NAR) and adaptive neuro-fuzzy inference system\n(ANFIS), as measured by five performance criteria.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 03:28:17 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Neshat", "Mehdi", ""], ["Nezhad", "Meysam Majidi", ""], ["Abbasnejad", "Ehsan", ""], ["Tjernberg", "Lina Bertling", ""], ["Garcia", "Davide Astiaso", ""], ["Alexander", "Bradley", ""], ["Wagner", "Markus", ""]]}, {"id": "2002.09227", "submitter": "Jacinto Carrasco", "authors": "J. Carrasco, S. Garc\\'ia, M.M. Rueda, S. Das and F. Herrera", "title": "Recent Trends in the Use of Statistical Tests for Comparing Swarm and\n  Evolutionary Computing Algorithms: Practical Guidelines and a Critical Review", "comments": "52 pages, 10 figures, 19 tables", "journal-ref": "SWEVO, Volume 54, May 2020, 100665", "doi": "10.1016/j.swevo.2020.100665", "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of the design of evolutionary and swarm intelligence algorithms\nis studying their performance. Statistical comparisons are also a crucial part\nwhich allows for reliable conclusions to be drawn. In the present paper we\ngather and examine the approaches taken from different perspectives to\nsummarise the assumptions made by these statistical tests, the conclusions\nreached and the steps followed to perform them correctly. In this paper, we\nconduct a survey on the current trends of the proposals of statistical analyses\nfor the comparison of algorithms of computational intelligence and include a\ndescription of the statistical background of these tests. We illustrate the use\nof the most common tests in the context of the Competition on single-objective\nreal parameter optimisation of the IEEE Congress on Evolutionary Computation\n(CEC) 2017 and describe the main advantages and drawbacks of the use of each\nkind of test and put forward some recommendations concerning their use.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 11:06:47 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Carrasco", "J.", ""], ["Garc\u00eda", "S.", ""], ["Rueda", "M. M.", ""], ["Das", "S.", ""], ["Herrera", "F.", ""]]}, {"id": "2002.09259", "submitter": "Theo Ladune", "authors": "Th\\'eo Ladune (IETR), Pierrick Philippe, Wassim Hamidouche (IETR), Lu\n  Zhang (IETR), Olivier Deforges (IETR)", "title": "Binary Probability Model for Learning Based Image Compression", "comments": null, "journal-ref": "International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2020, 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to enhance learned image compression systems with a\nricher probability model for the latent variables. Previous works model the\nlatents with a Gaussian or a Laplace distribution. Inspired by binary\narithmetic coding , we propose to signal the latents with three binary values\nand one integer, with different probability models. A relaxation method is\ndesigned to perform gradient-based training. The richer probability model\nresults in a better entropy coding leading to lower rate. Experiments under the\nChallenge on Learned Image Compression (CLIC) test conditions demonstrate that\nthis method achieves 18% rate saving compared to Gaussian or Laplace models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:09:58 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Ladune", "Th\u00e9o", "", "IETR"], ["Philippe", "Pierrick", "", "IETR"], ["Hamidouche", "Wassim", "", "IETR"], ["Zhang", "Lu", "", "IETR"], ["Deforges", "Olivier", "", "IETR"]]}, {"id": "2002.09285", "submitter": "Maxime Martineau", "authors": "Maxime Martineau, Romain Raveaux, Donatello Conte, Gilles Venturini", "title": "A Convolutional Neural Network into graph space", "comments": "arXiv admin note: text overlap with arXiv:1611.08402 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs), in a few decades, have outperformed the\nexisting state of the art methods in classification context. However, in the\nway they were formalised, CNNs are bound to operate on euclidean spaces.\nIndeed, convolution is a signal operation that are defined on euclidean spaces.\nThis has restricted deep learning main use to euclidean-defined data such as\nsound or image. And yet, numerous computer application fields (among which\nnetwork analysis, computational social science, chemo-informatics or computer\ngraphics) induce non-euclideanly defined data such as graphs, networks or\nmanifolds. In this paper we propose a new convolution neural network\narchitecture, defined directly into graph space. Convolution and pooling\noperators are defined in graph domain. We show its usability in a\nback-propagation context. Experimental results show that our model performance\nis at state of the art level on simple tasks. It shows robustness with respect\nto graph domain changes and improvement with respect to other euclidean and\nnon-euclidean convolutional architectures.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 15:14:21 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 12:59:14 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Martineau", "Maxime", ""], ["Raveaux", "Romain", ""], ["Conte", "Donatello", ""], ["Venturini", "Gilles", ""]]}, {"id": "2002.09286", "submitter": "M. Umut Isik", "authors": "Jonah Casebeer, Umut Isik, Shrikant Venkataramani, Arvindh\n  Krishnaswamy", "title": "Efficient Trainable Front-Ends for Neural Speech Enhancement", "comments": "5 pages, 5 figures, ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neural speech enhancement and source separation systems operate in the\ntime-frequency domain. Such models often benefit from making their Short-Time\nFourier Transform (STFT) front-ends trainable. In current literature, these are\nimplemented as large Discrete Fourier Transform matrices; which are\nprohibitively inefficient for low-compute systems. We present an efficient,\ntrainable front-end based on the butterfly mechanism to compute the Fast\nFourier Transform, and show its accuracy and efficiency benefits for\nlow-compute neural speech enhancement models. We also explore the effects of\nmaking the STFT window trainable.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 01:51:15 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Casebeer", "Jonah", ""], ["Isik", "Umut", ""], ["Venkataramani", "Shrikant", ""], ["Krishnaswamy", "Arvindh", ""]]}, {"id": "2002.09398", "submitter": "Moshe Gabel", "authors": "Gal Yehuda, Moshe Gabel, Assaf Schuster", "title": "It's Not What Machines Can Learn, It's What We Cannot Teach", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can deep neural networks learn to solve any task, and in particular problems\nof high complexity? This question attracts a lot of interest, with recent works\ntackling computationally hard tasks such as the traveling salesman problem and\nsatisfiability. In this work we offer a different perspective on this question.\nGiven the common assumption that $\\textit{NP} \\neq \\textit{coNP}$ we prove that\nany polynomial-time sample generator for an $\\textit{NP}$-hard problem samples,\nin fact, from an easier sub-problem. We empirically explore a case study,\nConjunctive Query Containment, and show how common data generation techniques\ngenerate biased datasets that lead practitioners to over-estimate model\naccuracy. Our results suggest that machine learning approaches that require\ntraining on a dense uniform sampling from the target distribution cannot be\nused to solve computationally hard problems, the reason being the difficulty of\ngenerating sufficiently large and unbiased training sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:26:55 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 16:43:06 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Yehuda", "Gal", ""], ["Gabel", "Moshe", ""], ["Schuster", "Assaf", ""]]}, {"id": "2002.09518", "submitter": "Kaveh Hassani", "authors": "Amir Hosein Khasahmadi, Kaveh Hassani, Parsa Moradi, Leo Lee, Quaid\n  Morris", "title": "Memory-Based Graph Networks", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are a class of deep models that operate on data\nwith arbitrary topology represented as graphs. We introduce an efficient memory\nlayer for GNNs that can jointly learn node representations and coarsen the\ngraph. We also introduce two new networks based on this layer: memory-based GNN\n(MemGNN) and graph memory network (GMN) that can learn hierarchical graph\nrepresentations. The experimental results shows that the proposed models\nachieve state-of-the-art results in eight out of nine graph classification and\nregression benchmarks. We also show that the learned representations could\ncorrespond to chemical features in the molecule data. Code and reference\nimplementations are released at: https://github.com/amirkhas/GraphMemoryNet\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:26:31 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 04:50:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Khasahmadi", "Amir Hosein", ""], ["Hassani", "Kaveh", ""], ["Moradi", "Parsa", ""], ["Lee", "Leo", ""], ["Morris", "Quaid", ""]]}, {"id": "2002.09571", "submitter": "Nicholas Cheney", "authors": "Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O.\n  Stanley, Jeff Clune, Nick Cheney", "title": "Learning to Continually Learn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual lifelong learning requires an agent or model to learn many\nsequentially ordered tasks, building on previous knowledge without\ncatastrophically forgetting it. Much work has gone towards preventing the\ndefault tendency of machine learning models to catastrophically forget, yet\nvirtually all such work involves manually-designed solutions to the problem. We\ninstead advocate meta-learning a solution to catastrophic forgetting, allowing\nAI to learn to continually learn. Inspired by neuromodulatory processes in the\nbrain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It\ndifferentiates through a sequential learning process to meta-learn an\nactivation-gating function that enables context-dependent selective activation\nwithin a deep neural network. Specifically, a neuromodulatory (NM) neural\nnetwork gates the forward pass of another (otherwise normal) neural network\ncalled the prediction learning network (PLN). The NM network also thus\nindirectly controls selective plasticity (i.e. the backward pass of) the PLN.\nANML enables continual learning without catastrophic forgetting at scale: it\nproduces state-of-the-art continual learning performance, sequentially learning\nas many as 600 classes (over 9,000 SGD updates).\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 22:52:00 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 03:22:48 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Beaulieu", "Shawn", ""], ["Frati", "Lapo", ""], ["Miconi", "Thomas", ""], ["Lehman", "Joel", ""], ["Stanley", "Kenneth O.", ""], ["Clune", "Jeff", ""], ["Cheney", "Nick", ""]]}, {"id": "2002.09706", "submitter": "TingTing Zhang", "authors": "Tingting Zhang, Yushi Lan, Aiguo Song, Kun Liu, Nan Wang", "title": "Structural Combinatorial of Network Information System of Systems based\n  on Evolutionary Optimization Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network information system is a military information network system with\nevolution characteristics. Evolution is a process of replacement between\ndisorder and order, chaos and equilibrium. Given that the concept of evolution\noriginates from biological systems, in this article, the evolution of network\ninformation architecture is analyzed by genetic algorithms, and the network\ninformation architecture is represented by chromosomes. Besides, the genetic\nalgorithm is also applied to find the optimal chromosome in the architecture\nspace. The evolutionary simulation is used to predict the optimal scheme of the\nnetwork information architecture and provide a reference for system\nconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 14:17:52 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Zhang", "Tingting", ""], ["Lan", "Yushi", ""], ["Song", "Aiguo", ""], ["Liu", "Kun", ""], ["Wang", "Nan", ""]]}, {"id": "2002.09815", "submitter": "Amirata Ghorbani", "authors": "Amirata Ghorbani and James Zou", "title": "Neuron Shapley: Discovering the Responsible Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Neuron Shapley as a new framework to quantify the contribution of\nindividual neurons to the prediction and performance of a deep network. By\naccounting for interactions across neurons, Neuron Shapley is more effective in\nidentifying important filters compared to common approaches based on activation\npatterns. Interestingly, removing just 30 filters with the highest Shapley\nscores effectively destroys the prediction accuracy of Inception-v3 on\nImageNet. Visualization of these few critical filters provides insights into\nhow the network functions. Neuron Shapley is a flexible framework and can be\napplied to identify responsible neurons in many tasks. We illustrate additional\napplications of identifying filters that are responsible for biased prediction\nin facial recognition and filters that are vulnerable to adversarial attacks.\nRemoving these filters is a quick way to repair models. Enabling all these\napplications is a new multi-arm bandit algorithm that we developed to\nefficiently estimate Neuron Shapley values.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 03:29:58 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 06:57:29 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 22:06:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Zou", "James", ""]]}, {"id": "2002.09854", "submitter": "Huanneng Qiu", "authors": "Huanneng Qiu, Matthew Garratt, David Howard and Sreenatha Anavatti", "title": "Towards Crossing the Reality Gap with Evolved Plastic Neurocontrollers", "comments": "GECCO2020", "journal-ref": null, "doi": "10.1145/3377930.3389843", "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical issue in evolutionary robotics is the transfer of controllers\nlearned in simulation to reality. This is especially the case for small\nUnmanned Aerial Vehicles (UAVs), as the platforms are highly dynamic and\nsusceptible to breakage. Previous approaches often require simulation models\nwith a high level of accuracy, otherwise significant errors may arise when the\nwell-designed controller is being deployed onto the targeted platform. Here we\ntry to overcome the transfer problem from a different perspective, by designing\na spiking neurocontroller which uses synaptic plasticity to cross the reality\ngap via online adaptation. Through a set of experiments we show that the\nevolved plastic spiking controller can maintain its functionality by\nself-adapting to model changes that take place after evolutionary training, and\nconsequently exhibit better performance than its non-plastic counterpart.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 07:44:42 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 03:57:21 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Qiu", "Huanneng", ""], ["Garratt", "Matthew", ""], ["Howard", "David", ""], ["Anavatti", "Sreenatha", ""]]}, {"id": "2002.09860", "submitter": "Andrea Asperti", "authors": "Andrea Asperti", "title": "Variance Loss in Variational Autoencoders", "comments": "Article accepted at the Sixth International Conference on Machine\n  Learning, Optimization, and Data Science. July 19-23, 2020 - Certosa di\n  Pontignano, Siena, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we highlight what appears to be major issue of Variational\nAutoencoders, evinced from an extensive experimentation with different network\narchitectures and datasets: the variance of generated data is significantly\nlower than that of training data. Since generative models are usually evaluated\nwith metrics such as the Frechet Inception Distance (FID) that compare the\ndistributions of (features of) real versus generated images, the variance loss\ntypically results in degraded scores. This problem is particularly relevant in\na two stage setting, where we use a second VAE to sample in the latent space of\nthe first VAE. The minor variance creates a mismatch between the actual\ndistribution of latent variables and those generated by the second VAE, that\nhinders the beneficial effects of the second stage. Renormalizing the output of\nthe second VAE towards the expected normal spherical distribution, we obtain a\nsudden burst in the quality of generated samples, as also testified in terms of\nFID.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 08:23:51 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 20:55:37 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Asperti", "Andrea", ""]]}, {"id": "2002.09958", "submitter": "Sai Aparna Aketi", "authors": "Sai Aparna Aketi, Sourjya Roy, Anand Raghunathan, Kaushik Roy", "title": "Gradual Channel Pruning while Training using Feature Relevance Scores\n  for Convolutional Neural Networks", "comments": "15 pages, 2 figures, 4 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3024992", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enormous inference cost of deep neural networks can be scaled down by\nnetwork compression. Pruning is one of the predominant approaches used for deep\nnetwork compression. However, existing pruning techniques have one or more of\nthe following limitations: 1) Additional energy cost on top of the compute\nheavy training stage due to pruning and fine-tuning stages, 2) Layer-wise\npruning based on the statistics of a particular, ignoring the effect of error\npropagation in the network, 3) Lack of an efficient estimate for determining\nthe important channels globally, 4) Unstructured pruning requires specialized\nhardware for effective use. To address all the above issues, we present a\nsimple-yet-effective gradual channel pruning while training methodology using a\nnovel data-driven metric referred to as feature relevance score. The proposed\ntechnique gets rid of the additional retraining cycles by pruning the least\nimportant channels in a structured fashion at fixed intervals during the actual\ntraining phase. Feature relevance scores help in efficiently evaluating the\ncontribution of each channel towards the discriminative power of the network.\nWe demonstrate the effectiveness of the proposed methodology on architectures\nsuch as VGG and ResNet using datasets such as CIFAR-10, CIFAR-100 and ImageNet,\nand successfully achieve significant model compression while trading off less\nthan $1\\%$ accuracy. Notably on CIFAR-10 dataset trained on ResNet-110, our\napproach achieves $2.4\\times$ compression and a $56\\%$ reduction in FLOPs with\nan accuracy drop of $0.01\\%$ compared to the unpruned network.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 17:56:18 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 15:01:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Aketi", "Sai Aparna", ""], ["Roy", "Sourjya", ""], ["Raghunathan", "Anand", ""], ["Roy", "Kaushik", ""]]}, {"id": "2002.09970", "submitter": "Mario Krenn", "authors": "Mario Krenn, Manuel Erhard, Anton Zeilinger", "title": "Computer-inspired Quantum Experiments", "comments": "Comments and suggestions for additional references are welcome!", "journal-ref": "Nature Reviews Physics 2, 649-661 (2020)", "doi": "10.1038/s42254-020-0230-4", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of new devices and experiments in science and engineering has\nhistorically relied on the intuitions of human experts. This credo, however,\nhas changed. In many disciplines, computer-inspired design processes, also\nknown as inverse-design, have augmented the capability of scientists. Here we\nvisit different fields of physics in which computer-inspired designs are\napplied. We will meet vastly diverse computational approaches based on\ntopological optimization, evolutionary strategies, deep learning, reinforcement\nlearning or automated reasoning. Then we draw our attention specifically on\nquantum physics. In the quest for designing new quantum experiments, we face\ntwo challenges: First, quantum phenomena are unintuitive. Second, the number of\npossible configurations of quantum experiments explodes combinatorially. To\novercome these challenges, physicists began to use algorithms for\ncomputer-designed quantum experiments. We focus on the most mature and\n\\textit{practical} approaches that scientists used to find new complex quantum\nexperiments, which experimentalists subsequently have realized in the\nlaboratories. The underlying idea is a highly-efficient topological search,\nwhich allows for scientific interpretability. In that way, some of the\ncomputer-designs have led to the discovery of new scientific concepts and ideas\n-- demonstrating how computer algorithm can genuinely contribute to science by\nproviding unexpected inspirations. We discuss several extensions and\nalternatives based on optimization and machine learning techniques, with the\npotential of accelerating the discovery of practical computer-inspired\nexperiments or concepts in the future. Finally, we discuss what we can learn\nfrom the different approaches in the fields of physics, and raise several\nfascinating possibilities for future research.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 18:59:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Krenn", "Mario", ""], ["Erhard", "Manuel", ""], ["Zeilinger", "Anton", ""]]}, {"id": "2002.10085", "submitter": "Wenrui Zhang", "authors": "Wenrui Zhang, Peng Li", "title": "Temporal Spike Sequence Learning via Backpropagation for Deep Spiking\n  Neural Networks", "comments": "Accepted for spotlight presentation of NeurIPS (Neural Information\n  Processing System) 2020:\n  https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are well suited for spatio-temporal learning\nand implementations on energy-efficient event-driven neuromorphic processors.\nHowever, existing SNN error backpropagation (BP) methods lack proper handling\nof spiking discontinuities and suffer from low performance compared with the BP\nmethods for traditional artificial neural networks. In addition, a large number\nof time steps are typically required to achieve decent performance, leading to\nhigh latency and rendering spike-based computation unscalable to deep\narchitectures. We present a novel Temporal Spike Sequence Learning\nBackpropagation (TSSL-BP) method for training deep SNNs, which breaks down\nerror backpropagation across two types of inter-neuron and intra-neuron\ndependencies and leads to improved temporal learning precision. It captures\ninter-neuron dependencies through presynaptic firing times by considering the\nall-or-none characteristics of firing activities and captures intra-neuron\ndependencies by handling the internal evolution of each neuronal state in time.\nTSSL-BP efficiently trains deep SNNs within a much shortened temporal window of\na few steps while improving the accuracy for various image classification\ndatasets including CIFAR10.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 05:49:37 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 07:41:13 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 21:42:18 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 06:24:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Wenrui", ""], ["Li", "Peng", ""]]}, {"id": "2002.10090", "submitter": "Junfei Zhang", "authors": "Junfei Zhang, Yimiao Huang, Guowei Ma, Brett Nener", "title": "Multi-objective beetle antennae search algorithm", "comments": "5 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering optimization problems, multiple objectives with a large number\nof variables under highly nonlinear constraints are usually required to be\nsimultaneously optimized. Significant computing effort are required to find the\nPareto front of a nonlinear multi-objective optimization problem. Swarm\nintelligence based metaheuristic algorithms have been successfully applied to\nsolve multi-objective optimization problems. Recently, an individual\nintelligence based algorithm called beetle antennae search algorithm was\nproposed. This algorithm was proved to be more computationally efficient.\nTherefore, we extended this algorithm to solve multi-objective optimization\nproblems. The proposed multi-objective beetle antennae search algorithm is\ntested using four well-selected benchmark functions and its performance is\ncompared with other multi-objective optimization algorithms. The results show\nthat the proposed multi-objective beetle antennae search algorithm has higher\ncomputational efficiency with satisfactory accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 06:34:32 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 00:23:00 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Zhang", "Junfei", ""], ["Huang", "Yimiao", ""], ["Ma", "Guowei", ""], ["Nener", "Brett", ""]]}, {"id": "2002.10177", "submitter": "Ioan Marius Bilasco PhD", "authors": "Pierre Falez and Pierre Tirilly and Ioan Marius Bilasco", "title": "Improving STDP-based Visual Feature Learning with Whitening", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207373", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spiking neural networks (SNNs) emerge as an alternative to\ndeep neural networks (DNNs). SNNs present a higher computational efficiency\nusing low-power neuromorphic hardware and require less labeled data for\ntraining using local and unsupervised learning rules such as spike\ntiming-dependent plasticity (STDP). SNN have proven their effectiveness in\nimage classification on simple datasets such as MNIST. However, to process\nnatural images, a pre-processing step is required. Difference-of-Gaussians\n(DoG) filtering is typically used together with on-center/off-center coding,\nbut it results in a loss of information that is detrimental to the\nclassification performance. In this paper, we propose to use whitening as a\npre-processing step before learning features with STDP. Experiments on CIFAR-10\nshow that whitening allows STDP to learn visual features that are closer to the\nones learned with standard neural networks, with a significantly increased\nclassification performance as compared to DoG filtering. We also propose an\napproximation of whitening as convolution kernels that is computationally\ncheaper to learn and more suited to be implemented on neuromorphic hardware.\nExperiments on CIFAR-10 show that it performs similarly to regular whitening.\nCross-dataset experiments on CIFAR-10 and STL-10 also show that it is fairly\nstable across datasets, making it possible to learn a single whitening\ntransformation to process different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:48:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Falez", "Pierre", ""], ["Tirilly", "Pierre", ""], ["Bilasco", "Ioan Marius", ""]]}, {"id": "2002.10228", "submitter": "Srikanth Chandar", "authors": "Srikanth Chandar and Harsha Sunder", "title": "Dynamic Systems Simulation and Control Using Consecutive Recurrent\n  Neural Networks", "comments": "14 pages, granted for publication in Communications in Computer and\n  Information Science (CCIS) proceedings by Springer Nature, presented in the\n  International Conference on Modelling, Machine Learning and Astronomy (MMLA\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel architecture to connecting adaptive\nlearning and neural networks into an arbitrary machine's control system\nparadigm. Two consecutive Recurrent Neural Networks (RNNs) are used together to\naccurately model the dynamic characteristics of electromechanical systems that\ninclude controllers, actuators and motors. The age-old method of achieving\ncontrol with the use of the- Proportional, Integral and Derivative constants is\nwell understood as a simplified method that does not capture the complexities\nof the inherent nonlinearities of complex control systems. In the context of\ncontrolling and simulating electromechanical systems, we propose an alternative\nto PID controllers, employing a sequence of two Recurrent Neural Networks. The\nfirst RNN emulates the behavior of the controller, and the second the\nactuator/motor. The second RNN when used in isolation, potentially serves as an\nadvantageous alternative to extant testing methods of electromechanical\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:08:44 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 01:57:06 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Chandar", "Srikanth", ""], ["Sunder", "Harsha", ""]]}, {"id": "2002.10295", "submitter": "David P\\\"atzel", "authors": "Michael Heider and David P\\\"atzel and J\\\"org H\\\"ahner", "title": "SupRB: A Supervised Rule-based Learning System for Continuous Problems", "comments": "Submitted to the Genetic and Evolutionary Computation Conference 2020\n  (GECCO 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the SupRB learning system, a new Pittsburgh-style learning\nclassifier system (LCS) for supervised learning on multi-dimensional continuous\ndecision problems. SupRB learns an approximation of a quality function from\nexamples (consisting of situations, choices and associated qualities) and is\nthen able to make an optimal choice as well as predict the quality of a choice\nin a given situation. One area of application for SupRB is parametrization of\nindustrial machinery. In this field, acceptance of the recommendations of\nmachine learning systems is highly reliant on operators' trust. While an\nessential and much-researched ingredient for that trust is prediction quality,\nit seems that this alone is not enough. At least as important is a\nhuman-understandable explanation of the reasoning behind a recommendation.\nWhile many state-of-the-art methods such as artificial neural networks fall\nshort of this, LCSs such as SupRB provide human-readable rules that can be\nunderstood very easily. The prevalent LCSs are not directly applicable to this\nproblem as they lack support for continuous choices. This paper lays the\nfoundations for SupRB and shows its general applicability on a simplified model\nof an additive manufacturing problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:54:54 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Heider", "Michael", ""], ["P\u00e4tzel", "David", ""], ["H\u00e4hner", "J\u00f6rg", ""]]}, {"id": "2002.10365", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle and David J. Schwab and Ari S. Morcos", "title": "The Early Phase of Neural Network Training", "comments": "ICLR 2020 Camera Ready. Available on OpenReview at\n  https://openreview.net/forum?id=Hkl1iRNFwS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that many important aspects of neural network\nlearning take place within the very earliest iterations or epochs of training.\nFor example, sparse, trainable sub-networks emerge (Frankle et al., 2019),\ngradient descent moves into a small subspace (Gur-Ari et al., 2018), and the\nnetwork undergoes a critical period (Achille et al., 2019). Here, we examine\nthe changes that deep neural networks undergo during this early phase of\ntraining. We perform extensive measurements of the network state during these\nearly iterations of training and leverage the framework of Frankle et al.\n(2019) to quantitatively probe the weight distribution and its reliance on\nvarious aspects of the dataset. We find that, within this framework, deep\nnetworks are not robust to reinitializing with random weights while maintaining\nsigns, and that weight distributions are highly non-independent even after only\na few hundred iterations. Despite this behavior, pre-training with blurred\ninputs or an auxiliary self-supervised task can approximate the changes in\nsupervised networks, suggesting that these changes are not inherently\nlabel-dependent, though labels significantly accelerate this process. Together,\nthese results help to elucidate the network changes occurring during this\npivotal initial period of learning.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:51:01 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Frankle", "Jonathan", ""], ["Schwab", "David J.", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2002.10451", "submitter": "Mayank Mittal", "authors": "Mayank Mittal, Marco Gallieri, Alessio Quaglino, Seyed Sina Mirrazavi\n  Salehian, Jan Koutn\\'ik", "title": "Neural Lyapunov Model Predictive Control: Learning Safe Global\n  Controllers from Sub-optimal Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a growing interest in data-driven control techniques, Model Predictive\nControl (MPC) provides an opportunity to exploit the surplus of data reliably,\nparticularly while taking safety and stability into account. In many real-world\nand industrial applications, it is typical to have an existing control\nstrategy, for instance, execution from a human operator. The objective of this\nwork is to improve upon this unknown, safe but suboptimal policy by learning a\nnew controller that retains safety and stability. Learning how to be safe is\nachieved directly from data and from a knowledge of the system constraints. The\nproposed algorithm alternatively learns the terminal cost and updates the MPC\nparameters according to a stability metric. The terminal cost is constructed as\na Lyapunov function neural network with the aim of recovering or extending the\nstable region of the initial demonstrator using a short prediction horizon.\nTheorems that characterize the stability and performance of the learned MPC in\nthe bearing of model uncertainties and sub-optimality due to function\napproximation are presented. The efficacy of the proposed algorithm is\ndemonstrated on non-linear continuous control tasks with soft constraints. The\nproposed approach can improve upon the initial demonstrator also in practice\nand achieve better stability than popular reinforcement learning baselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:57:38 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:37:05 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Mittal", "Mayank", ""], ["Gallieri", "Marco", ""], ["Quaglino", "Alessio", ""], ["Salehian", "Seyed Sina Mirrazavi", ""], ["Koutn\u00edk", "Jan", ""]]}, {"id": "2002.10583", "submitter": "Tan Nguyen", "authors": "Bao Wang, Tan M. Nguyen, Andrea L. Bertozzi, Richard G. Baraniuk,\n  Stanley J. Osher", "title": "Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent", "comments": "35 pages, 16 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) with constant momentum and its variants\nsuch as Adam are the optimization algorithms of choice for training deep neural\nnetworks (DNNs). Since DNN training is incredibly computationally expensive,\nthere is great interest in speeding up the convergence. Nesterov accelerated\ngradient (NAG) improves the convergence rate of gradient descent (GD) for\nconvex optimization using a specially designed momentum; however, it\naccumulates error when an inexact gradient is used (such as in SGD), slowing\nconvergence at best and diverging at worst. In this paper, we propose Scheduled\nRestart SGD (SRSGD), a new NAG-style scheme for training DNNs. SRSGD replaces\nthe constant momentum in SGD by the increasing momentum in NAG but stabilizes\nthe iterations by resetting the momentum to zero according to a schedule. Using\na variety of models and benchmarks for image classification, we demonstrate\nthat, in training DNNs, SRSGD significantly improves convergence and\ngeneralization; for instance in training ResNet200 for ImageNet classification,\nSRSGD achieves an error rate of 20.93% vs. the benchmark of 22.13%. These\nimprovements become more significant as the network grows deeper. Furthermore,\non both CIFAR and ImageNet, SRSGD reaches similar or even better error rates\nwith significantly fewer training epochs compared to the SGD baseline.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 23:16:19 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 11:55:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Bao", ""], ["Nguyen", "Tan M.", ""], ["Bertozzi", "Andrea L.", ""], ["Baraniuk", "Richard G.", ""], ["Osher", "Stanley J.", ""]]}, {"id": "2002.10585", "submitter": "Thomas Miconi", "authors": "Thomas Miconi and Aditya Rawal and Jeff Clune and Kenneth O. Stanley", "title": "Backpropamine: training self-modifying neural networks with\n  differentiable neuromodulated plasticity", "comments": "Presented at the 7th International Conference on Learning\n  Representations (ICLR 2019)", "journal-ref": "7th International Conference on Learning Representations, ICLR\n  2019, New Orleans, LA, USA, May 6-9, 2019", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 23:19:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Miconi", "Thomas", ""], ["Rawal", "Aditya", ""], ["Clune", "Jeff", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "2002.10636", "submitter": "Daniel Bedau", "authors": "Wen Ma, Pi-Feng Chiu, Won Ho Choi, Minghai Qin, Daniel Bedau, Martin\n  Lueker-Boden", "title": "Non-Volatile Memory Array Based Quantization- and Noise-Resilient LSTM\n  Neural Networks", "comments": "Published in: 2019 IEEE International Conference on Rebooting\n  Computing (ICRC)", "journal-ref": null, "doi": "10.1109/ICRC.2019.8914713", "report-no": null, "categories": "cs.NE cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In cloud and edge computing models, it is important that compute devices at\nthe edge be as power efficient as possible. Long short-term memory (LSTM)\nneural networks have been widely used for natural language processing, time\nseries prediction and many other sequential data tasks. Thus, for these\napplications there is increasing need for low-power accelerators for LSTM model\ninference at the edge. In order to reduce power dissipation due to data\ntransfers within inference devices, there has been significant interest in\naccelerating vector-matrix multiplication (VMM) operations using non-volatile\nmemory (NVM) weight arrays. In NVM array-based hardware, reduced bit-widths\nalso significantly increases the power efficiency. In this paper, we focus on\nthe application of quantization-aware training algorithm to LSTM models, and\nthe benefits these models bring in terms of resilience against both\nquantization error and analog device noise. We have shown that only 4-bit NVM\nweights and 4-bit ADC/DACs are needed to produce equivalent LSTM network\nperformance as floating-point baseline. Reasonable levels of ADC quantization\nnoise and weight noise can be naturally tolerated within our NVMbased quantized\nLSTM network. Benchmark analysis of our proposed LSTM accelerator for inference\nhas shown at least 2.4x better computing efficiency and 40x higher area\nefficiency than traditional digital approaches (GPU, FPGA, and ASIC). Some\nother novel approaches based on NVM promise to deliver higher computing\nefficiency (up to 4.7x) but require larger arrays with potential higher error\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 02:59:45 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ma", "Wen", ""], ["Chiu", "Pi-Feng", ""], ["Choi", "Won Ho", ""], ["Qin", "Minghai", ""], ["Bedau", "Daniel", ""], ["Lueker-Boden", "Martin", ""]]}, {"id": "2002.10674", "submitter": "Elaina Chai", "authors": "Elaina Chai, Mert Pilanci, Boris Murmann", "title": "Separating the Effects of Batch Normalization on CNN Training Speed and\n  Stability Using Classical Adaptive Filter Theory", "comments": "Presented at Asilomar Conference on Signals, Systems, and Computers,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BatchNorm) is commonly used in Convolutional Neural\nNetworks (CNNs) to improve training speed and stability. However, there is\nstill limited consensus on why this technique is effective. This paper uses\nconcepts from the traditional adaptive filter domain to provide insight into\nthe dynamics and inner workings of BatchNorm. First, we show that the\nconvolution weight updates have natural modes whose stability and convergence\nspeed are tied to the eigenvalues of the input autocorrelation matrices, which\nare controlled by BatchNorm through the convolution layers' channel-wise\nstructure. Furthermore, our experiments demonstrate that the speed and\nstability benefits are distinct effects. At low learning rates, it is\nBatchNorm's amplification of the smallest eigenvalues that improves convergence\nspeed, while at high learning rates, it is BatchNorm's suppression of the\nlargest eigenvalues that ensures stability. Lastly, we prove that in the first\ntraining step, when normalization is needed most, BatchNorm satisfies the same\noptimization as Normalized Least Mean Square (NLMS), while it continues to\napproximate this condition in subsequent steps. The analyses provided in this\npaper lay the groundwork for gaining further insight into the operation of\nmodern neural network structures using adaptive filter theory.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 05:25:40 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 11:22:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Chai", "Elaina", ""], ["Pilanci", "Mert", ""], ["Murmann", "Boris", ""]]}, {"id": "2002.10842", "submitter": "Elizabeth Wanner Dr", "authors": "Claudio Lucio do Val Lopes, Fl\\'avio Vin\\'icius Cruzeiro Martins,\n  Elizabeth F. Wanner", "title": "An Assignment Problem Formulation for Dominance Move Indicator", "comments": "arXiv admin note: text overlap with arXiv:2001.03657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dominance move (DoM) is a binary quality indicator to compare solution sets\nin multiobjective optimization. The indicator allows a more natural and\nintuitive relation when comparing solution sets. It is Pareto compliant and\ndoes not demand any parameters or reference sets. In spite of its advantages,\nthe combinatorial calculation nature is a limitation. The original formulation\npresents an efficient method to calculate it in a biobjective case only. This\nwork presents an assignment formulation to calculate DoM in problems with three\nobjectives or more. Some initial experiments, in the biobjective space, were\ndone to present the model correctness. Next, other experiments, using three\ndimensions, were also done to show how DoM could be compared with other\nindicators: inverted generational distance (IGD) and hypervolume (HV). Results\nshow the assignment formulation for DoM is valid for more than three\nobjectives. However, there are some strengths and weaknesses, which are\ndiscussed and detailed. Some notes, considerations, and future research paths\nconclude this work.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 13:04:55 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:48:52 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lopes", "Claudio Lucio do Val", ""], ["Martins", "Fl\u00e1vio Vin\u00edcius Cruzeiro", ""], ["Wanner", "Elizabeth F.", ""]]}, {"id": "2002.11152", "submitter": "Carole Twining Dr", "authors": "Neil A. Thacker, Carole J. Twining, Paul D. Tar, Scott Notley and\n  Visvanathan Ramesh", "title": "Fundamental Issues Regarding Uncertainties in Artificial Neural Networks", "comments": "21 pages, 8 Figures, 2 Tables. To be submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) implement a specific form of multi-variate\nextrapolation and will generate an output for any input pattern, even when\nthere is no similar training pattern. Extrapolations are not necessarily to be\ntrusted, and in order to support safety critical systems, we require such\nsystems to give an indication of the training sample related uncertainty\nassociated with their output. Some readers may think that this is a well known\nissue which is already covered by the basic principles of pattern recognition.\nWe will explain below how this is not the case and how the conventional\n(Likelihood estimate of) conditional probability of classification does not\ncorrectly assess this uncertainty. We provide a discussion of the standard\ninterpretations of this problem and show how a quantitative approach based upon\nlong standing methods can be practically applied. The methods are illustrated\non the task of early diagnosis of dementing diseases using Magnetic Resonance\nImaging.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 19:32:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Thacker", "Neil A.", ""], ["Twining", "Carole J.", ""], ["Tar", "Paul D.", ""], ["Notley", "Scott", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "2002.11318", "submitter": "Sandesh Kamath K", "authors": "Sandesh Kamath, Amit Deshpande, K V Subrahmanyam, Vineeth N\n  Balasubramanian", "title": "Can we have it all? On the Trade-off between Spatial and Adversarial\n  Robustness of Neural Networks", "comments": "Preliminary version consisting early experimental results was\n  presented in ICML 2018 Workshop on \"Towards learning with limited labels:\n  Equivariance, Invariance,and Beyond\" as \"Understanding Adversarial Robustness\n  of Symmetric Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  (Non-)robustness of neural networks to small, adversarial pixel-wise\nperturbations, and as more recently shown, to even random spatial\ntransformations (e.g., translations, rotations) entreats both theoretical and\nempirical understanding. Spatial robustness to random translations and\nrotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)\nand training augmentation, whereas adversarial robustness is typically achieved\nby adversarial training. In this paper, we prove a quantitative trade-off\nbetween spatial and adversarial robustness in a simple statistical setting. We\ncomplement this empirically by showing that: (a) as the spatial robustness of\nequivariant models improves by training augmentation with progressively larger\ntransformations, their adversarial robustness worsens progressively, and (b) as\nthe state-of-the-art robust models are adversarially trained with progressively\nlarger pixel-wise perturbations, their spatial robustness drops progressively.\nTowards achieving pareto-optimality in this trade-off, we propose a method\nbased on curriculum learning that trains gradually on more difficult\nperturbations (both spatial and adversarial) to improve spatial and adversarial\nrobustness simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:25:06 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:32:03 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 06:46:08 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 16:28:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kamath", "Sandesh", ""], ["Deshpande", "Amit", ""], ["Subrahmanyam", "K V", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2002.11319", "submitter": "Milo M. Lin", "authors": "Paul J. Blazek, Milo M. Lin", "title": "A neural network model of perception and reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How perception and reasoning arise from neuronal network activity is poorly\nunderstood. This is reflected in the fundamental limitations of connectionist\nartificial intelligence, typified by deep neural networks trained via\ngradient-based optimization. Despite success on many tasks, such networks\nremain unexplainable black boxes incapable of symbolic reasoning and concept\ngeneralization. Here we show that a simple set of biologically consistent\norganizing principles confer these capabilities to neuronal networks. To\ndemonstrate, we implement these principles in a novel machine learning\nalgorithm, based on concept construction instead of optimization, to design\ndeep neural networks that reason with explainable neuron activity. On a range\nof tasks including NP-hard problems, their reasoning capabilities grant\nadditional cognitive functions, like deliberating through self-analysis,\ntolerating adversarial attacks, and learning transferable rules from simple\nexamples to solve problems of unencountered complexity. The networks also\nnaturally display properties of biological nervous systems inherently absent in\ncurrent deep neural networks, including sparsity, modularity, and both\ndistributed and localized firing patterns. Because they do not sacrifice\nperformance, compactness, or training time on standard learning tasks, these\nnetworks provide a new black-box-free approach to artificial intelligence. They\nlikewise serve as a quantitative framework to understand the emergence of\ncognition from neuronal networks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:26:04 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Blazek", "Paul J.", ""], ["Lin", "Milo M.", ""]]}, {"id": "2002.11338", "submitter": "Zhanzhan Cheng", "authors": "Zhanzhan Cheng, Yunlu Xu, Mingjian Cheng, Yu Qiao, Shiliang Pu, Yi Niu\n  and Fei Wu", "title": "Refined Gate: A Simple and Effective Gating Mechanism for Recurrent\n  Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recurrent neural network (RNN) has been widely studied in sequence learning\ntasks, while the mainstream models (e.g., LSTM and GRU) rely on the gating\nmechanism (in control of how information flows between hidden states). However,\nthe vanilla gates in RNN (e.g., the input gate in LSTM) suffer from the problem\nof gate undertraining, which can be caused by various factors, such as the\nsaturating activation functions, the gate layouts (e.g., the gate number and\ngating functions), or even the suboptimal memory state etc.. Those may result\nin failures of learning gating switch roles and thus the weak performance. In\nthis paper, we propose a new gating mechanism within general gated recurrent\nneural networks to handle this issue. Specifically, the proposed gates directly\nshort connect the extracted input features to the outputs of vanilla gates,\ndenoted as refined gates. The refining mechanism allows enhancing gradient\nback-propagation as well as extending the gating activation scope, which can\nguide RNN to reach possibly deeper minima. We verify the proposed gating\nmechanism on three popular types of gated RNNs including LSTM, GRU and MGU.\nExtensive experiments on 3 synthetic tasks, 3 language modeling tasks and 5\nscene text recognition benchmarks demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 07:51:38 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:59:48 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Cheng", "Zhanzhan", ""], ["Xu", "Yunlu", ""], ["Cheng", "Mingjian", ""], ["Qiao", "Yu", ""], ["Pu", "Shiliang", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2002.11523", "submitter": "Evgeny Ponomarev", "authors": "Evgeny Ponomarev, Ivan Oseledets, Andrzej Cichocki", "title": "Using Reinforcement Learning in the Algorithmic Trading Problem", "comments": null, "journal-ref": "ISSN 1064-2269, Journal of Communications Technology and\n  Electronics, 2019, Vol. 64, No. 12, pp. 1450-1457", "doi": "10.1134/S1064226919120131", "report-no": null, "categories": "q-fin.TR cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of reinforced learning methods has extended application to\nmany areas including algorithmic trading. In this paper trading on the stock\nexchange is interpreted into a game with a Markov property consisting of\nstates, actions, and rewards. A system for trading the fixed volume of a\nfinancial instrument is proposed and experimentally tested; this is based on\nthe asynchronous advantage actor-critic method with the use of several neural\nnetwork architectures. The application of recurrent layers in this approach is\ninvestigated. The experiments were performed on real anonymized data. The best\narchitecture demonstrated a trading strategy for the RTS Index futures\n(MOEX:RTSI) with a profitability of 66% per annum accounting for commission.\nThe project source code is available via the following link:\nhttp://github.com/evgps/a3c_trading.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 14:30:18 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ponomarev", "Evgeny", ""], ["Oseledets", "Ivan", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "2002.11571", "submitter": "Artjom Zern", "authors": "Artjom Zern, Alexander Zeilmann, Christoph Schn\\\"orr", "title": "Assignment Flows for Data Labeling on Graphs: Convergence and Stability", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assignment flow recently introduced in the J. Math. Imaging and Vision\n58/2 (2017), constitutes a high-dimensional dynamical system that evolves on an\nelementary statistical manifold and performs contextual labeling\n(classification) of data given in any metric space. Vertices of a given graph\nindex the data points and define a system of neighborhoods. These neighborhoods\ntogether with nonnegative weight parameters define regularization of the\nevolution of label assignments to data points, through geometric averaging\ninduced by the affine e-connection of information geometry. Regarding\nevolutionary game dynamics, the assignment flow may be characterized as a large\nsystem of replicator equations that are coupled by geometric averaging. This\npaper establishes conditions on the weight parameters that guarantee\nconvergence of the continuous-time assignment flow to integral assignments\n(labelings), up to a negligible subset of situations that will not be\nencountered when working with real data in practice. Furthermore, we classify\nattractors of the flow and quantify corresponding basins of attraction. This\nprovides convergence guarantees for the assignment flow which are extended to\nthe discrete-time assignment flow that results from applying a\nRunge-Kutta-Munthe-Kaas scheme for numerical geometric integration of the\nassignment flow. Several counter-examples illustrate that violating the\nconditions may entail unfavorable behavior of the assignment flow regarding\ncontextual data classification.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:45:38 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zern", "Artjom", ""], ["Zeilmann", "Alexander", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "2002.11656", "submitter": "Raymond Baldwin", "authors": "R Wes Baldwin, Mohammed Almatrafi, Jason R Kaufman, Vijayan Asari,\n  Keigo Hirakawa", "title": "Inceptive Event Time-Surfaces for Object Classification Using\n  Neuromorphic Cameras", "comments": null, "journal-ref": "Image Analysis and Recognition. ICIAR 2019. Lecture Notes in\n  Computer Science, vol 11663. Springer, Cham", "doi": "10.1007/978-3-030-27272-2_35", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel fusion of low-level approaches for dimensionality\nreduction into an effective approach for high-level objects in neuromorphic\ncamera data called Inceptive Event Time-Surfaces (IETS). IETSs overcome several\nlimitations of conventional time-surfaces by increasing robustness to noise,\npromoting spatial consistency, and improving the temporal localization of\n(moving) edges. Combining IETS with transfer learning improves state-of-the-art\nperformance on the challenging problem of object classification utilizing event\ncamera data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:36:27 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Baldwin", "R Wes", ""], ["Almatrafi", "Mohammed", ""], ["Kaufman", "Jason R", ""], ["Asari", "Vijayan", ""], ["Hirakawa", "Keigo", ""]]}, {"id": "2002.11708", "submitter": "Alexander Li", "authors": "Alexander C. Li, Lerrel Pinto, Pieter Abbeel", "title": "Generalized Hindsight for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key reasons for the high sample complexity in reinforcement\nlearning (RL) is the inability to transfer knowledge from one task to another.\nIn standard multi-task RL settings, low-reward data collected while trying to\nsolve one task provides little to no signal for solving that particular task\nand is hence effectively wasted. However, we argue that this data, which is\nuninformative for one task, is likely a rich source of information for other\ntasks. To leverage this insight and efficiently reuse data, we present\nGeneralized Hindsight: an approximate inverse reinforcement learning technique\nfor relabeling behaviors with the right tasks. Intuitively, given a behavior\ngenerated under one task, Generalized Hindsight returns a different task that\nthe behavior is better suited for. Then, the behavior is relabeled with this\nnew task before being used by an off-policy RL optimizer. Compared to standard\nrelabeling techniques, Generalized Hindsight provides a substantially more\nefficient reuse of samples, which we empirically demonstrate on a suite of\nmulti-task navigation and manipulation tasks. Videos and code can be accessed\nhere: https://sites.google.com/view/generalized-hindsight.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:57:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Li", "Alexander C.", ""], ["Pinto", "Lerrel", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2002.11843", "submitter": "Ruthvik Vaila", "authors": "Ruthvik Vaila, John Chiasson, Vishal Saxena", "title": "A Deep Unsupervised Feature Learning Spiking Neural Network with\n  Binarized Classification Layers for EMNIST Classification using SpykeFlow", "comments": "A section of of this work is Submitted to IEEE TETCI 2020 Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 23:47:35 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 18:35:35 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 13:59:09 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 12:54:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Vaila", "Ruthvik", ""], ["Chiasson", "John", ""], ["Saxena", "Vishal", ""]]}, {"id": "2002.11847", "submitter": "Ankush Garg", "authors": "Ankush Garg, Yuan Cao, and Qi Ge", "title": "Echo State Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present neural machine translation (NMT) models inspired by echo state\nnetwork (ESN), named Echo State NMT (ESNMT), in which the encoder and decoder\nlayer weights are randomly generated then fixed throughout training. We show\nthat even with this extremely simple model construction and training procedure,\nESNMT can already reach 70-80% quality of fully trainable baselines. We examine\nhow spectral radius of the reservoir, a key quantity that characterizes the\nmodel, determines the model behavior. Our findings indicate that randomized\nnetworks can work well even for complicated sequence-to-sequence prediction NLP\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:08:45 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Garg", "Ankush", ""], ["Cao", "Yuan", ""], ["Ge", "Qi", ""]]}, {"id": "2002.11898", "submitter": "Jamal Molin", "authors": "Jamal Lottier Molin, Chetan Singh Thakur, Ralph Etienne-Cummings,\n  Ernst Niebur", "title": "A Neuromorphic Proto-Object Based Dynamic Visual Saliency Model with an\n  FPGA Implementation", "comments": "15 pages, 6 figures, 6 tables, journal", "journal-ref": null, "doi": "10.1109/TBCAS.2021.3089622", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to attend to salient regions of a visual scene is an innate and\nnecessary preprocessing step for both biological and engineered systems\nperforming high-level visual tasks (e.g. object detection, tracking, and\nclassification). Computational efficiency, in regard to processing bandwidth\nand speed, is improved by only devoting computational resources to salient\nregions of the visual stimuli. In this paper, we first present a neuromorphic,\nbottom-up, dynamic visual saliency model based on the notion of proto-objects.\nThis is achieved by incorporating the temporal characteristics of the visual\nstimulus into the model, similarly to the manner in which early stages of the\nhuman visual system extracts temporal information. This neuromorphic model\noutperforms state-of-the-art dynamic visual saliency models in predicting human\neye fixations on a commonly used video dataset with associated eye tracking\ndata. Secondly, for this model to have practical applications, it must be\ncapable of performing its computations in real-time under low-power,\nsmall-size, and lightweight constraints. To address this, we introduce a\nField-Programmable Gate Array implementation of the model on an Opal Kelly 7350\nKintex-7 board. This novel hardware implementation allows for processing of up\nto 23.35 frames per second running on a 100 MHz clock - better than 26x speedup\nfrom the software implementation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:31:56 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 01:49:42 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 02:04:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Molin", "Jamal Lottier", ""], ["Thakur", "Chetan Singh", ""], ["Etienne-Cummings", "Ralph", ""], ["Niebur", "Ernst", ""]]}, {"id": "2002.11945", "submitter": "Sumon Bose Mr.", "authors": "Sumon Kumar Bose, Jyotibdha Acharya, and Arindam Basu", "title": "Is my Neural Network Neuromorphic? Taxonomy, Recent Trends and Future\n  Directions in Neuromorphic Engineering", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we review recent work published over the last 3 years under\nthe umbrella of Neuromorphic engineering to analyze what are the common\nfeatures among such systems. We see that there is no clear consensus but each\nsystem has one or more of the following features:(1) Analog computing (2) Non\nvonNeumann Architecture and low-precision digital processing (3) Spiking Neural\nNetworks (SNN) with components closely related to biology. We compare recent\nmachine learning accelerator chips to show that indeed analog processing and\nreduced bit precision architectures have best throughput, energy and area\nefficiencies. However, pure digital architectures can also achieve quite high\nefficiencies by just adopting a non von-Neumann architecture. Given the design\nautomation tools for digital hardware design, it raises a question on the\nlikelihood of adoption of analog processing in the near future for industrial\ndesigns. Next, we argue about the importance of defining standards and choosing\nproper benchmarks for the progress of neuromorphic system designs and propose\nsome desired characteristics of such benchmarks. Finally, we show brain-machine\ninterfaces as a potential task that fulfils all the criteria of such\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 07:10:23 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Bose", "Sumon Kumar", ""], ["Acharya", "Jyotibdha", ""], ["Basu", "Arindam", ""]]}, {"id": "2002.12126", "submitter": "Tarik A. Rashid", "authors": "Chnoor M. Rahman and Tarik A. Rashid, Abeer Alsadoon, Nebojsa Bacanin,\n  Polla Fattah", "title": "A Survey on Dragonfly Algorithm and its Applications in Engineering", "comments": "23 pages. arXiv admin note: text overlap with arXiv:2001.02292", "journal-ref": "2020", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dragonfly algorithm (DA) is one of the most recently developed heuristic\noptimization algorithms by Mirjalili in 2016. It is now one of the most widely\nused algorithms. In some cases, it outperforms the most popular algorithms.\nHowever, this algorithm is not far from obstacles when it comes to complex\noptimization problems. In this work, along with the strengths of the algorithm\nin solving real-world optimization problems, the weakness of the algorithm to\noptimize complex optimization problems is addressed. This survey presents a\ncomprehensive investigation of DA in the engineering area. First, an overview\nof the algorithm is discussed. Additionally, the different variants of the\nalgorithm are addressed too. The combined versions of the DA with other\ntechniques and the modifications that have been done to make the algorithm work\nbetter are shown. Besides, a survey on applications in engineering area that\nused DA is offered. The algorithm is compared with some other metaheuristic\nalgorithms to demonstrate its ability to optimize problems comparing to the\nothers. The results of the algorithm from the works that utilized the DA in the\nliterature and the results of the benchmark functions showed that in comparison\nwith some other algorithms DA has an excellent performance, especially for\nsmall to medium problems. Moreover, the bottlenecks of the algorithm and some\nfuture trends are discussed. Authors conduct this research with the hope of\noffering beneficial information about the DA to the researchers who want to\nstudy the algorithm and utilize it to optimize engineering problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:23:26 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 11:08:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rahman", "Chnoor M.", ""], ["Rashid", "Tarik A.", ""], ["Alsadoon", "Abeer", ""], ["Bacanin", "Nebojsa", ""], ["Fattah", "Polla", ""]]}, {"id": "2002.12133", "submitter": "Aritz D. Martinez", "authors": "Aritz D. Martinez, Eneko Osaba, Javier Del Ser and Francisco Herrera", "title": "Simultaneously Evolving Deep Reinforcement Learning Models using\n  Multifactorial Optimization", "comments": "8 pages, 5 figures, submitted to IEEE Conference on Evolutionary\n  Computation 2020 (IEEE CEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Multifactorial Optimization (MFO) has gained a notable\nmomentum in the research community. MFO is known for its inherent capability to\nefficiently address multiple optimization tasks at the same time, while\ntransferring information among such tasks to improve their convergence speed.\nOn the other hand, the quantum leap made by Deep Q Learning (DQL) in the\nMachine Learning field has allowed facing Reinforcement Learning (RL) problems\nof unprecedented complexity. Unfortunately, complex DQL models usually find it\ndifficult to converge to optimal policies due to the lack of exploration or\nsparse rewards. In order to overcome these drawbacks, pre-trained models are\nwidely harnessed via Transfer Learning, extrapolating knowledge acquired in a\nsource task to the target task. Besides, meta-heuristic optimization has been\nshown to reduce the lack of exploration of DQL models. This work proposes a MFO\nframework capable of simultaneously evolving several DQL models towards solving\ninterrelated RL tasks. Specifically, our proposed framework blends together the\nbenefits of meta-heuristic optimization, Transfer Learning and DQL to automate\nthe process of knowledge transfer and policy learning of distributed RL agents.\nA thorough experimentation is presented and discussed so as to assess the\nperformance of the framework, its comparison to the traditional methodology for\nTransfer Learning in terms of convergence, speed and policy quality , and the\nintertask relationships found and exploited over the search process.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:36:57 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 10:47:41 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Martinez", "Aritz D.", ""], ["Osaba", "Eneko", ""], ["Del Ser", "Javier", ""], ["Herrera", "Francisco", ""]]}, {"id": "2002.12287", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio and Simone Scardapane", "title": "Deep Randomized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized Neural Networks explore the behavior of neural systems where the\nmajority of connections are fixed, either in a stochastic or a deterministic\nfashion. Typical examples of such systems consist of multi-layered neural\nnetwork architectures where the connections to the hidden layer(s) are left\nuntrained after initialization. Limiting the training algorithms to operate on\na reduced set of weights inherently characterizes the class of Randomized\nNeural Networks with a number of intriguing features. Among them, the extreme\nefficiency of the resulting learning processes is undoubtedly a striking\nadvantage with respect to fully trained architectures. Besides, despite the\ninvolved simplifications, randomized neural systems possess remarkable\nproperties both in practice, achieving state-of-the-art results in multiple\ndomains, and theoretically, allowing to analyze intrinsic properties of neural\narchitectures (e.g. before training of the hidden layers' connections). In\nrecent years, the study of Randomized Neural Networks has been extended towards\ndeep architectures, opening new research directions to the design of effective\nyet extremely efficient deep learning models in vectorial as well as in more\ncomplex data domains. This chapter surveys all the major aspects regarding the\ndesign and analysis of Randomized Neural Networks, and some of the key results\nwith respect to their approximation capabilities. In particular, we first\nintroduce the fundamentals of randomized neural models in the context of\nfeed-forward networks (i.e., Random Vector Functional Link and equivalent\nmodels) and convolutional filters, before moving to the case of recurrent\nsystems (i.e., Reservoir Computing networks). For both, we focus specifically\non recent results in the domain of deep randomized systems, and (for recurrent\nmodels) their application to structured domains.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:57:58 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 15:19:10 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Scardapane", "Simone", ""]]}, {"id": "2002.12336", "submitter": "Thanard Kurutach", "authors": "Kara Liu, Thanard Kurutach, Christine Tung, Pieter Abbeel, Aviv Tamar", "title": "Hallucinative Topological Memory for Zero-Shot Visual Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual planning (VP), an agent learns to plan goal-directed behavior from\nobservations of a dynamical system obtained offline, e.g., images obtained from\nself-supervised robot interaction. Most previous works on VP approached the\nproblem by planning in a learned latent space, resulting in low-quality visual\nplans, and difficult training algorithms. Here, instead, we propose a simple VP\nmethod that plans directly in image space and displays competitive performance.\nWe build on the semi-parametric topological memory (SPTM) method: image samples\nare treated as nodes in a graph, the graph connectivity is learned from image\nsequence data, and planning can be performed using conventional graph search\nmethods. We propose two modifications on SPTM. First, we train an energy-based\ngraph connectivity function using contrastive predictive coding that admits\nstable training. Second, to allow zero-shot planning in new domains, we learn a\nconditional VAE model that generates images given a context of the domain, and\nuse these hallucinated samples for building the connectivity graph and\nplanning. We show that this simple approach significantly outperform the\nstate-of-the-art VP methods, in terms of both plan interpretability and success\nrate when using the plan to guide a trajectory-following controller.\nInterestingly, our method can pick up non-trivial visual properties of objects,\nsuch as their geometry, and account for it in the plans.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:54:42 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Liu", "Kara", ""], ["Kurutach", "Thanard", ""], ["Tung", "Christine", ""], ["Abbeel", "Pieter", ""], ["Tamar", "Aviv", ""]]}, {"id": "2002.12485", "submitter": "Mateusz Zaborski", "authors": "Micha{\\l} Okulewicz, Mateusz Zaborski, Jacek Ma\\'ndziuk", "title": "Generalized Self-Adapting Particle Swarm Optimization algorithm with\n  archive of samples", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we enhance Generalized Self-Adapting Particle Swarm\nOptimization algorithm (GAPSO), initially introduced at the Parallel Problem\nSolving from Nature 2018 conference, and to investigate its properties. The\nresearch on GAPSO is underlined by the two following assumptions: (1) it is\npossible to achieve good performance of an optimization algorithm through\nutilization of all of the gathered samples, (2) the best performance can be\naccomplished by means of a combination of specialized sampling behaviors\n(Particle Swarm Optimization, Differential Evolution, and locally fitted square\nfunctions). From a software engineering point of view, GAPSO considers a\nstandard Particle Swarm Optimization algorithm as an ideal starting point for\ncreating a generalpurpose global optimization framework. Within this framework\nhybrid optimization algorithms are developed, and various additional techniques\n(like algorithm restart management or adaptation schemes) are tested. The paper\nintroduces a new version of the algorithm, abbreviated as M-GAPSO. In\ncomparison with the original GAPSO formulation it includes the following four\nfeatures: a global restart management scheme, samples gathering within an\nR-Tree based index (archive/memory of samples), adaptation of a sampling\nbehavior based on a global particle performance, and a specific approach to\nlocal search. The above-mentioned enhancements resulted in improved performance\nof M-GAPSO over GAPSO, observed on both COCO BBOB testbed and in the black-box\noptimization competition BBComp. Also, for lower dimensionality functions (up\nto 5D) results of M-GAPSO are better or comparable to the state-of-the art\nversion of CMA-ES (namely the KL-BIPOP-CMA-ES algorithm presented at the GECCO\n2017 conference).\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 00:03:17 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Okulewicz", "Micha\u0142", ""], ["Zaborski", "Mateusz", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2002.12620", "submitter": "Yiming Cui", "authors": "Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting Liu, Shijin\n  Wang, Guoping Hu", "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural\n  Language Processing", "comments": "To appear at ACL 2020 Demo Session", "journal-ref": null, "doi": "10.18653/v1/2020.acl-demos.2", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce TextBrewer, an open-source knowledge distillation\ntoolkit designed for natural language processing. It works with different\nneural network models and supports various kinds of supervised learning tasks,\nsuch as text classification, reading comprehension, sequence labeling.\nTextBrewer provides a simple and uniform workflow that enables quick setting up\nof distillation experiments with highly flexible configurations. It offers a\nset of predefined distillation methods and can be extended with custom code. As\na case study, we use TextBrewer to distill BERT on several typical NLP tasks.\nWith simple configurations, we achieve results that are comparable with or even\nhigher than the public distilled BERT models with similar numbers of\nparameters. Our toolkit is available through: http://textbrewer.hfl-rc.com\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:44:07 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 02:34:38 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Yang", "Ziqing", ""], ["Cui", "Yiming", ""], ["Chen", "Zhipeng", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2002.12704", "submitter": "Kefan Chen", "authors": "Kefan Chen, Wei Pang", "title": "ImmuNetNAS: An Immune-network approach for searching Convolutional\n  Neural Network Architectures", "comments": "7 pages, 7 figures, 5 tables. No conference right now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we propose ImmuNetNAS, a novel Neural Architecture Search\n(NAS) approach inspired by the immune network theory. The core of ImmuNetNAS is\nbuilt on the original immune network algorithm, which iteratively updates the\npopulation through hypermutation and selection, and eliminates the\nself-generation individuals that do not meet the requirements through comparing\nantibody affinity and inter-specific similarity. In addition, in order to\nfacilitate the mutation operation, we propose a novel two-component based\nneural structure coding strategy. Furthermore, an improved mutation strategy\nbased on Standard Genetic Algorithm (SGA) was proposed according to this\nencoding method. Finally, based on the proposed two-component based coding\nmethod, a new antibody affinity calculation method was developed to screen\nsuitable neural architectures. Systematic evaluations demonstrate that our\nsystem has achieved good performance on both the MNIST and CIFAR-10 datasets.\nWe open-source our code on GitHub in order to share it with other deep learning\nresearchers and practitioners.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 13:32:57 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Chen", "Kefan", ""], ["Pang", "Wei", ""]]}, {"id": "2002.12795", "submitter": "Hossein Valavi", "authors": "Hossein Valavi, Sulin Liu and Peter J. Ramadge", "title": "The Landscape of Matrix Factorization Revisited", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the landscape of the simple matrix factorization problem. For\nlow-rank matrix factorization, prior work has shown that there exist infinitely\nmany critical points all of which are either global minima or strict saddles.\nAt a strict saddle the minimum eigenvalue of the Hessian is negative. Of\ninterest is whether this minimum eigenvalue is uniformly bounded below zero\nover all strict saddles. To answer this we consider orbits of critical points\nunder the general linear group. For each orbit we identify a representative\npoint, called a canonical point. If a canonical point is a strict saddle, so is\nevery point on its orbit. We derive an expression for the minimum eigenvalue of\nthe Hessian at each canonical strict saddle and use this to show that the\nminimum eigenvalue of the Hessian over the set of strict saddles is not\nuniformly bounded below zero. We also show that a known invariance property of\ngradient flow ensures the solution of gradient flow only encounters critical\npoints on an invariant manifold $\\mathcal{M}_C$ determined by the initial\ncondition. We show that, in contrast to the general situation, the minimum\neigenvalue of strict saddles in $\\mathcal{M}_{0}$ is uniformly bounded below\nzero. We obtain an expression for this bound in terms of the singular values of\nthe matrix being factorized. This bound depends on the size of the nonzero\nsingular values and on the separation between distinct nonzero singular values\nof the matrix.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 04:27:22 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 21:47:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Valavi", "Hossein", ""], ["Liu", "Sulin", ""], ["Ramadge", "Peter J.", ""]]}]