[{"id": "1609.00085", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er", "title": "A Novel Progressive Learning Technique for Multi-class Classification", "comments": "23 pages, 13 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a progressive learning technique for multi-class\nclassification is proposed. This newly developed learning technique is\nindependent of the number of class constraints and it can learn new classes\nwhile still retaining the knowledge of previous classes. Whenever a new class\n(non-native to the knowledge learnt thus far) is encountered, the neural\nnetwork structure gets remodeled automatically by facilitating new neurons and\ninterconnections, and the parameters are calculated in such a way that it\nretains the knowledge learnt thus far. This technique is suitable for\nreal-world applications where the number of classes is often unknown and online\nlearning from real-time data is required. The consistency and the complexity of\nthe progressive learning technique are analyzed. Several standard datasets are\nused to evaluate the performance of the developed technique. A comparative\nstudy shows that the developed technique is superior.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 01:50:18 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 09:52:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""]]}, {"id": "1609.00086", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er, Mihika Dave, Mahardhika Pratama,\n  Shiqian Wu", "title": "A novel online multi-label classifier for high-speed streaming data\n  applications", "comments": "18 pages, 7 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:1608.08898", "journal-ref": null, "doi": "10.1007/s12530-016-9162-8", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a high-speed online neural network classifier based on extreme\nlearning machines for multi-label classification is proposed. In multi-label\nclassification, each of the input data sample belongs to one or more than one\nof the target labels. The traditional binary and multi-class classification\nwhere each sample belongs to only one target class forms the subset of\nmulti-label classification. Multi-label classification problems are far more\ncomplex than binary and multi-class classification problems, as both the number\nof target labels and each of the target labels corresponding to each of the\ninput samples are to be identified. The proposed work exploits the high-speed\nnature of the extreme learning machines to achieve real-time multi-label\nclassification of streaming data. A new threshold-based online sequential\nlearning algorithm is proposed for high speed and streaming data classification\nof multi-label problems. The proposed method is experimented with six different\ndatasets from different application domains such as multimedia, text, and\nbiology. The hamming loss, accuracy, training time and testing time of the\nproposed technique is compared with nine different state-of-the-art methods.\nExperimental studies shows that the proposed technique outperforms the existing\nmulti-label classifiers in terms of performance and speed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 01:58:50 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""], ["Dave", "Mihika", ""], ["Pratama", "Mahardhika", ""], ["Wu", "Shiqian", ""]]}, {"id": "1609.00222", "submitter": "Hande Alemdar", "authors": "Hande Alemdar and Vincent Leroy and Adrien Prost-Boucle and\n  Fr\\'ed\\'eric P\\'etrot", "title": "Ternary Neural Networks for Resource-Efficient AI Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation and storage requirements for Deep Neural Networks (DNNs) are\nusually high. This issue limits their deployability on ubiquitous computing\ndevices such as smart phones, wearables and autonomous drones. In this paper,\nwe propose ternary neural networks (TNNs) in order to make deep learning more\nresource-efficient. We train these TNNs using a teacher-student approach based\non a novel, layer-wise greedy methodology. Thanks to our two-stage training\nprocedure, the teacher network is still able to use state-of-the-art methods\nsuch as dropout and batch normalization to increase accuracy and reduce\ntraining time. Using only ternary weights and activations, the student ternary\nnetwork learns to mimic the behavior of its teacher network without using any\nmultiplication. Unlike its -1,1 binary counterparts, a ternary neural network\ninherently prunes the smaller weights by setting them to zero during training.\nThis makes them sparser and thus more energy-efficient. We design a\npurpose-built hardware architecture for TNNs and implement it on FPGA and ASIC.\nWe evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x\nbetter energy efficiency with respect to the state of the art while also\nimproving accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 13:08:47 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 09:44:34 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Alemdar", "Hande", ""], ["Leroy", "Vincent", ""], ["Prost-Boucle", "Adrien", ""], ["P\u00e9trot", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1609.00843", "submitter": "Rajasekar Venkatesan", "authors": "Meng Joo Er, Rajasekar Venkatesan, Ning Wang", "title": "An Online Universal Classifier for Binary, Multi-class and Multi-label\n  Classification", "comments": "6 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification involves the learning of the mapping function that associates\ninput samples to corresponding target label. There are two major categories of\nclassification problems: Single-label classification and Multi-label\nclassification. Traditional binary and multi-class classifications are\nsub-categories of single-label classification. Several classifiers are\ndeveloped for binary, multi-class and multi-label classification problems, but\nthere are no classifiers available in the literature capable of performing all\nthree types of classification. In this paper, a novel online universal\nclassifier capable of performing all the three types of classification is\nproposed. Being a high speed online classifier, the proposed technique can be\napplied to streaming data applications. The performance of the developed\nclassifier is evaluated using datasets from binary, multi-class and multi-label\nproblems. The results obtained are compared with state-of-the-art techniques\nfrom each of the classification types.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 17:03:14 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""], ["Wang", "Ning", ""]]}, {"id": "1609.01037", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Distribution-Specific Hardness of Learning Neural Networks", "comments": "Simpler and more explicit theorems in section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks are routinely and successfully trained in practice\nusing simple gradient-based methods, most existing theoretical results are\nnegative, showing that learning such networks is difficult, in a worst-case\nsense over all data distributions. In this paper, we take a more nuanced view,\nand consider whether specific assumptions on the \"niceness\" of the input\ndistribution, or \"niceness\" of the target function (e.g. in terms of\nsmoothness, non-degeneracy, incoherence, random choice of parameters etc.), are\nsufficient to guarantee learnability using gradient-based methods. We provide\nevidence that neither class of assumptions alone is sufficient: On the one\nhand, for any member of a class of \"nice\" target functions, there are difficult\ninput distributions. On the other hand, we identify a family of simple target\nfunctions, which are difficult to learn even if the input distribution is\n\"nice\". To prove our results, we develop some tools which may be of independent\ninterest, such as extending Fourier-based hardness techniques developed in the\ncontext of statistical queries \\cite{blum1994weakly}, from the Boolean cube to\nEuclidean space and to more general classes of functions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 06:47:10 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 08:56:44 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1609.01360", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee and Alexander Wong", "title": "Evolutionary Synthesis of Deep Neural Networks via Synaptic\n  Cluster-driven Genetic Encoding", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest towards achieving highly efficient\ndeep neural network architectures. A promising paradigm for achieving this is\nthe concept of evolutionary deep intelligence, which attempts to mimic\nbiological evolution processes to synthesize highly-efficient deep neural\nnetworks over successive generations. An important aspect of evolutionary deep\nintelligence is the genetic encoding scheme used to mimic heredity, which can\nhave a significant impact on the quality of offspring deep neural networks.\nMotivated by the neurobiological phenomenon of synaptic clustering, we\nintroduce a new genetic encoding scheme where synaptic probability is driven\ntowards the formation of a highly sparse set of synaptic clusters. Experimental\nresults for the task of image classification demonstrated that the synthesized\noffspring networks using this synaptic cluster-driven genetic encoding scheme\ncan achieve state-of-the-art performance while having network architectures\nthat are not only significantly more efficient (with a ~125-fold decrease in\nsynapses for MNIST) compared to the original ancestor network, but also\ntailored for GPU-accelerated machine learning applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 01:08:03 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 16:00:01 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1609.01459", "submitter": "Emmanuel Osegi", "authors": "Emmanuel Ndidi Osegi (NOUN), Vincent Ike Anireh", "title": "Deviant Learning Algorithm: Learning Sparse Mismatch Representations\n  through Time and Space", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predictive coding (PDC) has recently attracted attention in the neuroscience\nand computing community as a candidate unifying paradigm for neuronal studies\nand artificial neural network implementations particularly targeted at\nunsupervised learning systems. The Mismatch Negativity (MMN) has also recently\nbeen studied in relation to PC and found to be a useful ingredient in neural\npredictive coding systems. Backed by the behavior of living organisms, such\nnetworks are particularly useful in forming spatio-temporal transitions and\ninvariant representations of the input world. However, most neural systems\nstill do not account for large number of synapses even though this has been\nshown by a few machine learning researchers as an effective and very important\ncomponent of any neural system if such a system is to behave properly. Our\nmajor point here is that PDC systems with the MMN effect in addition to a large\nnumber of synapses can greatly improve any neural learning system's performance\nand ability to make decisions in the machine world. In this paper, we propose a\nnovel bio-mimetic computational intelligence algorithm -- the Deviant Learning\nAlgorithm, inspired by these key ideas and functional properties of recent\nbrain-cognitive discoveries and theories. We also show by numerical experiments\nguided by theoretical insights, how our invented bio-mimetic algorithm can\nachieve competitive predictions even with very small problem specific data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 09:35:14 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 09:20:09 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 18:53:24 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 20:55:33 GMT"}, {"version": "v5", "created": "Sun, 1 Jan 2017 11:26:06 GMT"}, {"version": "v6", "created": "Tue, 3 Jan 2017 02:49:15 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Osegi", "Emmanuel Ndidi", "", "NOUN"], ["Anireh", "Vincent Ike", ""]]}, {"id": "1609.01926", "submitter": "Giovanni Carmantini", "authors": "Giovanni Sirio Carmantini, Peter beim Graben, Mathieu Desroches,\n  Serafim Rodrigues", "title": "A modular architecture for transparent computation in Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2016.09.001", "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.FL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation is classically studied in terms of automata, formal languages and\nalgorithms; yet, the relation between neural dynamics and symbolic\nrepresentations and operations is still unclear in traditional eliminative\nconnectionism. Therefore, we suggest a unique perspective on this central\nissue, to which we would like to refer as to transparent connectionism, by\nproposing accounts of how symbolic computation can be implemented in neural\nsubstrates. In this study we first introduce a new model of dynamics on a\nsymbolic space, the versatile shift, showing that it supports the real-time\nsimulation of a range of automata. We then show that the Goedelization of\nversatile shifts defines nonlinear dynamical automata, dynamical systems\nevolving on a vectorial space. Finally, we present a mapping between nonlinear\ndynamical automata and recurrent artificial neural networks. The mapping\ndefines an architecture characterized by its granular modularity, where data,\nsymbolic operations and their control are not only distinguishable in\nactivation space, but also spatially localizable in the network itself, while\nmaintaining a distributed encoding of symbolic representations. The resulting\nnetworks simulate automata in real-time and are programmed directly, in absence\nof network training. To discuss the unique characteristics of the architecture\nand their consequences, we present two examples: i) the design of a Central\nPattern Generator from a finite-state locomotive controller, and ii) the\ncreation of a network simulating a system of interactive automata that supports\nthe parsing of garden-path sentences as investigated in psycholinguistics\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 10:44:28 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Carmantini", "Giovanni Sirio", ""], ["Graben", "Peter beim", ""], ["Desroches", "Mathieu", ""], ["Rodrigues", "Serafim", ""]]}, {"id": "1609.01982", "submitter": "Eric Kee", "authors": "Eric Kee", "title": "Uniform Transformation of Non-Separable Probability Distributions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical framework is developed to describe the transformation that\ndistributes probability density functions uniformly over space. In one\ndimension, the cumulative distribution can be used, but does not generalize to\nhigher dimensions, or non-separable distributions. A potential function is\nshown to link probability density functions to their transformation, and to\ngeneralize the cumulative. A numerical method is developed to compute the\npotential, and examples are shown in two dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 02:18:33 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Kee", "Eric", ""]]}, {"id": "1609.02053", "submitter": "Davide Zambrano", "authors": "Davide Zambrano and Sander M. Bohte", "title": "Fast and Efficient Asynchronous Neural Computation with Adapting Spiking\n  Neural Networks", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neurons communicate with a sparing exchange of pulses - spikes. It\nis an open question how real spiking neurons produce the kind of powerful\nneural computation that is possible with deep artificial neural networks, using\nonly so very few spikes to communicate. Building on recent insights in\nneuroscience, we present an Adapting Spiking Neural Network (ASNN) based on\nadaptive spiking neurons. These spiking neurons efficiently encode information\nin spike-trains using a form of Asynchronous Pulsed Sigma-Delta coding while\nhomeostatically optimizing their firing rate. In the proposed paradigm of\nspiking neuron computation, neural adaptation is tightly coupled to synaptic\nplasticity, to ensure that downstream neurons can correctly decode upstream\nspiking neurons. We show that this type of network is inherently able to carry\nout asynchronous and event-driven neural computation, while performing\nidentical to corresponding artificial neural networks (ANNs). In particular, we\nshow that these adaptive spiking neurons can be drop in replacements for ReLU\nneurons in standard feedforward ANNs comprised of such units. We demonstrate\nthat this can also be successfully applied to a ReLU based deep convolutional\nneural network for classifying the MNIST dataset. The ASNN thus outperforms\ncurrent Spiking Neural Networks (SNNs) implementations, while responding (up\nto) an order of magnitude faster and using an order of magnitude fewer spikes.\nAdditionally, in a streaming setting where frames are continuously classified,\nwe show that the ASNN requires substantially fewer network updates as compared\nto the corresponding ANN.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 16:30:01 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Zambrano", "Davide", ""], ["Bohte", "Sander M.", ""]]}, {"id": "1609.02226", "submitter": "Navid Kardan", "authors": "Navid Kardan, Kenneth O. Stanley", "title": "Fitted Learning: Models with Awareness of their Limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning has pushed the boundaries of classification forward, in\nrecent years hints of the limits of standard classification have begun to\nemerge. Problems such as fooling, adding new classes over time, and the need to\nretrain learning models only for small changes to the original problem all\npoint to a potential shortcoming in the classic classification regime, where a\ncomprehensive a priori knowledge of the possible classes or concepts is\ncritical. Without such knowledge, classifiers misjudge the limits of their\nknowledge and overgeneralization therefore becomes a serious obstacle to\nconsistent performance. In response to these challenges, this paper extends the\nclassic regime by reframing classification instead with the assumption that\nconcepts present in the training set are only a sample of the hypothetical\nfinal set of concepts. To bring learning models into this new paradigm, a novel\nelaboration of standard architectures called the competitive overcomplete\noutput layer (COOL) neural network is introduced. Experiments demonstrate the\neffectiveness of COOL by applying it to fooling, separable concept learning,\none-class neural networks, and standard classification benchmarks. The results\nsuggest that, unlike conventional classifiers, the amount of generalization in\nCOOL networks can be tuned to match the problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 23:59:36 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 06:34:56 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 17:14:41 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 04:21:34 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kardan", "Navid", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "1609.02228", "submitter": "Thomas Miconi", "authors": "Thomas Miconi", "title": "Learning to learn with backpropagation of Hebbian plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hebbian plasticity is a powerful principle that allows biological brains to\nlearn from their lifetime experience. By contrast, artificial neural networks\ntrained with backpropagation generally have fixed connection weights that do\nnot change once training is complete. While recent methods can endow neural\nnetworks with long-term memories, Hebbian plasticity is currently not amenable\nto gradient descent. Here we derive analytical expressions for activity\ngradients in neural networks with Hebbian plastic connections. Using these\nexpressions, we can use backpropagation to train not just the baseline weights\nof the connections, but also their plasticity. As a result, the networks \"learn\nhow to learn\" in order to solve the problem at hand: the trained networks\nautomatically perform fast learning of unpredictable environmental features\nduring their lifetime, expanding the range of solvable problems. We test the\nalgorithm on various on-line learning tasks, including pattern completion,\none-shot learning, and reversal learning. The algorithm successfully learns how\nto learn the relevant associations from one-shot instruction, and fine-tunes\nthe temporal dynamics of plasticity to allow for continual learning in response\nto changing environmental parameters. We conclude that backpropagation of\nHebbian plasticity offers a powerful model for lifelong learning.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 00:02:20 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 17:51:25 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Miconi", "Thomas", ""]]}, {"id": "1609.03068", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Lorenzo Livi, Cesare Alippi and Robert Jenssen", "title": "Multiplex visibility graphs to investigate recurrent neural networks\n  dynamics", "comments": null, "journal-ref": null, "doi": "10.1038/srep44037", "report-no": null, "categories": "cs.NE math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurrent neural network (RNN) is a universal approximator of dynamical\nsystems, whose performance often depends on sensitive hyperparameters. Tuning\nof such hyperparameters may be difficult and, typically, based on a\ntrial-and-error approach. In this work, we adopt a graph-based framework to\ninterpret and characterize the internal RNN dynamics. Through this insight, we\nare able to design a principled unsupervised method to derive configurations\nwith maximized performances, in terms of prediction error and memory capacity.\nIn particular, we propose to model time series of neurons activations with the\nrecently introduced horizontal visibility graphs, whose topological properties\nreflect important dynamical features of the underlying dynamic system.\nSuccessively, each graph becomes a layer of a larger structure, called\nmultiplex. We show that topological properties of such a multiplex reflect\nimportant features of RNN dynamics and are used to guide the tuning procedure.\nTo validate the proposed method, we consider a class of RNNs called echo state\nnetworks. We perform experiments and discuss results on several benchmarks and\nreal-world dataset of call data records.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 16:12:27 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 09:01:14 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2017 17:47:44 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Livi", "Lorenzo", ""], ["Alippi", "Cesare", ""], ["Jenssen", "Robert", ""]]}, {"id": "1609.03348", "submitter": "Thomas Ward", "authors": "Thomas H. Ward", "title": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generic and scalable Reinforcement Learning scheme for Artificial Neural\nNetworks is presented, providing a general purpose learning machine. By\nreference to a node threshold three features are described 1) A mechanism for\nPrimary Reinforcement, capable of solving linearly inseparable problems 2) The\nlearning scheme is extended to include a mechanism for Conditioned\nReinforcement, capable of forming long term strategy 3) The learning scheme is\nmodified to use a threshold-based deep learning algorithm, providing a robust\nand biologically inspired alternative to backpropagation. The model may be used\nfor supervised as well as unsupervised training regimes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 11:23:20 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 04:20:01 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 05:11:01 GMT"}, {"version": "v4", "created": "Sat, 14 Jan 2017 05:54:29 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Ward", "Thomas H.", ""]]}, {"id": "1609.03777", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Wonyong Sung", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural\n  Networks", "comments": "Submitted to NIPS 2016 on May 20, 2016 (v1), accepted to ICASSP 2017\n  (v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN) based character-level language models (CLMs)\nare extremely useful for modeling out-of-vocabulary words by nature. However,\ntheir performance is generally much worse than the word-level language models\n(WLMs), since CLMs need to consider longer history of tokens to properly\npredict the next one. We address this problem by proposing hierarchical RNN\narchitectures, which consist of multiple modules with different timescales.\nDespite the multi-timescale structures, the input and output layers operate\nwith the character-level clock, which allows the existing RNN CLM training\napproaches to be directly applicable without any modifications. Our CLM models\nshow better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\nBenchmark with only 2% of parameters. Also, we present real-time\ncharacter-level end-to-end speech recognition examples on the Wall Street\nJournal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the\nproposed models results in better recognition accuracies even though the number\nof parameters are reduced to 30%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:41:48 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 13:49:41 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1609.03894", "submitter": "Francisco Massa", "authors": "Francisco Massa, Renaud Marlet, Mathieu Aubry", "title": "Crafting a multi-task CNN for viewpoint estimation", "comments": "To appear in BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) were recently shown to provide\nstate-of-the-art results for object category viewpoint estimation. However\ndifferent ways of formulating this problem have been proposed and the competing\napproaches have been explored with very different design choices. This paper\npresents a comparison of these approaches in a unified setting as well as a\ndetailed analysis of the key factors that impact performance. Followingly, we\npresent a new joint training method with the detection task and demonstrate its\nbenefit. We also highlight the superiority of classification approaches over\nregression approaches, quantify the benefits of deeper architectures and\nextended training data, and demonstrate that synthetic data is beneficial even\nwhen using ImageNet training data. By combining all these elements, we\ndemonstrate an improvement of approximately 5% mAVP over previous\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\nchallenging 24 view classification task we improve the results from 31.1% to\n36.1% mAVP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 15:19:38 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Massa", "Francisco", ""], ["Marlet", "Renaud", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1609.03971", "submitter": "Fergal Byrne", "authors": "Eric Laukien, Richard Crowder and Fergal Byrne", "title": "Feynman Machine: The Universal Dynamical Systems Computer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.ET math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efforts at understanding the computational processes in the brain have met\nwith limited success, despite their importance and potential uses in building\nintelligent machines. We propose a simple new model which draws on recent\nfindings in Neuroscience and the Applied Mathematics of interacting Dynamical\nSystems. The Feynman Machine is a Universal Computer for Dynamical Systems,\nanalogous to the Turing Machine for symbolic computing, but with several\nimportant differences. We demonstrate that networks and hierarchies of simple\ninteracting Dynamical Systems, each adaptively learning to forecast its\nevolution, are capable of automatically building sensorimotor models of the\nexternal and internal world. We identify such networks in mammalian neocortex,\nand show how existing theories of cortical computation combine with our model\nto explain the power and flexibility of mammalian intelligence. These findings\nlead directly to new architectures for machine intelligence. A suite of\nsoftware implementations has been built based on these principles, and applied\nto a number of spatiotemporal learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 18:34:59 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Laukien", "Eric", ""], ["Crowder", "Richard", ""], ["Byrne", "Fergal", ""]]}, {"id": "1609.03976", "submitter": "Ozan \\c{C}a\\u{g}layan", "authors": "Ozan Caglayan, Lo\\\"ic Barrault, Fethi Bougares", "title": "Multimodal Attention for Neural Machine Translation", "comments": "10 pages, under review COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention mechanism is an important part of the neural machine\ntranslation (NMT) where it was reported to produce richer source representation\ncompared to fixed-length encoding sequence-to-sequence models. Recently, the\neffectiveness of attention has also been explored in the context of image\ncaptioning. In this work, we assess the feasibility of a multimodal attention\nmechanism that simultaneously focus over an image and its natural language\ndescription for generating a description in another language. We train several\nvariants of our proposed attention mechanism on the Multi30k multilingual image\ncaptioning dataset. We show that a dedicated attention for each modality\nachieves up to 1.6 points in BLEU and METEOR compared to a textual NMT\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 18:46:03 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Caglayan", "Ozan", ""], ["Barrault", "Lo\u00efc", ""], ["Bougares", "Fethi", ""]]}, {"id": "1609.04243", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler, Kyunghyun Cho", "title": "Convolutional Recurrent Neural Networks for Music Classification", "comments": "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN\n  architectures and update experiment results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a convolutional recurrent neural network (CRNN) for music\ntagging. CRNNs take advantage of convolutional neural networks (CNNs) for local\nfeature extraction and recurrent neural networks for temporal summarisation of\nthe extracted features. We compare CRNN with three CNN structures that have\nbeen used for music tagging while controlling the number of parameters with\nrespect to their performance and training time per sample. Overall, we found\nthat CRNNs show a strong performance with respect to the number of parameter\nand training time, indicating the effectiveness of its hybrid structure in\nmusic feature extraction and feature summarisation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:52:08 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 07:50:14 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 06:52:30 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1609.04453", "submitter": "Terrell Mundhenk", "authors": "T. Nathan Mundhenk, Goran Konjevod, Wesam A. Sakla, Kofi Boakye", "title": "A Large Contextual Dataset for Classification, Detection and Counting of\n  Cars with Deep Learning", "comments": "ECCV 2016 Pre-press revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have created a large diverse set of cars from overhead images, which are\nuseful for training a deep learner to binary classify, detect and count them.\nThe dataset and all related material will be made publically available. The set\ncontains contextual matter to aid in identification of difficult targets. We\ndemonstrate classification and detection on this dataset using a neural network\nwe call ResCeption. This network combines residual learning with\nInception-style layers and is used to count cars in one look. This is a new way\nto count objects rather than by localization or density estimation. It is\nfairly accurate, fast and easy to implement. Additionally, the counting method\nis not car or scene specific. It would be easy to train this method to count\nother kinds of objects and counting over new scenes requires no extra set up or\nassumptions about object locations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 21:44:58 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Mundhenk", "T. Nathan", ""], ["Konjevod", "Goran", ""], ["Sakla", "Wesam A.", ""], ["Boakye", "Kofi", ""]]}, {"id": "1609.04468", "submitter": "Tom White", "authors": "Tom White", "title": "Sampling Generative Networks", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce several techniques for sampling and visualizing the latent\nspaces of generative models. Replacing linear interpolation with spherical\nlinear interpolation prevents diverging from a model's prior distribution and\nproduces sharper samples. J-Diagrams and MINE grids are introduced as\nvisualizations of manifolds created by analogies and nearest neighbors. We\ndemonstrate two new techniques for deriving attribute vectors: bias-corrected\nvectors with data replication and synthetic vectors with data augmentation.\nBinary classification using attribute vectors is presented as a technique\nsupporting quantitative analysis of the latent space. Most techniques are\nintended to be independent of model type and examples are shown on both\nVariational Autoencoders and Generative Adversarial Networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 22:42:23 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 09:38:48 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 14:39:05 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["White", "Tom", ""]]}, {"id": "1609.04846", "submitter": "Sebasti\\'an Basterrech", "authors": "Sebasti\\'an Basterrech and Gerardo Rubino", "title": "A Tutorial about Random Neural Networks in Supervised Learning", "comments": "This paper is a draft of an article to be published in Neural Network\n  World", "journal-ref": "Neural Network World, Volume 5, Number 15, pp.:457-499, 2015", "doi": "10.14311/NNW.2015.25.024", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can\nalso be seen as a specific type of queuing network. They have been successfully\nused in several domains during the last 25 years, as queuing networks to\nanalyze the performance of resource sharing in many engineering areas, as\nlearning tools and in combinatorial optimization, where they are seen as neural\nsystems, and also as models of neurological aspects of living beings. In this\narticle we focus on their learning capabilities, and more specifically, we\npresent a practical guide for using the RNN to solve supervised learning\nproblems. We give a general description of these models using almost\nindistinctly the terminology of Queuing Theory and the neural one. We present\nthe standard learning procedures used by RNNs, adapted from similar\nwell-established improvements in the standard NN field. We describe in\nparticular a set of learning algorithms covering techniques based on the use of\nfirst order and, then, of second order derivatives. We also discuss some issues\nrelated to these objects and present new perspectives about their use in\nsupervised learning problems. The tutorial describes their most relevant\napplications, and also provides a large bibliography.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:21:30 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Basterrech", "Sebasti\u00e1n", ""], ["Rubino", "Gerardo", ""]]}, {"id": "1609.04938", "submitter": "Yuntian Deng", "authors": "Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander M. Rush", "title": "Image-to-Markup Generation with Coarse-to-Fine Attention", "comments": "Accepted by ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural encoder-decoder model to convert images into\npresentational markup based on a scalable coarse-to-fine attention mechanism.\nOur method is evaluated in the context of image-to-LaTeX generation, and we\nintroduce a new dataset of real-world rendered mathematical expressions paired\nwith LaTeX markup. We show that unlike neural OCR techniques using CTC-based\nmodels, attention-based approaches can tackle this non-standard OCR task. Our\napproach outperforms classical mathematical OCR systems by a large margin on\nin-domain rendered data, and, with pretraining, also performs well on\nout-of-domain handwritten data. To reduce the inference complexity associated\nwith the attention-based approaches, we introduce a new coarse-to-fine\nattention layer that selects a support region before applying attention.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 08:14:50 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 22:48:53 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Deng", "Yuntian", ""], ["Kanervisto", "Anssi", ""], ["Ling", "Jeffrey", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1609.05123", "submitter": "Hamid Tizhoosh", "authors": "Shivam Kalra, Aditya Sriram, Shahryar Rahnamayan, H.R. Tizhoosh", "title": "Learning Opposites Using Neural Networks", "comments": "To appear in proceedings of the 23rd International Conference on\n  Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research works have successfully extended algorithms such as\nevolutionary algorithms, reinforcement agents and neural networks using\n\"opposition-based learning\" (OBL). Two types of the \"opposites\" have been\ndefined in the literature, namely \\textit{type-I} and \\textit{type-II}. The\nformer are linear in nature and applicable to the variable space, hence easy to\ncalculate. On the other hand, type-II opposites capture the \"oppositeness\" in\nthe output space. In fact, type-I opposites are considered a special case of\ntype-II opposites where inputs and outputs have a linear relationship. However,\nin many real-world problems, inputs and outputs do in fact exhibit a nonlinear\nrelationship. Therefore, type-II opposites are expected to be better in\ncapturing the sense of \"opposition\" in terms of the input-output relation. In\nthe absence of any knowledge about the problem at hand, there seems to be no\nintuitive way to calculate the type-II opposites. In this paper, we introduce\nan approach to learn type-II opposites from the given inputs and their outputs\nusing the artificial neural networks (ANNs). We first perform \\emph{opposition\nmining} on the sample data, and then use the mined data to learn the\nrelationship between input $x$ and its opposite $\\breve{x}$. We have validated\nour algorithm using various benchmark functions to compare it against an\nevolving fuzzy inference approach that has been recently introduced. The\nresults show the better performance of a neural approach to learn the\nopposites. This will create new possibilities for integrating oppositional\nschemes within existing algorithms promising a potential increase in\nconvergence speed and/or accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 16:19:56 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Kalra", "Shivam", ""], ["Sriram", "Aditya", ""], ["Rahnamayan", "Shahryar", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1609.05132", "submitter": "James Garland", "authors": "James Garland, David Gregg", "title": "Low Complexity Multiply Accumulate Unit for Weight-Sharing Convolutional\n  Neural Networks", "comments": "4 pages", "journal-ref": null, "doi": "10.1109/LCA.2017.2656880", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are one of the most successful deep\nmachine learning technologies for processing image, voice and video data. CNNs\nrequire large amounts of processing capacity and memory, which can exceed the\nresources of low power mobile and embedded systems. Several designs for\nhardware accelerators have been proposed for CNNs which typically contain large\nnumbers of Multiply Accumulate (MAC) units. One approach to reducing data sizes\nand memory traffic in CNN accelerators is \"weight sharing\", where the full\nrange of values in a trained CNN are put in bins and the bin index is stored\ninstead of the original weight value. In this paper we propose a novel MAC\ncircuit that exploits binning in weight-sharing CNNs. Rather than computing the\nMAC directly we instead count the frequency of each weight and place it in a\nbin. We then compute the accumulated value in a subsequent multiply phase. This\nallows hardware multipliers in the MAC circuit to be replaced with adders and\nselection logic. Experiments show that for the same clock speed our approach\nresults in fewer gates, smaller logic, and reduced power.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 13:41:41 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 19:23:59 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 14:36:21 GMT"}, {"version": "v4", "created": "Thu, 19 Jan 2017 16:07:03 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Garland", "James", ""], ["Gregg", "David", ""]]}, {"id": "1609.05284", "submitter": "Po-Sen Huang", "authors": "Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen", "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "comments": "in KDD 2017", "journal-ref": null, "doi": "10.1145/3097983.3098177", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching a computer to read and answer general questions pertaining to a\ndocument is a challenging yet unsolved problem. In this paper, we describe a\nnovel neural network architecture called the Reasoning Network (ReasoNet) for\nmachine comprehension tasks. ReasoNets make use of multiple turns to\neffectively exploit and then reason over the relation among queries, documents,\nand answers. Different from previous approaches using a fixed number of turns\nduring inference, ReasoNets introduce a termination state to relax this\nconstraint on the reasoning depth. With the use of reinforcement learning,\nReasoNets can dynamically determine whether to continue the comprehension\nprocess after digesting intermediate results, or to terminate reading when it\nconcludes that existing information is adequate to produce an answer. ReasoNets\nhave achieved exceptional performance in machine comprehension datasets,\nincluding unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset,\nand a structured Graph Reachability dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 05:12:50 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 06:29:36 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 01:12:07 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Shen", "Yelong", ""], ["Huang", "Po-Sen", ""], ["Gao", "Jianfeng", ""], ["Chen", "Weizhu", ""]]}, {"id": "1609.05396", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Benjam\\'in Guti\\'errez-Becker, Diana Mateus, Nassir\n  Navab, Nikos Komodakis", "title": "A Deep Metric for Multimodal Registration", "comments": "Accepted to MICCAI 2016; extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal registration is a challenging problem in medical imaging due the\nhigh variability of tissue appearance under different imaging modalities. The\ncrucial component here is the choice of the right similarity measure. We make a\nstep towards a general learning-based solution that can be adapted to specific\nsituations and present a metric based on a convolutional neural network. Our\nnetwork can be trained from scratch even from a few aligned image pairs. The\nmetric is validated on intersubject deformable registration on a dataset\ndifferent from the one used for training, demonstrating good generalization. In\nthis task, we outperform mutual information by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 21:46:21 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Simonovsky", "Martin", ""], ["Guti\u00e9rrez-Becker", "Benjam\u00edn", ""], ["Mateus", "Diana", ""], ["Navab", "Nassir", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1609.05716", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari, Simon Bull, Samaneh Kouchaki, Deborah Cooke and\n  Norman Poh", "title": "Visualisation of Survey Responses using Self-Organising Maps: A Case\n  Study on Diabetes Self-care Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the chronic nature of diabetes, patient self-care factors play an\nimportant role in any treatment plan. In order to understand the behaviour of\npatients in response to medical advice on self-care, clinicians often conduct\ncross-sectional surveys. When analysing the survey data, statistical machine\nlearning methods can potentially provide additional insight into the data\neither through deeper understanding of the patterns present or making\ninformation available to clinicians in an intuitive manner. In this study, we\nuse self-organising maps (SOMs) to visualise the responses of patients who\nshare similar responses to survey questions, with the goal of helping\nclinicians understand how patients are managing their treatment and where\naction should be taken. The principle behavioural patterns revealed through\nthis are that: patients who take the correct dose of insulin also tend to take\ntheir injections at the correct time, patients who eat on time also tend to\ncorrectly manage their food portions and patients who check their blood glucose\nwith a monitor also tend to adjust their insulin dosage and carry snacks to\ncounter low blood glucose. The identification of these positive behavioural\npatterns can also help to inform treatment by exploiting their negative\ncorollaries.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 18:49:50 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Tirunagari", "Santosh", ""], ["Bull", "Simon", ""], ["Kouchaki", "Samaneh", ""], ["Cooke", "Deborah", ""], ["Poh", "Norman", ""]]}, {"id": "1609.05866", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, Pascal Vincent", "title": "A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax content-based attention mechanism has proven to be very\nbeneficial in many applications of recurrent neural networks. Nevertheless it\nsuffers from two major computational limitations. First, its computations for\nan attention lookup scale linearly in the size of the attended sequence.\nSecond, it does not encode the sequence into a fixed-size representation but\ninstead requires to memorize all the hidden states. These two limitations\nrestrict the use of the softmax attention mechanism to relatively small-scale\napplications with short sequences and few lookups per sequence. In this work we\nintroduce a family of linear attention mechanisms designed to overcome the two\nlimitations listed above. We show that removing the softmax non-linearity from\nthe traditional attention formulation yields constant-time attention lookups\nand fixed-size representations of the attended sequences. These properties make\nthese linear attention mechanisms particularly suitable for large-scale\napplications with extreme query loads, real-time requirements and memory\nconstraints. Early experiments on a question answering task show that these\nlinear mechanisms yield significantly better accuracy results than no\nattention, but obviously worse than their softmax alternative.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 18:55:18 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1609.05884", "submitter": "Ammar Daskin", "authors": "Ammar Daskin", "title": "A Quantum Implementation Model for Artificial Neural Networks", "comments": null, "journal-ref": "Quanta, 7, pg: 7-18, 2018", "doi": "10.12743/quanta.v7i1.65", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning process for multi layered neural networks with many nodes makes\nheavy demands on computational resources. In some neural network models, the\nlearning formulas, such as the Widrow-Hoff formula, do not change the\neigenvectors of the weight matrix while flatting the eigenvalues. In infinity,\nthis iterative formulas result in terms formed by the principal components of\nthe weight matrix: i.e., the eigenvectors corresponding to the non-zero\neigenvalues. In quantum computing, the phase estimation algorithm is known to\nprovide speed-ups over the conventional algorithms for the eigenvalue-related\nproblems. Combining the quantum amplitude amplification with the phase\nestimation algorithm, a quantum implementation model for artificial neural\nnetworks using the Widrow-Hoff learning rule is presented. The complexity of\nthe model is found to be linear in the size of the weight matrix. This provides\na quadratic improvement over the classical algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:47:52 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 09:39:52 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Daskin", "Ammar", ""]]}, {"id": "1609.06374", "submitter": "Dimitrios Adamos Dr", "authors": "Fotis Kalaganis (1), Dimitrios A. Adamos (2 and 3), Nikos Laskaris (1\n  and 3) ((1) AIIA Lab, Department of Informatics, Aristotle University of\n  Thessaloniki, (2) School of Music Studies, Aristotle University of\n  Thessaloniki, (3) Neuroinformatics GRoup, Aristotle University of\n  Thessaloniki)", "title": "A Consumer BCI for Automated Music Evaluation Within a Popular On-Demand\n  Music Streaming Service - Taking Listener's Brainwaves to Extremes", "comments": "12th IFIP WG 12.5 International Conference and Workshops, AIAI 2016,\n  Thessaloniki, Greece, September 16-18, 2016, Proceedings", "journal-ref": "Artificial Intelligence Applications and Innovations, Volume 475\n  of the series IFIP Advances in Information and Communication Technology pp\n  429-440, 2016", "doi": "10.1007/978-3-319-44944-9_37", "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the possibility of using a machine-learning scheme in\nconjunction with commercial wearable EEG-devices for translating listener's\nsubjective experience of music into scores that can be used for the automated\nannotation of music in popular on-demand streaming services. Based on the\nestablished -neuroscientifically sound- concepts of brainwave frequency bands,\nactivation asymmetry index and cross-frequency-coupling (CFC), we introduce a\nBrain Computer Interface (BCI) system that automatically assigns a rating score\nto the listened song. Our research operated in two distinct stages: i) a\ngeneric feature engineering stage, in which features from signal-analytics were\nranked and selected based on their ability to associate music induced\nperturbations in brainwaves with listener's appraisal of music. ii) a\npersonalization stage, during which the efficiency of ex- treme learning\nmachines (ELMs) is exploited so as to translate the derived pat- terns into a\nlistener's score. Encouraging experimental results, from a pragmatic use of the\nsystem, are presented.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 22:29:02 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 11:06:37 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Kalaganis", "Fotis", "", "2 and 3"], ["Adamos", "Dimitrios A.", "", "2 and 3"], ["Laskaris", "Nikos", "", "1\n  and 3"]]}, {"id": "1609.06492", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic, Alessia Amelio, Zoran N. Milivojevic, Milena Jevtic", "title": "Document Image Coding and Clustering for Script Discrimination", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": "ICIC Express Letters Vol. 10 n. 7 July 2016 pp. 1561-1566", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new method for discrimination of documents given in\ndifferent scripts. The document is mapped into a uniformly coded text of\nnumerical values. It is derived from the position of the letters in the text\nline, based on their typographical characteristics. Each code is considered as\na gray level. Accordingly, the coded text determines a 1-D image, on which\ntexture analysis by run-length statistics and local binary pattern is\nperformed. It defines feature vectors representing the script content of the\ndocument. A modified clustering approach employed on document feature vector\ngroups documents written in the same script. Experimentation performed on two\ncustom oriented databases of historical documents in old Cyrillic, angular and\nround Glagolitic as well as Antiqua and Fraktur scripts demonstrates the\nsuperiority of the proposed method with respect to well-known methods in the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 10:52:03 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Brodic", "Darko", ""], ["Amelio", "Alessia", ""], ["Milivojevic", "Zoran N.", ""], ["Jevtic", "Milena", ""]]}, {"id": "1609.06560", "submitter": "Marcos Cardinot", "authors": "Marcos Cardinot, Colm O'Riordan, Josephine Griffith", "title": "The Optional Prisoner's Dilemma in a Spatial Environment: Coevolving\n  Game Strategy and Link Weights", "comments": "To be presented at ECTA 2016, Porto, Portugal", "journal-ref": "Proceedings of the 8th International Joint Conference on\n  Computational Intelligence, 86-93, 2016, Porto, Portugal", "doi": "10.5220/0006053900860093", "report-no": null, "categories": "cs.NE cs.MA math.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, the Optional Prisoner's Dilemma game in a spatial environment,\nwith coevolutionary rules for both the strategy and network links between\nagents, is studied. Using a Monte Carlo simulation approach, a number of\nexperiments are performed to identify favourable configurations of the\nenvironment for the emergence of cooperation in adverse scenarios. Results show\nthat abstainers play a key role in the protection of cooperators against\nexploitation from defectors. Scenarios of cyclic competition and of full\ndominance of cooperation are also observed. This work provides insights towards\ngaining an in-depth understanding of the emergence of cooperative behaviour in\nreal-world systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 22:24:57 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Cardinot", "Marcos", ""], ["O'Riordan", "Colm", ""], ["Griffith", "Josephine", ""]]}, {"id": "1609.06616", "submitter": "John J Nay", "authors": "John J. Nay", "title": "Gov2Vec: Learning Distributed Representations of Institutions and Their\n  Legal Text", "comments": "Forthcoming paper in the 2016 Proceedings of the Conference on\n  Empirical Methods in Natural Language Processing Workshop on Natural Language\n  Processing and Computational Social Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare policy differences across institutions by embedding\nrepresentations of the entire legal corpus of each institution and the\nvocabulary shared across all corpora into a continuous vector space. We apply\nour method, Gov2Vec, to Supreme Court opinions, Presidential actions, and\nofficial summaries of Congressional bills. The model discerns meaningful\ndifferences between government branches. We also learn representations for more\nfine-grained word sources: individual Presidents and (2-year) Congresses. The\nsimilarities between learned representations of Congresses over time and\nsitting Presidents are negatively correlated with the bill veto rate, and the\ntemporal ordering of Presidents and Congresses was implicitly learned from only\ntext. With the resulting vectors we answer questions such as: how does Obama\nand the 113th House differ in addressing climate change and how does this vary\nfrom environmental or economic perspectives? Our work illustrates\nvector-arithmetic-based investigations of complex relationships between word\nsources based on their texts. We are extending this to create a more\ncomprehensive legal semantic map.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 16:09:12 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 22:20:12 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Nay", "John J.", ""]]}, {"id": "1609.06666", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Dushyant Rao, Dominic Zeng Wang, Chi Hay Tong, Ingmar\n  Posner", "title": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient\n  Convolutional Neural Networks", "comments": "To be published at the IEEE International Conference on Robotics and\n  Automation 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computationally efficient approach to detecting objects\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\nparticular, this is achieved by leveraging a feature-centric voting scheme to\nimplement novel convolutional layers which explicitly exploit the sparsity\nencountered in the input. To this end, we examine the trade-off between\naccuracy and speed for different architectures and additionally propose to use\nan L1 penalty on the filter activations to further encourage sparsity in the\nintermediate representations. To the best of our knowledge, this is the first\nwork to propose sparse convolutional layers and L1 regularisation for efficient\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\non the KITTI object detection benchmark and show that Vote3Deep models with as\nfew as three layers outperform the previous state of the art in both laser and\nlaser-vision based approaches by margins of up to 40% while remaining highly\ncompetitive in terms of processing time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:32:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 15:29:45 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Engelcke", "Martin", ""], ["Rao", "Dushyant", ""], ["Wang", "Dominic Zeng", ""], ["Tong", "Chi Hay", ""], ["Posner", "Ingmar", ""]]}, {"id": "1609.06741", "submitter": "Kate\\v{r}ina Henclov\\'a", "authors": "Katerina Henclova", "title": "Using CMA-ES for tuning coupled PID controllers within models of\n  combustion engines", "comments": "28 pages (single column); after major revision; comparison with SHADE\n  method added; author's name and affiliation have changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional integral derivative (PID) controllers are important and widely\nused tools in system control. Tuning of the controller gains is a laborious\ntask, especially for complex systems such as combustion engines. To minimize\nthe time of an engineer for tuning of the gains in a simulation software, we\npropose to formulate a part of the problem as a black-box optimization task. In\nthis paper, we summarize the properties and practical limitations of tuning of\nthe gains in this particular application. We investigate the latest methods of\nblack-box optimization and conclude that the Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES) with bi-population restart strategy, elitist parent\nselection and active covariance matrix adaptation is best suited for this task.\nDetails of the algorithm's experiment-based calibration are explained as well\nas derivation of a suitable objective function. The method's performance is\ncompared with that of PSO and SHADE. Finally, its usability is verified on six\nmodels of real engines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 20:35:56 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 11:20:18 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 15:33:24 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 18:39:55 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Henclova", "Katerina", ""]]}, {"id": "1609.06845", "submitter": "Sebastien Lefevre", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "On the usability of deep networks for object-based image analysis", "comments": "in International Conference on Geographic Object-Based Image Analysis\n  (GEOBIA), Sep 2016, Enschede, Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computer vision before, remote sensing has been radically changed by the\nintroduction of Convolution Neural Networks. Land cover use, object detection\nand scene understanding in aerial images rely more and more on deep learning to\nachieve new state-of-the-art results. Recent architectures such as Fully\nConvolutional Networks (Long et al., 2015) can even produce pixel level\nannotations for semantic mapping. In this work, we show how to use such deep\nnetworks to detect, segment and classify different varieties of wheeled\nvehicles in aerial images from the ISPRS Potsdam dataset. This allows us to\ntackle object detection and classification on a complex dataset made up of\nvisually similar classes, and to demonstrate the relevance of such a subclass\nmodeling approach. Especially, we want to show that deep learning is also\nsuitable for object-oriented analysis of Earth Observation data. First, we\ntrain a FCN variant on the ISPRS Potsdam dataset and show how the learnt\nsemantic maps can be used to extract precise segmentation of vehicles, which\nallow us studying the repartition of vehicles in the city. Second, we train a\nCNN to perform vehicle classification on the VEDAI (Razakarivony and Jurie,\n2016) dataset, and transfer its knowledge to classify candidate segmented\nvehicles on the Potsdam dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:39:37 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1609.06846", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Semantic Segmentation of Earth Observation Data Using Multimodal and\n  Multi-scale Deep Networks", "comments": "Asian Conference on Computer Vision (ACCV16), Nov 2016, Taipei,\n  Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the use of deep fully convolutional neural networks\n(DFCNN) for pixel-wise scene labeling of Earth Observation images. Especially,\nwe train a variant of the SegNet architecture on remote sensing data over an\nurban area and study different strategies for performing accurate semantic\nsegmentation. Our contributions are the following: 1) we transfer efficiently a\nDFCNN from generic everyday images to remote sensing images; 2) we introduce a\nmulti-kernel convolutional layer for fast aggregation of predictions at\nmultiple scales; 3) we perform data fusion from heterogeneous sensors (optical\nand laser) using residual correction. Our framework improves state-of-the-art\naccuracy on the ISPRS Vaihingen 2D Semantic Labeling dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:42:06 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1609.06935", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Quantum Neural Machine Learning - Backpropagation and Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn nlin.AO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current work addresses quantum machine learning in the context of Quantum\nArtificial Neural Networks such that the networks' processing is divided in two\nstages: the learning stage, where the network converges to a specific quantum\ncircuit, and the backpropagation stage where the network effectively works as a\nself-programing quantum computing system that selects the quantum circuits to\nsolve computing problems. The results are extended to general architectures\nincluding recurrent networks that interact with an environment, coupling with\nit in the neural links' activation order, and self-organizing in a dynamical\nregime that intermixes patterns of dynamical stochasticity and persistent\nquasiperiodic dynamics, making emerge a form of noise resilient dynamical\nrecord.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 12:12:05 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1609.07061", "submitter": "Itay Hubara", "authors": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv and\n  Yoshua Bengio", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision\n  Weights and Activations", "comments": "arXiv admin note: text overlap with arXiv:1602.02830", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to train Quantized Neural Networks (QNNs) --- neural\nnetworks with extremely low precision (e.g., 1-bit) weights and activations, at\nrun-time. At train-time the quantized weights and activations are used for\ncomputing the parameter gradients. During the forward pass, QNNs drastically\nreduce memory size and accesses, and replace most arithmetic operations with\nbit-wise operations. As a result, power consumption is expected to be\ndrastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and\nImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to\ntheir 32-bit counterparts. For example, our quantized version of AlexNet with\n1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover,\nwe quantize the parameter gradients to 6-bits as well which enables gradients\ncomputation using only bit-wise operation. Quantized recurrent neural networks\nwere tested over the Penn Treebank dataset, and achieved comparable accuracy as\ntheir 32-bit counterparts using only 4-bits. Last but not least, we programmed\na binary matrix multiplication GPU kernel with which it is possible to run our\nMNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering\nany loss in classification accuracy. The QNN code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:48:03 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Hubara", "Itay", ""], ["Courbariaux", "Matthieu", ""], ["Soudry", "Daniel", ""], ["El-Yaniv", "Ran", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1609.07093", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Neural Photo Editing with Introspective Adversarial Networks", "comments": "10 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 18:07:56 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 13:16:21 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 18:46:50 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1609.07160", "submitter": "Yonghua Yin", "authors": "Yonghua Yin and Erol Gelenbe", "title": "Deep Learning in Multi-Layer Architectures of Dense Nuclei", "comments": "10 pages (a small edit to the abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume that, within the dense clusters of neurons that can be found in\nnuclei, cells may interconnect via soma-to-soma interactions, in addition to\nconventional synaptic connections. We illustrate this idea with a multi-layer\narchitecture (MLA) composed of multiple clusters of recurrent sub-networks of\nspiking Random Neural Networks (RNN) with dense soma-to-soma interactions, and\nuse this RNN-MLA architecture for deep learning. The inputs to the clusters are\nfirst normalised by adjusting the external arrival rates of spikes to each\ncluster. Then we apply this architecture to learning from multi-channel\ndatasets. Numerical results based on both images and sensor based data, show\nthe value of this novel architecture for deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 20:55:16 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 11:19:23 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Yin", "Yonghua", ""], ["Gelenbe", "Erol", ""]]}, {"id": "1609.07215", "submitter": "Rajasekar Venkatesan", "authors": "Mihika Dave, Sahil Tapiawala, Meng Joo Er, Rajasekar Venkatesan", "title": "A Novel Progressive Multi-label Classifier for Classincremental Data", "comments": "5 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a progressive learning algorithm for multi-label\nclassification to learn new labels while retaining the knowledge of previous\nlabels is designed. New output neurons corresponding to new labels are added\nand the neural network connections and parameters are automatically\nrestructured as if the label has been introduced from the beginning. This work\nis the first of the kind in multi-label classifier for class-incremental\nlearning. It is useful for real-world applications such as robotics where\nstreaming data are available and the number of labels is often unknown. Based\non the Extreme Learning Machine framework, a novel universal classifier with\nplug and play capabilities for progressive multi-label classification is\ndeveloped. Experimental results on various benchmark synthetic and real\ndatasets validate the efficiency and effectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 03:09:24 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Dave", "Mihika", ""], ["Tapiawala", "Sahil", ""], ["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""]]}, {"id": "1609.07378", "submitter": "Anton Bezuglov", "authors": "Anton Bezuglov, Brian Blanton, and Reinaldo Santiago", "title": "Multi-Output Artificial Neural Network for Storm Surge Prediction in\n  North Carolina", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During hurricane seasons, emergency managers and other decision makers need\naccurate and `on-time' information on potential storm surge impacts. Fully\ndynamical computer models, such as the ADCIRC tide, storm surge, and wind-wave\nmodel take several hours to complete a forecast when configured at high spatial\nresolution. Additionally, statically meaningful ensembles of high-resolution\nmodels (needed for uncertainty estimation) cannot easily be computed in near\nreal-time. This paper discusses an artificial neural network model for storm\nsurge prediction in North Carolina. The network model provides fast, real-time\nstorm surge estimates at coastal locations in North Carolina. The paper studies\nthe performance of the neural network model vs. other models on synthetic and\nreal hurricane data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:24:44 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Bezuglov", "Anton", ""], ["Blanton", "Brian", ""], ["Santiago", "Reinaldo", ""]]}, {"id": "1609.07434", "submitter": "Matt Oberdorfer", "authors": "Matt Oberdorfer, Matt Abuzalaf", "title": "Regulating Reward Training by Means of Certainty Prediction in a Neural\n  Network-Implemented Pong Game", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first reinforcement-learning model to self-improve its\nreward-modulated training implemented through a continuously improving\n\"intuition\" neural network. An agent was trained how to play the arcade video\ngame Pong with two reward-based alternatives, one where the paddle was placed\nrandomly during training, and a second where the paddle was simultaneously\ntrained on three additional neural networks such that it could develop a sense\nof \"certainty\" as to how probable its own predicted paddle position will be to\nreturn the ball. If the agent was less than 95% certain to return the ball, the\npolicy used an intuition neural network to place the paddle. We trained both\narchitectures for an equivalent number of epochs and tested learning\nperformance by letting the trained programs play against a near-perfect\nopponent. Through this, we found that the reinforcement learning model that\nuses an intuition neural network for placing the paddle during reward training\nquickly overtakes the simple architecture in its ability to outplay the\nnear-perfect opponent, additionally outscoring that opponent by an increasingly\nwide margin after additional epochs of training.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 17:11:53 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Oberdorfer", "Matt", ""], ["Abuzalaf", "Matt", ""]]}, {"id": "1609.07706", "submitter": "Lana Sinapayen", "authors": "Lana Sinapayen, Atsushi Masumori, Takashi Ikegami", "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural\n  Networks Dynamics", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0170388", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based on networks of real neurons, and by extension biologically\ninspired models of neural networks, has yet to find general learning rules\nleading to widespread applications. In this paper, we argue for the existence\nof a principle allowing to steer the dynamics of a biologically inspired neural\nnetwork. Using carefully timed external stimulation, the network can be driven\ntowards a desired dynamical state. We term this principle \"Learning by\nStimulation Avoidance\" (LSA). We demonstrate through simulation that the\nminimal sufficient conditions leading to LSA in artificial networks are also\nsufficient to reproduce learning results similar to those obtained in\nbiological neurons by Shahaf and Marom [1]. We examine the mechanism's basic\ndynamics in a reduced network, and demonstrate how it scales up to a network of\n100 neurons. We show that LSA has a higher explanatory power than existing\nhypotheses about the response of biological neural networks to external\nsimulation, and can be used as a learning rule for an embodied application:\nlearning of wall avoidance by a simulated robot. The surge in popularity of\nartificial neural networks is mostly directed to disembodied models of neurons\nwith biologically irrelevant dynamics: to the authors' knowledge, this is the\nfirst work demonstrating sensory-motor learning with random spiking networks\nthrough pure Hebbian learning.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 06:44:42 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 05:38:53 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sinapayen", "Lana", ""], ["Masumori", "Atsushi", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1609.07722", "submitter": "Payam Zahadat", "authors": "Thomas Schmickl, Payam Zahadat and Heiko Hamann", "title": "Sooner than Expected: Hitting the Wall of Complexity in Evolution", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary robotics an encoding of the control software, which maps\nsensor data (input) to motor control values (output), is shaped by stochastic\noptimization methods to complete a predefined task. This approach is assumed to\nbe beneficial compared to standard methods of controller design in those cases\nwhere no a-priori model is available that could help to optimize performance.\nAlso for robots that have to operate in unpredictable environments, an\nevolutionary robotics approach is favorable. We demonstrate here that such a\nmodel-free approach is not a free lunch, as already simple tasks can represent\nunsolvable barriers for fully open-ended uninformed evolutionary computation\ntechniques. We propose here the 'Wankelmut' task as an objective for an\nevolutionary approach that starts from scratch without pre-shaped controller\nsoftware or any other informed approach that would force the behavior to be\nevolved in a desired way. Our focal claim is that 'Wankelmut' represents the\nsimplest set of problems that makes plain-vanilla evolutionary computation\nfail. We demonstrate this by a series of simple standard evolutionary\napproaches using different fitness functions and standard artificial neural\nnetworks as well as continuous-time recurrent neural networks. All our tested\napproaches failed. We claim that any other evolutionary approach will also fail\nthat does per-se not favor or enforce modularity and does not freeze or protect\nalready evolved functionalities. Thus we propose a hard-to-pass benchmark and\nmake a strong statement for self-complexifying and generative approaches in\nevolutionary computation. We anticipate that defining such a 'simplest task to\nfail' is a valuable benchmark for promoting future development in the field of\nartificial intelligence, evolutionary robotics and artificial life.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 10:00:41 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Schmickl", "Thomas", ""], ["Zahadat", "Payam", ""], ["Hamann", "Heiko", ""]]}, {"id": "1609.07724", "submitter": "Athanasios Vlontzos", "authors": "Athanasios Vlontzos", "title": "The RNN-ELM Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine learning methods combining the Random Neural\nNetwork, a biologically inspired neural network and the Extreme Learning\nMachine that achieve state of the art classification performance while\nrequiring much shorter training time. The Random Neural Network is a integrate\nand fire computational model of a neural network whose mathematical structure\npermits the efficient analysis of large ensembles of neurons. An activation\nfunction is derived from the RNN and used in an Extreme Learning Machine. We\ncompare the performance of this combination against the ELM with various\nactivation functions, we reduce the input dimensionality via PCA and compare\nits performance vs. autoencoder based versions of the RNN-ELM.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 10:18:19 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Vlontzos", "Athanasios", ""]]}, {"id": "1609.07750", "submitter": "Ahmed Abdelsalam Mr", "authors": "Ahmed M. Abdelsalam, J.M. Pierre Langlois and F. Cheriet", "title": "Accurate and Efficient Hyperbolic Tangent Activation Function on FPGA\n  using the DCT Interpolation Filter", "comments": "8 pages, 6 figures, 5 tables, submitted for the 25th ACM/SIGDA\n  International Symposium on Field-Programmable Gate Arrays (ISFPGA), 22-24\n  February 2017, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Implementing an accurate and fast activation function with low cost is a\ncrucial aspect to the implementation of Deep Neural Networks (DNNs) on FPGAs.\nWe propose a high-accuracy approximation approach for the hyperbolic tangent\nactivation function of artificial neurons in DNNs. It is based on the Discrete\nCosine Transform Interpolation Filter (DCTIF). The proposed architecture\ncombines simple arithmetic operations on stored samples of the hyperbolic\ntangent function and on input data. The proposed DCTIF implementation achieves\ntwo orders of magnitude greater precision than previous work while using the\nsame or fewer computational resources. Various combinations of DCTIF parameters\ncan be chosen to tradeoff the accuracy and complexity of the hyperbolic tangent\nfunction. In one case, the proposed architecture approximates the hyperbolic\ntangent activation function with 10E-5 maximum error while requiring only 1.52\nKbits memory and 57 LUTs of a Virtex-7 FPGA. We also discuss how the activation\nfunction accuracy affects the performance of DNNs in terms of their training\nand testing accuracies. We show that a high accuracy approximation can be\nnecessary in order to maintain the same DNN training and testing performances\nrealized by the exact function.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 14:30:33 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Abdelsalam", "Ahmed M.", ""], ["Langlois", "J. M. Pierre", ""], ["Cheriet", "F.", ""]]}, {"id": "1609.07959", "submitter": "Benjamin Krause", "authors": "Ben Krause, Liang Lu, Iain Murray, Steve Renals", "title": "Multiplicative LSTM for sequence modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce multiplicative LSTM (mLSTM), a recurrent neural network\narchitecture for sequence modelling that combines the long short-term memory\n(LSTM) and multiplicative recurrent neural network architectures. mLSTM is\ncharacterised by its ability to have different recurrent transition functions\nfor each possible input, which we argue makes it more expressive for\nautoregressive density estimation. We demonstrate empirically that mLSTM\noutperforms standard LSTM and its deep variants for a range of character level\nlanguage modelling tasks. In this version of the paper, we regularise mLSTM to\nachieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also\napply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a\ncharacter level entropy of 1.26 bits/char, corresponding to a word level\nperplexity of 88.8, which is comparable to word level LSTMs regularised in\nsimilar ways on the same task.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 13:12:51 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 10:30:16 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 17:05:47 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Krause", "Ben", ""], ["Lu", "Liang", ""], ["Murray", "Iain", ""], ["Renals", "Steve", ""]]}, {"id": "1609.08082", "submitter": "Longmei Li", "authors": "Longmei Li, Iryna Yevseyeva, Vitor Basto-Fernandes, Heike Trautmann,\n  Ning Jing and Michael Emmerich", "title": "An Ontology of Preference-Based Multiobjective Metaheuristics", "comments": "submitted to European Journal of Operational Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User preference integration is of great importance in multi-objective\noptimization, in particular in many objective optimization. Preferences have\nlong been considered in traditional multicriteria decision making (MCDM) which\nis based on mathematical programming. Recently, it is integrated in\nmulti-objective metaheuristics (MOMH), resulting in focus on preferred parts of\nthe Pareto front instead of the whole Pareto front. The number of publications\non preference-based multi-objective metaheuristics has increased rapidly over\nthe past decades. There already exist various preference handling methods and\nMOMH methods, which have been combined in diverse ways. This article proposes\nto use the Web Ontology Language (OWL) to model and systematize the results\ndeveloped in this field. A review of the existing work is provided, based on\nwhich an ontology is built and instantiated with state-of-the-art results. The\nOWL ontology is made public and open to future extension. Moreover, the usage\nof the ontology is exemplified for different use-cases, including querying for\nmethods that match an engineering application, bibliometric analysis, checking\nexistence of combinations of preference models and MOMH techniques, and\ndiscovering opportunities for new research and open research questions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 17:16:54 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 13:58:27 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Li", "Longmei", ""], ["Yevseyeva", "Iryna", ""], ["Basto-Fernandes", "Vitor", ""], ["Trautmann", "Heike", ""], ["Jing", "Ning", ""], ["Emmerich", "Michael", ""]]}, {"id": "1609.08194", "submitter": "Lei Yu", "authors": "Lei Yu, Jan Buys and Phil Blunsom", "title": "Online Segment to Segment Neural Transduction", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an online neural sequence to sequence model that learns to\nalternate between encoding and decoding segments of the input as it is read. By\nindependently tracking the encoding and decoding representations our algorithm\npermits exact polynomial marginalization of the latent segmentation during\ntraining, and during decoding beam search is employed to find the best\nalignment path together with the predicted output sequence. Our model tackles\nthe bottleneck of vanilla encoder-decoders that have to read and memorize the\nentire input sequence in their fixed-length hidden states before producing any\noutput. It is different from previous attentive models in that, instead of\ntreating the attention weights as output of a deterministic function, our model\nassigns attention weights to a sequential latent variable which can be\nmarginalized out and permits online generation. Experiments on abstractive\nsentence summarization and morphological inflection show significant\nperformance gains over the baseline encoder-decoders.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 21:13:49 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Yu", "Lei", ""], ["Buys", "Jan", ""], ["Blunsom", "Phil", ""]]}, {"id": "1609.08337", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Lantian Li and Dong Wang", "title": "Multi-task Recurrent Model for True Multilingual Speech Recognition", "comments": "APSIPA 2016. arXiv admin note: text overlap with arXiv:1603.09643", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on multilingual speech recognition remains attractive yet\nchallenging. Recent studies focus on learning shared structures under the\nmulti-task paradigm, in particular a feature sharing structure. This approach\nhas been found effective to improve performance on each individual language.\nHowever, this approach is only useful when the deployed system supports just\none language. In a true multilingual scenario where multiple languages are\nallowed, performance will be significantly reduced due to the competition among\nlanguages in the decoding space. This paper presents a multi-task recurrent\nmodel that involves a multilingual speech recognition (ASR) component and a\nlanguage recognition (LR) component, and the ASR component is informed of the\nlanguage information by the LR component, leading to a language-aware\nrecognition. We tested the approach on an English-Chinese bilingual recognition\ntask. The results show that the proposed multi-task recurrent model can improve\nperformance of multilingual recognition systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 09:56:09 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Li", "Lantian", ""], ["Wang", "Dong", ""]]}, {"id": "1609.08414", "submitter": "Mohamed Moustafa", "authors": "Hesham Eraqi, Youssef EmadEldin and Mohamed Moustafa", "title": "Reactive Collision Avoidance using Evolutionary Neural Networks", "comments": "ECTA 2016. Final paper is at SCITEPRESS digital library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision avoidance systems can play a vital role in reducing the number of\naccidents and saving human lives. In this paper, we introduce and validate a\nnovel method for vehicles reactive collision avoidance using evolutionary\nneural networks (ENN). A single front-facing rangefinder sensor is the only\ninput required by our method. The training process and the proposed method\nanalysis and validation are carried out using simulation. Extensive experiments\nare conducted to analyse the proposed method and evaluate its performance.\nFirstly, we experiment the ability to learn collision avoidance in a static\nfree track. Secondly, we analyse the effect of the rangefinder sensor\nresolution on the learning process. Thirdly, we experiment the ability of a\nvehicle to individually and simultaneously learn collision avoidance. Finally,\nwe test the generality of the proposed method. We used a more realistic and\npowerful simulation environment (CarMaker), a camera as an alternative input\nsensor, and lane keeping as an extra feature to learn. The results are\nencouraging; the proposed method successfully allows vehicles to learn\ncollision avoidance in different scenarios that are unseen during training. It\nalso generalizes well if any of the input sensor, the simulator, or the task to\nbe learned is changed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:26:10 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Eraqi", "Hesham", ""], ["EmadEldin", "Youssef", ""], ["Moustafa", "Mohamed", ""]]}, {"id": "1609.08663", "submitter": "Safoora Yousefi", "authors": "Safoora Yousefi, Congzheng Song, Nelson Nauata, Lee Cooper", "title": "Learning Genomic Representations to Predict Clinical Outcomes in Cancer", "comments": "ICLR 2016 Workshop Track- May 2nd 2016 International Conference on\n  Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomics are rapidly transforming medical practice and basic biomedical\nresearch, providing insights into disease mechanisms and improving therapeutic\nstrategies, particularly in cancer. The ability to predict the future course of\na patient's disease from high-dimensional genomic profiling will be essential\nin realizing the promise of genomic medicine, but presents significant\nchallenges for state-of-the-art survival analysis methods. In this abstract we\npresent an investigation in learning genomic representations with neural\nnetworks to predict patient survival in cancer. We demonstrate the advantages\nof this approach over existing survival analysis methods using brain tumor\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 20:53:16 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Yousefi", "Safoora", ""], ["Song", "Congzheng", ""], ["Nauata", "Nelson", ""], ["Cooper", "Lee", ""]]}, {"id": "1609.08686", "submitter": "Sukru Burc Eryilmaz", "authors": "S. Burc Eryilmaz, Emre Neftci, Siddharth Joshi, SangBum Kim, Matthew\n  BrightSky, Hsiang-Lan Lung, Chung Lam, Gert Cauwenberghs, H.-S. Philip Wong", "title": "Training a Probabilistic Graphical Model with Resistive Switching\n  Electronic Synapses", "comments": "Accepted for publication in IEEE Transactions on Electron Devices.\n  This version is the submitted version", "journal-ref": null, "doi": "10.1109/TED.2016.2616483", "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current large scale implementations of deep learning and data mining require\nthousands of processors, massive amounts of off-chip memory, and consume\ngigajoules of energy. Emerging memory technologies such as nanoscale\ntwo-terminal resistive switching memory devices offer a compact, scalable and\nlow power alternative that permits on-chip co-located processing and memory in\nfine-grain distributed parallel architecture. Here we report first use of\nresistive switching memory devices for implementing and training a Restricted\nBoltzmann Machine (RBM), a generative probabilistic graphical model as a key\ncomponent for unsupervised learning in deep networks. We experimentally\ndemonstrate a 45-synapse RBM realized with 90 resistive switching phase change\nmemory (PCM) elements trained with a bio-inspired variant of the Contrastive\nDivergence (CD) algorithm, implementing Hebbian and anti-Hebbian weight\nupdates. The resistive PCM devices show a two-fold to ten-fold reduction in\nerror rate in a missing pixel pattern completion task trained over 30 epochs,\ncompared to untrained case. Measured programming energy consumption is 6.1 nJ\nper epoch with the resistive switching PCM devices, a factor of ~150 times\nlower than conventional processor-memory systems. We analyze and discuss the\ndependence of learning performance on cycle-to-cycle variations as well as\nnumber of gradual levels in the PCM analog memory devices.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 22:07:48 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 03:20:46 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Eryilmaz", "S. Burc", ""], ["Neftci", "Emre", ""], ["Joshi", "Siddharth", ""], ["Kim", "SangBum", ""], ["BrightSky", "Matthew", ""], ["Lung", "Hsiang-Lan", ""], ["Lam", "Chung", ""], ["Cauwenberghs", "Gert", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1609.08703", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee", "title": "Optimizing Neural Network Hyperparameters with Gaussian Processes for\n  Dialog Act Classification", "comments": "Accepted as a conference paper at IEEE SLT 2016. The two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems based on artificial neural networks (ANNs) have achieved\nstate-of-the-art results in many natural language processing tasks. Although\nANNs do not require manually engineered features, ANNs have many\nhyperparameters to be optimized. The choice of hyperparameters significantly\nimpacts models' performances. However, the ANN hyperparameters are typically\nchosen by manual, grid, or random search, which either requires expert\nexperiences or is computationally expensive. Recent approaches based on\nBayesian optimization using Gaussian processes (GPs) is a more systematic way\nto automatically pinpoint optimal or near-optimal machine learning\nhyperparameters. Using a previously published ANN model yielding\nstate-of-the-art results for dialog act classification, we demonstrate that\noptimizing hyperparameters using GP further improves the results, and reduces\nthe computational time by a factor of 4 compared to a random search. Therefore\nit is a useful technique for tuning ANN models to yield the best performances\nfor natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 23:10:42 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""]]}, {"id": "1609.08789", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Ying Shi, Dong Wang, Yang Feng and Shiyue Zhang", "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech\n  Recognition", "comments": "ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown clear superiority in sequence\nmodeling, particularly the ones with gated units, such as long short-term\nmemory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties\nbehind the remarkable performance remain unclear in many applications, e.g.,\nautomatic speech recognition (ASR). This paper employs visualization techniques\nto study the behavior of LSTM and GRU when performing speech recognition tasks.\nOur experiments show some interesting patterns in the gated memory, and some of\nthem have inspired simple yet effective modifications on the network structure.\nWe report two of such modifications: (1) lazy cell update in LSTM, and (2)\nshortcut connections for residual learning. Both modifications lead to more\ncomprehensible and powerful networks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 06:26:16 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 09:25:14 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 02:07:34 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Shi", "Ying", ""], ["Wang", "Dong", ""], ["Feng", "Yang", ""], ["Zhang", "Shiyue", ""]]}, {"id": "1609.09116", "submitter": "Yu Ding", "authors": "Yu Ding", "title": "Analysis of Massive Heterogeneous Temporal-Spatial Data with 3D\n  Self-Organizing Map and Time Vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organizing map(SOM) have been widely applied in clustering, this paper\nfocused on centroids of clusters and what they reveal. When the input vectors\nconsists of time, latitude and longitude, the map can be strongly linked to\nphysical world, providing valuable information. Beyond basic clustering, a\nnovel approach to address the temporal element is developed, enabling 3D SOM to\ntrack behaviors in multiple periods concurrently. Combined with adaptations\ntargeting to process heterogeneous data relating to distribution in time and\nspace, the paper offers a fresh scope for business and services based on\ntemporal-spatial pattern.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 08:25:40 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Ding", "Yu", ""]]}, {"id": "1609.09158", "submitter": "Akhilesh Jaiswal", "authors": "Akhilesh Jaiswal, Sourjya Roy, Gopalakrishnan Srinivasan, Kaushik Roy", "title": "Proposal for a Leaky-Integrate-Fire Spiking Neuron based on\n  Magneto-Electric Switching of Ferro-magnets", "comments": null, "journal-ref": null, "doi": "10.1109/TED.2017.2671353", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of the human brain in performing classification tasks has\nattracted considerable research interest in brain-inspired neuromorphic\ncomputing. Hardware implementations of a neuromorphic system aims to mimic the\ncomputations in the brain through interconnection of neurons and synaptic\nweights. A leaky-integrate-fire (LIF) spiking model is widely used to emulate\nthe dynamics of neuronal action potentials. In this work, we propose a spin\nbased LIF spiking neuron using the magneto-electric (ME) switching of\nferro-magnets. The voltage across the ME oxide exhibits a typical\nleaky-integrate behavior, which in turn switches an underlying ferro-magnet.\nDue to the effect of thermal noise, the ferro-magnet exhibits probabilistic\nswitching dynamics, which is reminiscent of the stochasticity exhibited by\nbiological neurons. The energy-efficiency of the ME switching mechanism coupled\nwith the intrinsic non-volatility of ferro-magnets result in lower energy\nconsumption, when compared to a CMOS LIF neuron. A device to system-level\nsimulation framework has been developed to investigate the feasibility of the\nproposed LIF neuron for a hand-written digit recognition problem\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 00:03:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Jaiswal", "Akhilesh", ""], ["Roy", "Sourjya", ""], ["Srinivasan", "Gopalakrishnan", ""], ["Roy", "Kaushik", ""]]}, {"id": "1609.09315", "submitter": "Tom\\'a\\v{s} Ko\\v{c}isk\\'y", "authors": "Tom\\'a\\v{s} Ko\\v{c}isk\\'y and G\\'abor Melis and Edward Grefenstette\n  and Chris Dyer and Wang Ling and Phil Blunsom and Karl Moritz Hermann", "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel semi-supervised approach for sequence transduction and\napply it to semantic parsing. The unsupervised component is based on a\ngenerative model in which latent sentences generate the unpaired logical forms.\nWe apply this method to a number of semantic parsing tasks focusing on domains\nwith limited access to labelled training data and extend those datasets with\nsynthetically generated logical forms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 12:20:13 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Melis", "G\u00e1bor", ""], ["Grefenstette", "Edward", ""], ["Dyer", "Chris", ""], ["Ling", "Wang", ""], ["Blunsom", "Phil", ""], ["Hermann", "Karl Moritz", ""]]}]