[{"id": "1409.0280", "submitter": "Alireza Goudarzi", "authors": "Alireza Goudarzi and Darko Stefanovic", "title": "Towards a Calculus of Echo State Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a recent trend in neural networks which uses the\ndynamical perturbations on the phase space of a system to compute a desired\ntarget function. We present how one can formulate an expectation of system\nperformance in a simple class of reservoir computing called echo state\nnetworks. In contrast with previous theoretical frameworks, which only reveal\nan upper bound on the total memory in the system, we analytically calculate the\nentire memory curve as a function of the structure of the system and the\nproperties of the input and the target function. We demonstrate the precision\nof our framework by validating its result for a wide range of system sizes and\nspectral radii. Our analytical calculation agrees with numerical simulations.\nTo the best of our knowledge this work presents the first exact analytical\ncharacterization of the memory curve in echo state networks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 02:12:04 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Goudarzi", "Alireza", ""], ["Stefanovic", "Darko", ""]]}, {"id": "1409.0334", "submitter": "Xiaoran Jiang", "authors": "Xiaoran Jiang, Vincent Gripon, Claude Berrou and Michael Rabbat", "title": "Storing sequences in binary tournament-based neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extension to a recently introduced architecture of clique-based neural\nnetworks is presented. This extension makes it possible to store sequences with\nhigh efficiency. To obtain this property, network connections are provided with\norientation and with flexible redundancy carried by both spatial and temporal\nredundancy, a mechanism of anticipation being introduced in the model. In\naddition to the sequence storage with high efficiency, this new scheme also\noffers biological plausibility. In order to achieve accurate sequence\nretrieval, a double layered structure combining hetero-association and\nauto-association is also proposed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 08:59:27 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Jiang", "Xiaoran", ""], ["Gripon", "Vincent", ""], ["Berrou", "Claude", ""], ["Rabbat", "Michael", ""]]}, {"id": "1409.0470", "submitter": "Tom Froese", "authors": "Alexander Woodward, Tom Froese, Takashi Ikegami", "title": "Neural coordination can be enhanced by occasional interruption of normal\n  firing patterns: A self-optimizing spiking neural network model", "comments": "22 pages, 6 figures; Neural Networks, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state space of a conventional Hopfield network typically exhibits many\ndifferent attractors of which only a small subset satisfy constraints between\nneurons in a globally optimal fashion. It has recently been demonstrated that\ncombining Hebbian learning with occasional alterations of normal neural states\navoids this problem by means of self-organized enlargement of the best basins\nof attraction. However, so far it is not clear to what extent this process of\nself-optimization is also operative in real brains. Here we demonstrate that it\ncan be transferred to more biologically plausible neural networks by\nimplementing a self-optimizing spiking neural network model. In addition, by\nusing this spiking neural network to emulate a Hopfield network with Hebbian\nlearning, we attempt to make a connection between rate-based and temporal\ncoding based neural systems. Although further work is required to make this\nmodel more realistic, it already suggests that the efficacy of the\nself-optimizing process is independent from the simplifying assumptions of a\nconventional Hopfield network. We also discuss natural and cultural processes\nthat could be responsible for occasional alteration of neural firing patterns\nin actual brains\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 16:20:41 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Woodward", "Alexander", ""], ["Froese", "Tom", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1409.0473", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "comments": "Accepted at ICLR 2015 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 16:33:02 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 18:32:00 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 18:10:39 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 21:39:11 GMT"}, {"version": "v5", "created": "Sun, 22 Mar 2015 17:08:39 GMT"}, {"version": "v6", "created": "Fri, 24 Apr 2015 13:25:33 GMT"}, {"version": "v7", "created": "Thu, 19 May 2016 21:53:22 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1057", "submitter": "Uwe Aickelin", "authors": "Alexandros Ladas, Jonathan M. Garibaldi, Rodrigo Scarpel and Uwe\n  Aickelin", "title": "Augmented Neural Networks for Modelling Consumer Indebtness", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 3086-3093, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer Debt has risen to be an important problem of modern societies,\ngenerating a lot of research in order to understand the nature of consumer\nindebtness, which so far its modelling has been carried out by statistical\nmodels. In this work we show that Computational Intelligence can offer a more\nholistic approach that is more suitable for the complex relationships an\nindebtness dataset has and Linear Regression cannot uncover. In particular, as\nour results show, Neural Networks achieve the best performance in modelling\nconsumer indebtness, especially when they manage to incorporate the significant\nand experimentally verified results of the Data Mining process in the model,\nexploiting the flexibility Neural Networks offer in designing their topology.\nThis novel method forms an elaborate framework to model Consumer indebtness\nthat can be extended to any other real world application.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:23:50 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Ladas", "Alexandros", ""], ["Garibaldi", "Jonathan M.", ""], ["Scarpel", "Rodrigo", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.1073", "submitter": "Jun He", "authors": "Xinsheng Lai, Yuren Zhou, Jun He and Jun Zhang", "title": "Performance Analysis on Evolutionary Algorithms for the Minimum Label\n  Spanning Tree Problem", "comments": null, "journal-ref": null, "doi": "10.1109/TEVC.2013.2291790", "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some experimental investigations have shown that evolutionary algorithms\n(EAs) are efficient for the minimum label spanning tree (MLST) problem.\nHowever, we know little about that in theory. As one step towards this issue,\nwe theoretically analyze the performances of the (1+1) EA, a simple version of\nEAs, and a multi-objective evolutionary algorithm called GSEMO on the MLST\nproblem. We reveal that for the MLST$_{b}$ problem the (1+1) EA and GSEMO\nachieve a $\\frac{b+1}{2}$-approximation ratio in expected polynomial times of\n$n$ the number of nodes and $k$ the number of labels. We also show that GSEMO\nachieves a $(2ln(n))$-approximation ratio for the MLST problem in expected\npolynomial time of $n$ and $k$. At the same time, we show that the (1+1) EA and\nGSEMO outperform local search algorithms on three instances of the MLST\nproblem. We also construct an instance on which GSEMO outperforms the (1+1) EA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 13:17:31 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Lai", "Xinsheng", ""], ["Zhou", "Yuren", ""], ["He", "Jun", ""], ["Zhang", "Jun", ""]]}, {"id": "1409.1143", "submitter": "Narine Manukyan", "authors": "Narine Manukyan, Margaret J. Eppstein, and Jeffrey S. Buzas", "title": "Tunably Rugged Landscapes with Known Maximum and Minimum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose NM landscapes as a new class of tunably rugged benchmark problems.\nNM landscapes are well-defined on alphabets of any arity, including both\ndiscrete and real-valued alphabets, include epistasis in a natural and\ntransparent manner, are proven to have known value and location of the global\nmaximum and, with some additional constraints, are proven to also have a known\nglobal minimum. Empirical studies are used to illustrate that, when\ncoefficients are selected from a recommended distribution, the ruggedness of NM\nlandscapes is smoothly tunable and correlates with several measures of search\ndifficulty. We discuss why these properties make NM landscapes preferable to\nboth NK landscapes and Walsh polynomials as benchmark landscape models with\ntunable epistasis.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 16:20:43 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Manukyan", "Narine", ""], ["Eppstein", "Margaret J.", ""], ["Buzas", "Jeffrey S.", ""]]}, {"id": "1409.1257", "submitter": "KyungHyun Cho", "authors": "Jean Pouget-Abadie and Dzmitry Bahdanau and Bart van Merrienboer and\n  Kyunghyun Cho and Yoshua Bengio", "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation\n  using Automatic Segmentation", "comments": "Eighth Workshop on Syntax, Semantics and Structure in Statistical\n  Translation (SSST-8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors of (Cho et al., 2014a) have shown that the recently introduced\nneural network translation systems suffer from a significant drop in\ntranslation quality when translating long sentences, unlike existing\nphrase-based translation systems. In this paper, we propose a way to address\nthis issue by automatically segmenting an input sentence into phrases that can\nbe easily translated by the neural network translation model. Once each segment\nhas been independently translated by the neural machine translation model, the\ntranslated clauses are concatenated to form a final translation. Empirical\nresults show a significant improvement in translation quality for long\nsentences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 21:00:49 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:09:37 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Bahdanau", "Dzmitry", ""], ["van Merrienboer", "Bart", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1715", "submitter": "Fr\\'ed\\'eric Saubion", "authors": "Giacomo di Tollo and Fr\\'ed\\'eric Lardeux and Jorge Maturana and\n  Fr\\'ed\\'eric Saubion", "title": "An Experimental Study of Adaptive Control for Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balance of exploration versus exploitation (EvE) is a key issue on\nevolutionary computation. In this paper we will investigate how an adaptive\ncontroller aimed to perform Operator Selection can be used to dynamically\nmanage the EvE balance required by the search, showing that the search\nstrategies determined by this control paradigm lead to an improvement of\nsolution quality found by the evolutionary algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 10:01:41 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["di Tollo", "Giacomo", ""], ["Lardeux", "Fr\u00e9d\u00e9ric", ""], ["Maturana", "Jorge", ""], ["Saubion", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1409.2329", "submitter": "Wojciech Zaremba", "authors": "Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals", "title": "Recurrent Neural Network Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful\ntechnique for regularizing neural networks, does not work well with RNNs and\nLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show\nthat it substantially reduces overfitting on a variety of tasks. These tasks\ninclude language modeling, speech recognition, image caption generation, and\nmachine translation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 13:08:00 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 17:08:13 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 05:19:23 GMT"}, {"version": "v4", "created": "Sun, 21 Dec 2014 03:13:05 GMT"}, {"version": "v5", "created": "Thu, 19 Feb 2015 14:46:00 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1409.2390", "submitter": "Telmo Menezes", "authors": "Telmo Menezes and Camille Roth", "title": "Symbolic regression of generative network models", "comments": null, "journal-ref": "Scientific Reports volume 4, Article number: 6284 (2015)", "doi": "10.1038/srep06284", "report-no": null, "categories": "cs.NE cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Networks are a powerful abstraction with applicability to a variety of\nscientific fields. Models explaining their morphology and growth processes\npermit a wide range of phenomena to be more systematically analysed and\nunderstood. At the same time, creating such models is often challenging and\nrequires insights that may be counter-intuitive. Yet there currently exists no\ngeneral method to arrive at better models. We have developed an approach to\nautomatically detect realistic decentralised network growth models from\nempirical data, employing a machine learning technique inspired by natural\nselection and defining a unified formalism to describe such models as computer\nprograms. As the proposed method is completely general and does not assume any\npre-existing models, it can be applied \"out of the box\" to any given network.\nTo validate our approach empirically, we systematically rediscover pre-defined\ngrowth laws underlying several canonical network generation models and credible\nlaws for diverse real-world networks. We were able to find programs that are\nsimple enough to lead to an actual understanding of the mechanisms proposed,\nnamely for a simple brain and a social network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 15:07:25 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Menezes", "Telmo", ""], ["Roth", "Camille", ""]]}, {"id": "1409.2574", "submitter": "John Hershey", "authors": "John R. Hershey, Jonathan Le Roux, Felix Weninger", "title": "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures", "comments": "Added sections on reducing belief propagation to network activation\n  functions, and on conversion between conventional network parameters and BP\n  potentials for binary MRFs. Some bugs and typos were also fixed, and notation\n  made a bit clearer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based methods and deep neural networks have both been tremendously\nsuccessful paradigms in machine learning. In model-based methods, problem\ndomain knowledge can be built into the constraints of the model, typically at\nthe expense of difficulties during inference. In contrast, deterministic deep\nneural networks are constructed in such a way that inference is\nstraightforward, but their architectures are generic and it is unclear how to\nincorporate knowledge. This work aims to obtain the advantages of both\napproaches. To do so, we start with a model-based approach and an associated\ninference algorithm, and \\emph{unfold} the inference iterations as layers in a\ndeep network. Rather than optimizing the original model, we \\emph{untie} the\nmodel parameters across layers, in order to create a more powerful network. The\nresulting architecture can be trained discriminatively to perform accurate\ninference within a fixed network size. We show how this framework allows us to\ninterpret conventional networks as mean-field inference in Markov random\nfields, and to obtain new architectures by instead using belief propagation as\nthe inference algorithm. We then show its application to a non-negative matrix\nfactorization model that incorporates the problem-domain knowledge that sound\nsources are additive. Deep unfolding of this model yields a new kind of\nnon-negative deep neural network, that can be trained using a multiplicative\nbackpropagation-style update algorithm. We present speech enhancement\nexperiments showing that our approach is competitive with conventional neural\nnetworks despite using far fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 02:31:11 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 22:59:52 GMT"}, {"version": "v3", "created": "Wed, 8 Oct 2014 23:50:51 GMT"}, {"version": "v4", "created": "Thu, 20 Nov 2014 01:52:53 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Hershey", "John R.", ""], ["Roux", "Jonathan Le", ""], ["Weninger", "Felix", ""]]}, {"id": "1409.2650", "submitter": "Ihab Sbeity", "authors": "Ihab Sbeity, Mohamed Dbouk, Habib Kobeissi", "title": "Combining the analytical hierarchy process and the genetic algorithm to\n  solve the timetable problem", "comments": "International Journal of Software Engineering & Applications (IJSEA),\n  Vol.5, No.4, July 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main problems of school course timetabling are time, curriculum, and\nclassrooms. In addition there are other problems that vary from one institution\nto another. This paper is intended to solve the problem of satisfying the\nteachers preferred schedule in a way that regards the importance of the teacher\nto the supervising institute, i.e. his score according to some criteria.\nGenetic algorithm (GA) has been presented as an elegant method in solving\ntimetable problem (TTP) in order to produce solutions with no conflict. In this\npaper, we consider the analytic hierarchy process (AHP) to efficiently obtain a\nscore for each teacher, and consequently produce a GA-based TTP solution that\nsatisfies most of the teachers preferences.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 09:30:48 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Sbeity", "Ihab", ""], ["Dbouk", "Mohamed", ""], ["Kobeissi", "Habib", ""]]}, {"id": "1409.2697", "submitter": "Sanjaya Sahu Mr", "authors": "Sanjaya Kumar Sahu, T. V. Dixit and D.D. Neema", "title": "Particle Swarm Optimized Fuzzy Controller for Indirect Vector Control of\n  Multilevel Inverter Fed Induction Motor", "comments": "9 pages, published in Volume 11, issue 4, july 2014, IJCSI", "journal-ref": "Volume 11, issue 4, july 2014, IJCSI", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Particle Swarm Optimized (PSO) fuzzy controller has been proposed for\nindirect vector control of induction motor. In this proposed scheme a Neutral\nPoint Clamped (NPC) multilevel inverter is used and hysteresis current control\ntechnique has been adopted for switching the IGBTs. A Mamdani type fuzzy\ncontroller is used in place of conventional PI controller. To ensure better\nperformance of fuzzy controller all parameters such as membership functions,\nnormalizing and de-normalizing parameters are optimized using PSO. The\nperformance of proposed controller is investigated under various load and speed\nconditions. The simulation results show its stability and robustness for high\nperformance derives applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 19:49:04 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Sahu", "Sanjaya Kumar", ""], ["Dixit", "T. V.", ""], ["Neema", "D. D.", ""]]}, {"id": "1409.2710", "submitter": "Gopinath Chennupati", "authors": "Gopinath Chennupati", "title": "eAnt-Miner : An Ensemble Ant-Miner to Improve the ACO Classification", "comments": "13 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ant Colony Optimization (ACO) has been applied in supervised learning in\norder to induce classification rules as well as decision trees, named\nAnt-Miners. Although these are competitive classifiers, the stability of these\nclassifiers is an important concern that owes to their stochastic nature. In\nthis paper, to address this issue, an acclaimed machine learning technique\nnamed, ensemble of classifiers is applied, where an ACO classifier is used as a\nbase classifier to prepare the ensemble. The main trade-off is, the predictions\nin the new approach are determined by discovering a group of models as opposed\nto the single model classification. In essence, we prepare multiple models from\nthe randomly replaced samples of training data from which, a unique model is\nprepared by aggregating the models to test the unseen data points. The main\nobjective of this new approach is to increase the stability of the Ant-Miner\nresults there by improving the performance of ACO classification. We found that\nthe ensemble Ant-Miners significantly improved the stability by reducing the\nclassification error on unseen data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 12:24:22 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Chennupati", "Gopinath", ""]]}, {"id": "1409.2752", "submitter": "Alireza Makhzani", "authors": "Alireza Makhzani, Brendan Frey", "title": "Winner-Take-All Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a winner-take-all method for learning hierarchical\nsparse representations in an unsupervised fashion. We first introduce\nfully-connected winner-take-all autoencoders which use mini-batch statistics to\ndirectly enforce a lifetime sparsity in the activations of the hidden units. We\nthen propose the convolutional winner-take-all autoencoder which combines the\nbenefits of convolutional architectures and autoencoders for learning\nshift-invariant sparse representations. We describe a way to train\nconvolutional autoencoders layer by layer, where in addition to lifetime\nsparsity, a spatial sparsity within each feature map is achieved using\nwinner-take-all activation functions. We will show that winner-take-all\nautoencoders can be used to to learn deep sparse representations from the\nMNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,\nand achieve competitive classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 14:38:43 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 18:28:22 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Makhzani", "Alireza", ""], ["Frey", "Brendan", ""]]}, {"id": "1409.2944", "submitter": "Hao Wang", "authors": "Hao Wang and Naiyan Wang and Dit-Yan Yeung", "title": "Collaborative Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 03:05:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 09:23:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Naiyan", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1409.3078", "submitter": "Keivan Borna", "authors": "Keivan Borna and Vahid Haji Hashemi", "title": "An improved genetic algorithm with a local optimization strategy and an\n  extra mutation level for solving traveling salesman problem", "comments": "7 pages, 1 Figure", "journal-ref": "International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol. 4, No.4, August 2014, 47-53", "doi": "10.5121/ijcseit.2014.4405", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Traveling salesman problem (TSP) is proved to be NP-complete in most\ncases. The genetic algorithm (GA) is one of the most useful algorithms for\nsolving this problem. In this paper a conventional GA is compared with an\nimproved hybrid GA in solving TSP. The improved or hybrid GA consist of\nconventional GA and two local optimization strategies. The first strategy is\nextracting all sequential groups including four cities of samples and changing\nthe two central cities with each other. The second local optimization strategy\nis similar to an extra mutation process. In this step with a low probability a\nsample is selected. In this sample two random cities are defined and the path\nbetween these cities is reversed. The computation results show that the\nproposed method also finds better paths than the conventional GA within an\nacceptable computation time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 14:15:29 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Borna", "Keivan", ""], ["Hashemi", "Vahid Haji", ""]]}, {"id": "1409.3358", "submitter": "Lili Mou", "authors": "Lili Mou, Ge Li, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang", "title": "Building Program Vector Representations for Deep Learning", "comments": "This paper was submitted to ICSE'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has made significant breakthroughs in various fields of\nartificial intelligence. Advantages of deep learning include the ability to\ncapture highly complicated features, weak involvement of human engineering,\netc. However, it is still virtually impossible to use deep learning to analyze\nprograms since deep architectures cannot be trained effectively with pure back\npropagation. In this pioneering paper, we propose the \"coding criterion\" to\nbuild program vector representations, which are the premise of deep learning\nfor program analysis. Our representation learning approach directly makes deep\nlearning a reality in this new field. We evaluate the learned vector\nrepresentations both qualitatively and quantitatively. We conclude, based on\nthe experiments, the coding criterion is successful in building program\nrepresentations. To evaluate whether deep learning is beneficial for program\nanalysis, we feed the representations to deep neural networks, and achieve\nhigher accuracy in the program classification task than \"shallow\" methods, such\nas logistic regression and the support vector machine. This result confirms the\nfeasibility of deep learning to analyze programs. It also gives primary\nevidence of its success in this new field. We believe deep learning will become\nan outstanding technique for program analysis in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 08:44:28 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Mou", "Lili", ""], ["Li", "Ge", ""], ["Liu", "Yuxuan", ""], ["Peng", "Hao", ""], ["Jin", "Zhi", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""]]}, {"id": "1409.3924", "submitter": "Yuguang Wang", "authors": "Yuguang Wang and Feilong Cao and Yubo Yuan", "title": "A study on effectiveness of extreme learning machine", "comments": null, "journal-ref": "Neurocomputing, 74(16):2483--2490, 2011", "doi": "10.1016/j.neucom.2010.11.030", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM), proposed by Huang et al., has been shown a\npromising learning algorithm for single-hidden layer feedforward neural\nnetworks (SLFNs). Nevertheless, because of the random choice of input weights\nand biases, the ELM algorithm sometimes makes the hidden layer output matrix H\nof SLFN not full column rank, which lowers the effectiveness of ELM. This paper\ndiscusses the effectiveness of ELM and proposes an improved algorithm called\nEELM that makes a proper selection of the input weights and bias before\ncalculating the output weights, which ensures the full column rank of H in\ntheory. This improves to some extend the learning rate (testing accuracy,\nprediction accuracy, learning time) and the robustness property of the\nnetworks. The experimental results based on both the benchmark function\napproximation and real-world problems including classification and regression\napplications show the good performances of EELM.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 07:47:37 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Wang", "Yuguang", ""], ["Cao", "Feilong", ""], ["Yuan", "Yubo", ""]]}, {"id": "1409.3970", "submitter": "Yin Zheng", "authors": "Yin Zheng, Yu-Jin Zhang, Hugo Larochelle", "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "comments": "24 pages, 10 figures. A version has been accepted by TPAMI on Aug\n  4th, 2015. Add footnote about how to train the model in practice in Section\n  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2476802", "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to deal with multimodal data, such as in image annotation\ntasks. Another popular approach to model the multimodal data is through deep\nneural networks, such as the deep Boltzmann machine (DBM). Recently, a new type\nof topic model called the Document Neural Autoregressive Distribution Estimator\n(DocNADE) was proposed and demonstrated state-of-the-art performance for text\ndocument modeling. In this work, we show how to successfully apply and extend\nthis model to multimodal data, such as simultaneous image classification and\nannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,\nthat increases the discriminative power of the learned hidden topic features\nand show how to employ it to learn a joint representation from image visual\nwords, annotation words and class label information. We test our model on the\nLabelMe and UIUC-Sports data sets and show that it compares favorably to other\ntopic models. Second, we propose a deep extension of our model and provide an\nefficient way of training the deep model. Experimental results show that our\ndeep model outperforms its shallow version and reaches state-of-the-art\nperformance on the Multimedia Information Retrieval (MIR) Flickr data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 17:17:05 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 02:44:29 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 16:12:31 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zheng", "Yin", ""], ["Zhang", "Yu-Jin", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1409.4244", "submitter": "Ana Carolina Olivera", "authors": "Enrique Gabriel Baquela and Ana Carolina Olivera", "title": "An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem", "comments": "Submitted to ALIO/EURO 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lane reversal has proven to be a useful method to mitigate traffic\ncongestion during rush hour or in case of specific events that affect high\ntraffic volumes. In this work we propose a methodology that is placed within\noptimization via Simulation, by means of which a multi-objective genetic\nalgorithm and simulations of traffic are used to determine the configuration of\nideal lane reversal.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 12:07:47 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Baquela", "Enrique Gabriel", ""], ["Olivera", "Ana Carolina", ""]]}, {"id": "1409.4326", "submitter": "Jure \\v{Z}bontar", "authors": "Jure \\v{Z}bontar and Yann LeCun", "title": "Computing the Stereo Matching Cost with a Convolutional Neural Network", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR), June\n  2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298767", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting depth information from a rectified image\npair. We train a convolutional neural network to predict how well two image\npatches match and use it to compute the stereo matching cost. The cost is\nrefined by cross-based cost aggregation and semiglobal matching, followed by a\nleft-right consistency check to eliminate errors in the occluded regions. Our\nstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and\nis currently (August 2014) the top performing method on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:54:42 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 15:08:48 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["\u017dbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "1409.4565", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo and Emiliano Tramontana", "title": "Improving files availability for BitTorrent using a diffusion model", "comments": null, "journal-ref": "IEEE 23rd International WETICE Conference, pp. 191-196, 2014", "doi": "10.1109/WETICE.2014.65", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BitTorrent mechanism effectively spreads file fragments by copying the\nrarest fragments first. We propose to apply a mathematical model for the\ndiffusion of fragments on a P2P in order to take into account both the effects\nof peer distances and the changing availability of peers while time goes on.\nMoreover, we manage to provide a forecast on the availability of a torrent\nthanks to a neural network that models the behaviour of peers on the P2P\nsystem. The combination of the mathematical model and the neural network\nprovides a solution for choosing file fragments that need to be copied first,\nin order to ensure their continuous availability, counteracting possible\ndisconnections by some peers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 10:14:52 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1409.4727", "submitter": "Hindayati Mustafidah", "authors": "Hindayati Mustafidah, Sri Hartati, Retantyo Wardoyo, Agus Harjoko", "title": "Selection of Most Appropriate Backpropagation Training Algorithm in Data\n  Pattern Recognition", "comments": "4 pages, 2 figures, 6 tables, Published with International Journal of\n  Computer Trends and Technology (IJCTT) 14(2):92-95, Aug 2014", "journal-ref": null, "doi": "10.14445/22312803/IJCTT-V14P120", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several training algorithms for backpropagation method in neural\nnetwork. Not all of these algorithms have the same accuracy level demonstrated\nthrough the percentage level of suitability in recognizing patterns in the\ndata. In this research tested 12 training algorithms specifically in recognize\ndata patterns of test validity. The basic network parameters used are the\nmaximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05.\nOf the twelve training algorithms each performed 20 times looping. The test\nresults obtained that the percentage rate of the great match is trainlm\nalgorithm with alpha 5% have adequate levels of suitability of 87.5% at the\nlevel of significance of 0.000. This means the most appropriate training\nalgorithm in recognizing the the data pattern of test validity is the trainlm\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 09:03:38 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Mustafidah", "Hindayati", ""], ["Hartati", "Sri", ""], ["Wardoyo", "Retantyo", ""], ["Harjoko", "Agus", ""]]}, {"id": "1409.5185", "submitter": "Zhuowen Tu", "authors": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen\n  Tu", "title": "Deeply-Supervised Nets", "comments": "Patent disclosure, UCSD Docket No. SD2014-313, filed on May 22, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Our proposed deeply-supervised nets (DSN) method simultaneously minimizes\nclassification error while making the learning process of hidden layers direct\nand transparent. We make an attempt to boost the classification performance by\nstudying a new formulation in deep networks. Three aspects in convolutional\nneural networks (CNN) style architectures are being looked at: (1) transparency\nof the intermediate layers to the overall classification; (2)\ndiscriminativeness and robustness of learned features, especially in the early\nlayers; (3) effectiveness in training due to the presence of the exploding and\nvanishing gradients. We introduce \"companion objective\" to the individual\nhidden layers, in addition to the overall objective at the output layer (a\ndifferent strategy to layer-wise pre-training). We extend techniques from\nstochastic gradient methods to analyze our algorithm. The advantage of our\nmethod is evident and our experimental result on benchmark datasets shows\nsignificant performance gain over existing methods (e.g. all state-of-the-art\nresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 04:08:25 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 05:03:06 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Xie", "Saining", ""], ["Gallagher", "Patrick", ""], ["Zhang", "Zhengyou", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1409.5326", "submitter": "Marcus Kaiser", "authors": "Richard J. Tomsett, Matt Ainsworth, Alexander Thiele, Mehdi Sanayei,\n  Xing Chen, Alwin Gieselmann, Miles A. Whittington, Mark O. Cunningham and\n  Marcus Kaiser", "title": "Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX):\n  Comparing multi-electrode recordings from simulated and biological mammalian\n  cortical tissue", "comments": "appears in Brain Struct Funct 2014", "journal-ref": null, "doi": "10.1007/s00429-014-0793-x", "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local field potentials (LFPs) sampled with extracellular electrodes are\nfrequently used as a measure of population neuronal activity. However, relating\nsuch measurements to underlying neuronal behaviour and connectivity is\nnon-trivial. To help study this link, we developed the Virtual Electrode\nRecording Tool for EXtracellular potentials (VERTEX). We first identified a\nreduced neuron model that retained the spatial and frequency filtering\ncharacteristics of extracellular potentials from neocortical neurons. We then\ndeveloped VERTEX as an easy-to-use Matlab tool for simulating LFPs from large\npopulations (>100 000 neurons). A VERTEX-based simulation successfully\nreproduced features of the LFPs from an in vitro multi-electrode array\nrecording of macaque neocortical tissue. Our model, with virtual electrodes\nplaced anywhere in 3D, allows direct comparisons with the in vitro recording\nsetup. We envisage that VERTEX will stimulate experimentalists, clinicians, and\ncomputational neuroscientists to use models to understand the mechanisms\nunderlying measured brain dynamics in health and disease.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 15:04:53 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Tomsett", "Richard J.", ""], ["Ainsworth", "Matt", ""], ["Thiele", "Alexander", ""], ["Sanayei", "Mehdi", ""], ["Chen", "Xing", ""], ["Gieselmann", "Alwin", ""], ["Whittington", "Miles A.", ""], ["Cunningham", "Mark O.", ""], ["Kaiser", "Marcus", ""]]}, {"id": "1409.5718", "submitter": "Lili Mou", "authors": "Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin", "title": "Convolutional Neural Networks over Tree Structures for Programming\n  Language Processing", "comments": "Accepted at AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming language processing (similar to natural language processing) is a\nhot research topic in the field of software engineering; it has also aroused\ngrowing interest in the artificial intelligence community. However, different\nfrom a natural language sentence, a program contains rich, explicit, and\ncomplicated structural information. Hence, traditional NLP models may be\ninappropriate for programs. In this paper, we propose a novel tree-based\nconvolutional neural network (TBCNN) for programming language processing, in\nwhich a convolution kernel is designed over programs' abstract syntax trees to\ncapture structural information. TBCNN is a generic architecture for programming\nlanguage processing; our experiments show its effectiveness in two different\nprogram analysis tasks: classifying programs according to functionality, and\ndetecting code snippets of certain patterns. TBCNN outperforms baseline\nmethods, including several neural models for NLP.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 06:50:52 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:31:51 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Mou", "Lili", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Wang", "Tao", ""], ["Jin", "Zhi", ""]]}, {"id": "1409.6023", "submitter": "Jerry  Van Aken", "authors": "Jerry R. Van Aken", "title": "A High-Level Model of Neocortical Feedback Based on an Event Window\n  Segmentation Algorithm", "comments": "44 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author previously presented an event window segmentation (EWS) algorithm\n[5] that uses purely statistical methods to learn to recognize recurring\npatterns in an input stream of events. In the following discussion, the EWS\nalgorithm is first extended to make predictions about future events. Next, this\nextended algorithm is used to construct a high-level, simplified model of a\nneocortical hierarchy. An event stream enters at the bottom of the hierarchy,\nand drives processing activity upward in the hierarchy. Successively higher\nregions in the hierarchy learn to recognize successively deeper levels of\npatterns in these events as they propagate from the bottom of the hierarchy.\nThe lower levels in the hierarchy use the predictions from the levels above to\nstrengthen their own predictions. A C++ source code listing of the model\nimplementation and test program is included as an appendix.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 18:14:48 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Van Aken", "Jerry R.", ""]]}, {"id": "1409.6041", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang", "title": "Domain Adaptive Neural Networks for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 20:42:00 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1409.6046", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Approximation errors of online sparsification criteria", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2442960", "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning frameworks, such as resource-allocating networks,\nkernel-based methods, Gaussian processes, and radial-basis-function networks,\nrequire a sparsification scheme in order to address the online learning\nparadigm. For this purpose, several online sparsification criteria have been\nproposed to restrict the model definition on a subset of samples. The most\nknown criterion is the (linear) approximation criterion, which discards any\nsample that can be well represented by the already contributing samples, an\noperation with excessive computational complexity. Several computationally\nefficient sparsification criteria have been introduced in the literature, such\nas the distance, the coherence and the Babel criteria. In this paper, we\nprovide a framework that connects these sparsification criteria to the issue of\napproximating samples, by deriving theoretical bounds on the approximation\nerrors. Moreover, we investigate the error of approximating any feature, by\nproposing upper-bounds on the approximation error for each of the\naforementioned sparsification criteria. Two classes of features are described\nin detail, the empirical mean and the principal axes in the kernel principal\ncomponent analysis.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:53:08 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6070", "submitter": "Benjamin Graham", "authors": "Benjamin Graham", "title": "Spatially-sparse convolutional neural networks", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) perform well on problems such as\nhandwriting recognition and image classification. However, the performance of\nthe networks is often limited by budget and time constraints, particularly when\ntrying to train deep networks.\n  Motivated by the problem of online handwriting recognition, we developed a\nCNN for processing spatially-sparse inputs; a character drawn with a one-pixel\nwide pen on a high resolution grid looks like a sparse matrix. Taking advantage\nof the sparsity allowed us more efficiently to train and test large, deep CNNs.\nOn the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test\nerror of 3.82%.\n  Although pictures are not sparse, they can be thought of as sparse by adding\npadding. Applying a deep convolutional network using sparsity has resulted in a\nsubstantial reduction in test error on the CIFAR small picture datasets: 6.28%\non CIFAR-10 and 24.30% for CIFAR-100.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 02:39:27 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Graham", "Benjamin", ""]]}, {"id": "1409.7478", "submitter": "Sebastien Verel", "authors": "Hernan Aguirre, Arnaud Liefooghe (INRIA Lille - Nord Europe, LIFL),\n  S\\'ebastien Verel (LISIC), Kiyoshi Tanaka", "title": "An Analysis on Selection for High-Resolution Approximations in\n  Many-Objective Optimization", "comments": "apperas in Parallel Problem Solving from Nature - PPSN XIII,\n  Ljubljana : Slovenia (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the behavior of three elitist multi- and many-objective\nevolutionary algorithms generating a high-resolution approximation of the\nPareto optimal set. Several search-assessment indicators are defined to trace\nthe dynamics of survival selection and measure the ability to simultaneously\nkeep optimal solutions and discover new ones under different population sizes,\nset as a fraction of the size of the Pareto optimal set.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:32:52 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Aguirre", "Hernan", "", "INRIA Lille - Nord Europe, LIFL"], ["Liefooghe", "Arnaud", "", "INRIA Lille - Nord Europe, LIFL"], ["Verel", "S\u00e9bastien", "", "LISIC"], ["Tanaka", "Kiyoshi", ""]]}, {"id": "1409.7495", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Victor Lempitsky", "title": "Unsupervised Domain Adaptation by Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-performing deep architectures are trained on massive amounts of labeled\ndata. In the absence of labeled data for a certain task, domain adaptation\noften provides an attractive option given that labeled data of similar nature\nbut from a different domain (e.g. synthetic images) are available. Here, we\npropose a new approach to domain adaptation in deep architectures that can be\ntrained on large amount of labeled data from the source domain and large amount\nof unlabeled data from the target domain (no labeled target-domain data is\nnecessary).\n  As the training progresses, the approach promotes the emergence of \"deep\"\nfeatures that are (i) discriminative for the main learning task on the source\ndomain and (ii) invariant with respect to the shift between the domains. We\nshow that this adaptation behaviour can be achieved in almost any feed-forward\nmodel by augmenting it with few standard layers and a simple new gradient\nreversal layer. The resulting augmented architecture can be trained using\nstandard backpropagation.\n  Overall, the approach can be implemented with little effort using any of the\ndeep-learning packages. The method performs very well in a series of image\nclassification experiments, achieving adaptation effect in the presence of big\ndomain shifts and outperforming previous state-of-the-art on Office datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 08:22:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 14:54:37 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1409.7758", "submitter": "Zhe Yao", "authors": "Zhe Yao and Vincent Gripon and Michael Rabbat", "title": "Combating Corrupt Messages in Sparse Clustered Associative Memories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze and extend the neural network based associative\nmemory proposed by Gripon and Berrou. This associative memory resembles the\ncelebrated Willshaw model with an added partite cluster structure. In the\nliterature, two retrieving schemes have been proposed for the network dynamics,\nnamely sum-of-sum and sum-of-max. They both offer considerably better\nperformance than Willshaw and Hopfield networks, when comparable retrieval\nscenarios are considered. Former discussions and experiments concentrate on the\nerasure scenario, where a partial message is used as a probe to the network, in\nthe hope of retrieving the full message. In this regard, sum-of-max outperforms\nsum-of-sum in terms of retrieval rate by a large margin. However, we observe\nthat when noise and errors are present and the network is queried by a corrupt\nprobe, sum-of-max faces a severe limitation as its stringent activation rule\nprevents a neuron from reviving back into play once deactivated. In this\nmanuscript, we categorize and analyze different error scenarios so that both\nthe erasure and the corrupt scenarios can be treated consistently. We make an\namendment to the network structure to improve the retrieval rate, at the cost\nof an extra scalar per neuron. Afterwards, five different approaches are\nproposed to deal with corrupt probes. As a result, we extend the network\ncapability, and also increase the robustness of the retrieving procedure. We\nthen experimentally compare all these proposals and discuss pros and cons of\neach approach under different types of errors. Simulation results show that if\ncarefully designed, the network is able to preserve both a high retrieval rate\nand a low running time simultaneously, even when queried by a corrupt probe.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 03:34:02 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Yao", "Zhe", ""], ["Gripon", "Vincent", ""], ["Rabbat", "Michael", ""]]}, {"id": "1409.7842", "submitter": "Indranil Pan", "authors": "Indranil Pan and Saptarshi Das", "title": "When Darwin meets Lorenz: Evolving new chaotic attractors through\n  genetic programming", "comments": "81 pages, 128 figures", "journal-ref": "Chaos, Solitons & Fractals, Volume 76, Pages 141-155, 2015", "doi": "10.1016/j.chaos.2015.03.017", "report-no": null, "categories": "nlin.CD cs.NE math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel methodology for automatically finding new\nchaotic attractors through a computational intelligence technique known as\nmulti-gene genetic programming (MGGP). We apply this technique to the case of\nthe Lorenz attractor and evolve several new chaotic attractors based on the\nbasic Lorenz template. The MGGP algorithm automatically finds new nonlinear\nexpressions for the different state variables starting from the original Lorenz\nsystem. The Lyapunov exponents of each of the attractors are calculated\nnumerically based on the time series of the state variables using time delay\nembedding techniques. The MGGP algorithm tries to search the functional space\nof the attractors by aiming to maximise the largest Lyapunov exponent (LLE) of\nthe evolved attractors. To demonstrate the potential of the proposed\nmethodology, we report over one hundred new chaotic attractor structures along\nwith their parameters, which are evolved from just the Lorenz system alone.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 21:09:33 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 22:13:02 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 18:18:42 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Pan", "Indranil", ""], ["Das", "Saptarshi", ""]]}, {"id": "1409.7963", "submitter": "Arjun Jain", "authors": "Arjun Jain, Jonathan Tompson, Yann LeCun and Christoph Bregler", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel and efficient method for articulated human\npose estimation in videos using a convolutional network architecture, which\nincorporates both color and motion features. We propose a new human body pose\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\nfeatures. We apply our architecture to this dataset and report significantly\nbetter performance than current state-of-the-art pose detection systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 21:32:15 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Jain", "Arjun", ""], ["Tompson", "Jonathan", ""], ["LeCun", "Yann", ""], ["Bregler", "Christoph", ""]]}, {"id": "1409.8191", "submitter": "Djallel Bouneffouf", "authors": "Robin Allesiardo, Raphael Feraud and Djallel Bouneffouf", "title": "A Neural Networks Committee for the Contextual Bandit Problem", "comments": "21st International Conference on Neural Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new contextual bandit algorithm, NeuralBandit, which\ndoes not need hypothesis on stationarity of contexts and rewards. Several\nneural networks are trained to modelize the value of rewards knowing the\ncontext. Two variants, based on multi-experts approach, are proposed to choose\nonline the parameters of multi-layer perceptrons. The proposed algorithms are\nsuccessfully tested on a large dataset with and without stationarity of\nrewards.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:08:21 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Allesiardo", "Robin", ""], ["Feraud", "Raphael", ""], ["Bouneffouf", "Djallel", ""]]}, {"id": "1409.8484", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "An agent-driven semantical identifier using radial basis neural networks\n  and reinforcement learning", "comments": "Published on: Proceedings of the XV Workshop \"Dagli Oggetti agli\n  Agenti\" (WOA 2014), Catania, Italy, Sepember. 25-26, 2014", "journal-ref": "Proceedings of the XV Workshop \"Dagli Oggetti agli Agenti\" (WOA\n  2014), on CEUR-WS, volume 1260, ISSN: 1613-073, Catania, Italy, Sepember.\n  25-26, 2014. http://ceur-ws.org/Vol-1260/", "doi": "10.13140/2.1.1446.7843", "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the huge availability of documents in digital form, and the deception\npossibility raise bound to the essence of digital documents and the way they\nare spread, the authorship attribution problem has constantly increased its\nrelevance. Nowadays, authorship attribution,for both information retrieval and\nanalysis, has gained great importance in the context of security, trust and\ncopyright preservation. This work proposes an innovative multi-agent driven\nmachine learning technique that has been developed for authorship attribution.\nBy means of a preprocessing for word-grouping and time-period related analysis\nof the common lexicon, we determine a bias reference level for the recurrence\nfrequency of the words within analysed texts, and then train a Radial Basis\nNeural Networks (RBPNN)-based classifier to identify the correct author. The\nmain advantage of the proposed approach lies in the generality of the semantic\nanalysis, which can be applied to different contexts and lexical domains,\nwithout requiring any modification. Moreover, the proposed system is able to\nincorporate an external input, meant to tune the classifier, and then\nself-adjust by means of continuous learning reinforcement.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:10:23 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1409.8558", "submitter": "Prasanna Kumar Muthukumar", "authors": "Prasanna Kumar Muthukumar and Alan W. Black", "title": "A Deep Learning Approach to Data-driven Parameterizations for\n  Statistical Parametric Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral\ncoefficients as the vocal tract parameterization of the speech signal. Mel\nCepstral coefficients were never intended to work in a parametric speech\nsynthesis framework, but as yet, there has been little success in creating a\nbetter parameterization that is more suited to synthesis. In this paper, we use\ndeep learning algorithms to investigate a data-driven parameterization\ntechnique that is designed for the specific requirements of synthesis. We\ncreate an invertible, low-dimensional, noise-robust encoding of the Mel Log\nSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is\nthen unwrapped and used as the initialization for a Multi-Layer Perceptron\n(MLP). The MLP is fine-tuned by training it to reconstruct the input at the\noutput layer. This MLP is then split down the middle to form encoding and\ndecoding networks. These networks produce a parameterization of the Mel Log\nSpectrum that is intended to better fulfill the requirements of synthesis.\nResults are reported for experiments conducted using this resulting\nparameterization with the ClusterGen speech synthesizer.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:20:29 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Muthukumar", "Prasanna Kumar", ""], ["Black", "Alan W.", ""]]}]