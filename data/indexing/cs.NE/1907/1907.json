[{"id": "1907.00250", "submitter": "Xiaobiao Huang", "authors": "Xiaobiao Huang, Minghao Song, Zhe Zhang", "title": "Multi-objective multi-generation Gaussian process optimizer for design\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": "SLAC-PUB-17451", "categories": "cs.NE physics.acc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-objective evolutionary optimization algorithm that uses\nGaussian process (GP) regression-based models to select trial solutions in a\nmulti-generation iterative procedure. In each generation, a surrogate model is\nconstructed for each objective function with the sample data. The models are\nused to evaluate solutions and to select the ones with a high potential before\nthey are evaluated on the actual system. Since the trial solutions selected by\nthe GP models tend to have better performance than other methods that only rely\non random operations, the new algorithm has much higher efficiency in exploring\nthe parameter space. Simulations with multiple test cases show that the new\nalgorithm has a substantially higher convergence speed and stability than\nNSGA-II, MOPSO, and some other more recent algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 18:22:02 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 17:13:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Huang", "Xiaobiao", ""], ["Song", "Minghao", ""], ["Zhang", "Zhe", ""]]}, {"id": "1907.00262", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle and David Bau", "title": "Dissecting Pruned Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is a standard technique for removing unnecessary structure from a\nneural network to reduce its storage footprint, computational demands, or\nenergy consumption. Pruning can reduce the parameter-counts of many\nstate-of-the-art neural networks by an order of magnitude without compromising\naccuracy, meaning these networks contain a vast amount of unnecessary\nstructure. In this paper, we study the relationship between pruning and\ninterpretability. Namely, we consider the effect of removing unnecessary\nstructure on the number of hidden units that learn disentangled representations\nof human-recognizable concepts as identified by network dissection. We aim to\nevaluate how the interpretability of pruned neural networks changes as they are\ncompressed. We find that pruning has no detrimental effect on this measure of\ninterpretability until so few parameters remain that accuracy beings to drop.\nResnet-50 models trained on ImageNet maintain the same number of interpretable\nconcepts and units until more than 90% of parameters have been pruned.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:27:57 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Frankle", "Jonathan", ""], ["Bau", "David", ""]]}, {"id": "1907.00263", "submitter": "Emily Toomey", "authors": "Emily Toomey, Ken Segall, and Karl K. Berggren", "title": "A Power Efficient Artificial Neuron Using Superconducting Nanowires", "comments": "12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.supr-con cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising societal demand for more information-processing capacity with\nlower power consumption, alternative architectures inspired by the parallelism\nand robustness of the human brain have recently emerged as possible solutions.\nIn particular, spiking neural networks (SNNs) offer a bio-realistic approach,\nrelying on pulses analogous to action potentials as units of information. While\nsoftware encoded networks provide flexibility and precision, they are often\ncomputationally expensive. As a result, hardware SNNs based on the spiking\ndynamics of a device or circuit represent an increasingly appealing direction.\nHere, we propose to use superconducting nanowires as a platform for the\ndevelopment of an artificial neuron. Building on an architecture first proposed\nfor Josephson junctions, we rely on the intrinsic nonlinearity of two coupled\nnanowires to generate spiking behavior, and use electrothermal circuit\nsimulations to demonstrate that the nanowire neuron reproduces multiple\ncharacteristics of biological neurons. Furthermore, by harnessing the\nnonlinearity of the superconducting nanowire's inductance, we develop a design\nfor a variable inductive synapse capable of both excitatory and inhibitory\ncontrol. We demonstrate that this synapse design supports direct fanout, a\nfeature that has been difficult to achieve in other superconducting\narchitectures, and that the nanowire neuron's nominal energy performance is\ncompetitive with that of current technologies.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:28:25 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Toomey", "Emily", ""], ["Segall", "Ken", ""], ["Berggren", "Karl K.", ""]]}, {"id": "1907.00426", "submitter": "Michael Hanrath", "authors": "Nils Herrmann, Michael Hanrath", "title": "Automatic Routing of Goldstone Diagrams using Genetic Algorithms", "comments": "15 pages, 4 tables, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for an automatic transformation (=routing)\nof time ordered topologies of Goldstone diagrams (i.e. Wick contractions) into\ngraphical representations of these topologies. Since there is no hard criterion\nfor an optimal routing, the proposed algorithm minimizes an empirically chosen\ncost function over a set of parameters. Some of the latter are naturally of\ndiscrete type (e.g. interchange of particle/hole lines due to antisymmetry)\nwhile others (e.g. x,y-position of nodes) are naturally continuous. In order to\narrive at a manageable optimization problem the position space is artificially\ndiscretized. In terms of the (i) cost function, (ii) the discrete vertex\nplacement, (iii) the interchange of particle/hole lines the routing problem is\nnow well defined and fully discrete. However, it shows an exponential\ncomplexity with the number of vertices suggesting to apply a genetic algorithm\nfor its solution. The presented algorithm is capable of routing non trivial\n(several loops and crossings) Goldstone diagrams. The resulting diagrams are\nqualitatively fully equivalent to manually routed ones. The proposed algorithm\nis successfully applied to several Coupled Cluster approaches and a\nperturbative (fixpoint iterative) CCSD expansion with repeated diagram\nsubstitution.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 18:11:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Herrmann", "Nils", ""], ["Hanrath", "Michael", ""]]}, {"id": "1907.00625", "submitter": "Debanjan Bhowmik", "authors": "Nilabjo Dey, Janak Sharda, Utkarsh Saxena, Divya Kaushik, Utkarsh\n  Singh and Debanjan Bhowmik", "title": "On-chip learning in a conventional silicon MOSFET based Analog Hardware\n  Neural Network", "comments": "18 pages, 10 figures, 1 table (shorter version submitted to\n  conference for review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-chip learning in a crossbar array based analog hardware Neural Network\n(NN) has been shown to have major advantages in terms of speed and energy\ncompared to training NN on a traditional computer. However analog hardware NN\nproposals and implementations thus far have mostly involved Non Volatile Memory\n(NVM) devices like Resistive Random Access Memory (RRAM), Phase Change Memory\n(PCM), spintronic devices or floating gate transistors as synapses. Fabricating\nsystems based on RRAM, PCM or spintronic devices need in-house laboratory\nfacilities and cannot be done through merchant foundries, unlike conventional\nsilicon based CMOS chips. Floating gate transistors need large voltage pulses\nfor weight update, making on-chip learning in such systems energy inefficient.\nThis paper proposes and implements through SPICE simulations on-chip learning\nin analog hardware NN using only conventional silicon based MOSFETs (without\nany floating gate) as synapses since they are easy to fabricate. We first model\nthe synaptic characteristic of our single transistor synapse using SPICE\ncircuit simulator and benchmark it against experimentally obtained\ncurrent-voltage characteristics of a transistor. Next we design a Fully\nConnected Neural Network (FCNN) crossbar array using such transistor synapses.\nWe also design analog peripheral circuits for neuron and synaptic weight update\ncalculation, needed for on-chip learning, again using conventional transistors.\nSimulating the entire system on SPICE simulator, we obtain high training and\ntest accuracy on the standard Fisher's Iris dataset, widely used in machine\nlearning. We also compare the speed and energy performance of our transistor\nbased implementation of analog hardware NN with some previous implementations\nof NN with NVM devices and show comparable performance with respect to on-chip\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 09:39:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Dey", "Nilabjo", ""], ["Sharda", "Janak", ""], ["Saxena", "Utkarsh", ""], ["Kaushik", "Divya", ""], ["Singh", "Utkarsh", ""], ["Bhowmik", "Debanjan", ""]]}, {"id": "1907.00670", "submitter": "Bruno Magalhaes", "authors": "Bruno Magalh\\~aes, Michael Hines, Thomas Sterling, Felix Schuermann", "title": "Fully-Asynchronous Fully-Implicit Variable-Order Variable-Timestep\n  Simulation of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art simulations of detailed neural models follow the Bulk\nSynchronous Parallel execution model. Execution is divided in equidistant\ncommunication intervals, equivalent to the shortest synaptic delay in the\nnetwork. Neurons stepping is performed independently, with collective\ncommunication guiding synchronization and exchange of synaptic events.\n  The interpolation step size is fixed and chosen based on some prior knowledge\nof the fastest possible dynamics in the system. However, simulations driven by\nstiff dynamics or a wide range of time scales - such as multiscale simulations\nof neural networks - struggle with fixed step interpolation methods, yielding\nexcessive computation of intervals of quasi-constant activity, inaccurate\ninterpolation of periods of high volatility solution, and being incapable of\nhandling unknown or distinct time constants. A common alternative is the usage\nof adaptive stepping methods, however they have been deemed inefficient in\nparallel executions due to computational load imbalance at the synchronization\nbarriers that characterize the BSP execution model.\n  We introduce a distributed fully-asynchronous execution model that removes\nglobal synchronization, allowing for longer variable timestep interpolations.\nAsynchronicity is provided by active point-to-point communication notifying\nneurons' time advancement to synaptic connectivities. Time stepping is driven\nby scheduled neuron advancements based on synaptic delays across neurons,\nyielding an \"exhaustive yet not speculative\" adaptive-step execution. Execution\nbenchmarks on 64 Cray XE6 compute nodes demonstrate a reduced number of\ninterpolation steps, higher numerical accuracy and lower time to solution,\ncompared to state-of-the-art methods. Efficiency is shown to be\nactivity-dependent, with scaling of the algorithm demonstrated on a simulation\nof a laboratory experiment.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:38:03 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 00:16:26 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 23:16:17 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Magalh\u00e3es", "Bruno", ""], ["Hines", "Michael", ""], ["Sterling", "Thomas", ""], ["Schuermann", "Felix", ""]]}, {"id": "1907.00689", "submitter": "Soaad Hossain Mr", "authors": "Soaad Hossain", "title": "Application and Computation of Probabilistic Neural Plasticity", "comments": "10 pages, submitted to Frontiers in Human Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of neural plasticity has proved that throughout the life of a\nhuman being, the brain reorganizes itself through forming new neural\nconnections. The formation of new neural connections are achieved through the\nbrain's effort to adapt to new environments or to changes in the existing\nenvironment. Despite the realization of neural plasticity, there is a lack of\nunderstanding the probability of neural plasticity occurring given some event.\nUsing ordinary differential equations, neural firing equations and spike-train\nstatistics, we show how an additive short-term memory (STM) equation can be\nformulated to approach the computation of neural plasticity. We then show how\nthe additive STM equation can be used for probabilistic inference in computable\nneural plasticity, and the computation of probabilistic neural plasticity. We\nwill also provide a brief introduction to the theory of probabilistic neural\nplasticity and conclude with showing how it can be applied to multiple\ndisciplines such as behavioural science, machine learning, artificial\nintelligence and psychiatry.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 07:03:56 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 01:23:53 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hossain", "Soaad", ""]]}, {"id": "1907.00707", "submitter": "James King", "authors": "James King, Masoud Mohseni, William Bernoudy, Alexandre Fr\\'echette,\n  Hossein Sadeghi, Sergei V. Isakov, Hartmut Neven, Mohammad H. Amin", "title": "Quantum-Assisted Genetic Algorithm", "comments": "13 pages, 5 figures, presented at AQC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms, which mimic evolutionary processes to solve optimization\nproblems, can be enhanced by using powerful semi-local search algorithms as\nmutation operators. Here, we introduce reverse quantum annealing, a class of\nquantum evolutions that can be used for performing families of quasi-local or\nquasi-nonlocal search starting from a classical state, as novel sources of\nmutations. Reverse annealing enables the development of genetic algorithms that\nuse quantum fluctuation for mutations and classical mechanisms for the\ncrossovers -- we refer to these as Quantum-Assisted Genetic Algorithms (QAGAs).\nWe describe a QAGA and present experimental results using a D-Wave 2000Q\nquantum annealing processor. On a set of spin-glass inputs, standard (forward)\nquantum annealing finds good solutions very quickly but struggles to find\nglobal optima. In contrast, our QAGA proves effective at finding global optima\nfor these inputs. This successful interplay of non-local classical and quantum\nfluctuations could provide a promising step toward practical applications of\nNoisy Intermediate-Scale Quantum (NISQ) devices for heuristic discrete\noptimization.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 06:08:07 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["King", "James", ""], ["Mohseni", "Masoud", ""], ["Bernoudy", "William", ""], ["Fr\u00e9chette", "Alexandre", ""], ["Sadeghi", "Hossein", ""], ["Isakov", "Sergei V.", ""], ["Neven", "Hartmut", ""], ["Amin", "Mohammad H.", ""]]}, {"id": "1907.00820", "submitter": "Kexin Wang", "authors": "Kexin Wang, Yu Zhou, Shaonan Wang, Jiajun Zhang and Chengqing Zong", "title": "Understanding Memory Modules on Learning Simple Algorithms", "comments": "Accepted at the XAI Workshop in IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that memory modules are crucial for the generalization\nability of neural networks on learning simple algorithms. However, we still\nhave little understanding of the working mechanism of memory modules. To\nalleviate this problem, we apply a two-step analysis pipeline consisting of\nfirst inferring hypothesis about what strategy the model has learned according\nto visualization and then verify it by a novel proposed qualitative analysis\nmethod based on dimension reduction. Using this method, we have analyzed two\npopular memory-augmented neural networks, neural Turing machine and\nstack-augmented neural network on two simple algorithm tasks including\nreversing a random sequence and evaluation of arithmetic expressions. Results\nhave shown that on the former task both models can learn to generalize and on\nthe latter task only the stack-augmented model can do so. We show that\ndifferent strategies are learned by the models, in which specific categories of\ninput are monitored and different policies are made based on that to change the\nmemory.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:27:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Kexin", ""], ["Zhou", "Yu", ""], ["Wang", "Shaonan", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1907.01003", "submitter": "Wieland Brendel", "authors": "Wieland Brendel, Jonas Rauber, Matthias K\\\"ummerer, Ivan\n  Ustyuzhaninov, Matthias Bethge", "title": "Accurate, reliable and fast robustness evaluation", "comments": "Accepted at the 2019 Conference on Neural Information Processing\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the past five years, the susceptibility of neural networks to\nminimal adversarial perturbations has moved from a peculiar phenomenon to a\ncore issue in Deep Learning. Despite much attention, however, progress towards\nmore robust models is significantly impaired by the difficulty of evaluating\nthe robustness of neural network models. Today's methods are either fast but\nbrittle (gradient-based attacks), or they are fairly reliable but slow (score-\nand decision-based attacks). We here develop a new set of gradient-based\nadversarial attacks which (a) are more reliable in the face of gradient-masking\nthan other gradient-based attacks, (b) perform better and are more query\nefficient than current state-of-the-art gradient-based attacks, (c) can be\nflexibly adapted to a wide range of adversarial criteria and (d) require\nvirtually no hyperparameter tuning. These findings are carefully validated\nacross a diverse set of six different models and hold for L0, L1, L2 and Linf\nin both targeted as well as untargeted scenarios. Implementations will soon be\navailable in all major toolboxes (Foolbox, CleverHans and ART). We hope that\nthis class of attacks will make robustness evaluations easier and more\nreliable, thus contributing to more signal in the search for more robust\nmachine learning models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:18:10 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:32:51 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Brendel", "Wieland", ""], ["Rauber", "Jonas", ""], ["K\u00fcmmerer", "Matthias", ""], ["Ustyuzhaninov", "Ivan", ""], ["Bethge", "Matthias", ""]]}, {"id": "1907.01095", "submitter": "Tae Jong Choi", "authors": "Tae Jong Choi, Julian Togelius, Yun-Gyung Cheong", "title": "Advanced Cauchy Mutation for Differential Evolution in Numerical\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many evolutionary algorithms, differential evolution (DE) has received\nmuch attention over the last two decades. DE is a simple yet powerful\nevolutionary algorithm that has been used successfully to optimize various\nreal-world problems. Since it was introduced, many researchers have developed\nnew methods for DE, and one of them makes use of a mutation based on the Cauchy\ndistribution to increase the convergence speed of DE. The method monitors the\nresults of each individual in the selection operator and performs the Cauchy\nmutation on consecutively failed individuals, which generates mutant vectors by\nperturbing the best individual with the Cauchy distribution. Therefore, the\nmethod can locate the consecutively failed individuals to new positions close\nto the best individual. Although this approach is interesting, it fails to take\ninto account establishing a balance between exploration and exploitation. In\nthis paper, we propose a sigmoid based parameter control that alters the\nfailure threshold for performing the Cauchy mutation in a time-varying\nschedule, which can establish a good ratio between exploration and\nexploitation. Experiments and comparisons have been done with six conventional\nand six advanced DE variants on a set of 30 benchmark problems, which indicate\nthat the DE variants assisted by the proposed algorithm are highly competitive,\nespecially for multimodal functions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 23:13:43 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 21:38:08 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 06:44:34 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Choi", "Tae Jong", ""], ["Togelius", "Julian", ""], ["Cheong", "Yun-Gyung", ""]]}, {"id": "1907.01167", "submitter": "Jibin Wu", "authors": "Jibin Wu, Yansong Chua, Malu Zhang, Guoqi Li, Haizhou Li, Kay Chen Tan", "title": "A Tandem Learning Rule for Effective Training and Rapid Inference of\n  Deep Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) represent the most prominent biologically\ninspired computing model for neuromorphic computing (NC) architectures.\nHowever, due to the non-differentiable nature of spiking neuronal functions,\nthe standard error back-propagation algorithm is not directly applicable to\nSNNs. In this work, we propose a tandem learning framework, that consists of an\nSNN and an Artificial Neural Network (ANN) coupled through weight sharing. The\nANN is an auxiliary structure that facilitates the error back-propagation for\nthe training of the SNN at the spike-train level. To this end, we consider the\nspike count as the discrete neural representation in the SNN, and design ANN\nneuronal activation function that can effectively approximate the spike count\nof the coupled SNN. The proposed tandem learning rule demonstrates competitive\npattern recognition and regression capabilities on both the conventional\nframe-based and event-based vision datasets, with at least an order of\nmagnitude reduced inference time and total synaptic operations over other\nstate-of-the-art SNN implementations. Therefore, the proposed tandem learning\nrule offers a novel solution to training efficient, low latency, and high\naccuracy deep SNNs with low computing resources.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 04:55:18 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 09:50:37 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 09:37:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wu", "Jibin", ""], ["Chua", "Yansong", ""], ["Zhang", "Malu", ""], ["Li", "Guoqi", ""], ["Li", "Haizhou", ""], ["Tan", "Kay Chen", ""]]}, {"id": "1907.01453", "submitter": "Son Duy Dao", "authors": "Son Duy Dao", "title": "A Note On The Popularity of Stochastic Optimization Algorithms in\n  Different Fields: A Quantitative Analysis from 2007 to 2017", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic optimization algorithms are often used to solve complex\nlarge-scale optimization problems in various fields. To date, there have been a\nnumber of stochastic optimization algorithms such as Genetic Algorithm, Cuckoo\nSearch, Tabu Search, Simulated Annealing, Particle Swarm Optimization, Ant\nColony Optimization, etc. Each algorithm has some advantages and disadvantages.\nCurrently, there is no study that can help researchers to choose the most\npopular optimization algorithm to deal with the problems in different research\nfields. In this note, a quantitative analysis of the popularity of 14\nstochastic optimization algorithms in 18 different research fields in the last\nten years from 2007 to 2017 is provided. This quantitative analysis can help\nresearchers/practitioners select the best optimization algorithm to solve\ncomplex large-scale optimization problems in the fields of Engineering,\nComputer science, Operations research, Mathematics, Physics, Chemistry,\nAutomation control systems, Materials science, Energy fuels, Mechanics,\nTelecommunications, Thermodynamics, Optics, Environmental sciences ecology,\nWater resources, Transportation, Construction building technology, and\nRobotics.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 16:40:14 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 08:17:48 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Dao", "Son Duy", ""]]}, {"id": "1907.01515", "submitter": "Yasith Jayawardana", "authors": "Yasith Jayawardana, Mark Jaime, Sashi Thapaliya, Sampath Jayarathna", "title": "Electroencephalogram (EEG) for Delineating Objective Measure of Autism\n  Spectrum Disorder (ASD) (Extended Version)", "comments": null, "journal-ref": null, "doi": "10.4018/978-1-5225-7467-5.ch002", "report-no": null, "categories": "eess.SP cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism Spectrum Disorder (ASD) is a developmental disorder that often impairs\na child's normal development of the brain. According to CDC, it is estimated\nthat 1 in 6 children in the US suffer from development disorders, and 1 in 68\nchildren in the US suffer from ASD. This condition has a negative impact on a\nperson's ability to hear, socialize and communicate. Overall, ASD has a broad\nrange of symptoms and severity; hence the term spectrum is used. One of the\nmain contributors to ASD is known to be genetics. Up to date, no suitable cure\nfor ASD has been found. Early diagnosis is crucial for the long-term treatment\nof ASD, but this is challenging due to the lack of a proper objective measures.\nSubjective measures often take more time, resources, and have false positives\nor false negatives. There is a need for efficient objective measures that can\nhelp in diagnosing this disease early as possible with less effort.\n  EEG measures the electric signals of the brain via electrodes placed on\nvarious places on the scalp. These signals can be used to study complex\nneuropsychiatric issues. Studies have shown that EEG has the potential to be\nused as a biomarker for various neurological conditions including ASD. This\nchapter will outline the usage of EEG measurement for the classification of ASD\nusing machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 01:13:21 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Jayawardana", "Yasith", ""], ["Jaime", "Mark", ""], ["Thapaliya", "Sashi", ""], ["Jayarathna", "Sampath", ""]]}, {"id": "1907.01525", "submitter": "Bhavin Shastri", "authors": "Viraj Bangari, Bicky A. Marquez, Heidi B. Miller, Alexander N. Tait,\n  Mitchell A. Nahmias, Thomas Ferreira de Lima, Hsuan-Tung Peng, Paul R.\n  Prucnal, Bhavin J. Shastri", "title": "Digital Electronics and Analog Photonics for Convolutional Neural\n  Networks (DEAP-CNNs)", "comments": "12 pages, 9 figures, 3 tables", "journal-ref": null, "doi": "10.1109/JSTQE.2019.2945540", "report-no": null, "categories": "eess.SP cs.NE physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are powerful and highly ubiquitous tools\nfor extracting features from large datasets for applications such as computer\nvision and natural language processing. However, a convolution is a\ncomputationally expensive operation in digital electronics. In contrast,\nneuromorphic photonic systems, which have experienced a recent surge of\ninterest over the last few years, propose higher bandwidth and energy\nefficiencies for neural network training and inference. Neuromorphic photonics\nexploits the advantages of optical electronics, including the ease of analog\nprocessing, and busing multiple signals on a single waveguide at the speed of\nlight. Here, we propose a Digital Electronic and Analog Photonic (DEAP) CNN\nhardware architecture that has potential to be 2.8 to 14 times faster while\nmaintaining the same power usage of current state-of-the-art GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 00:50:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bangari", "Viraj", ""], ["Marquez", "Bicky A.", ""], ["Miller", "Heidi B.", ""], ["Tait", "Alexander N.", ""], ["Nahmias", "Mitchell A.", ""], ["de Lima", "Thomas Ferreira", ""], ["Peng", "Hsuan-Tung", ""], ["Prucnal", "Paul R.", ""], ["Shastri", "Bhavin J.", ""]]}, {"id": "1907.01620", "submitter": "Konstantinos Michmizos", "authors": "Guangzhi Tang, Ioannis E. Polykretis, Vladimir A. Ivanov, Arpit Shah,\n  Konstantinos P. Michmizos", "title": "Introducing Astrocytes on a Neuromorphic Processor: Synchronization,\n  Local Plasticity and Edge of Chaos", "comments": "9 pages, 7 figures", "journal-ref": "ACM Proceeding NICE '19 Proceedings of the 7th Annual\n  Neuro-inspired Computational Elements Workshop, 2019", "doi": "10.1145/3320288.3320302", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there is still a lot to learn about astrocytes and their\nneuromodulatory role in the spatial and temporal integration of neuronal\nactivity, their introduction to neuromorphic hardware is timely, facilitating\ntheir computational exploration in basic science questions as well as their\nexploitation in real-world applications. Here, we present an astrocytic module\nthat enables the development of a spiking Neuronal-Astrocytic Network (SNAN)\ninto Intel's Loihi neuromorphic chip. The basis of the Loihi module is an\nend-to-end biophysically plausible compartmental model of an astrocyte that\nsimulates the intracellular activity in response to the synaptic activity in\nspace and time. To demonstrate the functional role of astrocytes in SNAN, we\ndescribe how an astrocyte may sense and induce activity-dependent neuronal\nsynchronization, switch on and off spike-time-dependent plasticity (STDP) to\nintroduce single-shot learning, and monitor the transition between ordered and\nchaotic activity at the synaptic space. Our module may serve as an extension\nfor neuromorphic hardware, by either replicating or exploring the distinct\ncomputational roles that astrocytes have in forming biological intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:19:25 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 16:19:01 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Tang", "Guangzhi", ""], ["Polykretis", "Ioannis E.", ""], ["Ivanov", "Vladimir A.", ""], ["Shah", "Arpit", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "1907.01623", "submitter": "Fernando de Mesentier Silva", "authors": "Fernando de Mesentier Silva, Rodrigo Canaan, Scott Lee, Matthew C.\n  Fontaine, Julian Togelius and Amy K. Hoover", "title": "Evolving the Hearthstone Meta", "comments": "IEEE Conference on Games 2019. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing an ever growing strategic game of high complexity, such as\nHearthstone is a complex task. The target of making strategies diverse and\ncustomizable results in a delicate intricate system. Tuning over 2000 cards to\ngenerate the desired outcome without disrupting the existing environment\nbecomes a laborious challenge. In this paper, we discuss the impacts that\nchanges to existing cards can have on strategy in Hearthstone. By analyzing the\nwin rate on match-ups across different decks, being played by different\nstrategies, we propose to compare their performance before and after changes\nare made to improve or worsen different cards. Then, using an evolutionary\nalgorithm, we search for a combination of changes to the card attributes that\ncause the decks to approach equal, 50% win rates. We then expand our\nevolutionary algorithm to a multi-objective solution to search for this result,\nwhile making the minimum amount of changes, and as a consequence disruption, to\nthe existing cards. Lastly, we propose and evaluate metrics to serve as\nheuristics with which to decide which cards to target with balance changes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:32:08 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Silva", "Fernando de Mesentier", ""], ["Canaan", "Rodrigo", ""], ["Lee", "Scott", ""], ["Fontaine", "Matthew C.", ""], ["Togelius", "Julian", ""], ["Hoover", "Amy K.", ""]]}, {"id": "1907.01773", "submitter": "Kaijie Tu", "authors": "Dawen Xu, Ying Wang, Kaijie Tu, Cheng Liu, Bingsheng He, and Lei Zhang", "title": "Accelerating Generative Neural Networks on Unmodified Deep Learning\n  Processors -- A Software Approach", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural network is a new category of neural networks and it has\nbeen widely utilized in applications such as content generation, unsupervised\nlearning, segmentation and pose estimation. It typically involves massive\ncomputing-intensive deconvolution operations that cannot be fitted to\nconventional neural network processors directly. However, prior works mainly\ninvestigated specialized hardware architectures through intensive hardware\nmodifications to the existing deep learning processors to accelerate\ndeconvolution together with the convolution. In contrast, this work proposes a\nnovel deconvolution implementation with a software approach and enables fast\nand efficient deconvolution execution on the legacy deep learning processors.\nOur proposed method reorganizes the computation of deconvolution and allows the\ndeep learning processors to treat it as the standard convolution by splitting\nthe original deconvolution filters into multiple small filters. Compared to\nprior acceleration schemes, the implemented acceleration scheme achieves 2.41x\n- 4.34x performance speedup and reduces the energy consumption by 27.7% - 54.5%\non a set of realistic benchmarks. In addition, we also applied the\ndeconvolution computing approach to the off-the-shelf commodity deep learning\nprocessors. The performance of deconvolution also exhibits significant\nperformance speedup over prior deconvolution implementations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 07:18:57 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 08:19:41 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 02:50:01 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xu", "Dawen", ""], ["Wang", "Ying", ""], ["Tu", "Kaijie", ""], ["Liu", "Cheng", ""], ["He", "Bingsheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1907.01856", "submitter": "Sidney Pontes-Filho", "authors": "Sidney Pontes-Filho, Anis Yazidi, Jianhua Zhang, Hugo Hammer, Gustavo\n  B. M. Mello, Ioanna Sandvig, Gunnar Tufte, Stefano Nichele", "title": "A general representation of dynamical systems for reservoir computing", "comments": "5 pages, 3 figures, accepted workshop paper at Workshop on Novel\n  Substrates and Models for the Emergence of Developmental, Learning and\n  Cognitive Capabilities at IEEE ICDL-EPIROB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems are capable of performing computation in a reservoir\ncomputing paradigm. This paper presents a general representation of these\nsystems as an artificial neural network (ANN). Initially, we implement the\nsimplest dynamical system, a cellular automaton. The mathematical fundamentals\nbehind an ANN are maintained, but the weights of the connections and the\nactivation function are adjusted to work as an update rule in the context of\ncellular automata. The advantages of such implementation are its usage on\nspecialized and optimized deep learning libraries, the capabilities to\ngeneralize it to other types of networks and the possibility to evolve cellular\nautomata and other dynamical systems in terms of connectivity, update and\nlearning rules. Our implementation of cellular automata constitutes an initial\nstep towards a general framework for dynamical systems. It aims to evolve such\nsystems to optimize their usage in reservoir computing and to model physical\ncomputing substrates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 11:12:42 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Pontes-Filho", "Sidney", ""], ["Yazidi", "Anis", ""], ["Zhang", "Jianhua", ""], ["Hammer", "Hugo", ""], ["Mello", "Gustavo B. M.", ""], ["Sandvig", "Ioanna", ""], ["Tufte", "Gunnar", ""], ["Nichele", "Stefano", ""]]}, {"id": "1907.01939", "submitter": "Marcus M\\\"artens", "authors": "Marcus M\\\"artens and Dario Izzo", "title": "Neural Network Architecture Search with Differentiable Cartesian Genetic\n  Programming for Regression", "comments": "a short version of this was accepted as poster paper at GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to design complex neural network architectures which enable\neffective training by stochastic gradient descent has been the key for many\nachievements in the field of deep learning. However, developing such\narchitectures remains a challenging and resourceintensive process full of\ntrial-and-error iterations. All in all, the relation between the network\ntopology and its ability to model the data remains poorly understood. We\npropose to encode neural networks with a differentiable variant of Cartesian\nGenetic Programming (dCGPANN) and present a memetic algorithm for architecture\ndesign: local searches with gradient descent learn the network parameters while\nevolutionary operators act on the dCGPANN genes shaping the network\narchitecture towards faster learning. Studying a particular instance of such a\nlearning scheme, we are able to improve the starting feed forward topology by\nlearning how to rewire and prune links, adapt activation functions and\nintroduce skip connections for chosen regression tasks. The evolved network\narchitectures require less space for network parameters and reach, given the\nsame amount of time, a significantly lower error on average.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:40:02 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["M\u00e4rtens", "Marcus", ""], ["Izzo", "Dario", ""]]}, {"id": "1907.02050", "submitter": "Daniel Saunders", "authors": "Sam Wenke, Dan Saunders, Mike Qiu, Jim Fleming", "title": "Reasoning and Generalization in RL: A Tool Use Perspective", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to use tools to solve a variety of tasks is an innate ability of\nhumans and has been observed of animals in the wild. However, the underlying\nmechanisms that are required to learn to use tools are abstract and widely\ncontested in the literature. In this paper, we study tool use in the context of\nreinforcement learning and propose a framework for analyzing generalization\ninspired by a classic study of tool using behavior, the trap-tube task.\nRecently, it has become common in reinforcement learning to measure\ngeneralization performance on a single test set of environments. We instead\npropose transfers that produce multiple test sets that are used to measure\nspecified types of generalization, inspired by abilities demonstrated by animal\nand human tool users. The source code to reproduce our experiments is publicly\navailable at https://github.com/fomorians/gym_tool_use.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:35:58 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Wenke", "Sam", ""], ["Saunders", "Dan", ""], ["Qiu", "Mike", ""], ["Fleming", "Jim", ""]]}, {"id": "1907.02124", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Zhang, Geng\n  Yuan, Sia Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, Xue Lin, Kaisheng\n  Ma, Yanzhi Wang", "title": "Non-Structured DNN Weight Pruning -- Is It Beneficial in Any Platform?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large deep neural network (DNN) models pose the key challenge to energy\nefficiency due to the significantly higher energy consumption of off-chip DRAM\naccesses than arithmetic or SRAM operations. It motivates the intensive\nresearch on model compression with two main approaches. Weight pruning\nleverages the redundancy in the number of weights and can be performed in a\nnon-structured, which has higher flexibility and pruning rate but incurs index\naccesses due to irregular weights, or structured manner, which preserves the\nfull matrix structure with lower pruning rate. Weight quantization leverages\nthe redundancy in the number of bits in weights. Compared to pruning,\nquantization is much more hardware-friendly, and has become a \"must-do\" step\nfor FPGA and ASIC implementations. This paper provides a definitive answer to\nthe question for the first time. First, we build ADMM-NN-S by extending and\nenhancing ADMM-NN, a recently proposed joint weight pruning and quantization\nframework. Second, we develop a methodology for fair and fundamental comparison\nof non-structured and structured pruning in terms of both storage and\ncomputation efficiency. Our results show that ADMM-NN-S consistently\noutperforms the prior art: (i) it achieves 348x, 36x, and 8x overall weight\npruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero\naccuracy loss; (ii) we demonstrate the first fully binarized (for all layers)\nDNNs can be lossless in accuracy in many cases. These results provide a strong\nbaseline and credibility of our study. Based on the proposed comparison\nframework, with the same accuracy and quantization, the results show that\nnon-structrued pruning is not competitive in terms of both storage and\ncomputation efficiency. Thus, we conclude that non-structured pruning is\nconsidered harmful. We urge the community not to continue the DNN inference\nacceleration for non-structured sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 20:27:51 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 19:43:16 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Ye", "Shaokai", ""], ["He", "Zhezhi", ""], ["Zhang", "Linfeng", ""], ["Yuan", "Geng", ""], ["Tan", "Sia Huat", ""], ["Li", "Zhengang", ""], ["Fan", "Deliang", ""], ["Qian", "Xuehai", ""], ["Lin", "Xue", ""], ["Ma", "Kaisheng", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1907.02129", "submitter": "Marat Dukhan", "authors": "Marat Dukhan", "title": "The Indirect Convolution Algorithm", "comments": "Presented on Efficient Deep Learning for Computer Vision workshop at\n  CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks commonly implement convolution operators with\nGEMM-based algorithms. In these algorithms, convolution is implemented on top\nof matrix-matrix multiplication (GEMM) functions, provided by highly optimized\nBLAS libraries. Convolutions with 1x1 kernels can be directly represented as a\nGEMM call, but convolutions with larger kernels require a special memory layout\ntransformation - im2col or im2row - to fit into GEMM interface.\n  The Indirect Convolution algorithm provides the efficiency of the GEMM\nprimitive without the overhead of im2col transformation. In contrast to\nGEMM-based algorithms, the Indirect Convolution does not reshuffle the data to\nfit into the GEMM primitive but introduces an indirection buffer - a buffer of\npointers to the start of each row of image pixels. This broadens the\napplication of our modified GEMM function to convolutions with arbitrary kernel\nsize, padding, stride, and dilation.\n  The Indirect Convolution algorithm reduces memory overhead proportionally to\nthe number of input channels and outperforms the GEMM-based algorithm by up to\n62% on convolution parameters which involve im2col transformations in\nGEMM-based algorithms. This, however, comes at cost of minor performance\nreduction on 1x1 stride-1 convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 20:51:18 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Dukhan", "Marat", ""]]}, {"id": "1907.02260", "submitter": "Marco Virgolin", "authors": "Marco Virgolin, Tanja Alderliesten, Peter A.N. Bosman", "title": "On Explaining Machine Learning Models by Evolving Crucial and Compact\n  Features", "comments": "We included more experiments: - A high-dimensional dataset is\n  considered - The machine learning algorithm XGBoost is considered We also\n  repeated the experiments using the Naive Bayes classifier, because we\n  discovered that the implementation we relied on had issues (see\n  https://github.com/mlpack/mlpack/issues/2017)", "journal-ref": null, "doi": "10.1016/j.swevo.2019.100640", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature construction can substantially improve the accuracy of Machine\nLearning (ML) algorithms. Genetic Programming (GP) has been proven to be\neffective at this task by evolving non-linear combinations of input features.\nGP additionally has the potential to improve ML explainability since explicit\nexpressions are evolved. Yet, in most GP works the complexity of evolved\nfeatures is not explicitly bound or minimized though this is arguably key for\nexplainability. In this article, we assess to what extent GP still performs\nfavorably at feature construction when constructing features that are (1) Of\nsmall-enough number, to enable visualization of the behavior of the ML model;\n(2) Of small-enough size, to enable interpretability of the features\nthemselves; (3) Of sufficient informative power, to retain or even improve the\nperformance of the ML algorithm. We consider a simple feature construction\nscheme using three different GP algorithms, as well as random search, to evolve\nfeatures for five ML algorithms, including support vector machines and random\nforest. Our results on 21 datasets pertaining to classification and regression\nproblems show that constructing only two compact features can be sufficient to\nrival the use of the entire original feature set. We further find that a modern\nGP algorithm, GP-GOMEA, performs best overall. These results, combined with\nexamples that we provide of readable constructed features and of 2D\nvisualizations of ML behavior, lead us to positively conclude that GP-based\nfeature construction still works well when explicitly searching for compact\nfeatures, making it extremely helpful to explain ML models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 07:52:23 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 14:55:31 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 11:21:17 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Virgolin", "Marco", ""], ["Alderliesten", "Tanja", ""], ["Bosman", "Peter A. N.", ""]]}, {"id": "1907.02649", "submitter": "Owen Marschall", "authors": "Owen Marschall, Kyunghyun Cho, Cristina Savin", "title": "A Unified Framework of Online Learning Algorithms for Training Recurrent\n  Neural Networks", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for compactly summarizing many recent results in\nefficient and/or biologically plausible online training of recurrent neural\nnetworks (RNN). The framework organizes algorithms according to several\ncriteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs.\ndeterministic, and (d) closed form vs. numerical. These axes reveal latent\nconceptual connections among several recent advances in online learning.\nFurthermore, we provide novel mathematical intuitions for their degree of\nsuccess. Testing various algorithms on two synthetic tasks shows that\nperformances cluster according to our criteria. Although a similar clustering\nis also observed for gradient alignment, alignment with exact methods does not\nalone explain ultimate performance, especially for stochastic algorithms. This\nsuggests the need for better comparison metrics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 01:49:45 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Marschall", "Owen", ""], ["Cho", "Kyunghyun", ""], ["Savin", "Cristina", ""]]}, {"id": "1907.02863", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Cognitive Functions of the Brain: Perception, Attention and Memory", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a follow-up tutorial article of [17] and [16], in this paper, we will\nintroduce several important cognitive functions of the brain. Brain cognitive\nfunctions are the mental processes that allow us to receive, select, store,\ntransform, develop, and recover information that we've received from external\nstimuli. This process allows us to understand and to relate to the world more\neffectively. Cognitive functions are brain-based skills we need to carry out\nany task from the simplest to the most complex. They are related with the\nmechanisms of how we learn, remember, problem-solve, and pay attention, etc. To\nbe more specific, in this paper, we will talk about the perception, attention\nand memory functions of the human brain. Several other brain cognitive\nfunctions, e.g., arousal, decision making, natural language, motor\ncoordination, planning, problem solving and thinking, will be added to this\npaper in the later versions, respectively. Many of the materials used in this\npaper are from wikipedia and several other neuroscience introductory articles,\nwhich will be properly cited in this paper. This is the last of the three\ntutorial articles about the brain. The readers are suggested to read this paper\nafter the previous two tutorial articles on brain structure and functions [17]\nas well as the brain basic neural units [16].\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 23:17:16 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "1907.02871", "submitter": "Hai Victor Habi", "authors": "Hai Victor Habi, Gil Rafalovich", "title": "Genetic Network Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning the neural network architecture that based\non Genetic Algorithm (GA). Our approach uses a genetic algorithm integrated\nwith standard Stochastic Gradient Descent(SGD) which allows the sharing of\nweights across all architecture solutions. The method uses GA to design a\nsub-graph of Convolution cell which maximizes the accuracy on the\nvalidation-set. Through experiments, we demonstrate this methods performance on\nboth CIFAR10 and CIFAR100 dataset with an accuracy of 96% and 80.1%. The code\nand result of this work available in\nGitHub:https://github.com/haihabi/GeneticNAS.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:50:00 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Habi", "Hai Victor", ""], ["Rafalovich", "Gil", ""]]}, {"id": "1907.03076", "submitter": "Mehdi Neshat", "authors": "Mehdi Neshat, Ehsan Abbasnejad, Qinfeng Shi, Bradley Alexander, Markus\n  Wagner", "title": "Adaptive Neuro-Surrogate-Based Optimisation Method for Wave Energy\n  Converters Placement Optimisation", "comments": "12 pages, 2 tables, 6 figures", "journal-ref": "International Conference on Neural Information Processing,2019,pp.\n  353-366. Springer", "doi": "10.1007/978-3-030-36711-4_30", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The installed amount of renewable energy has expanded massively in recent\nyears. Wave energy, with its high capacity factors has great potential to\ncomplement established sources of solar and wind energy. This study explores\nthe problem of optimising the layout of advanced, three-tether wave energy\nconverters in a size-constrained farm in a numerically modelled ocean\nenvironment. Simulating and computing the complicated hydrodynamic interactions\nin wave farms can be computationally costly, which limits optimisation methods\nto have just a few thousand evaluations. For dealing with this expensive\noptimisation problem, an adaptive neuro-surrogate optimisation (ANSO) method is\nproposed that consists of a surrogate Recurrent Neural Network (RNN) model\ntrained with a very limited number of observations. This model is coupled with\na fast meta-heuristic optimiser for adjusting the model's hyper-parameters. The\ntrained model is applied using a greedy local search with a backtracking\noptimisation strategy. For evaluating the performance of the proposed approach,\nsome of the more popular and successful Evolutionary Algorithms (EAs) are\ncompared in four real wave scenarios (Sydney, Perth, Adelaide and Tasmania).\nExperimental results show that the adaptive neuro model is competitive with\nother optimisation methods in terms of total harnessed power output and faster\nin terms of total computational costs.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 04:25:28 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 01:24:32 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Neshat", "Mehdi", ""], ["Abbasnejad", "Ehsan", ""], ["Shi", "Qinfeng", ""], ["Alexander", "Bradley", ""], ["Wagner", "Markus", ""]]}, {"id": "1907.03098", "submitter": "Mehmet Guzel", "authors": "Elit Cenk Alp, Mehmet Serdar Guzel", "title": "Playing Flappy Bird via Asynchronous Advantage Actor Critic Algorithm", "comments": "8 pages , 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flappy Bird, which has a very high popularity, has been trained in many\nalgorithms. Some of these studies were trained from raw pixel values of game\nand some from specific attributes. In this study, the model was trained with\nraw game images, which had not been seen before. The trained model has learned\nas reinforcement when to make which decision. As an input to the model, the\nreward or penalty at the end of each step was returned and the training was\ncompleted. Flappy Bird game was trained with the Reinforcement Learning\nalgorithm Deep Q-Network and Asynchronous Advantage Actor Critic (A3C)\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 09:07:28 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Alp", "Elit Cenk", ""], ["Guzel", "Mehmet Serdar", ""]]}, {"id": "1907.03122", "submitter": "Bicky Marquez", "authors": "Bicky A. Marquez, Jose Suarez-Vargas, Bhavin J. Shastri", "title": "Takens-inspired neuromorphic processor: a downsizing tool for random\n  recurrent neural networks via feature extraction", "comments": "12 pages, 8 figures", "journal-ref": "Phys. Rev. Research 1, 033030 (2019)", "doi": "10.1103/PhysRevResearch.1.033030", "report-no": null, "categories": "cs.NE cs.LG nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new technique which minimizes the amount of neurons in the\nhidden layer of a random recurrent neural network (rRNN) for time series\nprediction. Merging Takens-based attractor reconstruction methods with machine\nlearning, we identify a mechanism for feature extraction that can be leveraged\nto lower the network size. We obtain criteria specific to the particular\nprediction task and derive the scaling law of the prediction error. The\nconsequences of our theory are demonstrated by designing a Takens-inspired\nhybrid processor, which extends a rRNN with a priori designed delay external\nmemory. Our hybrid architecture is therefore designed including both, real and\nvirtual nodes. Via this symbiosis, we show performance of the hybrid processor\nby stabilizing an arrhythmic neural model. Thanks to our obtained design rules,\nwe can reduce the stabilizing neural network's size by a factor of 15 with\nrespect to a standard system.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 12:10:25 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Marquez", "Bicky A.", ""], ["Suarez-Vargas", "Jose", ""], ["Shastri", "Bhavin J.", ""]]}, {"id": "1907.03141", "submitter": "Ning Liu", "authors": "Ning Liu and Xiaolong Ma and Zhiyuan Xu and Yanzhi Wang and Jian Tang\n  and Jieping Ye", "title": "AutoCompress: An Automatic DNN Structured Pruning Framework for\n  Ultra-High Compression Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured weight pruning is a representative model compression technique of\nDNNs to reduce the storage and computation requirements and accelerate\ninference. An automatic hyperparameter determination process is necessary due\nto the large number of flexible hyperparameters. This work proposes\nAutoCompress, an automatic structured pruning framework with the following key\nperformance improvements: (i) effectively incorporate the combination of\nstructured pruning schemes in the automatic process; (ii) adopt the\nstate-of-art ADMM-based structured weight pruning as the core algorithm, and\npropose an innovative additional purification step for further weight reduction\nwithout accuracy loss; and (iii) develop effective heuristic search method\nenhanced by experience-based guided search, replacing the prior deep\nreinforcement learning technique which has underlying incompatibility with the\ntarget pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets\ndemonstrate that AutoCompress is the key to achieve ultra-high pruning rates on\nthe number of weights and FLOPs that cannot be achieved before. As an example,\nAutoCompress outperforms the prior work on automatic model compression by up to\n33x in pruning rate (120x reduction in the actual parameter count) under the\nsame accuracy. Significant inference speedup has been observed from the\nAutoCompress framework on actual measurements on smartphone. We release all\nmodels of this work at anonymous link: http://bit.ly/2VZ63dS.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 15:40:02 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 12:15:38 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Liu", "Ning", ""], ["Ma", "Xiaolong", ""], ["Xu", "Zhiyuan", ""], ["Wang", "Yanzhi", ""], ["Tang", "Jian", ""], ["Ye", "Jieping", ""]]}, {"id": "1907.03202", "submitter": "Anupiya Nugaliyadde Mr", "authors": "J.K. Joseph, W.M.T. Chathurika, A. Nugaliyadde, Y. Mallawarachchi", "title": "Evolutionary Algorithm for Sinhala to English Translation", "comments": "The paper was submitted to National Information Technology Conference\n  (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation (MT) is an area in natural language processing, which\nfocus on translating from one language to another. Many approaches ranging from\nstatistical methods to deep learning approaches are used in order to achieve\nMT. However, these methods either require a large number of data or a clear\nunderstanding about the language. Sinhala language has less digital text which\ncould be used to train a deep neural network. Furthermore, Sinhala has complex\nrules therefore, it is harder to create statistical rules in order to apply\nstatistical methods in MT. This research focuses on Sinhala to English\ntranslation using an Evolutionary Algorithm (EA). EA is used to identifying the\ncorrect meaning of Sinhala text and to translate it to English. The Sinhala\ntext is passed to identify the meaning in order to get the correct meaning of\nthe sentence. With the use of the EA the translation is carried out. The\ntranslated text is passed on to grammatically correct the sentence. This has\nshown to achieve accurate results.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:51:28 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Joseph", "J. K.", ""], ["Chathurika", "W. M. T.", ""], ["Nugaliyadde", "A.", ""], ["Mallawarachchi", "Y.", ""]]}, {"id": "1907.03313", "submitter": "Jacob Sakhnini", "authors": "Jacob Sakhnini and Hadis Karimipour and Ali Dehghantanha", "title": "Smart Grid Cyber Attacks Detection using Supervised Learning and\n  Heuristic Feature Selection", "comments": "5 pages (including references), 3 picture files in 1 figure, to\n  appear in the proceeding of IEEE SEGE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False Data Injection (FDI) attacks are a common form of Cyber-attack\ntargetting smart grids. Detection of stealthy FDI attacks is impossible by the\ncurrent bad data detection systems. Machine learning is one of the alternative\nmethods proposed to detect FDI attacks. This paper analyzes three various\nsupervised learning techniques, each to be used with three different feature\nselection (FS) techniques. These methods are tested on the IEEE 14-bus, 57-bus,\nand 118-bus systems for evaluation of versatility. Accuracy of the\nclassification is used as the main evaluation method for each detection\ntechnique. Simulation study clarify the supervised learning combined with\nheuristic FS methods result in an improved performance of the classification\nalgorithms for FDI attack detection.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 16:27:35 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Sakhnini", "Jacob", ""], ["Karimipour", "Hadis", ""], ["Dehghantanha", "Ali", ""]]}, {"id": "1907.03742", "submitter": "Stella Biderman", "authors": "Stella Rose Biderman", "title": "Neural Networks on Groups", "comments": "Under review at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks traditionally are typically used to approximate\nfunctions defined over $\\mathbb{R}^n$, the successes of graph neural networks,\npoint-cloud neural networks, and manifold deep learning among other methods\nhave demonstrated the clear value of leveraging neural networks to approximate\nfunctions defined over more general spaces. The theory of neural networks has\nnot kept up however,and the relevant theoretical results (when they exist at\nall) have been proven on a case-by-case basis without a general theory or\nconnection to classical work. The process of deriving new theoretical backing\nfor each new type of network has become a bottleneck to understanding and\nvalidating new approaches.\n  In this paper we extend the definition of neural networks to general\ntopological groups and prove that neural networks with a single hidden layer\nand a bounded non-constant activation function can approximate any\n$\\mathcal{L}^p$ function defined over any locally compact Abelian group. This\nframework and universal approximation theorem encompass all of the\naforementioned contexts. We also derive important corollaries and extensions\nwith minor modification, including the case for approximating continuous\nfunctions on a compact subset, neural networks with ReLU activation functions\non a linearly bi-ordered group, and neural networks with affine transformations\non a vector space. Our work obtains as special cases the recent theorems of Qi\net al. [2017], Sennai et al. [2019], Keriven and Peyre [2019], and Maron et al.\n[2019]\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 01:34:12 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 22:36:51 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Biderman", "Stella Rose", ""]]}, {"id": "1907.03743", "submitter": "Hui Yu", "authors": "Hui Yu", "title": "A K-means-based Multi-subpopulation Particle Swarm Optimization for\n  Neural Network Ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a k-means-based multi-subpopulation particle swarm\noptimization, denoted as KMPSO, for training the neural network ensemble. In\nthe proposed KMPSO, particles are dynamically partitioned into clusters via the\nk-means clustering algorithm at every iteration, and each of the resulting\nclusters is responsible for training a component neural network. The\nperformance of the KMPSO has been evaluated on several benchmark problems. Our\nresults show that the proposed method can effectively control the trade-off\nbetween the diversity and accuracy in the ensemble, thus achieving competitive\nresults in comparison with related algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 11:30:38 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Yu", "Hui", ""]]}, {"id": "1907.03773", "submitter": "W B Langdon", "authors": "William B. Langdon and Westley Weimer and Christopher Timperley and\n  Oliver Krauss and Zhen Yu Ding and Yiwei Lyu and Nicolas Chausseau and Eric\n  Schulte and Shin Hwei Tan and Kevin Leach and Yu Huang and Gabin An", "title": "The State and Future of Genetic Improvement", "comments": "University College London, Computer Science", "journal-ref": "SIGSOFT Software Engineering Notes, 44(3) p25-29, July 2019", "doi": "10.1145/3356773.3356801", "report-no": "RN/19/02", "categories": "cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the discussion session at the sixth international Genetic\nImprovement workshop, GI-2019 @ ICSE, which was held as part of the 41st\nACM/IEEE International Conference on Software Engineering on Tuesday 28th May\n2019. Topics included GI representations, the maintainability of evolved code,\nautomated software testing, future areas of GI research, such as co-evolution,\nand existing GI tools and benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 18:44:14 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Langdon", "William B.", ""], ["Weimer", "Westley", ""], ["Timperley", "Christopher", ""], ["Krauss", "Oliver", ""], ["Ding", "Zhen Yu", ""], ["Lyu", "Yiwei", ""], ["Chausseau", "Nicolas", ""], ["Schulte", "Eric", ""], ["Tan", "Shin Hwei", ""], ["Leach", "Kevin", ""], ["Huang", "Yu", ""], ["An", "Gabin", ""]]}, {"id": "1907.03799", "submitter": "Vincenzo Lomonaco PhD", "authors": "Vincenzo Lomonaco, Davide Maltoni, Lorenzo Pellegrini", "title": "Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches", "comments": "Accepted in the CLVision Workshop at CVPR2020: 12 pages, 7 figures, 5\n  tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic vision is a field where continual learning can play a significant\nrole. An embodied agent operating in a complex environment subject to frequent\nand unpredictable changes is required to learn and adapt continuously. In the\ncontext of object recognition, for example, a robot should be able to learn\n(without forgetting) objects of never before seen classes as well as improving\nits recognition capabilities as new instances of already known classes are\ndiscovered. Ideally, continual learning should be triggered by the availability\nof short videos of single objects and performed on-line on on-board hardware\nwith fine-grained updates. In this paper, we introduce a novel continual\nlearning protocol based on the CORe50 benchmark and propose two rehearsal-free\ncontinual learning techniques, CWR* and AR1*, that can learn effectively even\nin the challenging case of nearly 400 small non-i.i.d. incremental batches. In\nparticular, our experiments show that AR1* can outperform other\nstate-of-the-art rehearsal-free techniques by more than 15% accuracy in some\ncases, with a very light and constant computational and memory overhead across\ntraining batches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:32:25 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 21:18:49 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 16:13:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""], ["Pellegrini", "Lorenzo", ""]]}, {"id": "1907.03840", "submitter": "Rodrigo Canaan", "authors": "Rodrigo Canaan, Julian Togelius, Andy Nealen, Stefan Menzel", "title": "Diverse Agents for Ad-Hoc Cooperation in Hanabi", "comments": "8 pages, 4 figures. Accepted at the 2019 IEEE Conference on Games\n  (CoG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex scenarios where a model of other actors is necessary to predict\nand interpret their actions, it is often desirable that the model works well\nwith a wide variety of previously unknown actors. Hanabi is a card game that\nbrings the problem of modeling other players to the forefront, but there is no\nagreement on how to best generate a pool of agents to use as partners in ad-hoc\ncooperation evaluation. This paper proposes Quality Diversity algorithms as a\npromising class of algorithms to generate populations for this purpose and\nshows an initial implementation of an agent generator based on this idea. We\nalso discuss what metrics can be used to compare such generators, and how the\nproposed generator could be leveraged to help build adaptive agents for the\ngame.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:04:22 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Canaan", "Rodrigo", ""], ["Togelius", "Julian", ""], ["Nealen", "Andy", ""], ["Menzel", "Stefan", ""]]}, {"id": "1907.03876", "submitter": "Beren Millidge Mr", "authors": "Beren Millidge", "title": "Deep Active Inference as Variational Policy Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Inference is a theory of action arising from neuroscience which casts\naction and planning as a bayesian inference problem to be solved by minimizing\na single quantity - the variational free energy. Active Inference promises a\nunifying account of action and perception coupled with a biologically plausible\nprocess theory. Despite these potential advantages, current implementations of\nActive Inference can only handle small, discrete policy and state-spaces and\ntypically require the environmental dynamics to be known. In this paper we\npropose a novel deep Active Inference algorithm which approximates key\ndensities using deep neural networks as flexible function approximators, which\nenables Active Inference to scale to significantly larger and more complex\ntasks. We demonstrate our approach on a suite of OpenAIGym benchmark tasks and\nobtain performance comparable with common reinforcement learning baselines.\nMoreover, our algorithm shows similarities with maximum entropy reinforcement\nlearning and the policy gradients algorithm, which reveals interesting\nconnections between the Active Inference framework and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 21:14:29 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Millidge", "Beren", ""]]}, {"id": "1907.03885", "submitter": "Hamidreza Ghader", "authors": "Hamidreza Ghader, Christof Monz", "title": "An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation\n  Architectures", "comments": "To be presented at Machine Translation Summit 2019 (MTSUMMIT XVII),\n  Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Earlier approaches indirectly studied the information captured by the hidden\nstates of recurrent and non-recurrent neural machine translation models by\nfeeding them into different classifiers. In this paper, we look at the encoder\nhidden states of both transformer and recurrent machine translation models from\nthe nearest neighbors perspective. We investigate to what extent the nearest\nneighbors share information with the underlying word embeddings as well as\nrelated WordNet entries. Additionally, we study the underlying syntactic\nstructure of the nearest neighbors to shed light on the role of syntactic\nsimilarities in bringing the neighbors together. We compare transformer and\nrecurrent models in a more intrinsic way in terms of capturing lexical\nsemantics and syntactic structures, in contrast to extrinsic approaches used by\nprevious works. In agreement with the extrinsic evaluations in the earlier\nworks, our experimental results show that transformers are superior in\ncapturing lexical semantics, but not necessarily better in capturing the\nunderlying syntax. Additionally, we show that the backward recurrent layer in a\nrecurrent model learns more about the semantics of words, whereas the forward\nrecurrent layer encodes more context.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 21:39:29 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ghader", "Hamidreza", ""], ["Monz", "Christof", ""]]}, {"id": "1907.04001", "submitter": "Ygor Sousa", "authors": "Ygor C. N. Sousa, Hansenclever F. Bassani", "title": "Incremental Semantic Mapping with Unsupervised On-line Learning", "comments": null, "journal-ref": "IEEE International Joint Conference on Neural Networks (IJCNN),\n  July 2018", "doi": "10.1109/IJCNN.2018.8489430", "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces an incremental semantic mapping approach, with on-line\nunsupervised learning, based on Self-Organizing Maps (SOM) for robotic agents.\nThe method includes a mapping module, which incrementally creates a topological\nmap of the environment, enriched with objects recognized around each\ntopological node, and a module of places categorization, endowed with an\nincremental unsupervised learning SOM with on-line training. The proposed\napproach was tested in experiments with real-world data, in which it\ndemonstrates promising capabilities of incremental acquisition of topological\nmaps enriched with semantic information, and for clustering together similar\nplaces based on this information. The approach was also able to continue\nlearning from newly visited environments without degrading the information\npreviously learned.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:18:29 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 03:52:13 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Sousa", "Ygor C. N.", ""], ["Bassani", "Hansenclever F.", ""]]}, {"id": "1907.04053", "submitter": "Daniele Gravina", "authors": "Daniele Gravina, Ahmed Khalifa, Antonios Liapis, Julian Togelius,\n  Georgios N. Yannakakis", "title": "Procedural Content Generation through Quality Diversity", "comments": "8 pages, Accepted and to appear in proceedings of the IEEE Conference\n  on Games, 2019", "journal-ref": null, "doi": "10.1109/CIG.2019.8848053", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality-diversity (QD) algorithms search for a set of good solutions which\ncover a space as defined by behavior metrics. This simultaneous focus on\nquality and diversity with explicit metrics sets QD algorithms apart from\nstandard single- and multi-objective evolutionary algorithms, as well as from\ndiversity preservation approaches such as niching. These properties open up new\navenues for artificial intelligence in games, in particular for procedural\ncontent generation. Creating multiple systematically varying solutions allows\nnew approaches to creative human-AI interaction as well as adaptivity. In the\nlast few years, a handful of applications of QD to procedural content\ngeneration and game playing have been proposed; we discuss these and propose\nchallenges for future work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:22:16 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gravina", "Daniele", ""], ["Khalifa", "Ahmed", ""], ["Liapis", "Antonios", ""], ["Togelius", "Julian", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1907.04060", "submitter": "Yulia Sandamirskaya", "authors": "Alpha Renner, Matthew Evanusa, Yulia Sandamirskaya", "title": "Event-based attention and tracking on neuromorphic hardware", "comments": "IEEE Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully event-driven vision and processing system for selective\nattention and tracking, realized on a neuromorphic processor Loihi interfaced\nto an event-based Dynamic Vision Sensor DAVIS. The attention mechanism is\nrealized as a recurrent spiking neural network that implements\nattractor-dynamics of dynamic neural fields. We demonstrate capability of the\nsystem to create sustained activation that supports object tracking when\ndistractors are present or when the object slows down or stops, reducing the\nnumber of generated events.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:51:04 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Renner", "Alpha", ""], ["Evanusa", "Matthew", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1907.04160", "submitter": "Ninad Joshi", "authors": "N. Joshi", "title": "Learning in Competitive Network with Haeusslers Equation adapted using\n  FIREFLY algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the competitive neural network consists of spatially arranged\nneurons. The weigh matrix that connects cells represents local excitation and\nlong-range inhibition. They are known as soft-winner-take-all networks and\nshown to exhibit desirable information-processing. The local excitatory\nconnections are many times predefined hand-wired based depending on spatial\narrangement which is chosen using the previous knowledge of data. Here we\npresent learning in recurrent network through Haeusslers equation and modified\nwiring scheme based on biologically based Firefly algorithm. Following results\nshow learning in such network from input patterns without hand-wiring with\nfixed topology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:40:14 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Joshi", "N.", ""]]}, {"id": "1907.04190", "submitter": "Sadek Bouroubi", "authors": "Nabil Boumedine and Sadek Bouroubi", "title": "A new hybrid genetic algorithm for protein structure prediction on the\n  2D triangular lattice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flawless functioning of a protein is essentially linked to its own\nthree-dimensional structure. Therefore, the prediction of a protein structure\nfrom its amino acid sequence is a fundamental problem in many fields that draws\nresearchers attention. This problem can be formulated as a combinatorial\noptimization problem based on simplified lattice models such as the\nhydrophobic-polar model. In this paper, we propose a new hybrid algorithm\ncombining three different well-known heuristic algorithms: genetic algorithm,\ntabu search strategy and local search algorithm in order to solve the PSP\nproblem. Regarding the assessment of suggested algorithm, an experimental study\nis included, where we considered the quality of the produced solution as the\nmain quality criterion. Furthermore, we compared the suggested algorithm with\nstate-of-the-art algorithms using a selection of well-studied benchmark\ninstances.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:33:10 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Boumedine", "Nabil", ""], ["Bouroubi", "Sadek", ""]]}, {"id": "1907.04258", "submitter": "Majid Farzaneh", "authors": "Majid Farzaneh and Rahil Mahdian Toroghi", "title": "Melody Generation using an Interactive Evolutionary Algorithm", "comments": "5 pages, 4 images, submitted to MEDPRAI2019 conference", "journal-ref": null, "doi": "10.1007/978-3-030-37548-5_16", "report-no": null, "categories": "cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music generation with the aid of computers has been recently grabbed the\nattention of many scientists in the area of artificial intelligence. Deep\nlearning techniques have evolved sequence production methods for this purpose.\nYet, a challenging problem is how to evaluate generated music by a machine. In\nthis paper, a methodology has been developed based upon an interactive\nevolutionary optimization method, with which the scoring of the generated\nmelodies is primarily performed by human expertise, during the training. This\nmusic quality scoring is modeled using a Bi-LSTM recurrent neural network.\nMoreover, the innovative generated melody through a Genetic algorithm will then\nbe evaluated using this Bi-LSTM network. The results of this mechanism clearly\nshow that the proposed method is able to create pleasurable melodies with\ndesired styles and pieces. This method is also quite fast, compared to the\nstate-of-the-art data-oriented evolutionary systems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 02:08:25 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Farzaneh", "Majid", ""], ["Toroghi", "Rahil Mahdian", ""]]}, {"id": "1907.04281", "submitter": "Giulia Cisotto", "authors": "Matteo Gadaleta, Giulia Cisotto, Michele Rossi, Rana Zia Ur Rehman,\n  Lynn Rochester, Silvia Del Din", "title": "Deep Learning Techniques for Improving Digital Gait Segmentation", "comments": null, "journal-ref": "2019 41st Annual International Conference of the IEEE Engineering\n  in Medicine and Biology Society (EMBC)", "doi": "10.1109/EMBC.2019.8856685", "report-no": null, "categories": "eess.SP cs.LG cs.NE eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable technology for the automatic detection of gait events has recently\ngained growing interest, enabling advanced analyses that were previously\nlimited to specialist centres and equipment (e.g., instrumented walkway). In\nthis study, we present a novel method based on dilated convolutions for an\naccurate detection of gait events (initial and final foot contacts) from\nwearable inertial sensors. A rich dataset has been used to validate the method,\nfeaturing 71 people with Parkinson's disease (PD) and 67 healthy control\nsubjects. Multiple sensors have been considered, one located on the fifth\nlumbar vertebrae and two on the ankles. The aims of this study were: (i) to\napply deep learning (DL) techniques on wearable sensor data for gait\nsegmentation and quantification in older adults and in people with PD; (ii) to\nvalidate the proposed technique for measuring gait against traditional gold\nstandard laboratory reference and a widely used algorithm based on wavelet\ntransforms (WT); (iii) to assess the performance of DL methods in assessing\nhigh-level gait characteristics, with focus on stride, stance and swing related\nfeatures. The results showed a high reliability of the proposed approach, which\nachieves temporal errors considerably smaller than WT, in particular for the\ndetection of final contacts, with an inter-quartile range below 70 ms in the\nworst case. This study showes encouraging results, and paves the road for\nfurther research, addressing the effectiveness and the generalization of\ndata-driven learning systems for accurate event detection in challenging\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:29:58 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gadaleta", "Matteo", ""], ["Cisotto", "Giulia", ""], ["Rossi", "Michele", ""], ["Rehman", "Rana Zia Ur", ""], ["Rochester", "Lynn", ""], ["Del Din", "Silvia", ""]]}, {"id": "1907.04482", "submitter": "Cheng He", "authors": "Cheng He, Shihua Huang, Ran Cheng, Kay Chen Tan, and Yaochu Jin", "title": "Evolutionary Multi-Objective Optimization Driven by Generative\n  Adversarial Networks", "comments": "This is a redundant version of 1910.04966", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, more and more works have proposed to drive evolutionary algorithms\nusing machine learning models.Usually, the performance of such model based\nevolutionary algorithms is highly dependent on the training qualities of the\nadopted models.Since it usually requires a certain amount of data (i.e. the\ncandidate solutions generated by the algorithms) for model training, the\nperformance deteriorates rapidly with the increase of the problem scales, due\nto the curse of dimensionality.To address this issue, we propose a\nmulti-objective evolutionary algorithm driven by the generative adversarial\nnetworks (GANs).At each generation of the proposed algorithm, the parent\nsolutions are first classified into \\emph{real} and \\emph{fake} samples to\ntrain the GANs; then the offspring solutions are sampled by the trained\nGANs.Thanks to the powerful generative ability of the GANs, our proposed\nalgorithm is capable of generating promising offspring solutions in\nhigh-dimensional decision space with limited training data.The proposed\nalgorithm is tested on 10 benchmark problems with up to 200 decision\nvariables.Experimental results on these test problems demonstrate the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 01:50:20 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:24:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["He", "Cheng", ""], ["Huang", "Shihua", ""], ["Cheng", "Ran", ""], ["Tan", "Kay Chen", ""], ["Jin", "Yaochu", ""]]}, {"id": "1907.04629", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Evolutionary techniques in lattice sieving algorithms", "comments": "9 pages, 2 figures", "journal-ref": "11th International Conference on Evolutionary Computation Theory\n  and Applications (ECTA), pp. 31-39, 2019", "doi": "10.5220/0007968800310039", "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattice-based cryptography has recently emerged as a prominent candidate for\nsecure communication in the quantum age. Its security relies on the hardness of\ncertain lattice problems, and the inability of known lattice algorithms, such\nas lattice sieving, to solve these problems efficiently. In this paper we\ninvestigate the similarities between lattice sieving and evolutionary\nalgorithms, how various improvements to lattice sieving can be viewed as\napplications of known techniques from evolutionary computation, and how other\nevolutionary techniques can benefit lattice sieving in practice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:38:33 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1907.04650", "submitter": "Weiwen Jiang", "authors": "Weiwen Jiang, Lei Yang, Edwin Sha, Qingfeng Zhuge, Shouzhen Gu,\n  Sakyasingha Dasgupta, Yiyu Shi, Jingtong Hu", "title": "Hardware/Software Co-Exploration of Neural Architectures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hardware and software co-exploration framework for\nefficient neural architecture search (NAS). Different from existing\nhardware-aware NAS which assumes a fixed hardware design and explores the\nneural architecture search space only, our framework simultaneously explores\nboth the architecture search space and the hardware design space to identify\nthe best neural architecture and hardware pairs that maximize both test\naccuracy and hardware efficiency. Such a practice greatly opens up the design\nfreedom and pushes forward the Pareto frontier between hardware efficiency and\ntest accuracy for better design tradeoffs. The framework iteratively performs a\ntwo-level (fast and slow) exploration. Without lengthy training, the fast\nexploration can effectively fine-tune hyperparameters and prune inferior\narchitectures in terms of hardware specifications, which significantly\naccelerates the NAS process. Then, the slow exploration trains candidates on a\nvalidation set and updates a controller using the reinforcement learning to\nmaximize the expected accuracy together with the hardware efficiency.\nExperiments on ImageNet show that our co-exploration NAS can find the neural\narchitectures and associated hardware design with the same accuracy, 35.24%\nhigher throughput, 54.05% higher energy efficiency and 136x reduced search\ntime, compared with the state-of-the-art hardware-aware NAS.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 14:16:51 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 14:15:01 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Jiang", "Weiwen", ""], ["Yang", "Lei", ""], ["Sha", "Edwin", ""], ["Zhuge", "Qingfeng", ""], ["Gu", "Shouzhen", ""], ["Dasgupta", "Sakyasingha", ""], ["Shi", "Yiyu", ""], ["Hu", "Jingtong", ""]]}, {"id": "1907.04736", "submitter": "Sneha Aenugu", "authors": "Sneha Aenugu, Lee Spector", "title": "Lexicase selection in Learning Classifier Systems", "comments": "Genetic and Evolutionary Computation Conference, 2019", "journal-ref": null, "doi": "10.1145/3321707.3321828", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lexicase parent selection method selects parents by considering\nperformance on individual data points in random order instead of using a\nfitness function based on an aggregated data accuracy. While the method has\ndemonstrated promise in genetic programming and more recently in genetic\nalgorithms, its applications in other forms of evolutionary machine learning\nhave not been explored. In this paper, we investigate the use of lexicase\nparent selection in Learning Classifier Systems (LCS) and study its effect on\nclassification problems in a supervised setting. We further introduce a new\nvariant of lexicase selection, called batch-lexicase selection, which allows\nfor the tuning of selection pressure. We compare the two lexicase selection\nmethods with tournament and fitness proportionate selection methods on binary\nclassification problems. We show that batch-lexicase selection results in the\ncreation of more generic rules which is favorable for generalization on future\ndata. We further show that batch-lexicase selection results in better\ngeneralization in situations of partial or missing data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:06:22 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Aenugu", "Sneha", ""], ["Spector", "Lee", ""]]}, {"id": "1907.04840", "submitter": "Tim Dettmers", "authors": "Tim Dettmers, Luke Zettlemoyer", "title": "Sparse Networks from Scratch: Faster Training without Losing Performance", "comments": "9 page NeurIPS 2019 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the possibility of what we call sparse learning: accelerated\ntraining of deep neural networks that maintain sparse weights throughout\ntraining while achieving dense performance levels. We accomplish this by\ndeveloping sparse momentum, an algorithm which uses exponentially smoothed\ngradients (momentum) to identify layers and weights which reduce the error\nefficiently. Sparse momentum redistributes pruned weights across layers\naccording to the mean momentum magnitude of each layer. Within a layer, sparse\nmomentum grows weights according to the momentum magnitude of zero-valued\nweights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10,\nand ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared\nto other sparse algorithms. Furthermore, we show that sparse momentum reliably\nreproduces dense performance levels while providing up to 5.61x faster\ntraining. In our analysis, ablations show that the benefits of momentum\nredistribution and growth increase with the depth and size of the network.\nAdditionally, we find that sparse momentum is insensitive to the choice of its\nhyperparameters suggesting that sparse momentum is robust and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:40:20 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 18:30:16 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Dettmers", "Tim", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1907.04913", "submitter": "Amir Mosavi", "authors": "Danial Mohammadzadeh, Seyed-Farzan Kazemi, Amir Mosavi, Ehsan\n  Nasseralshariati, Joseph H. M. Tah", "title": "Prediction of Compression Index of Fine-Grained Soils Using a Gene\n  Expression Programming Model", "comments": "8 figures, 5 tables, 12 pages", "journal-ref": "Infrastructures 2019, 4, 26", "doi": "10.3390/infrastructures4020026", "report-no": null, "categories": "stat.AP cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In construction projects, estimation of the settlement of fine-grained soils\nis of critical importance, and yet is a challenging task. The coefficient of\nconsolidation for the compression index (Cc) is a key parameter in modeling the\nsettlement of fine-grained soil layers. However, the estimation of this\nparameter is costly, time-consuming, and requires skilled technicians. To\novercome these drawbacks, we aimed to predict Cc through other soil parameters,\ni.e., the liquid limit (LL), plastic limit (PL), and initial void ratio (e0).\nUsing these parameters is more convenient and requires substantially less time\nand cost compared to the conventional tests to estimate Cc. This study presents\na novel prediction model for the Cc of fine-grained soils using gene expression\nprogramming (GEP). A database consisting of 108 different data points was used\nto develop the model. A closed-form equation solution was derived to estimate\nCc based on LL, PL, and e0. The performance of the developed GEP-based model\nwas evaluated through the coefficient of determination (R2), the root mean\nsquared error (RMSE), and the mean average error (MAE). The proposed model\nperformed better in terms of R2, RMSE, and MAE compared to the other models.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 07:12:35 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Mohammadzadeh", "Danial", ""], ["Kazemi", "Seyed-Farzan", ""], ["Mosavi", "Amir", ""], ["Nasseralshariati", "Ehsan", ""], ["Tah", "Joseph H. M.", ""]]}, {"id": "1907.05200", "submitter": "Francisco Yepes Barrera Dr.", "authors": "Francisco Yepes Barrera", "title": "Eigen Artificial Neural Networks", "comments": "45 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work has its origin in intuitive physical and statistical\nconsiderations. The problem of optimizing an artificial neural network is\ntreated as a physical system, composed of a conservative vector force field.\nThe derived scalar potential is a measure of the potential energy of the\nnetwork, a function of the distance between predictions and targets.\n  Starting from some analogies with wave mechanics, the description of the\nsystem is justified with an eigenvalue equation that is a variant of the\nSchr\\~odinger equation, in which the potential is defined by the mutual\ninformation between inputs and targets. The weights and parameters of the\nnetwork, as well as those of the state function, are varied so as to minimize\nenergy, using an equivalent of the variational theorem of wave mechanics. The\nminimum energy thus obtained implies the principle of minimum mutual\ninformation (MinMI). We also propose a definition of the potential work\nproduced by the force field to bring a network from an arbitrary probability\ndistribution to the potential-constrained system, which allows to establish a\nmeasure of the complexity of the system. At the end of the discussion we expose\na recursive procedure that allows to refine the state function and bypass some\ninitial assumptions, as well as a discussion of some topics in quantum\nmechanics applied to the formalism, such as the uncertainty principle and the\ntemporal evolution of the system.\n  Results demonstrate how the minimization of energy effectively leads to a\ndecrease in the average error between network predictions and targets.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 21:31:52 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 13:35:47 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 21:36:01 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Barrera", "Francisco Yepes", ""]]}, {"id": "1907.05482", "submitter": "Marcos Cardinot", "authors": "Marcos Cardinot, Colm O'Riordan, Josephine Griffith and Attila\n  Szolnoki", "title": "Mobility restores the mechanism which supports cooperation in the\n  voluntary prisoner's dilemma game", "comments": "15 pages, 8 figures; accepted for publication in New Journal of\n  Physics", "journal-ref": null, "doi": "10.1088/1367-2630/ab3064", "report-no": null, "categories": "physics.soc-ph cs.GT cs.NE nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is generally believed that in a situation where individual and collective\ninterests are in conflict, the availability of optional participation is a key\nmechanism to maintain cooperation. Surprisingly, this effect is sensitive to\nthe use of microscopic dynamics and can easily be broken when agents make a\nfully rational decision during their strategy updates. In the framework of the\ncelebrated prisoner's dilemma game, we show that this discrepancy can be fixed\nautomatically if we leave the strict and frequently artifact condition of a\nfully occupied interaction graph, and allow agents to change not just their\nstrategies but also their positions according to their success. In this way, a\ndiluted graph where agents may move offers a natural and alternative way to\nhandle artifacts arising from the application of specific and sometimes awkward\nmicroscopic rules.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 20:51:14 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Cardinot", "Marcos", ""], ["O'Riordan", "Colm", ""], ["Griffith", "Josephine", ""], ["Szolnoki", "Attila", ""]]}, {"id": "1907.05827", "submitter": "Ayon Borthakur Mr", "authors": "Ayon Borthakur, Thomas A. Cleland", "title": "Signal Conditioning for Learning in the Wild", "comments": "Neuro-inspired Computational Elements Workshop(NICE 19), March 26-28,\n  2019, Albany, NY, USA. ACM, New York, NY, USA, 11 pages", "journal-ref": null, "doi": "10.1145/3320288.3320293", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mammalian olfactory system learns rapidly from very few examples,\npresented in unpredictable online sequences, and then recognizes these learned\nodors under conditions of substantial interference without exhibiting\ncatastrophic forgetting. We have developed a brain-mimetic algorithm that\nreplicates these properties, provided that sensory inputs adhere to a common\nstatistical structure. However, in natural, unregulated environments, this\nconstraint cannot be assured. We here present a series of signal conditioning\nsteps, inspired by the mammalian olfactory system, that transform diverse\nsensory inputs into a regularized statistical structure to which the learning\nnetwork can be tuned. This pre-processing enables a single instantiated network\nto be applied to widely diverse classification tasks and datasets - here\nincluding gas sensor data, remote sensing from spectral characteristics, and\nmulti-label hierarchical identification of wild species - without adjusting\nnetwork hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 16:25:50 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Borthakur", "Ayon", ""], ["Cleland", "Thomas A.", ""]]}, {"id": "1907.05951", "submitter": "Alfonso Rojas Dr", "authors": "S. Ivvan Valdez and Alfonso Rojas-Dom\\'inguez", "title": "An Evolutionary Algorithm of Linear complexity: Application to Training\n  of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep neural networks, such as Deep Belief Networks formed\nby Restricted Boltzmann Machines (RBMs), strongly depends on their training,\nwhich is the process of adjusting their parameters. This process can be posed\nas an optimization problem over n dimensions. However, typical networks contain\ntens of thousands of parameters, making this a High-Dimensional Problem (HDP).\nAlthough different optimization methods have been employed for this goal, the\nuse of most of the Evolutionary Algorithms (EAs) becomes prohibitive due to\ntheir inability to deal with HDPs. For instance, the Covariance Matrix\nAdaptation Evolutionary Strategy (CMA-ES) which is regarded as one of the most\neffective EAs, exhibits the enormous disadvantage of requiring $O(n^2)$ memory\nand operations, making it unpractical for problems with more than a few hundred\nvariables. In this paper, we introduce a novel EA that requires $O(n)$\noperations and memory, but delivers competitive solutions for the training\nstage of RBMs with over one million variables, when compared against CMA-ES and\nthe Contrastive Divergence algorithm, which is the standard method for training\nRBMs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 20:49:19 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Valdez", "S. Ivvan", ""], ["Rojas-Dom\u00ednguez", "Alfonso", ""]]}, {"id": "1907.06062", "submitter": "Nibaran Das", "authors": "Bodhisatwa Mandal, Swarnendu Ghosh, Ritesh Sarkhel, Nibaran Das, Mita\n  Nasipuri", "title": "Using dynamic routing to extract intermediate features for developing\n  scalable capsule networks", "comments": "Second International Conference on Advanced Computational and\n  Communication Paradigms held at Sikkim Manipal Institute of Technology,\n  Sikkim, India during February 25 - 28 , 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks have gained a lot of popularity in short time due to its\nunique approach to model equivariant class specific properties as capsules from\nimages. However the dynamic routing algorithm comes with a steep computational\ncomplexity. In the proposed approach we aim to create scalable versions of the\ncapsule networks that are much faster and provide better accuracy in problems\nwith higher number of classes. By using dynamic routing to extract intermediate\nfeatures instead of generating output class specific capsules, a large increase\nin the computational speed has been observed. Moreover, by extracting\nequivariant feature capsules instead of class specific capsules, the\ngeneralization capability of the network has also increased as a result of\nwhich there is a boost in accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 12:12:36 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mandal", "Bodhisatwa", ""], ["Ghosh", "Swarnendu", ""], ["Sarkhel", "Ritesh", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1907.06077", "submitter": "Alexander Gajewski", "authors": "Alexander Gajewski, Jeff Clune, Kenneth O. Stanley, and Joel Lehman", "title": "Evolvability ES: Scalable and Direct Optimization of Evolvability", "comments": "Published in GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing evolutionary algorithms capable of uncovering highly evolvable\nrepresentations is an open challenge; such evolvability is important because it\naccelerates evolution and enables fast adaptation to changing circumstances.\nThis paper introduces evolvability ES, an evolutionary algorithm designed to\nexplicitly and efficiently optimize for evolvability, i.e. the ability to\nfurther adapt. The insight is that it is possible to derive a novel objective\nin the spirit of natural evolution strategies that maximizes the diversity of\nbehaviors exhibited when an individual is subject to random mutations, and that\nefficiently scales with computation. Experiments in 2-D and 3-D locomotion\ntasks highlight the potential of evolvability ES to generate solutions with\ntens of thousands of parameters that can quickly be adapted to solve different\ntasks and that can productively seed further evolution. We further highlight a\nconnection between evolvability and a recent and popular gradient-based\nmeta-learning algorithm called MAML; results show that evolvability ES can\nperform competitively with MAML and that it discovers solutions with distinct\nproperties. The conclusion is that evolvability ES opens up novel research\ndirections for studying and exploiting the potential of evolvable\nrepresentations for deep neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 13:37:29 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Gajewski", "Alexander", ""], ["Clune", "Jeff", ""], ["Stanley", "Kenneth O.", ""], ["Lehman", "Joel", ""]]}, {"id": "1907.06119", "submitter": "Nibaran Das", "authors": "Swarnendu Ghosh, Nibaran Das, Ishita Das, Ujjwal Maulik", "title": "Understanding Deep Learning Techniques for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community has been overwhelmed by a plethora of deep\nlearning based approaches. Many challenging computer vision tasks such as\ndetection, localization, recognition and segmentation of objects in\nunconstrained environment are being efficiently addressed by various types of\ndeep neural networks like convolutional neural networks, recurrent networks,\nadversarial networks, autoencoders and so on. While there have been plenty of\nanalytical studies regarding the object detection or recognition domain, many\nnew deep learning techniques have surfaced with respect to image segmentation\ntechniques. This paper approaches these various deep learning techniques of\nimage segmentation from an analytical perspective. The main goal of this work\nis to provide an intuitive understanding of the major techniques that has made\nsignificant contribution to the image segmentation domain. Starting from some\nof the traditional image segmentation approaches, the paper progresses\ndescribing the effect deep learning had on the image segmentation domain.\nThereafter, most of the major segmentation algorithms have been logically\ncategorized with paragraphs dedicated to their unique contribution. With an\nample amount of intuitive explanations, the reader is expected to have an\nimproved ability to visualize the internal dynamics of these processes.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 19:23:42 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ghosh", "Swarnendu", ""], ["Das", "Nibaran", ""], ["Das", "Ishita", ""], ["Maulik", "Ujjwal", ""]]}, {"id": "1907.06269", "submitter": "Lasse Bj{\\o}rn Kristensen", "authors": "Lasse Bj{\\o}rn Kristensen, Matthias Degroote, Peter Wittek, Al\\'an\n  Aspuru-Guzik, Nikolaj T. Zinner", "title": "An Artificial Spiking Quantum Neuron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial spiking neural networks have found applications in areas where the\ntemporal nature of activation offers an advantage, such as time series\nprediction and signal processing. To improve their efficiency, spiking\narchitectures often run on custom-designed neuromorphic hardware, but, despite\ntheir attractive properties, these implementations have been limited to digital\nsystems. We describe an artificial quantum spiking neuron that relies on the\ndynamical evolution of two easy to implement Hamiltonians and subsequent local\nmeasurements. The architecture allows exploiting complex amplitudes and\nback-action from measurements to influence the input. This approach to learning\nprotocols is advantageous in the case where the input and output of the system\nare both quantum states. We demonstrate this through the classification of Bell\npairs which can be seen as a certification protocol. Stacking the introduced\nelementary building blocks into larger networks combines the spatiotemporal\nfeatures of a spiking neural network with the non-local quantum correlations\nacross the graph.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 19:35:49 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 23:08:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kristensen", "Lasse Bj\u00f8rn", ""], ["Degroote", "Matthias", ""], ["Wittek", "Peter", ""], ["Aspuru-Guzik", "Al\u00e1n", ""], ["Zinner", "Nikolaj T.", ""]]}, {"id": "1907.06341", "submitter": "Shota Saito", "authors": "Shota Saito, Shinichi Shirakawa", "title": "Controlling Model Complexity in Probabilistic Model-Based Dynamic\n  Optimization of Neural Network Structures", "comments": "Accepted as a conference paper at the 28th International Conference\n  on Artificial Neural Networks (ICANN 2019). The final authenticated\n  publication will be available in the Springer Lecture Notes in Computer\n  Science (LNCS). 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method of simultaneously optimizing both the structure of neural networks\nand the connection weights in a single training loop can reduce the enormous\ncomputational cost of neural architecture search. We focus on the probabilistic\nmodel-based dynamic neural network structure optimization that considers the\nprobability distribution of structure parameters and simultaneously optimizes\nboth the distribution parameters and connection weights based on gradient\nmethods. Since the existing algorithm searches for the structures that only\nminimize the training loss, this method might find overly complicated\nstructures. In this paper, we propose the introduction of a penalty term to\ncontrol the model complexity of obtained structures. We formulate a penalty\nterm using the number of weights or units and derive its analytical natural\ngradient. The proposed method minimizes the objective function injected the\npenalty term based on the stochastic gradient descent. We apply the proposed\nmethod in the unit selection of a fully-connected neural network and the\nconnection selection of a convolutional neural network. The experimental\nresults show that the proposed method can control model complexity while\nmaintaining performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 06:28:40 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Saito", "Shota", ""], ["Shirakawa", "Shinichi", ""]]}, {"id": "1907.06511", "submitter": "Xingyou Song", "authors": "Xingyou Song, Krzysztof Choromanski, Jack Parker-Holder, Yunhao Tang,\n  Wenbo Gao, Aldo Pacchiano, Tamas Sarlos, Deepali Jain, Yuxiang Yang", "title": "Reinforcement Learning with Chromatic Networks for Compact Architecture\n  Search", "comments": "Published at ICLR 2020 Neural Architecture Search Workshop. This\n  paper is deprecated; please see arXiv:2101.07415 for the newer version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural architecture search algorithm to construct compact\nreinforcement learning (RL) policies, by combining ENAS and ES in a highly\nscalable and intuitive way. By defining the combinatorial search space of NAS\nto be the set of different edge-partitionings (colorings) into same-weight\nclasses, we represent compact architectures via efficient learned\nedge-partitionings. For several RL tasks, we manage to learn colorings\ntranslating to effective policies parameterized by as few as $17$ weight\nparameters, providing >90% compression over vanilla policies and 6x compression\nover state-of-the-art compact policies based on Toeplitz matrices, while still\nmaintaining good reward. We believe that our work is one of the first attempts\nto propose a rigorous approach to training structured neural network\narchitectures for RL problems that are of interest especially in mobile\nrobotics with limited storage and computational resources.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 16:57:50 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 16:04:11 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 22:37:04 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 17:00:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Song", "Xingyou", ""], ["Choromanski", "Krzysztof", ""], ["Parker-Holder", "Jack", ""], ["Tang", "Yunhao", ""], ["Gao", "Wenbo", ""], ["Pacchiano", "Aldo", ""], ["Sarlos", "Tamas", ""], ["Jain", "Deepali", ""], ["Yang", "Yuxiang", ""]]}, {"id": "1907.06732", "submitter": "Alejandro Molina", "authors": "Alejandro Molina, Patrick Schramowski, Kristian Kersting", "title": "Pad\\'e Activation Units: End-to-end Learning of Flexible Activation\n  Functions in Deep Networks", "comments": "17 Pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep network learning strongly depends on the choice of\nthe non-linear activation function associated with each neuron. However,\ndeciding on the best activation is non-trivial, and the choice depends on the\narchitecture, hyper-parameters, and even on the dataset. Typically these\nactivations are fixed by hand before training. Here, we demonstrate how to\neliminate the reliance on first picking fixed activation functions by using\nflexible parametric rational functions instead. The resulting Pad\\'e Activation\nUnits (PAUs) can both approximate common activation functions and also learn\nnew ones while providing compact representations. Our empirical evidence shows\nthat end-to-end learning deep networks with PAUs can increase the predictive\nperformance. Moreover, PAUs pave the way to approximations with provable\nrobustness. https://github.com/ml-research/pau\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:24:22 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 10:05:39 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 11:25:30 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Molina", "Alejandro", ""], ["Schramowski", "Patrick", ""], ["Kersting", "Kristian", ""]]}, {"id": "1907.06902", "submitter": "Maurizio Ferrari Dacrema", "authors": "Maurizio Ferrari Dacrema, Paolo Cremonesi and Dietmar Jannach", "title": "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural\n  Recommendation Approaches", "comments": "Source code available at:\n  https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation", "journal-ref": "Proceedings of the 13th ACM Conference on Recommender Systems\n  (RecSys 2019)", "doi": "10.1145/3298689.3347058", "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have become the method of choice for researchers\nworking on algorithmic aspects of recommender systems. With the strongly\nincreased interest in machine learning in general, it has, as a result, become\ndifficult to keep track of what represents the state-of-the-art at the moment,\ne.g., for top-n recommendation tasks. At the same time, several recent\npublications point out problems in today's research practice in applied machine\nlearning, e.g., in terms of the reproducibility of the results or the choice of\nthe baselines when proposing new models. In this work, we report the results of\na systematic analysis of algorithmic proposals for top-n recommendation tasks.\nSpecifically, we considered 18 algorithms that were presented at top-level\nresearch conferences in the last years. Only 7 of them could be reproduced with\nreasonable effort. For these methods, it however turned out that 6 of them can\noften be outperformed with comparably simple heuristic methods, e.g., based on\nnearest-neighbor or graph-based techniques. The remaining one clearly\noutperformed the baselines but did not consistently outperform a well-tuned\nnon-neural linear ranking method. Overall, our work sheds light on a number of\npotential problems in today's machine learning scholarship and calls for\nimproved scientific practices in this area. Source code of our experiments and\nfull results are available at:\nhttps://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:11:07 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 09:44:36 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 18:20:03 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""], ["Jannach", "Dietmar", ""]]}, {"id": "1907.06912", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Alexander Asteroth, Thomas B\\\"ack", "title": "Modeling User Selection in Quality Diversity", "comments": null, "journal-ref": null, "doi": "10.1145/3321707.3321823", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The initial phase in real world engineering optimization and design is a\nprocess of discovery in which not all requirements can be made in advance, or\nare hard to formalize. Quality diversity algorithms, which produce a variety of\nhigh performing solutions, provide a unique chance to support engineers and\ndesigners in the search for what is possible and high performing. In this work\nwe begin to answer the question how a user can interact with quality diversity\nand turn it into an interactive innovation aid. By modeling a user's selection\nit can be determined whether the optimization is drifting away from the user's\npreferences. The optimization is then constrained by adding a penalty to the\nobjective function. We present an interactive quality diversity algorithm that\ncan take into account the user's selection. The approach is evaluated in a new\nmultimodal optimization benchmark that allows various optimization tasks to be\nperformed. The user selection drift of the approach is compared to a state of\nthe art alternative on both a planning and a neuroevolution control task,\nthereby showing its limits and possibilities.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:39:55 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hagg", "Alexander", ""], ["Asteroth", "Alexander", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1907.06916", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell, Hesham Mostafa, Runchun Wang and Andre van Schaik", "title": "Single-bit-per-weight deep convolutional neural networks without\n  batch-normalization layers for embedded systems", "comments": "8 pages, published IEEE conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch-normalization (BN) layers are thought to be an integrally important\nlayer type in today's state-of-the-art deep convolutional neural networks for\ncomputer vision tasks such as classification and detection. However, BN layers\nintroduce complexity and computational overheads that are highly undesirable\nfor training and/or inference on low-power custom hardware implementations of\nreal-time embedded vision systems such as UAVs, robots and Internet of Things\n(IoT) devices. They are also problematic when batch sizes need to be very small\nduring training, and innovations such as residual connections introduced more\nrecently than BN layers could potentially have lessened their impact. In this\npaper we aim to quantify the benefits BN layers offer in image classification\nnetworks, in comparison with alternative choices. In particular, we study\nnetworks that use shifted-ReLU layers instead of BN layers. We found, following\nexperiments with wide residual networks applied to the ImageNet, CIFAR 10 and\nCIFAR 100 image classification datasets, that BN layers do not consistently\noffer a significant advantage. We found that the accuracy margin offered by BN\nlayers depends on the data set, the network size, and the bit-depth of weights.\nWe conclude that in situations where BN layers are undesirable due to speed,\nmemory or complexity costs, that using shifted-ReLU layers instead should be\nconsidered; we found they can offer advantages in all these areas, and often do\nnot impose a significant accuracy cost.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:42:02 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:04:27 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Mostafa", "Hesham", ""], ["Wang", "Runchun", ""], ["van Schaik", "Andre", ""]]}, {"id": "1907.06996", "submitter": "Alberto Testolin Dr.", "authors": "Alberto Testolin, Serena Dolfi, Mathijs Rochus and Marco Zorzi", "title": "Perception of visual numerosity in humans and machines", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerosity perception is foundational to mathematical learning, but its\ncomputational bases are strongly debated. Some investigators argue that humans\nare endowed with a specialized system supporting numerical representation;\nothers argue that visual numerosity is estimated using continuous magnitudes,\nsuch as density or area, which usually co-vary with number. Here we reconcile\nthese contrasting perspectives by testing deep networks on the same numerosity\ncomparison task that was administered to humans, using a stimulus space that\nallows to measure the contribution of non-numerical features. Our model\naccurately simulated the psychophysics of numerosity perception and the\nassociated developmental changes: discrimination was driven by numerosity\ninformation, but non-numerical features had a significant impact, especially\nearly during development. Representational similarity analysis further\nhighlighted that both numerosity and continuous magnitudes were spontaneously\nencoded even when no task had to be carried out, demonstrating that numerosity\nis a major, salient property of our visual environment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:45:18 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Testolin", "Alberto", ""], ["Dolfi", "Serena", ""], ["Rochus", "Mathijs", ""], ["Zorzi", "Marco", ""]]}, {"id": "1907.07029", "submitter": "Rituraj Kaushik", "authors": "Rituraj Kaushik, Pierre Desreumaux, Jean-Baptiste Mouret", "title": "Adaptive Prior Selection for Repertoire-based Online Adaptation in\n  Robotics", "comments": "Frontiers in Robotics and AI. Vol. 6, p. 151, 2020. Video :\n  http://tiny.cc/aprol_video", "journal-ref": "Frontiers in Robotics and AI. 6 (2020) 151", "doi": "10.3389/frobt.2019.00151", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repertoire-based learning is a data-efficient adaptation approach based on a\ntwo-step process in which (1) a large and diverse set of policies is learned in\nsimulation, and (2) a planning or learning algorithm chooses the most\nappropriate policies according to the current situation (e.g., a damaged robot,\na new object, etc.). In this paper, we relax the assumption of previous works\nthat a single repertoire is enough for adaptation. Instead, we generate\nrepertoires for many different situations (e.g., with a missing leg, on\ndifferent floors, etc.) and let our algorithm selects the most useful prior.\nOur main contribution is an algorithm, APROL (Adaptive Prior selection for\nRepertoire-based Online Learning) to plan the next action by incorporating\nthese priors when the robot has no information about the current situation. We\nevaluate APROL on two simulated tasks: (1) pushing unknown objects of various\nshapes and sizes with a robotic arm and (2) a goal reaching task with a damaged\nhexapod robot. We compare with \"Reset-free Trial and Error\" (RTE) and various\nsingle repertoire-based baselines. The results show that APROL solves both the\ntasks in less interaction time than the baselines. Additionally, we demonstrate\nAPROL on a real, damaged hexapod that quickly learns to pick compensatory\npolicies to reach a goal by avoiding obstacles in the path.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:26:13 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 11:13:57 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 22:25:25 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kaushik", "Rituraj", ""], ["Desreumaux", "Pierre", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1907.07066", "submitter": "Mario Graff", "authors": "Claudia N. S\\'anchez and Mario Graff", "title": "Selection Heuristics on Semantic Genetic Programming for Classification\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual's semantics have been used for guiding the learning process of\nGenetic Programming solving supervised learning problems. The semantics has\nbeen used to proposed novel genetic operators as well as different ways of\nperforming parent selection. The latter is the focus of this contribution by\nproposing three heuristics for parent selection that replace the fitness\nfunction on the selection mechanism entirely. These heuristics complement\nprevious work by being inspired in the characteristics of the addition, Naive\nBayes, and Nearest Centroid functions and applying them only when the function\nis used to create an offspring. These heuristics use different similarity\nmeasures among the parents to decide which of them is more appropriate given a\nfunction. The similarity functions considered are the cosine similarity,\nPearson's correlation, and agreement. We analyze these heuristics' performance\nagainst random selection, state-of-the-art selection schemes, and 18\nclassifiers, including auto-machine-learning techniques, on 30 classification\nproblems with a variable number of samples, variables, and classes. The result\nindicated that the combination of parent selection based on agreement and\nrandom selection to replace an individual in the population produces\nstatistically better results than the classical selection and state-of-the-art\nschemes, and it is competitive with state-of-the-art classifiers. Finally, the\ncode is released as open-source software.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:25:01 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 15:12:15 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 03:10:59 GMT"}, {"version": "v4", "created": "Fri, 2 Apr 2021 18:48:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["S\u00e1nchez", "Claudia N.", ""], ["Graff", "Mario", ""]]}, {"id": "1907.07075", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Martin Zaefferer, J\\\"org Stork, Adam Gaier", "title": "Prediction of neural network performance by phenotypic modeling", "comments": null, "journal-ref": null, "doi": "10.1145/3319619.3326815", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate models are used to reduce the burden of expensive-to-evaluate\nobjective functions in optimization. By creating models which map genomes to\nobjective values, these models can estimate the performance of unknown inputs,\nand so be used in place of expensive objective functions. Evolutionary\ntechniques such as genetic programming or neuroevolution commonly alter the\nstructure of the genome itself. A lack of consistency in the genotype is a\nfatal blow to data-driven modeling techniques: interpolation between points is\nimpossible without a common input space. However, while the dimensionality of\ngenotypes may differ across individuals, in many domains, such as controllers\nor classifiers, the dimensionality of the input and output remains constant. In\nthis work we leverage this insight to embed differing neural networks into the\nsame input space. To judge the difference between the behavior of two neural\nnetworks, we give them both the same input sequence, and examine the difference\nin output. This difference, the phenotypic distance, can then be used to\nsituate these networks into a common input space, allowing us to produce\nsurrogate models which can predict the performance of neural networks\nregardless of topology. In a robotic navigation task, we show that models\ntrained using this phenotypic embedding perform as well or better as those\ntrained on the weight values of a fixed topology neural network. We establish\nsuch phenotypic surrogate models as a promising and flexible approach which\nenables surrogate modeling even for representations that undergo structural\nchanges.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:37:51 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hagg", "Alexander", ""], ["Zaefferer", "Martin", ""], ["Stork", "J\u00f6rg", ""], ["Gaier", "Adam", ""]]}, {"id": "1907.07229", "submitter": "Vojtech Mrazek", "authors": "Vojtech Mrazek and Zdenek Vasicek and Lukas Sekanina and Muhammad\n  Abdullah Hanif and Muhammad Shafique", "title": "ALWANN: Automatic Layer-Wise Approximation of Deep Neural Network\n  Accelerators without Retraining", "comments": "Accepted for 2019 IEEE/ACM International Conference On Computer-Aided\n  Design (ICCAD'19)", "journal-ref": null, "doi": "10.1109/ICCAD45719.2019.8942068", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art approaches employ approximate computing to reduce the\nenergy consumption of DNN hardware. Approximate DNNs then require extensive\nretraining afterwards to recover from the accuracy loss caused by the use of\napproximate operations. However, retraining of complex DNNs does not scale\nwell. In this paper, we demonstrate that efficient approximations can be\nintroduced into the computational path of DNN accelerators while retraining can\ncompletely be avoided. ALWANN provides highly optimized implementations of DNNs\nfor custom low-power accelerators in which the number of computing units is\nlower than the number of DNN layers. First, a fully trained DNN is converted to\noperate with 8-bit weights and 8-bit multipliers in convolutional layers. A\nsuitable approximate multiplier is then selected for each computing element\nfrom a library of approximate multipliers in such a way that (i) one\napproximate multiplier serves several layers, and (ii) the overall\nclassification error and energy consumption are minimized. The optimizations\nincluding the multiplier selection problem are solved by means of a\nmultiobjective optimization NSGA-II algorithm. In order to completely avoid the\ncomputationally expensive retraining of DNN, which is usually employed to\nimprove the classification accuracy, we propose a simple weight updating scheme\nthat compensates the inaccuracy introduced by employing approximate\nmultipliers. The proposed approach is evaluated for two architectures of DNN\naccelerators with approximate multipliers from the open-source \"EvoApprox\"\nlibrary. We report that the proposed approach saves 30% of energy needed for\nmultiplication in convolutional layers of ResNet-50 while the accuracy is\ndegraded by only 0.6%. The proposed technique and approximate layers are\navailable as an open-source extension of TensorFlow at\nhttps://github.com/ehw-fit/tf-approximate.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:36:55 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 08:01:30 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Mrazek", "Vojtech", ""], ["Vasicek", "Zdenek", ""], ["Sekanina", "Lukas", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1907.07254", "submitter": "Mohammad Ibrahim Sarker", "authors": "Mohammad Ibrahim Sarker, Zubaer Ibna Mannan, Hyongsuk Kim", "title": "Optimizing method for Neural Network based on Genetic Random Weight\n  Change Learning Algorithm", "comments": "2 pages, Published in ICROS 2017 conference, South Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random weight change (RWC) algorithm is extremely component and robust for\nthe hardware implementation of neural networks. RWC and Genetic algorithm (GA)\nare well known methodologies used for optimizing and learning the neural\nnetwork (NN). Individually, each of these two algorithms has its strength and\nweakness along with separate objectives. However, recently, researchers combine\nthese two algorithms for better learning and optimization of NN. In this paper,\nwe proposed a methodology by combining the RWC and GA, namely Genetic Random\nWeight Change (GRWC), as well as demonstrate a seminal way to reduce the\ncomplexity of the neural network by removing weak weights of GRWC. In contrast\nto RWC and GA, GRWC contains an effective optimization procedure which is\nworthy at exploring a large and complex space in intellectual strategies\ninfluenced by the GA/RWC synergy. The learning behavior of the proposed\nalgorithm was tested on MNIST dataset and it was able to prove its performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:12:22 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Sarker", "Mohammad Ibrahim", ""], ["Mannan", "Zubaer Ibna", ""], ["Kim", "Hyongsuk", ""]]}, {"id": "1907.07255", "submitter": "Aras Dargazany", "authors": "Aras R. Dargazany", "title": "Iterative temporal differencing with random synaptic feedback weights\n  support error backpropagation for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows that a differentiable activation function is not necessary\nany more for error backpropagation. The derivative of the activation function\ncan be replaced by an iterative temporal differencing using fixed random\nfeedback alignment. Using fixed random synaptic feedback alignment with an\niterative temporal differencing is transforming the traditional error\nbackpropagation into a more biologically plausible approach for learning deep\nneural network architectures. This can be a big step toward the integration of\nSTDP-based error backpropagation in deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 06:00:41 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Dargazany", "Aras R.", ""]]}, {"id": "1907.07325", "submitter": "Thomas Ferreira De Lima", "authors": "Thomas Ferreira de Lima, Alexander N. Tait, Hooman Saeidi, Mitchell A.\n  Nahmias, Hsuan-Tung Peng, Siamak Abbaslou, Bhavin J. Shastri, and Paul R.\n  Prucnal", "title": "Noise Analysis of Photonic Modulator Neurons", "comments": "8 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.1109/JSTQE.2019.2931252", "report-no": null, "categories": "physics.app-ph cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic photonics relies on efficiently emulating analog neural networks\nat high speeds. Prior work showed that transducing signals from the optical to\nthe electrical domain and back with transimpedance gain was an efficient\napproach to implementing analog photonic neurons and scalable networks. Here,\nwe examine modulator-based photonic neuron circuits with passive and active\ntransimpedance gains, with special attention to the sources of noise\npropagation. We find that a modulator nonlinear transfer function can suppress\nnoise, which is necessary to avoid noise propagation in hardware neural\nnetworks. In addition, while efficient modulators can reduce power for an\nindividual neuron, signal-to-noise ratios must be traded off with power\nconsumption at a system level. Active transimpedance amplifiers may help relax\nthis tradeoff for conventional p-n junction silicon photonic modulators, but a\npassive transimpedance circuit is sufficient when very efficient modulators\n(i.e. low C and low V-pi) are employed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 04:19:14 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["de Lima", "Thomas Ferreira", ""], ["Tait", "Alexander N.", ""], ["Saeidi", "Hooman", ""], ["Nahmias", "Mitchell A.", ""], ["Peng", "Hsuan-Tung", ""], ["Abbaslou", "Siamak", ""], ["Shastri", "Bhavin J.", ""], ["Prucnal", "Paul R.", ""]]}, {"id": "1907.07381", "submitter": "Won-Yong Shin", "authors": "Cong Tran, Won-Yong Shin, Andreas Spitz, Michael Gertz", "title": "DeepNC: Deep Generative Network Completion", "comments": "16 pages, 10 figures, 5 tables; to appear in the IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (Please cite our journal version\n  that will appear in an upcoming issue.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most network data are collected from partially observable networks with both\nmissing nodes and missing edges, for example, due to limited resources and\nprivacy settings specified by users on social media. Thus, it stands to reason\nthat inferring the missing parts of the networks by performing network\ncompletion should precede downstream applications. However, despite this need,\nthe recovery of missing nodes and edges in such incomplete networks is an\ninsufficiently explored problem due to the modeling difficulty, which is much\nmore challenging than link prediction that only infers missing edges. In this\npaper, we present DeepNC, a novel method for inferring the missing parts of a\nnetwork based on a deep generative model of graphs. Specifically, our method\nfirst learns a likelihood over edges via an autoregressive generative model,\nand then identifies the graph that maximizes the learned likelihood conditioned\non the observable graph topology. Moreover, we propose a computationally\nefficient DeepNC algorithm that consecutively finds individual nodes that\nmaximize the probability in each node generation step, as well as an enhanced\nversion using the expectation-maximization algorithm. The runtime complexities\nof both algorithms are shown to be almost linear in the number of nodes in the\nnetwork. We empirically demonstrate the superiority of DeepNC over\nstate-of-the-art network completion approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 08:25:20 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:18:09 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 01:26:05 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 02:26:49 GMT"}, {"version": "v5", "created": "Tue, 20 Oct 2020 08:58:24 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Tran", "Cong", ""], ["Shin", "Won-Yong", ""], ["Spitz", "Andreas", ""], ["Gertz", "Michael", ""]]}, {"id": "1907.07568", "submitter": "Paulo Roberto de Oliveira da Costa", "authors": "Dylan Rijnen, Jason Rhuggenaath, Paulo R. de O. da Costa and Yingqian\n  Zhang", "title": "Machine Learning based Simulation Optimisation for Trailer Management", "comments": "Submitted to IEEE SMC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations, simulation models are developed to handle complex\nreal-world business optimisation problems. For example, a discrete-event\nsimulation model is used to simulate the trailer management process in a big\nFast-Moving Consumer Goods company. To address the problem of finding suitable\ninputs to this simulator for optimising fleet configuration, we propose a\nsimulation optimisation approach in this paper. The simulation optimisation\nmodel combines a metaheuristic search (genetic algorithm), with an\napproximation model filter (feed-forward neural network) to optimise the\nparameter configuration of the simulation model. We introduce an ensure\nprobability that overrules the rejection of potential solutions by the\napproximation model and we demonstrate its effectiveness. In addition, we\nevaluate the impact of the parameters of the optimisation model on its\neffectiveness and show the parameters such as population size, filter\nthreshold, and mutation probability can have a significant impact on the\noverall optimisation performance. Moreover, we compare the proposed method with\na single global approximation model approach and a random-based approach. The\nresults show the effectiveness of our method in terms of computation time and\nsolution quality.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:09:02 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Rijnen", "Dylan", ""], ["Rhuggenaath", "Jason", ""], ["da Costa", "Paulo R. de O.", ""], ["Zhang", "Yingqian", ""]]}, {"id": "1907.07640", "submitter": "Emin Orhan", "authors": "A. Emin Orhan", "title": "Robustness properties of Facebook's ResNeXt WSL models", "comments": "10 pages, 4 figures, 4 tables; v5 corrects the ImageNet-A results and\n  revises the discussion accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the robustness properties of ResNeXt class image recognition\nmodels trained with billion scale weakly supervised data (ResNeXt WSL models).\nThese models, recently made public by Facebook AI, were trained with ~1B images\nfrom Instagram and fine-tuned on ImageNet. We show that these models display an\nunprecedented degree of robustness against common image corruptions and\nperturbations, as measured by the ImageNet-C and ImageNet-P benchmarks. They\nalso achieve substantially improved accuracies on the recently introduced\n\"natural adversarial examples\" benchmark (ImageNet-A). The largest of the\nreleased models, in particular, achieves state-of-the-art results on\nImageNet-C, ImageNet-P, and ImageNet-A by a large margin. The gains on\nImageNet-C, ImageNet-P, and ImageNet-A far outpace the gains on ImageNet\nvalidation accuracy, suggesting the former as more useful benchmarks to measure\nfurther progress in image recognition. Remarkably, the ResNeXt WSL models even\nachieve a limited degree of adversarial robustness against state-of-the-art\nwhite-box attacks (10-step PGD attacks). However, in contrast to adversarially\ntrained models, the robustness of the ResNeXt WSL models rapidly declines with\nthe number of PGD steps, suggesting that these models do not achieve genuine\nadversarial robustness. Visualization of the learned features also confirms\nthis conclusion. Finally, we show that although the ResNeXt WSL models are more\nshape-biased than comparable ImageNet-trained models in a shape-texture cue\nconflict experiment, they still remain much more texture-biased than humans,\nsuggesting that they share some of the underlying characteristics of\nImageNet-trained models that make this benchmark challenging.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 17:03:52 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 17:59:19 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 15:52:53 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 16:30:13 GMT"}, {"version": "v5", "created": "Mon, 9 Dec 2019 16:28:47 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Orhan", "A. Emin", ""]]}, {"id": "1907.07746", "submitter": "Robin Tibor Schirrmeister", "authors": "Robin Tibor Schirrmeister, Tonio Ball", "title": "Deep Invertible Networks for EEG-based brain-signal decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we investigate deep invertible networks for EEG-based\nbrain signal decoding and find them to generate realistic EEG signals as well\nas classify novel signals above chance. Further ideas for their regularization\ntowards better decoding accuracies are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 20:26:21 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Schirrmeister", "Robin Tibor", ""], ["Ball", "Tonio", ""]]}, {"id": "1907.07804", "submitter": "Subhojeet Pramanik", "authors": "Subhojeet Pramanik, Priyanka Agrawal, Aman Hussain", "title": "OmniNet: A unified architecture for multi-modal multi-task learning", "comments": "Source code available at: https://github.com/subho406/OmniNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is a popularly used neural network architecture, especially for\nlanguage understanding. We introduce an extended and unified architecture that\ncan be used for tasks involving a variety of modalities like image, text,\nvideos, etc. We propose a spatio-temporal cache mechanism that enables learning\nspatial dimension of the input in addition to the hidden states corresponding\nto the temporal input sequence. The proposed architecture further enables a\nsingle model to support tasks with multiple input modalities as well as\nasynchronous multi-task learning, thus we refer to it as OmniNet. For example,\na single instance of OmniNet can concurrently learn to perform the tasks of\npart-of-speech tagging, image captioning, visual question answering and video\nactivity recognition. We demonstrate that training these four tasks together\nresults in about three times compressed model while retaining the performance\nin comparison to training them individually. We also show that using this\nneural network pre-trained on some modalities assists in learning unseen tasks\nsuch as video captioning and video question answering. This illustrates the\ngeneralization capacity of the self-attention mechanism on the spatio-temporal\ncache present in OmniNet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:59:56 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 09:59:06 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Pramanik", "Subhojeet", ""], ["Agrawal", "Priyanka", ""], ["Hussain", "Aman", ""]]}, {"id": "1907.07853", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Ying Xu, Jonathan Tapson, Andr\\'e van Schaik, Gregory\n  Cohen", "title": "Event-based Feature Extraction Using Adaptive Selection Thresholds", "comments": "15 Pages. 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised feature extraction algorithms form one of the most important\nbuilding blocks in machine learning systems. These algorithms are often adapted\nto the event-based domain to perform online learning in neuromorphic hardware.\nHowever, not designed for the purpose, such algorithms typically require\nsignificant simplification during implementation to meet hardware constraints,\ncreating trade offs with performance. Furthermore, conventional feature\nextraction algorithms are not designed to generate useful intermediary signals\nwhich are valuable only in the context of neuromorphic hardware limitations. In\nthis work a novel event-based feature extraction method is proposed that\nfocuses on these issues. The algorithm operates via simple adaptive selection\nthresholds which allow a simpler implementation of network homeostasis than\nprevious works by trading off a small amount of information loss in the form of\nmissed events that fall outside the selection thresholds. The behavior of the\nselection thresholds and the output of the network as a whole are shown to\nprovide uniquely useful signals indicating network weight convergence without\nthe need to access network weights. A novel heuristic method for network size\nselection is proposed which makes use of noise events and their feature\nrepresentations. The use of selection thresholds is shown to produce network\nactivation patterns that predict classification accuracy allowing rapid\nevaluation and optimization of system parameters without the need to run\nback-end classifiers. The feature extraction method is tested on both the\nN-MNIST benchmarking dataset and a dataset of airplanes passing through the\nfield of view. Multiple configurations with different classifiers are tested\nwith the results quantifying the resultant performance gains at each processing\nstage.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 03:15:09 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 04:41:30 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Afshar", "Saeed", ""], ["Xu", "Ying", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""], ["Cohen", "Gregory", ""]]}, {"id": "1907.07897", "submitter": "Karlis Freivalds", "authors": "K\\=arlis Freivalds, Em\\=ils Ozoli\\c{n}\\v{s}, Agris \\v{S}ostaks", "title": "Neural Shuffle-Exchange Networks -- Sequence Processing in O(n log n)\n  Time", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement in sequence to sequence processing is the modeling of long\nrange dependencies. To this end, a vast majority of the state-of-the-art models\nuse attention mechanism which is of O($n^2$) complexity that leads to slow\nexecution for long sequences. We introduce a new Shuffle-Exchange neural\nnetwork model for sequence to sequence tasks which have O(log n) depth and O(n\nlog n) total complexity. We show that this model is powerful enough to infer\nefficient algorithms for common algorithmic benchmarks including sorting,\naddition and multiplication. We evaluate our architecture on the challenging\nLAMBADA question answering dataset and compare it with the state-of-the-art\nmodels which use attention. Our model achieves competitive accuracy and scales\nto sequences with more than a hundred thousand of elements. We are confident\nthat the proposed model has the potential for building more efficient\narchitectures for processing large interrelated data in language modeling,\nmusic generation and other application domains.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 06:47:48 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 10:38:03 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 10:02:27 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Freivalds", "K\u0101rlis", ""], ["Ozoli\u0146\u0161", "Em\u012bls", ""], ["\u0160ostaks", "Agris", ""]]}, {"id": "1907.08040", "submitter": "Hanten Chang", "authors": "Hanten Chang and Katsuya Futagami", "title": "Convolutional Reservoir Computing for World Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, reinforcement learning models have achieved great success,\ncompleting complex tasks such as mastering Go and other games with higher\nscores than human players. Many of these models collect considerable data on\nthe tasks and improve accuracy by extracting visual and time-series features\nusing convolutional neural networks (CNNs) and recurrent neural networks,\nrespectively. However, these networks have very high computational costs\nbecause they need to be trained by repeatedly using a large volume of past\nplaying data. In this study, we propose a novel practical approach called\nreinforcement learning with convolutional reservoir computing (RCRC) model. The\nRCRC model has several desirable features: 1. it can extract visual and\ntime-series features very fast because it uses random fixed-weight CNN and the\nreservoir computing model; 2. it does not require the training data to be\nstored because it extracts features without training and decides action with\nevolution strategy. Furthermore, the model achieves state of the art score in\nthe popular reinforcement learning task. Incredibly, we find the random\nweight-fixed simple networks like only one dense layer network can also reach\nhigh score in the RL task.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 13:16:39 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Chang", "Hanten", ""], ["Futagami", "Katsuya", ""]]}, {"id": "1907.08220", "submitter": "Mojtaba Moattari", "authors": "Mojtaba Moattari, Mohammad Hassan Moradi, Reza Boostani", "title": "Modified swarm-based metaheuristics enhance Gradient Descent\n  initialization performance: Application for EEG spatial filtering", "comments": "10 tables, 32 references, 11 formulas. arXiv admin note: text overlap\n  with arXiv:1209.4115 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient Descent (GD) approximators often fail in the solution space with\nmultiple scales of convexities, i.e., in subspace learning and neural network\nscenarios. To handle that, one solution is to run GD multiple times from\ndifferent randomized initial states and select the best solution over all\nexperiments. However, this idea is proved impractical in plenty of cases. Even\nSwarm-based optimizers like Particle Swarm Optimization (PSO) or Imperialistic\nCompetitive Algorithm (ICA), as commonly used GD initializers, have failed to\nfind optimal solutions in some applications. In this paper, Swarm-based\noptimizers like ICA and PSO are modified by a new optimization framework to\nimprove GD optimization performance. This improvement is for applications with\nhigh number of convex localities in multiple scales. Performance of the\nproposed method is analyzed in a nonlinear subspace filtering objective\nfunction over EEG data. The proposed metaheuristic outperforms commonly used\nbaseline optimizers as GD initializers in both the EEG classification accuracy\nand EEG loss function fitness. The optimizers have been also compared to each\nother in some of CEC 2014 benchmark functions, where again our method\noutperforms other algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 07:50:16 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 16:44:02 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Moattari", "Mojtaba", ""], ["Moradi", "Mohammad Hassan", ""], ["Boostani", "Reza", ""]]}, {"id": "1907.08307", "submitter": "Martin Wistuba", "authors": "Martin Wistuba", "title": "XferNAS: Transfer Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term Neural Architecture Search (NAS) refers to the automatic\noptimization of network architectures for a new, previously unknown task. Since\ntesting an architecture is computationally very expensive, many optimizers need\ndays or even weeks to find suitable architectures. However, this search time\ncan be significantly reduced if knowledge from previous searches on different\ntasks is reused. In this work, we propose a generally applicable framework that\nintroduces only minor changes to existing optimizers to leverage this feature.\nAs an example, we select an existing optimizer and demonstrate the complexity\nof the integration of the framework as well as its impact. In experiments on\nCIFAR-10 and CIFAR-100, we observe a reduction in the search time from 200 to\nonly 6 GPU days, a speed up by a factor of 33. In addition, we observe new\nrecords of 1.99 and 14.06 for NAS optimizers on the CIFAR benchmarks,\nrespectively. In a separate study, we analyze the impact of the amount of\nsource and target data. Empirically, we demonstrate that the proposed framework\ngenerally gives better results and, in the worst case, is just as good as the\nunmodified optimizer.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 22:05:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wistuba", "Martin", ""]]}, {"id": "1907.08325", "submitter": "Shusen Liu", "authors": "Shusen Liu, Di Wang, Dan Maljovec, Rushil Anirudh, Jayaraman J.\n  Thiagarajan, Sam Ade Jacobs, Brian C. Van Essen, David Hysom, Jae-Seung Yeom,\n  Jim Gaffney, Luc Peterson, Peter B. Robinson, Harsh Bhatia, Valerio Pascucci,\n  Brian K. Spears and Peer-Timo Bremer", "title": "Scalable Topological Data Analysis and Visualization for Evaluating\n  Data-Driven Models in Scientific Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid adoption of machine learning techniques for large-scale\napplications in science and engineering comes the convergence of two grand\nchallenges in visualization. First, the utilization of black box models (e.g.,\ndeep neural networks) calls for advanced techniques in exploring and\ninterpreting model behaviors. Second, the rapid growth in computing has\nproduced enormous datasets that require techniques that can handle millions or\nmore samples. Although some solutions to these interpretability challenges have\nbeen proposed, they typically do not scale beyond thousands of samples, nor do\nthey provide the high-level intuition scientists are looking for. Here, we\npresent the first scalable solution to explore and analyze high-dimensional\nfunctions often encountered in the scientific data analysis pipeline. By\ncombining a new streaming neighborhood graph construction, the corresponding\ntopology computation, and a novel data aggregation scheme, namely topology\naware datacubes, we enable interactive exploration of both the topological and\nthe geometric aspect of high-dimensional data. Following two use cases from\nhigh-energy-density (HED) physics and computational biology, we demonstrate how\nthese capabilities have led to crucial new insights in both applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 00:37:39 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Liu", "Shusen", ""], ["Wang", "Di", ""], ["Maljovec", "Dan", ""], ["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Jacobs", "Sam Ade", ""], ["Van Essen", "Brian C.", ""], ["Hysom", "David", ""], ["Yeom", "Jae-Seung", ""], ["Gaffney", "Jim", ""], ["Peterson", "Luc", ""], ["Robinson", "Peter B.", ""], ["Bhatia", "Harsh", ""], ["Pascucci", "Valerio", ""], ["Spears", "Brian K.", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1907.08544", "submitter": "Enzo Tartaglione", "authors": "Enzo Tartaglione, Daniele Perlo and Marco Grangetto", "title": "Post-synaptic potential regularization has potential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving generalization is one of the main challenges for training deep\nneural networks on classification tasks. In particular, a number of techniques\nhave been proposed, aiming to boost the performance on unseen data: from\nstandard data augmentation techniques to the $\\ell_2$ regularization, dropout,\nbatch normalization, entropy-driven SGD and many more.\\\\ In this work we\npropose an elegant, simple and principled approach: post-synaptic potential\nregularization (PSP). We tested this regularization on a number of different\nstate-of-the-art scenarios. Empirical results show that PSP achieves a\nclassification error comparable to more sophisticated learning strategies in\nthe MNIST scenario, while improves the generalization compared to $\\ell_2$\nregularization in deep architectures trained on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 15:25:21 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Tartaglione", "Enzo", ""], ["Perlo", "Daniele", ""], ["Grangetto", "Marco", ""]]}, {"id": "1907.08549", "submitter": "Niru Maheswaranathan", "authors": "Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya\n  Ganguli, David Sussillo", "title": "Universality and individuality in neural dynamics across large\n  populations of recurrent networks", "comments": "Presented at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-based modeling with recurrent neural networks (RNNs) has emerged as a\npopular way to infer the computational function of different brain regions.\nThese models are quantitatively assessed by comparing the low-dimensional\nneural representations of the model with the brain, for example using canonical\ncorrelation analysis (CCA). However, the nature of the detailed neurobiological\ninferences one can draw from such efforts remains elusive. For example, to what\nextent does training neural networks to solve common tasks uniquely determine\nthe network dynamics, independent of modeling architectural choices? Or\nalternatively, are the learned dynamics highly sensitive to different model\nchoices? Knowing the answer to these questions has strong implications for\nwhether and how we should use task-based RNN modeling to understand brain\ndynamics. To address these foundational questions, we study populations of\nthousands of networks, with commonly used RNN architectures, trained to solve\nneuroscientifically motivated tasks and characterize their nonlinear dynamics.\nWe find the geometry of the RNN representations can be highly sensitive to\ndifferent network architectures, yielding a cautionary tale for measures of\nsimilarity that rely representational geometry, such as CCA. Moreover, we find\nthat while the geometry of neural dynamics can vary greatly across\narchitectures, the underlying computational scaffold---the topological\nstructure of fixed points, transitions between them, limit cycles, and\nlinearized dynamics---often appears universal across all architectures.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 15:35:38 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 20:43:41 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Maheswaranathan", "Niru", ""], ["Williams", "Alex H.", ""], ["Golub", "Matthew D.", ""], ["Ganguli", "Surya", ""], ["Sussillo", "David", ""]]}, {"id": "1907.08578", "submitter": "Giovanni Grano", "authors": "Giovanni Grano, Christoph Laaber, Annibale Panichella, and Sebastiano\n  Panichella", "title": "Testing with Fewer Resources: An Adaptive Approach to Performance-Aware\n  Test Case Generation", "comments": "16 pages, 3 figures, accepted for IEEE Transaction on Software\n  Engineering", "journal-ref": null, "doi": "10.1109/TSE.2019.2946773", "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated test case generation is an effective technique to yield\nhigh-coverage test suites. While the majority of research effort has been\ndevoted to satisfying coverage criteria, a recent trend emerged towards\noptimizing other non-coverage aspects. In this regard, runtime and memory usage\nare two essential dimensions: less expensive tests reduce the resource demands\nfor the generation process and later regression testing phases. This study\nshows that performance-aware test case generation requires solving two main\nchallenges: providing a good approximation of resource usage with minimal\noverhead and avoiding detrimental effects on both final coverage and fault\ndetection effectiveness. To tackle these challenges, we conceived a set of\nperformance proxies -- inspired by previous work on performance testing -- that\nprovide a reasonable estimation of the test execution costs (i.e., runtime and\nmemory usage). Thus, we propose an adaptive strategy, called aDynaMOSA, which\nleverages these proxies by extending DynaMOSA, a state-of-the-art evolutionary\nalgorithm in unit testing. Our empirical study -- involving 110 non-trivial\nJava classes -- reveals that our adaptive approach generates test suite with\nstatistically significant improvements in runtime (-25%) and heap memory\nconsumption (-15%) compared to DynaMOSA. Additionally, aDynaMOSA has comparable\nresults to DynaMOSA over seven different coverage criteria and similar fault\ndetection effectiveness. Our empirical investigation also highlights that the\nusage of performance proxies (i.e., without the adaptiveness) is not sufficient\nto generate more performant test cases without compromising the overall\ncoverage.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:07:22 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 11:29:16 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 07:29:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Grano", "Giovanni", ""], ["Laaber", "Christoph", ""], ["Panichella", "Annibale", ""], ["Panichella", "Sebastiano", ""]]}, {"id": "1907.08610", "submitter": "Michael Zhang", "authors": "Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba", "title": "Lookahead Optimizer: k steps forward, 1 step back", "comments": "Accepted to Neural Information Processing Systems 2019. Code\n  available at: https://github.com/michaelrzhang/lookahead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of successful deep neural networks are trained using\nvariants of stochastic gradient descent (SGD) algorithms. Recent attempts to\nimprove SGD can be broadly categorized into two approaches: (1) adaptive\nlearning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes,\nsuch as heavy-ball and Nesterov momentum. In this paper, we propose a new\noptimization algorithm, Lookahead, that is orthogonal to these previous\napproaches and iteratively updates two sets of weights. Intuitively, the\nalgorithm chooses a search direction by looking ahead at the sequence of fast\nweights generated by another optimizer. We show that Lookahead improves the\nlearning stability and lowers the variance of its inner optimizer with\nnegligible computation and memory cost. We empirically demonstrate Lookahead\ncan significantly improve the performance of SGD and Adam, even with their\ndefault hyperparameter settings on ImageNet, CIFAR-10/100, neural machine\ntranslation, and Penn Treebank.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:59:50 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 15:55:38 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhang", "Michael R.", ""], ["Lucas", "James", ""], ["Hinton", "Geoffrey", ""], ["Ba", "Jimmy", ""]]}, {"id": "1907.08651", "submitter": "Daniel Karapetyan Dr", "authors": "Dobromir Marinov and Daniel Karapetyan", "title": "Hyperparameter Optimisation with Early Termination of Poor Performers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is typical for a machine learning system to have numerous hyperparameters\nthat affect its learning rate and prediction quality. Finding a good\ncombination of the hyperparameters is, however, a challenging job. This is\nmainly because evaluation of each combination is extremely expensive\ncomputationally; indeed, training a machine learning system on real data with\njust a single combination of hyperparameters usually takes hours or even days.\nIn this paper, we address this challenge by trying to predict the performance\nof the machine learning system with a given combination of hyperparameters\nwithout completing the expensive learning process. Instead, we terminate the\ntraining process at an early stage, collect the model performance data and use\nit to predict which of the combinations of hyperparameters is most promising.\nOur preliminary experiments show that such a prediction improves the\nperformance of the commonly used random search approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 19:14:18 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 19:29:13 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Marinov", "Dobromir", ""], ["Karapetyan", "Daniel", ""]]}, {"id": "1907.08801", "submitter": "Mauricio Barahona", "authors": "Amadeus Maes, Mauricio Barahona, Claudia Clopath", "title": "Learning spatiotemporal signals using a recurrent spiking network that\n  discretizes time", "comments": "To appear in Plos Computational Biology", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007606", "report-no": null, "categories": "q-bio.NC cs.NE nlin.AO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to produce spatiotemporal sequences is a common task that the brain\nhas to solve. The same neural substrate may be used by the brain to produce\ndifferent sequential behaviours. The way the brain learns and encodes such\ntasks remains unknown as current computational models do not typically use\nrealistic biologically-plausible learning. Here, we propose a model where a\nspiking recurrent network of excitatory and inhibitory biophysical neurons\ndrives a read-out layer: the dynamics of the driver recurrent network is\ntrained to encode time which is then mapped through the read-out neurons to\nencode another dimension, such as space or a phase. Different spatiotemporal\npatterns can be learned and encoded through the synaptic weights to the\nread-out neurons that follow common Hebbian learning rules. We demonstrate that\nthe model is able to learn spatiotemporal dynamics on time scales that are\nbehaviourally relevant and we show that the learned sequences are robustly\nreplayed during a regime of spontaneous activity.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 11:54:20 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 05:40:07 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Maes", "Amadeus", ""], ["Barahona", "Mauricio", ""], ["Clopath", "Claudia", ""]]}, {"id": "1907.08931", "submitter": "Kensuke Nakamura", "authors": "Kensuke Nakamura, Byung-Woo Hong", "title": "Adaptive Weight Decay for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization in the optimization of deep neural networks is often critical\nto avoid undesirable over-fitting leading to better generalization of model.\nOne of the most popular regularization algorithms is to impose L-2 penalty on\nthe model parameters resulting in the decay of parameters, called weight-decay,\nand the decay rate is generally constant to all the model parameters in the\ncourse of optimization. In contrast to the previous approach based on the\nconstant rate of weight-decay, we propose to consider the residual that\nmeasures dissimilarity between the current state of model and observations in\nthe determination of the weight-decay for each parameter in an adaptive way,\ncalled adaptive weight-decay (AdaDecay) where the gradient norms are normalized\nwithin each layer and the degree of regularization for each parameter is\ndetermined in proportional to the magnitude of its gradient using the sigmoid\nfunction. We empirically demonstrate the effectiveness of AdaDecay in\ncomparison to the state-of-the-art optimization algorithms using popular\nbenchmark datasets: MNIST, Fashion-MNIST, and CIFAR-10 with conventional neural\nnetwork models ranging from shallow to deep. The quantitative evaluation of our\nproposed algorithm indicates that AdaDecay improves generalization leading to\nbetter accuracy across all the datasets and models.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 08:04:29 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 20:27:52 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Nakamura", "Kensuke", ""], ["Hong", "Byung-Woo", ""]]}, {"id": "1907.08996", "submitter": "Mazharul Islam", "authors": "Mazharul Islam, Shuangrong Liu, Lin Wang, Xiaojing Zhang", "title": "Improving Neural Network Classifier using Gradient-based Floating\n  Centroid Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating centroid method (FCM) offers an efficient way to solve a\nfixed-centroid problem for the neural network classifiers. However,\nevolutionary computation as its optimization method restrains the FCM to\nachieve satisfactory performance for different neural network structures,\nbecause of the high computational complexity and inefficiency. Traditional\ngradient-based methods have been extensively adopted to optimize the neural\nnetwork classifiers. In this study, a gradient-based floating centroid (GDFC)\nmethod is introduced to address the fixed centroid problem for the neural\nnetwork classifiers optimized by gradient-based methods. Furthermore, a new\nloss function for optimizing GDFC is introduced. The experimental results\ndisplay that GDFC obtains promising classification performance than the\ncomparison methods on the benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 16:09:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Islam", "Mazharul", ""], ["Liu", "Shuangrong", ""], ["Wang", "Lin", ""], ["Zhang", "Xiaojing", ""]]}, {"id": "1907.09002", "submitter": "Daniel Brunner", "authors": "Nadezhda Semenova, Xavier Porte, Louis Andreoli, Maxime Jacquot,\n  Laurent Larger, Daniel Brunner", "title": "Fundamental aspects of noise in analog-hardware neural networks", "comments": null, "journal-ref": "Chaos 29, 103128 (2019)", "doi": "10.1063/1.5120824", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study and analyze the fundamental aspects of noise propagation in\nrecurrent as well as deep, multi-layer networks. The main focus of our study\nare neural networks in analogue hardware, yet the methodology provides insight\nfor networks in general. The system under study consists of noisy linear nodes,\nand we investigate the signal-to-noise ratio at the network's outputs which is\nthe upper limit to such a system's computing accuracy. We consider additive and\nmultiplicative noise which can be purely local as well as correlated across\npopulations of neurons. This covers the chief internal-perturbations of\nhardware networks and noise amplitudes were obtained from a physically\nimplemented recurrent neural network and therefore correspond to a real-world\nsystem. Analytic solutions agree exceptionally well with numerical data,\nenabling clear identification of the most critical components and aspects for\nnoise management. Focusing on linear nodes isolates the impact of network\nconnections and allows us to derive strategies for mitigating noise. Our work\nis the starting point in addressing this aspect of analogue neural networks,\nand our results identify notoriously sensitive points while simultaneously\nhighlighting the robustness of such computational systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 16:51:42 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Semenova", "Nadezhda", ""], ["Porte", "Xavier", ""], ["Andreoli", "Louis", ""], ["Jacquot", "Maxime", ""], ["Larger", "Laurent", ""], ["Brunner", "Daniel", ""]]}, {"id": "1907.09050", "submitter": "Richard Jiang", "authors": "Richard Jiang and Danny Crookes", "title": "Shallow Unorganized Neural Networks using Smart Neuron Model for Visual\n  Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent success of Deep Neural Networks (DNNs) has revealed the\nsignificant capability of neural computing in many challenging applications.\nAlthough DNNs are derived from emulating biological neurons, there still exist\ndoubts over whether or not DNNs are the final and best model to emulate the\nmechanism of human intelligence. In particular, there are two discrepancies\nbetween computational DNN models and the observed facts of biological neurons.\nFirst, human neurons are interconnected randomly, while DNNs need\ncarefully-designed architectures to work properly. Second, human neurons\nusually have a long spiking latency (~100ms) which implies that not many layers\ncan be involved in making a decision, while DNNs could have hundreds of layers\nto guarantee high accuracy. In this paper, we propose a new computational\nmodel, namely shallow unorganized neural networks (SUNNs), in contrast to\nANNs/DNNs. The proposed SUNNs differ from standard ANNs or DNNs in three\nfundamental aspects: 1) SUNNs are based on an adaptive neuron cell model, Smart\nNeurons, that allows each artificial neuron cell to adaptively respond to its\ninputs rather than carrying out a fixed weighted-sum operation like the classic\nneuron model in ANNs/DNNs; 2) SUNNs can cope with computational tasks with very\nshallow architectures; 3) SUNNs have a natural topology with random\ninterconnections, as the human brain does, and as proposed by Turing's B-type\nunorganized machines. We implemented the proposed SUNN architecture and tested\nit on a number of unsupervised early stage visual perception tasks.\nSurprisingly, such simple shallow architectures achieved very good results in\nour experiments. The success of our new computational model makes it the first\nworkable example of Turing's B-Type unorganized machine that can achieve\ncomparable or better performance against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:09:35 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 23:50:49 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Jiang", "Richard", ""], ["Crookes", "Danny", ""]]}, {"id": "1907.09077", "submitter": "Ruizhe Cai", "authors": "Ruizhe Cai, Ao Ren, Olivia Chen, Ning Liu, Caiwen Ding, Xuehai Qian,\n  Jie Han, Wenhui Luo, Nobuyuki Yoshikawa, Yanzhi Wang", "title": "A Stochastic-Computing based Deep Learning Framework using Adiabatic\n  Quantum-Flux-Parametron SuperconductingTechnology", "comments": null, "journal-ref": null, "doi": "10.1145/3307650.3322270", "report-no": null, "categories": "cs.NE cs.ET cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adiabatic Quantum-Flux-Parametron (AQFP) superconducting technology has\nbeen recently developed, which achieves the highest energy efficiency among\nsuperconducting logic families, potentially huge gain compared with\nstate-of-the-art CMOS. In 2016, the successful fabrication and testing of\nAQFP-based circuits with the scale of 83,000 JJs have demonstrated the\nscalability and potential of implementing large-scale systems using AQFP. As a\nresult, it will be promising for AQFP in high-performance computing and deep\nspace applications, with Deep Neural Network (DNN) inference acceleration as an\nimportant example. Besides ultra-high energy efficiency, AQFP exhibits two\nunique characteristics: the deep pipelining nature since each AQFP logic gate\nis connected with an AC clock signal, which increases the difficulty to avoid\nRAW hazards; the second is the unique opportunity of true random number\ngeneration (RNG) using a single AQFP buffer, far more efficient than RNG in\nCMOS. We point out that these two characteristics make AQFP especially\ncompatible with the \\emph{stochastic computing} (SC) technique, which uses a\ntime-independent bit sequence for value representation, and is compatible with\nthe deep pipelining nature. Further, the application of SC has been\ninvestigated in DNNs in prior work, and the suitability has been illustrated as\nSC is more compatible with approximate computations. This work is the first to\ndevelop an SC-based DNN acceleration framework using AQFP technology.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 01:44:49 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Cai", "Ruizhe", ""], ["Ren", "Ao", ""], ["Chen", "Olivia", ""], ["Liu", "Ning", ""], ["Ding", "Caiwen", ""], ["Qian", "Xuehai", ""], ["Han", "Jie", ""], ["Luo", "Wenhui", ""], ["Yoshikawa", "Nobuyuki", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1907.09109", "submitter": "Miao Zhang", "authors": "Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su", "title": "Efficient Novelty-Driven Neural Architecture Search", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-Shot Neural architecture search (NAS) attracts broad attention recently\ndue to its capacity to reduce the computational hours through weight sharing.\nHowever, extensive experiments on several recent works show that there is no\npositive correlation between the validation accuracy with inherited weights\nfrom the supernet and the test accuracy after re-training for One-Shot NAS.\nDifferent from devising a controller to find the best performing architecture\nwith inherited weights, this paper focuses on how to sample architectures to\ntrain the supernet to make it more predictive. A single-path supernet is\nadopted, where only a small part of weights are optimized in each step, to\nreduce the memory demand greatly. Furthermore, we abandon devising complicated\nreward based architecture sampling controller, and sample architectures to\ntrain supernet based on novelty search. An efficient novelty search method for\nNAS is devised in this paper, and extensive experiments demonstrate the\neffectiveness and efficiency of our novelty search based architecture sampling\nmethod. The best architecture obtained by our algorithm with the same search\nspace achieves the state-of-the-art test error rate of 2.51\\% on CIFAR-10 with\nonly 7.5 hours search time in a single GPU, and a validation perplexity of\n60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell\nstructures to larger datasets ImageNet and WikiText-2, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:17:13 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhang", "Miao", ""], ["Li", "Huiqi", ""], ["Pan", "Shirui", ""], ["Liu", "Taoping", ""], ["Su", "Steven", ""]]}, {"id": "1907.09126", "submitter": "Zhiri Tang", "authors": "Zhiri Tang", "title": "Performance and Comparisons of STDP based and Non-STDP based Memristive\n  Neural Networks on Hardware", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of research on memristor, memristive neural networks\n(MNNs) have become a hot research topic recently. Because memristor can mimic\nthe spike timing-dependent plasticity (STDP), the research on STDP based MNNs\nis rapidly increasing. However, although state-of-the-art works on STDP based\nMNNs have many applications such as pattern recognition, STDP mechanism brings\nrelatively complex hardware framework and low processing speed, which block\nMNNs' hardware realization. A non-STDP based unsupervised MNN is constructed in\nthis paper. Through the comparison with STDP method on the basis of two common\nstructures including feedforward and crossbar, non-STDP based MNNs not only\nremain the same advantages as STDP based MNNs including high accuracy and\nconvergence speed in pattern recognition, but also better hardware performance\nas few hardware resources and higher processing speed. By virtue of the\ncombination of memristive character and simple mechanism, non-STDP based MNNs\nhave better hardware compatibility, which may give a new viewpoint for\nmemristive neural networks' engineering applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 04:29:05 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 07:05:01 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 03:12:21 GMT"}, {"version": "v4", "created": "Sun, 8 Dec 2019 12:54:15 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Tang", "Zhiri", ""]]}, {"id": "1907.09173", "submitter": "Jindong Wang", "authors": "Yiqiang Chen, Jindong Wang, Chaohui Yu, Wen Gao, Xin Qin", "title": "FedHealth: A Federated Transfer Learning Framework for Wearable\n  Healthcare", "comments": "IJCAI-19 Workshop on Federated Machine Learning for User Privacy and\n  Data Confidentiality (IJCAI (FML)) 2019; fix typos; journal version:\n  https://ieeexplore.ieee.org/document/9076082", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of computing technology, wearable devices such as\nsmart phones and wristbands make it easy to get access to people's health\ninformation including activities, sleep, sports, etc. Smart healthcare achieves\ngreat success by training machine learning models on a large quantity of user\ndata. However, there are two critical challenges. Firstly, user data often\nexists in the form of isolated islands, making it difficult to perform\naggregation without compromising privacy security. Secondly, the models trained\non the cloud fail on personalization. In this paper, we propose FedHealth, the\nfirst federated transfer learning framework for wearable healthcare to tackle\nthese challenges. FedHealth performs data aggregation through federated\nlearning, and then builds personalized models by transfer learning. It is able\nto achieve accurate and personalized healthcare without compromising privacy\nand security. Experiments demonstrate that FedHealth produces higher accuracy\n(5.3% improvement) for wearable activity recognition when compared to\ntraditional methods. FedHealth is general and extensible and has the potential\nto be used in many healthcare applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:56:33 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 10:22:03 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Yiqiang", ""], ["Wang", "Jindong", ""], ["Yu", "Chaohui", ""], ["Gao", "Wen", ""], ["Qin", "Xin", ""]]}, {"id": "1907.09209", "submitter": "Leo Cazenille Dr", "authors": "Leo Cazenille, Nicolas Bredeche, Jos\\'e Halloy", "title": "Automatic Calibration of Artificial Neural Networks for Zebrafish\n  Collective Behaviours using a Quality Diversity Algorithm", "comments": "8 pages, 4 figures, 1 table", "journal-ref": "Conference on Biomimetic and Biohybrid Systems. Springer, Cham,\n  2019", "doi": "10.1007/978-3-030-24741-6_4", "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last two decades, various models have been proposed for fish\ncollective motion. These models are mainly developed to decipher the biological\nmechanisms of social interaction between animals. They consider very simple\nhomogeneous unbounded environments and it is not clear that they can simulate\naccurately the collective trajectories. Moreover when the models are more\naccurate, the question of their scalability to either larger groups or more\nelaborate environments remains open. This study deals with learning how to\nsimulate realistic collective motion of collective of zebrafish, using\nreal-world tracking data. The objective is to devise an agent-based model that\ncan be implemented on an artificial robotic fish that can blend into a\ncollective of real fish. We present a novel approach that uses Quality\nDiversity algorithms, a class of algorithms that emphasise exploration over\npure optimisation. In particular, we use CVT-MAP-Elites, a variant of the\nstate-of-the-art MAP-Elites algorithm for high dimensional search space.\nResults show that Quality Diversity algorithms not only outperform classic\nevolutionary reinforcement learning methods at the macroscopic level (i.e.\ngroup behaviour), but are also able to generate more realistic biomimetic\nbehaviours at the microscopic level (i.e. individual behaviour).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 10:04:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Cazenille", "Leo", ""], ["Bredeche", "Nicolas", ""], ["Halloy", "Jos\u00e9", ""]]}, {"id": "1907.09248", "submitter": "Luk\\'a\\v{s} Adam", "authors": "Luk\\'a\\v{s} Adam and Xin Yao", "title": "A Simple Yet Effective Approach to Robust Optimization Over Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust optimization over time (ROOT) refers to an optimization problem where\nits performance is evaluated over a period of future time. Most of the existing\nalgorithms use particle swarm optimization combined with another method which\npredicts future solutions to the optimization problem. We argue that this\napproach may perform subpar and suggest instead a method based on a random\nsampling of the search space. We prove its theoretical guarantees and show that\nit significantly outperforms the state-of-the-art methods for ROOT.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:51:47 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 04:47:55 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 08:05:22 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Adam", "Luk\u00e1\u0161", ""], ["Yao", "Xin", ""]]}, {"id": "1907.09285", "submitter": "Clement Leroy", "authors": "Clement Leroy, Eric Anquetil, Nathalie Girard", "title": "ParaFIS:A new online fuzzy inference system based on parallel drift\n  anticipation", "comments": null, "journal-ref": "FUZZ-IEEE, Jun 2019, New Orleans, United States", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new architecture of incremen-tal fuzzy inference system\n(also called Evolving Fuzzy System-EFS). In the context of classifying data\nstream in non stationary environment, concept drifts problems must be\naddressed. Several studies have shown that EFS can deal with such environment\nthanks to their high structural flexibility. These EFS perform well with smooth\ndrift (or incremental drift). The new architecture we propose is focused on\nimproving the processing of brutal changes in the data distribution (often\ncalled brutal concept drift). More precisely, a generalized EFS is paired with\na module of anticipation to improve the adaptation of new rules after a brutal\ndrift. The proposed architecture is evaluated on three datasets from UCI\nrepository where artificial brutal drifts have been applied. A fit model is\nalso proposed to get a \"reactivity time\" needed to converge to the steady-state\nand the score at end. Both characteristics are compared between the same system\nwith and without anticipation and with a similar EFS from state-of-the-art. The\nexperiments demonstrates improvements in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 08:30:59 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Leroy", "Clement", ""], ["Anquetil", "Eric", ""], ["Girard", "Nathalie", ""]]}, {"id": "1907.09300", "submitter": "J\\\"org Stork", "authors": "J\\\"org Stork, Martin Zaefferer, Thomas Bartz-Beielstein, A. E. Eiben", "title": "Surrogate Models for Enhancing the Efficiency of Neuroevolution in\n  Reinforcement Learning", "comments": "This is the authors version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published in Genetic and Evolutionary Computation Conference (GECCO 2019)", "journal-ref": "2019, Genetic and Evolutionary Computation Conference (GECCO\n  2019), Prague, Czech Republic. ACM, New York, NY, USA", "doi": "10.1145/3321707.3321829", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, reinforcement learning received a lot of attention. One\nmethod to solve reinforcement learning tasks is Neuroevolution, where neural\nnetworks are optimized by evolutionary algorithms. A disadvantage of\nNeuroevolution is that it can require numerous function evaluations, while not\nfully utilizing the available information from each fitness evaluation. This is\nespecially problematic when fitness evaluations become expensive. To reduce the\ncost of fitness evaluations, surrogate models can be employed to partially\nreplace the fitness function. The difficulty of surrogate modeling for\nNeuroevolution is the complex search space and how to compare different\nnetworks. To that end, recent studies showed that a kernel based approach,\nparticular with phenotypic distance measures, works well. These kernels compare\ndifferent networks via their behavior (phenotype) rather than their topology or\nencoding (genotype). In this work, we discuss the use of surrogate model-based\nNeuroevolution (SMB-NE) using a phenotypic distance for reinforcement learning.\nIn detail, we investigate a) the potential of SMB-NE with respect to evaluation\nefficiency and b) how to select adequate input sets for the phenotypic distance\nmeasure in a reinforcement learning problem. The results indicate that we are\nable to considerably increase the evaluation efficiency using dynamic input\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:14:23 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Stork", "J\u00f6rg", ""], ["Zaefferer", "Martin", ""], ["Bartz-Beielstein", "Thomas", ""], ["Eiben", "A. E.", ""]]}, {"id": "1907.09320", "submitter": "Guangcun Shan", "authors": "Hongyu Wang, Wei Liang, Guangcun Shan", "title": "An Efficient Method of Detection and Recognition in Remote Sensing Image\n  Based on multi-angle Region of Interests", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently, deep learning technology has been widely used in the field of\nimage recognition. However, it mainly aims at the recognition and detection of\nordinary pictures and common scenes. As special images, remote sensing images\nhave different shooting angles and shooting methods compared with ordinary\nones, which makes remote sensing images play an irreplaceable role in some\nareas. In this paper, based on a deep convolution neural network for providing\nmulti-level information of images and combines RPN (Region Proposal Network)\nfor generating multi-angle ROIs (Region of Interest), a new model for object\ndetection and recognition in remote sensing images is proposed. In the\nexperiment, it achieves better results than traditional ways, which demonstrate\nthat the model proposed here would have a huge potential application in remote\nsensing image recognition.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:48:05 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wang", "Hongyu", ""], ["Liang", "Wei", ""], ["Shan", "Guangcun", ""]]}, {"id": "1907.09468", "submitter": "Robert Manger", "authors": "Marko \\v{S}poljarec, Robert Manger", "title": "Heuristic solutions to robust variants of the minimum-cost integer flow\n  problem", "comments": null, "journal-ref": "Journal of Heuristics, 2020", "doi": "10.1007/s10732-020-09441-1", "report-no": null, "categories": "cs.AI cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with robust optimization applied to network flows. Two\nrobust variants of the minimum-cost integer flow problem are considered.\nThereby, uncertainty in problem formulation is limited to arc unit costs and\nexpressed by a finite set of explicitly given scenarios. It is shown that both\nproblem variants are NP-hard. To solve the considered variants, several\nheuristics based on local search or evolutionary computing are proposed. The\nheuristics are experimentally evaluated on appropriate problem instances.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 10:11:09 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["\u0160poljarec", "Marko", ""], ["Manger", "Robert", ""]]}, {"id": "1907.09720", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang and Adam\n  Trischler", "title": "Metalearned Neural Memory", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We augment recurrent neural networks with an external memory mechanism that\nbuilds upon recent progress in metalearning. We conceptualize this memory as a\nrapidly adaptable function that we parameterize as a deep neural network.\nReading from the neural memory function amounts to pushing an input (the key\nvector) through the function to produce an output (the value vector). Writing\nto memory means changing the function; specifically, updating the parameters of\nthe neural network to encode desired information. We leverage training and\nalgorithmic techniques from metalearning to update the neural memory function\nin one shot. The proposed memory-augmented model achieves strong performance on\na variety of learning problems, from supervised question answering to\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:04:07 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 14:16:23 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Sordoni", "Alessandro", ""], ["Wang", "Tong", ""], ["Trischler", "Adam", ""]]}, {"id": "1907.09789", "submitter": "Victor Pankratius", "authors": "Ho Chit Siu, Victor Pankratius", "title": "Genetic Algorithms for Starshade Retargeting in Space-Based Telescopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future space-based telescopes will leverage starshades as components that can\nbe independently positioned. Starshades will adjust the light coming in from\nexoplanet host stars and enhance the direct imaging of exoplanets and other\nphenomena. In this context, scheduling of space-based telescope observations is\nsubject to a large number of dynamic constraints, including target\nobservability, fuel, and target priorities. We present an application of\ngenetic algorithm (GA) scheduling on this problem that not only takes physical\nconstraints into account, but also considers direct human suggestions on\nschedules. By allowing direct suggestions on schedules, this type of heuristic\ncan capture the scheduling preferences and expertise of stakeholders without\nthe need to always formally codify such objectives. Additionally, this approach\nallows schedules to be constructed from existing ones when scenarios change;\nfor example, this capability allows for optimization without the need to\nrecompute schedules from scratch after changes such as new discoveries or new\ntargets of opportunity. We developed a specific graph-traversal-based framework\nupon which to apply GA for telescope scheduling, and use it to demonstrate the\nconvergence behavior of a particular implementation of GA. From this work,\ndifficulties with regards to assigning values to observational targets are also\nnoted, and recommendations are made for different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:45:21 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Siu", "Ho Chit", ""], ["Pankratius", "Victor", ""]]}, {"id": "1907.10072", "submitter": "Alex Cole", "authors": "Alex Cole, Andreas Schachner, Gary Shiu", "title": "Searching the Landscape of Flux Vacua with Genetic Algorithms", "comments": "31 pages, many figures. v2: journal version", "journal-ref": "https://doi.org/10.1007/JHEP11(2019)045", "doi": "10.1007/JHEP11(2019)045", "report-no": "MAD-TH-19-05", "categories": "hep-th cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we employ genetic algorithms to explore the landscape of type\nIIB flux vacua. We show that genetic algorithms can efficiently scan the\nlandscape for viable solutions satisfying various criteria. More specifically,\nwe consider a symmetric $T^{6}$ as well as the conifold region of a Calabi-Yau\nhypersurface. We argue that in both cases genetic algorithms are powerful tools\nfor finding flux vacua with interesting phenomenological properties. We also\ncompare genetic algorithms to algorithms based on different breeding mechanisms\nas well as random walk approaches.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:00:04 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 14:05:34 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Cole", "Alex", ""], ["Schachner", "Andreas", ""], ["Shiu", "Gary", ""]]}, {"id": "1907.10228", "submitter": "Seyoung Kim", "authors": "Hyungjun Kim, Malte Rasch, Tayfun Gokmen, Takashi Ando, Hiroyuki\n  Miyazoe, Jae-Joon Kim, John Rozen and Seyoung Kim", "title": "Zero-shifting Technique for Deep Neural Network Training on Resistive\n  Cross-point Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A resistive memory device-based computing architecture is one of the\npromising platforms for energy-efficient Deep Neural Network (DNN) training\naccelerators. The key technical challenge in realizing such accelerators is to\naccumulate the gradient information without a bias. Unlike the digital numbers\nin software which can be assigned and accessed with desired accuracy, numbers\nstored in resistive memory devices can only be manipulated following the\nphysics of the device, which can significantly limit the training performance.\nTherefore, additional techniques and algorithm-level remedies are required to\nachieve the best possible performance in resistive memory device-based\naccelerators. In this paper, we analyze asymmetric conductance modulation\ncharacteristics in RRAM by Soft-bound synapse model and present an in-depth\nanalysis on the relationship between device characteristics and DNN model\naccuracy using a 3-layer DNN trained on the MNIST dataset. We show that the\nimbalance between up and down update leads to a poor network performance. We\nintroduce a concept of symmetry point and propose a zero-shifting technique\nwhich can compensate imbalance by programming the reference device and changing\nthe zero value point of the weight. By using this zero-shifting method, we show\nthat network performance dramatically improves for imbalanced synapse devices.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 04:16:03 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 05:58:56 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Kim", "Hyungjun", ""], ["Rasch", "Malte", ""], ["Gokmen", "Tayfun", ""], ["Ando", "Takashi", ""], ["Miyazoe", "Hiroyuki", ""], ["Kim", "Jae-Joon", ""], ["Rozen", "John", ""], ["Kim", "Seyoung", ""]]}, {"id": "1907.10515", "submitter": "Kourosh Hakhamaneshi", "authors": "Kourosh Hakhamaneshi, Nick Werblun, Pieter Abbeel, Vladimir Stojanovic", "title": "BagNet: Berkeley Analog Generator with Layout Optimizer Boosted with\n  Deep Neural Networks", "comments": "Accepted on ICCAD 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrepancy between post-layout and schematic simulation results\ncontinues to widen in analog design due in part to the domination of layout\nparasitics. This paradigm shift is forcing designers to adopt design\nmethodologies that seamlessly integrate layout effects into the standard design\nflow. Hence, any simulation-based optimization framework should take into\naccount time-consuming post-layout simulation results. This work presents a\nlearning framework that learns to reduce the number of simulations of\nevolutionary-based combinatorial optimizers, using a DNN that discriminates\nagainst generated samples, before running simulations. Using this approach, the\ndiscriminator achieves at least two orders of magnitude improvement on sample\nefficiency for several large circuit examples including an optical link\nreceiver layout.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:02:51 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Hakhamaneshi", "Kourosh", ""], ["Werblun", "Nick", ""], ["Abbeel", "Pieter", ""], ["Stojanovic", "Vladimir", ""]]}, {"id": "1907.10599", "submitter": "Greg Yang", "authors": "Greg Yang and Hadi Salman", "title": "A Fine-Grained Spectral Perspective on Neural Networks", "comments": "8 pages of main text, 19 figures, 51 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are neural networks biased toward simple functions? Does depth always help\nlearn more complex features? Is training the last layer of a network as good as\ntraining all layers? How to set the range for learning rate tuning? These\nquestions seem unrelated at face value, but in this work we give all of them a\ncommon treatment from the spectral perspective. We will study the spectra of\nthe *Conjugate Kernel, CK,* (also called the *Neural Network-Gaussian Process\nKernel*), and the *Neural Tangent Kernel, NTK*. Roughly, the CK and the NTK\ntell us respectively \"what a network looks like at initialization\" and \"what a\nnetwork looks like during and after training.\" Their spectra then encode\nvaluable information about the initial distribution and the training and\ngeneralization properties of neural networks. By analyzing the eigenvalues, we\nlend novel insights into the questions put forth at the beginning, and we\nverify these insights by extensive experiments of neural networks. We derive\nfast algorithms for computing the spectra of CK and NTK when the data is\nuniformly distributed over the boolean cube, and show this spectra is the same\nin high dimensions when data is drawn from isotropic Gaussian or uniformly over\nthe sphere. Code replicating our results is available at\ngithub.com/thegregyang/NNspectra.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 17:58:45 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 23:05:20 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 21:12:24 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 15:12:11 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Yang", "Greg", ""], ["Salman", "Hadi", ""]]}, {"id": "1907.10988", "submitter": "Stef Maree", "authors": "S.C. Maree, T. Alderliesten, P.A.N. Bosman", "title": "Benchmarking HillVallEA for the GECCO 2019 Competition on Multimodal\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents benchmarking results of the Hill-Valley Evolutionary\nAlgorithm version 2019 (HillVallEA19) on the CEC2013 niching benchmark suite\nunder the restrictions of the GECCO 2019 niching competition on multimodal\noptimization. Performance is compared to algorithms that participated in\nprevious editions of the niching competition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:58:41 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Maree", "S. C.", ""], ["Alderliesten", "T.", ""], ["Bosman", "P. A. N.", ""]]}, {"id": "1907.11114", "submitter": "Jiawei Zhang", "authors": "Yixin Chen, Lin Meng, and Jiawei Zhang", "title": "Graph Neural Lasso for Dynamic Network Regression", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression of multiple inter-connected sequence data is a problem in\nvarious disciplines. Formally, we name the regression problem of multiple\ninter-connected data entities as the \"dynamic network regression\" in this\npaper. Within the problem of stock forecasting or traffic speed prediction, we\nneed to consider both the trends of the entities and the relationships among\nthe entities. A majority of existing approaches can't capture that information\ntogether. Some of the approaches are proposed to deal with the sequence data,\nlike LSTM. The others use the prior knowledge in a network to get a fixed graph\nstructure and do prediction on some unknown entities, like GCN. To overcome the\nlimitations in those methods, we propose a novel graph neural network, namely\nGraph Neural Lasso (GNL), to deal with the dynamic network problem. GNL extends\nthe GDU (gated diffusive unit) as the base neuron to capture the information\nbehind the sequence. Rather than using a fixed graph structure, GNL can learn\nthe dynamic graph structure automatically. By adding the attention mechanism in\nGNL, we can learn the dynamic relations among entities within each network\nsnapshot. Combining these two parts, GNL is able to model the dynamic network\nproblem well. Experimental results provided on two networked sequence datasets,\ni.e., Nasdaq-100 and METR-LA, show that GNL can address the network regression\nproblem very well and is also very competitive among the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:52:10 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:58:03 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Chen", "Yixin", ""], ["Meng", "Lin", ""], ["Zhang", "Jiawei", ""]]}, {"id": "1907.11129", "submitter": "Casey Kneale Ph.D", "authors": "Casey Kneale, Kolia Sadeghi", "title": "Semisupervised Adversarial Neural Networks for Cyber Security Transfer\n  Learning", "comments": "14 figures, 17 pages, Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the path to establishing a global cybersecurity framework where each\nenterprise shares information about malicious behavior, an important question\narises. How can a machine learning representation characterizing a cyber attack\non one network be used to detect similar attacks on other enterprise networks\nif each networks has wildly different distributions of benign and malicious\ntraffic? We address this issue by comparing the results of naively transferring\na model across network domains and using CORrelation ALignment, to our novel\nadversarial Siamese neural network. Our proposed model learns attack\nrepresentations that are more invariant to each network's particularities via\nan adversarial approach. It uses a simple ranking loss that prioritizes the\nlabeling of the most egregious malicious events correctly over average\naccuracy. This is appropriate for driving an alert triage workflow wherein an\nanalyst only has time to inspect the top few events ranked highest by the\nmodel. In terms of accuracy, the other approaches fail completely to detect any\nmalicious events when models were trained on one dataset are evaluated on\nanother for the first 100 events. While, the method presented here retrieves\nsizable proportions of malicious events, at the expense of some training\ninstabilities due in adversarial modeling. We evaluate these approaches using 2\npublicly available networking datasets, and suggest areas for future research.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 15:14:38 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Kneale", "Casey", ""], ["Sadeghi", "Kolia", ""]]}, {"id": "1907.11554", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Autoencoding with a Learning Classifier System: Initial Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders enable data dimensionality reduction and a key component of many\n(deep) learning systems. This short paper introduces a form of Holland's\nLearning Classifier System (LCS) to perform autoencoding building upon a\npreviously presented form of LCS that utilises unsupervised learning for\nclustering. Initial results using a neural network representation suggest it is\nan effective approach to reduction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:04:53 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 09:54:55 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1907.11639", "submitter": "Michael Hauser", "authors": "Michael Hauser", "title": "Training capsules as a routing-weighted product of expert neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsules are the multidimensional analogue to scalar neurons in neural\nnetworks, and because they are multidimensional, much more complex routing\nschemes can be used to pass information forward through the network than what\ncan be used in traditional neural networks. This work treats capsules as\ncollections of neurons in a fully connected neural network, where sub-networks\nconnecting capsules are weighted according to the routing coefficients\ndetermined by routing by agreement. An energy function is designed to reflect\nthis model, and it follows that capsule networks with dynamic routing can be\nformulated as a product of expert neurons. By alternating between dynamic\nrouting, which acts to both find subnetworks within the overall network as well\nas to mix the model distribution, and updating the parameters by the gradient\nof the contrastive divergence, a bottom-up, unsupervised learning algorithm is\nconstructed for capsule networks with dynamic routing. The model and its\ntraining algorithm are qualitatively tested in the generative sense, and is\nable to produce realistic looking images from standard vision datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:51:49 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Hauser", "Michael", ""]]}, {"id": "1907.11643", "submitter": "Michael Hauser", "authors": "Michael Hauser", "title": "Training products of expert capsules with mixing by dynamic routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops an unsupervised learning algorithm for products of expert\ncapsules with dynamic routing. Analogous to binary-valued neurons in Restricted\nBoltzmann Machines, the magnitude of a squashed capsule firing takes values\nbetween zero and one, representing the probability of the capsule being on.\nThis analogy motivates the design of an energy function for capsule networks.\nIn order to have an efficient sampling procedure where hidden layer nodes are\nnot connected, the energy function is made consistent with dynamic routing in\nthe sense of the probability of a capsule firing, and inference on the capsule\nnetwork is computed with the dynamic routing between capsules procedure. In\norder to optimize the log-likelihood of the visible layer capsules, the\ngradient is found in terms of this energy function. The developed unsupervised\nlearning algorithm is used to train a capsule network on standard vision\ndatasets, and is able to generate realistic looking images from its learned\ndistribution.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:58:56 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Hauser", "Michael", ""]]}, {"id": "1907.11910", "submitter": "Ivona-Alexandra Chili", "authors": "Vlad-Ioan Lupoaie, Ivona-Alexandra Chili, Mihaela Elena Breaban,\n  Madalina Raschip", "title": "SOM-Guided Evolutionary Search for Solving MinMax Multiple-TSP", "comments": "8 pages, 12 figures, 2 tables, CEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-TSP, also abbreviated in the literature as mTSP, is an extension of\nthe Traveling Salesman Problem that lies at the core of many variants of the\nVehicle Routing problem of great practical importance. The current paper\ndevelops and experiments with Self Organizing Maps, Evolutionary Algorithms and\nAnt Colony Systems to tackle the MinMax formulation of the Single-Depot\nMultiple-TSP. Hybridization between the neural network approach and the two\nmeta-heuristics shows to bring significant improvements, outperforming results\nreported in the literature on a set of problem instances taken from TSPLIB.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 13:48:40 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lupoaie", "Vlad-Ioan", ""], ["Chili", "Ivona-Alexandra", ""], ["Breaban", "Mihaela Elena", ""], ["Raschip", "Madalina", ""]]}, {"id": "1907.12071", "submitter": "Xiaohan Lin", "authors": "Yuanyuan Mi, Xiaohan Lin, Xiaolong Zou, Zilong Ji, Tiejun Huang, Si Wu", "title": "Spatiotemporal Information Processing with a Reservoir Decision-making\n  Network", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal information processing is fundamental to brain functions. The\npresent study investigates a canonic neural network model for spatiotemporal\npattern recognition. Specifically, the model consists of two modules, a\nreservoir subnetwork and a decision-making subnetwork. The former projects\ncomplex spatiotemporal patterns into spatially separated neural\nrepresentations, and the latter reads out these neural representations via\nintegrating information over time; the two modules are combined together via\nsupervised-learning using known examples. We elucidate the working mechanism of\nthe model and demonstrate its feasibility for discriminating complex\nspatiotemporal patterns. Our model reproduces the phenomenon of recognizing\nlooming patterns in the neural system, and can learn to discriminate gait with\nvery few training examples. We hope this study gives us insight into\nunderstanding how spatiotemporal information is processed in the brain and\nhelps us to develop brain-inspired application algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 11:04:34 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Mi", "Yuanyuan", ""], ["Lin", "Xiaohan", ""], ["Zou", "Xiaolong", ""], ["Ji", "Zilong", ""], ["Huang", "Tiejun", ""], ["Wu", "Si", ""]]}, {"id": "1907.12160", "submitter": "Soumya Mohanty", "authors": "Soumya D. Mohanty, Ethan Fahnestock", "title": "Adaptive spline fitting with particle swarm optimization", "comments": "Accepted version; Typo corrected in equation 3; Minor changes to text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fitting data with a spline, finding the optimal placement of knots can\nsignificantly improve the quality of the fit. However, the challenging\nhigh-dimensional and non-convex optimization problem associated with completely\nfree knot placement has been a major roadblock in using this approach. We\npresent a method that uses particle swarm optimization (PSO) combined with\nmodel selection to address this challenge. The problem of overfitting due to\nknot clustering that accompanies free knot placement is mitigated in this\nmethod by explicit regularization, resulting in a significantly improved\nperformance on highly noisy data. The principal design choices available in the\nmethod are delineated and a statistically rigorous study of their effect on\nperformance is carried out using simulated data and a wide variety of benchmark\nfunctions. Our results demonstrate that PSO-based free knot placement leads to\na viable and flexible adaptive spline fitting approach that allows the fitting\nof both smooth and non-smooth functions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 23:30:15 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 02:29:55 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 23:52:23 GMT"}, {"version": "v4", "created": "Sun, 28 Jun 2020 17:37:24 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 21:43:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mohanty", "Soumya D.", ""], ["Fahnestock", "Ethan", ""]]}, {"id": "1907.12179", "submitter": "Fatima Zahra Azayite", "authors": "Fatima Zahra Azayite, Said Achchab", "title": "A hybrid neural network model based on improved PSO and SA for\n  bankruptcy prediction", "comments": "13 pages", "journal-ref": "International Journal of Computer Science Issues, Vol 16, Issue 1,\n  January 2019", "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting firm's failure is one of the most interesting subjects for\ninvestors and decision makers. In this paper, a bankruptcy prediction model is\nproposed based on Artificial Neural networks (ANN). Taking into consideration\nthat the choice of variables to discriminate between bankrupt and non-bankrupt\nfirms influences significantly the model's accuracy and considering the problem\nof local minima, we propose a hybrid ANN based on variables selection\ntechniques. Moreover, we evolve the convergence of Particle Swarm Optimization\n(PSO) by proposing a training algorithm based on an improved PSO and Simulated\nAnnealing. A comparative performance study is reported, and the proposed hybrid\nmodel shows a high performance and convergence in the context of missing data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:07:21 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Azayite", "Fatima Zahra", ""], ["Achchab", "Said", ""]]}, {"id": "1907.12309", "submitter": "Sushrut Thorat", "authors": "Sushrut Thorat, Giacomo Aldegheri, Marcel A. J. van Gerven, Marius V.\n  Peelen", "title": "Modulation of early visual processing alleviates capacity limits in\n  solving multiple tasks", "comments": "Main paper - 4 pages, 2 figures; Appendix - 2 pages, 2 figures;\n  Published at the 2019 Conference on Cognitive Computational Neuroscience", "journal-ref": null, "doi": "10.32470/CCN.2019.1229-0", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In daily life situations, we have to perform multiple tasks given a visual\nstimulus, which requires task-relevant information to be transmitted through\nour visual system. When it is not possible to transmit all the possibly\nrelevant information to higher layers, due to a bottleneck, task-based\nmodulation of early visual processing might be necessary. In this work, we\nreport how the effectiveness of modulating the early processing stage of an\nartificial neural network depends on the information bottleneck faced by the\nnetwork. The bottleneck is quantified by the number of tasks the network has to\nperform and the neural capacity of the later stage of the network. The\neffectiveness is gauged by the performance on multiple object detection tasks,\nwhere the network is trained with a recent multi-task optimization scheme. By\nassociating neural modulations with task-based switching of the state of the\nnetwork and characterizing when such switching is helpful in early processing,\nour results provide a functional perspective towards understanding why\ntask-based modulation of early neural processes might be observed in the\nprimate visual cortex\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 09:56:40 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 08:10:26 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 17:42:12 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Thorat", "Sushrut", ""], ["Aldegheri", "Giacomo", ""], ["van Gerven", "Marcel A. J.", ""], ["Peelen", "Marius V.", ""]]}, {"id": "1907.12438", "submitter": "Phan Trung Hai Nguyen", "authors": "Per Kristian Lehre and Phan Trung Hai Nguyen", "title": "On the Limitations of the Univariate Marginal Distribution Algorithm to\n  Deception and Where Bivariate EDAs might help", "comments": "To appear in the 15th ACM/SIGEVO Workshop on Foundations of Genetic\n  Algorithms (FOGA XV), Potsdam, Germany", "journal-ref": null, "doi": "10.1145/3299904.3340316", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new benchmark problem called Deceptive Leading Blocks (DLB) to\nrigorously study the runtime of the Univariate Marginal Distribution Algorithm\n(UMDA) in the presence of epistasis and deception. We show that simple\nEvolutionary Algorithms (EAs) outperform the UMDA unless the selective pressure\n$\\mu/\\lambda$ is extremely high, where $\\mu$ and $\\lambda$ are the parent and\noffspring population sizes, respectively. More precisely, we show that the UMDA\nwith a parent population size of $\\mu=\\Omega(\\log n)$ has an expected runtime\nof $e^{\\Omega(\\mu)}$ on the DLB problem assuming any selective pressure\n$\\frac{\\mu}{\\lambda} \\geq \\frac{14}{1000}$, as opposed to the expected runtime\nof $\\mathcal{O}(n\\lambda\\log \\lambda+n^3)$ for the non-elitist\n$(\\mu,\\lambda)~\\text{EA}$ with $\\mu/\\lambda\\leq 1/e$. These results illustrate\ninherent limitations of univariate EDAs against deception and epistasis, which\nare common characteristics of real-world problems. In contrast, empirical\nevidence reveals the efficiency of the bi-variate MIMIC algorithm on the DLB\nproblem. Our results suggest that one should consider EDAs with more complex\nprobabilistic models when optimising problems with some degree of epistasis and\ndeception.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:59:04 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lehre", "Per Kristian", ""], ["Nguyen", "Phan Trung Hai", ""]]}, {"id": "1907.12545", "submitter": "Dylan Cashman", "authors": "Dylan Cashman, Genevieve Patterson, Abigail Mosca, Nathan Watts,\n  Shannon Robinson, Remco Chang", "title": "RNNbow: Visualizing Learning via Backpropagation Gradients in Recurrent\n  Neural Networks", "comments": null, "journal-ref": "IEEE Computer Graphics and Applications ( Volume: 38 , Issue: 6 ,\n  Nov.-Dec. 1 2018 ) pg 39-50", "doi": "10.1109/MCG.2018.2878902", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RNNbow, an interactive tool for visualizing the gradient flow\nduring backpropagation training in recurrent neural networks. RNNbow is a web\napplication that displays the relative gradient contributions from Recurrent\nNeural Network (RNN) cells in a neighborhood of an element of a sequence. We\ndescribe the calculation of backpropagation through time (BPTT) that keeps\ntrack of itemized gradients, or gradient contributions from one element of a\nsequence to previous elements of a sequence. By visualizing the gradient, as\nopposed to activations, RNNbow offers insight into how the network is learning.\nWe use it to explore the learning of an RNN that is trained to generate code in\nthe C programming language. We show how it uncovers insights into the vanishing\ngradient as well as the evolution of training as the RNN works its way through\na corpus.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 17:36:29 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Cashman", "Dylan", ""], ["Patterson", "Genevieve", ""], ["Mosca", "Abigail", ""], ["Watts", "Nathan", ""], ["Robinson", "Shannon", ""], ["Chang", "Remco", ""]]}, {"id": "1907.12659", "submitter": "Bin Wang", "authors": "Bin Wang, Bing Xue, Mengjie Zhang", "title": "Particle Swarm Optimisation for Evolving Deep Neural Networks for Image\n  Classification by Evolving and Stacking Transferable Blocks", "comments": "To appear in ieee wcci 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been widely used in image\nclassification tasks, but the process of designing CNN architectures is very\ncomplex, so Neural Architecture Search (NAS), automatically searching for\noptimal CNN architectures, has attracted more and more research interests.\nHowever, the computational cost of NAS is often too high to apply NAS on\nreal-life applications. In this paper, an efficient particle swarm optimisation\nmethod named EPSOCNN is proposed to evolve CNN architectures inspired by the\nidea of transfer learning. EPSOCNN successfully reduces the computation cost by\nminimising the search space to a single block and utilising a small subset of\nthe training set to evaluate CNNs during evolutionary process. Meanwhile,\nEPSOCNN also keeps very competitive classification accuracy by stacking the\nevolved block multiple times to fit the whole dataset. The proposed EPSOCNN\nalgorithm is evaluated on CIFAR-10 dataset and compared with 13 peer\ncompetitors comprised of deep CNNs crafted by hand, learned by reinforcement\nlearning methods and evolved by evolutionary computation approaches, which\nshows very promising results by outperforming all of the peer competitors with\nregard to the classification accuracy, number of parameters and the\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:30:36 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 04:25:01 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Wang", "Bin", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1907.12698", "submitter": "Antonio Mora Dr.", "authors": "A.M. Mora and A.I. Esparcia-Alc\\'azar", "title": "EVO* 2019 -- Late-Breaking Abstracts Volume", "comments": "LBAs accepted in EVO* 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the Late-Breaking Abstracts submitted to the EVO* 2019\nConference, that took place in Leipzig, from 24 to 26 of April. These papers\nwhere presented as short talks and also at the poster session of the conference\ntogether with other regular submissions. All of them present ongoing research\nand preliminary results investigating on the application of different\napproaches of Evolutionary Computation to different problems, most of them real\nworld ones.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 01:42:42 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Mora", "A. M.", ""], ["Esparcia-Alc\u00e1zar", "A. I.", ""]]}, {"id": "1907.12812", "submitter": "David Kadish", "authors": "David Kadish, Sebastian Risi and Laura Beloff", "title": "An artifcial life approach to studying niche differentiation in\n  soundscape ecology", "comments": "9 pages, 4 figures, The 2019 Conference on Artificial Life", "journal-ref": "The 2019 Conference on Artificial Life, 52-59", "doi": "10.1162/isal_a_00140", "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial life simulations are an important tool in the study of ecological\nphenomena that can be difficult to examine directly in natural environments.\nRecent work has established the soundscape as an ecologically important\nresource and it has been proposed that the differentiation of animal\nvocalizations within a soundscape is driven by the imperative of intraspecies\ncommunication. The experiments in this paper test that hypothesis in a\nsimulated soundscape in order to verify the feasibility of intraspecies\ncommunication as a driver of acoustic niche differentiation. The impact of\nintraspecies communication is found to be a significant factor in the division\nof a soundscape's frequency spectrum when compared to simulations where the\nneed to identify signals from conspecifics does not drive the evolution of\nsignalling. The method of simulating the effects of interspecies interactions\non the soundscape is positioned as a tool for developing artificial life agents\nthat can inhabit and interact with physical ecosystems and soundscapes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 09:51:30 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kadish", "David", ""], ["Risi", "Sebastian", ""], ["Beloff", "Laura", ""]]}, {"id": "1907.12821", "submitter": "Xun Zou", "authors": "Johannes Lengler, Xun Zou", "title": "Exponential Slowdown for Larger Populations: The $(\\mu+1)$-EA on\n  Monotone Functions", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2021.03.025", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pseudo-Boolean monotone functions are unimodal functions which are trivial to\noptimize for some hillclimbers, but are challenging for a surprising number of\nevolutionary algorithms (EAs). A general trend is that EAs are efficient if\nparameters like the mutation rate are set conservatively, but may need\nexponential time otherwise. In particular, it was known that the $(1+1)$-EA and\nthe $(1+\\lambda)$-EA can optimize every monotone function in pseudolinear time\nif the mutation rate is $c/n$ for some $c<1$, but they need exponential time\nfor some monotone functions for $c>2.2$. The second part of the statement was\nalso known for the $(\\mu+1)$-EA. In this paper we show that the first statement\ndoes not apply to the $(\\mu+1)$-EA. More precisely, we prove that for every\nconstant $c>0$ there is a constant integer $\\mu_0$ such that the $(\\mu+1)$-EA\nwith mutation rate $c/n$ and population size $\\mu_0\\le\\mu\\le n$ needs\nsuperpolynomial time to optimize some monotone functions. Thus, increasing the\npopulation size by just a constant has devastating effects on the performance.\nThis is in stark contrast to many other benchmark functions on which increasing\nthe population size either increases the performance significantly, or affects\nperformance mildly. The reason why larger populations are harmful lies in the\nfact that larger populations may temporarily decrease selective pressure on\nparts of the population. This allows unfavorable mutations to accumulate in\nsingle individuals and their descendants. If the population moves sufficiently\nfast through the search space, such unfavorable descendants can become\nancestors of future generations, and the bad mutations are preserved.\nRemarkably, this effect only occurs if the population renews itself\nsufficiently fast, which can only happen far away from the optimum. This is\ncounter-intuitive since usually optimization gets harder as we approach the\noptimum.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 10:17:17 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 19:10:35 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 14:55:17 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lengler", "Johannes", ""], ["Zou", "Xun", ""]]}, {"id": "1907.12899", "submitter": "Roja Eini", "authors": "Adam Morrissett, Roja Eini, Mostafa Zaman, Nasibeh Zohrabi, Sherif\n  Abdelwahed", "title": "A Physical Testbed for Intelligent Transportation Systems", "comments": "7 pages, 8 figures, 1 table, going to be published in the proceedings\n  of 12th IEEE International Conference on Human System Interaction (HSI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent transportation systems (ITSs) and other smart-city technologies\nare increasingly advancing in capability and complexity. While simulation\nenvironments continue to improve, their fidelity and ease of use can quickly\ndegrade as newer systems become increasingly complex. To remedy this, we\npropose a hardware- and software-based traffic management system testbed as\npart of a larger smart-city testbed. It comprises a network of connected\nvehicles, a network of intersection controllers, a variety of control services,\nand data analytics services. The main goal of our testbed is to provide\nresearchers and students with the means to develop novel traffic and vehicle\ncontrol algorithms with higher fidelity than what can be achieved with\nsimulation alone. Specifically, we are using the testbed to develop an\nintegrated management system that combines model-based control and data\nanalytics to improve the system performance over time. In this paper, we give a\ndetailed description of each component within the testbed and discuss its\ncurrent developmental state. Additionally, we present initial results and\npropose future work.\n  Index Terms: Smart city, Intelligent transportation systems,\nHuman-in-the-loop, Data analytics, Data visualization, Traffic network\nmanagement and control, Machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 15:49:44 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Morrissett", "Adam", ""], ["Eini", "Roja", ""], ["Zaman", "Mostafa", ""], ["Zohrabi", "Nasibeh", ""], ["Abdelwahed", "Sherif", ""]]}, {"id": "1907.12914", "submitter": "Farid Ghareh Mohammadi", "authors": "Farid Ghareh Mohammadi, Farzan Shenavarmasouleh, M. Hadi Amini, Hamid\n  R. Arabnia", "title": "Evolutionary Algorithms and Efficient Data Analytics for Image\n  Processing", "comments": "8 pages,5 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steganography algorithms facilitate communication between a source and a\ndestination in a secret manner. This is done by embedding messages/text/data\ninto images without impacting the appearance of the resultant images/videos.\nSteganalysis is the science of determining if an image has secret messages\nembedded/hidden in it. Because there are numerous steganography algorithms, and\nsince each one of them requires a different type of steganalysis, the\nsteganalysis process is extremely challenging. Thus, researchers aim to develop\none universal steganalysis to detect all known and unknown steganography\nalgorithms, ideally in real-time. Universal steganalysis extracts a large\nnumber of features to distinguish stego images from cover images. However, the\nincrease in features leads to the problem of the curse of dimensionality (CoD),\nwhich is considered to be an NP-hard problem. This COD problem additionally\nmakes real-time steganalysis hard. A large number of features generates large\ndatasets for which machine learning cannot generate an optimal model.\nGenerating a machine learning based model also takes a long time which makes\nreal-time processing appear impossible in any optimization for time-intensive\nfields such as visual computing. Possible solutions for CoD are deep learning\nand evolutionary algorithms that overcome the machine learning limitations. In\nthis study, we investigate previously developed evolutionary algorithms for\nboosting real-time image processing and argue that they provide the most\npromising solutions for the CoD problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:13:53 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 05:14:06 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 16:10:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mohammadi", "Farid Ghareh", ""], ["Shenavarmasouleh", "Farzan", ""], ["Amini", "M. Hadi", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "1907.12920", "submitter": "Elie Aljalbout", "authors": "Axel Sauer, Elie Aljalbout, Sami Haddadin", "title": "Tracking Holistic Object Representations", "comments": "Accepted for oral presentation at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in visual tracking are based on siamese feature extractors\nand template matching. For this category of trackers, latest research focuses\non better feature embeddings and similarity measures. In this work, we focus on\nbuilding holistic object representations for tracking. We propose a framework\nthat is designed to be used on top of previous trackers without any need for\nfurther training of the siamese network. The framework leverages the idea of\nobtaining additional object templates during the tracking process. Since the\nnumber of stored templates is limited, our method only keeps the most diverse\nones. We achieve this by providing a new diversity measure in the space of\nsiamese features. The obtained representation contains information beyond the\nground truth object location provided to the system. It is then useful for\ntracking itself but also for further tasks which require a visual understanding\nof objects. Strong empirical results on tracking benchmarks indicate that our\nmethod can improve the performance and robustness of the underlying trackers\nwhile barely reducing their speed. In addition, our method is able to match\ncurrent state-of-the-art results, while using a simpler and older network\narchitecture and running three times faster.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 10:51:21 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 09:27:19 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Sauer", "Axel", ""], ["Aljalbout", "Elie", ""], ["Haddadin", "Sami", ""]]}, {"id": "1907.12933", "submitter": "Lucas Carvalho Cordeiro", "authors": "Luiz H. Sena, Iury V. Bessa, Mikhail R. Gadelha, Lucas C. Cordeiro,\n  and Edjard Mota", "title": "Incremental Bounded Model Checking of Artificial Neural Networks in CUDA", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural networks (ANNs) are powerful computing systems employed for\nvarious applications due to their versatility to generalize and to respond to\nunexpected inputs/patterns. However, implementations of ANNs for\nsafety-critical systems might lead to failures, which are hardly predicted in\nthe design phase since ANNs are highly parallel and their parameters are hardly\ninterpretable. Here we develop and evaluate a novel symbolic software\nverification framework based on incremental bounded model checking (BMC) to\ncheck for adversarial cases and coverage methods in multi-layer perceptron\n(MLP). In particular, we further develop the efficient SMT-based\nContext-Bounded Model Checker for Graphical Processing Units (ESBMC-GPU) in\norder to ensure the reliability of certain safety properties in which\nsafety-critical systems can fail and make incorrect decisions, thereby leading\nto unwanted material damage or even put lives in danger. This paper marks the\nfirst symbolic verification framework to reason over ANNs implemented in CUDA.\nOur experimental results show that our approach implemented in ESBMC-GPU can\nsuccessfully verify safety properties and covering methods in ANNs and\ncorrectly generate 28 adversarial cases in MLPs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:50:34 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sena", "Luiz H.", ""], ["Bessa", "Iury V.", ""], ["Gadelha", "Mikhail R.", ""], ["Cordeiro", "Lucas C.", ""], ["Mota", "Edjard", ""]]}, {"id": "1907.13052", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner", "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric\n  Latent Representations", "comments": "Published at the International Conference on Learning Representations\n  (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative latent-variable models are emerging as promising tools in robotics\nand reinforcement learning. Yet, even though tasks in these domains typically\ninvolve distinct objects, most state-of-the-art generative models do not\nexplicitly capture the compositional nature of visual scenes. Two recent\nexceptions, MONet and IODINE, decompose scenes into objects in an unsupervised\nfashion. Their underlying generative processes, however, do not account for\ncomponent interactions. Hence, neither of them allows for principled sampling\nof novel scenes. Here we present GENESIS, the first object-centric generative\nmodel of 3D visual scenes capable of both decomposing and generating scenes by\ncapturing relationships between scene components. GENESIS parameterises a\nspatial GMM over images which is decoded from a set of object-centric latent\nvariables that are either inferred sequentially in an amortised fashion or\nsampled from an autoregressive prior. We train GENESIS on several publicly\navailable datasets and evaluate its performance on scene generation,\ndecomposition, and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:22:39 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 20:19:08 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 14:02:16 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 10:31:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Engelcke", "Martin", ""], ["Kosiorek", "Adam R.", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "1907.13100", "submitter": "Chao Qian", "authors": "Chao Bian, Chao Qian, Yang Yu", "title": "On the Robustness of Median Sampling in Noisy Evolutionary Optimization", "comments": "19 pages. arXiv admin note: text overlap with arXiv:1810.05045,\n  arXiv:1711.00956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world optimization tasks, the objective (i.e., fitness) function\nevaluation is often disturbed by noise due to a wide range of uncertainties.\nEvolutionary algorithms (EAs) have been widely applied to tackle noisy\noptimization, where reducing the negative effect of noise is a crucial issue.\nOne popular strategy to cope with noise is sampling, which evaluates the\nfitness multiple times and uses the sample average to approximate the true\nfitness. In this paper, we introduce median sampling as a noise handling\nstrategy into EAs, which uses the median of the multiple evaluations to\napproximate the true fitness instead of the mean. We theoretically show that\nmedian sampling can reduce the expected running time of EAs from exponential to\npolynomial by considering the (1+1)-EA on OneMax under the commonly used\none-bit noise. We also compare mean sampling with median sampling by\nconsidering two specific noise models, suggesting that when the 2-quantile of\nthe noisy fitness increases with the true fitness, median sampling can be a\nbetter choice. The results provide us with some guidance to employ median\nsampling efficiently in practice.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 11:54:18 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bian", "Chao", ""], ["Qian", "Chao", ""], ["Yu", "Yang", ""]]}, {"id": "1907.13223", "submitter": "Iulia Comsa", "authors": "Iulia M. Comsa, Krzysztof Potempa, Luca Versari, Thomas Fischbacher,\n  Andrea Gesmundo and Jyrki Alakuijala", "title": "Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function:\n  Learning with Backpropagation", "comments": "Open-source code related to this paper is available at\n  https://github.com/google/ihmehimmeli v2: Added references and added some\n  clarifications for the methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The timing of individual neuronal spikes is essential for biological brains\nto make fast responses to sensory stimuli. However, conventional artificial\nneural networks lack the intrinsic temporal coding ability present in\nbiological networks. We propose a spiking neural network model that encodes\ninformation in the relative timing of individual neuron spikes. In\nclassification tasks, the output of the network is indicated by the first\nneuron to spike in the output layer. This temporal coding scheme allows the\nsupervised training of the network with backpropagation, using locally exact\nderivatives of the postsynaptic spike times with respect to presynaptic spike\ntimes. The network operates using a biologically-plausible alpha synaptic\ntransfer function. Additionally, we use trainable synchronisation pulses that\nprovide bias, add flexibility during training and exploit the decay part of the\nalpha function. We show that such networks can be trained successfully on noisy\nBoolean logic tasks and on the MNIST dataset encoded in time. The results show\nthat the spiking neural network outperforms comparable spiking models on MNIST\nand achieves similar quality to fully connected conventional networks with the\nsame architecture. We also find that the spiking network spontaneously\ndiscovers two operating regimes, mirroring the accuracy-speed trade-off\nobserved in human decision-making: a slow regime, where a decision is taken\nafter all hidden neurons have spiked and the accuracy is very high, and a fast\nregime, where a decision is taken very fast but the accuracy is lower. These\nresults demonstrate the computational power of spiking networks with biological\ncharacteristics that encode information in the timing of individual neurons. By\nstudying temporal coding in spiking networks, we aim to create building blocks\ntowards energy-efficient and more complex biologically-inspired neural\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:05:18 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 11:20:25 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 21:34:55 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Comsa", "Iulia M.", ""], ["Potempa", "Krzysztof", ""], ["Versari", "Luca", ""], ["Fischbacher", "Thomas", ""], ["Gesmundo", "Andrea", ""], ["Alakuijala", "Jyrki", ""]]}, {"id": "1907.13440", "submitter": "William Guss", "authors": "William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang,\n  Cayden Codel, Manuela Veloso, Ruslan Salakhutdinov", "title": "MineRL: A Large-Scale Dataset of Minecraft Demonstrations", "comments": "Accepted at IJCAI 2019, 7 pages, 6 figures. arXiv admin note: text\n  overlap with arXiv:1904.10079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample inefficiency of standard deep reinforcement learning methods\nprecludes their application to many real-world problems. Methods which leverage\nhuman demonstrations require fewer samples but have been researched less. As\ndemonstrated in the computer vision and natural language processing\ncommunities, large-scale datasets have the capacity to facilitate research by\nserving as an experimental and benchmarking platform for new methods. However,\nexisting datasets compatible with reinforcement learning simulators do not have\nsufficient scale, structure, and quality to enable the further development and\nevaluation of methods focused on using human examples. Therefore, we introduce\na comprehensive, large-scale, simulator-paired dataset of human demonstrations:\nMineRL. The dataset consists of over 60 million automatically annotated\nstate-action pairs across a variety of related tasks in Minecraft, a dynamic,\n3D, open-world environment. We present a novel data collection scheme which\nallows for the ongoing introduction of new tasks and the gathering of complete\nstate information suitable for a variety of methods. We demonstrate the\nhierarchality, diversity, and scale of the MineRL dataset. Further, we show the\ndifficulty of the Minecraft domain along with the potential of MineRL in\ndeveloping techniques to solve key research challenges within it.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 18:10:30 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Guss", "William H.", ""], ["Houghton", "Brandon", ""], ["Topin", "Nicholay", ""], ["Wang", "Phillip", ""], ["Codel", "Cayden", ""], ["Veloso", "Manuela", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1907.13494", "submitter": "Alberto Testolin Dr.", "authors": "Alberto Cenzato, Alberto Testolin and Marco Zorzi", "title": "On the difficulty of learning and predicting the long-term dynamics of\n  bouncing objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately predict the surrounding environment is a\nfoundational principle of intelligence in biological and artificial agents. In\nrecent years, a variety of approaches have been proposed for learning to\npredict the physical dynamics of objects interacting in a visual scene. Here we\nconduct a systematic empirical evaluation of several state-of-the-art\nunsupervised deep learning models that are considered capable of learning the\nspatio-temporal structure of a popular dataset composed by synthetic videos of\nbouncing objects. We show that most of the models indeed obtain high accuracy\non the standard benchmark of predicting the next frame of a sequence, and one\nof them even achieves state-of-the-art performance. However, all models fall\nshort when probed with the more challenging task of generating multiple\nsuccessive frames. Our results show that the ability to perform short-term\npredictions does not imply that the model has captured the underlying structure\nand dynamics of the visual environment, thereby calling for a careful\nrethinking of the metrics commonly adopted for evaluating temporal models. We\nalso investigate whether the learning outcome could be affected by the use of\ncurriculum-based teaching.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:29:34 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Cenzato", "Alberto", ""], ["Testolin", "Alberto", ""], ["Zorzi", "Marco", ""]]}, {"id": "1907.13508", "submitter": "Henry Wilde", "authors": "Henry Wilde, Vincent Knight, Jonathan Gillard", "title": "Evolutionary Dataset Optimisation: learning algorithm quality through\n  evolution", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a novel method for learning how algorithms perform.\nClassically, algorithms are compared on a finite number of existing (or newly\nsimulated) benchmark datasets based on some fixed metrics. The algorithm(s)\nwith the smallest value of this metric are chosen to be the `best performing'.\nWe offer a new approach to flip this paradigm. We instead aim to gain a richer\npicture of the performance of an algorithm by generating artificial data\nthrough genetic evolution, the purpose of which is to create populations of\ndatasets for which a particular algorithm performs well on a given metric.\nThese datasets can be studied so as to learn what attributes lead to a\nparticular progression of a given algorithm. Following a detailed description\nof the algorithm as well as a brief description of an open source\nimplementation, a case study in clustering is presented. This case study\ndemonstrates the performance and nuances of the method which we call\nEvolutionary Dataset Optimisation. In this study, a number of known properties\nabout preferable datasets for the clustering algorithms known as (k)-means and\nDBSCAN are realised in the generated datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:03:20 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 16:05:28 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 10:07:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Wilde", "Henry", ""], ["Knight", "Vincent", ""], ["Gillard", "Jonathan", ""]]}, {"id": "1907.13529", "submitter": "Xiaofen Lu", "authors": "Xiaofen Lu, Ke Tang, Stefan Menzel, Xin Yao", "title": "Competitive Coevolution as an Adversarial Approach to Dynamic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic optimization, for which the objective functions change over time, has\nattracted intensive investigations due to the inherent uncertainty associated\nwith many real-world problems. For its robustness with respect to noise,\nEvolutionary Algorithms (EAs) have been expected to have great potential for\ndynamic optimization. On the other hand, EAs are also criticized for its high\ncomputational complexity, which appears to be contradictory to the core\nrequirement of real-world dynamic optimization, i.e., fast adaptation\n(typically in terms of wall-clock time) to the environmental change. So far,\nwhether EAs would indeed lead to a truly effective approach for real-world\ndynamic optimization remain unclear. In this paper, a new framework of\nemploying EAs in the context of dynamic optimization is explored. We suggest\nthat, instead of online evolving (searching) solutions for the ever-changing\nobjective function, EAs are more suitable for acquiring an archive of solutions\nin an offline way, which could be adopted to construct a system to provide\nhigh-quality solutions efficiently in a dynamic environment. To be specific, we\nfirst re-formulate dynamic optimization problems as static set-oriented\noptimization problems. Then, a particular type of EAs, namely competitive\ncoevolution, is employed to search for the archive of solutions in an\nadversarial way. The general framework is instantiated for continuous dynamic\nconstrained optimization problems, and the empirical results showed the\npotential of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:38:09 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 13:14:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lu", "Xiaofen", ""], ["Tang", "Ke", ""], ["Menzel", "Stefan", ""], ["Yao", "Xin", ""]]}]