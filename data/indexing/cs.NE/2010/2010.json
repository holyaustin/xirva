[{"id": "2010.00041", "submitter": "Mohammad Saber Hashemi", "authors": "Mohammad Saber Hashemi, Masoud Safdari, Azadeh Sheidaei", "title": "A Supervised Machine Learning Approach for Accelerating the Design of\n  Particulate Composites: Application to Thermal Conductivity", "comments": "24 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A supervised machine learning (ML) based computational methodology for the\ndesign of particulate multifunctional composite materials with desired thermal\nconductivity (TC) is presented. The design variables are physical descriptors\nof the material microstructure that directly link microstructure to the\nmaterial's properties. A sufficiently large and uniformly sampled database was\ngenerated based on the Sobol sequence. Microstructures were realized using an\nefficient dense packing algorithm, and the TCs were obtained using our\npreviously developed Fast Fourier Transform (FFT) homogenization method. Our\noptimized ML method is trained over the generated database and establishes the\ncomplex relationship between the structure and properties. Finally, the\napplication of the trained ML model in the inverse design of a new class of\ncomposite materials, liquid metal (LM) elastomer, with desired TC is discussed.\nThe results show that the surrogate model is accurate in predicting the\nmicrostructure behavior with respect to high-fidelity FFT simulations, and\ninverse design is robust in finding microstructure parameters according to case\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:18:00 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 14:47:45 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 02:15:23 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Hashemi", "Mohammad Saber", ""], ["Safdari", "Masoud", ""], ["Sheidaei", "Azadeh", ""]]}, {"id": "2010.00236", "submitter": "Ryoji Tanabe", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "A Niching Indicator-Based Multi-modal Many-objective Optimizer", "comments": "This is an accepted version of a paper published in Swarm and\n  Evolutionary Computation", "journal-ref": null, "doi": "10.1016/j.swevo.2019.06.001", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal multi-objective optimization is to locate (almost) equivalent\nPareto optimal solutions as many as possible. Some evolutionary algorithms for\nmulti-modal multi-objective optimization have been proposed in the literature.\nHowever, there is no efficient method for multi-modal many-objective\noptimization, where the number of objectives is more than three. To address\nthis issue, this paper proposes a niching indicator-based multi-modal multi-\nand many-objective optimization algorithm. In the proposed method, the fitness\ncalculation is performed among a child and its closest individuals in the\nsolution space to maintain the diversity. The performance of the proposed\nmethod is evaluated on multi-modal multi-objective test problems with up to 15\nobjectives. Results show that the proposed method can handle a large number of\nobjectives and find a good approximation of multiple equivalent Pareto optimal\nsolutions. The results also show that the proposed method performs\nsignificantly better than eight multi-objective evolutionary algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 07:45:46 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2010.00265", "submitter": "Ryoji Tanabe", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "Review and Analysis of Three Components of Differential Evolution\n  Mutation Operator in MOEA/D-DE", "comments": "This is an accepted version of a paper published in Soft Computing", "journal-ref": null, "doi": "10.1007/s00500-019-03842-6", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decomposition-based multi-objective evolutionary algorithm with a\ndifferential evolution variation operator (MOEA/D-DE) shows high performance on\nchallenging multi-objective problems (MOPs). The DE mutation consists of three\nkey components: a mutation strategy, an index selection method for parent\nindividuals, and a bound-handling method. However, the configuration of the DE\nmutation operator that should be used for MOEA/D-DE has not been thoroughly\ninvestigated in the literature. This configuration choice confuses researchers\nand users of MOEA/D-DE. To address this issue, we present a review of the\nexisting configurations of the DE mutation operator in MOEA/D-DE and\nsystematically examine the influence of each component on the performance of\nMOEA/D-DE. Our review reveals that the configuration of the DE mutation\noperator differs depending on the source code of MOEA/D-DE. In our analysis, a\ntotal of 30 configurations (three index selection methods, two mutation\nstrategies, and five bound handling methods) are investigated on 16 MOPs with\nup to five objectives. Results show that each component significantly affects\nthe performance of MOEA/D-DE. We also present the most suitable configuration\nof the DE mutation operator, which maximizes the effectiveness of MOEA/D-DE.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 09:16:30 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2010.00525", "submitter": "David Lipshutz", "authors": "David Lipshutz, Yanis Bahroun, Siavash Golkar, Anirvan M. Sengupta,\n  Dmitri B. Chklovskii", "title": "A biologically plausible neural network for multi-channel Canonical\n  Correlation Analysis", "comments": "46 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical pyramidal neurons receive inputs from multiple distinct neural\npopulations and integrate these inputs in separate dendritic compartments. We\nexplore the possibility that cortical microcircuits implement Canonical\nCorrelation Analysis (CCA), an unsupervised learning method that projects the\ninputs onto a common subspace so as to maximize the correlations between the\nprojections. To this end, we seek a multi-channel CCA algorithm that can be\nimplemented in a biologically plausible neural network. For biological\nplausibility, we require that the network operates in the online setting and\nits synaptic update rules are local. Starting from a novel CCA objective\nfunction, we derive an online optimization algorithm whose optimization steps\ncan be implemented in a single-layer neural network with multi-compartmental\nneurons and local non-Hebbian learning rules. We also derive an extension of\nour online CCA algorithm with adaptive output rank and output whitening.\nInterestingly, the extension maps onto a neural network whose neural\narchitecture and synaptic updates resemble neural circuitry and synaptic\nplasticity observed experimentally in cortical pyramidal neurons.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:17:53 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 22:27:18 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 16:52:07 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 16:18:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lipshutz", "David", ""], ["Bahroun", "Yanis", ""], ["Golkar", "Siavash", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.00788", "submitter": "Santiago Gonzalez", "authors": "Santiago Gonzalez and Risto Miikkulainen", "title": "Effective Regularization Through Loss-Function Metalearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss-function metalearning can be used to discover novel, customized loss\nfunctions for deep neural networks, resulting in improved performance, faster\ntraining, and improved data utilization. A likely explanation is that such\nfunctions discourage overfitting, leading to effective regularization. This\npaper theoretically demonstrates that this is indeed the case: decomposition of\nlearning rules makes it possible to characterize the training dynamics and show\nthat loss functions evolved through TaylorGLO regularize both in the beginning\nand end of learning, and maintain an invariant in between. The invariant can be\nutilized to make the metalearning process more efficient in practice, and the\nregularization can train networks that are robust against adversarial attacks.\nLoss-function optimization can thus be seen as a well-founded new aspect of\nmetalearning in neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:22:21 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gonzalez", "Santiago", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2010.00818", "submitter": "Ryoji Tanabe", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "An Analysis of Control Parameters of MOEA/D Under Two Different\n  Optimization Scenarios", "comments": "This is an accepted version of a paper published in Applied Soft\n  Computing", "journal-ref": null, "doi": "10.1016/j.asoc.2018.05.014", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unbounded external archive (UEA), which stores all nondominated solutions\nfound during the search process, is frequently used to evaluate the performance\nof multi-objective evolutionary algorithms (MOEAs) in recent studies. A recent\nbenchmarking study also shows that decomposition-based MOEA (MOEA/D) is\ncompetitive with state-of-the-art MOEAs when the UEA is incorporated into\nMOEA/D. However, a parameter study of MOEA/D using the UEA has not yet been\nperformed. Thus, it is unclear how control parameter settings influence the\nperformance of MOEA/D with the UEA. In this paper, we present an analysis of\ncontrol parameters of MOEA/D under two performance evaluation scenarios. One is\na final population scenario where the performance assessment of MOEAs is\nperformed based on all nondominated solutions in the final population, and the\nother is a reduced UEA scenario where it is based on a pre-specified number of\nselected nondominated solutions from the UEA. Control parameters of MOEA/D\ninvestigated in this paper include the population size, scalarizing functions,\nand the penalty parameter of the penalty-based boundary intersection (PBI)\nfunction. Experimental results indicate that suitable settings of the three\ncontrol parameters significantly depend on the choice of an optimization\nscenario. We also analyze the reason why the best parameter setting is totally\ndifferent for each scenario.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:35:35 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2010.00826", "submitter": "Patrick Kenekayoro", "authors": "Patrick Kenekayoro", "title": "Incorporating Machine Learning to Evaluate Solutions to the University\n  Course Timetabling Problem", "comments": null, "journal-ref": "Covenant Journal of Informatics & Communication Technology (2019)\n  7(1) 18-35", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating solutions to optimization problems is arguably the most important\nstep for heuristic algorithms, as it is used to guide the algorithms towards\nthe optimal solution in the solution search space. Research has shown\nevaluation functions to some optimization problems to be impractical to compute\nand have thus found surrogate less expensive evaluation functions to those\nproblems. This study investigates the extent to which supervised learning\nalgorithms can be used to find approximations to evaluation functions for the\nuniversity course timetabling problem. Up to 97 percent of the time, the\ntraditional evaluation function agreed with the supervised learning regression\nmodel on the result of comparison of the quality of pair of solutions to the\nuniversity course timetabling problem, suggesting that supervised learning\nregression models can be suitable alternatives for optimization problems'\nevaluation functions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:44:04 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Kenekayoro", "Patrick", ""]]}, {"id": "2010.00918", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Are Artificial Dendrites useful in NeuroEvolution?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant role of dendritic processing within neuronal networks has\nbecome increasingly clear. This letter explores the effects of including a\nsimple dendrite-inspired mechanism into neuroevolution. The phenomenon of\nseparate dendrite activation thresholds on connections is allowed to emerge\nunder an evolutionary process. It is shown how such processing can be\npositively selected for, particularly for connections between the hidden and\noutput layer, and increases performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:53:46 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 12:42:21 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "2010.00951", "submitter": "T. Konstantin Rusch", "authors": "T. Konstantin Rusch, Siddhartha Mishra", "title": "Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and\n  (gradient) stable architecture for learning long time dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circuits of biological neurons, such as in the functional parts of the brain\ncan be modeled as networks of coupled oscillators. Inspired by the ability of\nthese systems to express a rich set of outputs while keeping (gradients of)\nstate variables bounded, we propose a novel architecture for recurrent neural\nnetworks. Our proposed RNN is based on a time-discretization of a system of\nsecond-order ordinary differential equations, modeling networks of controlled\nnonlinear oscillators. We prove precise bounds on the gradients of the hidden\nstates, leading to the mitigation of the exploding and vanishing gradient\nproblem for this RNN. Experiments show that the proposed RNN is comparable in\nperformance to the state of the art on a variety of benchmarks, demonstrating\nthe potential of this architecture to provide stable and accurate RNNs for\nprocessing complex sequential data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:35:04 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 19:12:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Rusch", "T. Konstantin", ""], ["Mishra", "Siddhartha", ""]]}, {"id": "2010.01004", "submitter": "Pascal Kerschke", "authors": "Vera Steinhoff and Pascal Kerschke and Pelin Aspar and Heike Trautmann\n  and Christian Grimme", "title": "Multiobjectivization of Local Search: Single-Objective Optimization\n  Benefits From Multi-Objective Gradient Descent", "comments": "This version has been accepted for publication at the IEEE Symposium\n  Series on Computational Intelligence (IEEE SSCI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodality is one of the biggest difficulties for optimization as local\noptima are often preventing algorithms from making progress. This does not only\nchallenge local strategies that can get stuck. It also hinders meta-heuristics\nlike evolutionary algorithms in convergence to the global optimum. In this\npaper we present a new concept of gradient descent, which is able to escape\nlocal traps. It relies on multiobjectivization of the original problem and\napplies the recently proposed and here slightly modified multi-objective local\nsearch mechanism MOGSA. We use a sophisticated visualization technique for\nmulti-objective problems to prove the working principle of our idea. As such,\nthis work highlights the transfer of new insights from the multi-objective to\nthe single-objective domain and provides first visual evidence that\nmultiobjectivization can link single-objective local optima in multimodal\nlandscapes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:56:44 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Steinhoff", "Vera", ""], ["Kerschke", "Pascal", ""], ["Aspar", "Pelin", ""], ["Trautmann", "Heike", ""], ["Grimme", "Christian", ""]]}, {"id": "2010.01032", "submitter": "Ryoji Tanabe", "authors": "Ryoji Tanabe and Alex Fukunaga", "title": "How Far Are We From an Optimal, Adaptive DE?", "comments": "This is an accepted version of a paper published in the proceedings\n  of PPSN 2016", "journal-ref": null, "doi": "10.1007/978-3-319-45823-6_14", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how an (almost) optimal parameter adaptation process for an\nadaptive DE might behave, and compare the behavior and performance of this\napproximately optimal process to that of existing, adaptive mechanisms for DE.\nAn optimal parameter adaptation process is an useful notion for analyzing the\nparameter adaptation methods in adaptive DE as well as other adaptive\nevolutionary algorithms, but it cannot be known generally. Thus, we propose a\nGreedy Approximate Oracle method (GAO) which approximates an optimal parameter\nadaptation process. We compare the behavior of GAODE, a DE algorithm with GAO,\nto typical adaptive DEs on six benchmark functions and the BBOB benchmarks, and\nshow that GAO can be used to (1) explore how much room for improvement there is\nin the performance of the adaptive DEs, and (2) obtain hints for developing\nfuture, effective parameter adaptation methods for adaptive DEs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:38:29 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Fukunaga", "Alex", ""]]}, {"id": "2010.01035", "submitter": "Ryoji Tanabe", "authors": "Ryoji Tanabe and Alex Fukunaga", "title": "Reviewing and Benchmarking Parameter Control Methods in Differential\n  Evolution", "comments": "This is an accepted version of a paper published in the IEEE\n  Transactions on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2019.2892735", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Differential Evolution (DE) algorithms with various parameter control\nmethods (PCMs) have been proposed. However, previous studies usually considered\nPCMs to be an integral component of a complex DE algorithm. Thus the\ncharacteristics and performance of each method are poorly understood. We\npresent an in-depth review of 24 PCMs for the scale factor and crossover rate\nin DE and a large scale benchmarking study. We carefully extract the 24 PCMs\nfrom their original, complex algorithms and describe them according to a\nsystematic manner. Our review facilitates the understanding of similarities and\ndifferences between existing, representative PCMs. The performance of DEs with\nthe 24 PCMs and 16 variation operators is investigated on 24 black-box\nbenchmark functions. Our benchmarking results reveal which methods exhibit high\nperformance when embedded in a standardized framework under 16 different\nconditions, independent from their original, complex algorithms. We also\ninvestigate how much room there is for further improvement of PCMs by comparing\nthe 24 methods with an oracle-based model, which can be considered to be a\nconservative lower bound on the performance of an optimal method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:52:44 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Fukunaga", "Alex", ""]]}, {"id": "2010.01039", "submitter": "Grzegorz G{\\l}uch", "authors": "Grzegorz G{\\l}uch, R\\\"udiger Urbanke", "title": "Query complexity of adversarial attacks", "comments": "32 pages, 2 figures Generalized the results. Adversarial examples no\n  longer need to be in the support of the data distribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main attack models considered in the adversarial robustness\nliterature: black-box and white-box. We consider these threat models as two\nends of a fine-grained spectrum, indexed by the number of queries the adversary\ncan ask. Using this point of view we investigate how many queries the adversary\nneeds to make to design an attack that is comparable to the best possible\nattack in the white-box model. We give a lower bound on that number of queries\nin terms of entropy of decision boundaries of the classifier. Using this result\nwe analyze two classical learning algorithms on two synthetic tasks for which\nwe prove meaningful security guarantees. The obtained bounds suggest that some\nlearning algorithms are inherently more robust against query-bounded\nadversaries than others.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:01:29 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 14:38:56 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Urbanke", "R\u00fcdiger", ""]]}, {"id": "2010.01200", "submitter": "Shikhar Gupta", "authors": "Shikhar Gupta, Arpan Vyas, Gaurav Trivedi", "title": "FPGA Implementation of Simplified Spiking Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNN) are third-generation Artificial Neural Networks\n(ANN) which are close to the biological neural system. In recent years SNN has\nbecome popular in the area of robotics and embedded applications, therefore, it\nhas become imperative to explore its real-time and energy-efficient\nimplementations. SNNs are more powerful than their predecessors because they\nencode temporal information and use biologically plausible plasticity rules. In\nthis paper, a simpler and computationally efficient SNN model using FPGA\narchitecture is described. The proposed model is validated on a Xilinx Virtex 6\nFPGA and analyzes a fully connected network which consists of 800 neurons and\n12,544 synapses in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 21:02:35 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gupta", "Shikhar", ""], ["Vyas", "Arpan", ""], ["Trivedi", "Gaurav", ""]]}, {"id": "2010.01472", "submitter": "Pavel Tolmachev", "authors": "Pavel Tolmachev and Jonathan H. Manton", "title": "New Insights on Learning Rules for Hopfield Networks: Memory and\n  Objective Function Minimisation", "comments": "8 pages, IEEE-Xplore, 2020 International Joint Conference on Neural\n  Networks (IJCNN), Glasgow", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  Glasgow, United Kingdom, 2020, pp. 1-8", "doi": "10.1109/IJCNN48605.2020.9207405", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hopfield neural networks are a possible basis for modelling associative\nmemory in living organisms. After summarising previous studies in the field, we\ntake a new look at learning rules, exhibiting them as descent-type algorithms\nfor various cost functions. We also propose several new cost functions suitable\nfor learning. We discuss the role of biases (the external inputs) in the\nlearning process in Hopfield networks. Furthermore, we apply Newtons method for\nlearning memories, and experimentally compare the performances of various\nlearning rules. Finally, to add to the debate whether allowing connections of a\nneuron to itself enhances memory capacity, we numerically investigate the\neffects of self coupling.\n  Keywords: Hopfield Networks, associative memory, content addressable memory,\nlearning rules, gradient descent, attractor networks\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:02:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tolmachev", "Pavel", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "2010.01524", "submitter": "Ofer Shir", "authors": "Ofer M. Shir and Xi Xing and Herschel Rabitz", "title": "Multi-Level Evolution Strategies for High-Resolution Black-Box Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a multi-level (m-lev) mechanism into Evolution\nStrategies (ESs) in order to address a class of global optimization problems\nthat could benefit from fine discretization of their decision variables. Such\nproblems arise in engineering and scientific applications, which possess a\nmulti-resolution control nature, and thus may be formulated either by means of\nlow-resolution variants (providing coarser approximations with presumably lower\naccuracy for the general problem) or by high-resolution controls. A particular\nscientific application concerns practical Quantum Control (QC) problems, whose\ntargeted optimal controls may be discretized to increasingly higher resolution,\nwhich in turn carries the potential to obtain better control yields. However,\nstate-of-the-art derivative-free optimization heuristics for high-resolution\nformulations nominally call for an impractically large number of objective\nfunction calls. Therefore, an effective algorithmic treatment for such problems\nis needed. We introduce a framework with an automated scheme to facilitate\nguided-search over increasingly finer levels of control resolution for the\noptimization problem, whose on-the-fly learned parameters require careful\nadaptation. We instantiate the proposed m-lev self-adaptive ES framework by two\nspecific strategies, namely the classical elitist single-child (1+1)-ES and the\nnon-elitist multi-child derandomized $(\\mu_W,\\lambda)$-sep-CMA-ES. We first\nshow that the approach is suitable by simulation-based optimization of QC\nsystems which were heretofore viewed as too complex to address. We also present\na laboratory proof-of-concept for the proposed approach on a basic experimental\nQC system objective.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 09:24:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shir", "Ofer M.", ""], ["Xing", "Xi", ""], ["Rabitz", "Herschel", ""]]}, {"id": "2010.01693", "submitter": "Oluwatobi Olabiyi", "authors": "Oluwatobi O. Olabiyi, Prarthana Bhattarai, C. Bayan Bruss, Zachary\n  Kulis", "title": "DLGNet-Task: An End-to-end Neural Network Framework for Modeling\n  Multi-turn Multi-domain Task-Oriented Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Task oriented dialogue (TOD) requires the complex interleaving of a number of\nindividually controllable components with strong guarantees for explainability\nand verifiability. This has made it difficult to adopt the multi-turn\nmulti-domain dialogue generation capabilities of streamlined end-to-end\nopen-domain dialogue systems. In this paper, we present a new framework,\nDLGNet-Task, a unified task-oriented dialogue system which employs\nautoregressive transformer networks such as DLGNet and GPT-2/3 to complete user\ntasks in multi-turn multi-domain conversations. Our framework enjoys the\ncontrollable, verifiable, and explainable outputs of modular approaches, and\nthe low development, deployment and maintenance cost of end-to-end systems.\nTreating open-domain system components as additional TOD system modules allows\nDLGNet-Task to learn the joint distribution of the inputs and outputs of all\nthe functional blocks of existing modular approaches such as, natural language\nunderstanding (NLU), state tracking, action policy, as well as natural language\ngeneration (NLG). Rather than training the modules individually, as is common\nin real-world systems, we trained them jointly with appropriate module\nseparations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows\ncomparable performance to the existing state-of-the-art approaches.\nFurthermore, using DLGNet-Task in conversational AI systems reduces the level\nof effort required for developing, deploying, and maintaining intelligent\nassistants at scale.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 21:43:17 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:31:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Olabiyi", "Oluwatobi O.", ""], ["Bhattarai", "Prarthana", ""], ["Bruss", "C. Bayan", ""], ["Kulis", "Zachary", ""]]}, {"id": "2010.01700", "submitter": "Emery Berger", "authors": "Breanna Devore-McDonald and Emery D. Berger", "title": "Mossad: Defeating Software Plagiarism Detection", "comments": "30 pages. To appear, OOPSLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic software plagiarism detection tools are widely used in educational\nsettings to ensure that submitted work was not copied. These tools have grown\nin use together with the rise in enrollments in computer science programs and\nthe widespread availability of code on-line. Educators rely on the robustness\nof plagiarism detection tools; the working assumption is that the effort\nrequired to evade detection is as high as that required to actually do the\nassigned work.\n  This paper shows this is not the case. It presents an entirely automatic\nprogram transformation approach, Mossad, that defeats popular software\nplagiarism detection tools. Mossad comprises a framework that couples\ntechniques inspired by genetic programming with domain-specific knowledge to\neffectively undermine plagiarism detectors. Mossad is effective at defeating\nfour plagiarism detectors, including Moss and JPlag. Mossad is both fast and\neffective: it can, in minutes, generate modified versions of programs that are\nlikely to escape detection. More insidiously, because of its non-deterministic\napproach, Mossad can, from a single program, generate dozens of variants, which\nare classified as no more suspicious than legitimate assignments. A detailed\nstudy of Mossad across a corpus of real student assignments demonstrates its\nefficacy at evading detection. A user study shows that graduate student\nassistants consistently rate Mossad-generated code as just as readable as\nauthentic student code. This work motivates the need for both research on more\nrobust plagiarism detection tools and greater integration of naturally\nplagiarism-resistant methodologies like code review into computer science\neducation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:02:38 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Devore-McDonald", "Breanna", ""], ["Berger", "Emery D.", ""]]}, {"id": "2010.01729", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Priyadarshini Panda", "title": "Revisiting Batch Normalization for Training Low-latency Deep Spiking\n  Neural Networks from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have recently emerged as an alternative to\ndeep learning owing to sparse, asynchronous and binary event (or spike) driven\nprocessing, that can yield huge energy efficiency benefits on neuromorphic\nhardware. However, training high-accuracy and low-latency SNNs from scratch\nsuffers from non-differentiable nature of a spiking neuron. To address this\ntraining issue in SNNs, we revisit batch normalization and propose a temporal\nBatch Normalization Through Time (BNTT) technique. Most prior SNN works till\nnow have disregarded batch normalization deeming it ineffective for training\ntemporal SNNs. Different from previous works, our proposed BNTT decouples the\nparameters in a BNTT layer along the time axis to capture the temporal dynamics\nof spikes. The temporally evolving learnable parameters in BNTT allow a neuron\nto control its spike rate through different time-steps, enabling low-latency\nand low-energy training from scratch. We conduct experiments on CIFAR-10,\nCIFAR-100, Tiny-ImageNet and event-driven DVS-CIFAR10 datasets. BNTT allows us\nto train deep SNN architectures from scratch, for the first time, on complex\ndatasets with just few 25-30 time-steps. We also propose an early exit\nalgorithm using the distribution of parameters in BNTT to reduce the latency at\ninference, that further improves the energy-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 00:49:30 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 21:42:42 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 17:14:42 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 00:36:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kim", "Youngeun", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2010.01851", "submitter": "David Holzm\\\"uller", "authors": "David Holzm\\\"uller", "title": "On the Universality of the Double Descent Peak in Ridgeless Regression", "comments": "Accepted at ICLR 2021. 9 pages + 34 pages appendix. Changes in v5:\n  Added link to repository with generated data. Experimental results can be\n  reproduced using the code at\n  https://github.com/dholzmueller/universal_double_descent", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a non-asymptotic distribution-independent lower bound for the\nexpected mean squared generalization error caused by label noise in ridgeless\nlinear regression. Our lower bound generalizes a similar known result to the\noverparameterized (interpolating) regime. In contrast to most previous works,\nour analysis applies to a broad class of input distributions with almost surely\nfull-rank feature matrices, which allows us to cover various types of\ndeterministic or random feature maps. Our lower bound is asymptotically sharp\nand implies that in the presence of label noise, ridgeless linear regression\ndoes not perform well around the interpolation threshold for any of these\nfeature maps. We analyze the imposed assumptions in detail and provide a theory\nfor analytic (random) feature maps. Using this theory, we can show that our\nassumptions are satisfied for input distributions with a (Lebesgue) density and\nfeature maps given by random deep neural networks with analytic activation\nfunctions like sigmoid, tanh, softplus or GELU. As further examples, we show\nthat feature maps from random Fourier features and polynomial kernels also\nsatisfy our assumptions. We complement our theory with further experimental and\nanalytic results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:30:25 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:09:03 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 13:56:02 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 17:15:33 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 10:33:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Holzm\u00fcller", "David", ""]]}, {"id": "2010.01877", "submitter": "Ryoji Tanabe", "authors": "Ryoji Tanabe and Alex Fukunaga", "title": "TPAM: A Simulation-Based Model for Quantitatively Analyzing Parameter\n  Adaptation Methods", "comments": "This is an accepted version of a paper published in the proceedings\n  of GECCO 2017", "journal-ref": null, "doi": "10.1145/3071178.3071226", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a large number of adaptive Differential Evolution (DE) algorithms have\nbeen proposed, their Parameter Adaptation Methods (PAMs) are not well\nunderstood. We propose a Target function-based PAM simulation (TPAM) framework\nfor evaluating the tracking performance of PAMs. The proposed TPAM simulation\nframework measures the ability of PAMs to track predefined target parameters,\nthus enabling quantitative analysis of the adaptive behavior of PAMs. We\nevaluate the tracking performance of PAMs of widely used five adaptive DEs\n(jDE, EPSDE, JADE, MDE, and SHADE) on the proposed TPAM, and show that TPAM can\nprovide important insights on PAMs, e.g., why the PAM of SHADE performs better\nthan that of JADE, and under what conditions the PAM of EPSDE fails at\nparameter adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:25:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Fukunaga", "Alex", ""]]}, {"id": "2010.01939", "submitter": "Manuel Le Gallo", "authors": "Geethan Karunaratne, Manuel Schmuck, Manuel Le Gallo, Giovanni\n  Cherubini, Luca Benini, Abu Sebastian, Abbas Rahimi", "title": "Robust High-dimensional Memory-augmented Neural Networks", "comments": "This is a pre-print of an article accepted for publication in Nature\n  Communications", "journal-ref": "Nature Communications volume 12, Article number: 2468 (2021)", "doi": "10.1038/s41467-021-22364-0", "report-no": null, "categories": "cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional neural networks require enormous amounts of data to build their\ncomplex mappings during a slow training procedure that hinders their abilities\nfor relearning and adapting to new data. Memory-augmented neural networks\nenhance neural networks with an explicit memory to overcome these issues.\nAccess to this explicit memory, however, occurs via soft read and write\noperations involving every individual memory entry, resulting in a bottleneck\nwhen implemented using the conventional von Neumann computer architecture. To\novercome this bottleneck, we propose a robust architecture that employs a\ncomputational memory unit as the explicit memory performing analog in-memory\ncomputation on high-dimensional (HD) vectors, while closely matching 32-bit\nsoftware-equivalent accuracy. This is achieved by a content-based attention\nmechanism that represents unrelated items in the computational memory with\nuncorrelated HD vectors, whose real-valued components can be readily\napproximated by binary, or bipolar components. Experimental results demonstrate\nthe efficacy of our approach on few-shot image classification tasks on the\nOmniglot dataset using more than 256,000 phase-change memory devices. Our\napproach effectively merges the richness of deep neural network representations\nwith HD computing that paves the way for robust vector-symbolic manipulations\napplicable in reasoning, fusion, and compression.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:01:56 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:46:59 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Karunaratne", "Geethan", ""], ["Schmuck", "Manuel", ""], ["Gallo", "Manuel Le", ""], ["Cherubini", "Giovanni", ""], ["Benini", "Luca", ""], ["Sebastian", "Abu", ""], ["Rahimi", "Abbas", ""]]}, {"id": "2010.02039", "submitter": "Manh Duong Phung", "authors": "Manh Duong Phung, Quang Phuc Ha", "title": "Motion-Encoded Particle Swarm Optimization for Moving Target Search\n  Using UAVs", "comments": "Applied Soft Computing, 2020", "journal-ref": null, "doi": "10.1016/j.asoc.2020.106705", "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm named the motion-encoded particle swarm\noptimization (MPSO) for finding a moving target with unmanned aerial vehicles\n(UAVs). From the Bayesian theory, the search problem can be converted to the\noptimization of a cost function that represents the probability of detecting\nthe target. Here, the proposed MPSO is developed to solve that problem by\nencoding the search trajectory as a series of UAV motion paths evolving over\nthe generation of particles in a PSO algorithm. This motion-encoded approach\nallows for preserving important properties of the swarm including the cognitive\nand social coherence, and thus resulting in better solutions. Results from\nextensive simulations with existing methods show that the proposed MPSO\nimproves the detection performance by 24\\% and time performance by 4.71 times\ncompared to the original PSO, and moreover, also outperforms other\nstate-of-the-art metaheuristic optimization algorithms including the artificial\nbee colony (ABC), ant colony optimization (ACO), genetic algorithm (GA),\ndifferential evolution (DE), and tree-seed algorithm (TSA) in most search\nscenarios. Experiments have been conducted with real UAVs in searching for a\ndynamic target in different scenarios to demonstrate MPSO merits in a practical\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:17:49 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Phung", "Manh Duong", ""], ["Ha", "Quang Phuc", ""]]}, {"id": "2010.02066", "submitter": "R\\'obert Csord\\'as", "authors": "R\\'obert Csord\\'as, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Are Neural Nets Modular? Inspecting Functional Modularity Through\n  Differentiable Weight Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) whose subnetworks implement reusable functions are\nexpected to offer numerous advantages, including compositionality through\nefficient recombination of functional building blocks, interpretability,\npreventing catastrophic interference, etc. Understanding if and how NNs are\nmodular could provide insights into how to improve them. Current inspection\nmethods, however, fail to link modules to their functionality. In this paper,\nwe present a novel method based on learning binary weight masks to identify\nindividual weights and subnets responsible for specific functions. Using this\npowerful tool, we contribute an extensive study of emerging modularity in NNs\nthat covers several standard architectures and datasets. We demonstrate how\ncommon NNs fail to reuse submodules and offer new insights into the related\nissue of systematic generalization on language tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:04:11 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 07:24:16 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 17:35:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Csord\u00e1s", "R\u00f3bert", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2010.02244", "submitter": "Jamal Toutouh", "authors": "Jamal Toutouh", "title": "Conditional Generative Adversarial Networks to Model Urban Outdoor Air\n  Pollution", "comments": "Submitted to ICSC-CITIES 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a relevant problem because the design of most cities prioritizes the\nuse of motorized vehicles, which has degraded air quality in recent years,\nhaving a negative effect on urban health. Modeling, predicting, and forecasting\nambient air pollution is an important way to deal with this issue because it\nwould be helpful for decision-makers and urban city planners to understand the\nphenomena and to take solutions. In general, data-driven methods for modeling,\npredicting, and forecasting outdoor pollution requires an important amount of\ndata, which may limit their accuracy. In order to deal with such a lack of\ndata, we propose to train models able to generate synthetic nitrogen dioxide\ndaily time series according to a given classification that will allow an\nunlimited generation of realistic data. The main experimental results indicate\nthat the proposed approach is able to generate accurate and diverse pollution\ndaily time series, while requiring reduced computational time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:01:10 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Toutouh", "Jamal", ""]]}, {"id": "2010.02354", "submitter": "Elliot Meyerson", "authors": "Elliot Meyerson and Risto Miikkulainen", "title": "The Traveling Observer Model: Multi-task Learning Through Spatial\n  Variable Embeddings", "comments": "Accepted for spotlight presentation as a conference paper at ICLR\n  2021. Main paper: 9 pages; with references: 12 pages; with appendix: 17\n  pages. Best viewed in color", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper frames a general prediction system as an observer traveling around\na continuous space, measuring values at some locations, and predicting them at\nothers. The observer is completely agnostic about any particular task being\nsolved; it cares only about measurement locations and their values. This\nperspective leads to a machine learning framework in which seemingly unrelated\ntasks can be solved by a single model, by embedding their input and output\nvariables into a shared space. An implementation of the framework is developed\nin which these variable embeddings are learned jointly with internal model\nparameters. In experiments, the approach is shown to (1) recover intuitive\nlocations of variables in space and time, (2) exploit regularities across\nrelated datasets with completely disjoint input and output spaces, and (3)\nexploit regularities across seemingly unrelated tasks, outperforming\ntask-specific single-task models and multi-task learning alternatives. The\nresults suggest that even seemingly unrelated tasks may originate from similar\nunderlying processes, a fact that the traveling observer model can use to make\nbetter predictions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:51:37 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 02:27:48 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 23:22:23 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 23:11:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2010.02369", "submitter": "Aigerim Bogyrbayeva", "authors": "Aigerim Bogyrbayeva, Sungwook Jang, Ankit Shah, Young Jae Jang,\n  Changhyun Kwon", "title": "A Reinforcement Learning Approach for Rebalancing Electric Vehicle\n  Sharing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a reinforcement learning approach for nightly offline\nrebalancing operations in free-floating electric vehicle sharing systems\n(FFEVSS). Due to sparse demand in a network, FFEVSS require relocation of\nelectrical vehicles (EVs) to charging stations and demander nodes, which is\ntypically done by a group of drivers. A shuttle is used to pick up and drop off\ndrivers throughout the network. The objective of this study is to solve the\nshuttle routing problem to finish the rebalancing work in the minimal time. We\nconsider a reinforcement learning framework for the problem, in which a central\ncontroller determines the routing policies of a fleet of multiple shuttles. We\ndeploy a policy gradient method for training recurrent neural networks and\ncompare the obtained policy results with heuristic solutions. Our numerical\nstudies show that unlike the existing solutions in the literature, the proposed\nmethods allow to solve the general version of the problem with no restrictions\non the urban EV network structure and charging requirements of EVs. Moreover,\nthe learned policies offer a wide range of flexibility resulting in a\nsignificant reduction in the time needed to rebalance the network.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:24:36 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 14:14:30 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bogyrbayeva", "Aigerim", ""], ["Jang", "Sungwook", ""], ["Shah", "Ankit", ""], ["Jang", "Young Jae", ""], ["Kwon", "Changhyun", ""]]}, {"id": "2010.02634", "submitter": "Ethan Harris", "authors": "Ethan Harris, Daniela Mihai, Jonathon Hare", "title": "How Convolutional Neural Network Architecture Biases Learned Opponency\n  and Colour Tuning", "comments": "Final version; Accepted for publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests that changing Convolutional Neural Network (CNN)\narchitecture by introducing a bottleneck in the second layer can yield changes\nin learned function. To understand this relationship fully requires a way of\nquantitatively comparing trained networks. The fields of electrophysiology and\npsychophysics have developed a wealth of methods for characterising visual\nsystems which permit such comparisons. Inspired by these methods, we propose an\napproach to obtaining spatial and colour tuning curves for convolutional\nneurons, which can be used to classify cells in terms of their spatial and\ncolour opponency. We perform these classifications for a range of CNNs with\ndifferent depths and bottleneck widths. Our key finding is that networks with a\nbottleneck show a strong functional organisation: almost all cells in the\nbottleneck layer become both spatially and colour opponent, cells in the layer\nfollowing the bottleneck become non-opponent. The colour tuning data can\nfurther be used to form a rich understanding of how colour is encoded by a\nnetwork. As a concrete demonstration, we show that shallower networks without a\nbottleneck learn a complex non-linear colour system, whereas deeper networks\nwith tight bottlenecks learn a simple channel opponent code in the bottleneck\nlayer. We further develop a method of obtaining a hue sensitivity curve for a\ntrained CNN which enables high level insights that complement the low level\nfindings from the colour tuning data. We go on to train a series of networks\nunder different conditions to ascertain the robustness of the discussed\nresults. Ultimately, our methods and findings coalesce with prior art,\nstrengthening our ability to interpret trained CNNs and furthering our\nunderstanding of the connection between architecture and learned\nrepresentation. Code for all experiments is available at\nhttps://github.com/ecs-vlc/opponency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:33:48 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Harris", "Ethan", ""], ["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2010.02684", "submitter": "Alvin Chan", "authors": "Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang", "title": "Poison Attacks against Text Datasets with Conditional Adversarially\n  Regularized Autoencoder", "comments": "Accepted in EMNLP-Findings 2020, Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates a fatal vulnerability in natural language inference\n(NLI) and text classification systems. More concretely, we present a 'backdoor\npoisoning' attack on NLP models. Our poisoning attack utilizes conditional\nadversarially regularized autoencoder (CARA) to generate poisoned training\nsamples by poison injection in latent space. Just by adding 1% poisoned data,\nour experiments show that a victim BERT finetuned classifier's predictions can\nbe steered to the poison target class with success rates of >80% when the input\nhypothesis is injected with the poison signature, demonstrating that NLI and\ntext classification systems face a huge security risk.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:03:49 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chan", "Alvin", ""], ["Tay", "Yi", ""], ["Ong", "Yew-Soon", ""], ["Zhang", "Aston", ""]]}, {"id": "2010.02820", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, Daria Zenkova, Stanislav Nikolov, Simon Osindero", "title": "From Language Games to Drawing Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attempt to automate various artistic processes by inventing a set of\ndrawing games, analogous to the approach taken by emergent language research in\ninventing communication games. A critical difference is that drawing games\ndemand much less effort from the receiver than do language games. Artists must\nwork with pre-trained viewers who spend little time learning artist specific\nrepresentational conventions, but who instead have a pre-trained visual system\noptimized for behaviour in the world by understanding to varying extents the\nenvironment's visual affordances. After considering various kinds of drawing\ngame we present some preliminary experiments which have generated images by\nclosing the generative-critical loop.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:32:32 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 11:09:47 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Zenkova", "Daria", ""], ["Nikolov", "Stanislav", ""], ["Osindero", "Simon", ""]]}, {"id": "2010.02860", "submitter": "Pietro Verzelli", "authors": "Pietro Verzelli and Cesare Alippi and Lorenzo Livi", "title": "Learn to Synchronize, Synchronize to Learn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the machine learning community has seen a continuous growing\ninterest in research aimed at investigating dynamical aspects of both training\nprocedures and machine learning models. Of particular interest among recurrent\nneural networks we have the Reservoir Computing (RC) paradigm characterized by\nconceptual simplicity and a fast training scheme. Yet, the guiding principles\nunder which RC operates are only partially understood. In this work, we analyze\nthe role played by Generalized Synchronization (GS) when training a RC to solve\na generic task. In particular, we show how GS allows the reservoir to correctly\nencode the system generating the input signal into its dynamics. We also\ndiscuss necessary and sufficient conditions for the learning to be feasible in\nthis approach. Moreover, we explore the role that ergodicity plays in this\nprocess, showing how its presence allows the learning outcome to apply to\nmultiple input trajectories. Finally, we show that satisfaction of the GS can\nbe measured by means of the Mutual False Nearest Neighbors index, which makes\neffective to practitioners theoretical derivations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:29:18 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 22:47:38 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 08:41:51 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Verzelli", "Pietro", ""], ["Alippi", "Cesare", ""], ["Livi", "Lorenzo", ""]]}, {"id": "2010.03140", "submitter": "Tielin Zhang", "authors": "Xiang Cheng and Tielin Zhang and Shuncheng Jia and Bo Xu", "title": "Finite Meta-Dynamic Neurons in Spiking Neural Networks for\n  Spatio-temporal Learning", "comments": "24 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have incorporated more biologically-plausible\nstructures and learning principles, hence are playing critical roles in\nbridging the gap between artificial and natural neural networks. The spikes are\nthe sparse signals describing the above-threshold event-based firing and\nunder-threshold dynamic computation of membrane potentials, which give us an\nalternative uniformed and efficient way on both information representation and\ncomputation. Inspired from the biological network, where a finite number of\nmeta neurons integrated together for various of cognitive functions, we\nproposed and constructed Meta-Dynamic Neurons (MDN) to improve SNNs for a\nbetter network generalization during spatio-temporal learning. The MDNs are\ndesigned with basic neuronal dynamics containing 1st-order and 2nd-order\ndynamics of membrane potentials, including the spatial and temporal meta types\nsupported by some hyper-parameters. The MDNs generated from a spatial (MNIST)\nand a temporal (TIDigits) datasets first, and then extended to various other\ndifferent spatio-temporal tasks (including Fashion-MNIST, NETtalk, Cifar-10,\nTIMIT and N-MNIST). The comparable accuracy was reached compared to other SOTA\nSNN algorithms, and a better generalization was also achieved by SNNs using\nMDNs than that without using MDNs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:49:28 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cheng", "Xiang", ""], ["Zhang", "Tielin", ""], ["Jia", "Shuncheng", ""], ["Xu", "Bo", ""]]}, {"id": "2010.03483", "submitter": "Waleed Bin Owais", "authors": "Waleed Bin Owais, Iyad W. J. Alkhazendar and Dr.Mohammad Saleh", "title": "Evaluating the impact of different types of crossover and selection\n  methods on the convergence of 0/1 Knapsack using Genetic Algorithm", "comments": "7th International Conference on Computer Science, Engineering and\n  Information Technology (CSEIT 2020) September 26 ~ 27, 2020, Copenhagen,\n  Denmark", "journal-ref": null, "doi": "10.5121/csit.2020.101101", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genetic Algorithm is an evolutionary algorithm and a metaheuristic that was\nintroduced to overcome the failure of gradient based method in solving the\noptimization and search problems. The purpose of this paper is to evaluate the\nimpact on the convergence of Genetic Algorithm vis-a-vis 0/1 knapsack. By\nkeeping the number of generations and the initial population fixed, different\ncrossover methods like one point crossover and two-point crossover were\nevaluated and juxtaposed with each other. In addition to this, the impact of\ndifferent selection methods like rank-selection, roulette wheel and tournament\nselection were evaluated and compared. Our results indicate that convergence\nrate of combination of one point crossover with tournament selection, with\nrespect to 0/1 knapsack problem that we considered, is the highest and thereby\nmost efficient in solving 0/1 knapsack.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:36:33 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Owais", "Waleed Bin", ""], ["Alkhazendar", "Iyad W. J.", ""], ["Saleh", "Dr. Mohammad", ""]]}, {"id": "2010.03585", "submitter": "Denis Kleyko", "authors": "Denis Kleyko, E. Paxon Frady, Friedrich T. Sommer", "title": "Cellular Automata Can Reduce Memory Requirements of Collective-State\n  Computing", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various non-classical approaches of distributed information processing, such\nas neural networks, computation with Ising models, reservoir computing, vector\nsymbolic architectures, and others, employ the principle of collective-state\ncomputing. In this type of computing, the variables relevant in a computation\nare superimposed into a single high-dimensional state vector, the\ncollective-state. The variable encoding uses a fixed set of random patterns,\nwhich has to be stored and kept available during the computation. Here we show\nthat an elementary cellular automaton with rule 90 (CA90) enables space-time\ntradeoff for collective-state computing models that use random dense binary\nrepresentations, i.e., memory requirements can be traded off with computation\nrunning CA90. We investigate the randomization behavior of CA90, in particular,\nthe relation between the length of the randomization period and the size of the\ngrid, and how CA90 preserves similarity in the presence of the initialization\nnoise. Based on these analyses we discuss how to optimize a collective-state\ncomputing model, in which CA90 expands representations on the fly from short\nseed patterns - rather than storing the full set of random patterns. The CA90\nexpansion is applied and tested in concrete scenarios using reservoir computing\nand vector symbolic architectures. Our experimental results show that\ncollective-state computing with CA90 expansion performs similarly compared to\ntraditional collective-state models, in which random patterns are generated\ninitially by a pseudo-random number generator and then stored in a large\nmemory.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:00:31 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Kleyko", "Denis", ""], ["Frady", "E. Paxon", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "2010.03733", "submitter": "Span Spanbauer", "authors": "Span Spanbauer, Luke Sciarappa", "title": "Neural Group Actions", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm for designing Neural Group Actions, collections of\ndeep neural network architectures which model symmetric transformations\nsatisfying the laws of a given finite group. This generalizes involutive neural\nnetworks $\\mathcal{N}$, which satisfy $\\mathcal{N}(\\mathcal{N}(x))=x$ for any\ndata $x$, the group law of $\\mathbb{Z}_2$. We show how to optionally enforce an\nadditional constraint that the group action be volume-preserving. We\nconjecture, by analogy to a universality result for involutive neural networks,\nthat generative models built from Neural Group Actions are universal\napproximators for collections of probabilistic transitions adhering to the\ngroup laws. We demonstrate experimentally that a Neural Group Action for the\nquaternion group $Q_8$ can learn how a set of nonuniversal quantum gates\nsatisfying the $Q_8$ group laws act on single qubit quantum states.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:27:05 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Spanbauer", "Span", ""], ["Sciarappa", "Luke", ""]]}, {"id": "2010.03795", "submitter": "Manojkumar Parmar", "authors": "Palak Sukharamwala and Manojkumar Parmar", "title": "Mapping of Real World Problems to Nature Inspired Algorithm using Goal\n  based Classification and TRIZ", "comments": "17 pages, 9 figures, 3 figures; Under review for publication as book\n  chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MA cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technologies and algorithms are growing at an exponential rate. The\ntechnologies are capable enough to solve technically challenging and complex\nproblems which seemed impossible task. However, the trending methods and\napproaches are facing multiple challenges on various fronts of data,\nalgorithms, software, computational complexities, and energy efficiencies.\nNature also faces similar challenges. Nature has solved those challenges and\nformulation of those are available as Nature Inspired Algorithms (NIA), which\nare derived based on the study of nature. A novel method based on TRIZ to map\nthe real-world problems to nature problems is explained here.TRIZ is a Theory\nof inventive problem solving. Using the proposed framework, best NIA can be\nidentified to solve the real-world problems. For this framework to work, a\nnovel classification of NIA based on the end goal that nature is trying to\nachieve is devised. The application of the this framework along with examples\nis also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 06:55:31 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Sukharamwala", "Palak", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2010.03834", "submitter": "Iztok Fister", "authors": "Iztok Fister Jr., Iztok Fister", "title": "Association rules over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisions made nowadays by Artificial Intelligence powered systems are\nusually hard for users to understand. One of the more important issues faced by\ndevelopers is exposed as how to create more explainable Machine Learning\nmodels. In line with this, more explainable techniques need to be developed,\nwhere visual explanation also plays a more important role. This technique could\nalso be applied successfully for explaining the results of Association Rule\nMining.This Chapter focuses on two issues: (1) How to discover the relevant\nassociation rules, and (2) How to express relations between more attributes\nvisually. For the solution of the first issue, the proposed method uses\nDifferential Evolution, while Sankey diagrams are adopted to solve the second\none. This method was applied to a transaction database containing data\ngenerated by an amateur cyclist in past seasons, using a mobile device worn\nduring the realization of training sessions that is divided into four time\nperiods. The results of visualization showed that a trend in improving\nperformance of an athlete can be indicated by changing the attributes appearing\nin the selected association rules in different time periods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:31:34 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Fister", "Iztok", "Jr."], ["Fister", "Iztok", ""]]}, {"id": "2010.03917", "submitter": "Eneko Osaba", "authors": "Eneko Osaba, Javier Del Ser, Aritz D. Martinez, Jesus L. Lobo and\n  Francisco Herrera", "title": "AT-MFCGA: An Adaptive Transfer-guided Multifactorial Cellular Genetic\n  Algorithm for Evolutionary Multitasking", "comments": "31 pages, 4 figures, paper accepted for being published in\n  Information Sciences journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer Optimization is an incipient research area dedicated to solving\nmultiple optimization tasks simultaneously. Among the different approaches that\ncan address this problem effectively, Evolutionary Multitasking resorts to\nconcepts from Evolutionary Computation to solve multiple problems within a\nsingle search process. In this paper we introduce a novel adaptive\nmetaheuristic algorithm to deal with Evolutionary Multitasking environments\ncoined as Adaptive Transfer-guided Multifactorial Cellular Genetic Algorithm\n(AT-MFCGA). AT-MFCGA relies on cellular automata to implement mechanisms in\norder to exchange knowledge among the optimization problems under\nconsideration. Furthermore, our approach is able to explain by itself the\nsynergies among tasks that were encountered and exploited during the search,\nwhich helps us to understand interactions between related optimization tasks. A\ncomprehensive experimental setup is designed to assess and compare the\nperformance of AT-MFCGA to that of other renowned evolutionary multitasking\nalternatives (MFEA and MFEA-II). Experiments comprise 11 multitasking scenarios\ncomposed of 20 instances of 4 combinatorial optimization problems, yielding the\nlargest discrete multitasking environment solved to date. Results are\nconclusive in regard to the superior quality of solutions provided by AT-MFCGA\nwith respect to the rest of the methods, which are complemented by a\nquantitative examination of the genetic transferability among tasks throughout\nthe search process.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 12:00:10 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 13:55:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Osaba", "Eneko", ""], ["Del Ser", "Javier", ""], ["Martinez", "Aritz D.", ""], ["Lobo", "Jesus L.", ""], ["Herrera", "Francisco", ""]]}, {"id": "2010.04216", "submitter": "Oriol Barbany Mayor", "authors": "Oriol Barbany Mayor", "title": "Affine-Invariant Robust Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of adversarial robustness has attracted significant attention in\nmachine learning. Contrary to the common approach of training models that are\naccurate in average case, it aims at training models that are accurate for\nworst case inputs, hence it yields more robust and reliable models. Put\ndifferently, it tries to prevent an adversary from fooling a model. The study\nof adversarial robustness is largely focused on $\\ell_p-$bounded adversarial\nperturbations, i.e. modifications of the inputs, bounded in some $\\ell_p$ norm.\nNevertheless, it has been shown that state-of-the-art models are also\nvulnerable to other more natural perturbations such as affine transformations,\nwhich were already considered in machine learning within data augmentation.\nThis project reviews previous work in spatial robustness methods and proposes\nevolution strategies as zeroth order optimization algorithms to find the worst\naffine transforms for each input. The proposed method effectively yields robust\nmodels and allows introducing non-parametric adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 18:59:19 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mayor", "Oriol Barbany", ""]]}, {"id": "2010.04261", "submitter": "Xingyu Zhu", "authors": "Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, Rong Ge", "title": "Dissecting Hessian: Understanding Common Structure of Hessian in Neural\n  Networks", "comments": "60 pages, 30 figures. Main text: 10 pages, 7 figures. First two\n  authors have equal contribution and are in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hessian captures important properties of the deep neural network loss\nlandscape. Previous works have observed low rank structure in the Hessians of\nneural networks. We make several new observations about the top eigenspace of\nlayer-wise Hessian: top eigenspaces for different models have surprisingly high\noverlap, and top eigenvectors form low rank matrices when they are reshaped\ninto the same shape as the corresponding weight matrix. Towards formally\nexplaining such structures of the Hessian, we show that the new eigenspace\nstructure can be explained by approximating the Hessian using Kronecker\nfactorization; we also prove the low rank structure for random data at random\ninitialization for over-parametrized two-layer neural nets. Our new\nunderstanding can explain why some of these structures become weaker when the\nnetwork is trained with batch normalization. The Kronecker factorization also\nleads to better explicit generalization bounds.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:18:11 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 18:02:21 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 08:44:47 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2021 15:39:47 GMT"}, {"version": "v5", "created": "Wed, 16 Jun 2021 15:27:49 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wu", "Yikai", ""], ["Zhu", "Xingyu", ""], ["Wu", "Chenwei", ""], ["Wang", "Annie", ""], ["Ge", "Rong", ""]]}, {"id": "2010.04340", "submitter": "Scott Field", "authors": "Dwyer S. Deighan, Scott E. Field, Collin D. Capano, Gaurav Khanna", "title": "Genetic-algorithm-optimized neural networks for gravitational wave\n  classification", "comments": "25 pages, 8 figures, and 2 tables; Version 2 includes an expanded\n  discussion of our hyperparameter optimization model", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gravitational-wave detection strategies are based on a signal analysis\ntechnique known as matched filtering. Despite the success of matched filtering,\ndue to its computational cost, there has been recent interest in developing\ndeep convolutional neural networks (CNNs) for signal detection. Designing these\nnetworks remains a challenge as most procedures adopt a trial and error\nstrategy to set the hyperparameter values. We propose a new method for\nhyperparameter optimization based on genetic algorithms (GAs). We compare six\ndifferent GA variants and explore different choices for the GA-optimized\nfitness score. We show that the GA can discover high-quality architectures when\nthe initial hyperparameter seed values are far from a good solution as well as\nrefining already good networks. For example, when starting from the\narchitecture proposed by George and Huerta, the network optimized over the\n20-dimensional hyperparameter space has 78% fewer trainable parameters while\nobtaining an 11% increase in accuracy for our test problem. Using genetic\nalgorithm optimization to refine an existing network should be especially\nuseful if the problem context (e.g. statistical properties of the noise, signal\nmodel, etc) changes and one needs to rebuild a network. In all of our\nexperiments, we find the GA discovers significantly less complicated networks\nas compared to the seed network, suggesting it can be used to prune wasteful\nnetwork structures. While we have restricted our attention to CNN classifiers,\nour GA hyperparameter optimization strategy can be applied within other machine\nlearning settings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:14:20 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 15:58:19 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Deighan", "Dwyer S.", ""], ["Field", "Scott E.", ""], ["Capano", "Collin D.", ""], ["Khanna", "Gaurav", ""]]}, {"id": "2010.04351", "submitter": "Thao Nguyen", "authors": "Thao N.N. Nguyen, Bharadwaj Veeravalli, Xuanyao Fong", "title": "Connection Pruning for Deep Spiking Neural Networks with On-Chip\n  Learning", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long training time hinders the potential of the deep Spiking Neural Network\n(SNN) with the online learning capability to be realized on the embedded\nsystems hardware. Our work proposes a novel connection pruning approach that\ncan be applied during the online Spike Timing Dependent Plasticity (STDP)-based\nlearning to optimize the learning time and the network connectivity of the SNN.\nOur connection pruning approach was evaluated on a deep SNN with the Time To\nFirst Spike (TTFS) coding and has successfully achieved 2.1x speed-up in the\nonline learning and reduced the network connectivity by 92.83%. The energy\nconsumption in the online learning was saved by 64%. Moreover, the connectivity\nreduction results in 2.83x speed-up and 78.24% energy saved in the inference.\nMeanwhile, the classification accuracy remains the same as our non-pruning\nbaseline on the Caltech 101 dataset. In addition, we developed an event-driven\nhardware architecture on the Field Programmable Gate Array (FPGA) platform that\nefficiently incorporates our proposed connection pruning approach while\nincurring as little as 0.56% power overhead. Moreover, we performed a\ncomparison between our work and the existing works on connection pruning for\nSNN to highlight the key features of each approach. To the best of our\nknowledge, our work is the first to propose a connection pruning algorithm that\ncan be applied during the online STDP-based learning for a deep SNN with the\nTTFS coding.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:44:42 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 12:44:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Nguyen", "Thao N. N.", ""], ["Veeravalli", "Bharadwaj", ""], ["Fong", "Xuanyao", ""]]}, {"id": "2010.04434", "submitter": "Tielin Zhang", "authors": "Tielin Zhang and Shuncheng Jia and Xiang Cheng and Bo Xu", "title": "Tuning Convolutional Spiking Neural Network with Biologically-plausible\n  Reward Propagation", "comments": "Final Version. Accepted by IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) contain more biologically realistic structures\nand biologically-inspired learning principles than those in standard Artificial\nNeural Networks (ANNs). SNNs are considered the third generation of ANNs,\npowerful on the robust computation with a low computational cost. The neurons\nin SNNs are non-differential, containing decayed historical states and\ngenerating event-based spikes after their states reaching the firing threshold.\nThese dynamic characteristics of SNNs make it difficult to be directly trained\nwith the standard backpropagation (BP), which is also considered not\nbiologically plausible. In this paper, a Biologically-plausible Reward\nPropagation (BRP) algorithm is proposed and applied to the SNN architecture\nwith both spiking-convolution (with both 1D and 2D convolutional kernels) and\nfull-connection layers. Unlike the standard BP that propagates error signals\nfrom post to presynaptic neurons layer by layer, the BRP propagates target\nlabels instead of errors directly from the output layer to all pre-hidden\nlayers. This effort is more consistent with the top-down reward-guiding\nlearning in cortical columns of the neocortex. Synaptic modifications with only\nlocal gradient differences are induced with pseudo-BP that might also be\nreplaced with the Spike-Timing Dependent Plasticity (STDP). The performance of\nthe proposed BRP-SNN is further verified on the spatial (including MNIST and\nCifar-10) and temporal (including TIDigits and DvsGesture) tasks, where the SNN\nusing BRP has reached a similar accuracy compared to other state-of-the-art\nBP-based SNNs and saved 50% more computational cost than ANNs. We think the\nintroduction of biologically plausible learning rules to the training procedure\nof biologically realistic SNNs will give us more hints and inspirations toward\na better understanding of the biological system's intelligent nature.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:42:13 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 06:06:27 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 13:50:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Tielin", ""], ["Jia", "Shuncheng", ""], ["Cheng", "Xiang", ""], ["Xu", "Bo", ""]]}, {"id": "2010.04445", "submitter": "Mengjun Ming", "authors": "Mengjun Ming, Rui Wang, Tao Zhang", "title": "Investigating Constraint Relationship in Evolutionary Many-Constraint\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the treatment of extensive constraints in\nevolutionary many-constraint optimization through consideration of the\nrelationships between pair-wise constraints. In a conflicting relationship, the\nfunctional value of one constraint increases as the value in another constraint\ndecreases. In a harmonious relationship, the improvement in one constraint is\nrewarded with simultaneous improvement in the other constraint. In an\nindependent relationship, the adjustment to one constraint never affects the\nadjustment to the other. Based on the different features, methods for\nidentifying constraint relationships are discussed, helping to simplify\nmany-constraint optimization problems (MCOPs). Additionally, the transitivity\nof the relationships is further discussed at the aim of determining the\nrelationship in a new pair of constraints.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:15:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ming", "Mengjun", ""], ["Wang", "Rui", ""], ["Zhang", "Tao", ""]]}, {"id": "2010.04463", "submitter": "Jingan Yang", "authors": "Jingan Yang, Yang Peng", "title": "Bioinspired Bipedal Locomotion Control for Humanoid Robotics Based on\n  EACO", "comments": "20 pages, 10 figures, 53 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To construct a robot that can walk as efficiently and steadily as humans or\nother legged animals, we develop an enhanced elitist-mutated ant colony\noptimization~(EACO) algorithm with genetic and crossover operators in real-time\napplications to humanoid robotics or other legged robots. This work presents\npromoting global search capability and convergence rate of the EACO applied to\nhumanoid robots in real-time by estimating the expected convergence rate using\nMarkov chain. Furthermore, we put a special focus on the EACO algorithm on a\nwide range of problems, from ACO, real-coded GAs, GAs with neural\nnetworks~(NNs), particle swarm optimization~(PSO) to complex robotics systems\nincluding gait synthesis, dynamic modeling of parameterizable trajectories and\ngait optimization of humanoid robotics. The experimental results illustrate the\ncapability of this method to discover the premature convergence probability,\ntackle successfully inherent stagnation, and promote the convergence rate of\nthe EACO-based humanoid robotics systems and demonstrated the applicability and\nthe effectiveness of our strategy for solving sophisticated optimization tasks.\nWe found reliable and fast walking gaits with a velocity of up to 0.47m/s using\nthe EACO optimization strategy. These findings have significant implications\nfor understanding and tackling inherent stagnation and poor convergence rate of\nthe EACO and provide new insight into the genetic architectures and control\noptimization of humanoid robotics.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:43:48 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yang", "Jingan", ""], ["Peng", "Yang", ""]]}, {"id": "2010.04466", "submitter": "Robert Tjarko Lange", "authors": "Robert Tjarko Lange and Henning Sprekeler", "title": "Learning not to learn: Nature versus nurture in silico", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals are equipped with a rich innate repertoire of sensory, behavioral and\nmotor skills, which allows them to interact with the world immediately after\nbirth. At the same time, many behaviors are highly adaptive and can be tailored\nto specific environments by means of learning. In this work, we use\nmathematical analysis and the framework of meta-learning (or 'learning to\nlearn') to answer when it is beneficial to learn such an adaptive strategy and\nwhen to hard-code a heuristic behavior. We find that the interplay of\necological uncertainty, task complexity and the agents' lifetime has crucial\neffects on the meta-learned amortized Bayesian inference performed by an agent.\nThere exist two regimes: One in which meta-learning yields a learning algorithm\nthat implements task-dependent information-integration and a second regime in\nwhich meta-learning imprints a heuristic or 'hard-coded' behavior. Further\nanalysis reveals that non-adaptive behaviors are not only optimal for aspects\nof the environment that are stable across individuals, but also in situations\nwhere an adaptation to the environment would in fact be highly beneficial, but\ncould not be done quickly enough to be exploited within the remaining lifetime.\nHard-coded behaviors should hence not only be those that always work, but also\nthose that are too complex to be learned within a reasonable time frame.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:47:40 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:27:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Lange", "Robert Tjarko", ""], ["Sprekeler", "Henning", ""]]}, {"id": "2010.04501", "submitter": "Jens Weise", "authors": "Jens Weise, Sanaz Mostaghim", "title": "Scalable Many-Objective Pathfinding Benchmark Suite", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Route planning also known as pathfinding is one of the key elements in\nlogistics, mobile robotics and other applications, where engineers face many\nconflicting objectives. However, most of the current route planning algorithms\nconsider only up to three objectives. In this paper, we propose a scalable\nmany-objective benchmark problem covering most of the important features for\nrouting applications based on real-world data. We define five objective\nfunctions representing distance, traveling time, delays caused by accidents,\nand two route specific features such as curvature and elevation. We analyse\nseveral different instances for this test problem and provide their true\nPareto-front to analyse the problem difficulties. We apply three well-known\nevolutionary multi-objective algorithms. Since this test benchmark can be\neasily transferred to real-world routing problems, we construct a routing\nproblem from OpenStreetMap data. We evaluate the three optimisation algorithms\nand observe that we are able to provide promising results for such a real-world\napplication. The proposed benchmark represents a scalable many-objective route\nplanning optimisation problem enabling researchers and engineers to evaluate\ntheir many-objective approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:17:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Weise", "Jens", ""], ["Mostaghim", "Sanaz", ""]]}, {"id": "2010.04524", "submitter": "Varun Ojha", "authors": "Varun Ojha and Giuseppe Nicosia", "title": "Multi-Objective Optimisation of Multi-Output Neural Trees", "comments": "19-24 July 2020", "journal-ref": "2020 IEEE Congress on Evolutionary Computation (CEC)", "doi": "10.1109/CEC48606.2020.9185600", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an algorithm and a new method to tackle the classification\nproblems. We propose a multi-output neural tree (MONT) algorithm, which is an\nevolutionary learning algorithm trained by the non-dominated sorting genetic\nalgorithm (NSGA)-III. Since evolutionary learning is stochastic, a hypothesis\nfound in the form of MONT is unique for each run of evolutionary learning,\ni.e., each hypothesis (tree) generated bears distinct properties compared to\nany other hypothesis both in topological space and parameter-space. This leads\nto a challenging optimisation problem where the aim is to minimise the\ntree-size and maximise the classification accuracy. Therefore, the\nPareto-optimality concerns were met by hypervolume indicator analysis. We used\nnine benchmark classification learning problems to evaluate the performance of\nthe MONT. As a result of our experiments, we obtained MONTs which are able to\ntackle the classification problems with high accuracy. The performance of MONT\nemerged better over a set of problems tackled in this study compared with a set\nof well-known classifiers: multilayer perceptron, reduced-error pruning tree,\nnaive Bayes classifier, decision tree, and support vector machine. Moreover,\nthe performances of three versions of MONT's training using genetic\nprogramming, NSGA-II, and NSGA-III suggest that the NSGA-III gives the best\nPareto-optimal solution.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:21:59 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ojha", "Varun", ""], ["Nicosia", "Giuseppe", ""]]}, {"id": "2010.04551", "submitter": "Chen Feng", "authors": "Feng Chen", "title": "AI Centered on Scene Fitting and Dynamic Cognitive Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper briefly analyzes the advantages and problems of AI mainstream\ntechnology and puts forward: To achieve stronger Artificial Intelligence, the\nend-to-end function calculation must be changed and adopt the technology system\ncentered on scene fitting. It also discusses the concrete scheme named Dynamic\nCognitive Network model (DC Net). Discussions : The knowledge and data in the\ncomprehensive domain are uniformly represented by using the rich connection\nheterogeneous Dynamic Cognitive Network constructed by conceptualized elements;\nA network structure of two dimensions and multi layers is designed to achieve\nunified implementation of AI core processing such as combination and\ngeneralization; This paper analyzes the implementation differences of computer\nsystems in different scenes, such as open domain, closed domain, significant\nprobability and non-significant probability, and points out that the\nimplementation in open domain and significant probability scene is the key of\nAI, and a cognitive probability model combining bidirectional conditional\nprobability, probability passing and superposition, probability col-lapse is\ndesigned; An omnidirectional network matching-growth algorithm system driven by\ntarget and probability is designed to realize the integration of parsing,\ngenerating, reasoning, querying, learning and so on; The principle of cognitive\nnetwork optimization is proposed, and the basic framework of Cognitive Network\nLearning algorithm (CNL) is designed that structure learning is the primary\nmethod and parameter learning is the auxiliary. The logical similarity of\nimplementation between DC Net model and human intelligence is analyzed in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 06:13:41 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chen", "Feng", ""]]}, {"id": "2010.04555", "submitter": "Nikita Benkovich", "authors": "Alan Savushkin, Nikita Benkovich and Dmitry Golubev", "title": "Neural Random Projection: From the Initial Task To the Input Similarity\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for implicit data representation\nto evaluate similarity of input data using a trained neural network. In\ncontrast to the previous approach, which uses gradients for representation, we\nutilize only the outputs from the last hidden layer of a neural network and do\nnot use a backward step. The proposed technique explicitly takes into account\nthe initial task and significantly reduces the size of the vector\nrepresentation, as well as the computation time. The key point is minimization\nof information loss between layers. Generally, a neural network discards\ninformation that is not related to the problem, which makes the last hidden\nlayer representation useless for input similarity task. In this work, we\nconsider two main causes of information loss: correlation between neurons and\ninsufficient size of the last hidden layer. To reduce the correlation between\nneurons we use orthogonal weight initialization for each layer and modify the\nloss function to ensure orthogonality of the weights during training. Moreover,\nwe show that activation functions can potentially increase correlation. To\nsolve this problem, we apply modified Batch-Normalization with Dropout. Using\northogonal weight matrices allow us to consider such neural networks as an\napplication of the Random Projection method and get a lower bound estimate for\nthe size of the last hidden layer. We perform experiments on MNIST and physical\nexamination datasets. In both experiments, initially, we split a set of labels\ninto two disjoint subsets to train a neural network for binary classification\nproblem, and then use this model to measure similarity between input data and\ndefine hidden classes. Our experimental results show that the proposed approach\nachieves competitive results on the input similarity task while reducing both\ncomputation time and the size of the input representation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:20:24 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Savushkin", "Alan", ""], ["Benkovich", "Nikita", ""], ["Golubev", "Dmitry", ""]]}, {"id": "2010.04605", "submitter": "Zhi Wang", "authors": "Zhi Wang and Chunlin Chen and Daoyi Dong", "title": "Instance Weighted Incremental Evolution Strategies for Reinforcement\n  Learning in Dynamic Environments", "comments": "Under review, 18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution strategies (ES), as a family of black-box optimization algorithms,\nrecently emerge as a scalable alternative to reinforcement learning (RL)\napproaches such as Q-learning or policy gradient, and are much faster when many\ncentral processing units (CPUs) are available due to better parallelization. In\nthis paper, we propose a systematic incremental learning method for ES in\ndynamic environments. The goal is to adjust previously learned policy to a new\none incrementally whenever the environment changes. We incorporate an instance\nweighting mechanism with ES to facilitate its learning adaptation, while\nretaining scalability of ES. During parameter updating, higher weights are\nassigned to instances that contain more new knowledge, thus encouraging the\nsearch distribution to move towards new promising areas of parameter space. We\npropose two easy-to-implement metrics to calculate the weights: instance\nnovelty and instance quality. Instance novelty measures an instance's\ndifference from the previous optimum in the original environment, while\ninstance quality corresponds to how well an instance performs in the new\nenvironment. The resulting algorithm, Instance Weighted Incremental Evolution\nStrategies (IW-IES), is verified to achieve significantly improved performance\non a suite of robot navigation tasks. This paper thus introduces a family of\nscalable ES algorithms for RL domains that enables rapid learning adaptation to\ndynamic environments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:31:44 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wang", "Zhi", ""], ["Chen", "Chunlin", ""], ["Dong", "Daoyi", ""]]}, {"id": "2010.04767", "submitter": "Tanmay Samak", "authors": "Tanmay Vilas Samak, Chinmay Vilas Samak and Sivanathan Kandhasamy", "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End\n  Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a lightweight pipeline for robust behavioral cloning\nof a human driver using end-to-end imitation learning. The proposed pipeline\nwas employed to train and deploy three distinct driving behavior models onto a\nsimulated vehicle. The training phase comprised of data collection, balancing,\naugmentation, preprocessing and training a neural network, following which, the\ntrained model was deployed onto the ego vehicle to predict steering commands\nbased on the feed from an onboard camera. A novel coupled control law was\nformulated to generate longitudinal control commands on-the-go based on the\npredicted steering angle and other parameters such as actual speed of the ego\nvehicle and the prescribed constraints for speed and steering. We analyzed\ncomputational efficiency of the pipeline and evaluated robustness of the\ntrained models through exhaustive experimentation during the deployment phase.\nWe also compared our approach against state-of-the-art implementation in order\nto comment on its validity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:03:15 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 18:07:14 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 09:33:08 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Samak", "Tanmay Vilas", ""], ["Samak", "Chinmay Vilas", ""], ["Kandhasamy", "Sivanathan", ""]]}, {"id": "2010.04773", "submitter": "Anup Das", "authors": "Twisha Titirsha and Anup Das", "title": "Thermal-Aware Compilation of Spiking Neural Networks to Neuromorphic\n  Hardware", "comments": "Accepted for publication at LCPC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hardware implementation of neuromorphic computing can significantly improve\nperformance and energy efficiency of machine learning tasks implemented with\nspiking neural networks (SNNs), making these hardware platforms particularly\nsuitable for embedded systems and other energy-constrained environments. We\nobserve that the long bitlines and wordlines in a crossbar of the hardware\ncreate significant current variations when propagating spikes through its\nsynaptic elements, which are typically designed with non-volatile memory (NVM).\nSuch current variations create a thermal gradient within each crossbar of the\nhardware, depending on the machine learning workload and the mapping of neurons\nand synapses of the workload to these crossbars. \\mr{This thermal gradient\nbecomes significant at scaled technology nodes and it increases the leakage\npower in the hardware leading to an increase in the energy consumption.} We\npropose a novel technique to map neurons and synapses of SNN-based machine\nlearning workloads to neuromorphic hardware. We make two novel contributions.\nFirst, we formulate a detailed thermal model for a crossbar in a neuromorphic\nhardware incorporating workload dependency, where the temperature of each\nNVM-based synaptic cell is computed considering the thermal contributions from\nits neighboring cells. Second, we incorporate this thermal model in the mapping\nof neurons and synapses of SNN-based workloads using a hill-climbing heuristic.\nThe objective is to reduce the thermal gradient in crossbars. We evaluate our\nneuron and synapse mapping technique using 10 machine learning workloads for a\nstate-of-the-art neuromorphic hardware. We demonstrate an average 11.4K\nreduction in the average temperature of each crossbar in the hardware, leading\nto a 52% reduction in the leakage power consumption (11% lower total energy\nconsumption) compared to a performance-oriented SNN mapping technique.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:29:14 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 21:26:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Titirsha", "Twisha", ""], ["Das", "Anup", ""]]}, {"id": "2010.04786", "submitter": "David Sprunger", "authors": "David Sprunger", "title": "Reparametrizing gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an optimization algorithm which we call norm-adapted\ngradient descent. This algorithm is similar to other gradient-based\noptimization algorithms like Adam or Adagrad in that it adapts the learning\nrate of stochastic gradient descent at each iteration. However, rather than\nusing statistical properties of observed gradients, norm-adapted gradient\ndescent relies on a first-order estimate of the effect of a standard gradient\ndescent update step, much like the Newton-Raphson method in many dimensions.\nOur algorithm can also be compared to quasi-Newton methods, but we seek roots\nrather than stationary points. Seeking roots can be justified by the fact that\nfor models with sufficient capacity measured by nonnegative loss functions,\nroots coincide with global optima. This work presents several experiments where\nwe have used our algorithm; in these results, it appears norm-adapted descent\nis particularly strong in regression settings but is also capable of training\nclassifiers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 20:22:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Sprunger", "David", ""]]}, {"id": "2010.04963", "submitter": "Haiqin Yang", "authors": "Jinmian Ye, Guangxi Li, Di Chen, Haiqin Yang, Shandian Zhe, and\n  Zenglin Xu", "title": "Block-term Tensor Neural Networks", "comments": "12 pages, 15 figures", "journal-ref": "Neural Networks, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved outstanding performance in a wide\nrange of applications, e.g., image classification, natural language processing,\netc. Despite the good performance, the huge number of parameters in DNNs brings\nchallenges to efficient training of DNNs and also their deployment in low-end\ndevices with limited computing resources. In this paper, we explore the\ncorrelations in the weight matrices, and approximate the weight matrices with\nthe low-rank block-term tensors. We name the new corresponding structure as\nblock-term tensor layers (BT-layers), which can be easily adapted to neural\nnetwork models, such as CNNs and RNNs. In particular, the inputs and the\noutputs in BT-layers are reshaped into low-dimensional high-order tensors with\na similar or improved representation power. Sufficient experiments have\ndemonstrated that BT-layers in CNNs and RNNs can achieve a very large\ncompression ratio on the number of parameters while preserving or improving the\nrepresentation power of the original DNNs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 09:58:43 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 06:47:53 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Ye", "Jinmian", ""], ["Li", "Guangxi", ""], ["Chen", "Di", ""], ["Yang", "Haiqin", ""], ["Zhe", "Shandian", ""], ["Xu", "Zenglin", ""]]}, {"id": "2010.05176", "submitter": "Alexander Hadjiivanov", "authors": "Alexander Hadjiivanov and Alan Blair", "title": "Complexity-based speciation and genotype representation for\n  neuroevolution", "comments": null, "journal-ref": "2016 IEEE Congress on Evolutionary Computation (CEC), Vancouver,\n  BC, 2016, pp. 3092-3101", "doi": "10.1109/CEC.2016.7744180", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a speciation principle for neuroevolution where\nevolving networks are grouped into species based on the number of hidden\nneurons, which is indicative of the complexity of the search space. This\nspeciation principle is indivisibly coupled with a novel genotype\nrepresentation which is characterised by zero genome redundancy, high\nresilience to bloat, explicit marking of recurrent connections, as well as an\nefficient and reproducible stack-based evaluation procedure for networks with\narbitrary topology. Furthermore, the proposed speciation principle is employed\nin several techniques designed to promote and preserve diversity within species\nand in the ecosystem as a whole. The competitive performance of the proposed\nframework, named Cortex, is demonstrated through experiments. A highly\ncustomisable software platform which implements the concepts proposed in this\nstudy is also introduced in the hope that it will serve as a useful and\nreliable tool for experimentation in the field of neuroevolution.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 06:26:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hadjiivanov", "Alexander", ""], ["Blair", "Alan", ""]]}, {"id": "2010.05197", "submitter": "Kossar Pourahmadi Meibodi", "authors": "Reza Hojabr, Kamyar Givaki, Kossar Pourahmadi, Parsa Nooralinejad,\n  Ahmad Khonsari, Dara Rahmati, M. Hassan Najafi", "title": "TaxoNN: A Light-Weight Accelerator for Deep Neural Network Training", "comments": "Accepted to ISCAS 2020. 5 pages, 5 figures", "journal-ref": "2020 IEEE International Symposium on Circuits and Systems (ISCAS),\n  2020, pp. 1-5", "doi": "10.1109/ISCAS45731.2020.9181001", "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging intelligent embedded devices rely on Deep Neural Networks (DNNs) to\nbe able to interact with the real-world environment. This interaction comes\nwith the ability to retrain DNNs, since environmental conditions change\ncontinuously in time. Stochastic Gradient Descent (SGD) is a widely used\nalgorithm to train DNNs by optimizing the parameters over the training data\niteratively. In this work, first we present a novel approach to add the\ntraining ability to a baseline DNN accelerator (inference only) by splitting\nthe SGD algorithm into simple computational elements. Then, based on this\nheuristic approach we propose TaxoNN, a light-weight accelerator for DNN\ntraining. TaxoNN can easily tune the DNN weights by reusing the hardware\nresources used in the inference process using a time-multiplexing approach and\nlow-bitwidth units. Our experimental results show that TaxoNN delivers, on\naverage, 0.97% higher misclassification rate compared to a full-precision\nimplementation. Moreover, TaxoNN provides 2.1$\\times$ power saving and\n1.65$\\times$ area reduction over the state-of-the-art DNN training accelerator.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 09:04:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hojabr", "Reza", ""], ["Givaki", "Kamyar", ""], ["Pourahmadi", "Kossar", ""], ["Nooralinejad", "Parsa", ""], ["Khonsari", "Ahmad", ""], ["Rahmati", "Dara", ""], ["Najafi", "M. Hassan", ""]]}, {"id": "2010.05343", "submitter": "Jonatan Gomez", "authors": "Jonatan Gomez and Carlos Rivera", "title": "Non-Stationary Stochastic Global Optimization Algorithms", "comments": "Submitted to Natural Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gomez proposes a formal and systematic approach for characterizing stochastic\nglobal optimization algorithms. Using it, Gomez formalizes algorithms with a\nfixed next-population stochastic method, i.e., algorithms defined as stationary\nMarkov processes. These are the cases of standard versions of hill-climbing,\nparallel hill-climbing, generational genetic, steady-state genetic, and\ndifferential evolution algorithms. This paper continues such a systematic\nformal approach. First, we generalize the sufficient conditions convergence\nlemma from stationary to non-stationary Markov processes. Second, we develop\nMarkov kernels for some selection schemes. Finally, we formalize both\nsimulated-annealing and evolutionary-strategies using the systematic formal\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:04:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gomez", "Jonatan", ""], ["Rivera", "Carlos", ""]]}, {"id": "2010.05429", "submitter": "Shayan Hassantabar", "authors": "Shayan Hassantabar, Prerit Terway, and Niraj K. Jha", "title": "TUTOR: Training Neural Networks Using Decision Rules as Model Priors", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain has the ability to carry out new tasks with limited\nexperience. It utilizes prior learning experiences to adapt the solution\nstrategy to new domains. On the other hand, deep neural networks (DNNs)\ngenerally need large amounts of data and computational resources for training.\nHowever, this requirement is not met in many settings. To address these\nchallenges, we propose the TUTOR DNN synthesis framework. TUTOR targets\nnon-image datasets. It synthesizes accurate DNN models with limited available\ndata, and reduced memory and computational requirements. It consists of three\nsequential steps: (1) drawing synthetic data from the same probability\ndistribution as the training data and labeling the synthetic data based on a\nset of rules extracted from the real dataset, (2) use of two training schemes\nthat combine synthetic data and training data to learn DNN weights, and (3)\nemploying a grow-and-prune synthesis paradigm to learn both the weights and the\narchitecture of the DNN to reduce model size while ensuring its accuracy. We\nshow that in comparison with fully-connected DNNs, on an average TUTOR reduces\nthe need for data by 6.0x (geometric mean), improves accuracy by 3.6%, and\nreduces the number of parameters (floating-point operations) by 4.7x (4.3x)\n(geometric mean). Thus, TUTOR is a less data-hungry, accurate, and efficient\nDNN synthesis framework.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 03:25:47 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 01:53:05 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hassantabar", "Shayan", ""], ["Terway", "Prerit", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2010.05463", "submitter": "Francisco Leonardo Bezerra Martins", "authors": "Francisco Leonardo Bezerra Martins, Jos\\'e Cl\\'audio do Nascimento", "title": "Power law dynamics in genealogical graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several populational networks present complex topologies when implemented in\nevolutionary algorithms. A common feature of these topologies is the emergence\nof a power law. In genealogical networks, the power law can be observed by\nmeasuring the impact of individuals in the population, which can be calculated\nthrough the Event Takeover Value (ETV) algorithm. In this paper, we show\nevidence that the different power-law deviations, resulting from the ETV\ndistributions of genealogical graphs, are static images of a dynamic evolution\nthat can be well described by $q$-exponential distribution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:35:31 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 05:09:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Martins", "Francisco Leonardo Bezerra", ""], ["Nascimento", "Jos\u00e9 Cl\u00e1udio do", ""]]}, {"id": "2010.05494", "submitter": "Sriram Krishna", "authors": "Sriram Krishna, Niharika Pentapati", "title": "Genetic Bi-objective Optimization Approach to Habitability Score", "comments": "15 pages, 9 figures, granted for publication in Communications in\n  Computer and Information Science (CCIS) proceedings by Springer Nature,\n  presented at the International Conference on Modeling, Machine Learning and\n  Astronomy (MMLA19)", "journal-ref": null, "doi": "10.1007/978-981-33-6463-9_12", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for life outside the Solar System is an endeavour of astronomers\nall around the world. With hundreds of exoplanets being discovered due to\nadvances in astronomy, there is a need to classify the habitability of these\nexoplanets. This is typically done using various metrics such as the Earth\nSimilarity Index or the Planetary Habitability Index. In this paper, Genetic\nAlgorithms are used to evaluate the best possible habitability scores using the\nCobb-Douglas Habitability Score. Genetic Algorithm is a classic evolutionary\nalgorithm used for solving optimization problems. It is based on Darwin's\ntheory of evolution, \"Survival of the fittest\". The working of the algorithm is\nestablished through comparison with various benchmark functions and extended\nits functionality to Multi-Objective optimization. The Cobb-Douglas\nHabitability Function is formulated as a bi-objective as well as a single\nobjective optimization problem to find the optimal values to maximize the\nCobb-Douglas Habitability Score for a set of promising exoplanets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:42:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Krishna", "Sriram", ""], ["Pentapati", "Niharika", ""]]}, {"id": "2010.05943", "submitter": "Adam Dubowski", "authors": "Adam Dubowski", "title": "Activation function impact on Sparse Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the concept of a Sparse Neural Network has been researched for some\ntime, researchers have only recently made notable progress in the matter.\nTechniques like Sparse Evolutionary Training allow for significantly lower\ncomputational complexity when compared to fully connected models by reducing\nredundant connections. That typically takes place in an iterative process of\nweight creation and removal during network training. Although there have been\nnumerous approaches to optimize the redistribution of the removed weights,\nthere seems to be little or no study on the effect of activation functions on\nthe performance of the Sparse Networks. This research provides insights into\nthe relationship between the activation function used and the network\nperformance at various sparsity levels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:05:04 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dubowski", "Adam", ""]]}, {"id": "2010.06099", "submitter": "Felipe Farias Mr.", "authors": "Felipe Farias, Teresa Ludermir, Carmelo Bastos-Filho", "title": "Similarity Based Stratified Splitting: an approach to train better\n  classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Similarity-Based Stratified Splitting (SBSS) technique, which\nuses both the output and input space information to split the data. The splits\nare generated using similarity functions among samples to place similar samples\nin different splits. This approach allows for a better representation of the\ndata in the training phase. This strategy leads to a more realistic performance\nestimation when used in real-world applications. We evaluate our proposal in\ntwenty-two benchmark datasets with classifiers such as Multi-Layer Perceptron,\nSupport Vector Machine, Random Forest and K-Nearest Neighbors, and five\nsimilarity functions Cityblock, Chebyshev, Cosine, Correlation, and Euclidean.\nAccording to the Wilcoxon Sign-Rank test, our approach consistently\noutperformed ordinary stratified 10-fold cross-validation in 75\\% of the\nassessed scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:07:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Farias", "Felipe", ""], ["Ludermir", "Teresa", ""], ["Bastos-Filho", "Carmelo", ""]]}, {"id": "2010.06209", "submitter": "Matthew Evanusa", "authors": "Matthew Evanusa and Cornelia Ferm\\\"uller and Yiannis Aloimonos", "title": "Deep Reservoir Networks with Learned Hidden Reservoir Weights using\n  Direct Feedback Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Reservoir Computing has emerged as a new paradigm for deep learning,\nwhich is based around the reservoir computing principle of maintaining random\npools of neurons combined with hierarchical deep learning. The reservoir\nparadigm reflects and respects the high degree of recurrence in biological\nbrains, and the role that neuronal dynamics play in learning. However, one\nissue hampering deep reservoir network development is that one cannot\nbackpropagate through the reservoir layers. Recent deep reservoir architectures\ndo not learn hidden or hierarchical representations in the same manner as deep\nartificial neural networks, but rather concatenate all hidden reservoirs\ntogether to perform traditional regression. Here we present a novel Deep\nReservoir Network for time series prediction and classification that learns\nthrough the non-differentiable hidden reservoir layers using a\nbiologically-inspired backpropagation alternative called Direct Feedback\nAlignment, which resembles global dopamine signal broadcasting in the brain. We\ndemonstrate its efficacy on two real world multidimensional time series\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 07:31:05 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 03:27:44 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 03:00:29 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Evanusa", "Matthew", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2010.06219", "submitter": "Beren Millidge Mr", "authors": "Beren Millidge, Alexander Tschantz, Anil Seth, Christopher L Buckley", "title": "Investigating the Scalability and Biological Plausibility of the\n  Activation Relaxation Algorithm", "comments": "13/10/20 initial upload", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Activation Relaxation (AR) algorithm provides a simple\nand robust approach for approximating the backpropagation of error algorithm\nusing only local learning rules. Unlike competing schemes, it converges to the\nexact backpropagation gradients, and utilises only a single type of\ncomputational unit and a single backwards relaxation phase. We have previously\nshown that the algorithm can be further simplified and made more biologically\nplausible by (i) introducing a learnable set of backwards weights, which\novercomes the weight-transport problem, and (ii) avoiding the computation of\nnonlinear derivatives at each neuron. However, tthe efficacy of these\nsimplifications has, so far, only been tested on simple multi-layer-perceptron\n(MLP) networks. Here, we show that these simplifications still maintain\nperformance using more complex CNN architectures and challenging datasets,\nwhich have proven difficult for other biologically-plausible schemes to scale\nto. We also investigate whether another biologically implausible assumption of\nthe original AR algorithm -- the frozen feedforward pass -- can be relaxed\nwithout damaging performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:02:38 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Millidge", "Beren", ""], ["Tschantz", "Alexander", ""], ["Seth", "Anil", ""], ["Buckley", "Christopher L", ""]]}, {"id": "2010.06223", "submitter": "Anubhav Garg", "authors": "Anubhav Garg, Amit Kumar Saha, Debo Dutta", "title": "Direct Federated Neural Architecture Search", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is a collection of methods to craft the way\nneural networks are built. We apply this idea to Federated Learning (FL),\nwherein predefined neural network models are trained on the client/device data.\nThis approach is not optimal as the model developers can't observe the local\ndata, and hence, are unable to build highly accurate and efficient models. NAS\nis promising for FL which can search for global and personalized models\nautomatically for the non-IID data. Most NAS methods are computationally\nexpensive and require fine-tuning after the search, making it a two-stage\ncomplex process with possible human intervention. Thus there is a need for\nend-to-end NAS which can run on the heterogeneous data and resource\ndistribution typically seen in the FL scenario. In this paper, we present an\neffective approach for direct federated NAS which is hardware agnostic,\ncomputationally lightweight, and a one-stage method to search for\nready-to-deploy neural network models. Our results show an order of magnitude\nreduction in resource consumption while edging out prior art in accuracy. This\nopens up a window of opportunity to create optimized and computationally\nefficient federated learning systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:11:35 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 11:43:49 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 07:55:54 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Garg", "Anubhav", ""], ["Saha", "Amit Kumar", ""], ["Dutta", "Debo", ""]]}, {"id": "2010.06379", "submitter": "Jingfei Chang", "authors": "Jingfei Chang", "title": "Coarse and fine-grained automatic cropping deep convolutional neural\n  network", "comments": "12 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing convolutional neural network pruning algorithms can be divided\ninto two categories: coarse-grained clipping and fine-grained clipping. This\npaper proposes a coarse and fine-grained automatic pruning algorithm, which can\nachieve more efficient and accurate compression acceleration for convolutional\nneural networks. First, cluster the intermediate feature maps of the\nconvolutional neural network to obtain the network structure after\ncoarse-grained clipping, and then use the particle swarm optimization algorithm\nto iteratively search and optimize the structure. Finally, the optimal network\ntailoring substructure is obtained.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:36:33 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:42:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chang", "Jingfei", ""]]}, {"id": "2010.06406", "submitter": "Jakub Vincalek", "authors": "Jakub Vincalek, Sean Walton and Ben Evans", "title": "It's the Journey Not the Destination: Building Genetic Algorithms\n  Practitioners Can Trust", "comments": "10 pages, 4 figures, submitted to IEEE Transactions on Evolutionary\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms have been developed for decades by researchers in academia\nand perform well in engineering applications, yet their uptake in industry\nremains limited. In order to understand why this is the case, the opinions of\nusers of engineering design tools were gathered. The results from a survey\nshowing the attitudes of engineers and students with design experience with\nrespect to optimisation algorithms are presented. A survey was designed to\nanswer two research questions: To what extent is there a pre-existing sentiment\n(negative or positive) among students, engineers, and managers towards genetic\nalgorithm-based design? and What are the requirements of practitioners with\nregards to design optimisation and the design optimisation process? A total of\n23 participants (N = 23) took part in the 3-part mixed methods survey. Thematic\nanalysis was conducted on the open-ended questions. A common thread throughout\nparticipants responses is that there is a question of trust towards genetic\nalgorithms within industry. Perhaps surprising is that the key to gaining this\ntrust is not producing good results, but creating algorithms which explain the\nprocess they take in reaching a result. Participants have expressed a desire to\ncontinue to remain in the design loop. This is at odds with the motivation of a\nportion of the genetic algorithms community of removing humans from the loop.\nIt is clear we need to take a different approach to increase industrial uptake.\nBased on this, the following recommendations have been made to increase their\nuse in industry: an increase of transparency and explainability of genetic\nalgorithms, an increased focus on user experience, better communication between\ndevelopers and engineers, and visualising algorithm behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:07:30 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Vincalek", "Jakub", ""], ["Walton", "Sean", ""], ["Evans", "Ben", ""]]}, {"id": "2010.06456", "submitter": "Jakub Tomczak", "authors": "Ewelina Weglarz-Tomczak, Jakub M. Tomczak, Agoston E. Eiben, Stanley\n  Brul", "title": "Population-based Optimization for Kinetic Parameter Identification in\n  Glycolytic Pathway in Saccharomyces cerevisiae", "comments": "Code at https://github.com/jmtomczak/popi", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models in systems biology are mathematical descriptions of biological\nprocesses that are used to answer questions and gain a better understanding of\nbiological phenomena. Dynamic models represent the network through rates of the\nproduction and consumption for the individual species. The ordinary\ndifferential equations that describe rates of the reactions in the model\ninclude a set of parameters. The parameters are important quantities to\nunderstand and analyze biological systems. Moreover, the perturbation of the\nkinetic parameters are correlated with upregulation of the system by\ncell-intrinsic and cell-extrinsic factors, including mutations and the\nenvironment changes. Here, we aim at using well-established models of\nbiological pathways to identify parameter values and point their potential\nperturbation/deviation. We present our population-based optimization framework\nthat is able to identify kinetic parameters in the dynamic model based on only\ninput and output data (i.e., timecourses of selected metabolites). Our approach\ncan deal with the identification of the non-measurable parameters as well as\nwith discovering deviation of the parameters. We present our proposed\noptimization framework on the example of the well-studied glycolytic pathway in\nSaccharomyces cerevisiae.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:57:28 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Weglarz-Tomczak", "Ewelina", ""], ["Tomczak", "Jakub M.", ""], ["Eiben", "Agoston E.", ""], ["Brul", "Stanley", ""]]}, {"id": "2010.06512", "submitter": "Maria Attarian", "authors": "Maria Attarian, Brett D. Roads, Michael C. Mozer", "title": "Transforming Neural Network Visual Representations to Predict Human\n  Judgments of Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning vision models have shown intriguing similarities and\ndifferences with respect to human vision. We investigate how to bring machine\nvisual representations into better alignment with human representations. Human\nrepresentations are often inferred from behavioral evidence such as the\nselection of an image most similar to a query image. We find that with\nappropriate linear transformations of deep embeddings, we can improve\nprediction of human binary choice on a data set of bird images from 72% at\nbaseline to 89%. We hypothesized that deep embeddings have redundant, high\n(4096) dimensional representations; however, reducing the rank of these\nrepresentations results in a loss of explanatory power. We hypothesized that\nthe dilation transformation of representations explored in past research is too\nrestrictive, and indeed we found that model explanatory power can be\nsignificantly improved with a more expressive linear transform. Most surprising\nand exciting, we found that, consistent with classic psychological literature,\nhuman similarity judgments are asymmetric: the similarity of X to Y is not\nnecessarily equal to the similarity of Y to X, and allowing models to express\nthis asymmetry improves explanatory power.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 16:09:47 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 20:40:33 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Attarian", "Maria", ""], ["Roads", "Brett D.", ""], ["Mozer", "Michael C.", ""]]}, {"id": "2010.06649", "submitter": "Silvija Kokalj-Filipovic", "authors": "Silvija Kokalj-Filipovic and Paul Toliver and William Johnson and\n  Raymond R. Hoare II and Joseph J. Jezak", "title": "Deep Delay Loop Reservoir Computing for Specific Emitter Identification", "comments": "6 pages, appeared in GOMACTech 2020, released for public", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current AI systems at the tactical edge lack the computational resources to\nsupport in-situ training and inference for situational awareness, and it is not\nalways practical to leverage backhaul resources due to security, bandwidth, and\nmission latency requirements. We propose a solution through Deep delay Loop\nReservoir Computing (DLR), a processing architecture supporting general machine\nlearning algorithms on compact mobile devices by leveraging delay-loop (DL)\nreservoir computing in combination with innovative photonic hardware exploiting\nthe inherent speed, and spatial, temporal and wavelength-based processing\ndiversity of signals in the optical domain. DLR delivers reductions in form\nfactor, hardware complexity, power consumption and latency, compared to\nState-of-the-Art . DLR can be implemented with a single photonic DL and a few\nelectro-optical components. In certain cases multiple DL layers increase\nlearning capacity of the DLR with no added latency. We demonstrate the\nadvantages of DLR on the application of RF Specific Emitter Identification.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:32:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kokalj-Filipovic", "Silvija", ""], ["Toliver", "Paul", ""], ["Johnson", "William", ""], ["Hoare", "Raymond R.", "II"], ["Jezak", "Joseph J.", ""]]}, {"id": "2010.06746", "submitter": "Rollin Omari M", "authors": "Rollin Omari, R. I. McKay and Tom Gedeon", "title": "Analogical and Relational Reasoning with Spiking Neural Networks", "comments": "7 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raven's Progressive Matrices have been widely used for measuring abstract\nreasoning and intelligence in humans. However for artificial learning systems,\nabstract reasoning remains a challenging problem. In this paper we investigate\nhow neural networks augmented with biologically inspired spiking modules gain a\nsignificant advantage in solving this problem. To illustrate this, we first\ninvestigate the performance of our networks with supervised learning, then with\nunsupervised learning. Experiments on the RAVEN dataset show that the overall\naccuracy of our supervised networks surpass human-level performance, while our\nunsupervised networks significantly outperform existing unsupervised methods.\nFinally, our results from both supervised and unsupervised learning illustrate\nthat, unlike their non-augmented counterparts, networks with spiking modules\nare able to extract and encode temporal features without any explicit\ninstruction, do not heavily rely on training data, and generalise more readily\nto new problems. In summary, the results reported here indicate that artificial\nneural networks with spiking modules are well suited to solving abstract\nreasoning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 00:18:00 GMT"}], "update_date": "2020-12-28", "authors_parsed": [["Omari", "Rollin", ""], ["McKay", "R. I.", ""], ["Gedeon", "Tom", ""]]}, {"id": "2010.06845", "submitter": "Span Spanbauer", "authors": "Span Spanbauer, Ian Hunter", "title": "Extended Koopman Models", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two novel generalizations of the Koopman operator method of\nnonlinear dynamic modeling. Each of these generalizations leads to greatly\nimproved predictive performance without sacrificing a unique trait of Koopman\nmethods: the potential for fast, globally optimal control of nonlinear,\nnonconvex systems. The first generalization, Convex Koopman Models, uses convex\nrather than linear dynamics in the lifted space. The second, Extended Koopman\nModels, additionally introduces an invertible transformation of the control\nsignal which contributes to the lifted convex dynamics. We describe a deep\nlearning architecture for parameterizing these classes of models, and show\nexperimentally that each significantly outperforms traditional Koopman models\nin trajectory prediction for two nonlinear, nonconvex dynamic systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:10:37 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Spanbauer", "Span", ""], ["Hunter", "Ian", ""]]}, {"id": "2010.07078", "submitter": "Andreas Look", "authors": "Andreas Look, Simona Doneva, Melih Kandemir, Rainer Gemulla, Jan\n  Peters", "title": "Differentiable Implicit Layers", "comments": null, "journal-ref": "Workshop on machine learning for engineering modeling, simulation\n  and design @ NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an efficient backpropagation scheme for\nnon-constrained implicit functions. These functions are parametrized by a set\nof learnable weights and may optionally depend on some input; making them\nperfectly suitable as a learnable layer in a neural network. We demonstrate our\nscheme on different applications: (i) neural ODEs with the implicit Euler\nmethod, and (ii) system identification in model predictive control.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:26:27 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 10:25:13 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Look", "Andreas", ""], ["Doneva", "Simona", ""], ["Kandemir", "Melih", ""], ["Gemulla", "Rainer", ""], ["Peters", "Jan", ""]]}, {"id": "2010.07103", "submitter": "Joschka Herteux", "authors": "Joschka Herteux, Christoph R\\\"ath", "title": "Breaking Symmetries of the Reservoir Equations in Echo State Networks", "comments": "14 pages, 10 figures, accepted by chaos", "journal-ref": "Chaos 30, 123142 (2020)", "doi": "10.1063/5.0028993", "report-no": null, "categories": "physics.data-an cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing has repeatedly been shown to be extremely successful in\nthe prediction of nonlinear time-series. However, there is no complete\nunderstanding of the proper design of a reservoir yet. We find that the\nsimplest popular setup has a harmful symmetry, which leads to the prediction of\nwhat we call mirror-attractor. We prove this analytically. Similar problems can\narise in a general context, and we use them to explain the success or failure\nof some designs. The symmetry is a direct consequence of the hyperbolic tangent\nactivation function. Further, four ways to break the symmetry are compared\nnumerically: A bias in the output, a shift in the input, a quadratic term in\nthe readout, and a mixture of even and odd activation functions. Firstly, we\ntest their susceptibility to the mirror-attractor. Secondly, we evaluate their\nperformance on the task of predicting Lorenz data with the mean shifted to\nzero. The short-time prediction is measured with the forecast horizon while the\nlargest Lyapunov exponent and the correlation dimension are used to represent\nthe climate. Finally, the same analysis is repeated on a combined dataset of\nthe Lorenz attractor and the Halvorsen attractor, which we designed to reveal\npotential problems with symmetry. We find that all methods except the output\nbias are able to fully break the symmetry with input shift and quadratic\nreadout performing the best overall.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:00:22 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 10:28:07 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Herteux", "Joschka", ""], ["R\u00e4th", "Christoph", ""]]}, {"id": "2010.07517", "submitter": "Mehdi Neshat", "authors": "Martin Schlueter, Mehdi Neshat, Mohamed Wahib, Masaharu Munetomo,\n  Markus Wagner", "title": "GTOPX Space Mission Benchmarks", "comments": null, "journal-ref": null, "doi": "10.1016/j.softx.2021.100666", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution introduces the GTOPX space mission benchmark collection,\nwhich is an extension of GTOP database published by the European Space Agency\n(ESA). GTOPX consists of ten individual benchmark instances representing\nreal-world interplanetary space trajectory design problems. In regard to the\noriginal GTOP collection, GTOPX includes three new problem instances featuring\nmixed-integer and multi-objective properties. GTOPX enables a simplified user\nhandling, unified benchmark function call and some minor bug corrections to the\noriginal GTOP implementation. Furthermore, GTOPX is linked from it's original\nC++ source code to Python and Matlab based on dynamic link libraries, assuring\ncomputationally fast and accurate reproduction of the benchmark results in all\nthree programming languages. Space mission trajectory design problems as those\nrepresented in GTOPX are known to be highly non-linear and difficult to solve.\nThe GTOPX collection, therefore, aims particularly at researchers wishing to\nput advanced (meta)heuristic and hybrid optimization algorithms to the test.\nThe goal of this paper is to provide researchers with a manual and reference to\nthe newly available GTOPX benchmark software.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:45:16 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 05:03:45 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 11:00:33 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 11:47:55 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Schlueter", "Martin", ""], ["Neshat", "Mehdi", ""], ["Wahib", "Mohamed", ""], ["Munetomo", "Masaharu", ""], ["Wagner", "Markus", ""]]}, {"id": "2010.07693", "submitter": "Matthew Leavitt", "authors": "Matthew L. Leavitt, Ari Morcos", "title": "Linking average- and worst-case perturbation robustness via class\n  selectivity and dimensionality", "comments": "arXiv admin note: text overlap with arXiv:2007.04440", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representational sparsity is known to affect robustness to input\nperturbations in deep neural networks (DNNs), but less is known about how the\nsemantic content of representations affects robustness. Class selectivity-the\nvariability of a unit's responses across data classes or dimensions-is one way\nof quantifying the sparsity of semantic representations. Given recent evidence\nthat class selectivity may not be necessary for, and in some cases can impair\ngeneralization, we investigate whether it also confers robustness (or\nvulnerability) to perturbations of input data. We found that networks\nregularized to have lower levels of class selectivity were more robust to\naverage-case (naturalistic) perturbations, while networks with higher class\nselectivity are more vulnerable. In contrast, class selectivity increases\nrobustness to multiple types of worst-case (i.e. white box adversarial)\nperturbations, suggesting that while decreasing class selectivity is helpful\nfor average-case perturbations, it is harmful for worst-case perturbations. To\nexplain this difference, we studied the dimensionality of the networks'\nrepresentations: we found that the dimensionality of early-layer\nrepresentations is inversely proportional to a network's class selectivity, and\nthat adversarial samples cause a larger increase in early-layer dimensionality\nthan corrupted samples. Furthermore, the input-unit gradient is more variable\nacross samples and units in high-selectivity networks compared to\nlow-selectivity networks. These results lead to the conclusion that units\nparticipate more consistently in low-selectivity regimes compared to\nhigh-selectivity regimes, effectively creating a larger attack surface and\nhence vulnerability to worst-case perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 00:45:29 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 22:49:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Leavitt", "Matthew L.", ""], ["Morcos", "Ari", ""]]}, {"id": "2010.07773", "submitter": "Shubhanker Banerjee", "authors": "Shubhanker Banerjee, Arun Jayapal and Sajeetha Thavareesan", "title": "NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of\n  Code-Mixed Dravidian text using XLNet", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social media has penetrated into multilingual societies, however most of them\nuse English to be a preferred language for communication. So it looks natural\nfor them to mix their cultural language with English during conversations\nresulting in abundance of multilingual data, call this code-mixed data,\navailable in todays' world.Downstream NLP tasks using such data is challenging\ndue to the semantic nature of it being spread across multiple languages.One\nsuch Natural Language Processing task is sentiment analysis, for this we use an\nauto-regressive XLNet model to perform sentiment analysis on code-mixed\nTamil-English and Malayalam-English datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:09:02 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Banerjee", "Shubhanker", ""], ["Jayapal", "Arun", ""], ["Thavareesan", "Sajeetha", ""]]}, {"id": "2010.07859", "submitter": "Julie Grollier", "authors": "Erwann Martin, Maxence Ernoult, J\\'er\\'emie Laydevant, Shuai Li,\n  Damien Querlioz, Teodora Petrisor, Julie Grollier", "title": "EqSpike: Spike-driven Equilibrium Propagation for Neuromorphic\n  Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding spike-based learning algorithms that can be implemented within the\nlocal constraints of neuromorphic systems, while achieving high accuracy,\nremains a formidable challenge. Equilibrium Propagation is a promising\nalternative to backpropagation as it only involves local computations, but\nhardware-oriented studies have so far focused on rate-based networks. In this\nwork, we develop a spiking neural network algorithm called EqSpike, compatible\nwith neuromorphic systems, which learns by Equilibrium Propagation. Through\nsimulations, we obtain a test recognition accuracy of 97.6% on MNIST, similar\nto rate-based Equilibrium Propagation, and comparing favourably to alternative\nlearning techniques for spiking neural networks. We show that EqSpike\nimplemented in silicon neuromorphic technology could reduce the energy\nconsumption of inference and training respectively by three orders and two\norders of magnitude compared to GPUs. Finally, we also show that during\nlearning, EqSpike weight updates exhibit a form of Spike Timing Dependent\nPlasticity, highlighting a possible connection with biology.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:25:29 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 08:25:16 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 14:48:02 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Martin", "Erwann", ""], ["Ernoult", "Maxence", ""], ["Laydevant", "J\u00e9r\u00e9mie", ""], ["Li", "Shuai", ""], ["Querlioz", "Damien", ""], ["Petrisor", "Teodora", ""], ["Grollier", "Julie", ""]]}, {"id": "2010.08031", "submitter": "Luca Parisi", "authors": "L. Parisi, D. Neagu, R. Ma, F. Campean", "title": "QReLU and m-QReLU: Two novel quantum activation functions to aid medical\n  diagnostics", "comments": "30 pages, 4 listings/Python code snippets, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ReLU activation function (AF) has been extensively applied in deep neural\nnetworks, in particular Convolutional Neural Networks (CNN), for image\nclassification despite its unresolved dying ReLU problem, which poses\nchallenges to reliable applications. This issue has obvious important\nimplications for critical applications, such as those in healthcare. Recent\napproaches are just proposing variations of the activation function within the\nsame unresolved dying ReLU challenge. This contribution reports a different\nresearch direction by investigating the development of an innovative quantum\napproach to the ReLU AF that avoids the dying ReLU problem by disruptive\ndesign. The Leaky ReLU was leveraged as a baseline on which the two quantum\nprinciples of entanglement and superposition were applied to derive the\nproposed Quantum ReLU (QReLU) and the modified-QReLU (m-QReLU) activation\nfunctions. Both QReLU and m-QReLU are implemented and made freely available in\nTensorFlow and Keras. This original approach is effective and validated\nextensively in case studies that facilitate the detection of COVID-19 and\nParkinson Disease (PD) from medical images. The two novel AFs were evaluated in\na two-layered CNN against nine ReLU-based AFs on seven benchmark datasets,\nincluding images of spiral drawings taken via graphic tablets from patients\nwith Parkinson Disease and healthy subjects, and point-of-care ultrasound\nimages on the lungs of patients with COVID-19, those with pneumonia and healthy\ncontrols. Despite a higher computational cost, results indicated an overall\nhigher classification accuracy, precision, recall and F1-score brought about by\neither quantum AFs on five of the seven bench-mark datasets, thus demonstrating\nits potential to be the new benchmark or gold standard AF in CNNs and aid image\nclassification tasks involved in critical applications, such as medical\ndiagnoses of COVID-19 and PD.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:38:36 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Parisi", "L.", ""], ["Neagu", "D.", ""], ["Ma", "R.", ""], ["Campean", "F.", ""]]}, {"id": "2010.08127", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Behnam Neyshabur, Hanie Sedghi", "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline\n  Generalizers", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for reasoning about generalization in deep\nlearning. The core idea is to couple the Real World, where optimizers take\nstochastic gradient steps on the empirical loss, to an Ideal World, where\noptimizers take steps on the population loss. This leads to an alternate\ndecomposition of test error into: (1) the Ideal World test error plus (2) the\ngap between the two worlds. If the gap (2) is universally small, this reduces\nthe problem of generalization in offline learning to the problem of\noptimization in online learning. We then give empirical evidence that this gap\nbetween worlds can be small in realistic deep learning settings, in particular\nsupervised image classification. For example, CNNs generalize better than MLPs\non image distributions in the Real World, but this is \"because\" they optimize\nfaster on the population loss in the Ideal World. This suggests our framework\nis a useful tool for understanding generalization in deep learning, and lays a\nfoundation for future research in the area.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:07:49 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 03:24:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Neyshabur", "Behnam", ""], ["Sedghi", "Hanie", ""]]}, {"id": "2010.08158", "submitter": "Rakshitha Godahewa", "authors": "Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Pablo\n  Montero-Manso", "title": "A Strong Baseline for Weekly Time Series Forecasting", "comments": "21 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many businesses and industries require accurate forecasts for weekly time\nseries nowadays. The forecasting literature however does not currently provide\neasy-to-use, automatic, reproducible and accurate approaches dedicated to this\ntask. We propose a forecasting method that can be used as a strong baseline in\nthis domain, leveraging state-of-the-art forecasting techniques, forecast\ncombination, and global modelling. Our approach uses four base forecasting\nmodels specifically suitable for forecasting weekly data: a global Recurrent\nNeural Network model, Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS),\nand Dynamic Harmonic Regression ARIMA (DHR-ARIMA). Those are then optimally\ncombined using a lasso regression stacking approach. We evaluate the\nperformance of our method against a set of state-of-the-art weekly forecasting\nmodels on six datasets. Across four evaluation metrics, we show that our method\nconsistently outperforms the benchmark methods by a considerable margin with\nstatistical significance. In particular, our model can produce the most\naccurate forecasts, in terms of mean sMAPE, for the M4 weekly dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 04:29:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Godahewa", "Rakshitha", ""], ["Bergmeir", "Christoph", ""], ["Webb", "Geoffrey I.", ""], ["Montero-Manso", "Pablo", ""]]}, {"id": "2010.08219", "submitter": "Yuge Zhang", "authors": "Yuge Zhang, Quanlu Zhang, Yaming Yang", "title": "How Does Supernet Help in Neural Architecture Search?", "comments": "Accepted by 2nd Workshop on Neural Architecture Search at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight sharing, as an approach to speed up architecture performance\nestimation has received wide attention. Instead of training each architecture\nseparately, weight sharing builds a supernet that assembles all the\narchitectures as its submodels. However, there has been debate over whether the\nNAS process actually benefits from weight sharing, due to the gap between\nsupernet optimization and the objective of NAS. To further understand the\neffect of weight sharing on NAS, we conduct a comprehensive analysis on five\nsearch spaces, including NAS-Bench-101, NAS-Bench-201, DARTS-CIFAR10,\nDARTS-PTB, and ProxylessNAS. We find that weight sharing works well on some\nsearch spaces but fails on others. Taking a step forward, we further identified\nbiases accounting for such phenomenon and the capacity of weight sharing. Our\nwork is expected to inspire future NAS researchers to better leverage the power\nof weight sharing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:07:03 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 07:26:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhang", "Yuge", ""], ["Zhang", "Quanlu", ""], ["Yang", "Yaming", ""]]}, {"id": "2010.08251", "submitter": "Andr\\'as Horv\\'ath", "authors": "Andras Horvath, Jalal Al-afandi", "title": "Filtered Batch Normalization", "comments": "Submitted to ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a common assumption that the activation of different layers in neural\nnetworks follow Gaussian distribution. This distribution can be transformed\nusing normalization techniques, such as batch-normalization, increasing\nconvergence speed and improving accuracy. In this paper we would like to\ndemonstrate, that activations do not necessarily follow Gaussian distribution\nin all layers. Neurons in deeper layers are more selective and specific which\ncan result extremely large, out-of-distribution activations.\n  We will demonstrate that one can create more consistent mean and variance\nvalues for batch normalization during training by filtering out these\nactivations which can further improve convergence speed and yield higher\nvalidation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:56:57 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Horvath", "Andras", ""], ["Al-afandi", "Jalal", ""]]}, {"id": "2010.08304", "submitter": "Daehoon Gwak", "authors": "Daehoon Gwak, Gyuhyeon Sim, Michael Poli, Stefano Massaroli, Jaegul\n  Choo, Edward Choi", "title": "Neural Ordinary Differential Equations for Intervention Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By interpreting the forward dynamics of the latent representation of neural\nnetworks as an ordinary differential equation, Neural Ordinary Differential\nEquation (Neural ODE) emerged as an effective framework for modeling a system\ndynamics in the continuous time domain. However, real-world systems often\ninvolves external interventions that cause changes in the system dynamics such\nas a moving ball coming in contact with another ball, or such as a patient\nbeing administered with particular drug. Neural ODE and a number of its recent\nvariants, however, are not suitable for modeling such interventions as they do\nnot properly model the observations and the interventions separately. In this\npaper, we propose a novel neural ODE-based approach (IMODE) that properly model\nthe effect of external interventions by employing two ODE functions to\nseparately handle the observations and the interventions. Using both synthetic\nand real-world time-series datasets involving interventions, our experimental\nresults consistently demonstrate the superiority of IMODE compared to existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:55:12 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gwak", "Daehoon", ""], ["Sim", "Gyuhyeon", ""], ["Poli", "Michael", ""], ["Massaroli", "Stefano", ""], ["Choo", "Jaegul", ""], ["Choi", "Edward", ""]]}, {"id": "2010.08431", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Measuring Behavioural Similarity of Cellular Automata", "comments": null, "journal-ref": "Artificial Life, 27(1), 62-71 (2021)", "doi": "10.1162/artl_a_00337", "report-no": null, "categories": "cs.NE nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conway's Game of Life is the best-known cellular automaton. It is a classic\nmodel of emergence and self-organization, it is Turing-complete, and it can\nsimulate a universal constructor. The Game of Life belongs to the set of\nsemi-totalistic cellular automata, a family with 262,144 members. Many of these\nautomata may deserve as much attention as the Game of Life, if not more. The\nchallenge we address here is to provide a structure for organizing this large\nfamily, to make it easier to find interesting automata, and to understand the\nrelations between automata. Packard and Wolfram (1985) divided the family into\nfour classes, based on the observed behaviours of the rules. Eppstein (2010)\nproposed an alternative four-class system, based on the forms of the rules.\nInstead of a class-based organization, we propose a continuous high-dimensional\nvector space, where each automaton is represented by a point in the space. The\ndistance between two automata in this space corresponds to the differences in\ntheir behavioural characteristics. Nearest neighbours in the space have similar\nbehaviours. This space should make it easier for researchers to see the\nstructure of the family of semi-totalistic rules and to find the hidden gems in\nthe family.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:53:33 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 22:19:26 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "2010.08508", "submitter": "Yamini Bansal", "authors": "Yamini Bansal, Gal Kaplun, Boaz Barak", "title": "For self-supervised learning, Rationality implies generalization,\n  provably", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a new upper bound on the generalization gap of classifiers that are\nobtained by first using self-supervision to learn a representation $r$ of the\ntraining data, and then fitting a simple (e.g., linear) classifier $g$ to the\nlabels. Specifically, we show that (under the assumptions described below) the\ngeneralization gap of such classifiers tends to zero if $\\mathsf{C}(g) \\ll n$,\nwhere $\\mathsf{C}(g)$ is an appropriately-defined measure of the simple\nclassifier $g$'s complexity, and $n$ is the number of training samples. We\nstress that our bound is independent of the complexity of the representation\n$r$. We do not make any structural or conditional-independence assumptions on\nthe representation-learning task, which can use the same training dataset that\nis later used for classification. Rather, we assume that the training procedure\nsatisfies certain natural noise-robustness (adding small amount of label noise\ncauses small degradation in performance) and rationality (getting the wrong\nlabel is not better than getting no label at all) conditions that widely hold\nacross many standard architectures. We show that our bound is non-vacuous for\nmany popular representation-learning based classifiers on CIFAR-10 and\nImageNet, including SimCLR, AMDIM and MoCo.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:07:52 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bansal", "Yamini", ""], ["Kaplun", "Gal", ""], ["Barak", "Boaz", ""]]}, {"id": "2010.08690", "submitter": "Jeffrey Shainline", "authors": "Jeffrey M. Shainline", "title": "Optoelectronic Intelligence", "comments": "10 pages, five figures, perspective article", "journal-ref": null, "doi": "10.1063/5.0040567", "report-no": null, "categories": "cs.ET cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To design and construct hardware for general intelligence, we must consider\nprinciples of both neuroscience and very-large-scale integration. For large\nneural systems capable of general intelligence, the attributes of photonics for\ncommunication and electronics for computation are complementary and\ninterdependent. Using light for communication enables high fan-out as well as\nlow-latency signaling across large systems with no traffic-dependent\nbottlenecks. For computation, the inherent nonlinearities, high speed, and low\npower consumption of Josephson circuits are conducive to complex neural\nfunctions. Operation at 4\\,K enables the use of single-photon detectors and\nsilicon light sources, two features that lead to efficiency and economical\nscalability. Here I sketch a concept for optoelectronic hardware, beginning\nwith synaptic circuits, continuing through wafer-scale integration, and\nextending to systems interconnected with fiber-optic white matter, potentially\nat the scale of the human brain and beyond.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:26:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shainline", "Jeffrey M.", ""]]}, {"id": "2010.08784", "submitter": "Zhuoer Xu", "authors": "Guanghui Zhu, Zhuoer Xu, Xu Guo, Chunfeng Yuan, Yihua Huang", "title": "DIFER: Differentiable Automated Feature Engineering", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering, a crucial step of machine learning, aims to extract\nuseful features from raw data to improve data quality. In recent years, great\nefforts have been devoted to Automated Feature Engineering (AutoFE) to replace\nexpensive human labor. However, existing methods are computationally demanding\ndue to treating AutoFE as a coarse-grained black-box optimization problem over\na discrete space. In this work, we propose an efficient gradient-based method\ncalled DIFER to perform differentiable automated feature engineering in a\ncontinuous vector space. DIFER selects potential features based on evolutionary\nalgorithm and leverages an encoder-predictor-decoder controller to optimize\nexisting features. We map features into the continuous vector space via the\nencoder, optimize the embedding along the gradient direction induced by the\npredicted score, and recover better features from the optimized embedding by\nthe decoder. Extensive experiments on classification and regression datasets\ndemonstrate that DIFER can significantly improve the performance of various\nmachine learning algorithms and outperform current state-of-the-art AutoFE\nmethods in terms of both efficiency and performance.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 12:55:45 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:23:16 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhu", "Guanghui", ""], ["Xu", "Zhuoer", ""], ["Guo", "Xu", ""], ["Yuan", "Chunfeng", ""], ["Huang", "Yihua", ""]]}, {"id": "2010.08853", "submitter": "Gilad Yehudai", "authors": "Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, Haggai Maron", "title": "From Local Structures to Size Generalization in Graph Neural Networks", "comments": "Camera ready version for ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) can process graphs of different sizes, but their\nability to generalize across sizes, specifically from small to large graphs, is\nstill not well understood. In this paper, we identify an important type of data\nwhere generalization from small to large graphs is challenging: graph\ndistributions for which the local structure depends on the graph size. This\neffect occurs in multiple important graph learning domains, including social\nand biological networks. We first prove that when there is a difference between\nthe local structures, GNNs are not guaranteed to generalize across sizes: there\nare \"bad\" global minima that do well on small graphs but fail on large graphs.\nWe then study the size-generalization problem empirically and demonstrate that\nwhen there is a discrepancy in local structure, GNNs tend to converge to\nnon-generalizing solutions. Finally, we suggest two approaches for improving\nsize generalization, motivated by our findings. Notably, we propose a novel\nSelf-Supervised Learning (SSL) task aimed at learning meaningful\nrepresentations of local structures that appear in large graphs. Our SSL task\nimproves classification accuracy on several popular datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 19:36:54 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:49:30 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 19:18:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Yehudai", "Gilad", ""], ["Fetaya", "Ethan", ""], ["Meirom", "Eli", ""], ["Chechik", "Gal", ""], ["Maron", "Haggai", ""]]}, {"id": "2010.08991", "submitter": "Wei  Xu", "authors": "Yifan Luo, Jindan Xu, Wei Xu, Kezhi Wang", "title": "Sliding Differential Evolution Scheduling for Federated Learning in\n  Bandwidth-Limited Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NE cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) in a bandwidth-limited network with energy-limited\nuser equipments (UEs) is under-explored. In this paper, to jointly save energy\nconsumed by the battery-limited UEs and accelerate the convergence of the\nglobal model in FL for the bandwidth-limited network, we propose the sliding\ndifferential evolution-based scheduling (SDES) policy. To this end, we first\nformulate an optimization that aims to minimize a weighted sum of energy\nconsumption and model training convergence. Then, we apply the SDES with\nparallel differential evolution (DE) operations in several small-scale windows,\nto address the above proposed problem effectively. Compared with existing\nscheduling policies, the proposed SDES performs well in reducing energy\nconsumption and the model convergence with lower computational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 14:08:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Luo", "Yifan", ""], ["Xu", "Jindan", ""], ["Xu", "Wei", ""], ["Wang", "Kezhi", ""]]}, {"id": "2010.09280", "submitter": "Yusheng Huang", "authors": "Yusheng Huang (1), Dong Chu (2), Yong Deng (1), Kang Hao Cheong (3 and\n  4) ((1) Institute of Fundamental and Frontier Science, University of\n  Electronic Science and Technology of China, Chengdu, 610054, China, (2)\n  Schools of Information and Communication Engineering, University of\n  Electronic Science and Technology of China, Chengdu, 610054, China, (3)\n  Science, Mathematics and Technology Cluster, Singapore University of\n  Technology and Design (SUTD), S487372, Singapore, (4) SUTD-Massachusetts\n  Institute of Technology International Design Centre, Singapore)", "title": "The Capacity Constraint Physarum Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physarum polycephalum inspired algorithm (PPA), also known as the Physarum\nSolver, has attracted great attention. By modelling real-world problems into a\ngraph with network flow and adopting proper equations to calculate the distance\nbetween the nodes in the graph, PPA could be used to solve system optimization\nproblems or user equilibrium problems. However, some problems such as the\nmaximum flow (MF) problem, minimum-cost-maximum-flow (MCMF) problem, and\nlink-capacitated traffic assignment problem (CTAP), require the flow flowing\nthrough links to follow capacity constraints. Motivated by the lack of related\nPPA-based research, a novel framework, the capacitated physarum polycephalum\ninspired algorithm (CPPA), is proposed to allow capacity constraints toward\nlink flow in the PPA. To prove the validity of the CPPA, we developed three\napplications of the CPPA, i.e., the CPPA for the MF problem (CPPA-MF), the CPPA\nfor the MCFC problem, and the CPPA for the link-capacitated traffic assignment\nproblem (CPPA-CTAP). In the experiments, all the applications of the CPPA solve\nthe problems successfully. Some of them demonstrate efficiency compared to the\nbaseline algorithms. The experimental results prove the validation of using the\nCPPA framework to control link flow in the PPA is valid. The CPPA is also very\nrobust and easy to implement since it could be successfully applied in three\ndifferent scenarios. The proposed method shows that: having the ability to\ncontrol the maximum among flow flowing through links in the PPA, the CPPA could\ntackle more complex real-world problems in the future.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:46:19 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Huang", "Yusheng", "", "3 and\n  4"], ["Chu", "Dong", "", "3 and\n  4"], ["Deng", "Yong", "", "3 and\n  4"], ["Cheong", "Kang Hao", "", "3 and\n  4"]]}, {"id": "2010.09309", "submitter": "Thanh Pham Dinh", "authors": "Phan Thi Hong Hanh, Pham Dinh Thanh and Huynh Thi Thanh Binh", "title": "Evolutionary Algorithm and Multifactorial Evolutionary Algorithm on\n  Clustered Shortest-Path Tree problem", "comments": "10 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In literature, Clustered Shortest-Path Tree Problem (CluSPT) is an NP-hard\nproblem. Previous studies often search for an optimal solution in relatively\nlarge space. To enhance the performance of the search process, two approaches\nare proposed: the first approach seeks for solutions as a set of edges. From\nthe original graph, we generate a new graph whose vertex set's cardinality is\nmuch smaller than that of the original one. Consequently, an effective\nEvolutionary Algorithm (EA) is proposed for solving CluSPT. The second approach\nlooks for vertex-based solutions. The search space of the CluSPT is transformed\ninto 2 nested search spaces (NSS). With every candidate in the high-level\noptimization, the search engine in the lower level will find a corresponding\ncandidate to combine with it to create the best solution for CluSPT.\nAccordingly, Nested Local Search EA (N-LSEA) is introduced to search for the\noptimal solution on the NSS. When solving this model in lower level by N-LSEA,\nvariety of similar tasks are handled. Thus, Multifactorial Evolutionary\nAlgorithm applied in order to enhance the implicit genetic transfer across\nthese optimizations. Proposed algorithms are conducted on a series of datasets\nand the obtained results demonstrate superior efficiency in comparison to\nprevious scientific works.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:37:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hanh", "Phan Thi Hong", ""], ["Thanh", "Pham Dinh", ""], ["Binh", "Huynh Thi Thanh", ""]]}, {"id": "2010.09452", "submitter": "Joe Townsend Dr", "authors": "Joe Townsend, Theodoros Kasioumis and Hiroya Inakoshi", "title": "ERIC: Extracting Relations Inferred from Convolutions", "comments": "Accepted for poster presentation at ACCV (Asian Conference on\n  Computer Vision) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main contribution is to show that the behaviour of kernels across\nmultiple layers of a convolutional neural network can be approximated using a\nlogic program. The extracted logic programs yield accuracies that correlate\nwith those of the original model, though with some information loss in\nparticular as approximations of multiple layers are chained together or as\nlower layers are quantised. We also show that an extracted program can be used\nas a framework for further understanding the behaviour of CNNs. Specifically,\nit can be used to identify key kernels worthy of deeper inspection and also\nidentify relationships with other kernels in the form of the logical rules.\nFinally, we make a preliminary, qualitative assessment of rules we extract from\nthe last convolutional layer and show that kernels identified are symbolic in\nthat they react strongly to sets of similar images that effectively divide\noutput classes into sub-classes with distinct characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:04:21 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Townsend", "Joe", ""], ["Kasioumis", "Theodoros", ""], ["Inakoshi", "Hiroya", ""]]}, {"id": "2010.09458", "submitter": "Tomasz Szanda{\\l}a", "authors": "Tomasz Szanda{\\l}a", "title": "Review and Comparison of Commonly Used Activation Functions for Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-15-5495-7", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The primary neural networks decision-making units are activation functions.\nMoreover, they evaluate the output of networks neural node; thus, they are\nessential for the performance of the whole network. Hence, it is critical to\nchoose the most appropriate activation function in neural networks calculation.\nAcharya et al. (2018) suggest that numerous recipes have been formulated over\nthe years, though some of them are considered deprecated these days since they\nare unable to operate properly under some conditions. These functions have a\nvariety of characteristics, which are deemed essential to successfully\nlearning. Their monotonicity, individual derivatives, and finite of their range\nare some of these characteristics (Bach 2017). This research paper will\nevaluate the commonly used additive functions, such as swish, ReLU, Sigmoid,\nand so forth. This will be followed by their properties, own cons and pros, and\nparticular formula application recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:09:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Szanda\u0142a", "Tomasz", ""]]}, {"id": "2010.09531", "submitter": "Gongjin Lan", "authors": "Gongjin Lan, Maarten van Hooft, Matteo De Carlo, Jakub M. Tomczak,\n  A.E. Eiben", "title": "Learning Locomotion Skills in Evolvable Robots", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of robotic reproduction -- making of new robots by recombining\ntwo existing ones -- has been recently cracked and physically evolving robot\nsystems have come within reach. Here we address the next big hurdle: producing\nan adequate brain for a newborn robot. In particular, we address the task of\ntargeted locomotion which is arguably a fundamental skill in any practical\nimplementation. We introduce a controller architecture and a generic learning\nmethod to allow a modular robot with an arbitrary shape to learn to walk\ntowards a target and follow this target if it moves. Our approach is validated\non three robots, a spider, a gecko, and their offspring, in three real-world\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:01:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Lan", "Gongjin", ""], ["van Hooft", "Maarten", ""], ["De Carlo", "Matteo", ""], ["Tomczak", "Jakub M.", ""], ["Eiben", "A. E.", ""]]}, {"id": "2010.09635", "submitter": "Konstantinos Michmizos", "authors": "Guangzhi Tang, Neelesh Kumar, Raymond Yoo, Konstantinos P. Michmizos", "title": "Deep Reinforcement Learning with Population-Coded Spiking Neural Network\n  for Continuous Control", "comments": "Conference on Robot Learning (CoRL) 2020, 14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy-efficient control of mobile robots is crucial as the complexity of\ntheir real-world applications increasingly involves high-dimensional\nobservation and action spaces, which cannot be offset by limited on-board\nresources. An emerging non-Von Neumann model of intelligence, where spiking\nneural networks (SNNs) are run on neuromorphic processors, is regarded as an\nenergy-efficient and robust alternative to the state-of-the-art real-time\nrobotic controllers for low dimensional control tasks. The challenge now for\nthis new computing paradigm is to scale so that it can keep up with real-world\ntasks. To do so, SNNs need to overcome the inherent limitations of their\ntraining, namely the limited ability of their spiking neurons to represent\ninformation and the lack of effective learning algorithms. Here, we propose a\npopulation-coded spiking actor network (PopSAN) trained in conjunction with a\ndeep critic network using deep reinforcement learning (DRL). The population\ncoding scheme dramatically increased the representation capacity of the network\nand the hybrid learning combined the training advantages of deep networks with\nthe energy-efficient inference of spiking networks. To show the general\napplicability of our approach, we integrated it with a spectrum of both\non-policy and off-policy DRL algorithms. We deployed the trained PopSAN on\nIntel's Loihi neuromorphic chip and benchmarked our method against the\nmainstream DRL algorithms for continuous control. To allow for a fair\ncomparison among all methods, we validated them on OpenAI gym tasks. Our\nLoihi-run PopSAN consumed 140 times less energy per inference when compared\nagainst the deep actor network on Jetson TX2, and had the same level of\nperformance. Our results support the efficiency of neuromorphic controllers and\nsuggest our hybrid RL as an alternative to deep learning, when both\nenergy-efficiency and robustness are important.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:20:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tang", "Guangzhi", ""], ["Kumar", "Neelesh", ""], ["Yoo", "Raymond", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2010.09690", "submitter": "Mingyuan Meng", "authors": "Xingyu Yang, Mingyuan Meng, Shanlin Xiao, and Zhiyi Yu", "title": "SPA: Stochastic Probability Adjustment for System Balance of\n  Unsupervised SNNs", "comments": "Published at the 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR),\n  2021, pp. 6417-6424", "doi": "10.1109/ICPR48806.2021.9412266", "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Spiking neural networks (SNNs) receive widespread attention because of their\nlow-power hardware characteristic and brain-like signal response mechanism, but\ncurrently, the performance of SNNs is still behind Artificial Neural Networks\n(ANNs). We build an information theory-inspired system called Stochastic\nProbability Adjustment (SPA) system to reduce this gap. The SPA maps the\nsynapses and neurons of SNNs into a probability space where a neuron and all\nconnected pre-synapses are represented by a cluster. The movement of synaptic\ntransmitter between different clusters is modeled as a Brownian-like stochastic\nprocess in which the transmitter distribution is adaptive at different firing\nphases. We experimented with a wide range of existing unsupervised SNN\narchitectures and achieved consistent performance improvements. The\nimprovements in classification accuracy have reached 1.99% and 6.29% on the\nMNIST and EMNIST datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:28:38 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 07:53:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yang", "Xingyu", ""], ["Meng", "Mingyuan", ""], ["Xiao", "Shanlin", ""], ["Yu", "Zhiyi", ""]]}, {"id": "2010.09923", "submitter": "Gil Shamir", "authors": "Gil I. Shamir and Lorenzo Coviello", "title": "Anti-Distillation: Improving reproducibility of deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have been revolutionary in improving performance of machine\nlearning and artificial intelligence systems. Their high prediction accuracy,\nhowever, comes at a price of \\emph{model irreproducibility\\/} in very high\nlevels that do not occur with classical linear models. Two models, even if they\nare supposedly identical, with identical architecture and identical trained\nparameter sets, and that are trained on the same set of training examples,\nwhile possibly providing identical average prediction accuracies, may predict\nvery differently on individual, previously unseen, examples. \\emph{Prediction\ndifferences\\/} may be as large as the order of magnitude of the predictions\nthemselves. Ensembles have been shown to somewhat mitigate this behavior, but\nwithout an extra push, may not be utilizing their full potential. In this work,\na novel approach, \\emph{Anti-Distillation\\/}, is proposed to address\nirreproducibility in deep networks, where ensemble models are used to generate\npredictions. Anti-Distillation forces ensemble components away from one another\nby techniques like de-correlating their outputs over mini-batches of examples,\nforcing them to become even more different and more diverse. Doing so enhances\nthe benefit of ensembles, making the final predictions more reproducible.\nEmpirical results demonstrate substantial prediction difference reductions\nachieved by Anti-Distillation on benchmark and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:47:12 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shamir", "Gil I.", ""], ["Coviello", "Lorenzo", ""]]}, {"id": "2010.09931", "submitter": "Gil Shamir", "authors": "Gil I. Shamir, Dong Lin, and Lorenzo Coviello", "title": "Smooth activations and reproducibility in deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are gradually penetrating almost every domain in our lives due\nto their amazing success. However, with substantive performance accuracy\nimprovements comes the price of \\emph{irreproducibility}. Two identical models,\ntrained on the exact same training dataset may exhibit large differences in\npredictions on individual examples even when average accuracy is similar,\nespecially when trained on highly distributed parallel systems. The popular\nRectified Linear Unit (ReLU) activation has been key to recent success of deep\nnetworks. We demonstrate, however, that ReLU is also a catalyzer to\nirreproducibility in deep networks. We show that not only can activations\nsmoother than ReLU provide better accuracy, but they can also provide better\naccuracy-reproducibility tradeoffs. We propose a new family of activations;\nSmooth ReLU (\\emph{SmeLU}), designed to give such better tradeoffs, while also\nkeeping the mathematical expression simple, and thus implementation cheap.\nSmeLU is monotonic, mimics ReLU, while providing continuous gradients, yielding\nbetter reproducibility. We generalize SmeLU to give even more flexibility and\nthen demonstrate that SmeLU and its generalized form are special cases of a\nmore general methodology of REctified Smooth Continuous Unit (RESCU)\nactivations. Empirical results demonstrate the superior\naccuracy-reproducibility tradeoffs with smooth activations, SmeLU in\nparticular.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:06:47 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 00:11:42 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Shamir", "Gil I.", ""], ["Lin", "Dong", ""], ["Coviello", "Lorenzo", ""]]}, {"id": "2010.10177", "submitter": "Matthew Ashman", "authors": "Matthew Ashman, Jonathan So, Will Tebbutt, Vincent Fortuin, Michael\n  Pearce, Richard E. Turner", "title": "Sparse Gaussian Process Variational Autoencoders", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, multi-dimensional spatio-temporal datasets are omnipresent in modern\nscience and engineering. An effective framework for handling such data are\nGaussian process deep generative models (GP-DGMs), which employ GP priors over\nthe latent variables of DGMs. Existing approaches for performing inference in\nGP-DGMs do not support sparse GP approximations based on inducing points, which\nare essential for the computational efficiency of GPs, nor do they handle\nmissing data -- a natural occurrence in many spatio-temporal datasets -- in a\nprincipled manner. We address these shortcomings with the development of the\nsparse Gaussian process variational autoencoder (SGP-VAE), characterised by the\nuse of partial inference networks for parameterising sparse GP approximations.\nLeveraging the benefits of amortised variational inference, the SGP-VAE enables\ninference in multi-output sparse GPs on previously unobserved data with no\nadditional training. The SGP-VAE is evaluated in a variety of experiments where\nit outperforms alternative approaches including multi-output GPs and structured\nVAEs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 10:19:56 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 10:29:06 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ashman", "Matthew", ""], ["So", "Jonathan", ""], ["Tebbutt", "Will", ""], ["Fortuin", "Vincent", ""], ["Pearce", "Michael", ""], ["Turner", "Richard E.", ""]]}, {"id": "2010.10359", "submitter": "Aleksandar Miladinovic", "authors": "Aleksandar Miladinovi\\'c, Milo\\v{s} Aj\\v{c}evi\\'c, Agostino Accardo", "title": "Performance of Dual-Augmented Lagrangian Method and Common Spatial\n  Patterns applied in classification of Motor-Imagery BCI", "comments": null, "journal-ref": "GNB2020, June 10th-12th 2020, Trieste, Italy", "doi": "10.6084/m9.figshare.13087376.v1", "report-no": null, "categories": "eess.SP cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motor-imagery based brain-computer interfaces (MI-BCI) have the potential to\nbecome ground-breaking technologies for neurorehabilitation, the\nreestablishment of non-muscular communication and commands for patients\nsuffering from neuronal disorders and disabilities, but also outside of\nclinical practice, for video game control and other entertainment purposes.\nHowever, due to the noisy nature of the used EEG signal, reliable BCI systems\nrequire specialized procedures for features optimization and extraction. This\npaper compares the two approaches, the Common Spatial Patterns with Linear\nDiscriminant Analysis classifier (CSP-LDA), widely used in BCI for extracting\nfeatures in Motor Imagery (MI) tasks, and the Dual-Augmented Lagrangian (DAL)\nframework with three different regularization methods: group sparsity with row\ngroups (DAL-GLR), dual-spectrum (DAL-DS) and l1-norm regularization (DAL-L1).\nThe test has been performed on 7 healthy subjects performing 5 BCI-MI sessions\neach. The preliminary results show that DAL-GLR method outperforms standard\nCSP-LDA, presenting 6.9% lower misclassification error (p-value = 0.008) and\ndemonstrate the advantage of DAL framework for MI-BCI.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 20:50:13 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Miladinovi\u0107", "Aleksandar", ""], ["Aj\u010devi\u0107", "Milo\u0161", ""], ["Accardo", "Agostino", ""]]}, {"id": "2010.10564", "submitter": "Sebastian Sanokowski", "authors": "Sebastian Sanokowski", "title": "Implicit recurrent networks: A novel approach to stationary input\n  processing with recurrent neural networks in deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain cortex, which processes visual, auditory and sensory data in the\nbrain, is known to have many recurrent connections within its layers and from\nhigher to lower layers. But, in the case of machine learning with neural\nnetworks, it is generally assumed that strict feed-forward architectures are\nsuitable for static input data, such as images, whereas recurrent networks are\nrequired mainly for the processing of sequential input, such as language.\nHowever, it is not clear whether also processing of static input data benefits\nfrom recurrent connectivity. In this work, we introduce and test a novel\nimplementation of recurrent neural networks with lateral and feed-back\nconnections into deep learning. This departure from the strict feed-forward\nstructure prevents the use of the standard error backpropagation algorithm for\ntraining the networks. Therefore we provide an algorithm which implements the\nbackpropagation algorithm on a implicit implementation of recurrent networks,\nwhich is different from state-of-the-art implementations of recurrent neural\nnetworks. Our method, in contrast to current recurrent neural networks,\neliminates the use of long chains of derivatives due to many iterative update\nsteps, which makes learning computationally less costly. It turns out that the\npresence of recurrent intra-layer connections within a one-layer implicit\nrecurrent network enhances the performance of neural networks considerably: A\nsingle-layer implicit recurrent network is able to solve the XOR problem, while\na feed-forward network with monotonically increasing activation function fails\nat this task. Finally, we demonstrate that a two-layer implicit recurrent\narchitecture leads to a better performance in a regression task of physical\nparameters from the measured trajectory of a damped pendulum.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:55:32 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Sanokowski", "Sebastian", ""]]}, {"id": "2010.10604", "submitter": "Xinjie Fan", "authors": "Xinjie Fan and Shujian Zhang and Bo Chen and Mingyuan Zhou", "title": "Bayesian Attention Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention modules, as simple and effective tools, have not only enabled deep\nneural networks to achieve state-of-the-art results in many domains, but also\nenhanced their interpretability. Most current models use deterministic\nattention modules due to their simplicity and ease of optimization. Stochastic\ncounterparts, on the other hand, are less popular despite their potential\nbenefits. The main reason is that stochastic attention often introduces\noptimization issues or requires significant model changes. In this paper, we\npropose a scalable stochastic version of attention that is easy to implement\nand optimize. We construct simplex-constrained attention distributions by\nnormalizing reparameterizable distributions, making the training process\ndifferentiable. We learn their parameters in a Bayesian framework where a\ndata-dependent prior is introduced for regularization. We apply the proposed\nstochastic attention modules to various attention-based models, with\napplications to graph node classification, visual question answering, image\ncaptioning, machine translation, and language understanding. Our experiments\nshow the proposed method brings consistent improvements over the corresponding\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:30:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Fan", "Xinjie", ""], ["Zhang", "Shujian", ""], ["Chen", "Bo", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2010.10687", "submitter": "Vinay Rao", "authors": "Vinay Rao, Jascha Sohl-Dickstein", "title": "Is Batch Norm unique? An empirical investigation and prescription to\n  emulate the best properties of common normalizers without batch dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform an extensive empirical study of the statistical properties of\nBatch Norm and other common normalizers. This includes an examination of the\ncorrelation between representations of minibatches, gradient norms, and Hessian\nspectra both at initialization and over the course of training. Through this\nanalysis, we identify several statistical properties which appear linked to\nBatch Norm's superior performance. We propose two simple normalizers,\nPreLayerNorm and RegNorm, which better match these desirable properties without\ninvolving operations along the batch dimension. We show that PreLayerNorm and\nRegNorm achieve much of the performance of Batch Norm without requiring batch\ndependence, that they reliably outperform LayerNorm, and that they can be\napplied in situations where Batch Norm is ineffective.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 00:41:38 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Rao", "Vinay", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2010.10884", "submitter": "Iztok Fister", "authors": "Iztok Fister, Iztok Fister Jr", "title": "uARMSolver: A framework for Association Rule Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel software framework for Association Rule Mining\nnamed uARMSolver. The framework is written fully in C++ and runs on all\nplatforms. It allows users to preprocess their data in a transaction database,\nto make discretization of data, to search for association rules and to guide a\npresentation/visualization of the best rules found using external tools. As\nopposed to the existing software packages or frameworks, this also supports\nnumerical and real-valued types of attributes besides the categorical ones.\nMining the association rules is defined as an optimization and solved using the\nnature-inspired algorithms that can be incorporated easily. Because the\nalgorithms normally discover a huge amount of association rules, the framework\nenables a modular inclusion of so-called visual guiders for extracting the\nknowledge hidden in data, and visualize these using external tools.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 10:36:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Fister", "Iztok", ""], ["Fister", "Iztok", "Jr"]]}, {"id": "2010.10885", "submitter": "Frank Neumann", "authors": "Frank Neumann and Mojgan Pourhassan and Carsten Witt", "title": "Improved Runtime Results for Simple Randomised Search Heuristics on\n  Linear Functions with a Uniform Constraint", "comments": "Journal version to appear in Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade remarkable progress has been made in development of\nsuitable proof techniques for analysing randomised search heuristics. The\ntheoretical investigation of these algorithms on classes of functions is\nessential to the understanding of the underlying stochastic process. Linear\nfunctions have been traditionally studied in this area resulting in tight\nbounds on the expected optimisation time of simple randomised search algorithms\nfor this class of problems. Recently, the constrained version of this problem\nhas gained attention and some theoretical results have also been obtained on\nthis class of problems. In this paper we study the class of linear functions\nunder uniform constraint and investigate the expected optimisation time of\nRandomised Local Search (RLS) and a simple evolutionary algorithm called (1+1)\nEA. We prove a tight bound of $\\Theta(n^2)$ for RLS and improve the previously\nbest known upper bound of (1+1) EA from $O(n^2 \\log (Bw_{\\max}))$ to $O(n^2\\log\nB)$ in expectation and to $O(n^2 \\log n)$ with high probability, where\n$w_{\\max}$ and $B$ are the maximum weight of the linear objective function and\nthe bound of the uniform constraint, respectively. Also, we obtain a tight\nbound of $O(n^2)$ for the (1+1) EA on a special class of instances. We\ncomplement our theoretical studies by experimental investigations that consider\ndifferent values of $B$ and also higher mutation rates that reflect the fact\nthat $2$-bit flips are crucial for dealing with the uniform constraint.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 10:42:39 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Neumann", "Frank", ""], ["Pourhassan", "Mojgan", ""], ["Witt", "Carsten", ""]]}, {"id": "2010.10913", "submitter": "Jakob Bossek", "authors": "Jakob Bossek, Frank Neumann", "title": "Evolutionary Diversity Optimization and the Minimum Spanning Tree\n  Problem", "comments": null, "journal-ref": null, "doi": "10.1145/3449639.3459363", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of evolutionary computation the calculation of diverse sets of\nhigh-quality solutions to a given optimization problem has gained momentum in\nrecent years under the term evolutionary diversity optimization. Theoretical\ninsights into the working principles of baseline evolutionary algorithms for\ndiversity optimization are still rare. In this paper we study the well-known\nMinimum Spanning Tree problem (MST) in the context of diversity optimization\nwhere population diversity is measured by the sum of pairwise edge overlaps.\nTheoretical results provide insights into the fitness landscape of the MST\ndiversity optimization problem pointing out that even for a population of\n$\\mu=2$ fitness plateaus (of constant length) can be reached, but nevertheless\ndiverse sets can be calculated in polynomial time. We supplement our\ntheoretical results with a series of experiments for the unconstrained and\nconstraint case where all solutions need to fulfill a minimal quality\nthreshold. Our results show that a simple $(\\mu+1)$-EA can effectively compute\na diversified population of spanning trees of high quality.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:50:49 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:38:45 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Bossek", "Jakob", ""], ["Neumann", "Frank", ""]]}, {"id": "2010.10926", "submitter": "Gerard Rinkus", "authors": "Rod Rinkus", "title": "Efficient Similarity-Preserving Unsupervised Learning using Modular\n  Sparse Distributed Codes and Novelty-Contingent Noise", "comments": "11 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1701.07879", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is increasing realization in neuroscience that information is\nrepresented in the brain, e.g., neocortex, hippocampus, in the form sparse\ndistributed codes (SDCs), a kind of cell assembly. Two essential questions are:\na) how are such codes formed on the basis of single trials, and how is\nsimilarity preserved during learning, i.e., how do more similar inputs get\nmapped to more similar SDCs. I describe a novel Modular Sparse Distributed Code\n(MSDC) that provides simple, neurally plausible answers to both questions. An\nMSDC coding field (CF) consists of Q WTA competitive modules (CMs), each\ncomprised of K binary units (analogs of principal cells). The modular nature of\nthe CF makes possible a single-trial, unsupervised learning algorithm that\napproximately preserves similarity and crucially, runs in fixed time, i.e., the\nnumber of steps needed to store an item remains constant as the number of\nstored items grows. Further, once items are stored as MSDCs in superposition\nand such that their intersection structure reflects input similarity, both\nfixed time best-match retrieval and fixed time belief update (updating the\nprobabilities of all stored items) also become possible. The algorithm's core\nprinciple is simply to add noise into the process of choosing a code, i.e.,\nchoosing a winner in each CM, which is proportional to the novelty of the\ninput. This causes the expected intersection of the code for an input, X, with\nthe code of each previously stored input, Y, to be proportional to the\nsimilarity of X and Y. Results demonstrating these capabilities for spatial\npatterns are given in the appendix.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:36:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Rinkus", "Rod", ""]]}, {"id": "2010.10995", "submitter": "Harikrishnan Nellippallil Balakrishnan", "authors": "Harikrishnan NB and Pranay SY and Nithin Nagaraj", "title": "A Neurochaos Learning Architecture for Genome Classification", "comments": "20 pages, 20 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been empirical evidence of presence of non-linearity and chaos at\nthe level of single neurons in biological neural networks. The properties of\nchaotic neurons inspires us to employ them in artificial learning systems.\nHere, we propose a Neurochaos Learning (NL) architecture, where the neurons\nused to extract features from data are 1D chaotic maps. ChaosFEX+SVM, an\ninstance of this NL architecture, is proposed as a hybrid combination of chaos\nand classical machine learning algorithm. We formally prove that a single layer\nof NL with a finite number of 1D chaotic neurons satisfies the Universal\nApproximation Theorem with an exact value for the number of chaotic neurons\nneeded to approximate a discrete real valued function with finite support. This\nis made possible due to the topological transitivity property of chaos and the\nexistence of uncountably infinite number of dense orbits for the chosen 1D\nchaotic map. The chaotic neurons in NL get activated under the presence of an\ninput stimulus (data) and output a chaotic firing trajectory. From such chaotic\nfiring trajectories of individual neurons of NL, we extract Firing Time, Firing\nRate, Energy and Entropy that constitute ChaosFEX features. These ChaosFEX\nfeatures are then fed to a Support Vector Machine with linear kernel for\nclassification. The effectiveness of chaotic feature engineering performed by\nNL (ChaosFEX+SVM) is demonstrated for synthetic and real world datasets in the\nlow and high training sample regimes. Specifically, we consider the problem of\nclassification of genome sequences of SARS-CoV-2 from other coronaviruses\n(SARS-CoV-1, MERS-CoV and others). With just one training sample per class for\n1000 random trials of training, we report an average macro F1-score > 0.99 for\nthe classification of SARS-CoV-2 from SARS-CoV-1 genome sequences. Robustness\nof ChaosFEX features to additive noise is also demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:07:02 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["NB", "Harikrishnan", ""], ["SY", "Pranay", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "2010.11158", "submitter": "Radu Tudor Ionescu", "authors": "Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, Marius Popescu", "title": "Black-Box Ripper: Copying black-box models using generative evolutionary\n  algorithms", "comments": "Accepted as Oral at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of replicating the functionality of black-box neural\nmodels, for which we only know the output class probabilities provided for a\nset of input images. We assume back-propagation through the black-box model is\nnot possible and its training images are not available, e.g. the model could be\nexposed only through an API. In this context, we present a teacher-student\nframework that can distill the black-box (teacher) model into a student model\nwith minimal accuracy loss. To generate useful data samples for training the\nstudent, our framework (i) learns to generate images on a proxy data set (with\nimages and classes different from those used to train the black-box) and (ii)\napplies an evolutionary strategy to make sure that each generated data sample\nexhibits a high response for a specific class when given as input to the black\nbox. Our framework is compared with several baseline and state-of-the-art\nmethods on three benchmark data sets. The empirical evidence indicates that our\nmodel is superior to the considered baselines. Although our method does not\nback-propagate through the black-box network, it generally surpasses\nstate-of-the-art methods that regard the teacher as a glass-box model. Our code\nis available at: https://github.com/antoniobarbalau/black-box-ripper.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:25:23 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Barbalau", "Antonio", ""], ["Cosma", "Adrian", ""], ["Ionescu", "Radu Tudor", ""], ["Popescu", "Marius", ""]]}, {"id": "2010.11223", "submitter": "Tim Genewein", "authors": "Vladimir Mikulik, Gr\\'egoire Del\\'etang, Tom McGrath, Tim Genewein,\n  Miljan Martic, Shane Legg, Pedro A. Ortega", "title": "Meta-trained agents implement Bayes-optimal agents", "comments": "Published at 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-based meta-learning is a powerful technique to build agents that adapt\nfast to any task within a target distribution. A previous theoretical study has\nargued that this remarkable performance is because the meta-training protocol\nincentivises agents to behave Bayes-optimally. We empirically investigate this\nclaim on a number of prediction and bandit tasks. Inspired by ideas from\ntheoretical computer science, we show that meta-learned and Bayes-optimal\nagents not only behave alike, but they even share a similar computational\nstructure, in the sense that one agent system can approximately simulate the\nother. Furthermore, we show that Bayes-optimal agents are fixed points of the\nmeta-learning dynamics. Our results suggest that memory-based meta-learning\nmight serve as a general technique for numerically approximating Bayes-optimal\nagents - that is, even for task distributions for which we currently don't\npossess tractable models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 18:05:21 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mikulik", "Vladimir", ""], ["Del\u00e9tang", "Gr\u00e9goire", ""], ["McGrath", "Tom", ""], ["Genewein", "Tim", ""], ["Martic", "Miljan", ""], ["Legg", "Shane", ""], ["Ortega", "Pedro A.", ""]]}, {"id": "2010.11328", "submitter": "Dhananjay Ashok", "authors": "Dhananjay Ashok, Joseph Scott, Sebastian Wetzel, Maysum Panju and\n  Vijay Ganesh", "title": "Logic Guided Genetic Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Auxiliary Truth enhanced Genetic Algorithm (GA) that uses\nlogical or mathematical constraints as a means of data augmentation as well as\nto compute loss (in conjunction with the traditional MSE), with the aim of\nincreasing both data efficiency and accuracy of symbolic regression (SR)\nalgorithms. Our method, logic-guided genetic algorithm (LGGA), takes as input a\nset of labelled data points and auxiliary truths (ATs) (mathematical facts\nknown a priori about the unknown function the regressor aims to learn) and\noutputs a specially generated and curated dataset that can be used with any SR\nmethod. Three key insights underpin our method: first, SR users often know\nsimple ATs about the function they are trying to learn. Second, whenever an SR\nsystem produces a candidate equation inconsistent with these ATs, we can\ncompute a counterexample to prove the inconsistency, and further, this\ncounterexample may be used to augment the dataset and fed back to the SR system\nin a corrective feedback loop. Third, the value addition of these ATs is that\ntheir use in both the loss function and the data augmentation process leads to\nbetter rates of convergence, accuracy, and data efficiency. We evaluate LGGA\nagainst state-of-the-art SR tools, namely, Eureqa and TuringBot on 16 physics\nequations from \"The Feynman Lectures on Physics\" book. We find that using these\nSR tools in conjunction with LGGA results in them solving up to 30.0% more\nequations, needing only a fraction of the amount of data compared to the same\ntool without LGGA, i.e., resulting in up to a 61.9% improvement in data\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 21:57:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ashok", "Dhananjay", ""], ["Scott", "Joseph", ""], ["Wetzel", "Sebastian", ""], ["Panju", "Maysum", ""], ["Ganesh", "Vijay", ""]]}, {"id": "2010.11486", "submitter": "Aneta Neumann", "authors": "Aneta Neumann, Jakob Bossek, Frank Neumann", "title": "Computing Diverse Sets of Solutions for Monotone Submodular Optimisation\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions allow to model many real-world optimisation problems.\nThis paper introduces approaches for computing diverse sets of high quality\nsolutions for submodular optimisation problems. We first present diversifying\ngreedy sampling approaches and analyse them with respect to the diversity\nmeasured by entropy and the approximation quality of the obtained solutions.\nAfterwards, we introduce an evolutionary diversity optimisation approach to\nfurther improve diversity of the set of solutions. We carry out experimental\ninvestigations on popular submodular benchmark functions that show that the\ncombined approaches achieve high quality solutions of large diversity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:11:34 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Neumann", "Aneta", ""], ["Bossek", "Jakob", ""], ["Neumann", "Frank", ""]]}, {"id": "2010.11733", "submitter": "Cedric Buron", "authors": "Nouredine Nour, Reda Belhaj-Soullami, C\\'edric Buron, Alain Peres,\n  Fr\\'ed\\'eric Barbaresco", "title": "Multi-Radar Tracking Optimization for Collaborative Combat", "comments": "Conference On Artificial Intelligence in Defense (CAID'2020), Nov\n  2020, Rennes, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Grids of collaborative netted radars accelerate kill chains through\nmore efficient cross-cueing over centralized command and control. In this\npaper, we propose two novel reward-based learning approaches to decentralized\nnetted radar coordination based on black-box optimization and Reinforcement\nLearning (RL). To make the RL approach tractable, we use a simplification of\nthe problem that we proved to be equivalent to the initial formulation. We\napply these techniques on a simulation where radars can follow multiple targets\nat the same time and show they can learn implicit cooperation by comparing them\nto a greedy baseline.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:42:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Nour", "Nouredine", ""], ["Belhaj-Soullami", "Reda", ""], ["Buron", "C\u00e9dric", ""], ["Peres", "Alain", ""], ["Barbaresco", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2010.11810", "submitter": "R. James Cotton", "authors": "R. James Cotton, Fabian H. Sinz, Andreas S. Tolias", "title": "Factorized Neural Processes for Neural Processes: $K$-Shot Prediction of\n  Neural Responses", "comments": "14 pages, 5 figures, NeurIPS 2020 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial neural networks have achieved state-of-the-art\nperformance for predicting the responses of neurons in the visual cortex to\nnatural stimuli. However, they require a time consuming parameter optimization\nprocess for accurately modeling the tuning function of newly observed neurons,\nwhich prohibits many applications including real-time, closed-loop experiments.\nWe overcome this limitation by formulating the problem as $K$-shot prediction\nto directly infer a neuron's tuning function from a small set of\nstimulus-response pairs using a Neural Process. This required us to developed a\nFactorized Neural Process, which embeds the observed set into a latent space\npartitioned into the receptive field location and the tuning function\nproperties. We show on simulated responses that the predictions and\nreconstructed receptive fields from the Factorized Neural Process approach\nground truth with increasing number of trials. Critically, the latent\nrepresentation that summarizes the tuning function of a neuron is inferred in a\nquick, single forward pass through the network. Finally, we validate this\napproach on real neural data from visual cortex and find that the predictive\naccuracy is comparable to -- and for small $K$ even greater than --\noptimization based approaches, while being substantially faster. We believe\nthis novel deep learning systems identification framework will facilitate\nbetter real-time integration of artificial neural network modeling into\nneuroscience experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:43:59 GMT"}], "update_date": "2020-10-24", "authors_parsed": [["Cotton", "R. James", ""], ["Sinz", "Fabian H.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "2010.11931", "submitter": "Emre Neftci", "authors": "Friedemann Zenke and Emre O. Neftci", "title": "Brain-Inspired Learning on Neuromorphic Substrates", "comments": "All authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic hardware strives to emulate brain-like neural networks and thus\nholds the promise for scalable, low-power information processing on temporal\ndata streams. Yet, to solve real-world problems, these networks need to be\ntrained. However, training on neuromorphic substrates creates significant\nchallenges due to the offline character and the required non-local computations\nof gradient-based learning algorithms. This article provides a mathematical\nframework for the design of practical online learning algorithms for\nneuromorphic substrates. Specifically, we show a direct connection between\nReal-Time Recurrent Learning (RTRL), an online algorithm for computing\ngradients in conventional Recurrent Neural Networks (RNNs), and biologically\nplausible learning rules for training Spiking Neural Networks (SNNs). Further,\nwe motivate a sparse approximation based on block-diagonal Jacobians, which\nreduces the algorithm's computational complexity, diminishes the non-local\ninformation requirements, and empirically leads to good learning performance,\nthereby improving its applicability to neuromorphic substrates. In summary, our\nframework bridges the gap between synaptic plasticity and gradient-based\napproaches from deep learning and lays the foundations for powerful information\nprocessing on future neuromorphic hardware systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:56:59 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zenke", "Friedemann", ""], ["Neftci", "Emre O.", ""]]}, {"id": "2010.11946", "submitter": "Md. Abu Bakr Siddique", "authors": "Mohammad Mahmudur Rahman Khan, Md. Abu Bakr Siddique, Shadman Sakib,\n  Anas Aziz, Ihtyaz Kader Tasawar, Ziad Hossain", "title": "Prediction of Temperature and Rainfall in Bangladesh using Long Short\n  Term Memory Recurrent Neural Networks", "comments": "4th International Symposium on Multidisciplinary Studies and\n  Innovative Technologies, IEEE, 22-24 October, 2020, TURKEY", "journal-ref": "2020 4th International Symposium on Multidisciplinary Studies and\n  Innovative Technologies (ISMSIT)", "doi": "10.1109/ISMSIT50672.2020.9254585", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temperature and rainfall have a significant impact on economic growth as well\nas the outbreak of seasonal diseases in a region. In spite of that inadequate\nstudies have been carried out for analyzing the weather pattern of Bangladesh\nimplementing the artificial neural network. Therefore, in this study, we are\nimplementing a Long Short-term Memory (LSTM) model to forecast the month-wise\ntemperature and rainfall by analyzing 115 years (1901-2015) of weather data of\nBangladesh. The LSTM model has shown a mean error of -0.38oC in case of\npredicting the month-wise temperature for 2 years and -17.64mm in case of\npredicting the rainfall. This prediction model can help to understand the\nweather pattern changes as well as studying seasonal diseases of Bangladesh\nwhose outbreaks are dependent on regional temperature and/or rainfall.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:10:32 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Khan", "Mohammad Mahmudur Rahman", ""], ["Siddique", "Md. Abu Bakr", ""], ["Sakib", "Shadman", ""], ["Aziz", "Anas", ""], ["Tasawar", "Ihtyaz Kader", ""], ["Hossain", "Ziad", ""]]}, {"id": "2010.11978", "submitter": "Md. Abu Bakr Siddique", "authors": "Md. Abu Bakr Siddique, Shadman Sakib, Mohammad Mahmudur Rahman Khan,\n  Abyaz Kader Tanzeem, Madiha Chowdhury, Nowrin Yasmin", "title": "Deep Convolutional Neural Networks Model-based Brain Tumor Detection in\n  Brain MRI Images", "comments": "4th International conference on I-SMAC (IoT in Social, Mobile,\n  Analytics and Cloud) (I-SMAC 2020), IEEE, 7-9 October 2020, TamilNadu, INDIA", "journal-ref": "2020 Fourth International Conference on I-SMAC (IoT in Social,\n  Mobile, Analytics and Cloud) (I-SMAC)", "doi": "10.1109/I-SMAC49090.2020.9243461", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Diagnosing Brain Tumor with the aid of Magnetic Resonance Imaging (MRI) has\ngained enormous prominence over the years, primarily in the field of medical\nscience. Detection and/or partitioning of brain tumors solely with the aid of\nMR imaging is achieved at the cost of immense time and effort and demands a lot\nof expertise from engaged personnel. This substantiates the necessity of\nfabricating an autonomous model brain tumor diagnosis. Our work involves\nimplementing a deep convolutional neural network (DCNN) for diagnosing brain\ntumors from MR images. The dataset used in this paper consists of 253 brain MR\nimages where 155 images are reported to have tumors. Our model can single out\nthe MR images with tumors with an overall accuracy of 96%. The model\noutperformed the existing conventional methods for the diagnosis of brain tumor\nin the test dataset (Precision = 0.93, Sensitivity = 1.00, and F1-score =\n0.97). Moreover, the proposed model's average precision-recall score is 0.93,\nCohen's Kappa 0.91, and AUC 0.95. Therefore, the proposed model can help\nclinical experts verify whether the patient has a brain tumor and,\nconsequently, accelerate the treatment procedure.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 07:42:17 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Siddique", "Md. Abu Bakr", ""], ["Sakib", "Shadman", ""], ["Khan", "Mohammad Mahmudur Rahman", ""], ["Tanzeem", "Abyaz Kader", ""], ["Chowdhury", "Madiha", ""], ["Yasmin", "Nowrin", ""]]}, {"id": "2010.11981", "submitter": "Luis Miralles PHD", "authors": "Luis Miralles-Pechu\\'an and Fernando Jim\\'enez and Jos\\'e Manuel\n  Garc\\'ia", "title": "A novel auction system for selecting advertisements in Real-Time bidding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-Time Bidding is a new Internet advertising system that has become very\npopular in recent years. This system works like a global auction where\nadvertisers bid to display their impressions in the publishers' ad slots. The\nmost popular system to select which advertiser wins each auction is the\nGeneralized second-price auction in which the advertiser that offers the most\nwins the bet and is charged with the price of the second largest bet. In this\npaper, we propose an alternative betting system with a new approach that not\nonly considers the economic aspect but also other relevant factors for the\nfunctioning of the advertising system. The factors that we consider are, among\nothers, the benefit that can be given to each advertiser, the probability of\nconversion from the advertisement, the probability that the visit is\nfraudulent, how balanced are the networks participating in RTB and if the\nadvertisers are not paying over the market price. In addition, we propose a\nmethodology based on genetic algorithms to optimize the selection of each\nadvertiser. We also conducted some experiments to compare the performance of\nthe proposed model with the famous Generalized Second-Price method. We think\nthat this new approach, which considers more relevant aspects besides the\nprice, offers greater benefits for RTB networks in the medium and long-term.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:36:41 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Miralles-Pechu\u00e1n", "Luis", ""], ["Jim\u00e9nez", "Fernando", ""], ["Garc\u00eda", "Jos\u00e9 Manuel", ""]]}, {"id": "2010.12047", "submitter": "Juan-Pablo Ortega", "authors": "Lukas Gonon and Juan-Pablo Ortega", "title": "Fading memory echo state networks are universal", "comments": "6 pages letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state networks (ESNs) have been recently proved to be universal\napproximants for input/output systems with respect to various $L ^p$-type\ncriteria. When $1\\leq p< \\infty$, only $p$-integrability hypotheses need to be\nimposed, while in the case $p=\\infty$ a uniform boundedness hypotheses on the\ninputs is required. This note shows that, in the last case, a universal family\nof ESNs can be constructed that contains exclusively elements that have the\necho state and the fading memory properties. This conclusion could not be drawn\nwith the results and methods available so far in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:42:50 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gonon", "Lukas", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "2010.12127", "submitter": "Dongqi Han", "authors": "Dongqi Han, Erik De Schutter, Sungho Hong", "title": "Lamina-specific neuronal properties promote robust, stable signal\n  propagation in feedforward networks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedforward networks (FFN) are ubiquitous structures in neural systems and\nhave been studied to understand mechanisms of reliable signal and information\ntransmission. In many FFNs, neurons in one layer have intrinsic properties that\nare distinct from those in their pre-/postsynaptic layers, but how this affects\nnetwork-level information processing remains unexplored. Here we show that\nlayer-to-layer heterogeneity arising from lamina-specific cellular properties\nfacilitates signal and information transmission in FFNs. Specifically, we found\nthat signal transformations, made by each layer of neurons on an input-driven\nspike signal, demodulate signal distortions introduced by preceding layers.\nThis mechanism boosts information transfer carried by a propagating spike\nsignal and thereby supports reliable spike signal and information transmission\nin a deep FFN. Our study suggests that distinct cell types in neural circuits,\nperforming different computational functions, facilitate information processing\non the whole.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:57:46 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Han", "Dongqi", ""], ["De Schutter", "Erik", ""], ["Hong", "Sungho", ""]]}, {"id": "2010.12197", "submitter": "Yinqian Sun", "authors": "Yinqian Sun, Yi Zeng, Tielin Zhang", "title": "Quantum Superposition Spiking Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum brain as a novel hypothesis states that some non-trivial mechanisms\nin quantum computation, such as superposition and entanglement, may have\nimportant influence for the formation of brain functions. Inspired by this\nidea, we propose Quantum Superposition Spiking Neural Network (QS-SNN), which\nintroduce quantum superposition to spiking neural network models to handel\nchallenges which are hard for other state-of-the-art machine learning models.\nFor human brain, grasping the main information no matter how the background\nchanges is necessary to interact efficiently with diverse environments. As an\nexample, it is easy for human to recognize the digits whether it is white\ncharacter with black background or inversely black character with white\nbackground. While if the current machine learning models are trained with one\nof the cases (e.g. white character with black background), it will be nearly\nimpossible for them to recognize the color inverted version. To handel this\nchallenge, we propose two-compartment spiking neural network with superposition\nstates encoding, which is inspired by quantum information theory and\nspatial-temporal spiking property from neuron information encoding in the\nbrain. Typical network structures like fully-connected ANN, VGG, ResNet and\nDenseNet are challenged with the same task. We train these networks on original\nimage dataset and then invert the background color to test their\ngeneralization. Result shows that artificial neural network can not deal with\nthis condition while the quantum superposition spiking neural network(QS-SNN)\nwhich we proposed in this paper recognizes the color-inverse image\nsuccessfully. Further the QS-SNN shows its robustness when noises are added on\ninputs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:11:53 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 01:27:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sun", "Yinqian", ""], ["Zeng", "Yi", ""], ["Zhang", "Tielin", ""]]}, {"id": "2010.12632", "submitter": "David Lipshutz", "authors": "David Lipshutz, Dmitri B. Chklovskii", "title": "Bio-NICA: A biologically inspired single-layer network for Nonnegative\n  Independent Component Analysis", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation, the problem of separating mixtures of unknown\nsignals into their distinct sources, is an important problem for both\nbiological and engineered signal processing systems. Nonnegative Independent\nComponent Analysis (NICA) is a special case of blind source separation that\nassumes the mixture is a linear combination of independent, nonnegative\nsources. In this work, we derive a single-layer neural network implementation\nof NICA satisfying the following 3 constraints, which are relevant for\nbiological systems and the design of neuromorphic hardware: (i) the network\noperates in the online setting, (ii) the synaptic learning rules are local, and\n(iii) the neural outputs are nonnegative.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:31:49 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lipshutz", "David", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.12644", "submitter": "David Lipshutz", "authors": "David Lipshutz, Charlie Windolf, Siavash Golkar, Dmitri B. Chklovskii", "title": "A biologically plausible neural network for Slow Feature Analysis", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning latent features from time series data is an important problem in\nboth machine learning and brain function. One approach, called Slow Feature\nAnalysis (SFA), leverages the slowness of many salient features relative to the\nrapidly varying input signals. Furthermore, when trained on naturalistic\nstimuli, SFA reproduces interesting properties of cells in the primary visual\ncortex and hippocampus, suggesting that the brain uses temporal slowness as a\ncomputational principle for learning latent features. However, despite the\npotential relevance of SFA for modeling brain function, there is currently no\nSFA algorithm with a biologically plausible neural network implementation, by\nwhich we mean an algorithm operates in the online setting and can be mapped\nonto a neural network with local synaptic updates. In this work, starting from\nan SFA objective, we derive an SFA algorithm, called Bio-SFA, with a\nbiologically plausible neural network implementation. We validate Bio-SFA on\nnaturalistic stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:09:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lipshutz", "David", ""], ["Windolf", "Charlie", ""], ["Golkar", "Siavash", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.12660", "submitter": "Siavash Golkar", "authors": "Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan M. Sengupta,\n  Dmitri B. Chklovskii", "title": "A simple normative network approximates local non-Hebbian learning in\n  the cortex", "comments": "Body and supplementary materials of NeurIPS 2020 paper. 19 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To guide behavior, the brain extracts relevant features from high-dimensional\ndata streamed by sensory organs. Neuroscience experiments demonstrate that the\nprocessing of sensory inputs by cortical neurons is modulated by instructive\nsignals which provide context and task-relevant information. Here, adopting a\nnormative approach, we model these instructive signals as supervisory inputs\nguiding the projection of the feedforward data. Mathematically, we start with a\nfamily of Reduced-Rank Regression (RRR) objective functions which include\nReduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation\nAnalysis (CCA), and derive novel offline and online optimization algorithms,\nwhich we call Bio-RRR. The online algorithms can be implemented by neural\nnetworks whose synaptic learning rules resemble calcium plateau potential\ndependent plasticity observed in the cortex. We detail how, in our model, the\ncalcium plateau potential can be interpreted as a backpropagating error signal.\nWe demonstrate that, despite relying exclusively on biologically plausible\nlocal learning rules, our algorithms perform competitively with existing\nimplementations of RRMSE and CCA.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:49:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Golkar", "Siavash", ""], ["Lipshutz", "David", ""], ["Bahroun", "Yanis", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.12681", "submitter": "Armineh Nourbakhsh", "authors": "Natraj Raman, Armineh Nourbakhsh, Sameena Shah, Manuela Veloso", "title": "Robust Document Representations using Latent Topics and Metadata", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task specific fine-tuning of a pre-trained neural language model using a\ncustom softmax output layer is the de facto approach of late when dealing with\ndocument classification problems. This technique is not adequate when labeled\nexamples are not available at training time and when the metadata artifacts in\na document must be exploited. We address these challenges by generating\ndocument representations that capture both text and metadata artifacts in a\ntask agnostic manner. Instead of traditional auto-regressive or auto-encoding\nbased training, our novel self-supervised approach learns a soft-partition of\nthe input space when generating text embeddings. Specifically, we employ a\npre-learned topic model distribution as surrogate labels and construct a loss\nfunction based on KL divergence. Our solution also incorporates metadata\nexplicitly rather than just augmenting them with text. The generated document\nembeddings exhibit compositional characteristics and are directly used by\ndownstream classification tasks to create decision boundaries from a small\nnumber of labeled examples, thereby eschewing complicated recognition methods.\nWe demonstrate through extensive evaluation that our proposed cross-model\nfusion solution outperforms several competitive baselines on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:52:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Raman", "Natraj", ""], ["Nourbakhsh", "Armineh", ""], ["Shah", "Sameena", ""], ["Veloso", "Manuela", ""]]}, {"id": "2010.12691", "submitter": "Wenrui Zhang", "authors": "Wenrui Zhang, Peng Li", "title": "Skip-Connected Self-Recurrent Spiking Neural Networks with Joint\n  Intrinsic Parameter and Synaptic Weight Training", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important class of spiking neural networks (SNNs), recurrent spiking\nneural networks (RSNNs) possess great computational power and have been widely\nused for processing sequential data like audio and text. However, most RSNNs\nsuffer from two problems. 1. Due to a lack of architectural guidance, random\nrecurrent connectivity is often adopted, which does not guarantee good\nperformance. 2. Training of RSNNs is in general challenging, bottlenecking\nachievable model accuracy. To address these problems, we propose a new type of\nRSNNs called Skip-Connected Self-Recurrent SNNs (ScSr-SNNs). Recurrence in\nScSr-SNNs is introduced in a stereotyped manner by adding self-recurrent\nconnections to spiking neurons, which implements local memory. The network\ndynamics is enriched by skip connections between nonadjacent layers.\nConstructed by simplified self-recurrent and skip connections, ScSr-SNNs are\nable to realize recurrent behaviors similar to those of more complex RSNNs\nwhile the error gradients can be more straightforwardly calculated due to the\nmostly feedforward nature of the network. Moreover, we propose a new\nbackpropagation (BP) method called backpropagated intrinsic plasticity (BIP) to\nfurther boost the performance of ScSr-SNNs by training intrinsic model\nparameters. Unlike standard intrinsic plasticity rules that adjust the neuron's\nintrinsic parameters according to neuronal activity, the proposed BIP methods\noptimize intrinsic parameters based on the backpropagated error gradient of a\nwell-defined global loss function in addition to synaptic weight training.\nBased upon challenging speech and neuromorphic speech datasets including\nTI46-Alpha, TI46-Digits, and N-TIDIGITS, the proposed ScSr-SNNs can boost\nperformance by up to 2.55% compared with other types of RSNNs trained by\nstate-of-the-art BP methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:27:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Wenrui", ""], ["Li", "Peng", ""]]}, {"id": "2010.12702", "submitter": "Juan Carlos Seck Tuoh Mora", "authors": "Juan Carlos Seck-Tuoh-Mora, Nayeli J. Escamilla-Serna, Joselito\n  Medina-Marin, Norberto Hernandez-Romero, Irving Barragan-Vite, Jose R.\n  Corona-Armenta", "title": "A global-local neighborhood search algorithm and tabu search for\n  flexible job shop scheduling problem", "comments": "33 pages, 25 figures, Submitted to: PeerJ Comput. Sci", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Flexible Job Shop Scheduling Problem (FJSP) is a combinatorial problem\nthat continues to be studied extensively due to its practical implications in\nmanufacturing systems and emerging new variants, in order to model and optimize\nmore complex situations that reflect the current needs of the industry better.\nThis work presents a new meta-heuristic algorithm called GLNSA (Global-local\nneighborhood search algorithm), in which the neighborhood concepts of a\ncellular automaton are used, so that a set of leading solutions called\n\"smart_cells\" generates and shares information that helps to optimize instances\nof FJSP. The GLNSA algorithm is complemented with a tabu search that implements\na simplified version of the Nopt1 neighborhood defined in [1] to complement the\noptimization task. The experiments carried out show a satisfactory performance\nof the proposed algorithm, compared with other results published in recent\nalgorithms and widely cited in the specialized bibliography, using 86 test\nproblems, improving the optimal result reported in previous works in two of\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 23:08:51 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 20:09:27 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Seck-Tuoh-Mora", "Juan Carlos", ""], ["Escamilla-Serna", "Nayeli J.", ""], ["Medina-Marin", "Joselito", ""], ["Hernandez-Romero", "Norberto", ""], ["Barragan-Vite", "Irving", ""], ["Corona-Armenta", "Jose R.", ""]]}, {"id": "2010.12747", "submitter": "Aydogan Ozcan", "authors": "Deniz Mengu, Yair Rivenson, Aydogan Ozcan", "title": "Scale-, shift- and rotation-invariant diffractive optical networks", "comments": "28 Pages, 6 Figures, 1 Table", "journal-ref": "ACS Photonics (2020)", "doi": "10.1021/acsphotonics.0c01583", "report-no": null, "categories": "physics.optics cs.NE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research efforts in optical computing have gravitated towards\ndeveloping optical neural networks that aim to benefit from the processing\nspeed and parallelism of optics/photonics in machine learning applications.\nAmong these endeavors, Diffractive Deep Neural Networks (D2NNs) harness\nlight-matter interaction over a series of trainable surfaces, designed using\ndeep learning, to compute a desired statistical inference task as the light\nwaves propagate from the input plane to the output field-of-view. Although,\nearlier studies have demonstrated the generalization capability of diffractive\noptical networks to unseen data, achieving e.g., >98% image classification\naccuracy for handwritten digits, these previous designs are in general\nsensitive to the spatial scaling, translation and rotation of the input\nobjects. Here, we demonstrate a new training strategy for diffractive networks\nthat introduces input object translation, rotation and/or scaling during the\ntraining phase as uniformly distributed random variables to build resilience in\ntheir blind inference performance against such object transformations. This\ntraining strategy successfully guides the evolution of the diffractive optical\nnetwork design towards a solution that is scale-, shift- and\nrotation-invariant, which is especially important and useful for dynamic\nmachine vision applications in e.g., autonomous cars, in-vivo imaging of\nbiomedical specimen, among others.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 02:18:39 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mengu", "Deniz", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2010.13100", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Yunjae Lee, Minsoo Rhu", "title": "Tensor Casting: Co-Designing Algorithm-Architecture for Personalized\n  Recommendation Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are one of the most widely deployed machine\nlearning (ML) workload serviced from cloud datacenters. As such, architectural\nsolutions for high-performance recommendation inference have recently been the\ntarget of several prior literatures. Unfortunately, little have been explored\nand understood regarding the training side of this emerging ML workload. In\nthis paper, we first perform a detailed workload characterization study on\ntraining recommendations, root-causing sparse embedding layer training as one\nof the most significant performance bottlenecks. We then propose our\nalgorithm-architecture co-design called Tensor Casting, which enables the\ndevelopment of a generic accelerator architecture for tensor gather-scatter\nthat encompasses all the key primitives of training embedding layers. When\nprototyped on a real CPU-GPU system, Tensor Casting provides 1.9-21x\nimprovements in training throughput compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:04:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kwon", "Youngeun", ""], ["Lee", "Yunjae", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13103", "submitter": "Minsoo Rhu", "authors": "Yujeong Choi, Yunseong Kim, Minsoo Rhu", "title": "LazyBatching: An SLA-aware Batching System for Cloud Machine Learning\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud ML inference systems, batching is an essential technique to increase\nthroughput which helps optimize total-cost-of-ownership. Prior graph batching\ncombines the individual DNN graphs into a single one, allowing multiple inputs\nto be concurrently executed in parallel. We observe that the coarse-grained\ngraph batching becomes suboptimal in effectively handling the dynamic inference\nrequest traffic, leaving significant performance left on the table. This paper\nproposes LazyBatching, an SLA-aware batching system that considers both\nscheduling and batching in the granularity of individual graph nodes, rather\nthan the entire graph for flexible batching. We show that LazyBatching can\nintelligently determine the set of nodes that can be efficiently batched\ntogether, achieving an average 15x, 1.5x, and 5.5x improvement than graph\nbatching in terms of average response time, throughput, and SLA satisfaction,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:13:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Choi", "Yujeong", ""], ["Kim", "Yunseong", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13290", "submitter": "David Anderson", "authors": "David F. Anderson, Badal Joshi, and Abhishek Deshpande", "title": "On reaction network implementations of neural networks", "comments": "Small edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.DS q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the utilization of deterministically modeled\nchemical reaction networks for the implementation of (feed-forward) neural\nnetworks. We develop a general mathematical framework and prove that the\nordinary differential equations (ODEs) associated with certain reaction network\nimplementations of neural networks have desirable properties including (i)\nexistence of unique positive fixed points that are smooth in the parameters of\nthe model (necessary for gradient descent), and (ii) fast convergence to the\nfixed point regardless of initial condition (necessary for efficient\nimplementation). We do so by first making a connection between neural networks\nand fixed points for systems of ODEs, and then by constructing reaction\nnetworks with the correct associated set of ODEs. We demonstrate the theory by\nconstructing a reaction network that implements a neural network with a\nsmoothed ReLU activation function, though we also demonstrate how to generalize\nthe construction to allow for other activation functions (each with the\ndesirable properties listed previously). As there are multiple types of\n\"networks\" utilized in this paper, we also give a careful introduction to both\nreaction networks and neural networks, in order to disambiguate the overlapping\nvocabulary in the two settings and to clearly highlight the role of each\nnetwork's properties.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:37:26 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:05:28 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 01:17:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Anderson", "David F.", ""], ["Joshi", "Badal", ""], ["Deshpande", "Abhishek", ""]]}, {"id": "2010.13309", "submitter": "C.-H. Huck Yang", "authors": "Chao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Pin-Yu Chen, Sabato\n  Marco Siniscalchi, Xiaoli Ma, Chin-Hui Lee", "title": "Decentralizing Feature Extraction with Quantum Convolutional Neural\n  Network for Automatic Speech Recognition", "comments": "Accepted to IEEE ICASSP 2021. Code is available:\n  https://github.com/huckiyang/QuantumSpeech-QCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS quant-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel decentralized feature extraction approach in federated\nlearning to address privacy-preservation issues for speech recognition. It is\nbuilt upon a quantum convolutional neural network (QCNN) composed of a quantum\ncircuit encoder for feature extraction, and a recurrent neural network (RNN)\nbased end-to-end acoustic model (AM). To enhance model parameter protection in\na decentralized architecture, an input speech is first up-streamed to a quantum\ncomputing server to extract Mel-spectrogram, and the corresponding\nconvolutional features are encoded using a quantum circuit algorithm with\nrandom parameters. The encoded features are then down-streamed to the local RNN\nmodel for the final recognition. The proposed decentralized framework takes\nadvantage of the quantum learning progress to secure models and to avoid\nprivacy leakage attacks. Testing on the Google Speech Commands Dataset, the\nproposed QCNN encoder attains a competitive accuracy of 95.12% in a\ndecentralized model, which is better than the previous architectures using\ncentralized RNN models with convolutional features. We also conduct an in-depth\nstudy of different quantum circuit encoder architectures to provide insights\ninto designing QCNN-based feature extractors. Neural saliency analyses\ndemonstrate a correlation between the proposed QCNN features, class activation\nmaps, and input spectrograms. We provide an implementation for future studies.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 03:36:01 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 05:53:26 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Yang", "Chao-Han Huck", ""], ["Qi", "Jun", ""], ["Chen", "Samuel Yen-Chi", ""], ["Chen", "Pin-Yu", ""], ["Siniscalchi", "Sabato Marco", ""], ["Ma", "Xiaoli", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2010.13311", "submitter": "Pin-Han Chen", "authors": "Chao-Yang Kao, Huang-Chih Kuo, Jian-Wen Chen, Chiung-Liang Lin,\n  Pin-Han Chen and Youn-Long Lin", "title": "RNNAccel: A Fusion Recurrent Neural Network Accelerator for Edge\n  Intelligence", "comments": "This is a paper summited in vlsicad2020 conference in Taiwan. For\n  more information about RNNAccel, see https://neuchips.ai/rnnaccel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many edge devices employ Recurrent Neural Networks (RNN) to enhance their\nproduct intelligence. However, the increasing computation complexity poses\nchallenges for performance, energy efficiency and product development time. In\nthis paper, we present an RNN deep learning accelerator, called RNNAccel, which\nsupports Long Short-Term Memory (LSTM) network, Gated Recurrent Unit (GRU)\nnetwork, and Fully Connected Layer (FC)/ Multiple-Perceptron Layer (MLP)\nnetworks. This RNN accelerator addresses (1) computing unit utilization\nbottleneck caused by RNN data dependency, (2) inflexible design for specific\napplications, (3) energy consumption dominated by memory access, (4) accuracy\nloss due to coefficient compression, and (5) unpredictable performance\nresulting from processor-accelerator integration. Our proposed RNN accelerator\nconsists of a configurable 32-MAC array and a coefficient decompression engine.\nThe MAC array can be scaled-up to meet throughput requirement and power budget.\nIts sophisticated off-line compression and simple hardware-friendly on-line\ndecompression, called NeuCompression, reduces memory footprint up to 16x and\ndecreases memory access power. Furthermore, for easy SOC integration, we\ndeveloped a tool set for bit-accurate simulation and integration result\nvalidation. Evaluated using a keyword spotting application, the 32-MAC RNN\naccelerator achieves 90% MAC utilization, 1.27 TOPs/W at 40nm process, 8x\ncompression ratio, and 90% inference accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 03:36:36 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kao", "Chao-Yang", ""], ["Kuo", "Huang-Chih", ""], ["Chen", "Jian-Wen", ""], ["Lin", "Chiung-Liang", ""], ["Chen", "Pin-Han", ""], ["Lin", "Youn-Long", ""]]}, {"id": "2010.13407", "submitter": "Dominique Vaufreydaz", "authors": "Niranjan Deshpande (CHROMA), Dominique Vaufreydaz (LIG), Anne\n  Spalanzani (CHROMA)", "title": "Behavioral decision-making for urban autonomous driving in the presence\n  of pedestrians using Deep Recurrent Q-Network", "comments": null, "journal-ref": "16th International Conference on Control, Automation, Robotics and\n  Vision (ICARCV), Dec 2020, Shenzhen, China", "doi": null, "report-no": null, "categories": "cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making for autonomous driving in urban environments is challenging\ndue to the complexity of the road structure and the uncertainty in the behavior\nof diverse road users. Traditional methods consist of manually designed rules\nas the driving policy, which require expert domain knowledge, are difficult to\ngeneralize and might give sub-optimal results as the environment gets complex.\nWhereas, using reinforcement learning, optimal driving policy could be learned\nand improved automatically through several interactions with the environment.\nHowever, current research in the field of reinforcement learning for autonomous\ndriving is mainly focused on highway setup with little to no emphasis on urban\nenvironments. In this work, a deep reinforcement learning based decision-making\napproach for high-level driving behavior is proposed for urban environments in\nthe presence of pedestrians. For this, the use of Deep Recurrent Q-Network\n(DRQN) is explored, a method combining state-of-the art Deep Q-Network (DQN)\nwith a long term short term memory (LSTM) layer helping the agent gain a memory\nof the environment. A 3-D state representation is designed as the input\ncombined with a well defined reward function to train the agent for learning an\nappropriate behavior policy in a real-world like urban simulator. The proposed\nmethod is evaluated for dense urban scenarios and compared with a rule-based\napproach and results show that the proposed DRQN based driving behavior\ndecision maker outperforms the rule-based approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:08:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Deshpande", "Niranjan", "", "CHROMA"], ["Vaufreydaz", "Dominique", "", "LIG"], ["Spalanzani", "Anne", "", "CHROMA"]]}, {"id": "2010.13428", "submitter": "Johannes Lengler", "authors": "Johannes Lengler, Simone Riedi", "title": "Runtime analysis of the (mu+1)-EA on the Dynamic BinVal function", "comments": "Full journal version. Conference version has appeared at Evostar\n  2021, journal version submitted to Springer Nature Computer Science (SNCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study evolutionary algorithms in a dynamic setting, where for each\ngeneration a different fitness function is chosen, and selection is performed\nwith respect to the current fitness function. Specifically, we consider Dynamic\nBinVal, in which the fitness functions for each generation is given by the\nlinear function BinVal, but in each generation the order of bits is randomly\npermuted. For the (1+1)-EA it was known that there is an efficiency threshold\n$c_0$ for the mutation parameter, at which the runtime switches from\nquasilinear to exponential. A previous empirical evidence suggested that for\nlarger population size $\\mu$, the threshold may increase. We prove that this is\nat least the case in an $\\varepsilon$-neighborhood around the optimum: the\nthreshold of the (\\mu+1)-EA becomes arbitrarily large if the $\\mu$ is chosen\nlarge enough.\n  However, the most surprising result is obtained by a second order analysis\nfor $\\mu=2$: the threshold INcreases with increasing proximity to the optimum.\nIn particular, the hardest region for optimization is NOT around the optimum.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:55:53 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 06:56:01 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lengler", "Johannes", ""], ["Riedi", "Simone", ""]]}, {"id": "2010.13662", "submitter": "Shun-Cheng Wu", "authors": "Shun-Cheng Wu, Keisuke Tateno, Nassir Navab and Federico Tombari", "title": "SCFusion: Real-time Incremental Scene Reconstruction with Semantic\n  Completion", "comments": null, "journal-ref": "International Conference on 3D Vision (2020), 801-810", "doi": "10.1109/3DV50981.2020.00090", "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time scene reconstruction from depth data inevitably suffers from\nocclusion, thus leading to incomplete 3D models. Partial reconstructions, in\nturn, limit the performance of algorithms that leverage them for applications\nin the context of, e.g., augmented reality, robotic navigation, and 3D mapping.\nMost methods address this issue by predicting the missing geometry as an\noffline optimization, thus being incompatible with real-time applications. We\npropose a framework that ameliorates this issue by performing scene\nreconstruction and semantic scene completion jointly in an incremental and\nreal-time manner, based on an input sequence of depth maps. Our framework\nrelies on a novel neural architecture designed to process occupancy maps and\nleverages voxel states to accurately and efficiently fuse semantic completion\nwith the 3D global model. We evaluate the proposed approach quantitatively and\nqualitatively, demonstrating that our method can obtain accurate 3D semantic\nscene completion in real-time.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:31:52 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 11:40:28 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 08:03:44 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wu", "Shun-Cheng", ""], ["Tateno", "Keisuke", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2010.13731", "submitter": "Francisco Jesus Martinez-Murcia", "authors": "Andr\\'es Ortiz, Francisco J. Martinez-Murcia, Marco A. Formoso, Juan\n  Luis Luque, Auxiliadora S\\'anchez", "title": "Dyslexia detection from EEG signals using SSA component correlation and\n  Convolutional Neural Networks", "comments": "11 pages, 7 figures. Submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective dyslexia diagnosis is not a straighforward task since it is\ntraditionally performed by means of the intepretation of different behavioural\ntests. Moreover, these tests are only applicable to readers. This way, early\ndiagnosis requires the use of specific tasks not only related to reading. Thus,\nthe use of Electroencephalography (EEG) constitutes an alternative for an\nobjective and early diagnosis that can be used with pre-readers. In this way,\nthe extraction of relevant features in EEG signals results crucial for\nclassification. However, the identification of the most relevant features is\nnot straighforward, and predefined statistics in the time or frequency domain\nare not always discriminant enough. On the other hand, classical processing of\nEEG signals based on extracting EEG bands frequency descriptors, usually make\nsome assumptions on the raw signals that could cause indormation loosing. In\nthis work we propose an alternative for analysis in the frequency domain based\non Singluar Spectrum Analysis (SSA) to split the raw signal into components\nrepresenting different oscillatory modes. Moreover, correlation matrices\nobtained for each component among EEG channels are classfied using a\nConvolutional Neural network.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:15:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ortiz", "Andr\u00e9s", ""], ["Martinez-Murcia", "Francisco J.", ""], ["Formoso", "Marco A.", ""], ["Luque", "Juan Luis", ""], ["S\u00e1nchez", "Auxiliadora", ""]]}, {"id": "2010.13900", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash, Ashwin Srinivasan, Lovekesh Vig", "title": "Incorporating Symbolic Domain Knowledge into Graph Neural Networks", "comments": "Accepted in Machine Learning Journal (MLJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our interest is in scientific problems with the following characteristics:\n(1) Data are naturally represented as graphs; (2) The amount of data available\nis typically small; and (3) There is significant domain-knowledge, usually\nexpressed in some symbolic form. These kinds of problems have been addressed\neffectively in the past by Inductive Logic Programming (ILP), by virtue of 2\nimportant characteristics: (a) The use of a representation language that easily\ncaptures the relation encoded in graph-structured data, and (b) The inclusion\nof prior information encoded as domain-specific relations, that can alleviate\nproblems of data scarcity, and construct new relations. Recent advances have\nseen the emergence of deep neural networks specifically developed for\ngraph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have\nbeen shown to be able to handle graph-structured data, less has been done to\ninvestigate the inclusion of domain-knowledge. Here we investigate this aspect\nof GNNs empirically by employing an operation we term \"vertex-enrichment\" and\ndenote the corresponding GNNs as \"VEGNNs\". Using over 70 real-world datasets\nand substantial amounts of symbolic domain-knowledge, we examine the result of\nvertex-enrichment across 5 different variants of GNNs. Our results provide\nsupport for the following: (a) Inclusion of domain-knowledge by\nvertex-enrichment can significantly improve the performance of a GNN. That is,\nthe performance VEGNNs is significantly better than GNNs across all GNN\nvariants; (b) The inclusion of domain-specific relations constructed using ILP\nimproves the performance of VEGNNs, across all GNN variants. Taken together,\nthe results provide evidence that it is possible to incorporate symbolic domain\nknowledge into a GNN, and that ILP can play an important role in providing\nhigh-level relationships that are not easily discovered by a GNN.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:22:21 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 15:55:52 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Srinivasan", "Ashwin", ""], ["Vig", "Lovekesh", ""]]}, {"id": "2010.13943", "submitter": "Jayanta Mandi", "authors": "Jayanta Mandi, Tias Guns", "title": "Interior Point Solving for LP-based prediction+optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving optimization problems is the key to decision making in many real-life\nanalytics applications. However, the coefficients of the optimization problems\nare often uncertain and dependent on external factors, such as future demand or\nenergy or stock prices. Machine learning (ML) models, especially neural\nnetworks, are increasingly being used to estimate these coefficients in a\ndata-driven way. Hence, end-to-end predict-and-optimize approaches, which\nconsider how effective the predicted values are to solve the optimization\nproblem, have received increasing attention. In case of integer linear\nprogramming problems, a popular approach to overcome their non-differentiabilty\nis to add a quadratic penalty term to the continuous relaxation, such that\nresults from differentiating over quadratic programs can be used. Instead we\ninvestigate the use of the more principled logarithmic barrier term, as widely\nused in interior point solvers for linear programming. Specifically, instead of\ndifferentiating the KKT conditions, we consider the homogeneous self-dual\nformulation of the LP and we show the relation between the interior point step\ndirection and corresponding gradients needed for learning. Finally our\nempirical experiments demonstrate our approach performs as good as if not\nbetter than the state-of-the-art QPTL (Quadratic Programming task loss)\nformulation of Wilder et al. and SPO approach of Elmachtoub and Grigas.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:05:21 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Mandi", "Jayanta", ""], ["Guns", "Tias", ""]]}, {"id": "2010.13952", "submitter": "Farzaneh Khoshnevisan", "authors": "Farzaneh Khoshnevisan and Min Chi", "title": "An Adversarial Domain Separation Framework for Septic Shock Early\n  Prediction Across EHR Systems", "comments": "to be published in 2020 IEEE International Conference on Big Data", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9378058", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling patient disease progression using Electronic Health Records (EHRs)\nis critical to assist clinical decision making. While most of prior work has\nmainly focused on developing effective disease progression models using EHRs\ncollected from an individual medical system, relatively little work has\ninvestigated building robust yet generalizable diagnosis models across\ndifferent systems. In this work, we propose a general domain adaptation (DA)\nframework that tackles two categories of discrepancies in EHRs collected from\ndifferent medical systems: one is caused by heterogeneous patient populations\n(covariate shift) and the other is caused by variations in data collection\nprocedures (systematic bias). Prior research in DA has mainly focused on\naddressing covariate shift but not systematic bias. In this work, we propose an\nadversarial domain separation framework that addresses both categories of\ndiscrepancies by maintaining one globally-shared invariant latent\nrepresentation across all systems} through an adversarial learning process,\nwhile also allocating a domain-specific model for each system to extract local\nlatent representations that cannot and should not be unified across systems.\nMoreover, our proposed framework is based on variational recurrent neural\nnetwork (VRNN) because of its ability to capture complex temporal dependencies\nand handling missing values in time-series data. We evaluate our framework for\nearly diagnosis of an extremely challenging condition, septic shock, using two\nreal-world EHRs from distinct medical systems in the U.S. The results show that\nby separating globally-shared from domain-specific representations, our\nframework significantly improves septic shock early prediction performance in\nboth EHRs and outperforms the current state-of-the-art DA models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:41:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Khoshnevisan", "Farzaneh", ""], ["Chi", "Min", ""]]}, {"id": "2010.13988", "submitter": "Zhun Deng", "authors": "Zhun Deng, Hangfeng He, Weijie J. Su", "title": "Toward Better Generalization Bounds with Locally Elastic Stability", "comments": "Published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic stability is a key characteristic to ensure the generalization\nability of a learning algorithm. Among different notions of stability,\n\\emph{uniform stability} is arguably the most popular one, which yields\nexponential generalization bounds. However, uniform stability only considers\nthe worst-case loss change (or so-called sensitivity) by removing a single data\npoint, which is distribution-independent and therefore undesirable. There are\nmany cases that the worst-case sensitivity of the loss is much larger than the\naverage sensitivity taken over the single data point that is removed,\nespecially in some advanced models such as random feature models or neural\nnetworks. Many previous works try to mitigate the distribution independent\nissue by proposing weaker notions of stability, however, they either only yield\npolynomial bounds or the bounds derived do not vanish as sample size goes to\ninfinity. Given that, we propose \\emph{locally elastic stability} as a weaker\nand distribution-dependent stability notion, which still yields exponential\ngeneralization bounds. We further demonstrate that locally elastic stability\nimplies tighter generalization bounds than those derived based on uniform\nstability in many situations by revisiting the examples of bounded support\nvector machines, regularized least square regressions, and stochastic gradient\ndescent.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 02:04:53 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 17:29:29 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Deng", "Zhun", ""], ["He", "Hangfeng", ""], ["Su", "Weijie J.", ""]]}, {"id": "2010.14075", "submitter": "Shijun Zhang", "authors": "Zuowei Shen and Haizhao Yang and Shijun Zhang", "title": "Neural Network Approximation: Three Hidden Layers Are Enough", "comments": null, "journal-ref": "Neural Networks, Volume 141, September 2021, Pages 160-173", "doi": "10.1016/j.neunet.2021.04.011", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A three-hidden-layer neural network with super approximation power is\nintroduced. This network is built with the floor function ($\\lfloor x\\rfloor$),\nthe exponential function ($2^x$), the step function ($1_{x\\geq 0}$), or their\ncompositions as the activation function in each neuron and hence we call such\nnetworks as Floor-Exponential-Step (FLES) networks. For any width\nhyper-parameter $N\\in\\mathbb{N}^+$, it is shown that FLES networks with width\n$\\max\\{d,N\\}$ and three hidden layers can uniformly approximate a H\\\"older\ncontinuous function $f$ on $[0,1]^d$ with an exponential approximation rate\n$3\\lambda (2\\sqrt{d})^{\\alpha} 2^{-\\alpha N}$, where $\\alpha \\in(0,1]$ and\n$\\lambda>0$ are the H\\\"older order and constant, respectively. More generally\nfor an arbitrary continuous function $f$ on $[0,1]^d$ with a modulus of\ncontinuity $\\omega_f(\\cdot)$, the constructive approximation rate is\n$2\\omega_f(2\\sqrt{d}){2^{-N}}+\\omega_f(2\\sqrt{d}\\,2^{-N})$. Moreover, we extend\nsuch a result to general bounded continuous functions on a bounded set\n$E\\subseteq\\mathbb{R}^d$. As a consequence, this new class of networks\novercomes the curse of dimensionality in approximation power when the variation\nof $\\omega_f(r)$ as $r\\rightarrow 0$ is moderate (e.g., $\\omega_f(r)\\lesssim\nr^\\alpha$ for H\\\"older continuous functions), since the major term to be\nconcerned in our approximation rate is essentially $\\sqrt{d}$ times a function\nof $N$ independent of $d$ within the modulus of continuity. Finally, we extend\nour analysis to derive similar approximation results in the $L^p$-norm for\n$p\\in[1,\\infty)$ via replacing Floor-Exponential-Step activation functions by\ncontinuous activation functions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 18:30:57 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 10:55:29 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 18:18:38 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 16:53:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shen", "Zuowei", ""], ["Yang", "Haizhao", ""], ["Zhang", "Shijun", ""]]}, {"id": "2010.14184", "submitter": "Anupam Kumar Gupta", "authors": "Anupam K. Gupta, Andrei Nakagawa, Nathan F. Lepora and Nitish V.\n  Thakor", "title": "Spatio-temporal encoding improves neuromorphic tactile texture\n  classification", "comments": "8 pages, 8 figures, accepted for publication to IEEE Sensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in interest in deployment of robots in unstructured\nenvironments to work alongside humans, the development of human-like sense of\ntouch for robots becomes important. In this work, we implement a multi-channel\nneuromorphic tactile system that encodes contact events as discrete spike\nevents that mimic the behavior of slow adapting mechanoreceptors. We study the\nimpact of information pooling across artificial mechanoreceptors on\nclassification performance of spatially non-uniform naturalistic textures. We\nencoded the spatio-temporal activation patterns of mechanoreceptors through\ngray-level co-occurrence matrix computed from time-varying mean spiking\nrate-based tactile response volume. We found that this approach greatly\nimproved texture classification in comparison to use of individual\nmechanoreceptor response alone. In addition, the performance was also more\nrobust to changes in sliding velocity. The importance of exploiting precise\nspatial and temporal correlations between sensory channels is evident from the\nfact that on either removal of precise temporal information or altering of\nspatial structure of response pattern, a significant performance drop was\nobserved. This study thus demonstrates the superiority of population coding\napproaches that can exploit the precise spatio-temporal information encoded in\nactivation patterns of mechanoreceptor populations. It, therefore, makes an\nadvance in the direction of development of bio-inspired tactile systems\nrequired for realistic touch applications in robotics and prostheses.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:37:02 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 12:24:11 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gupta", "Anupam K.", ""], ["Nakagawa", "Andrei", ""], ["Lepora", "Nathan F.", ""], ["Thakor", "Nitish V.", ""]]}, {"id": "2010.14208", "submitter": "Hyeryung Jang", "authors": "Hyeryung Jang, Nicolas Skatchkovsky, Osvaldo Simeone", "title": "Spiking Neural Networks -- Part I: Detecting Spatial Patterns", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) are biologically inspired machine learning\nmodels that build on dynamic neuronal models processing binary and sparse\nspiking signals in an event-driven, online, fashion. SNNs can be implemented on\nneuromorphic computing platforms that are emerging as energy-efficient\nco-processors for learning and inference. This is the first of a series of\nthree papers that introduce SNNs to an audience of engineers by focusing on\nmodels, algorithms, and applications. In this first paper, we first cover\nneural models used for conventional Artificial Neural Networks (ANNs) and SNNs.\nThen, we review learning algorithms and applications for SNNs that aim at\nmimicking the functionality of ANNs by detecting or generating spatial patterns\nin rate-encoded spiking signals. We specifically discuss ANN-to-SNN conversion\nand neural sampling. Finally, we validate the capabilities of SNNs for\ndetecting and generating spatial patterns through experiments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 11:37:22 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 16:58:51 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Jang", "Hyeryung", ""], ["Skatchkovsky", "Nicolas", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2010.14217", "submitter": "Hyeryung Jang", "authors": "Nicolas Skatchkovsky, Hyeryung Jang, Osvaldo Simeone", "title": "Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns", "comments": "The first two authors have equally contributed to this work. This\n  version corrects some errors in the published paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the operation of biological brains, Spiking Neural Networks\n(SNNs) have the unique ability to detect information encoded in spatio-temporal\npatterns of spiking signals. Examples of data types requiring spatio-temporal\nprocessing include logs of time stamps, e.g., of tweets, and outputs of neural\nprostheses and neuromorphic sensors. In this paper, the second of a series of\nthree review papers on SNNs, we first review models and training algorithms for\nthe dominant approach that considers SNNs as a Recurrent Neural Network (RNN)\nand adapt learning rules based on backpropagation through time to the\nrequirements of SNNs. In order to tackle the non-differentiability of the\nspiking mechanism, state-of-the-art solutions use surrogate gradients that\napproximate the threshold activation function with a differentiable function.\nThen, we describe an alternative approach that relies on probabilistic models\nfor spiking neurons, allowing the derivation of local learning rules via\nstochastic estimates of the gradient. Finally, experiments are provided for\nneuromorphic data sets, yielding insights on accuracy and convergence under\ndifferent SNN models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 11:47:42 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 17:06:05 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 17:02:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Skatchkovsky", "Nicolas", ""], ["Jang", "Hyeryung", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2010.14220", "submitter": "Hyeryung Jang", "authors": "Nicolas Skatchkovsky, Hyeryung Jang, Osvaldo Simeone", "title": "Spiking Neural Networks -- Part III: Neuromorphic Communications", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synergies between wireless communications and artificial intelligence are\nincreasingly motivating research at the intersection of the two fields. On the\none hand, the presence of more and more wirelessly connected devices, each with\nits own data, is driving efforts to export advances in machine learning (ML)\nfrom high performance computing facilities, where information is stored and\nprocessed in a single location, to distributed, privacy-minded, processing at\nthe end user. On the other hand, ML can address algorithm and model deficits in\nthe optimization of communication protocols. However, implementing ML models\nfor learning and inference on battery-powered devices that are connected via\nbandwidth-constrained channels remains challenging. This paper explores two\nways in which Spiking Neural Networks (SNNs) can help address these open\nproblems. First, we discuss federated learning for the distributed training of\nSNNs, and then describe the integration of neuromorphic sensing, SNNs, and\nimpulse radio technologies for low-power remote inference.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 11:52:35 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 17:18:15 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Skatchkovsky", "Nicolas", ""], ["Jang", "Hyeryung", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2010.14322", "submitter": "Oliver Hinder", "authors": "Rudy Bunel, Oliver Hinder, Srinadh Bhojanapalli, Krishnamurthy (Dj)\n  Dvijotham", "title": "An efficient nonconvex reformulation of stagewise convex optimization\n  problems", "comments": "First and second authors made equal contribution. To appear in\n  Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization problems with staged structure appear in several\ncontexts, including optimal control, verification of deep neural networks, and\nisotonic regression. Off-the-shelf solvers can solve these problems but may\nscale poorly. We develop a nonconvex reformulation designed to exploit this\nstaged structure. Our reformulation has only simple bound constraints, enabling\nsolution via projected gradient methods and their accelerated variants. The\nmethod automatically generates a sequence of primal and dual feasible solutions\nto the original convex problem, making optimality certification easy. We\nestablish theoretical properties of the nonconvex formulation, showing that it\nis (almost) free of spurious local minima and has the same global optimum as\nthe convex problem. We modify PGD to avoid spurious local minimizers so it\nalways converges to the global minimizer. For neural network verification, our\napproach obtains small duality gaps in only a few gradient steps. Consequently,\nit can quickly solve large-scale verification problems faster than both\noff-the-shelf and specialized solvers.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:30:32 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bunel", "Rudy", "", "Dj"], ["Hinder", "Oliver", "", "Dj"], ["Bhojanapalli", "Srinadh", "", "Dj"], ["Krishnamurthy", "", "", "Dj"], ["Dvijotham", "", ""]]}, {"id": "2010.14535", "submitter": "Dr. Suryansh Kumar", "authors": "Rhea Sanjay Sukthanker, Zhiwu Huang, Suryansh Kumar, Erik Goron\n  Endsjo, Yan Wu, Luc Van Gool", "title": "Neural Architecture Search of SPD Manifold Networks", "comments": "This paper is accepted for publication at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new neural architecture search (NAS) problem of\nSymmetric Positive Definite (SPD) manifold networks, aiming to automate the\ndesign of SPD neural architectures. To address this problem, we first introduce\na geometrically rich and diverse SPD neural architecture search space for an\nefficient SPD cell design. Further, we model our new NAS problem with a\none-shot training process of a single supernet. Based on the supernet modeling,\nwe exploit a differentiable NAS algorithm on our relaxed continuous search\nspace for SPD neural architecture search. Statistical evaluation of our method\non drone, action, and emotion recognition tasks mostly provides better results\nthan the state-of-the-art SPD networks and traditional NAS algorithms.\nEmpirical results show that our algorithm excels in discovering better\nperforming SPD network design and provides models that are more than three\ntimes lighter than searched by the state-of-the-art NAS algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:08:57 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 00:06:46 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 21:06:51 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 21:24:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sukthanker", "Rhea Sanjay", ""], ["Huang", "Zhiwu", ""], ["Kumar", "Suryansh", ""], ["Endsjo", "Erik Goron", ""], ["Wu", "Yan", ""], ["Van Gool", "Luc", ""]]}, {"id": "2010.14611", "submitter": "Matthew Evanusa", "authors": "Matthew Evanusa and Snehesh Shrestha and Michelle Girvan and Cornelia\n  Ferm\\\"uller and Yiannis Aloimonos", "title": "Hybrid Backpropagation Parallel Reservoir Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many real-world applications, fully-differentiable RNNs such as LSTMs and\nGRUs have been widely deployed to solve time series learning tasks. These\nnetworks train via Backpropagation Through Time, which can work well in\npractice but involves a biologically unrealistic unrolling of the network in\ntime for gradient updates, are computationally expensive, and can be hard to\ntune. A second paradigm, Reservoir Computing, keeps the recurrent weight matrix\nfixed and random. Here, we propose a novel hybrid network, which we call Hybrid\nBackpropagation Parallel Echo State Network (HBP-ESN) which combines the\neffectiveness of learning random temporal features of reservoirs with the\nreadout power of a deep neural network with batch normalization. We demonstrate\nthat our new network outperforms LSTMs and GRUs, including multi-layer \"deep\"\nversions of these networks, on two complex real-world multi-dimensional time\nseries datasets: gesture recognition using skeleton keypoints from ChaLearn,\nand the DEAP dataset for emotion recognition from EEG measurements. We show\nalso that the inclusion of a novel meta-ring structure, which we call HBP-ESN\nM-Ring, achieves similar performance to one large reservoir while decreasing\nthe memory required by an order of magnitude. We thus offer this new hybrid\nreservoir deep learning paradigm as a new alternative direction for RNN\nlearning of temporal or sequential data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 21:03:35 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Evanusa", "Matthew", ""], ["Shrestha", "Snehesh", ""], ["Girvan", "Michelle", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2010.14615", "submitter": "Juan-Pablo Ortega", "authors": "Christa Cuchiero, Lukas Gonon, Lyudmila Grigoryeva, Juan-Pablo Ortega,\n  and Josef Teichmann", "title": "Discrete-time signatures and randomness in reservoir computing", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new explanation of geometric nature of the reservoir computing phenomenon\nis presented. Reservoir computing is understood in the literature as the\npossibility of approximating input/output systems with randomly chosen\nrecurrent neural systems and a trained linear readout layer. Light is shed on\nthis phenomenon by constructing what is called strongly universal reservoir\nsystems as random projections of a family of state-space systems that generate\nVolterra series expansions. This procedure yields a state-affine reservoir\nsystem with randomly generated coefficients in a dimension that is\nlogarithmically reduced with respect to the original system. This reservoir\nsystem is able to approximate any element in the fading memory filters class\njust by training a different linear readout for each different filter. Explicit\nexpressions for the probability distributions needed in the generation of the\nprojected reservoir system are stated and bounds for the committed\napproximation error are provided.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 10:55:59 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cuchiero", "Christa", ""], ["Gonon", "Lukas", ""], ["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""], ["Teichmann", "Josef", ""]]}, {"id": "2010.14616", "submitter": "Zeyu Zhang", "authors": "Zeyu Zhang, Guisheng Yin", "title": "Lineage Evolution Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general agent population learning system, and on this basis, we\npropose lineage evolution reinforcement learning algorithm. Lineage evolution\nreinforcement learning is a kind of derivative algorithm which accords with the\ngeneral agent population learning system. We take the agents in DQN and its\nrelated variants as the basic agents in the population, and add the selection,\nmutation and crossover modules in the genetic algorithm to the reinforcement\nlearning algorithm. In the process of agent evolution, we refer to the\ncharacteristics of natural genetic behavior, add lineage factor to ensure the\nretention of potential performance of agent, and comprehensively consider the\ncurrent performance and lineage value when evaluating the performance of agent.\nWithout changing the parameters of the original reinforcement learning\nalgorithm, lineage evolution reinforcement learning can optimize different\nreinforcement learning algorithms. Our experiments show that the idea of\nevolution with lineage improves the performance of original reinforcement\nlearning algorithm in some games in Atari 2600.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 11:58:16 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhang", "Zeyu", ""], ["Yin", "Guisheng", ""]]}, {"id": "2010.14617", "submitter": "Yifei Mao", "authors": "Yifei Mao", "title": "From Artificial Intelligence to Brain Intelligence: The basis learning\n  and memory algorithm for brain-like intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithm of brain learning and memory is still undetermined. The\nbackpropagation algorithm of artificial neural networks was thought not\nsuitable for brain cortex, and there is a lack of algorithm for memory engram.\nWe designed a brain version of backpropagation algorithm, which are\nbiologically plausible and could be implemented with virtual neurons to\ncomplete image classification task. An encoding algorithm that can\nautomatically allocate engram cells is proposed, which is an algorithm\nimplementation for memory engram theory, and could simulate how hippocampus\nachieve fast associative memory. The role of the LTP and LTD in the cerebellum\nis also explained in algorithm level. Our results proposed a method for the\nbrain to deploy backpropagation algorithm, and sparse coding method for memory\nengram theory.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:25:05 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Mao", "Yifei", ""]]}, {"id": "2010.14618", "submitter": "David Powers", "authors": "David M W Powers", "title": "A computationally and cognitively plausible model of supervised and\n  unsupervised learning", "comments": "12 pages, 2 figures, 24 references. Amended version of paper\n  presented at BICS 2013", "journal-ref": "International Conference on Brain Inspired Cognitive Systems 2013,\n  pp. 145-156", "doi": null, "report-no": null, "categories": "cs.NE cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both empirical and mathematical demonstrations of the importance of\nchance-corrected measures are discussed, and a new model of learning is\nproposed based on empirical psychological results on association learning. Two\nforms of this model are developed, the Informatron as a chance-corrected\nPerceptron, and AdaBook as a chance-corrected AdaBoost procedure. Computational\nresults presented show chance correction facilitates learning.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 00:31:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Powers", "David M W", ""]]}, {"id": "2010.14619", "submitter": "Georgiana Neculae", "authors": "Georgiana Neculae and Gavin Brown", "title": "Ensembles of Spiking Neural Networks", "comments": "12 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates that ensembles of spiking neural networks can be\nconstructed so that the ensemble performance is guaranteed to be better than\nthe average performance of a single model. Spiking neural networks have not\nchallenged the performance obtained by conventional neural networks on the same\nproblems. Ensemble learning is a framework that has been used extensively to\nimprove the performance of machine learning models. In this paper, we show how\nto construct ensembles of spiking neural networks that both produce\nstate-of-the-art results, and achieve this with less than 50% of the parameters\nof the original models. We establish the methodology on combining model\npredictions such that performance improvements are guaranteed for spiking\nensembles. For this, we formalize spiking neural networks as GLM predictors,\nidentifying a suitable representation for their target domain. Further, we show\nhow the diversity of our spiking ensembles can be measured using the Ambiguity\nDecomposition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:45:18 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Neculae", "Georgiana", ""], ["Brown", "Gavin", ""]]}, {"id": "2010.14753", "submitter": "Yingshi Chen", "authors": "Yingshi Chen", "title": "A short note on the decision tree based neural turing machine", "comments": "5 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:2010.02921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turing machine and decision tree have developed independently for a long\ntime. With the recent development of differentiable models, there is an\nintersection between them. Neural turing machine(NTM) opens door for the memory\nnetwork. It use differentiable attention mechanism to read/write external\nmemory bank. Differentiable forest brings differentiable properties to\nclassical decision tree. In this short note, we show the deep connection\nbetween these two models. That is: differentiable forest is a special case of\nNTM. Differentiable forest is actually decision tree based neural turing\nmachine. Based on this deep connection, we propose a response augmented\ndifferential forest (RaDF). The controller of RaDF is differentiable forest,\nthe external memory of RaDF are response vectors which would be read/write by\nleaf nodes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 01:39:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chen", "Yingshi", ""]]}, {"id": "2010.14894", "submitter": "Fabien C. Y. Benureau", "authors": "Fabien C. Y. Benureau and Jun Tani", "title": "Morphological Development at the Evolutionary Timescale: Robotic\n  Developmental Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution and development operate at different timescales; generations for\nthe one, a lifetime for the other. These two processes, the basis of much of\nlife on earth, interact in many non-trivial ways, but their temporal\nhierarchy---evolution overarching development---is observed for all\nmulticellular lifeforms. When designing robots however, this tenet lifts: it\nbecomes---however natural---a design choice. We propose to inverse this\ntemporal hierarchy and design a developmental process happening at the\nphylogenetic timescale. Over a classic evolutionary search aimed at finding\ngood gaits for a tentacle robot, we add a developmental process over the\nrobots' morphologies. In each generation, the morphology of the robots does not\nchange. But from one generation to the next, the morphology develops. Much like\nwe become bigger, stronger and heavier as we age, our robots are bigger,\nstronger and heavier with each passing generation. Our robots start with baby\nmorphologies, and a few thousand generations later, end-up with adult ones. We\nshow that this produces better and qualitatively different gaits than an\nevolutionary search with only adult robots, and that it prevents premature\nconvergence by fostering exploration. This method is conceptually simple, and\ncan be effective on small or large populations of robots, and intrinsic to the\nrobot and its morphology, and thus not specific to the task and the fitness\nfunction it is evaluated on. Furthermore, by recasting the evolutionary search\nas a learning process, these results can be viewed in the context of\ndevelopmental learning robotics.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 11:24:23 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Benureau", "Fabien C. Y.", ""], ["Tani", "Jun", ""]]}, {"id": "2010.14998", "submitter": "Stef Maree", "authors": "S. C. Maree, T. Alderliesten, P. A. N. Bosman", "title": "Real-valued Evolutionary Multi-modal Multi-objective Optimization by\n  Hill-Valley Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based evolutionary algorithms (EAs), the underlying search\ndistribution is adapted to the problem at hand, for example based on\ndependencies between decision variables. Hill-valley clustering is an adaptive\nniching method in which a set of solutions is clustered such that each cluster\ncorresponds to a single mode in the fitness landscape. This can be used to\nadapt the search distribution of an EA to the number of modes, exploring each\nmode separately. Especially in a black-box setting, where the number of modes\nis a priori unknown, an adaptive approach is essential for good performance. In\nthis work, we introduce multi-objective hill-valley clustering and combine it\nwith MAMaLGaM, a multi-objective EA, into the multi-objective hill-valley EA\n(MO-HillVallEA). We empirically show that MO-HillVallEA outperforms MAMaLGaM\nand other well-known multi-objective optimization algorithms on a set of\nbenchmark functions. Furthermore, and perhaps most important, we show that\nMO-HillVallEA is capable of obtaining and maintaining multiple approximation\nsets simultaneously over time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:12:25 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Maree", "S. C.", ""], ["Alderliesten", "T.", ""], ["Bosman", "P. A. N.", ""]]}, {"id": "2010.15003", "submitter": "Bhaavan Goel", "authors": "Bhaavan Goel", "title": "Estimating Multiplicative Relations in Neural Networks", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Universal approximation theorem suggests that a shallow neural network can\napproximate any function. The input to neurons at each layer is a weighted sum\nof previous layer neurons and then an activation is applied. These activation\nfunctions perform very well when the output is a linear combination of input\ndata. When trying to learn a function which involves product of input data, the\nneural networks tend to overfit the data to approximate the function. In this\npaper we will use properties of logarithmic functions to propose a pair of\nactivation functions which can translate products into linear expression and\nlearn using backpropagation. We will try to generalize this approach for some\ncomplex arithmetic functions and test the accuracy on a disjoint distribution\nwith the training set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:28:24 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 08:38:33 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Goel", "Bhaavan", ""]]}, {"id": "2010.15045", "submitter": "Javier Lopez Randulfe", "authors": "Javier Lopez Randulfe, Leon Bonde Larsen", "title": "A multi-agent model for growing spiking neural networks", "comments": "79 pages. Master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence has looked into biological systems as a source of\ninspiration. Although there are many aspects of the brain yet to be discovered,\nneuroscience has found evidence that the connections between neurons\ncontinuously grow and reshape as a part of the learning process. This differs\nfrom the design of Artificial Neural Networks, that achieve learning by\nevolving the weights in the synapses between them and their topology stays\nunaltered through time.\n  This project has explored rules for growing the connections between the\nneurons in Spiking Neural Networks as a learning mechanism. These rules have\nbeen implemented on a multi-agent system for creating simple logic functions,\nthat establish a base for building up more complex systems and architectures.\nResults in a simulation environment showed that for a given set of parameters\nit is possible to reach topologies that reproduce the tested functions.\n  This project also opens the door to the usage of techniques like genetic\nalgorithms for obtaining the best suited values for the model parameters, and\nhence creating neural networks that can adapt to different functions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:11:29 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Randulfe", "Javier Lopez", ""], ["Larsen", "Leon Bonde", ""]]}, {"id": "2010.15058", "submitter": "Tomek Korbak", "authors": "Tomasz Korbak and Julian Zubek and Joanna R\\k{a}czaszek-Leonardi", "title": "Measuring non-trivial compositionality in emergent communication", "comments": "4th Workshop on Emergent Communication, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositionality is an important explanatory target in emergent communication\nand language evolution. The vast majority of computational models of\ncommunication account for the emergence of only a very basic form of\ncompositionality: trivial compositionality. A compositional protocol is\ntrivially compositional if the meaning of a complex signal (e.g. blue circle)\nboils down to the intersection of meanings of its constituents (e.g. the\nintersection of the set of blue objects and the set of circles). A protocol is\nnon-trivially compositional (NTC) if the meaning of a complex signal (e.g.\nbiggest apple) is a more complex function of the meanings of their\nconstituents. In this paper, we review several metrics of compositionality used\nin emergent communication and experimentally show that most of them fail to\ndetect NTC - i.e. they treat non-trivial compositionality as a failure of\ncompositionality. The one exception is tree reconstruction error, a metric\nmotivated by formal accounts of compositionality. These results emphasise\nimportant limitations of emergent communication research that could hamper\nprogress on modelling the emergence of NTC.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:11:07 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:22:44 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Korbak", "Tomasz", ""], ["Zubek", "Julian", ""], ["R\u0105czaszek-Leonardi", "Joanna", ""]]}, {"id": "2010.15524", "submitter": "Iztok Fister", "authors": "Iztok Fister Jr., Iztok Fister", "title": "A brief overview of swarm intelligence-based algorithms for numerical\n  association rule mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical Association Rule Mining is a popular variant of Association Rule\nMining, where numerical attributes are handled without discretization. This\nmeans that the algorithms for dealing with this problem can operate directly,\nnot only with categorical, but also with numerical attributes. Until recently,\na big portion of these algorithms were based on a stochastic nature-inspired\npopulation-based paradigm. As a result, evolutionary and swarm\nintelligence-based algorithms showed big efficiency for dealing with the\nproblem. In line with this, the main mission of this chapter is to make a\nhistorical overview of swarm intelligence-based algorithms for Numerical\nAssociation Rule Mining, as well as to present the main features of these\nalgorithms for the observed problem. A taxonomy of the algorithms was proposed\non the basis of the applied features found in this overview. Challenges,\nwaiting in the future, finish this paper.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:44:15 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Fister", "Iztok", "Jr."], ["Fister", "Iztok", ""]]}, {"id": "2010.15571", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios, Behnoosh Zamanlooy", "title": "Learning Sub-Patterns in Piecewise Continuous Functions", "comments": "12 Pages + 7 Page Appendix, 1 Figure, and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most stochastic gradient descent algorithms can optimize neural networks that\nare sub-differentiable in their parameters, which requires their activation\nfunction to exhibit a degree of continuity. However, this continuity constraint\non the activation function prevents these neural models from uniformly\napproximating discontinuous functions. This paper focuses on the case where the\ndiscontinuities arise from distinct sub-patterns, each defined on different\nparts of the input space. We propose a new discontinuous deep neural network\nmodel trainable via a decoupled two-step procedure that avoids passing gradient\nupdates through the network's non-differentiable unit. We provide universal\napproximation guarantees for our architecture in the space of bounded\ncontinuous functions and in the space of piecewise continuous functions, which\nwe introduced herein. We present a novel semi-supervised two-step training\nprocedure for our discontinuous deep learning model, and we provide theoretical\nsupport for its effectiveness. The performance of our architecture is evaluated\nexperimentally on two real-world datasets and one synthetic dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:44:13 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 19:47:11 GMT"}, {"version": "v3", "created": "Sat, 22 May 2021 14:21:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kratsios", "Anastasis", ""], ["Zamanlooy", "Behnoosh", ""]]}, {"id": "2010.15999", "submitter": "Gideon Kowadlo", "authors": "Gideon Kowadlo, Abdelrahman Ahmed, David Rawlinson", "title": "Unsupervised One-shot Learning of Both Specific Instances and\n  Generalised Classes with a Hippocampal Architecture", "comments": "To appear at AI 2020 (Australasian Joint Conference on Artificial\n  Intelligence - http://www.ajcai2020.net/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Established experimental procedures for one-shot machine learning do not test\nthe ability to learn or remember specific instances of classes, a key feature\nof animal intelligence. Distinguishing specific instances is necessary for many\nreal-world tasks, such as remembering which cup belongs to you. Generalisation\nwithin classes conflicts with the ability to separate instances of classes,\nmaking it difficult to achieve both capabilities within a single architecture.\nWe propose an extension to the standard Omniglot classification-generalisation\nframework that additionally tests the ability to distinguish specific instances\nafter one exposure and introduces noise and occlusion corruption. Learning is\ndefined as an ability to classify as well as recall training samples.\nComplementary Learning Systems (CLS) is a popular model of mammalian brain\nregions believed to play a crucial role in learning from a single exposure to a\nstimulus. We created an artificial neural network implementation of CLS and\napplied it to the extended Omniglot benchmark. Our unsupervised model\ndemonstrates comparable performance to existing supervised ANNs on the Omniglot\nclassification task (requiring generalisation), without the need for\ndomain-specific inductive biases. On the extended Omniglot instance-recognition\ntask, the same model also demonstrates significantly better performance than a\nbaseline nearest-neighbour approach, given partial occlusion and noise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 00:10:23 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Kowadlo", "Gideon", ""], ["Ahmed", "Abdelrahman", ""], ["Rawlinson", "David", ""]]}, {"id": "2010.16165", "submitter": "Guangli Li", "authors": "Guangli Li, Xiu Ma, Xueying Wang, Lei Liu, Jingling Xue and Xiaobing\n  Feng", "title": "Fusion-Catalyzed Pruning for Optimizing Deep Learning on Intelligent\n  Edge Devices", "comments": "Published in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2020.3013050", "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing computational cost of deep neural network models limits the\napplicability of intelligent applications on resource-constrained edge devices.\nWhile a number of neural network pruning methods have been proposed to compress\nthe models, prevailing approaches focus only on parametric operators (e.g.,\nconvolution), which may miss optimization opportunities. In this paper, we\npresent a novel fusion-catalyzed pruning approach, called FuPruner, which\nsimultaneously optimizes the parametric and non-parametric operators for\naccelerating neural networks. We introduce an aggressive fusion method to\nequivalently transform a model, which extends the optimization space of pruning\nand enables non-parametric operators to be pruned in a similar manner as\nparametric operators, and a dynamic filter pruning method is applied to\ndecrease the computational cost of models while retaining the accuracy\nrequirement. Moreover, FuPruner provides configurable optimization options for\ncontrolling fusion and pruning, allowing much more flexible\nperformance-accuracy trade-offs to be made. Evaluation with state-of-the-art\nresidual neural networks on five representative intelligent edge platforms,\nJetson TX2, Jetson Nano, Edge TPU, NCS, and NCS2, demonstrates the\neffectiveness of our approach, which can accelerate the inference of models on\nCIFAR-10 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:10:08 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 12:50:10 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Guangli", ""], ["Ma", "Xiu", ""], ["Wang", "Xueying", ""], ["Liu", "Lei", ""], ["Xue", "Jingling", ""], ["Feng", "Xiaobing", ""]]}, {"id": "2010.16262", "submitter": "Tim Bakker", "authors": "Tim Bakker, Herke van Hoof, Max Welling", "title": "Experimental design for MRI by greedy policy search", "comments": "Accepted to NeurIPS 2020 (spotlight), 15-12-2020: Fixed typos, Figure\n  9, and pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's clinical practice, magnetic resonance imaging (MRI) is routinely\naccelerated through subsampling of the associated Fourier domain. Currently,\nthe construction of these subsampling strategies - known as experimental design\n- relies primarily on heuristics. We propose to learn experimental design\nstrategies for accelerated MRI with policy gradient methods. Unexpectedly, our\nexperiments show that a simple greedy approximation of the objective leads to\nsolutions nearly on-par with the more general non-greedy approach. We offer a\npartial explanation for this phenomenon rooted in greater variance in the\nnon-greedy objective's gradient estimates, and experimentally verify that this\nvariance hampers non-greedy models in adapting their policies to individual MR\nimages. We empirically show that this adaptivity is key to improving\nsubsampling designs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:38:09 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 11:12:46 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Bakker", "Tim", ""], ["van Hoof", "Herke", ""], ["Welling", "Max", ""]]}, {"id": "2010.16358", "submitter": "Romain Egele", "authors": "Romain Egele, Prasanna Balaprakash, Venkatram Vishwanath, Isabelle\n  Guyon, Zhengying Liu", "title": "AgEBO-Tabular: Joint Neural Architecture and Hyperparameter Search with\n  Autotuned Data-Parallel Training for Tabular Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high-performing predictive models for large tabular data sets is a\nchallenging task. The state-of-the-art methods are based on expert-developed\nmodel ensembles from different supervised learning methods. Recently, automated\nmachine learning (AutoML) is emerging as a promising approach to automate\npredictive model development. Neural architecture search (NAS) is an AutoML\napproach that generates and evaluates multiple neural network architectures\nconcurrently and improves the accuracy of the generated models iteratively. A\nkey issue in NAS, particularly for large data sets, is the large computation\ntime required to evaluate each generated architecture. While data-parallel\ntraining is a promising approach that can address this issue, its use within\nNAS is difficult. For different data sets, the data-parallel training settings\nsuch as the number of parallel processes, learning rate, and batch size need to\nbe adapted to achieve high accuracy and reduction in training time. To that\nend, we have developed AgEBO-Tabular, an approach to combine aging evolution\n(AgE), a parallel NAS method that searches over neural architecture space, and\nan asynchronous Bayesian optimization method for tuning the hyperparameters of\nthe data-parallel training simultaneously. We demonstrate the efficacy of the\nproposed method to generate high-performing neural network models for large\ntabular benchmark data sets. Furthermore, we demonstrate that the automatically\ndiscovered neural network models using our method outperform the\nstate-of-the-art AutoML ensemble models in inference speed by two orders of\nmagnitude while reaching similar accuracy values.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:28:48 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Egele", "Romain", ""], ["Balaprakash", "Prasanna", ""], ["Vishwanath", "Venkatram", ""], ["Guyon", "Isabelle", ""], ["Liu", "Zhengying", ""]]}]