[{"id": "0812.0743", "submitter": "Qiang Li", "authors": "Qiang Li, Yan He, Jing-ping Jiang", "title": "A Novel Clustering Algorithm Based on Quantum Games", "comments": "19 pages, 5 figures, 5 tables", "journal-ref": "2009 J. Phys. A: Math. Theor. 42 445303", "doi": "10.1088/1751-8113/42/44/445303", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GT cs.MA cs.NE quant-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum game with the problem of data\nclustering, and then develop a quantum-game-based clustering algorithm, in\nwhich data points in a dataset are considered as players who can make decisions\nand implement quantum strategies in quantum games. After each round of a\nquantum game, each player's expected payoff is calculated. Later, he uses a\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\nthe strength of links connecting to them in order to maximize his payoff.\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\npayoff matrixes and two LRR functions. Consequently, the simulation results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms have fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2008 15:46:03 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2009 09:10:36 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Li", "Qiang", ""], ["He", "Yan", ""], ["Jiang", "Jing-ping", ""]]}, {"id": "0812.0882", "submitter": "Philippe Thomas", "authors": "Philippe Thomas (CRAN), Andr\\'e Thomas (CRAN)", "title": "Elagage d'un perceptron multicouches : utilisation de l'analyse de la\n  variance de la sensibilit\\'e des param\\`etres", "comments": "6 pages", "journal-ref": "Conf\\'erence Internationale Francophone d'Automatique CIFA'08,\n  Bucarest : Roumanie (2008)", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stucture determination of a neural network for the modelisation of a\nsystem remain the core of the problem. Within this framework, we propose a\npruning algorithm of the network based on the use of the analysis of the\nsensitivity of the variance of all the parameters of the network. This\nalgorithm will be tested on two examples of simulation and its performances\nwill be compared with three other algorithms of pruning of the literature\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2008 09:12:14 GMT"}], "update_date": "2008-12-05", "authors_parsed": [["Thomas", "Philippe", "", "CRAN"], ["Thomas", "Andr\u00e9", "", "CRAN"]]}, {"id": "0812.1094", "submitter": "Philippe Thomas", "authors": "Philippe Thomas (CRAN), Andr\\'e Thomas (CRAN)", "title": "S\\'election de la structure d'un perceptron multicouches pour la\n  r\\'eduction dun mod\\`ele de simulation d'une scierie", "comments": "7 pages", "journal-ref": "Conf\\'erence Internationale Francophone d'Automatique CIFA'08,\n  Bucarest : Roumanie (2008)", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is often used to evaluate the relevance of a Directing Program of\nProduction (PDP) or to evaluate its impact on detailed sc\\'enarii of\nscheduling. Within this framework, we propose to reduce the complexity of a\nmodel of simulation by exploiting a multilayer perceptron. A main phase of the\nmodeling of one system using a multilayer perceptron remains the determination\nof the structure of the network. We propose to compare and use various pruning\nalgorithms in order to determine the optimal structure of the network used to\nreduce the complexity of the model of simulation of our case of application: a\nsawmill.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2008 08:49:53 GMT"}], "update_date": "2008-12-08", "authors_parsed": [["Thomas", "Philippe", "", "CRAN"], ["Thomas", "Andr\u00e9", "", "CRAN"]]}, {"id": "0812.2535", "submitter": "Kumar Eswaran Dr.", "authors": "Dasika Ratna Deepthi and K.Eswaran", "title": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks", "comments": null, "journal-ref": "Paper No 336, IEEE, ICETiC 2009, International Conference on\n  Emerging Trends in Computing", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new kind of learning implementation to recognize\nthe patterns using the concept of Mirroring Neural Network (MNN) which can\nextract information from distinct sensory input patterns and perform pattern\nrecognition tasks. It is also capable of being used as an advanced associative\nmemory wherein image data is associated with voice inputs in an unsupervised\nmanner. Since the architecture is hierarchical and modular it has the potential\nof being used to devise learning engines of ever increasing complexity.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2008 09:21:31 GMT"}], "update_date": "2008-12-16", "authors_parsed": [["Deepthi", "Dasika Ratna", ""], ["Eswaran", "K.", ""]]}, {"id": "0812.2969", "submitter": "Marco Piastra", "authors": "Marco Piastra", "title": "A Growing Self-Organizing Network for Reconstructing Curves and Surfaces", "comments": null, "journal-ref": "Neural Networks, 2009. IJCNN 2009. International Joint Conference\n  on , vol., no., pp.2533,2540, 14-19 June 2009", "doi": "10.1109/IJCNN.2009.5178709", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organizing networks such as Neural Gas, Growing Neural Gas and many\nothers have been adopted in actual applications for both dimensionality\nreduction and manifold learning. Typically, in these applications, the\nstructure of the adapted network yields a good estimate of the topology of the\nunknown subspace from where the input data points are sampled. The approach\npresented here takes a different perspective, namely by assuming that the input\nspace is a manifold of known dimension. In return, the new type of growing\nself-organizing network presented gains the ability to adapt itself in way that\nmay guarantee the effective and stable recovery of the exact topological\nstructure of the input manifold.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2008 15:59:36 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2008 17:22:02 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Piastra", "Marco", ""]]}, {"id": "0812.4170", "submitter": "Jose Gallardo", "authors": "Jose E. Gallardo, Carlos Cotta, Antonio J. Fernandez", "title": "Finding Still Lifes with Memetic/Exact Hybrid Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum density still life problem (MDSLP) is a hard constraint\noptimization problem based on Conway's game of life. It is a prime example of\nweighted constrained optimization problem that has been recently tackled in the\nconstraint-programming community. Bucket elimination (BE) is a complete\ntechnique commonly used to solve this kind of constraint satisfaction problem.\nWhen the memory required to apply BE is too high, a heuristic method based on\nit (denominated mini-buckets) can be used to calculate bounds for the optimal\nsolution. Nevertheless, the curse of dimensionality makes these techniques\nunpractical for large size problems. In response to this situation, we present\na memetic algorithm for the MDSLP in which BE is used as a mechanism for\nrecombining solutions, providing the best possible child from the parental set.\nSubsequently, a multi-level model in which this exact/metaheuristic hybrid is\nfurther hybridized with branch-and-bound techniques and mini-buckets is\nstudied. Extensive experimental results analyze the performance of these models\nand multi-parent recombination. The resulting algorithm consistently finds\noptimal patterns for up to date solved instances in less time than current\napproaches. Moreover, it is shown that this proposal provides new best known\nsolutions for very large instances.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2008 13:09:11 GMT"}], "update_date": "2008-12-23", "authors_parsed": [["Gallardo", "Jose E.", ""], ["Cotta", "Carlos", ""], ["Fernandez", "Antonio J.", ""]]}, {"id": "0812.4360", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "Driven by Compression Progress: A Simple Principle Explains Essential\n  Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention,\n  Curiosity, Creativity, Art, Science, Music, Jokes", "comments": "35 pages, 3 figures, based on KES 2008 keynote and ALT 2007 / DS 2007\n  joint invited lecture", "journal-ref": "Short version: J. Schmidhuber. Simple Algorithmic Theory of\n  Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity,\n  Creativity, Art, Science, Music, Jokes. Journal of SICE 48(1), 21-32, 2009", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I argue that data becomes temporarily interesting by itself to some\nself-improving, but computationally limited, subjective observer once he learns\nto predict or compress the data in a better way, thus making it subjectively\nsimpler and more beautiful. Curiosity is the desire to create or discover more\nnon-random, non-arbitrary, regular data that is novel and surprising not in the\ntraditional sense of Boltzmann and Shannon but in the sense that it allows for\ncompression progress because its regularity was not yet known. This drive\nmaximizes interestingness, the first derivative of subjective beauty or\ncompressibility, that is, the steepness of the learning curve. It motivates\nexploring infants, pure mathematicians, composers, artists, dancers, comedians,\nyourself, and (since 1990) artificial systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2008 10:14:18 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2009 17:35:06 GMT"}], "update_date": "2009-04-15", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}]