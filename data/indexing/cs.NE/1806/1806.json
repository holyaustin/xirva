[{"id": "1806.00149", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun", "title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative\n  Operators", "comments": "12 pages, 5 figures, 1 table", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (2020)", "doi": "10.1109/TNNLS.2020.3005167", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generic type of stochastic neurons, called $q$-neurons, that\nconsiders activation functions based on Jackson's $q$-derivatives with\nstochastic parameters $q$. Our generalization of neural network architectures\nwith $q$-neurons is shown to be both scalable and very easy to implement. We\ndemonstrate experimentally consistently improved performances over\nstate-of-the-art standard activation functions, both on training and testing\nloss functions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 00:46:29 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 03:47:47 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""]]}, {"id": "1806.00201", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg, Martin Biehl, Nathaniel Virgo, Ryota Kanai", "title": "Being curious about the answers to questions: novelty search with\n  learned attention", "comments": "8 pages, 7 figures, ALife 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of attentional neural network layers in order to learn\na `behavior characterization' which can be used to drive novelty search and\ncuriosity-based policies. The space is structured towards answering a\nparticular distribution of questions, which are used in a supervised way to\ntrain the attentional neural network. We find that in a 2d exploration task,\nthe structure of the space successfully encodes local sensory-motor\ncontingencies such that even a greedy local `do the most novel action' policy\nwith no reinforcement learning or evolution can explore the space quickly. We\nalso apply this to a high/low number guessing game task, and find that guessing\naccording to the learned attention profile performs active inference and can\ndiscover the correct number more quickly than an exact but passive approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 05:32:47 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Biehl", "Martin", ""], ["Virgo", "Nathaniel", ""], ["Kanai", "Ryota", ""]]}, {"id": "1806.00299", "submitter": "Donya Yazdani", "authors": "Dogan Corus, Pietro S. Oliveto, Donya Yazdani", "title": "Fast Artificial Immune Systems", "comments": "Excluding the appendix, this paper will be published in the\n  proceedings of PPSN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various studies have shown that characteristic Artificial Immune System (AIS)\noperators such as hypermutations and ageing can be very efficient at escaping\nlocal optima of multimodal optimisation problems. However, this efficiency\ncomes at the expense of considerably slower runtimes during the exploitation\nphase compared to standard evolutionary algorithms. We propose modifications to\nthe traditional `hypermutations with mutation potential' (HMP) that allow them\nto be efficient at exploitation as well as maintaining their effective\nexplorative characteristics. Rather than deterministically evaluating fitness\nafter each bitflip of a hypermutation, we sample the fitness function\nstochastically with a `parabolic' distribution which allows the `stop at first\nconstructive mutation' (FCM) variant of HMP to reduce the linear amount of\nwasted function evaluations when no improvement is found to a constant. By\nreturning the best sampled solution during the hypermutation, rather than the\nfirst constructive mutation, we then turn the extremely inefficient HMP\noperator without FCM, into a very effective operator for the standard Opt-IA\nAIS using hypermutation, cloning and ageing. We rigorously prove the\neffectiveness of the two proposed operators by analysing them on all problems\nwhere the performance of HPM is rigorously understood in the literature. %\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 11:54:20 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Corus", "Dogan", ""], ["Oliveto", "Pietro S.", ""], ["Yazdani", "Donya", ""]]}, {"id": "1806.00300", "submitter": "Donya Yazdani", "authors": "Dogan Corus, Pietro S. Oliveto, Donya Yazdani", "title": "Artificial Immune Systems Can Find Arbitrarily Good Approximations for\n  the NP-Hard Number Partitioning Problem", "comments": "A conference version of this submission is published in the\n  proceedings of PPSN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typical artificial immune system (AIS) operators such as hypermutations with\nmutation potential and ageing allow to efficiently overcome local optima from\nwhich evolutionary algorithms (EAs) struggle to escape. Such behaviour has been\nshown for artificial example functions constructed especially to show\ndifficulties that EAs may encounter during the optimisation process.\n{\\color{black}However, no evidence is available indicating that these two\noperators have similar behaviour also in more realistic problems.} In this\npaper we perform an analysis for the standard NP-hard \\partition problem from\ncombinatorial optimisation and rigorously show that hypermutations and ageing\nallow AISs to efficiently escape from local optima where standard EAs require\nexponential time. As a result we prove that while EAs and random local search\n(RLS) may get trapped on 4/3 approximations, AISs find arbitrarily good\napproximate solutions of ratio (1+$\\epsilon$) {\\color{black}within $n(\\epsilon\n^{-(2/\\epsilon)-1})(1-\\epsilon)^{-2} e^{3} 2^{2/\\epsilon} + 2n^3 2^{2/\\epsilon}\n+ 2n^3$ function evaluations in expectation. This expectation is polynomial in\nthe problem size and exponential only in $1/\\epsilon$}.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 11:57:17 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 14:58:40 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Corus", "Dogan", ""], ["Oliveto", "Pietro S.", ""], ["Yazdani", "Donya", ""]]}, {"id": "1806.00526", "submitter": "Nima Mohajerin", "authors": "Nima Mohajerin, Steven L. Waslander", "title": "Multi-Step Prediction of Dynamic Systems with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) can encode rich dynamics which makes them\nsuitable for modeling dynamic systems. To train an RNN for multi-step\nprediction of dynamic systems, it is crucial to efficiently address the state\ninitialization problem, which seeks proper values for the RNN initial states at\nthe beginning of each prediction interval. In this work, the state\ninitialization problem is addressed using Neural Networks (NNs) to effectively\ntrain a variety of RNNs for modeling two aerial vehicles, a helicopter and a\nquadrotor, from experimental data. It is shown that the RNN initialized by the\nNN-based initialization method outperforms the state of the art. Further, a\ncomprehensive study of RNNs trained for multi-step prediction of the two aerial\nvehicles is presented. The multi-step prediction of the quadrotor is enhanced\nusing a hybrid model which combines a simplified physics-based motion model of\nthe vehicle with RNNs. While the maximum translational and rotational\nvelocities in the quadrotor dataset are about 4 m/s and 3.8 rad/s,\nrespectively, the hybrid model produces predictions, over 1.9 second, which\nremain within 9 cm/s and 0.12 rad/s of the measured translational and\nrotational velocities, with 99\\% confidence on the test dataset\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 00:37:10 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Mohajerin", "Nima", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1806.00628", "submitter": "Xi Chen", "authors": "Xi Chen, Zhihong Deng, Gehui Shen, Ting Huang", "title": "A Novel Framework for Recurrent Neural Networks with Enhancing\n  Information Processing and Transmission between Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for recurrent neural networks (RNNs)\ninspired by the human memory models in the field of cognitive neuroscience to\nenhance information processing and transmission between adjacent RNNs' units.\nThe proposed framework for RNNs consists of three stages that is working\nmemory, forget, and long-term store. The first stage includes taking input data\ninto sensory memory and transferring it to working memory for preliminary\ntreatment. And the second stage mainly focuses on proactively forgetting the\nsecondary information rather than the primary in the working memory. And\nfinally, we get the long-term store normally using some kind of RNN's unit. Our\nframework, which is generalized and simple, is evaluated on 6 datasets which\nfall into 3 different tasks, corresponding to text classification, image\nclassification and language modelling. Experiments reveal that our framework\ncan obviously improve the performance of traditional recurrent neural networks.\nAnd exploratory task shows the ability of our framework of correctly forgetting\nthe secondary information.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 12:59:18 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Chen", "Xi", ""], ["Deng", "Zhihong", ""], ["Shen", "Gehui", ""], ["Huang", "Ting", ""]]}, {"id": "1806.00730", "submitter": "Yamini Bansal", "authors": "Yamini Bansal, Madhu Advani, David D Cox, Andrew M Saxe", "title": "Minnorm training: an algorithm for training over-parameterized deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new training method for finding minimum weight\nnorm solutions in over-parameterized neural networks (NNs). This method seeks\nto improve training speed and generalization performance by framing NN training\nas a constrained optimization problem wherein the sum of the norm of the\nweights in each layer of the network is minimized, under the constraint of\nexactly fitting training data. It draws inspiration from support vector\nmachines (SVMs), which are able to generalize well, despite often having an\ninfinite number of free parameters in their primal form, and from recent\ntheoretical generalization bounds on NNs which suggest that lower norm\nsolutions generalize better. To solve this constrained optimization problem,\nour method employs Lagrange multipliers that act as integrators of error over\ntraining and identify `support vector'-like examples. The method can be\nimplemented as a wrapper around gradient based methods and uses standard\nback-propagation of gradients from the NN for both regression and\nclassification versions of the algorithm. We provide theoretical justifications\nfor the effectiveness of this algorithm in comparison to early stopping and\n$L_2$-regularization using simple, analytically tractable settings. In\nparticular, we show faster convergence to the max-margin hyperplane in a\nshallow network (compared to vanilla gradient descent); faster convergence to\nthe minimum-norm solution in a linear chain (compared to $L_2$-regularization);\nand initialization-independent generalization performance in a deep linear\nnetwork. Finally, using the MNIST dataset, we demonstrate that this algorithm\ncan boost test accuracy and identify difficult examples in real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 02:33:01 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 15:26:07 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Bansal", "Yamini", ""], ["Advani", "Madhu", ""], ["Cox", "David D", ""], ["Saxe", "Andrew M", ""]]}, {"id": "1806.00797", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva and Juan-Pablo Ortega", "title": "Echo state networks are universal", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that echo state networks are universal uniform approximants\nin the context of discrete-time fading memory filters with uniformly bounded\ninputs defined on negative infinite times. This result guarantees that any\nfading memory input/output system in discrete time can be realized as a simple\nfinite-dimensional neural network-type state-space model with a static linear\nreadout map. This approximation is valid for infinite time intervals. The proof\nof this statement is based on fundamental results, also presented in this work,\nabout the topological nature of the fading memory property and about reservoir\ncomputing systems generated by continuous reservoir maps.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 13:38:41 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 11:17:29 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1806.00851", "submitter": "Zhe Li", "authors": "Zhe Li, Xuehan Xiong, Zhou Ren, Ning Zhang, Xiaoyu Wang, Tianbao Yang", "title": "An Aggressive Genetic Programming Approach for Searching Neural Network\n  Structure Under Computational Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there emerged revived interests of designing automatic programs\n(e.g., using genetic/evolutionary algorithms) to optimize the structure of\nConvolutional Neural Networks (CNNs) for a specific task. The challenge in\ndesigning such programs lies in how to balance between large search space of\nthe network structures and high computational costs. Existing works either\nimpose strong restrictions on the search space or use enormous computing\nresources. In this paper, we study how to design a genetic programming approach\nfor optimizing the structure of a CNN for a given task under limited\ncomputational resources yet without imposing strong restrictions on the search\nspace. To reduce the computational costs, we propose two general strategies\nthat are observed to be helpful: (i) aggressively selecting strongest\nindividuals for survival and reproduction, and killing weaker individuals at a\nvery early age; (ii) increasing mutation frequency to encourage diversity and\nfaster evolution. The combined strategy with additional optimization techniques\nallows us to explore a large search space but with affordable computational\ncosts. Our results on standard benchmark datasets (MNIST, SVHN, CIFAR-10,\nCIFAR-100) are competitive to similar approaches with significantly reduced\ncomputational costs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 19:16:31 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Li", "Zhe", ""], ["Xiong", "Xuehan", ""], ["Ren", "Zhou", ""], ["Zhang", "Ning", ""], ["Wang", "Xiaoyu", ""], ["Yang", "Tianbao", ""]]}, {"id": "1806.01016", "submitter": "Naima Chouikhi", "authors": "Naima Chouikhi, Boudour Ammar, Adel M. Alimi", "title": "Hierarchical Bi-level Multi-Objective Evolution of Single- and\n  Multi-layer Echo State Network Autoencoders for Data Representations", "comments": "40 pages, 15 figures, Information Sciences Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo State Network (ESN) presents a distinguished kind of recurrent neural\nnetworks. It is built upon a sparse, random and large hidden infrastructure\ncalled reservoir. ESNs have succeeded in dealing with several non-linear\nproblems such as prediction, classification, etc. Thanks to its rich dynamics,\nESN is used as an Autoencoder (AE) to extract features from original data\nrepresentations. ESN is not only used with its basic single layer form but also\nwith the recently proposed Multi-Layer (ML) architecture. The well setting of\nESN (basic and ML) architectures and training parameters is a crucial and hard\nlabor task. Generally, a number of parameters (hidden neurons, sparsity rates,\ninput scaling) is manually altered to achieve minimum learning error. However,\nthis randomly hand crafted task, on one hand, may not guarantee best training\nresults and on the other hand, it can raise the network's complexity. In this\npaper, a hierarchical bi-level evolutionary optimization is proposed to deal\nwith these issues. The first level includes a multi-objective architecture\noptimization providing maximum learning accuracy while sustaining the\ncomplexity at a reduced standard. Multi-objective Particle Swarm Optimization\n(MOPSO) is used to optimize ESN structure in a way to provide a trade-off\nbetween the network complexity decreasing and the accuracy increasing. A\npareto-front of optimal solutions is generated by the end of the MOPSO process.\nThese solutions present the set of candidates that succeeded in providing a\ncompromise between different objectives (learning error and network\ncomplexity). At the second level, each of the solutions already found undergo a\nmono-objective weights optimization to enhance the obtained pareto-front.\nEmpirical results ensure the effectiveness of the evolved ESN recurrent AEs\n(basic and ML) for noisy and noise free data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:59:23 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 20:22:44 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chouikhi", "Naima", ""], ["Ammar", "Boudour", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.01107", "submitter": "Amir Yazdanbakhsh", "authors": "Amir Yazdanbakhsh, Hajar Falahati, Philip J. Wolfe, Kambiz Samadi, Nam\n  Sung Kim, and Hadi Esmaeilzadeh", "title": "GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial\n  Networks", "comments": "Proceedings of the 45th International Symposium on Computer\n  Architecture (ISCA), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are one of the most recent deep\nlearning models that generate synthetic data from limited genuine datasets.\nGANs are on the frontier as further extension of deep learning into many\ndomains (e.g., medicine, robotics, content synthesis) requires massive sets of\nlabeled data that is generally either unavailable or prohibitively costly to\ncollect. Although GANs are gaining prominence in various fields, there are no\naccelerators for these new models. In fact, GANs leverage a new operator,\ncalled transposed convolution, that exposes unique challenges for hardware\nacceleration. This operator first inserts zeros within the multidimensional\ninput, then convolves a kernel over this expanded array to add information to\nthe embedded zeros. Even though there is a convolution stage in this operator,\nthe inserted zeros lead to underutilization of the compute resources when a\nconventional convolution accelerator is employed. We propose the GANAX\narchitecture to alleviate the sources of inefficiency associated with the\nacceleration of GANs using conventional convolution accelerators, making the\nfirst GAN accelerator design possible. We propose a reorganization of the\noutput computations to allocate compute rows with similar patterns of zeros to\nadjacent processing engines, which also avoids inconsequential multiply-adds on\nthe zeros. This compulsory adjacency reclaims data reuse across these\nneighboring processing engines, which had otherwise diminished due to the\ninserted zeros. The reordering breaks the full SIMD execution model, which is\nprominent in convolution accelerators. Therefore, we propose a unified\nMIMD-SIMD design for GANAX that leverages repeated patterns in the computation\nto create distinct microprograms that execute concurrently in SIMD mode.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 22:54:29 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yazdanbakhsh", "Amir", ""], ["Falahati", "Hajar", ""], ["Wolfe", "Philip J.", ""], ["Samadi", "Kambiz", ""], ["Kim", "Nam Sung", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "1806.01128", "submitter": "Clemens Frahnow", "authors": "Clemens Frahnow and Timo K\\\"otzing", "title": "Ring Migration Topology Helps Bypassing Local Optima", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running several evolutionary algorithms in parallel and occasionally\nexchanging good solutions is referred to as island models. The idea is that the\nindependence of the different islands leads to diversity, thus possibly\nexploring the search space better. Many theoretical analyses so far have found\na complete (or sufficiently quickly expanding) topology as underlying migration\ngraph most efficient for optimization, even though a quick dissemination of\nindividuals leads to a loss of diversity. We suggest a simple fitness function\nFORK with two local optima parametrized by $r \\geq 2$ and a scheme for\ncomposite fitness functions. We show that, while the (1+1) EA gets stuck in a\nbad local optimum and incurs a run time of $\\Theta(n^{2r})$ fitness evaluations\non FORK, island models with a complete topology can achieve a run time of\n$\\Theta(n^{1.5r})$ by making use of rare migrations in order to explore the\nsearch space more effectively. Finally, the ring topology, making use of rare\nmigrations and a large diameter, can achieve a run time of\n$\\tilde{\\Theta}(n^r)$, the black box complexity of FORK. This shows that the\nring topology can be preferable over the complete topology in order to maintain\ndiversity.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 14:03:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Frahnow", "Clemens", ""], ["K\u00f6tzing", "Timo", ""]]}, {"id": "1806.01224", "submitter": "Nils M\\\"uller", "authors": "Nils M\\\"uller and Tobias Glasmachers", "title": "Challenges in High-dimensional Reinforcement Learning with Evolution\n  Strategies", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution Strategies (ESs) have recently become popular for training deep\nneural networks, in particular on reinforcement learning tasks, a special form\nof controller design. Compared to classic problems in continuous direct search,\ndeep networks pose extremely high-dimensional optimization problems, with many\nthousands or even millions of variables. In addition, many control problems\ngive rise to a stochastic fitness function. Considering the relevance of the\napplication, we study the suitability of evolution strategies for\nhigh-dimensional, stochastic problems. Our results give insights into which\nalgorithmic mechanisms of modern ES are of value for the class of problems at\nhand, and they reveal principled limitations of the approach. They are in line\nwith our theoretical understanding of ESs. We show that combining ESs that\noffer reduced internal algorithm cost with uncertainty handling techniques\nyields promising methods for this class of problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:08:23 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 18:30:36 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["M\u00fcller", "Nils", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "1806.01322", "submitter": "Tim Taylor", "authors": "Tim Taylor and Alan Dorin", "title": "Past Visions of Artificial Futures: One Hundred and Fifty Years under\n  the Spectre of Evolving Machines", "comments": "To appear in Proceedings of the Artificial Life Conference 2018\n  (ALIFE 2018), MIT Press", "journal-ref": "Proceedings of the Artificial Life Conference 2018, T. Ikegami et\n  al. (eds.), MIT Press (pp.91-98)", "doi": "10.1162/isal_a_00022", "report-no": null, "categories": "cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence of Artificial Intelligence (AI) and Artificial Life (ALife)\ntechnologies upon society, and their potential to fundamentally shape the\nfuture evolution of humankind, are topics very much at the forefront of current\nscientific, governmental and public debate. While these might seem like very\nmodern concerns, they have a long history that is often disregarded in\ncontemporary discourse. Insofar as current debates do acknowledge the history\nof these ideas, they rarely look back further than the origin of the modern\ndigital computer age in the 1940s-50s. In this paper we explore the earlier\nhistory of these concepts. We focus in particular on the idea of\nself-reproducing and evolving machines, and potential implications for our own\nspecies. We show that discussion of these topics arose in the 1860s, within a\ndecade of the publication of Darwin's The Origin of Species, and attracted\nincreasing interest from scientists, novelists and the general public in the\nearly 1900s. After introducing the relevant work from this period, we\ncategorise the various visions presented by these authors of the future\nimplications of evolving machines for humanity. We suggest that current debates\non the co-evolution of society and technology can be enriched by a proper\nappreciation of the long history of the ideas involved.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:52:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Taylor", "Tim", ""], ["Dorin", "Alan", ""]]}, {"id": "1806.01331", "submitter": "Denis Antipov", "authors": "Denis Antipov and Benjamin Doerr", "title": "Precise Runtime Analysis for Plateaus", "comments": "42 pages including appendix, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To gain a better theoretical understanding of how evolutionary algorithms\ncope with plateaus of constant fitness, we analyze how the $(1 + 1)$ EA\noptimizes the $n$-dimensional $Plateau_k$ function. This function has a plateau\nof second-best fitness in a radius of $k$ around the optimum. As optimization\nalgorithm, we regard the $(1 + 1)$ EA using an arbitrary unbiased mutation\noperator. Denoting by $\\alpha$ the random number of bits flipped in an\napplication of this operator and assuming $\\Pr[\\alpha = 1] = \\Omega(1)$, we\nshow the surprising result that for $k \\ge 2$ the expected optimization time of\nthis algorithm is \\[\\frac{n^k}{k!\\Pr[1 \\le \\alpha \\le k]}(1 + o(1)),\\] that is,\nthe size of the plateau times the expected waiting time for an iteration\nflipping between $1$ and $k$ bits. Our result implies that the optimal mutation\nrate for this function is approximately $k/en$. Our main analysis tool is a\ncombined analysis of the Markov chains on the search point space and on the\nHamming level space, an approach that promises to be useful also for other\nplateau problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:04:58 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:25:52 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2019 18:42:38 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Antipov", "Denis", ""], ["Doerr", "Benjamin", ""]]}, {"id": "1806.01363", "submitter": "Giuseppe Cuccu", "authors": "Giuseppe Cuccu, Julian Togelius, Philippe Cudre-Mauroux", "title": "Playing Atari with Six Neurons", "comments": "Accepted at AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning, applied to vision-based problems like Atari\ngames, maps pixels directly to actions; internally, the deep neural network\nbears the responsibility of both extracting useful information and making\ndecisions based on it. By separating the image processing from decision-making,\none could better understand the complexity of each task, as well as potentially\nfind smaller policy representations that are easier for humans to understand\nand may generalize better. To this end, we propose a new method for learning\npolicies and compact state representations separately but simultaneously for\npolicy approximation in reinforcement learning. State representations are\ngenerated by an encoder based on two novel algorithms: Increasing Dictionary\nVector Quantization makes the encoder capable of growing its dictionary size\nover time, to address new observations as they appear in an open-ended\nonline-learning context; Direct Residuals Sparse Coding encodes observations by\ndisregarding reconstruction error minimization, and aiming instead for highest\ninformation inclusion. The encoder autonomously selects observations online to\ntrain on, in order to maximize code sparsity. As the dictionary size increases,\nthe encoder produces increasingly larger inputs for the neural network: this is\naddressed by a variation of the Exponential Natural Evolution Strategies\nalgorithm which adapts its probability distribution dimensionality along the\nrun. We test our system on a selection of Atari games using tiny neural\nnetworks of only 6 to 18 neurons (depending on the game's controls). These are\nstill capable of achieving results comparable---and occasionally superior---to\nstate-of-the-art techniques which use two orders of magnitude more neurons.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:09:43 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 11:42:42 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cuccu", "Giuseppe", ""], ["Togelius", "Julian", ""], ["Cudre-Mauroux", "Philippe", ""]]}, {"id": "1806.01396", "submitter": "Saptarshi Sengupta", "authors": "Saptarshi Sengupta, Richard Alan Peters II", "title": "Learning to track on-the-fly using a particle filter with annealed-\n  weighted QPSO modeled after a singular Dirac delta potential", "comments": "16 pages, 13 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an evolutionary Particle Filter with a memory guided\nproposal step size update and an improved, fully-connected Quantum-behaved\nParticle Swarm Optimization (QPSO) resampling scheme for visual tracking\napplications. The proposal update step uses importance weights proportional to\nvelocities encountered in recent memory to limit the swarm movement within\nprobable regions of interest. The QPSO resampling scheme uses a fitness\nweighted mean best update to bias the swarm towards the fittest section of\nparticles while also employing a simulated annealing operator to avoid subpar\nfine tune during latter course of iterations. By moving particles closer to\nhigh likelihood landscapes of the posterior distribution using such constructs,\nthe sample impoverishment problem that plagues the Particle Filter is mitigated\nto a great extent. Experimental results using benchmark sequences imply that\nthe proposed method outperforms competitive candidate trackers such as the\nParticle Filter and the traditional Particle Swarm Optimization based Particle\nFilter on a suite of tracker performance indices.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 21:22:48 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Sengupta", "Saptarshi", ""], ["Peters", "Richard Alan", "II"]]}, {"id": "1806.01423", "submitter": "Hananel Hazan", "authors": "Hananel Hazan and Daniel J. Saunders and Hassaan Khan and Darpan T.\n  Sanghavi and Hava T. Siegelmann and Robert Kozma", "title": "BindsNET: A machine learning-oriented spiking neural networks library in\n  Python", "comments": null, "journal-ref": "Frontiers in Neuroinformatics. 12 December 2018", "doi": "10.3389/fninf.2018.00089", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of spiking neural network simulation software is a critical\ncomponent enabling the modeling of neural systems and the development of\nbiologically inspired algorithms. Existing software frameworks support a wide\nrange of neural functionality, software abstraction levels, and hardware\ndevices, yet are typically not suitable for rapid prototyping or application to\nproblems in the domain of machine learning. In this paper, we describe a new\nPython package for the simulation of spiking neural networks, specifically\ngeared towards machine learning and reinforcement learning. Our software,\ncalled BindsNET, enables rapid building and simulation of spiking networks and\nfeatures user-friendly, concise syntax. BindsNET is built on top of the PyTorch\ndeep neural networks library, enabling fast CPU and GPU computation for large\nspiking networks. The BindsNET framework can be adjusted to meet the needs of\nother existing computing and hardware environments, e.g., TensorFlow. We also\nprovide an interface into the OpenAI gym library, allowing for training and\nevaluation of spiking networks on reinforcement learning problems. We argue\nthat this package facilitates the use of spiking networks for large-scale\nmachine learning experimentation, and show some simple examples of how we\nenvision BindsNET can be used in practice. BindsNET code is available at\nhttps://github.com/Hananel-Hazan/bindsnet\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:09:52 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:27:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hazan", "Hananel", ""], ["Saunders", "Daniel J.", ""], ["Khan", "Hassaan", ""], ["Sanghavi", "Darpan T.", ""], ["Siegelmann", "Hava T.", ""], ["Kozma", "Robert", ""]]}, {"id": "1806.01610", "submitter": "Robin Tibor Schirrmeister", "authors": "Robin Tibor Schirrmeister, Patryk Chrab\\k{a}szcz, Frank Hutter, Tonio\n  Ball", "title": "Training Generative Reversible Networks", "comments": "Source code for this study is at\n  https://github.com/robintibor/generative-reversible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models with an encoding component such as autoencoders currently\nreceive great interest. However, training of autoencoders is typically\ncomplicated by the need to train a separate encoder and decoder model that have\nto be enforced to be reciprocal to each other. To overcome this problem,\nby-design reversible neural networks (RevNets) had been previously used as\ngenerative models either directly optimizing the likelihood of the data under\nthe model or using an adversarial approach on the generated data. Here, we\ninstead investigate their performance using an adversary on the latent space in\nthe adversarial autoencoder framework. We investigate the generative\nperformance of RevNets on the CelebA dataset, showing that generative RevNets\ncan generate coherent faces with similar quality as Variational Autoencoders.\nThis first attempt to use RevNets inside the adversarial autoencoder framework\nslightly underperformed relative to recent advanced generative models using an\nautoencoder component on CelebA, but this gap may diminish with further\noptimization of the training setup of generative RevNets. In addition to the\nexperiments on CelebA, we show a proof-of-principle experiment on the MNIST\ndataset suggesting that adversary-free trained RevNets can discover meaningful\nlatent dimensions without pre-specifying the number of dimensions of the latent\nsampling distribution. In summary, this study shows that RevNets can be\nemployed in different generative training settings.\n  Source code for this study is at\nhttps://github.com/robintibor/generative-reversible\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 11:16:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 08:40:34 GMT"}, {"version": "v3", "created": "Sun, 8 Jul 2018 23:22:27 GMT"}, {"version": "v4", "created": "Thu, 23 Aug 2018 12:07:40 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Schirrmeister", "Robin Tibor", ""], ["Chrab\u0105szcz", "Patryk", ""], ["Hutter", "Frank", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.01631", "submitter": "Xin-She Yang", "authors": "Xin-She Yang and Suash Deb", "title": "Cuckoo Search: State-of-the-Art and Opportunities", "comments": "4th IEEE Conference on Soft Computing and Machine Learning\n  (ISCMI2017), Mauritius, 23-24 Nov 2017", "journal-ref": null, "doi": "10.1109/ISCMI.2017.8279597", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the development of cuckoo search (CS) by Yang and Deb in 2009, CS has\nbeen applied in a diverse range of applications. This paper first outlines the\nkey features of the algorithm and its variants, and then briefly summarizes the\nstate-of-the-art developments in many applications. The opportunities for\nfurther research are also identified.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 10:27:54 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Yang", "Xin-She", ""], ["Deb", "Suash", ""]]}, {"id": "1806.01632", "submitter": "Xin-She Yang", "authors": "Xin-She Yang and Xingshi He", "title": "Why the Firefly Algorithm Works?", "comments": "Book chapter, 2018", "journal-ref": null, "doi": "10.1007/978-3-319-67669-2_11", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firefly algorithm is a nature-inspired optimization algorithm and there have\nbeen significant developments since its appearance about ten years ago. This\nchapter summarizes the latest developments about the firefly algorithm and its\nvariants as well as their diverse applications. Future research directions are\nalso highlighted.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 13:32:44 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Yang", "Xin-She", ""], ["He", "Xingshi", ""]]}, {"id": "1806.01677", "submitter": "Stepan Tulyakov", "authors": "Stepan Tulyakov and Anton Ivanov and Francois Fleuret", "title": "Practical Deep Stereo (PDS): Toward applications-friendly deep stereo\n  matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep-learning networks recently demonstrated extremely good\nperfor- mance for stereo matching. However, existing networks are difficult to\nuse for practical applications since (1) they are memory-hungry and unable to\nprocess even modest-size images, (2) they have to be trained for a given\ndisparity range. The Practical Deep Stereo (PDS) network that we propose\naddresses both issues: First, its architecture relies on novel bottleneck\nmodules that drastically reduce the memory footprint in inference, and\nadditional design choices allow to handle greater image size during training.\nThis results in a model that leverages large image context to resolve matching\nambiguities. Second, a novel sub-pixel cross- entropy loss combined with a MAP\nestimator make this network less sensitive to ambiguous matches, and applicable\nto any disparity range without re-training. We compare PDS to state-of-the-art\nmethods published over the recent months, and demonstrate its superior\nperformance on FlyingThings3D and KITTI sets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 13:24:40 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tulyakov", "Stepan", ""], ["Ivanov", "Anton", ""], ["Fleuret", "Francois", ""]]}, {"id": "1806.01681", "submitter": "Anand Kulkarni Dr", "authors": "Apoorva S Shastri and Anand J Kulkarni", "title": "Multi-Cohort Intelligence Algorithm: An Intra- and Inter-group Learning\n  Behavior based Socio-inspired Optimization Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multi-Cohort Intelligence (Multi-CI) metaheuristic algorithm in emerging\nsocio-inspired optimization domain is proposed. The algorithm implements\nintra-group and inter-group learning mechanisms. It focusses on the interaction\namongst different cohorts. The performance of the algorithm is validated by\nsolving 75 unconstrained test problems with dimensions up to 30. The solutions\nwere comparing with several recent algorithms such as Particle Swarm\nOptimization, Covariance Matrix Adaptation Evolution Strategy, Artificial Bee\nColony, Self-adaptive differential evolution algorithm, Comprehensive Learning\nParticle Swarm Optimization, Backtracking Search Optimization Algorithm and\nIdeology Algorithm. The Wilcoxon signed rank test was carried out for the\nstatistical analysis and verification of the performance. The proposed Multi-CI\noutperformed these algorithms in terms of the solution quality including\nobjective function value and computational cost, i.e. computational time and\nfunctional evaluations. The prominent feature of the Multi-CI algorithm along\nwith the limitations are discussed as well. In addition, an illustrative\nexample is also solved and every detail is provided.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 13:09:13 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Shastri", "Apoorva S", ""], ["Kulkarni", "Anand J", ""]]}, {"id": "1806.01683", "submitter": "Kamel Abdelouahab", "authors": "Kamel Abdelouahab and Maxime Pelcat and Jocelyn Serot and Fran\\c{c}ois\n  Berry", "title": "Accelerating CNN inference on FPGAs: A Survey", "comments": "Cloning our HAL submission in ArXiv, Technical Report - Universite\n  Clermont Auvergne, January 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are currently adopted to solve an ever\ngreater number of problems, ranging from speech recognition to image\nclassification and segmentation. The large amount of processing required by\nCNNs calls for dedicated and tailored hardware support methods. Moreover, CNN\nworkloads have a streaming nature, well suited to reconfigurable hardware\narchitectures such as FPGAs. The amount and diversity of research on the\nsubject of CNN FPGA acceleration within the last 3 years demonstrates the\ntremendous industrial and academic interest. This paper presents a\nstate-of-the-art of CNN inference accelerators over FPGAs. The computational\nworkloads, their parallelism and the involved memory accesses are analyzed. At\nthe level of neurons, optimizations of the convolutional and fully connected\nlayers are explained and the performances of the different methods compared. At\nthe network level, approximate computing and datapath optimization methods are\ncovered and state-of-the-art approaches compared. The methods and tools\ninvestigated in this survey represent the recent trends in FPGA CNN inference\naccelerators and will fuel the future advances on efficient hardware deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 12:24:49 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Abdelouahab", "Kamel", ""], ["Pelcat", "Maxime", ""], ["Serot", "Jocelyn", ""], ["Berry", "Fran\u00e7ois", ""]]}, {"id": "1806.01710", "submitter": "Phan Trung Hai Nguyen", "authors": "Per Kristian Lehre, Phan Trung Hai Nguyen", "title": "Level-Based Analysis of the Population-Based Incremental Learning\n  Algorithm", "comments": "To appear", "journal-ref": "Proceedings of the 15th International Conference on Parallel\n  Problem Solving from Nature 2018 (PPSN XV)", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Population-Based Incremental Learning (PBIL) algorithm uses a convex\ncombination of the current model and the empirical model to construct the next\nmodel, which is then sampled to generate offspring. The Univariate Marginal\nDistribution Algorithm (UMDA) is a special case of the PBIL, where the current\nmodel is ignored. Dang and Lehre (GECCO 2015) showed that UMDA can optimise\nLeadingOnes efficiently. The question still remained open if the PBIL performs\nequally well. Here, by applying the level-based theorem in addition to\nDvoretzky--Kiefer--Wolfowitz inequality, we show that the PBIL optimises\nfunction LeadingOnes in expected time $\\mathcal{O}(n\\lambda \\log \\lambda +\nn^2)$ for a population size $\\lambda = \\Omega(\\log n)$, which matches the bound\nof the UMDA. Finally, we show that the result carries over to BinVal, giving\nthe fist runtime result for the PBIL on the BinVal problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 14:21:50 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Lehre", "Per Kristian", ""], ["Nguyen", "Phan Trung Hai", ""]]}, {"id": "1806.01775", "submitter": "Fuqiang Liu", "authors": "F. Liu, C. Liu and F.Bi", "title": "A Memristor based Unsupervised Neuromorphic System Towards Fast and\n  Energy-Efficient GAN", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has gained immense success in pushing today's artificial\nintelligence forward. To solve the challenge of limited labeled data in the\nsupervised learning world, unsupervised learning has been proposed years ago\nwhile low accuracy hinters its realistic applications. Generative adversarial\nnetwork (GAN) emerges as an unsupervised learning approach with promising\naccuracy and are under extensively study. However, the execution of GAN is\nextremely memory and computation intensive and results in ultra-low speed and\nhigh-power consumption. In this work, we proposed a holistic solution for fast\nand energy-efficient GAN computation through a memristor-based neuromorphic\nsystem. First, we exploited a hardware and software co-design approach to map\nthe computation blocks in GAN efficiently. We also proposed an efficient data\nflow for optimal parallelism training and testing, depending on the computation\ncorrelations between different computing blocks. To compute the unique and\ncomplex loss of GAN, we developed a diff-block with optimized accuracy and\nperformance. The experiment results on big data show that our design achieves\n2.8x speedup and 6.1x energy-saving compared with the traditional GPU\naccelerator, as well as 5.5x speedup and 1.4x energy-saving compared with the\nprevious FPGA-based accelerator.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 02:45:38 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 06:00:55 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 02:32:16 GMT"}, {"version": "v4", "created": "Sun, 8 Sep 2019 08:46:12 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "F.", ""], ["Liu", "C.", ""], ["Bi", "F.", ""]]}, {"id": "1806.01782", "submitter": "Ibraheem Kasim Ibraheem AL-Timeemee", "authors": "Ibraheem Kasim Ibraheem", "title": "Adaptive System Identification Using LMS Algorithm Integrated with\n  Evolutionary Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System identification is an exceptionally expansive topic and of remarkable\nsignificance in the discipline of signal processing and communication. Our goal\nin this paper is to show how simple adaptive FIR and IIR filters can be used in\nsystem modeling and demonstrating the application of adaptive system\nidentification. The main objective of our research is to study the LMS\nalgorithm and its improvement by the genetic search approach, namely, LMS-GA,\nto search the multi-modal error surface of the IIR filter to avoid local minima\nand finding the optimal weight vector when only measured or estimated data are\navailable. Convergence analysis of the LMS algorithm in the case of coloured\ninput signal, i.e., correlated input signal is demonstrated on adaptive FIR\nfilter via power spectral density of the input signals and Fourier transform of\nthe autocorrelation matrix of the input signal. Simulations have been carried\nout on adaptive filtering of FIR and IIR filters and tested on white and\ncoloured input signals to validate the powerfulness of the genetic-based LMS\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 15:30:10 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 22:54:37 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Ibraheem", "Ibraheem Kasim", ""]]}, {"id": "1806.01883", "submitter": "Tim Taylor", "authors": "Tim Taylor", "title": "Evolutionary Innovations and Where to Find Them: Routes to Open-Ended\n  Evolution in Natural and Artificial Systems", "comments": "This version (v4) to appear in the journal Artificial Life 25(2),\n  2019. Previous version (v3) was presented at the Third Workshop on Open-Ended\n  Evolution (OEE3), Tokyo, Japan, July 2018 (hosted by the 2018 Conference on\n  Artificial Life)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a high-level conceptual framework to help orient the\ndiscussion and implementation of open-endedness in evolutionary systems.\nDrawing upon earlier work by Banzhaf et al., three different kinds of\nopen-endedness are identified: exploratory, expansive, and transformational.\nThese are characterised in terms of their relationship to the search space of\nphenotypic behaviours. A formalism is introduced to describe three key\nprocesses required for an evolutionary process: the generation of a phenotype\nfrom a genetic description, the evaluation of that phenotype, and the\nreproduction with variation of individuals according to their evaluation. The\ndistinction is made between intrinsic and extrinsic implementations of these\nprocesses. A discussion then investigates how various interactions between\nthese processes, and their modes of implementation, can lead to open-endedness.\nHowever, an important contribution of the paper is the demonstration that these\nconsiderations relate to exploratory open-endedness only. Conditions for the\nimplementation of the more interesting kinds of open-endedness - expansive and\ntransformational - are also discussed, emphasizing factors such as multiple\ndomains of behaviour, transdomain bridges, and non-additive compositional\nsystems. These factors relate not to the generic evolutionary properties of\nindividuals and populations, but rather to the nature of the building blocks\nout of which individual organisms are constructed, and the laws and properties\nof the environment in which they exist. The paper ends with suggestions of how\nthe framework can be used to categorise and compare the open-ended evolutionary\npotential of different systems, how it might guide the design of systems with\ngreater capacity for open-ended evolution, and how it might be further\nimproved.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:30:22 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 17:18:41 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 18:30:53 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2019 09:18:52 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Taylor", "Tim", ""]]}, {"id": "1806.01940", "submitter": "Jian Ren", "authors": "Jian Ren, Zhe Li, Jianchao Yang, Ning Xu, Tianbao Yang, David J. Foran", "title": "EIGEN: Ecologically-Inspired GENetic Approach for Neural Network\n  Structure Searching from Scratch", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing the structure of neural networks is considered one of the most\nchallenging tasks in deep learning, especially when there is few prior\nknowledge about the task domain. In this paper, we propose an\nEcologically-Inspired GENetic (EIGEN) approach that uses the concept of\nsuccession, extinction, mimicry, and gene duplication to search neural network\nstructure from scratch with poorly initialized simple network and few\nconstraints forced during the evolution, as we assume no prior knowledge about\nthe task domain. Specifically, we first use primary succession to rapidly\nevolve a population of poorly initialized neural network structures into a more\ndiverse population, followed by a secondary succession stage for fine-grained\nsearching based on the networks from the primary succession. Extinction is\napplied in both stages to reduce computational cost. Mimicry is employed during\nthe entire evolution process to help the inferior networks imitate the behavior\nof a superior network and gene duplication is utilized to duplicate the learned\nblocks of novel structures, both of which help to find better network\nstructures. Experimental results show that our proposed approach can achieve\nsimilar or better performance compared to the existing genetic approaches with\ndramatically reduced computation cost. For example, the network discovered by\nour approach on CIFAR-100 dataset achieves 78.1% test accuracy under 120 GPU\nhours, compared to 77.0% test accuracy in more than 65, 536 GPU hours in [35].\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 21:38:19 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 23:27:11 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 19:08:11 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ren", "Jian", ""], ["Li", "Zhe", ""], ["Yang", "Jianchao", ""], ["Xu", "Ning", ""], ["Yang", "Tianbao", ""], ["Foran", "David J.", ""]]}, {"id": "1806.02003", "submitter": "Abhejit Rajagopal", "authors": "Abhejit Rajagopal, Shivkumar Chandrasekaran, Hrushikesh N. Mhaskar", "title": "Deep Algorithms: designs for networks", "comments": "submitted to Thirty-second Annual Conference on Neural Information\n  Processing Systems (NIPS), May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new design methodology for neural networks that is guided by traditional\nalgorithm design is presented. To prove our point, we present two heuristics\nand demonstrate an algorithmic technique for incorporating additional weights\nin their signal-flow graphs. We show that with training the performance of\nthese networks can not only exceed the performance of the initial network, but\ncan match the performance of more-traditional neural network architectures. A\nkey feature of our approach is that these networks are initialized with\nparameters that provide a known performance threshold for the architecture on a\ngiven task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 04:39:37 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Rajagopal", "Abhejit", ""], ["Chandrasekaran", "Shivkumar", ""], ["Mhaskar", "Hrushikesh N.", ""]]}, {"id": "1806.02112", "submitter": "J. A. Gregor Lagodzinski", "authors": "Benjamin Doerr, Timo K\\\"otzing, J. A. Gregor Lagodzinski and Johannes\n  Lengler", "title": "Bounding Bloat in Genetic Programming", "comments": "An extended abstract has been published at GECCO 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many optimization problems work with a fixed number of decision\nvariables and thus a fixed-length representation of possible solutions, genetic\nprogramming (GP) works on variable-length representations. A naturally\noccurring problem is that of bloat (unnecessary growth of solutions) slowing\ndown optimization. Theoretical analyses could so far not bound bloat and\nrequired explicit assumptions on the magnitude of bloat. In this paper we\nanalyze bloat in mutation-based genetic programming for the two test functions\nORDER and MAJORITY. We overcome previous assumptions on the magnitude of bloat\nand give matching or close-to-matching upper and lower bounds for the expected\noptimization time. In particular, we show that the (1+1) GP takes (i)\n$\\Theta(T_{init} + n \\log n)$ iterations with bloat control on ORDER as well as\nMAJORITY; and (ii) $O(T_{init} \\log T_{init} + n (\\log n)^3)$ and\n$\\Omega(T_{init} + n \\log n)$ (and $\\Omega(T_{init} \\log T_{init})$ for $n=1$)\niterations without bloat control on MAJORITY.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 10:51:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Doerr", "Benjamin", ""], ["K\u00f6tzing", "Timo", ""], ["Lagodzinski", "J. A. Gregor", ""], ["Lengler", "Johannes", ""]]}, {"id": "1806.02448", "submitter": "Philip Bontrager", "authors": "Ruben Rodriguez Torrado, Philip Bontrager, Julian Togelius, Jialin\n  Liu, Diego Perez-Liebana", "title": "Deep Reinforcement Learning for General Video Game AI", "comments": "8 pages, 4 figures, Accepted at the conference on Computational\n  Intelligence and Games 2018 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Video Game AI (GVGAI) competition and its associated software\nframework provides a way of benchmarking AI algorithms on a large number of\ngames written in a domain-specific description language. While the competition\nhas seen plenty of interest, it has so far focused on online planning,\nproviding a forward model that allows the use of algorithms such as Monte Carlo\nTree Search.\n  In this paper, we describe how we interface GVGAI to the OpenAI Gym\nenvironment, a widely used way of connecting agents to reinforcement learning\nproblems. Using this interface, we characterize how widely used implementations\nof several deep reinforcement learning algorithms fare on a number of GVGAI\ngames. We further analyze the results to provide a first indication of the\nrelative difficulty of these games relative to each other, and relative to\nthose in the Arcade Learning Environment under similar conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 22:39:26 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Torrado", "Ruben Rodriguez", ""], ["Bontrager", "Philip", ""], ["Togelius", "Julian", ""], ["Liu", "Jialin", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "1806.02502", "submitter": "Hossein Izadi Rad", "authors": "Hossein Izadi Rad, Ji Feng, and Hitoshi Iba", "title": "GP-RVM: Genetic Programing-based Symbolic Regression Using Relevance\n  Vector Machine", "comments": "Accepted in IEEE SMC 2018. To be presented on 8-10 Oct. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hybrid basis function construction method (GP-RVM) for\nSymbolic Regression problem, which combines an extended version of Genetic\nProgramming called Kaizen Programming and Relevance Vector Machine to evolve an\noptimal set of basis functions. Different from traditional evolutionary\nalgorithms where a single individual is a complete solution, our method\nproposes a solution based on linear combination of basis functions built from\nindividuals during the evolving process. RVM which is a sparse Bayesian kernel\nmethod selects suitable functions to constitute the basis. RVM determines the\nposterior weight of a function by evaluating its quality and sparsity. The\nsolution produced by GP-RVM is a sparse Bayesian linear model of the\ncoefficients of many non-linear functions. Our hybrid approach is focused on\nnonlinear white-box models selecting the right combination of functions to\nbuild robust predictions without prior knowledge about data. Experimental\nresults show that GP-RVM outperforms conventional methods, which suggest that\nit is an efficient and accurate technique for solving SR. The computational\ncomplexity of GP-RVM scales in $O( M^{3})$, where $M$ is the number of\nfunctions in the basis set and is typically much smaller than the number $N$ of\ntraining patterns.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 03:51:46 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 03:48:33 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 00:08:33 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Rad", "Hossein Izadi", ""], ["Feng", "Ji", ""], ["Iba", "Hitoshi", ""]]}, {"id": "1806.02583", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX, DTIS, ONERA, Universit\\'e Paris Saclay),\n  Bertrand Le Saux (DTIS, ONERA, Universit\\'e Paris Saclay), S\\'ebastien\n  Lef\\`evre (OBELIX)", "title": "Generative Adversarial Networks for Realistic Synthesis of Hyperspectral\n  Samples", "comments": null, "journal-ref": "International Geoscience and Remote Sensing Symposium (IGARSS\n  2018), Jul 2018, Valencia, Spain", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the scarcity of annotated hyperspectral data required to\ntrain deep neural networks. Especially, we investigate generative adversarial\nnetworks and their application to the synthesis of consistent labeled spectra.\nBy training such networks on public datasets, we show that these models are not\nonly able to capture the underlying distribution, but also to generate\ngenuine-looking and physically plausible spectra. Moreover, we experimentally\nvalidate that the synthetic samples can be used as an effective data\naugmentation strategy. We validate our approach on several public\nhyper-spectral datasets using a variety of deep classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 09:36:12 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, DTIS, ONERA, Universit\u00e9 Paris Saclay"], ["Saux", "Bertrand Le", "", "DTIS, ONERA, Universit\u00e9 Paris Saclay"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1806.02654", "submitter": "Laura-Mar\\'ia Cornejo-Bueno", "authors": "L. Cornejo-Bueno", "title": "New Hybrid Neuro-Evolutionary Algorithms for Renewable Energy and\n  Facilities Management Problems", "comments": "arXiv admin note: text overlap with arXiv:1706.03673,\n  arXiv:1805.03463 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This Ph.D. thesis deals with the optimization of several renewable energy\nresources development as well as the improvement of facilities management in\noceanic engineering and airports, using computational hybrid methods belonging\nto AI to this end. Energy is essential to our society in order to ensure a good\nquality of life. This means that predictions over the characteristics on which\nrenewable energies depend are necessary, in order to know the amount of energy\nthat will be obtained at any time. The second topic tackled in this thesis is\nrelated to the basic parameters that influence in different marine activities\nand airports, whose knowledge is necessary to develop a proper facilities\nmanagement in these environments. Within this work, a study of the\nstate-of-the-art Machine Learning have been performed to solve the problems\nassociated with the topics above-mentioned, and several contributions have been\nproposed: One of the pillars of this work is focused on the estimation of the\nmost important parameters in the exploitation of renewable resources. The\nsecond contribution of this thesis is related to feature selection problems.\nThe proposed methodologies are applied to multiple problems: the prediction of\n$H_s$, relevant for marine energy applications and marine activities, the\nestimation of WPREs, undesirable variations in the electric power produced by a\nwind farm, the prediction of global solar radiation in areas from Spain and\nAustralia, really important in terms of solar energy, and the prediction of\nlow-visibility events at airports. All of these practical issues are developed\nwith the consequent previous data analysis, normally, in terms of\nmeteorological variables.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:53:01 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cornejo-Bueno", "L.", ""]]}, {"id": "1806.02679", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Daniel C. Castro, Loic Le Folgoc, Ian Walker,\n  Ryutaro Tanno, Daniel Rueckert, Ben Glocker, Antonio Criminisi, Aditya Nori", "title": "Semi-Supervised Learning via Compact Latent Space Clustering", "comments": "Presented as a long oral in ICML 2018. Post-conference camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel cost function for semi-supervised learning of neural\nnetworks that encourages compact clustering of the latent space to facilitate\nseparation. The key idea is to dynamically create a graph over embeddings of\nlabeled and unlabeled samples of a training batch to capture underlying\nstructure in feature space, and use label propagation to estimate its high and\nlow density regions. We then devise a cost function based on Markov chains on\nthe graph that regularizes the latent space to form a single compact cluster\nper class, while avoiding to disturb existing clusters during optimization. We\nevaluate our approach on three benchmarks and compare to state-of-the art with\npromising results. Our approach combines the benefits of graph-based\nregularization with efficient, inductive inference, does not require\nmodifications to a network architecture, and can thus be easily applied to\nexisting networks to enable an effective use of unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:41:56 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 19:20:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Castro", "Daniel C.", ""], ["Folgoc", "Loic Le", ""], ["Walker", "Ian", ""], ["Tanno", "Ryutaro", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Criminisi", "Antonio", ""], ["Nori", "Aditya", ""]]}, {"id": "1806.02706", "submitter": "Liangli Zhen", "authors": "Liangli Zhen, Miqing Li, Ran Cheng, Dezhong Peng, Xin Yao", "title": "Multiobjective Test Problems with Degenerate Pareto Fronts", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiobjective optimization, a set of scalable test problems with a\nvariety of features allows researchers to investigate and evaluate abilities of\ndifferent optimization algorithms, and thus can help them to design and develop\nmore effective and efficient approaches. Existing, commonly-used test problem\nsuites are mainly focused on the situations where all the objectives are\nconflicting with each other. However, in some many-objective optimization\nproblems, there may be unexpected characteristics among objectives, e.g.,\nredundancy. This leads to a degenerate problem. In this paper, we\nsystematically study degenerate problems. We abstract three generic\ncharacteristics of degenerate problems, and on the basis of these\ncharacteristics we present a set of test problems, in order to support the\ninvestigation of multiobjective search algorithms on problems with redundant\nobjectives. To assess the proposed test problems, ten representative\nmultiobjective evolutionary algorithms are tested. The results indicate that\nnone of the tested algorithms is able to effectively solve these proposed\nproblems, calling for the need of developing new approaches to addressing\ndegenerate multi-objective problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:38:24 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhen", "Liangli", ""], ["Li", "Miqing", ""], ["Cheng", "Ran", ""], ["Peng", "Dezhong", ""], ["Yao", "Xin", ""]]}, {"id": "1806.02855", "submitter": "Henri Palacci", "authors": "Henri Palacci, Henry Hess", "title": "Scalable Natural Gradient Langevin Dynamics in Practice", "comments": "ICML 2018 Workshop on Non-Convex Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for\nBayesian modeling adapted to large datasets and models. SGLD relies on the\ninjection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD)\nupdate. In this scheme, every component in the noise vector is independent and\nhas the same scale, whereas the parameters we seek to estimate exhibit strong\nvariations in scale and significant correlation structures, leading to poor\nconvergence and mixing times. We compare different preconditioning approaches\nto the normalization of the noise vector and benchmark these approaches on the\nfollowing criteria: 1) mixing times of the multivariate parameter vector, 2)\nregularizing effect on small dataset where it is easy to overfit, 3) covariate\nshift detection and 4) resistance to adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:38:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Palacci", "Henri", ""], ["Hess", "Henry", ""]]}, {"id": "1806.02925", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shengyang Sun, Jun Zhu", "title": "A Spectral Approach to Gradient Estimation for Implicit Distributions", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been increasing interests in learning and inference with\nimplicit distributions (i.e., distributions without tractable densities). To\nthis end, we develop a gradient estimator for implicit distributions based on\nStein's identity and a spectral decomposition of kernel operators, where the\neigenfunctions are approximated by the Nystr\\\"om method. Unlike the previous\nworks that only provide estimates at the sample points, our approach directly\nestimates the gradient function, thus allows for a simple and principled\nout-of-sample extension. We provide theoretical results on the error bound of\nthe estimator and discuss the bias-variance tradeoff in practice. The\neffectiveness of our method is demonstrated by applications to gradient-free\nHamiltonian Monte Carlo and variational inference with implicit distributions.\nFinally, we discuss the intuition behind the estimator by drawing connections\nbetween the Nystr\\\"om method and kernel PCA, which indicates that the estimator\ncan automatically adapt to the geometry of the underlying distribution.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 23:30:41 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shi", "Jiaxin", ""], ["Sun", "Shengyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1806.02932", "submitter": "Riley Simmons-Edler", "authors": "Riley Simmons-Edler, Anders Miltner, Sebastian Seung", "title": "Program Synthesis Through Reinforcement Learning Guided Tree Search", "comments": "9 pages, 5 figures, Submitted to NIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program Synthesis is the task of generating a program from a provided\nspecification. Traditionally, this has been treated as a search problem by the\nprogramming languages (PL) community and more recently as a supervised learning\nproblem by the machine learning community. Here, we propose a third approach,\nrepresenting the task of synthesizing a given program as a Markov decision\nprocess solvable via reinforcement learning(RL). From observations about the\nstates of partial programs, we attempt to find a program that is optimal over a\nprovided reward metric on pairs of programs and states. We instantiate this\napproach on a subset of the RISC-V assembly language operating on floating\npoint numbers, and as an optimization inspired by search-based techniques from\nthe PL community, we combine RL with a priority search tree. We evaluate this\ninstantiation and demonstrate the effectiveness of our combined method compared\nto a variety of baselines, including a pure RL ablation and a state of the art\nMarkov chain Monte Carlo search method on this task.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 00:53:43 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Simmons-Edler", "Riley", ""], ["Miltner", "Anders", ""], ["Seung", "Sebastian", ""]]}, {"id": "1806.02942", "submitter": "Yu Li", "authors": "Yu Li, Zhongxiao Li, Lizhong Ding, Yijie Pan, Chao Huang, Yuhui Hu,\n  Wei Chen, Xin Gao", "title": "SupportNet: solving catastrophic forgetting in class incremental\n  learning with support data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plain well-trained deep learning model often does not have the ability to\nlearn new knowledge without forgetting the previously learned knowledge, which\nis known as catastrophic forgetting. Here we propose a novel method,\nSupportNet, to efficiently and effectively solve the catastrophic forgetting\nproblem in the class incremental learning scenario. SupportNet combines the\nstrength of deep learning and support vector machine (SVM), where SVM is used\nto identify the support data from the old data, which are fed to the deep\nlearning model together with the new data for further training so that the\nmodel can review the essential information of the old data when learning the\nnew information. Two powerful consolidation regularizers are applied to\nstabilize the learned representation and ensure the robustness of the learned\nmodel. We validate our method with comprehensive experiments on various tasks,\nwhich show that SupportNet drastically outperforms the state-of-the-art\nincremental learning methods and even reaches similar performance as the deep\nlearning model trained from scratch on both old and new data. Our program is\naccessible at: https://github.com/lykaust15/SupportNet\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:58:51 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 12:37:58 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 08:51:17 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Yu", ""], ["Li", "Zhongxiao", ""], ["Ding", "Lizhong", ""], ["Pan", "Yijie", ""], ["Huang", "Chao", ""], ["Hu", "Yuhui", ""], ["Chen", "Wei", ""], ["Gao", "Xin", ""]]}, {"id": "1806.02957", "submitter": "Mohammad Amin Nabian", "authors": "Mohammad Amin Nabian, Hadi Meidani", "title": "A Deep Neural Network Surrogate for High-Dimensional Random Partial\n  Differential Equations", "comments": null, "journal-ref": "Probabilistic Engineering Mechanics, 57, pp.14-25 (2019)", "doi": "10.1016/j.probengmech.2019.05.001", "report-no": null, "categories": "cs.LG cs.NA cs.NE physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient numerical algorithms for the solution of high\ndimensional random Partial Differential Equations (PDEs) has been a challenging\ntask due to the well-known curse of dimensionality. We present a new solution\nframework for these problems based on a deep learning approach. Specifically,\nthe random PDE is approximated by a feed-forward fully-connected deep residual\nnetwork, with either strong or weak enforcement of initial and boundary\nconstraints. The framework is mesh-free, and can handle irregular computational\ndomains. Parameters of the approximating deep neural network are determined\niteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.\nThe satisfactory accuracy of the proposed frameworks is numerically\ndemonstrated on diffusion and heat conduction problems, in comparison with the\nconverged Monte Carlo-based finite element results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 03:24:50 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 23:59:31 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Nabian", "Mohammad Amin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1806.02960", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Hiroyuki Shindo, Yoshiyasu Takefuji", "title": "Representation Learning of Entities and Documents from Knowledge Base\n  Descriptions", "comments": "Accepted at COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe TextEnt, a neural network model that learns\ndistributed representations of entities and documents directly from a knowledge\nbase (KB). Given a document in a KB consisting of words and entity annotations,\nwe train our model to predict the entity that the document describes and map\nthe document and its target entity close to each other in a continuous vector\nspace. Our model is trained using a large number of documents extracted from\nWikipedia. The performance of the proposed model is evaluated using two tasks,\nnamely fine-grained entity typing and multiclass text classification. The\nresults demonstrate that our model achieves state-of-the-art performance on\nboth tasks. The code and the trained representations are made available online\nfor further academic research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 03:49:34 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Yamada", "Ikuya", ""], ["Shindo", "Hiroyuki", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "1806.02967", "submitter": "Xinye Cai", "authors": "Xinye Cai, Haoran Sun, Chunyang Zhu, Zhenyu Li, Qingfu Zhang", "title": "Locating the boundaries of Pareto fronts: A Many-Objective Evolutionary\n  Algorithm Based on Corner Solution Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an evolutionary many-objective optimization algorithm based on\ncorner solution search (MaOEA-CS) was proposed. MaOEA-CS implicitly contains\ntwo phases: the exploitative search for the most important boundary optimal\nsolutions - corner solutions, at the first phase, and the use of angle-based\nselection [1] with the explorative search for the extension of PF approximation\nat the second phase. Due to its high efficiency and robustness to the shapes of\nPFs, it has won the CEC'2017 Competition on Evolutionary Many-Objective\nOptimization. In addition, MaOEA-CS has also been applied on two real-world\nengineering optimization problems with very irregular PFs. The experimental\nresults show that MaOEA-CS outperforms other six state-of-the-art compared\nalgorithms, which indicates it has the ability to handle real-world complex\noptimization problems with irregular PFs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 04:34:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Cai", "Xinye", ""], ["Sun", "Haoran", ""], ["Zhu", "Chunyang", ""], ["Li", "Zhenyu", ""], ["Zhang", "Qingfu", ""]]}, {"id": "1806.02997", "submitter": "Aleksei Vasilev", "authors": "Aleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora\n  Sgarlata, Valentina Tomassini, Derek K. Jones, Daniel Cremers", "title": "q-Space Novelty Detection with Variational Autoencoders", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, novelty detection is the task of identifying novel\nunseen data. During training, only samples from the normal class are available.\nTest samples are classified as normal or abnormal by assignment of a novelty\nscore. Here we propose novelty detection methods based on training variational\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\ntraining, we define novelty metrics based on the (partially complementary)\nassumptions that the VAE is less capable of reconstructing abnormal samples\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\nabnormal samples differ from normal samples not only in input-feature space,\nbut also in the VAE latent space and VAE output. These approaches, combined\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\nobtain scalar novelty scores, yield a large family of methods. We apply these\nmethods to magnetic resonance imaging, namely to the detection of\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\nlesion labels for training. Many of our methods outperform previously proposed\nq-space novelty detection methods. We also evaluate the proposed methods on the\nMNIST handwritten digits dataset and show that many of them are able to\noutperform the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:28:36 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 17:34:51 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Vasilev", "Aleksei", ""], ["Golkov", "Vladimir", ""], ["Meissner", "Marc", ""], ["Lipp", "Ilona", ""], ["Sgarlata", "Eleonora", ""], ["Tomassini", "Valentina", ""], ["Jones", "Derek K.", ""], ["Cremers", "Daniel", ""]]}, {"id": "1806.03583", "submitter": "Mehdi Faraji", "authors": "Ji Yang, Lin Tong, Mehdi Faraji, Anup Basu", "title": "IVUS-Net: An Intravascular Ultrasound Segmentation Network", "comments": "7 pages, 3 figures, accepted to be published in International\n  Conference of Smart Multimedia. The final authenticated publication is\n  available online at https://doi.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IntraVascular UltraSound (IVUS) is one of the most effective imaging\nmodalities that provides assistance to experts in order to diagnose and treat\ncardiovascular diseases. We address a central problem in IVUS image analysis\nwith Fully Convolutional Network (FCN): automatically delineate the lumen and\nmedia-adventitia borders in IVUS images, which is crucial to shorten the\ndiagnosis process or benefits a faster and more accurate 3D reconstruction of\nthe artery. Particularly, we propose an FCN architecture, called IVUS-Net,\nfollowed by a post-processing contour extraction step, in order to\nautomatically segments the interior (lumen) and exterior (media-adventitia)\nregions of the human arteries. We evaluated our IVUS-Net on the test set of a\nstandard publicly available dataset containing 326 IVUS B-mode images with two\nmeasurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The\nevaluation result shows that IVUS-Net outperforms the state-of-the-art lumen\nand media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net\nperforms well on images in the test set that contain a significant amount of\nmajor artifacts such as bifurcations, shadows, and side branches that are not\ncommon in the training set. Furthermore, using a modern GPU, IVUS-Net segments\neach IVUS frame only in 0.15 seconds. The proposed work, to the best of our\nknowledge, is the first deep learning based method for segmentation of both the\nlumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the\nbest results without any manual intervention. Code is available at\nhttps://github.com/Kulbear/ivus-segmentation-icsm2018\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 04:28:53 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 17:53:51 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yang", "Ji", ""], ["Tong", "Lin", ""], ["Faraji", "Mehdi", ""], ["Basu", "Anup", ""]]}, {"id": "1806.03645", "submitter": "Goren Gordon", "authors": "Jonatan Barkan and Goren Gordon", "title": "Deep Curiosity Loops in Social Environments", "comments": "10 pages, 3 figures, submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by infants' intrinsic motivation to learn, which values informative\nsensory channels contingent on their immediate social environment, we developed\na deep curiosity loop (DCL) architecture. The DCL is composed of a learner,\nwhich attempts to learn a forward model of the agent's state-action transition,\nand a novel reinforcement-learning (RL) component, namely, an\nAction-Convolution Deep Q-Network, which uses the learner's prediction error as\nreward. The environment for our agent is composed of visual social scenes,\ncomposed of sitcom video streams, thereby both the learner and the RL are\nconstructed as deep convolutional neural networks. The agent's learner learns\nto predict the zero-th order of the dynamics of visual scenes, resulting in\nintrinsic rewards proportional to changes within its social environment. The\nsources of these socially informative changes within the sitcom are\npredominantly motions of faces and hands, leading to the unsupervised\ncuriosity-based learning of social interaction features. The face and hand\ndetection is represented by the value function and the social interaction\noptical-flow is represented by the policy. Our results suggest that face and\nhand detection are emergent properties of curiosity-based learning embedded in\nsocial environments.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 12:17:03 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Barkan", "Jonatan", ""], ["Gordon", "Goren", ""]]}, {"id": "1806.03674", "submitter": "Ofer Shir", "authors": "Ofer M. Shir and Amir Yehudayoff", "title": "On the Covariance-Hessian Relation in Evolution Strategies", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2019.09.002", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Evolution Strategies operating only with isotropic Gaussian\nmutations on positive quadratic objective functions, and investigate the\ncovariance matrix when constructed out of selected individuals by truncation.\nWe prove that the covariance matrix over $(1,\\lambda)$-selected decision\nvectors becomes proportional to the inverse of the landscape Hessian as the\npopulation-size $\\lambda$ increases. This generalizes a previous result that\nproved an equivalent phenomenon when sampling was assumed to take place in the\nvicinity of the optimum. It further confirms the classical hypothesis that\nstatistical learning of the landscape is an inherent characteristic of standard\nEvolution Strategies, and that this distinguishing capability stems only from\nthe usage of isotropic Gaussian mutations and rank-based selection. We provide\nbroad numerical validation for the proven results, and present empirical\nevidence for its generalization to $(\\mu,\\lambda)$-selection.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 15:30:10 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 16:51:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shir", "Ofer M.", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1806.03751", "submitter": "Michael Hauser", "authors": "Michael Hauser, Sean Gunn, Samer Saab Jr, Asok Ray", "title": "State Space Representations of Deep Neural Networks", "comments": null, "journal-ref": "Neural Computation, Volume 31, Issue 3, March 2019, p.538-554", "doi": "10.1162/neco_a_01165", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with neural networks as dynamical systems governed by\ndifferential or difference equations. It shows that the introduction of skip\nconnections into network architectures, such as residual networks and dense\nnetworks, turns a system of static equations into a system of dynamical\nequations with varying levels of smoothness on the layer-wise transformations.\nClosed form solutions for the state space representations of general dense\nnetworks, as well as $k^{th}$ order smooth networks, are found in general\nsettings. Furthermore, it is shown that imposing $k^{th}$ order smoothness on a\nnetwork architecture with $d$-many nodes per layer increases the state space\ndimension by a multiple of $k$, and so the effective embedding dimension of the\ndata manifold is $k \\cdot d$-many dimensions. It follows that network\narchitectures of these types reduce the number of parameters needed to maintain\nthe same embedding dimension by a factor of $k^2$ when compared to an\nequivalent first-order, residual network, significantly motivating the\ndevelopment of network architectures of these types. Numerical simulations were\nrun to validate parts of the developed theory.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 00:26:13 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 02:57:47 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 21:11:08 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Hauser", "Michael", ""], ["Gunn", "Sean", ""], ["Saab", "Samer", "Jr"], ["Ray", "Asok", ""]]}, {"id": "1806.03796", "submitter": "Yash Upadhyay", "authors": "Yash Upadhyay, Paul Schrater", "title": "Generative Adversarial Network Architectures For Image Synthesis Using\n  Capsule Networks", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Generative Adversarial Network (GAN) architectures\nthat use Capsule Networks for image-synthesis. Based on the principal of\npositional-equivariance of features, Capsule Network's ability to encode\nspatial relationships between the features of the image helps it become a more\npowerful critic in comparison to Convolutional Neural Networks (CNNs) used in\ncurrent architectures for image synthesis. Our proposed GAN architectures learn\nthe data manifold much faster and therefore, synthesize visually accurate\nimages in significantly lesser number of training samples and training epochs\nin comparison to GANs and its variants that use CNNs. Apart from analyzing the\nquantitative results corresponding the images generated by different\narchitectures, we also explore the reasons for the lower coverage and diversity\nexplored by the GAN architectures that use CNN critics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:54:24 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:55:20 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 15:45:28 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 18:33:11 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Upadhyay", "Yash", ""], ["Schrater", "Paul", ""]]}, {"id": "1806.03934", "submitter": "Ella Gale", "authors": "Ella M. Gale, Nicolas Martin, Jeffrey S. Bowers", "title": "When and where do feed-forward neural networks learn localist\n  representations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to parallel distributed processing (PDP) theory in psychology,\nneural networks (NN) learn distributed rather than interpretable localist\nrepresentations. This view has been held so strongly that few researchers have\nanalysed single units to determine if this assumption is correct. However,\nrecent results from psychology, neuroscience and computer science have shown\nthe occasional existence of local codes emerging in artificial and biological\nneural networks. In this paper, we undertake the first systematic survey of\nwhen local codes emerge in a feed-forward neural network, using generated input\nand output data with known qualities. We find that the number of local codes\nthat emerge from a NN follows a well-defined distribution across the number of\nhidden layer neurons, with a peak determined by the size of input data, number\nof examples presented and the sparsity of input data. Using a 1-hot output code\ndrastically decreases the number of local codes on the hidden layer. The number\nof emergent local codes increases with the percentage of dropout applied to the\nhidden layer, suggesting that the localist encoding may offer a resilience to\nnoisy networks. This data suggests that localist coding can emerge from\nfeed-forward PDP networks and suggests some of the conditions that may lead to\ninterpretable localist representations in the cortex. The findings highlight\nhow local codes should not be dismissed out of hand.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 12:21:55 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Gale", "Ella M.", ""], ["Martin", "Nicolas", ""], ["Bowers", "Jeffrey S.", ""]]}, {"id": "1806.04278", "submitter": "Chris Donahue", "authors": "Chris Donahue, Huanru Henry Mao, Julian McAuley", "title": "The NES Music Database: A multi-instrumental dataset with expressive\n  performance attributes", "comments": "Published as a conference paper at ISMIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research on music generation focuses on composition, but often\nignores the expressive performance characteristics required for plausible\nrenditions of resultant pieces. In this paper, we introduce the Nintendo\nEntertainment System Music Database (NES-MDB), a large corpus allowing for\nseparate examination of the tasks of composition and performance. NES-MDB\ncontains thousands of multi-instrumental songs composed for playback by the\ncompositionally-constrained NES audio synthesizer. For each song, the dataset\ncontains a musical score for four instrument voices as well as expressive\nattributes for the dynamics and timbre of each voice. Unlike datasets comprised\nof General MIDI files, NES-MDB includes all of the information needed to render\nexact acoustic performances of the original compositions. Alongside the\ndataset, we provide a tool that renders generated compositions as NES-style\naudio by emulating the device's audio processor. Additionally, we establish\nbaselines for the tasks of composition, which consists of learning the\nsemantics of composing for the NES synthesizer, and performance, which involves\nfinding a mapping between a composition and realistic expressive attributes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 00:28:50 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Donahue", "Chris", ""], ["Mao", "Huanru Henry", ""], ["McAuley", "Julian", ""]]}, {"id": "1806.04344", "submitter": "Johannes Otterbach", "authors": "Johannes S. Otterbach", "title": "Optimizing Variational Quantum Circuits using Evolution Strategies", "comments": "This version withdrawn by arXiv administrators because the submitter\n  did not have the right to agree to our license at the time of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This version withdrawn by arXiv administrators because the submitter did not\nhave the right to agree to our license at the time of submission.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 05:57:14 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Otterbach", "Johannes S.", ""]]}, {"id": "1806.04419", "submitter": "Harshit Mehrotra", "authors": "Harshit Mehrotra and Dr. Saibal K. Pal", "title": "Using Chaos in Grey Wolf Optimizer and Application to Prime\n  Factorization", "comments": "7th International Conference on Soft Computing for Problem Solving -\n  SocProS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Grey Wolf Optimizer (GWO) is a swarm intelligence meta-heuristic\nalgorithm inspired by the hunting behaviour and social hierarchy of grey wolves\nin nature. This paper analyses the use of chaos theory in this algorithm to\nimprove its ability to escape local optima by replacing the key parameters by\nchaotic variables. The optimal choice of chaotic maps is then used to apply the\nChaotic Grey Wolf Optimizer (CGWO) to the problem of factoring a large semi\nprime into its prime factors. Assuming the number of digits of the factors to\nbe equal, this is a computationally difficult task upon which the\nRSA-cryptosystem relies. This work proposes the use of a new objective function\nto solve the problem and uses the CGWO to optimize it and compute the factors.\nIt is shown that this function performs better than its predecessor for large\nsemi primes and CGWO is an efficient algorithm to optimize it.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:51:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Mehrotra", "Harshit", ""], ["Pal", "Dr. Saibal K.", ""]]}, {"id": "1806.04528", "submitter": "Martin Pil\\'at", "authors": "\\v{S}t\\v{e}p\\'an Balcar, Martin Pil\\'at", "title": "Online Parallel Portfolio Selection with Heterogeneous Island Model", "comments": "8 pages, submitted to ICTAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online parallel portfolio selection algorithm based on the\nisland model commonly used for parallelization of evolutionary algorithms. In\nour case each of the islands runs a different optimization algorithm. The\ndistributed computation is managed by a central planner which periodically\nchanges the running methods during the execution of the algorithm -- less\nsuccessful methods are removed while new instances of more successful methods\nare added.\n  We compare different types of planners in the heterogeneous island model\namong themselves and also to the traditional homogeneous model on a wide set of\nproblems. The tests include experiments with different representations of the\nindividuals and different duration of fitness function evaluations. The results\nshow that heterogeneous models are a more general and universal computational\ntool compared to homogeneous models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:57:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Balcar", "\u0160t\u011bp\u00e1n", ""], ["Pil\u00e1t", "Martin", ""]]}, {"id": "1806.04552", "submitter": "Sreecharan Sankaranarayanan", "authors": "Sreecharan Sankaranarayanan, Raghuram Mandyam Annasamy, Katia Sycara,\n  Carolyn Penstein Ros\\'e", "title": "Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed\n  Exploration", "comments": "Submitted to the Thirty-Second Annual Conference on Neural\n  Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-Ensembles are a model-free approach where input images are fed into\ndifferent Q-networks and exploration is driven by the assumption that\nuncertainty is proportional to the variance of the output Q-values obtained.\nThey have been shown to perform relatively well compared to other exploration\nstrategies. Further, model-based approaches, such as encoder-decoder models\nhave been used successfully for next frame prediction given previous frames.\nThis paper proposes to integrate the model-free Q-ensembles and model-based\napproaches with the hope of compounding the benefits of both and achieving\nsuperior exploration as a result. Results show that a model-based trajectory\nmemory approach when combined with Q-ensembles produces superior performance\nwhen compared to only using Q-ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:24:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sankaranarayanan", "Sreecharan", ""], ["Annasamy", "Raghuram Mandyam", ""], ["Sycara", "Katia", ""], ["Ros\u00e9", "Carolyn Penstein", ""]]}, {"id": "1806.04563", "submitter": "Michael Hellwig", "authors": "Michael Hellwig and Hans-Georg Beyer", "title": "Benchmarking Evolutionary Algorithms For Single Objective Real-valued\n  Constrained Optimization - A Critical Review", "comments": "This manuscript is a preprint version of an article published in\n  Swarm and Evolutionary Computation, Elsevier, 2018. Number of pages: 45", "journal-ref": null, "doi": "10.1016/j.swevo.2018.10.002", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarking plays an important role in the development of novel search\nalgorithms as well as for the assessment and comparison of contemporary\nalgorithmic ideas. This paper presents common principles that need to be taken\ninto account when considering benchmarking problems for constrained\noptimization. Current benchmark environments for testing Evolutionary\nAlgorithms are reviewed in the light of these principles. Along with this line,\nthe reader is provided with an overview of the available problem domains in the\nfield of constrained benchmarking. Hence, the review supports algorithms\ndevelopers with information about the merits and demerits of the available\nframeworks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:42:07 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 10:27:50 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Hellwig", "Michael", ""], ["Beyer", "Hans-Georg", ""]]}, {"id": "1806.04641", "submitter": "Sabine Hossenfelder", "authors": "Tobias Mistele, Tom Price, Sabine Hossenfelder", "title": "Predicting Citation Counts with a Neural Network", "comments": "12 figure, 17 pages, typo fixed, reference updated", "journal-ref": "Scientometrics (2019) 120: 87", "doi": "10.1007/s11192-019-03110-2", "report-no": null, "categories": "cs.DL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here describe and present results of a simple neural network that predicts\nindividual researchers' future citation counts based on a variety of data from\nthe researchers' past. For publications available on the open access-server\narXiv.org we find a higher predictability than previous studies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:54:12 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 06:08:45 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Mistele", "Tobias", ""], ["Price", "Tom", ""], ["Hossenfelder", "Sabine", ""]]}, {"id": "1806.04646", "submitter": "George Gondim-Ribeiro", "authors": "George Gondim-Ribeiro, Pedro Tabacof, Eduardo Valle", "title": "Adversarial Attacks on Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks are malicious inputs that derail machine-learning models.\nWe propose a scheme to attack autoencoders, as well as a quantitative\nevaluation framework that correlates well with the qualitative assessment of\nthe attacks. We assess --- with statistically validated experiments --- the\nresistance to attacks of three variational autoencoders (simple, convolutional,\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\nrecurrence and attention mechanism lead to better resistance. As autoencoders\nare proposed for compressing data --- a scenario in which their safety is\nparamount --- we expect more attention will be given to adversarial attacks on\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:59:14 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gondim-Ribeiro", "George", ""], ["Tabacof", "Pedro", ""], ["Valle", "Eduardo", ""]]}, {"id": "1806.04872", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Hao Tang, James Glass", "title": "Unsupervised Adaptation with Interpretable Disentangled Representations\n  for Distant Conversational Speech Recognition", "comments": "to appear in Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend in automatic speech recognition is to leverage large\namounts of labeled data to train supervised neural network models.\nUnfortunately, obtaining data for a wide range of domains to train robust\nmodels can be costly. However, it is relatively inexpensive to collect large\namounts of unlabeled data from domains that we want the models to generalize\nto. In this paper, we propose a novel unsupervised adaptation method that\nlearns to synthesize labeled data for the target domain from unlabeled\nin-domain data and labeled out-of-domain data. We first learn without\nsupervision an interpretable latent representation of speech that encodes\nlinguistic and nuisance factors (e.g., speaker and channel) using different\nlatent variables. To transform a labeled out-of-domain utterance without\naltering its transcript, we transform the latent nuisance variables while\nmaintaining the linguistic variables. To demonstrate our approach, we focus on\na channel mismatch setting, where the domain of interest is distant\nconversational speech, and labels are only available for close-talking speech.\nOur proposed method is evaluated on the AMI dataset, outperforming all\nbaselines and bridging the gap between unadapted and in-domain models by over\n77% without using any parallel data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 07:14:15 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Tang", "Hao", ""], ["Glass", "James", ""]]}, {"id": "1806.04932", "submitter": "Josep L. Rossello", "authors": "Alejandro Mor\\'an, Christiam F. Frasser and Josep L. Rossell\\'o", "title": "Reservoir Computing Hardware with Cellular Automata", "comments": "20 pages, 11 figures, draft of an article currently submitted to IEEE\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV nlin.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Elementary cellular automata (ECA) is a widely studied one-dimensional\nprocessing methodology where the successive iteration of the automaton may lead\nto the recreation of a rich pattern dynamic. Recently, cellular automata have\nbeen proposed as a feasible way to implement Reservoir Computing (RC) systems\nin which the automata rule is fixed and the training is performed using a\nlinear regression. In this work we perform an exhaustive study of the\nperformance of the different ECA rules when applied to pattern recognition of\ntime-independent input signals using a RC scheme. Once the different ECA rules\nhave been tested, the most accurate one (rule 90) is selected to implement a\ndigital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates\nand shift-registers, thus representing a high-performance alternative for RC\nhardware implementation in terms of processing time, circuit area, power\ndissipation and system accuracy. The model (both in software and its hardware\nimplementation) has been tested using a pattern recognition task of handwritten\nnumbers (the MNIST database) for which we obtained competitive results in terms\nof accuracy, speed and power dissipation. The proposed model can be considered\nto be a low-cost method to implement fast pattern recognition digital circuits.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:28:44 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 09:23:43 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Mor\u00e1n", "Alejandro", ""], ["Frasser", "Christiam F.", ""], ["Rossell\u00f3", "Josep L.", ""]]}, {"id": "1806.05034", "submitter": "Bernardino Romera-Paredes", "authors": "Simon A. A. Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De\n  Fauw, Joseph R. Ledsam, Klaus H. Maier-Hein, S. M. Ali Eslami, Danilo Jimenez\n  Rezende, Olaf Ronneberger", "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images", "comments": "Last update: added further details about the LIDC experiment. 11\n  pages for the main paper, 28 pages including appendix. 5 figures in the main\n  paper, 18 figures in total, Advances in Neural Information Processing Systems\n  (NeurIPS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world vision problems suffer from inherent ambiguities. In clinical\napplications for example, it might not be clear from a CT scan alone which\nparticular region is cancer tissue. Therefore a group of graders typically\nproduces a set of diverse but plausible segmentations. We consider the task of\nlearning a distribution over segmentations given an input. To this end we\npropose a generative segmentation model based on a combination of a U-Net with\na conditional variational autoencoder that is capable of efficiently producing\nan unlimited number of plausible hypotheses. We show on a lung abnormalities\nsegmentation task and on a Cityscapes segmentation task that our model\nreproduces the possible segmentation variants as well as the frequencies with\nwhich they occur, doing so significantly better than published approaches.\nThese models could have a high impact in real-world applications, such as being\nused as clinical decision-making algorithms accounting for multiple plausible\nsemantic segmentation hypotheses to provide possible diagnoses and recommend\nfurther actions to resolve the present ambiguities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:47:04 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:34:57 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 09:50:55 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 18:26:47 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kohl", "Simon A. A.", ""], ["Romera-Paredes", "Bernardino", ""], ["Meyer", "Clemens", ""], ["De Fauw", "Jeffrey", ""], ["Ledsam", "Joseph R.", ""], ["Maier-Hein", "Klaus H.", ""], ["Eslami", "S. M. Ali", ""], ["Rezende", "Danilo Jimenez", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1806.05141", "submitter": "Baibhab Chatterjee", "authors": "Baibhab Chatterjee, Priyadarshini Panda, Shovan Maity, Ayan Biswas,\n  Kaushik Roy, and Shreyas Sen", "title": "Exploiting Inherent Error-Resiliency of Neuromorphic Computing to\n  achieve Extreme Energy-Efficiency through Mixed-Signal Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing, inspired by the brain, promises extreme efficiency\nfor certain classes of learning tasks, such as classification and pattern\nrecognition. The performance and power consumption of neuromorphic computing\ndepends heavily on the choice of the neuron architecture. Digital neurons\n(Dig-N) are conventionally known to be accurate and efficient at high speed,\nwhile suffering from high leakage currents from a large number of transistors\nin a large design. On the other hand, analog/mixed-signal neurons are prone to\nnoise, variability and mismatch, but can lead to extremely low-power designs.\nIn this work, we will analyze, compare and contrast existing neuron\narchitectures with a proposed mixed-signal neuron (MS-N) in terms of\nperformance, power and noise, thereby demonstrating the applicability of the\nproposed mixed-signal neuron for achieving extreme energy-efficiency in\nneuromorphic computing. The proposed MS-N is implemented in 65 nm CMOS\ntechnology and exhibits > 100X better energy-efficiency across all frequencies\nover two traditional digital neurons synthesized in the same technology node.\nWe also demonstrate that the inherent error-resiliency of a fully connected or\neven convolutional neural network (CNN) can handle the noise as well as the\nmanufacturing non-idealities of the MS-N up to certain degrees. Notably, a\nsystem-level implementation on MNIST datasets exhibits a worst-case increase in\nclassification error by 2.1% when the integrated noise power in the bandwidth\nis ~ 0.1 uV2, along with +-3{\\sigma} amount of variation and mismatch\nintroduced in the transistor parameters for the proposed neuron with 8-bit\nprecision.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:43:05 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Chatterjee", "Baibhab", ""], ["Panda", "Priyadarshini", ""], ["Maity", "Shovan", ""], ["Biswas", "Ayan", ""], ["Roy", "Kaushik", ""], ["Sen", "Shreyas", ""]]}, {"id": "1806.05236", "submitter": "Vikas Verma", "authors": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis\n  Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio", "title": "Manifold Mixup: Better Representations by Interpolating Hidden States", "comments": "To appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel at learning the training data, but often provide\nincorrect and confident predictions when evaluated on slightly different test\nexamples. This includes distribution shifts, outliers, and adversarial\nexamples. To address these issues, we propose Manifold Mixup, a simple\nregularizer that encourages neural networks to predict less confidently on\ninterpolations of hidden representations. Manifold Mixup leverages semantic\ninterpolations as additional training signal, obtaining neural networks with\nsmoother decision boundaries at multiple levels of representation. As a result,\nneural networks trained with Manifold Mixup learn class-representations with\nfewer directions of variance. We prove theory on why this flattening happens\nunder ideal conditions, validate it on practical situations, and connect it to\nprevious works on information theory and generalization. In spite of incurring\nno significant computation and being implemented in a few lines of code,\nManifold Mixup improves strong baselines in supervised learning, robustness to\nsingle-step adversarial attacks, and test log-likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:32:59 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 18:10:16 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 20:49:11 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 18:18:18 GMT"}, {"version": "v5", "created": "Sat, 9 Mar 2019 18:27:06 GMT"}, {"version": "v6", "created": "Tue, 12 Mar 2019 02:14:07 GMT"}, {"version": "v7", "created": "Sat, 11 May 2019 16:50:55 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Verma", "Vikas", ""], ["Lamb", "Alex", ""], ["Beckham", "Christopher", ""], ["Najafi", "Amir", ""], ["Mitliagkas", "Ioannis", ""], ["Courville", "Aaron", ""], ["Lopez-Paz", "David", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.05298", "submitter": "Juan C. Cuevas-Tello", "authors": "J.C. Cuevas-Tello", "title": "Apuntes de Redes Neuronales Artificiales", "comments": "20 pages, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These handouts are designed for people who is just starting involved with the\ntopic artificial neural networks. We show how it works a single artificial\nneuron (McCulloch & Pitt model), mathematically and graphically. We do explain\nthe delta rule, a learning algorithm to find the neuron weights. We also\npresent some examples in MATLAB/Octave. There are examples for classification\ntask for lineal and non-lineal problems. At the end, we present an artificial\nneural network, a feed-forward neural network along its learning algorithm\nbackpropagation.\n  -----\n  Estos apuntes est\\'an dise\\~nados para personas que por primera vez se\nintroducen en el tema de las redes neuronales artificiales. Se muestra el\nfuncionamiento b\\'asico de una neurona, matem\\'aticamente y gr\\'aficamente. Se\nexplica la Regla Delta, algoritmo deaprendizaje para encontrar los pesos de una\nneurona. Tambi\\'en se muestran ejemplos en MATLAB/Octave. Hay ejemplos para\nproblemas de clasificaci\\'on, para problemas lineales y no-lineales. En la\nparte final se muestra la arquitectura de red neuronal artificial conocida como\nbackpropagation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:23:22 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Cuevas-Tello", "J. C.", ""]]}, {"id": "1806.05387", "submitter": "Erik Schlogl", "authors": "Karol Gellert and Erik Schl\\\"ogl", "title": "Parameter Learning and Change Detection Using a Particle Filter With\n  Accelerated Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": "QFRC working paper 392", "categories": "stat.ML cs.CE cs.LG cs.NE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the construction of a particle filter, which incorporates\nelements inspired by genetic algorithms, in order to achieve accelerated\nadaptation of the estimated posterior distribution to changes in model\nparameters. Specifically, the filter is designed for the situation where the\nsubsequent data in online sequential filtering does not match the model\nposterior filtered based on data up to a current point in time. The examples\nconsidered encompass parameter regime shifts and stochastic volatility. The\nfilter adapts to regime shifts extremely rapidly and delivers a clear heuristic\nfor distinguishing between regime shifts and stochastic volatility, even though\nthe model dynamics assumed by the filter exhibit neither of those features.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:41:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Gellert", "Karol", ""], ["Schl\u00f6gl", "Erik", ""]]}, {"id": "1806.05392", "submitter": "Martin Krejca", "authors": "Martin S. Krejca and Carsten Witt", "title": "Theory of Estimation-of-Distribution Algorithms", "comments": "This is a preliminary version of a chapter in the upcoming book\n  \"Theory of Randomized Search Heuristics in Discrete Search Spaces\", edited by\n  Benjamin Doerr and Frank Neumann, to be published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation-of-distribution algorithms (EDAs) are general metaheuristics used\nin optimization that represent a more recent alternative to classical\napproaches like evolutionary algorithms. In a nutshell, EDAs typically do not\ndirectly evolve populations of search points but build probabilistic models of\npromising solutions by repeatedly sampling and selecting points from the\nunderlying search space. Recently, there has been made significant progress in\nthe theoretical understanding of EDAs. This article provides an up-to-date\noverview of the most commonly analyzed EDAs and the most recent theoretical\nresults in this area. In particular, emphasis is put on the runtime analysis of\nsimple univariate EDAs, including a description of typical benchmark functions\nand tools for the analysis. Along the way, open problems and directions for\nfuture research are described.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:54:20 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Krejca", "Martin S.", ""], ["Witt", "Carsten", ""]]}, {"id": "1806.05695", "submitter": "Dennis George Wilson", "authors": "Dennis G Wilson, Sylvain Cussat-Blanc, Herv\\'e Luga, Julian F Miller", "title": "Evolving simple programs for playing Atari games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartesian Genetic Programming (CGP) has previously shown capabilities in\nimage processing tasks by evolving programs with a function set specialized for\ncomputer vision. A similar approach can be applied to Atari playing. Programs\nare evolved using mixed type CGP with a function set suited for matrix\noperations, including image processing, but allowing for controller behavior to\nemerge. While the programs are relatively small, many controllers are\ncompetitive with state of the art methods for the Atari benchmark set and\nrequire less training time. By evaluating the programs of the best evolved\nindividuals, simple but effective strategies can be found.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:10:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Wilson", "Dennis G", ""], ["Cussat-Blanc", "Sylvain", ""], ["Luga", "Herv\u00e9", ""], ["Miller", "Julian F", ""]]}, {"id": "1806.05759", "submitter": "Ari Morcos", "authors": "Ari S. Morcos, Maithra Raghu, and Samy Bengio", "title": "Insights on representational similarity in neural networks with\n  canonical correlation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 22:34:11 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 23:09:23 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 18:59:02 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Morcos", "Ari S.", ""], ["Raghu", "Maithra", ""], ["Bengio", "Samy", ""]]}, {"id": "1806.05794", "submitter": "Mohsen Imani", "authors": "Mohsen Imani, Mohammad Samragh, Yeseong Kim, Saransh Gupta, Farinaz\n  Koushanfar, Tajana Rosing", "title": "RAPIDNN: In-Memory Deep Neural Network Acceleration Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have demonstrated effectiveness for various\napplications such as image processing, video segmentation, and speech\nrecognition. Running state-of-the-art DNNs on current systems mostly relies on\neither generalpurpose processors, ASIC designs, or FPGA accelerators, all of\nwhich suffer from data movements due to the limited onchip memory and data\ntransfer bandwidth. In this work, we propose a novel framework, called RAPIDNN,\nwhich processes all DNN operations within the memory to minimize the cost of\ndata movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model\nand maps it into a specialized accelerator, which is designed using\nnon-volatile memory blocks that model four fundamental DNN operations, i.e.,\nmultiplication, addition, activation functions, and pooling. The framework\nextracts representative operands of a DNN model, e.g., weights and input\nvalues, using clustering methods to optimize the model for in-memory\nprocessing. Then, it maps the extracted operands and their precomputed results\ninto the accelerator memory blocks. At runtime, the accelerator identifies\ncomputation results based on efficient in-memory search capability which also\nprovides tunability of approximation to further improve computation efficiency.\nOur evaluation shows that RAPIDNN achieves 68.4x, 49.5x energy efficiency\nimprovement and 48.1x, 10.9x speedup as compared to ISAAC and PipeLayer, the\nstate-of-the-art DNN accelerators, while ensuring less than 0.3% of quality\nloss.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:07:55 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 05:05:01 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 03:09:32 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 18:36:50 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Imani", "Mohsen", ""], ["Samragh", "Mohammad", ""], ["Kim", "Yeseong", ""], ["Gupta", "Saransh", ""], ["Koushanfar", "Farinaz", ""], ["Rosing", "Tajana", ""]]}, {"id": "1806.05845", "submitter": "Michael Hellwig", "authors": "Patrick Spettel, Hans-Georg Beyer, and Michael Hellwig", "title": "A Covariance Matrix Self-Adaptation Evolution Strategy for Optimization\n  under Linear Constraints", "comments": "This is a PREPRINT of an article accepted by IEEE TRANSACTIONS ON\n  EVOLUTIONARY COMPUTATION. >>> COPYRIGHT 2018 IEEE <<< Manuscript received\n  Jan, 2018; revised Jun, 2018; accepted Sep, 2018. Due to size limitations,\n  this manuscript comprises figures with reduced resolution. The work was\n  supported by the Austrian Science Fund FWF under grant P29651-N32. Content:\n  10 pages + supplementary material", "journal-ref": null, "doi": "10.1109/TEVC.2018.2871944", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the development of a covariance matrix self-adaptation\nevolution strategy (CMSA-ES) for solving optimization problems with linear\nconstraints. The proposed algorithm is referred to as Linear Constraint CMSA-ES\n(lcCMSA-ES). It uses a specially built mutation operator together with repair\nby projection to satisfy the constraints. The lcCMSA-ES evolves itself on a\nlinear manifold defined by the constraints. The objective function is only\nevaluated at feasible search points (interior point method). This is a property\noften required in application domains such as simulation optimization and\nfinite element methods. The algorithm is tested on a variety of different test\nproblems revealing considerable results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 08:06:57 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 10:36:15 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Spettel", "Patrick", ""], ["Beyer", "Hans-Georg", ""], ["Hellwig", "Michael", ""]]}, {"id": "1806.05865", "submitter": "Adam Gaier", "authors": "Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret", "title": "Data-Efficient Design Exploration through Surrogate-Assisted\n  Illumination", "comments": "ArXiv preprint version, final version published in Evolutionary\n  Computation, doi: 10.1162/evco_a_00231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design optimization techniques are often used at the beginning of the design\nprocess to explore the space of possible designs. In these domains illumination\nalgorithms, such as MAP-Elites, are promising alternatives to classic\noptimization algorithms because they produce diverse, high-quality solutions in\na single run, instead of only a single near-optimal solution. Unfortunately,\nthese algorithms currently require a large number of function evaluations,\nlimiting their applicability. In this article we introduce a new illumination\nalgorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate\nmodeling techniques to create a map of the design space according to\nuser-defined features while minimizing the number of fitness evaluations. On a\n2-dimensional airfoil optimization problem SAIL produces hundreds of diverse\nbut high-performing designs with several orders of magnitude fewer evaluations\nthan MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of\nproducing maps of high-performing designs in realistic 3-dimensional\naerodynamic tasks with an accurate flow simulation. Data-efficient design\nexploration with SAIL can help designers understand what is possible, beyond\nwhat is optimal, by considering more than pure objective-based optimization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:00:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Gaier", "Adam", ""], ["Asteroth", "Alexander", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1806.05876", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Financial Risk and Returns Prediction with Modular Networked Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial agent for financial risk and returns' prediction is built with\na modular cognitive system comprised of interconnected recurrent neural\nnetworks, such that the agent learns to predict the financial returns, and\nlearns to predict the squared deviation around these predicted returns. These\ntwo expectations are used to build a volatility-sensitive interval prediction\nfor financial returns, which is evaluated on three major financial indices and\nshown to be able to predict financial returns with higher than 80% success rate\nin interval prediction in both training and testing, raising into question the\nEfficient Market Hypothesis. The agent is introduced as an example of a class\nof artificial intelligent systems that are equipped with a Modular Networked\nLearning cognitive system, defined as an integrated networked system of machine\nlearning modules, where each module constitutes a functional unit that is\ntrained for a given specific task that solves a subproblem of a complex main\nproblem expressed as a network of linked subproblems. In the case of neural\nnetworks, these systems function as a form of an \"artificial brain\", where each\nmodule is like a specialized brain region comprised of a neural network with a\nspecific architecture.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:49:39 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1806.05971", "submitter": "Wissem Abbes", "authors": "Wissem Abbes, Zied Kechaou, Adel M. Alimi", "title": "An Enhanced BPSO based Approach for Service Placement in Hybrid Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the challenges of competition and the rapidly evolving market,\ncompanies need to be innovative and agile, particularly in regard of web\napplications as used by customers. Nowadays, hybrid cloud stands as an\nattractive solution as organizations tend to use a combination of private and\npublic cloud implementations, in accordance with their appropriate needs to\nprofitably apply the available resources and speed of execution. In such a\ncase, deploying the new applications would certainly entail opting for placing\nand consecrating some components to the private cloud option, while reserving\nsome others to the public cloud option. In this respect, our primary goal in\nthis paper consists in minimizing the extra costs likely to be incurred by\napplying the public cloud related options, along with those costs involved in\nmaintaining communication between the private cloud system and the public cloud\nframework. As for our second targeted objective, it lies in reducing the\ndecision process relating to the execution time, necessary for selecting the\noptimal service placement solution. For this purpose, a novel Binary Particle\nSwarm Optimization (BPSO) based approach is proposed, useful for an effective\nservice placement optimization within hybrid cloud to take place. Using a real\nbenchmark, the experimental results appear to reveal that our proposed approach\nreached results that outperform those documented in the state of the art both\nin terms of cost and time.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 22:08:02 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Abbes", "Wissem", ""], ["Kechaou", "Zied", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.05978", "submitter": "Felix Laumann", "authors": "Kumar Shridhar, Felix Laumann, Marcus Liwicki", "title": "Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:55:18 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 11:35:35 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 05:37:03 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 08:16:32 GMT"}, {"version": "v5", "created": "Wed, 14 Nov 2018 13:48:37 GMT"}, {"version": "v6", "created": "Tue, 14 May 2019 09:04:11 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Shridhar", "Kumar", ""], ["Laumann", "Felix", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1806.06010", "submitter": "Thommen George Karimpanal", "authors": "Thommen George Karimpanal", "title": "A Self-Replication Basis for Designing Complex Agents", "comments": "2 pages, 1 figure", "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  Companion, Pages 45-46, Kyoto, Japan, July 15 - 19, 2018", "doi": "10.1145/3205651.3208762", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe a self-replication-based mechanism for designing\nagents of increasing complexity. We demonstrate the validity of this approach\nby solving simple, standard evolutionary computation problems in simulation. In\nthe context of these simulation results, we describe the fundamental\ndifferences of this approach when compared to traditional approaches. Further,\nwe highlight the possible advantages of applying this approach to the problem\nof designing complex artificial agents, along with the potential drawbacks and\nissues to be addressed in the future.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 03:57:41 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Karimpanal", "Thommen George", ""]]}, {"id": "1806.06062", "submitter": "Rahim Kamali", "authors": "Mina Yazdandoost, Peyman Khazaei, Salar Saadatian, Rahim Kamali", "title": "Distributed Optimization Strategy for Multi Area Economic Dispatch Based\n  on Electro Search Optimization Algorithm", "comments": "This paper is accepted for WAC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new adopted evolutionary algorithm is presented in this paper to solve the\nnon-smooth, non-convex and non-linear multi-area economic dispatch (MAED). MAED\nincludes some areas which contains its own power generation and loads. By\ntransmitting the power from the area with lower cost to the area with higher\ncost, the total cost function can be minimized greatly. The tie line capacity,\nmulti-fuel generator and the prohibited operating zones are satisfied in this\nstudy. In addition, a new algorithm based on electro search optimization\nalgorithm (ESOA) is proposed to solve the MAED optimization problem with\nconsidering all the constraints. In ESOA algorithm all probable moving states\nfor individuals to get away from or move towards the worst or best solution\nneeds to be considered. To evaluate the performance of the ESOA algorithm, the\nalgorithm is applied to both the original economic dispatch with 40 generator\nsystems and the multi-area economic dispatch with 3 different systems such as:\n6 generators in 2 areas; and 40 generators in 4 areas. It can be concluded\nthat, ESOA algorithm is more accurate and robust in comparison with other\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 06:25:17 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Yazdandoost", "Mina", ""], ["Khazaei", "Peyman", ""], ["Saadatian", "Salar", ""], ["Kamali", "Rahim", ""]]}, {"id": "1806.06296", "submitter": "Thomas Lansdall-Welfare", "authors": "Sen Jia, Thomas Lansdall-Welfare, Nello Cristianini", "title": "Right for the Right Reason: Training Agnostic Networks", "comments": "Author's original version", "journal-ref": null, "doi": "10.1007/978-3-030-01768-2_14", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of a neural network being requested to classify\nimages (or other inputs) without making implicit use of a \"protected concept\",\nthat is a concept that should not play any role in the decision of the network.\nTypically these concepts include information such as gender or race, or other\ncontextual information such as image backgrounds that might be implicitly\nreflected in unknown correlations with other variables, making it insufficient\nto simply remove them from the input features. In other words, making accurate\npredictions is not good enough if those predictions rely on information that\nshould not be used: predictive performance is not the only important metric for\nlearning systems. We apply a method developed in the context of domain\nadaptation to address this problem of \"being right for the right reason\", where\nwe request a classifier to make a decision in a way that is entirely 'agnostic'\nto a given protected concept (e.g. gender, race, background etc.), even if this\ncould be implicitly reflected in other attributes via unknown correlations.\nAfter defining the concept of an 'agnostic model', we demonstrate how the\nDomain-Adversarial Neural Network can remove unwanted information from a model\nusing a gradient reversal layer.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:09:40 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jia", "Sen", ""], ["Lansdall-Welfare", "Thomas", ""], ["Cristianini", "Nello", ""]]}, {"id": "1806.06446", "submitter": "Yi Tay", "authors": "Yi Tay, Shuai Zhang, Luu Anh Tuan, Siu Cheung Hui", "title": "Self-Attentive Neural Collaborative Filtering", "comments": "We discovered a bug in our tensorflow implementation that involved\n  accidental mixing of vectors across batches, rendering the main claim of the\n  paper incorrect. We are withdrawing this paper until we find out why", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn as we discovered a bug in our tensorflow\nimplementation that involved accidental mixing of vectors across batches. This\nlead to different inference results given different batch sizes which is\ncompletely strange. The performance scores still remain the same but we\nconcluded that it was not the self-attention that contributed to the\nperformance. We are withdrawing the paper because this renders the main claim\nof the paper false. Thanks to Guan Xinyu from NUS for discovering this issue in\nour previously open source code.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 20:58:12 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 12:04:56 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tay", "Yi", ""], ["Zhang", "Shuai", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1806.06464", "submitter": "Aditya Grover", "authors": "Aditya Grover, Maruan Al-Shedivat, Jayesh K. Gupta, Yura Burda,\n  Harrison Edwards", "title": "Learning Policy Representations in Multiagent Systems", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling agent behavior is central to understanding the emergence of complex\nphenomena in multiagent systems. Prior work in agent modeling has largely been\ntask-specific and driven by hand-engineering domain-specific prior knowledge.\nWe propose a general learning framework for modeling agent behavior in any\nmultiagent system using only a handful of interaction data. Our framework casts\nagent modeling as a representation learning problem. Consequently, we construct\na novel objective inspired by imitation learning and agent identification and\ndesign an algorithm for unsupervised learning of representations of agent\npolicies. We demonstrate empirically the utility of the proposed framework in\n(i) a challenging high-dimensional competitive environment for continuous\ncontrol and (ii) a cooperative environment for communication, on supervised\npredictive tasks, unsupervised clustering, and policy optimization using deep\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 23:29:19 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 21:36:26 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Grover", "Aditya", ""], ["Al-Shedivat", "Maruan", ""], ["Gupta", "Jayesh K.", ""], ["Burda", "Yura", ""], ["Edwards", "Harrison", ""]]}, {"id": "1806.06545", "submitter": "Nicolas Rougier", "authors": "Anthony Strock (Mnemosyne), Nicolas Rougier (Mnemosyne), Xavier Hinaut\n  (Mnemosyne)", "title": "A Simple Reservoir Model of Working Memory with Real Values", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), Jul\n  2018, Rio de Janeiro, Brazil", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prefrontal cortex is known to be involved in many high-level cognitive\nfunctions, in particular, working memory. Here, we study to what extent a group\nof randomly connected units (namely an Echo State Network, ESN) can store and\nmaintain (as output) an arbitrary real value from a streamed input, i.e. can\nact as a sustained working memory unit. Furthermore, we explore to what extent\nsuch an architecture can take advantage of the stored value in order to produce\nnon-linear computations. Comparison between different architectures (with and\nwithout feedback, with and without a working memory unit) shows that an\nexplicit memory improves the performances.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 08:22:45 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Strock", "Anthony", "", "Mnemosyne"], ["Rougier", "Nicolas", "", "Mnemosyne"], ["Hinaut", "Xavier", "", "Mnemosyne"]]}, {"id": "1806.06628", "submitter": "Hiroki Sayama", "authors": "Hiroki Sayama", "title": "Cardinality Leap for Open-Ended Evolution: Theoretical Consideration and\n  Demonstration by \"Hash Chemistry\"", "comments": "18 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DM nlin.AO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended evolution requires unbounded possibilities that evolving entities\ncan explore. The cardinality of a set of those possibilities thus has a\nsignificant implication for the open-endedness of evolution. We propose that\nfacilitating formation of higher-order entities is a generalizable, effective\nway to cause a \"cardinality leap\" in the set of possibilities that promotes\nopen-endedness. We demonstrate this idea with a simple, proof-of-concept toy\nmodel called \"Hash Chemistry\" that uses a hash function as a fitness evaluator\nof evolving entities of any size/order. Simulation results showed that the\ncumulative number of unique replicating entities that appeared in evolution\nincreased almost linearly along time without an apparent bound, demonstrating\nthe effectiveness of the proposed cardinality leap. It was also observed that\nthe number of individual entities involved in a single replication event\ngradually increased over time, indicating evolutionary appearance of\nhigher-order entities. Moreover, these behaviors were not observed in control\nexperiments in which fitness evaluators were replaced by random number\ngenerators. This strongly suggests that the dynamics observed in Hash Chemistry\nwere indeed evolutionary behaviors driven by selection and adaptation taking\nplace at multiple scales.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:38:14 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 13:47:43 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 12:40:10 GMT"}, {"version": "v4", "created": "Fri, 1 Feb 2019 18:44:57 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Sayama", "Hiroki", ""]]}, {"id": "1806.06676", "submitter": "Richard Vogl", "authors": "Richard Vogl and Gerhard Widmer and Peter Knees", "title": "Towards multi-instrument drum transcription", "comments": "Published in Proceedings of the 21th International Conference on\n  Digital Audio Effects (DAFx18), 4 - 8 September, 2018, Aveiro, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic drum transcription, a subtask of the more general automatic music\ntranscription, deals with extracting drum instrument note onsets from an audio\nsource. Recently, progress in transcription performance has been made using\nnon-negative matrix factorization as well as deep learning methods. However,\nthese works primarily focus on transcribing three drum instruments only: snare\ndrum, bass drum, and hi-hat. Yet, for many applications, the ability to\ntranscribe more drum instruments which make up standard drum kits used in\nwestern popular music would be desirable. In this work, convolutional and\nconvolutional recurrent neural networks are trained to transcribe a wider range\nof drum instruments. First, the shortcomings of publicly available datasets in\nthis context are discussed. To overcome these limitations, a larger synthetic\ndataset is introduced. Then, methods to train models using the new dataset\nfocusing on generalization to real world data are investigated. Finally, the\ntrained models are evaluated on publicly available datasets and results are\ndiscussed. The contributions of this work comprise: (i.) a large-scale\nsynthetic dataset for drum transcription, (ii.) first steps towards an\nautomatic drum transcription system that supports a larger range of instruments\nby evaluating and discussing training setups and the impact of datasets in this\ncontext, and (iii.) a publicly available set of trained models for drum\ntranscription. Additional materials are available at\nhttp://ifs.tuwien.ac.at/~vogl/dafx2018\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 13:45:48 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 11:56:38 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Vogl", "Richard", ""], ["Widmer", "Gerhard", ""], ["Knees", "Peter", ""]]}, {"id": "1806.06765", "submitter": "Jason Jo", "authors": "Jason Jo, Vikas Verma, Yoshua Bengio", "title": "Modularity Matters: Learning Invariant Relational Reasoning Tasks", "comments": "Modified abstract to fit arXiv character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on two supervised visual reasoning tasks whose labels encode a\nsemantic relational rule between two or more objects in an image: the MNIST\nParity task and the colorized Pentomino task. The objects in the images undergo\nrandom translation, scaling, rotation and coloring transformations. Thus these\ntasks involve invariant relational reasoning. We report uneven performance of\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\nreport that the VGG19 model soundly outperforms a family of ResNet models.\nMoreover, the family of ResNet models exhibits a general sensitivity to random\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\ntest generalization, hovering around 30% test error. The CNN we tested all\nlearn hierarchies of fully distributed features and thus encode the distributed\nrepresentation prior. We are motivated by a hypothesis from cognitive\nneuroscience which posits that the human visual cortex is modularized, and this\nallows the visual cortex to learn higher order invariances. To this end, we\nconsider a modularized variant of the ResNet model, referred to as a Residual\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\ninterleave distributed representations with more specialized, modular\nrepresentations. We show that very shallow ResMixNets are capable of learning\neach of the two tasks well, attaining less than 2% and 1% test error on the\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\nthe ResMixNet models are extremely parameter efficient: generalizing better\nthan various non-modular CNNs that have over 10x the number of parameters.\nThese experimental results support the hypothesis that modularity is a robust\nprior for learning invariant relational reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:19:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jo", "Jason", ""], ["Verma", "Vikas", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.06871", "submitter": "Nathan Killoran", "authors": "Nathan Killoran, Thomas R. Bromley, Juan Miguel Arrazola, Maria\n  Schuld, Nicol\\'as Quesada, Seth Lloyd", "title": "Continuous-variable quantum neural networks", "comments": null, "journal-ref": "Phys. Rev. Research 1, 033063 (2019)", "doi": "10.1103/PhysRevResearch.1.033063", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general method for building neural networks on quantum\ncomputers. The quantum neural network is a variational quantum circuit built in\nthe continuous-variable (CV) architecture, which encodes quantum information in\ncontinuous degrees of freedom such as the amplitudes of the electromagnetic\nfield. This circuit contains a layered structure of continuously parameterized\ngates which is universal for CV quantum computation. Affine transformations and\nnonlinear activation functions, two key elements in neural networks, are\nenacted in the quantum network using Gaussian and non-Gaussian gates,\nrespectively. The non-Gaussian gates provide both the nonlinearity and the\nuniversality of the model. Due to the structure of the CV model, the CV quantum\nneural network can encode highly nonlinear transformations while remaining\ncompletely unitary. We show how a classical network can be embedded into the\nquantum formalism and propose quantum versions of various specialized model\nsuch as convolutional, recurrent, and residual networks. Finally, we present\nnumerous modeling experiments built with the Strawberry Fields software\nlibrary. These experiments, including a classifier for fraud detection, a\nnetwork which generates Tetris images, and a hybrid classical-quantum\nautoencoder, demonstrate the capability and adaptability of CV quantum neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:05:19 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Killoran", "Nathan", ""], ["Bromley", "Thomas R.", ""], ["Arrazola", "Juan Miguel", ""], ["Schuld", "Maria", ""], ["Quesada", "Nicol\u00e1s", ""], ["Lloyd", "Seth", ""]]}, {"id": "1806.06926", "submitter": "Wojciech Samek", "authors": "Christopher Anders, Gr\\'egoire Montavon, Wojciech Samek, Klaus-Robert\n  M\\\"uller", "title": "Understanding Patch-Based Learning by Explaining Predictions", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are able to learn highly predictive models of video data. Due\nto video length, a common strategy is to train them on small video snippets. We\napply the deep Taylor / LRP technique to understand the deep network's\nclassification decisions, and identify a \"border effect\": a tendency of the\nclassifier to look mainly at the bordering frames of the input. This effect\nrelates to the step size used to build the video snippet, which we can then\ntune in order to improve the classifier's accuracy without retraining the\nmodel. To our knowledge, this is the the first work to apply the deep Taylor /\nLRP technique on any video analyzing neural network.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:44:45 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Anders", "Christopher", ""], ["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1806.06928", "submitter": "Risto Vuorio", "authors": "Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon Kim", "title": "Meta Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using neural networks in practical settings would benefit from the ability of\nthe networks to learn new tasks throughout their lifetimes without forgetting\nthe previous tasks. This ability is limited in the current deep neural networks\nby a problem called catastrophic forgetting, where training on new tasks tends\nto severely degrade performance on previous tasks. One way to lessen the impact\nof the forgetting problem is to constrain parameters that are important to\nprevious tasks to stay close to the optimal parameters. Recently, multiple\ncompetitive approaches for computing the importance of the parameters with\nrespect to the previous tasks have been presented. In this paper, we propose a\nlearning to optimize algorithm for mitigating catastrophic forgetting. Instead\nof trying to formulate a new constraint function ourselves, we propose to train\nanother neural network to predict parameter update steps that respect the\nimportance of parameters to the previous tasks. In the proposed meta-training\nscheme, the update predictor is trained to minimize loss on a combination of\ncurrent and past tasks. We show experimentally that the proposed approach works\nin the continual learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:49:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Vuorio", "Risto", ""], ["Cho", "Dong-Yeon", ""], ["Kim", "Daejoong", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.07073", "submitter": "Andreas Bartschat", "authors": "Nadezhda Prodanova, Johannes Stegmaier, Stephan Allgeier, Sebastian\n  Bohn, Oliver Stachs, Bernd K\\\"ohler, Ralf Mikut, Andreas Bartschat", "title": "Transfer Learning with Human Corneal Tissues: An Analysis of Optimal\n  Cut-Off Layer", "comments": "Extendend Abstract with 3 pages and 2 figures. Submitted to MIDL\n  Amsterdam, see openreviews.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning is a powerful tool to adapt trained neural networks to new\ntasks. Depending on the similarity of the original task to the new task, the\nselection of the cut-off layer is critical. For medical applications like\ntissue classification, the last layers of an object classification network\nmight not be optimal. We found that on real data of human corneal tissues the\nbest feature representation can be found in the middle layers of the\nInception-v3 and in the rear layers of the VGG-19 architecture.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:14:58 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 13:01:22 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Prodanova", "Nadezhda", ""], ["Stegmaier", "Johannes", ""], ["Allgeier", "Stephan", ""], ["Bohn", "Sebastian", ""], ["Stachs", "Oliver", ""], ["K\u00f6hler", "Bernd", ""], ["Mikut", "Ralf", ""], ["Bartschat", "Andreas", ""]]}, {"id": "1806.07336", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler", "title": "Neural Code Comprehension: A Learnable Representation of Code Semantics", "comments": "Published at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of embeddings in natural language processing,\nresearch has been conducted into applying similar methods to code analysis.\nMost works attempt to process the code directly or use a syntactic tree\nrepresentation, treating it like sentences written in a natural language.\nHowever, none of the existing methods are sufficient to comprehend program\nsemantics robustly, due to structural features such as function calls,\nbranching, and interchangeable order of statements. In this paper, we propose a\nnovel processing technique to learn code semantics, and apply it to a variety\nof program analysis tasks. In particular, we stipulate that a robust\ndistributional hypothesis of code applies to both human- and machine-generated\nprograms. Following this hypothesis, we define an embedding space, inst2vec,\nbased on an Intermediate Representation (IR) of the code that is independent of\nthe source programming language. We provide a novel definition of contextual\nflow for this IR, leveraging both the underlying data- and control-flow of the\nprogram. We then analyze the embeddings qualitatively using analogies and\nclustering, and evaluate the learned representation on three different\nhigh-level tasks. We show that even without fine-tuning, a single RNN\narchitecture and fixed inst2vec embeddings outperform specialized approaches\nfor performance prediction (compute device mapping, optimal thread coarsening);\nand algorithm classification from raw code (104 classes), where we set a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:43:44 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 16:10:52 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 08:15:00 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Jakobovits", "Alice Shoshana", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1806.07550", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Xin Dong, Hao Su", "title": "Binary Ensemble Neural Network: More Bits per Network or More Networks\n  per Bit?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNN) have been studied extensively since they run\ndramatically faster at lower memory and power consumption than floating-point\nnetworks, thanks to the efficiency of bit operations. However, contemporary\nBNNs whose weights and activations are both single bits suffer from severe\naccuracy degradation. To understand why, we investigate the representation\nability, speed and bias/variance of BNNs through extensive experiments. We\nconclude that the error of BNNs is predominantly caused by the intrinsic\ninstability (training time) and non-robustness (train & test time). Inspired by\nthis investigation, we propose the Binary Ensemble Neural Network (BENN) which\nleverages ensemble methods to improve the performance of BNNs with limited\nefficiency cost. While ensemble techniques have been broadly believed to be\nonly marginally helpful for strong classifiers such as deep neural networks,\nour analyses and experiments show that they are naturally a perfect fit to\nboost BNNs. We find that our BENN, which is faster and much more robust than\nstate-of-the-art binary networks, can even surpass the accuracy of the\nfull-precision floating number network with the same architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 04:48:18 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 07:08:37 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhu", "Shilin", ""], ["Dong", "Xin", ""], ["Su", "Hao", ""]]}, {"id": "1806.07572", "submitter": "Arthur Jacot", "authors": "Arthur Jacot, Franck Gabriel, Cl\\'ement Hongler", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "comments": null, "journal-ref": "In Advances in neural information processing systems (pp.\n  8571-8580) 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At initialization, artificial neural networks (ANNs) are equivalent to\nGaussian processes in the infinite-width limit, thus connecting them to kernel\nmethods. We prove that the evolution of an ANN during training can also be\ndescribed by a kernel: during gradient descent on the parameters of an ANN, the\nnetwork function $f_\\theta$ (which maps input vectors to output vectors)\nfollows the kernel gradient of the functional cost (which is convex, in\ncontrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel\n(NTK). This kernel is central to describe the generalization features of ANNs.\nWhile the NTK is random at initialization and varies during training, in the\ninfinite-width limit it converges to an explicit limiting kernel and it stays\nconstant during training. This makes it possible to study the training of ANNs\nin function space instead of parameter space. Convergence of the training can\nthen be related to the positive-definiteness of the limiting NTK. We prove the\npositive-definiteness of the limiting NTK when the data is supported on the\nsphere and the non-linearity is non-polynomial. We then focus on the setting of\nleast-squares regression and show that in the infinite-width limit, the network\nfunction $f_\\theta$ follows a linear differential equation during training. The\nconvergence is fastest along the largest kernel principal components of the\ninput data with respect to the NTK, hence suggesting a theoretical motivation\nfor early stopping. Finally we study the NTK numerically, observe its behavior\nfor wide networks, and compare it to the infinite-width limit.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:35:46 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 10:31:42 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 15:42:05 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 08:39:09 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jacot", "Arthur", ""], ["Gabriel", "Franck", ""], ["Hongler", "Cl\u00e9ment", ""]]}, {"id": "1806.07741", "submitter": "Felix Alexander Heilmeyer", "authors": "Felix A. Heilmeyer, Robin T. Schirrmeister, Lukas D. J. Fiederer,\n  Martin V\\\"olker, Joos Behncke, Tonio Ball", "title": "A large-scale evaluation framework for EEG deep learning architectures", "comments": "7 pages, 3 figures, final version accepted for presentation at IEEE\n  SMC 2018 conference", "journal-ref": null, "doi": "10.1109/SMC.2018.00185", "report-no": null, "categories": "eess.SP cs.LG cs.NE q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  EEG is the most common signal source for noninvasive BCI applications. For\nsuch applications, the EEG signal needs to be decoded and translated into\nappropriate actions. A recently emerging EEG decoding approach is deep learning\nwith Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many\ndifferent architectures already published. Here we present a novel framework\nfor the large-scale evaluation of different deep-learning architectures on\ndifferent EEG datasets. This framework comprises (i) a collection of EEG\ndatasets currently including 100 examples (recording sessions) from six\ndifferent classification problems, (ii) a collection of different EEG decoding\nalgorithms, and (iii) a wrapper linking the decoders to the data as well as\nhandling structured documentation of all settings and (hyper-) parameters and\nstatistics, designed to ensure transparency and reproducibility. As an\napplications example we used our framework by comparing three publicly\navailable CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow\nConvNet, and two versions of EEGNet. We also show how our framework can be used\nto study similarities and differences in the performance of different decoding\nmethods across tasks. We argue that the deep learning EEG framework as\ndescribed here could help to tap the full potential of deep learning for BCI\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:49:23 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:25:46 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Heilmeyer", "Felix A.", ""], ["Schirrmeister", "Robin T.", ""], ["Fiederer", "Lukas D. J.", ""], ["V\u00f6lker", "Martin", ""], ["Behncke", "Joos", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.07850", "submitter": "Corrado Possieri", "authors": "Giuseppe C. Calafiore, Stephane Gaubert, Corrado Possieri", "title": "Log-sum-exp neural networks and posynomial models for convex and\n  log-log-convex data", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, vol.\n  31, no. 3, pp. 827-838, Mar. 2020", "doi": "10.1109/TNNLS.2019.2910417", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show in this paper that a one-layer feedforward neural network with\nexponential activation functions in the inner layer and logarithmic activation\nin the output neuron is an universal approximator of convex functions. Such a\nnetwork represents a family of scaled log-sum exponential functions, here named\nLSET. Under a suitable exponential transformation, the class of LSET functions\nmaps to a family of generalized posynomials GPOST, which we similarly show to\nbe universal approximators for log-log-convex functions. A key feature of an\nLSET network is that, once it is trained on data, the resulting model is convex\nin the variables, which makes it readily amenable to efficient design based on\nconvex optimization. Similarly, once a GPOST model is trained on data, it\nyields a posynomial model that can be efficiently optimized with respect to its\nvariables by using geometric programming (GP). The proposed methodology is\nillustrated by two numerical examples, in which, first, models are constructed\nfrom simulation data of the two physical processes (namely, the level of\nvibration in a vehicle suspension system, and the peak power generated by the\ncombustion of propane), and then optimization-based design is performed on\nthese models.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:21:39 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 11:33:08 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Calafiore", "Giuseppe C.", ""], ["Gaubert", "Stephane", ""], ["Possieri", "Corrado", ""]]}, {"id": "1806.07912", "submitter": "Yanqi Zhou", "authors": "Yanqi Zhou, Siavash Ebrahimi, Sercan \\\"O. Ar{\\i}k, Haonan Yu, Hairong\n  Liu, Greg Diamos", "title": "Resource-Efficient Neural Architect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is a laborious process. Prior work on\nautomated NAS targets mainly on improving accuracy, but lacks consideration of\ncomputational resource use. We propose the Resource-Efficient Neural Architect\n(RENA), an efficient resource-constrained NAS using reinforcement learning with\nnetwork embedding. RENA uses a policy network to process the network embeddings\nto generate new configurations. We demonstrate RENA on image recognition and\nkeyword spotting (KWS) problems. RENA can find novel architectures that achieve\nhigh performance even with tight resource constraints. For CIFAR10, it achieves\n2.95% test error when compute intensity is greater than 100 FLOPs/byte, and\n3.87% test error when model size is less than 3M parameters. For Google Speech\nCommands Dataset, RENA achieves the state-of-the-art accuracy without resource\nconstraints, and it outperforms the optimized architectures with tight resource\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:41:32 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Zhou", "Yanqi", ""], ["Ebrahimi", "Siavash", ""], ["Ar\u0131k", "Sercan \u00d6.", ""], ["Yu", "Haonan", ""], ["Liu", "Hairong", ""], ["Diamos", "Greg", ""]]}, {"id": "1806.07915", "submitter": "Rahim Kamali", "authors": "Ali Yazdandoost, Peyman Khazaei, Rahim Kamali, Salar Saadatian", "title": "An Efficient Scheduling for Security Constraint Unit Commitment Problem\n  Via Modified Genetic Algorithm Based on Multicellular Organisms Mechanisms", "comments": "This paper is accepted in WAC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security Constraint Unit commitment (SCUC) is one of the significant\nchallenges in operation of power grids which tries to regulate the status of\nthe generation units (ON or OFF) and providing an efficient power dispatch\nwithin the grid. While many researches tried to address the SCUC challenges, it\nis a mixed-integer optimization problem that is difficult to reach global\noptimum. In this study, a novel modified genetic algorithm based on\nMulticellular Organisms Mechanisms (GAMOM) is developed to find an optimal\nsolution for SCUC problem. The presentation of the GAMOM on the SCUC contain\ntwo sections, the GA and modified GAMOM sections. Hence, a set of population is\nconsidered for the SCUC problem. Next, an iterative process is used to obtain\nthe greatest SCUC population. Indeed, the best population is selected so that\nthe total operating cost is minimized and also all system and units constraints\nare satisfied. The effectiveness of the proposed GAMOM algorithm is determined\nby the simulation studies which demonstrate the convergence speed. Finally, the\nproposed technique is compared with well-known existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 06:29:55 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Yazdandoost", "Ali", ""], ["Khazaei", "Peyman", ""], ["Kamali", "Rahim", ""], ["Saadatian", "Salar", ""]]}, {"id": "1806.07917", "submitter": "Jakub Sygnowski", "authors": "Chrisantha Thomas Fernando, Jakub Sygnowski, Simon Osindero, Jane\n  Wang, Tom Schaul, Denis Teplyashin, Pablo Sprechmann, Alexander Pritzel,\n  Andrei A. Rusu", "title": "Meta-Learning by the Baldwin Effect", "comments": null, "journal-ref": null, "doi": "10.1145/3205651.3205763", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of the Baldwin effect was recently called into question by two\npapers that closely examined the seminal work of Hinton and Nowlan. To this\ndate there has been no demonstration of its necessity in empirically\nchallenging tasks. Here we show that the Baldwin effect is capable of evolving\nfew-shot supervised and reinforcement learning mechanisms, by shaping the\nhyperparameters and the initial parameters of deep learning algorithms.\nFurthermore it can genetically accommodate strong learning biases on the same\nset of problems as a recent machine learning algorithm called MAML \"Model\nAgnostic Meta-Learning\" which uses second-order gradients instead of evolution\nto learn a set of reference parameters (initial weights) that can allow rapid\nadaptation to tasks sampled from a distribution. Whilst in simple cases MAML is\nmore data efficient than the Baldwin effect, the Baldwin effect is more general\nin that it does not require gradients to be backpropagated to the reference\nparameters or hyperparameters, and permits effectively any number of gradient\nupdates in the inner loop. The Baldwin effect learns strong learning dependent\nbiases, rather than purely genetically accommodating fixed behaviours in a\nlearning independent manner.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:39:03 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 09:55:17 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Fernando", "Chrisantha Thomas", ""], ["Sygnowski", "Jakub", ""], ["Osindero", "Simon", ""], ["Wang", "Jane", ""], ["Schaul", "Tom", ""], ["Teplyashin", "Denis", ""], ["Sprechmann", "Pablo", ""], ["Pritzel", "Alexander", ""], ["Rusu", "Andrei A.", ""]]}, {"id": "1806.07941", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Conditions for Major Transitions in Biological and Cultural Evolution", "comments": "To be presented at the Third Workshop on Open-Ended Evolution (OEE3),\n  Tokyo, Japan, July 2018 (hosted by the 2018 Conference on Artificial Life)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution by natural selection can be seen an algorithm for generating\ncreative solutions to difficult problems. More precisely, evolution by natural\nselection is a class of algorithms that share a set of properties. The question\nwe address here is, what are the conditions that define this class of\nalgorithms? There is a standard answer to this question: Briefly, the\nconditions are variation, heredity, and selection. We agree that these three\nconditions are sufficient for a limited type of evolution, but they are not\nsufficient for open-ended evolution. By open-ended evolution, we mean evolution\nthat generates a continuous stream of creative solutions, without stagnating.\nWe propose a set of conditions for open-ended evolution. The new conditions\nbuild on the standard conditions by adding fission, fusion, and cooperation. We\ntest the proposed conditions by applying them to major transitions in the\nevolution of life and culture. We find that the proposed conditions are able to\naccount for the major transitions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 19:41:55 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1806.08047", "submitter": "Damian Mrowca", "authors": "Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li Fei-Fei,\n  Joshua B. Tenenbaum, Daniel L. K. Yamins", "title": "Flexible Neural Representation for Physics Prediction", "comments": "23 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have a remarkable capacity to understand the physical dynamics of\nobjects in their environment, flexibly capturing complex structures and\ninteractions at multiple levels of detail. Inspired by this ability, we propose\na hierarchical particle-based object representation that covers a wide variety\nof types of three-dimensional objects, including both arbitrary rigid\ngeometrical shapes and deformable materials. We then describe the Hierarchical\nRelation Network (HRN), an end-to-end differentiable neural network based on\nhierarchical graph convolution, that learns to predict physical dynamics in\nthis representation. Compared to other neural network baselines, the HRN\naccurately handles complex collisions and nonrigid deformations, generating\nplausible dynamics predictions at long time scales in novel settings, and\nscaling to large scene configurations. These results demonstrate an\narchitecture with the potential to form the basis of next-generation physics\npredictors for use in computer vision, robotics, and quantitative cognitive\nscience.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 02:19:50 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 05:28:48 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Mrowca", "Damian", ""], ["Zhuang", "Chengxu", ""], ["Wang", "Elias", ""], ["Haber", "Nick", ""], ["Fei-Fei", "Li", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1806.08085", "submitter": "Thomas Preu{\\ss}er", "authors": "Thomas B. Preu{\\ss}er, Giulio Gambardella, Nicholas Fraser, Michaela\n  Blott", "title": "Inference of Quantized Neural Networks on Heterogeneous All-Programmable\n  Devices", "comments": null, "journal-ref": null, "doi": "10.23919/DATE.2018.8342121", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have established as a generic and powerful means to approach\nchallenging problems such as image classification, object detection or decision\nmaking. Their successful employment foots on an enormous demand of compute. The\nquantization of network parameters and the processed data has proven a valuable\nmeasure to reduce the challenges of network inference so effectively that the\nfeasible scope of applications is expanded even into the embedded domain. This\npaper describes the making of a real-time object detection in a live video\nstream processed on an embedded all-programmable device. The presented case\nillustrates how the required processing is tamed and parallelized across both\nthe CPU cores and the programmable logic and how the most suitable resources\nand powerful extensions, such as NEON vectorization, are leveraged for the\nindividual processing steps. The crafted result is an extended Darknet\nframework implementing a fully integrated, end-to-end solution from video\ncapture over object annotation to video output applying neural network\ninference at different quantization levels running at 16~frames per second on\nan embedded Zynq UltraScale+ (XCZU3EG) platform.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 07:03:36 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Preu\u00dfer", "Thomas B.", ""], ["Gambardella", "Giulio", ""], ["Fraser", "Nicholas", ""], ["Blott", "Michaela", ""]]}, {"id": "1806.08099", "submitter": "Jonas Prellberg", "authors": "Jonas Prellberg, Oliver Kramer", "title": "Lamarckian Evolution of Convolutional Neural Networks", "comments": "Accepted at PPSN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks belong to the most successul image classifiers,\nbut the adaptation of their network architecture to a particular problem is\ncomputationally expensive. We show that an evolutionary algorithm saves\ntraining time during the network architecture optimization, if learned network\nweights are inherited over generations by Lamarckian evolution. Experiments on\ntypical image datasets show similar or significantly better test accuracies and\nimproved convergence speeds compared to two different baselines without weight\ninheritance. On CIFAR-10 and CIFAR-100 a 75 % improvement in data efficiency is\nobserved.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 07:55:22 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 16:44:25 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Prellberg", "Jonas", ""], ["Kramer", "Oliver", ""]]}, {"id": "1806.08216", "submitter": "Ard de Gelder", "authors": "Ard de Gelder, Henkjan Huisman", "title": "Autoencoders for Multi-Label Prostate MR Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organ image segmentation can be improved by implementing prior knowledge\nabout the anatomy. One way of doing this is by training an autoencoder to learn\na lowdimensional representation of the segmentation. In this paper, this is\napplied in multi-label prostate MR segmentation, with some positive results.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 13:10:46 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 11:16:36 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["de Gelder", "Ard", ""], ["Huisman", "Henkjan", ""]]}, {"id": "1806.08547", "submitter": "Vahid Roostapour", "authors": "Vahid Roostapour, Mojgan Pourhassan, Frank Neumann", "title": "Analysis of Evolutionary Algorithms in Dynamic and Stochastic\n  Environments", "comments": "This book chapter is to appear in the book \"Theory of Randomized\n  Search Heuristics in Discrete Search Spaces\", which is edited by Benjamin\n  Doerr and Frank Neumann and is scheduled to be published by Springer in 2018", "journal-ref": null, "doi": "10.1007/978-3-030-29414-4_7", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world optimization problems occur in environments that change\ndynamically or involve stochastic components. Evolutionary algorithms and other\nbio-inspired algorithms have been widely applied to dynamic and stochastic\nproblems. This survey gives an overview of major theoretical developments in\nthe area of runtime analysis for these problems. We review recent theoretical\nstudies of evolutionary algorithms and ant colony optimization for problems\nwhere the objective functions or the constraints change over time. Furthermore,\nwe consider stochastic problems under various noise models and point out some\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 08:34:36 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Roostapour", "Vahid", ""], ["Pourhassan", "Mojgan", ""], ["Neumann", "Frank", ""]]}, {"id": "1806.08568", "submitter": "Vincenzo Lomonaco", "authors": "Davide Maltoni and Vincenzo Lomonaco", "title": "Continuous Learning in Single-Incremental-Task Scenarios", "comments": "26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4),\n  several typos and minor mistakes corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that architectural, regularization and rehearsal\nstrategies can be used to train deep models sequentially on a number of\ndisjoint tasks without forgetting previously acquired knowledge. However, these\nstrategies are still unsatisfactory if the tasks are not disjoint but\nconstitute a single incremental task (e.g., class-incremental learning). In\nthis paper we point out the differences between multi-task and\nsingle-incremental-task scenarios and show that well-known approaches such as\nLWF, EWC and SI are not ideal for incremental task scenarios. A new approach,\ndenoted as AR1, combining architectural and regularization strategies is then\nspecifically proposed. AR1 overhead (in term of memory and computation) is very\nsmall thus making it suitable for online learning. When tested on CORe50 and\niCIFAR-100, AR1 outperformed existing regularization strategies by a good\nmargin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:22:42 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 11:13:40 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 21:49:25 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Maltoni", "Davide", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "1806.08760", "submitter": "Khuong Vo", "authors": "Khuong Vo, Dang Pham, Mao Nguyen, Trung Mai and Tho Quan", "title": "Combination of Domain Knowledge and Deep Learning for Sentiment Analysis", "comments": "Accepted to MIWAI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-69456-6_14", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emerging technique of deep learning has been widely applied in many\ndifferent areas. However, when adopted in a certain specific domain, this\ntechnique should be combined with domain knowledge to improve efficiency and\naccuracy. In particular, when analyzing the applications of deep learning in\nsentiment analysis, we found that the current approaches are suffering from the\nfollowing drawbacks: (i) the existing works have not paid much attention to the\nimportance of different types of sentiment terms, which is an important concept\nin this area; and (ii) the loss function currently employed does not well\nreflect the degree of error of sentiment misclassification. To overcome such\nproblem, we propose to combine domain knowledge with deep learning. Our\nproposal includes using sentiment scores, learnt by quadratic programming, to\naugment training data; and introducing the penalty matrix for enhancing the\nloss function of cross entropy. When experimented, we achieved a significant\nimprovement in classification results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 16:39:37 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 05:47:31 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 00:58:24 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Vo", "Khuong", ""], ["Pham", "Dang", ""], ["Nguyen", "Mao", ""], ["Mai", "Trung", ""], ["Quan", "Tho", ""]]}, {"id": "1806.08804", "submitter": "Rex Ying", "authors": "Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L.\n  Hamilton, Jure Leskovec", "title": "Hierarchical Graph Representation Learning with Differentiable Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph neural networks (GNNs) have revolutionized the field of graph\nrepresentation learning through effectively learned node embeddings, and\nachieved state-of-the-art results in tasks such as node classification and link\nprediction. However, current GNN methods are inherently flat and do not learn\nhierarchical representations of graphs---a limitation that is especially\nproblematic for the task of graph classification, where the goal is to predict\nthe label associated with an entire graph. Here we propose DiffPool, a\ndifferentiable graph pooling module that can generate hierarchical\nrepresentations of graphs and can be combined with various graph neural network\narchitectures in an end-to-end fashion. DiffPool learns a differentiable soft\ncluster assignment for nodes at each layer of a deep GNN, mapping nodes to a\nset of clusters, which then form the coarsened input for the next GNN layer.\nOur experimental results show that combining existing GNN methods with DiffPool\nyields an average improvement of 5-10% accuracy on graph classification\nbenchmarks, compared to all existing pooling approaches, achieving a new\nstate-of-the-art on four out of five benchmark data sets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:04:46 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 21:34:40 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 00:20:42 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 08:54:36 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ying", "Rex", ""], ["You", "Jiaxuan", ""], ["Morris", "Christopher", ""], ["Ren", "Xiang", ""], ["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.08984", "submitter": "Markus Wagner", "authors": "Thomas Weise, Zijun Wu, Markus Wagner", "title": "An Improved Generic Bet-and-Run Strategy for Speeding Up Stochastic\n  Local Search", "comments": "results publicly available: https://zenodo.org/record/1253770", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly used strategy for improving optimization algorithms is to restart\nthe algorithm when it is believed to be trapped in an inferior part of the\nsearch space. Building on the recent success of Bet-and-Run approaches for\nrestarted local search solvers, we introduce an improved generic Bet-and-Run\nstrategy. The goal is to obtain the best possible results within a given time\nbudget t using a given black-box optimization algorithm. If no prior knowledge\nabout problem features and algorithm behavior is available, the question about\nhow to use the time budget most efficiently arises. We propose to first start\nk>=1 independent runs of the algorithm during an initialization budget t1<t,\npausing these runs, then apply a decision maker D to choose 1<=m<=k runs from\nthem (consuming t2>=0 time units in doing so), and then continuing these runs\nfor the remaining t3=t-t1-t2 time units. In previous Bet-and-Run strategies,\nthe decision maker D=currentBest would simply select the run with the best-\nso-far results at negligible time. We propose using more advanced methods to\ndiscriminate between \"good\" and \"bad\" sample runs, with the goal of increasing\nthe correlation of the chosen run with the a-posteriori best one. We test\nseveral different approaches, including neural networks trained or polynomials\nfitted on the current trace of the algorithm to predict which run may yield the\nbest results if granted the remaining budget. We show with extensive\nexperiments that this approach can yield better results than the previous\nmethods, but also find that the currentBest method is a very reliable and\nrobust baseline approach.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 15:36:04 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Weise", "Thomas", ""], ["Wu", "Zijun", ""], ["Wagner", "Markus", ""]]}, {"id": "1806.09057", "submitter": "Ankit Mondal", "authors": "Ankit Mondal and Ankur Srivastava", "title": "In-situ Stochastic Training of MTJ Crossbar based Neural Networks", "comments": "Accepted for poster presentation in the 2018 ACM/IEEE International\n  Symposium on Low Power Electronics and Design (ISLPED)", "journal-ref": null, "doi": "10.1145/3218603.3218616", "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to high device density, scalability and non-volatility, Magnetic Tunnel\nJunction-based crossbars have garnered significant interest for implementing\nthe weights of an artificial neural network. The existence of only two stable\nstates in MTJs implies a high overhead of obtaining optimal binary weights in\nsoftware. We illustrate that the inherent parallelism in the crossbar structure\nmakes it highly appropriate for in-situ training, wherein the network is taught\ndirectly on the hardware. It leads to significantly smaller training overhead\nas the training time is independent of the size of the network, while also\ncircumventing the effects of alternate current paths in the crossbar and\naccounting for manufacturing variations in the device. We show how the\nstochastic switching characteristics of MTJs can be leveraged to perform\nprobabilistic weight updates using the gradient descent algorithm. We describe\nhow the update operations can be performed on crossbars both with and without\naccess transistors and perform simulations on them to demonstrate the\neffectiveness of our techniques. The results reveal that stochastically trained\nMTJ-crossbar NNs achieve a classification accuracy nearly same as that of\nreal-valued-weight networks trained in software and exhibit immunity to device\nvariations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 00:28:12 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Mondal", "Ankit", ""], ["Srivastava", "Ankur", ""]]}, {"id": "1806.09174", "submitter": "Noshaba Cheema", "authors": "Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han\n  Du, Klaus Fischer, Philipp Slusallek", "title": "Dilated Temporal Fully-Convolutional Network for Semantic Segmentation\n  of Motion Capture Data", "comments": "Eurographics/ ACM SIGGRAPH Symposium on Computer Animation - Posters\n  2018;\n  $\\href{http://people.mpi-inf.mpg.de/~ncheema/SCA2018_poster.pdf}{\\textit{Poster\n  can be found here.}}$", "journal-ref": null, "doi": "10.2312/sca.20181185", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of motion capture sequences plays a key part in many\ndata-driven motion synthesis frameworks. It is a preprocessing step in which\nlong recordings of motion capture sequences are partitioned into smaller\nsegments. Afterwards, additional methods like statistical modeling can be\napplied to each group of structurally-similar segments to learn an abstract\nmotion manifold. The segmentation task however often remains a manual task,\nwhich increases the effort and cost of generating large-scale motion databases.\nWe therefore propose an automatic framework for semantic segmentation of motion\ncapture data using a dilated temporal fully-convolutional network. Our model\noutperforms a state-of-the-art model in action segmentation, as well as three\nnetworks for sequence modeling. We further show our model is robust against\nhigh noisy training labels.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 16:40:07 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Cheema", "Noshaba", ""], ["Hosseini", "Somayeh", ""], ["Sprenger", "Janis", ""], ["Herrmann", "Erik", ""], ["Du", "Han", ""], ["Fischer", "Klaus", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1806.09351", "submitter": "Rituraj Kaushik", "authors": "Rituraj Kaushik, Konstantinos Chatzilygeroudis, Jean-Baptiste Mouret", "title": "Multi-objective Model-based Policy Search for Data-efficient Learning\n  with Sparse Rewards", "comments": "Conference on Robot Learning (CoRL)- 2018; Code at\n  https://github.com/resibots/kaushik_2018_multi-dex ; Video at\n  https://youtu.be/9ZLwUxAAq6M", "journal-ref": "Proceedings of the Conference on Robot Learning, PMLR 87:839-855,\n  2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most data-efficient algorithms for reinforcement learning in robotics are\nmodel-based policy search algorithms, which alternate between learning a\ndynamical model of the robot and optimizing a policy to maximize the expected\nreturn given the model and its uncertainties. However, the current algorithms\nlack an effective exploration strategy to deal with sparse or misleading reward\nscenarios: if they do not experience any state with a positive reward during\nthe initial random exploration, it is very unlikely to solve the problem. Here,\nwe propose a novel model-based policy search algorithm, Multi-DEX, that\nleverages a learned dynamical model to efficiently explore the task space and\nsolve tasks with sparse rewards in a few episodes. To achieve this, we frame\nthe policy search problem as a multi-objective, model-based policy optimization\nproblem with three objectives: (1) generate maximally novel state trajectories,\n(2) maximize the expected return and (3) keep the system in state-space regions\nfor which the model is as accurate as possible. We then optimize these\nobjectives using a Pareto-based multi-objective optimization algorithm. The\nexperiments show that Multi-DEX is able to solve sparse reward scenarios (with\na simulated robotic arm) in much lower interaction time than VIME, TRPO,\nGEP-PG, CMA-ES and Black-DROPS.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:46:47 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 10:20:33 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 22:57:46 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kaushik", "Rituraj", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1806.09432", "submitter": "Sean Walton", "authors": "Sean P. Walton, M. Rowan Brown", "title": "Predicting Effective Control Parameters for Differential Evolution using\n  Cluster Analysis of Objective Function Features", "comments": "Cite this article as: Walton, S.P. & Brown, M.R. J Heuristics (2019).\n  https://doi.org/10.1007/s10732-019-09419-8", "journal-ref": null, "doi": "10.1007/s10732-019-09419-8", "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A methodology is introduced which uses three simple objective function\nfeatures to predict effective control parameters for differential evolution.\nThis is achieved using cluster analysis techniques to classify objective\nfunctions using these features. Information on prior performance of various\ncontrol parameters for each classification is then used to determine which\ncontrol parameters to use in future optimisations. Our approach is compared to\nstate-of-the-art adaptive and non-adaptive techniques. Two accepted bench mark\nsuites are used to compare performance and in all cases we show that the\nimprovement resulting from our approach is statistically significant. The\nmajority of the computational effort of this methodology is performed off-line,\nhowever even when taking into account the additional on-line cost our approach\noutperforms other adaptive techniques. We also investigate the key tuning\nparameters of our methodology, such as number of clusters, which further\nsupport the finding that the simple features selected are predictors of\neffective control parameters. The findings presented in this paper are\nsignificant because they show that simple to calculate features of objective\nfunctions can help to select control parameters for optimisation algorithms.\nThis can have an immediate positive impact on the application of these\noptimisation algorithms on real world problems, where it is often difficult to\nselect effective control parameters.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:08:45 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 07:47:25 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Walton", "Sean P.", ""], ["Brown", "M. Rowan", ""]]}, {"id": "1806.09617", "submitter": "Stephen Sinclair", "authors": "Stephen Sinclair", "title": "Sounderfeit: Cloning a Physical Model using a Conditional Adversarial\n  Autoencoder", "comments": "Extended conference paper published as article in Brazilian\n  open-access journal Musica Hodie. 17 pages, 10 figures. ISSN 1676-3939.\n  Dispon\\'ivel em: https://www.revistas.ufg.br/musica/article/view/53570. arXiv\n  admin note: substantial text overlap with arXiv:1802.08008", "journal-ref": "Revista M\\'usica Hodie, [S.l.], v. 18, n. 1, p. 44 - 60, jun. 2018", "doi": "10.5216/mh.v18i1.53570", "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An adversarial autoencoder conditioned on known parameters of a physical\nmodeling bowed string synthesizer is evaluated for use in parameter estimation\nand resynthesis tasks. Latent dimensions are provided to capture variance not\nexplained by the conditional parameters. Results are compared with and without\nthe adversarial training, and a system capable of \"copying\" a given\nparameter-signal bidirectional relationship is examined. A real-time synthesis\nsystem built on a generative, conditioned and regularized neural network is\npresented, allowing to construct engaging sound synthesizers based purely on\nrecorded data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:21:37 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Sinclair", "Stephen", ""]]}, {"id": "1806.09664", "submitter": "Oleg Pavlovsky", "authors": "V.I. Dorozhinsky, O.V. Pavlovsky", "title": "Artificial Quantum Neural Network: quantum neurons, logical elements and\n  tests of convolutional nets", "comments": "20 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model of an artificial neural network that uses\nquantum-mechanical particles in a two-humped potential as a neuron. To simulate\nsuch a quantum-mechanical system the Monte-Carlo integration method is used. A\nform of the self-potential of a particle and two potentials (exciting and\ninhibiting) interaction are proposed. The possibility of implementing the\nsimplest logical elements, (such as AND, OR and NOT) based on introduced\nquantum particles is shown. Further we show implementation of a simplest\nconvolutional network. Finally we construct a network that recognizes\nhandwritten symbols, which shows that in the case of simple architectures, it\nis possible to transfer weights from a classical network to a quantum one.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 18:50:12 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Dorozhinsky", "V. I.", ""], ["Pavlovsky", "O. V.", ""]]}, {"id": "1806.09731", "submitter": "Jo\\~ao Correia", "authors": "Tiago Martins, Jo\\~ao Correia, Ernesto Costa, Penousal Machado", "title": "Evotype: Towards the Evolution of Type Stencils", "comments": "EvoMUSART 2018 Best paper", "journal-ref": null, "doi": "10.1007/978-3-319-77583-8_20", "report-no": null, "categories": "cs.NE cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typefaces are an essential resource employed by graphic designers. The\nincreasing demand for innovative type design work increases the need for good\ntechnological means to assist the designer in the creation of a typeface. We\npresent an evolutionary computation approach for the generation of type\nstencils to draw coherent glyphs for different characters. The proposed system\nemploys a Genetic Algorithm to evolve populations of type stencils. The\nevaluation of each candidate stencil uses a hill climbing algorithm to search\nthe best configurations to draw the target glyphs. We study the interplay\nbetween legibility, coherence and expressiveness, and show how our framework\ncan be used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:03:23 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Martins", "Tiago", ""], ["Correia", "Jo\u00e3o", ""], ["Costa", "Ernesto", ""], ["Machado", "Penousal", ""]]}, {"id": "1806.09789", "submitter": "Tushar Semwal", "authors": "Tushar Semwal, Divya D Kulkarni and Shivashankar B. Nair", "title": "On an Immuno-inspired Distributed, Embodied Action-Evolution cum\n  Selection Algorithm", "comments": "8 pages, 5 figures, GECCO 18, Kyoto, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Evolutionary Robotics (ER) employs evolutionary techniques to\nsearch for a single monolithic controller which can aid a robot to learn a\ndesired task. These techniques suffer from bootstrap and deception issues when\nthe tasks are complex for a single controller to learn. Behaviour-decomposition\ntechniques have been used to divide a task into multiple subtasks and evolve\nseparate subcontrollers for each subtask. However, these subcontrollers and the\nassociated subcontroller arbitrator(s) are all evolved off-line. A distributed,\nfully embodied and evolutionary version of such approaches will greatly aid\nonline learning and help reduce the reality gap. In this paper, we propose an\nimmunology-inspired embodied action-evolution cum selection algorithm that can\ncater to distributed ER. This algorithm evolves different subcontrollers for\ndifferent portions of the search space in a distributed manner just as\nantibodies are evolved and primed for different antigens in the antigenic\nspace. Experimentation on a collective of real robots embodied with the\nalgorithm showed that a repertoire of antibody-like subcontrollers was created,\nevolved and shared on-the-fly to cope up with different environmental\nconditions. In addition, instead of the conventionally used approach of\nbroadcasting for sharing, we present an Intelligent Packet Migration scheme\nthat reduces energy consumption.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 04:33:11 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Semwal", "Tushar", ""], ["Kulkarni", "Divya D", ""], ["Nair", "Shivashankar B.", ""]]}, {"id": "1806.09819", "submitter": "Jonas Prellberg", "authors": "Jonas Prellberg, Oliver Kramer", "title": "Limited Evaluation Evolutionary Optimization of Large Neural Networks", "comments": "Accepted at KI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent is the most prevalent algorithm to train neural\nnetworks. However, other approaches such as evolutionary algorithms are also\napplicable to this task. Evolutionary algorithms bring unique trade-offs that\nare worth exploring, but computational demands have so far restricted\nexploration to small networks with few parameters. We implement an evolutionary\nalgorithm that executes entirely on the GPU, which allows to efficiently\nbatch-evaluate a whole population of networks. Within this framework, we\nexplore the limited evaluation evolutionary algorithm for neural network\ntraining and find that its batch evaluation idea comes with a large accuracy\ntrade-off. In further experiments, we explore crossover operators and find that\nunprincipled random uniform crossover performs extremely well. Finally, we\ntrain a network with 92k parameters on MNIST using an EA and achieve 97.6 %\ntest accuracy compared to 98 % test accuracy on the same network trained with\nAdam. Code is available at https://github.com/jprellberg/gpuea.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:17:51 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Prellberg", "Jonas", ""], ["Kramer", "Oliver", ""]]}, {"id": "1806.10115", "submitter": "Christian Lins", "authors": "Christian Lins, Daniel Eckhoff, Andreas Klausen, Sandra Hellmers,\n  Andreas Hein, Sebastian Fudickar", "title": "Cardiopulmonary resuscitation quality parameters from motion capture\n  data using Differential Evolution fitting of sinusoids", "comments": "Final paper, 22 pages", "journal-ref": "Applied Soft Computing, Volume 79, June 2019, Pages 300-309", "doi": "10.1016/j.asoc.2019.03.023", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiopulmonary resuscitation (CPR) is alongside electrical defibrillation\nthe most crucial countermeasure for sudden cardiac arrest, which affects\nthousands of individuals every year. In this paper, we present a novel approach\nincluding sinusoid models that use skeletal motion data from an RGB-D (Kinect)\nsensor and the Differential Evolution (DE) optimization algorithm to\ndynamically fit sinusoidal curves to derive frequency and depth parameters for\ncardiopulmonary resuscitation training. It is intended to be part of a robust\nand easy-to-use feedback system for CPR training, allowing its use for\nunsupervised training. The accuracy of this DE-based approach is evaluated in\ncomparison with data of 28 participants recorded by a state-of-the-art training\nmannequin. We optimized the DE algorithm hyperparameters and showed that with\nthese optimized parameters the frequency of the CPR is recognized with a median\nerror of $\\pm 2.9$ compressions per minute compared to the reference training\nmannequin.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 17:24:49 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 10:45:37 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 18:04:04 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 10:54:39 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lins", "Christian", ""], ["Eckhoff", "Daniel", ""], ["Klausen", "Andreas", ""], ["Hellmers", "Sandra", ""], ["Hein", "Andreas", ""], ["Fudickar", "Sebastian", ""]]}, {"id": "1806.10181", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John Hopfield", "title": "Unsupervised Learning by Competing Hidden Units", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences of the USA, 116\n  (16) 7723-7731 (2019)", "doi": "10.1073/pnas.1820458116", "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:32:58 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:36:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John", ""]]}, {"id": "1806.10230", "submitter": "Niru Maheswaranathan", "authors": "Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, Jascha\n  Sohl-Dickstein", "title": "Guided evolutionary strategies: Augmenting random search with surrogate\n  gradients", "comments": "Published at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in machine learning require optimizing a function whose\ntrue gradient is unknown, but where surrogate gradient information (directions\nthat may be correlated with, but not necessarily identical to, the true\ngradient) is available instead. This arises when an approximate gradient is\neasier to compute than the full gradient (e.g. in meta-learning or unrolled\noptimization), or when a true gradient is intractable and is replaced with a\nsurrogate (e.g. in certain reinforcement learning applications, or when using\nsynthetic gradients). We propose Guided Evolutionary Strategies, a method for\noptimally using surrogate gradient directions along with random search. We\ndefine a search distribution for evolutionary strategies that is elongated\nalong a guiding subspace spanned by the surrogate gradients. This allows us to\nestimate a descent direction which can then be passed to a first-order\noptimizer. We analytically and numerically characterize the tradeoffs that\nresult from tuning how strongly the search distribution is stretched along the\nguiding subspace, and we use this to derive a setting of the hyperparameters\nthat works well across problems. Finally, we apply our method to example\nproblems, demonstrating an improvement over both standard evolutionary\nstrategies and first-order methods (that directly follow the surrogate\ngradient). We provide a demo of Guided ES at\nhttps://github.com/brain-research/guided-evolutionary-strategies\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 22:14:36 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 06:08:23 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 18:17:14 GMT"}, {"version": "v4", "created": "Mon, 10 Jun 2019 18:19:33 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Maheswaranathan", "Niru", ""], ["Metz", "Luke", ""], ["Tucker", "George", ""], ["Choi", "Dami", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.10551", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "Naresh Mallenahalli and T. Hitendra Sarma", "title": "A Tunable Particle Swarm Size Optimization Algorithm for Feature\n  Selection", "comments": "7 pages, 1 figure, This paper is accepted for oral presentation at\n  IEEE Congress on Evolutionary Computation (CEC) - WCCI 2018, Rio de Janerio,\n  Brazil, #18120", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is the process of identifying statistically most relevant\nfeatures to improve the predictive capabilities of the classifiers. To find the\nbest features subsets, the population based approaches like Particle Swarm\nOptimization(PSO) and genetic algorithms are being widely employed. However, it\nis a general observation that not having right set of particles in the swarm\nmay result in sub-optimal solutions, affecting the accuracies of classifiers.\nTo address this issue, we propose a novel tunable swarm size approach to\nreconfigure the particles in a standard PSO, based on the data sets, in real\ntime. The proposed algorithm is named as Tunable Particle Swarm Size\nOptimization Algorithm (TPSO). It is a wrapper based approach wherein an\nAlternating Decision Tree (ADT) classifier is used for identifying influential\nfeature subset, which is further evaluated by a new objective function which\nintegrates the Classification Accuracy (CA) with a modified F-Score, to ensure\nbetter classification accuracy over varying population sizes. Experimental\nstudies on bench mark data sets and Wilcoxon statistical test have proved the\nfact that the proposed algorithm (TPSO) is efficient in identifying optimal\nfeature subsets that improve classification accuracies of base classifiers in\ncomparison to its standalone form.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 08:31:14 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Mallenahalli", "Naresh", ""], ["Sarma", "T. Hitendra", ""]]}, {"id": "1806.10950", "submitter": "Yingyu Zhang", "authors": "Yingyu Zhang and Bing Zeng", "title": "A Decomposition-Based Many-Objective Evolutionary Algorithm with Local\n  Iterative Update", "comments": "arXiv admin note: text overlap with arXiv:1803.06282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies have shown that the conventional multi-objective\nevolutionary algorithms (MOEAs) based on decomposition may lose the population\ndiversity when solving some many-objective optimization problems. In this\npaper, a simple decomposition-based MOEA with local iterative update (LIU) is\nproposed. The LIU strategy has two features that are expected to drive the\npopulation to approximate the Pareto Front with good distribution. One is that\nonly the worst solution in the current neighborhood is swapped out by the newly\ngenerated offspring, preventing the population from being occupied by copies of\na few individuals. The other is that its iterative process helps to assign\nbetter solutions to subproblems, which is beneficial to make full use of the\nsimilarity of solutions to neighboring subproblems and explore local areas in\nthe search space. In addition, the time complexity of the proposed algorithm is\nthe same as that of MOEA/D, and lower than that of other known MOEAs, since it\nconsiders only individuals within the current neighborhood at each update. The\nalgorithm is compared with several of the best MOEAs on problems chosen from\ntwo famous test suites DTLZ and WFG. Experimental results demonstrate that only\na handful of running instances of the algorithm on DTLZ4 lose their population\ndiversity. What's more, the algorithm wins in most of the test instances in\nterms of both running time and solution quality, indicating that it is very\neffective in solving MaOPs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 01:27:12 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Zhang", "Yingyu", ""], ["Zeng", "Bing", ""]]}, {"id": "1806.11379", "submitter": "Andrzej Banburski", "authors": "Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Banburski, Xavier\n  Boix and Jack Hidary", "title": "Theory IIIb: Generalization in Deep Networks", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main puzzle of deep neural networks (DNNs) revolves around the apparent\nabsence of \"overfitting\", defined in this paper as follows: the expected error\ndoes not get worse when increasing the number of neurons or of iterations of\ngradient descent. This is surprising because of the large capacity demonstrated\nby DNNs to fit randomly labeled data and the absence of explicit\nregularization. Recent results by Srebro et al. provide a satisfying solution\nof the puzzle for linear networks used in binary classification. They prove\nthat minimization of loss functions such as the logistic, the cross-entropy and\nthe exp-loss yields asymptotic, \"slow\" convergence to the maximum margin\nsolution for linearly separable datasets, independently of the initial\nconditions. Here we prove a similar result for nonlinear multilayer DNNs near\nzero minima of the empirical loss. The result holds for exponential-type losses\nbut not for the square loss. In particular, we prove that the weight matrix at\neach layer of a deep network converges to a minimum norm solution up to a scale\nfactor (in the separable case). Our analysis of the dynamical system\ncorresponding to gradient descent of a multilayer network suggests a simple\ncriterion for ranking the generalization performance of different zero\nminimizers of the empirical loss.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:39:08 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Poggio", "Tomaso", ""], ["Liao", "Qianli", ""], ["Miranda", "Brando", ""], ["Banburski", "Andrzej", ""], ["Boix", "Xavier", ""], ["Hidary", "Jack", ""]]}, {"id": "1806.11420", "submitter": "Chandrakant Bothe", "authors": "Chandrakant Bothe, Sven Magg, Cornelius Weber, Stefan Wermter", "title": "Discourse-Wizard: Discovering Deep Discourse Structure in your\n  Conversation with RNNs", "comments": "Submitted to EMNLP 2018: System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding is one of the key factors in a dialogue system,\nand a context in a conversation plays an important role to understand the\ncurrent utterance. In this work, we demonstrate the importance of context\nwithin the dialogue for neural network models through an online web interface\nlive demo. We developed two different neural models: a model that does not use\ncontext and a context-based model. The no-context model classifies dialogue\nacts at an utterance-level whereas the context-based model takes some preceding\nutterances into account. We make these trained neural models available as a\nlive demo called Discourse-Wizard using a modular server architecture. The live\ndemo provides an easy to use interface for conversational analysis and for\ndiscovering deep discourse structures in a conversation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 14:02:04 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Bothe", "Chandrakant", ""], ["Magg", "Sven", ""], ["Weber", "Cornelius", ""], ["Wermter", "Stefan", ""]]}]