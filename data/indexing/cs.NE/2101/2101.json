[{"id": "2101.00165", "submitter": "Bahareh Nakisa", "authors": "Mohammad Naim Rastgoo, Bahareh Nakisa, Andry Rakotonirainy, Frederic\n  Maire, Vinod Chandran", "title": "ECG-Based Driver Stress Levels Detection System Using Hyperparameter\n  Optimization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stress and driving are a dangerous combination which can lead to crashes, as\nevidenced by the large number of road traffic crashes that involve stress.\nMotivated by the need to address the significant costs of driver stress, it is\nessential to build a practical system that can classify driver stress level\nwith high accuracy. However, the performance of an accurate driving stress\nlevels classification system depends on hyperparameter optimization choices\nsuch as data segmentation (windowing hyperparameters). The configuration\nsetting of hyperparameters, which has an enormous impact on the system\nperformance, are typically hand-tuned while evaluating the algorithm. This\ntuning process is time consuming and often depends on personal experience.\nThere are also no generic optimal values for hyperparameters values. In this\nwork, we propose a meta-heuristic approach to support automated hyperparameter\noptimization and provide a real-time driver stress detection system. This is\nthe first systematic study of optimizing windowing hyperparameters based on\nElectrocardiogram (ECG) signal in the domain of driving safety. Our approach is\nto propose a framework based on Particle Swarm Optimization algorithm (PSO) to\nselect an optimal/near optimal windowing hyperparameters values. The\nperformance of the proposed framework is evaluated on two datasets: a public\ndataset (DRIVEDB dataset) and our collected dataset using an advanced\nsimulator. DRIVEDB dataset was collected in a real time driving scenario, and\nour dataset was collected using an advanced driving simulator in the control\nenvironment. We demonstrate that optimising the windowing hyperparameters\nyields significant improvement in terms of accuracy. The most accurate built\nmodel applied to the public dataset and our dataset, based on the selected\nwindowing hyperparameters, achieved 92.12% and 77.78% accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:18:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Rastgoo", "Mohammad Naim", ""], ["Nakisa", "Bahareh", ""], ["Rakotonirainy", "Andry", ""], ["Maire", "Frederic", ""], ["Chandran", "Vinod", ""]]}, {"id": "2101.00169", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Generative Deep Learning for Virtuosic Classical Music: Generative\n  Adversarial Networks as Renowned Composers", "comments": "13 pages, 6 figures Update: Revised format to align closer to IEEE\n  standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current AI-generated music lacks fundamental principles of good compositional\ntechniques. By narrowing down implementation issues both programmatically and\nmusically, we can create a better understanding of what parameters are\nnecessary for a generated composition nearly indistinguishable from that of a\nmaster composer.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:40:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:40:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00245", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "The Bayesian Method of Tensor Networks", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is a powerful learning framework which combines the\nexternal information of the data (background information) with the internal\ninformation (training data) in a logically consistent way in inference and\nprediction. By Bayes rule, the external information (prior distribution) and\nthe internal information (training data likelihood) are combined coherently,\nand the posterior distribution and the posterior predictive (marginal)\ndistribution obtained by Bayes rule summarize the total information needed in\nthe inference and prediction, respectively. In this paper, we study the\nBayesian framework of the Tensor Network from two perspective. First, we\nintroduce the prior distribution to the weights in the Tensor Network and\npredict the labels of the new observations by the posterior predictive\n(marginal) distribution. Since the intractability of the parameter integral in\nthe normalization constant computation, we approximate the posterior predictive\ndistribution by Laplace approximation and obtain the out-product approximation\nof the hessian matrix of the posterior distribution of the Tensor Network\nmodel. Second, to estimate the parameters of the stationary mode, we propose a\nstable initialization trick to accelerate the inference process by which the\nTensor Network can converge to the stationary path more efficiently and stably\nwith gradient descent method. We verify our work on the MNIST, Phishing Website\nand Breast Cancer data set. We study the Bayesian properties of the Bayesian\nTensor Network by visualizing the parameters of the model and the decision\nboundaries in the two dimensional synthetic data set. For a application\npurpose, our work can reduce the overfitting and improve the performance of\nnormal Tensor Network model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 14:59:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.00509", "submitter": "Benjamin Maschler", "authors": "Benjamin Maschler, Thi Thu Huong Pham, Michael Weyrich", "title": "Regularization-based Continual Learning for Anomaly Detection in\n  Discrete Manufacturing", "comments": "6 pages, 5 figures, 3 tables, submitted to the CIRP Conference on\n  Manufacturing Systems 2021", "journal-ref": null, "doi": "10.13140/RG.2.2.15631.00163", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early and robust detection of anomalies occurring in discrete\nmanufacturing processes allows operators to prevent harm, e.g. defects in\nproduction machinery or products. While current approaches for data-driven\nanomaly detection provide good results on the exact processes they were trained\non, they often lack the ability to flexibly adapt to changes, e.g. in products.\nContinual learning promises such flexibility, allowing for an automatic\nadaption of previously learnt knowledge to new tasks. Therefore, this article\ndiscusses different continual learning approaches from the group of\nregularization strategies, which are implemented, evaluated and compared based\non a real industrial metal forming dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 20:06:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Maschler", "Benjamin", ""], ["Pham", "Thi Thu Huong", ""], ["Weyrich", "Michael", ""]]}, {"id": "2101.00536", "submitter": "Guanrong Chen Professor", "authors": "Dinghua Shi, Zhifeng Chen, Xiang Sun, Qinghua Chen, Chuang Ma, Yang\n  Lou and Guanrong Chen", "title": "Computing Cliques and Cavities in Networks", "comments": "20 pages, 4+2 figures, 3+3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex networks have complete subgraphs such as nodes, edges, triangles,\netc., referred to as cliques of different orders. Notably, cavities consisting\nof higher-order cliques have been found playing an important role in brain\nfunctions. Since searching for the maximum clique in a large network is an\nNP-complete problem, we propose using k-core decomposition to determine the\ncomputability of a given network subject to limited computing resources. For a\ncomputable network, we design a search algorithm for finding cliques of\ndifferent orders, which also provides the Euler characteristic number. Then, we\ncompute the Betti number by using the ranks of the boundary matrices of\nadjacent cliques. Furthermore, we design an optimized algorithm for finding\ncavities of different orders. Finally, we apply the algorithm to the neuronal\nnetwork of C. elegans in one dataset, and find all of its cliques and some\ncavities of different orders therein, providing a basis for further\nmathematical analysis and computation of the structure and function of the C.\nelegans neuronal network.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 01:09:43 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:35:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shi", "Dinghua", ""], ["Chen", "Zhifeng", ""], ["Sun", "Xiang", ""], ["Chen", "Qinghua", ""], ["Ma", "Chuang", ""], ["Lou", "Yang", ""], ["Chen", "Guanrong", ""]]}, {"id": "2101.01558", "submitter": "Mirko Trisolini", "authors": "Mirko Trisolini and Hugh G. Lewis and Camilla Colombo", "title": "Constrained optimisation of preliminary spacecraft configurations under\n  the design-for-demise paradigm", "comments": "Pre-print submitted to the Journal of Space Safety Engineering", "journal-ref": "Journal of Space Safety Engineering. 8. 1. (2021) 63-74", "doi": "10.1016/j.jsse.2021.01.005", "report-no": null, "categories": "eess.SY cs.CE cs.NE cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the past few years, the interest towards the implementation of\ndesign-for-demise measures has increased steadily. Most mid-sized satellites\ncurrently launched and already in orbit fail to comply with the casualty risk\nthreshold of 0.0001. Therefore, satellites manufacturers and mission operators\nneed to perform a disposal through a controlled re-entry, which has a higher\ncost and increased complexity. Through the design-for-demise paradigm, this\nadditional cost and complexity can be removed as the spacecraft is directly\ncompliant with the casualty risk regulations. However, building a spacecraft\nsuch that most of its parts will demise may lead to designs that are more\nvulnerable to space debris impacts, thus compromising the reliability of the\nmission. In fact, the requirements connected to the demisability and the\nsurvivability are in general competing. Given this competing nature, trade-off\nsolutions can be found, which favour the implementation of design-for-demise\nmeasures while still maintaining the spacecraft resilient to space debris\nimpacts. A multi-objective optimisation framework has been developed by the\nauthors in previous works. The framework's objective is to find preliminary\ndesign solutions considering the competing nature of the demisability and the\nsurvivability of a spacecraft since the early stages of the mission design. In\nthis way, a more integrated design can be achieved. The present work focuses on\nthe improvement of the multi-objective optimisation framework by including\nconstraints. The paper shows the application of the constrained optimisation to\ntwo relevant examples: the optimisation of a tank assembly and the optimisation\nof a typical satellite configuration.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:48:29 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 19:07:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Trisolini", "Mirko", ""], ["Lewis", "Hugh G.", ""], ["Colombo", "Camilla", ""]]}, {"id": "2101.01998", "submitter": "Jian Cheng Wong", "authors": "Jian Cheng Wong, Abhishek Gupta, Yew-Soon Ong", "title": "Can Transfer Neuroevolution Tractably Solve Your Differential Equations?", "comments": null, "journal-ref": "IEEE Computational Intelligence Magazine, Volume 16, Number 2, May\n  2021, p14-30", "doi": "10.1109/MCI.2021.3061854", "report-no": null, "categories": "cs.NE physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces neuroevolution for solving differential equations. The\nsolution is obtained through optimizing a deep neural network whose loss\nfunction is defined by the residual terms from the differential equations.\nRecent studies have focused on learning such physics-informed neural networks\nthrough stochastic gradient descent (SGD) variants, yet they face the\ndifficulty of obtaining an accurate solution due to optimization challenges. In\nthe context of solving differential equations, we are faced with the problem of\nfinding globally optimum parameters of the network, instead of being concerned\nwith out-of-sample generalization. SGD, which searches along a single gradient\ndirection, is prone to become trapped in local optima, so it may not be the\nbest approach here. In contrast, neuroevolution carries out a parallel\nexploration of diverse solutions with the goal of circumventing local optima.\nIt could potentially find more accurate solutions with better optimized neural\nnetworks. However, neuroevolution can be slow, raising tractability issues in\npractice. With that in mind, a novel and computationally efficient transfer\nneuroevolution algorithm is proposed in this paper. Our method is capable of\nexploiting relevant experiential priors when solving a new problem, with\nadaptation to protect against the risk of negative transfer. The algorithm is\napplied on a variety of differential equations to empirically demonstrate that\ntransfer neuroevolution can indeed achieve better accuracy and faster\nconvergence than SGD. The experimental outcomes thus establish transfer\nneuroevolution as a noteworthy approach for solving differential equations, one\nthat has never been studied in the past. Our work expands the resource of\navailable algorithms for optimizing physics-informed neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 13:07:52 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 02:37:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wong", "Jian Cheng", ""], ["Gupta", "Abhishek", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "2101.02153", "submitter": "Benedek Rozemberczki", "authors": "Benedek Rozemberczki and Rik Sarkar", "title": "The Shapley Value of Classifiers in Ensemble Games", "comments": "Source code is available here:\n  https://github.com/benedekrozemberczki/shapley", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.GT cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What is the value of an individual model in an ensemble of binary\nclassifiers? We answer this question by introducing a class of transferable\nutility cooperative games called \\textit{ensemble games}. In machine learning\nensembles, pre-trained models cooperate to make classification decisions. To\nquantify the importance of models in these ensemble games, we define\n\\textit{Troupe} -- an efficient algorithm which allocates payoffs based on\napproximate Shapley values of the classifiers. We argue that the Shapley value\nof models in these games is an effective decision metric for choosing a high\nperforming subset of models from the ensemble. Our analytical findings prove\nthat our Shapley value estimation scheme is precise and scalable; its\nperformance increases with size of the dataset and ensemble. Empirical results\non real world graph classification tasks demonstrate that our algorithm\nproduces high quality estimates of the Shapley value. We find that Shapley\nvalues can be utilized for ensemble pruning, and that adversarial models\nreceive a low valuation. Complex classifiers are frequently found to be\nresponsible for both correct and incorrect classification decisions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:40:23 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:38:54 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rozemberczki", "Benedek", ""], ["Sarkar", "Rik", ""]]}, {"id": "2101.02333", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "Infinitely Wide Tensor Networks as Gaussian Process", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process is a non-parametric prior which can be understood as a\ndistribution on the function space intuitively. It is known that by introducing\nappropriate prior to the weights of the neural networks, Gaussian Process can\nbe obtained by taking the infinite-width limit of the Bayesian neural networks\nfrom a Bayesian perspective. In this paper, we explore the infinitely wide\nTensor Networks and show the equivalence of the infinitely wide Tensor Networks\nand the Gaussian Process. We study the pure Tensor Network and another two\nextended Tensor Network structures: Neural Kernel Tensor Network and Tensor\nNetwork hidden layer Neural Network and prove that each one will converge to\nthe Gaussian Process as the width of each model goes to infinity. (We note here\nthat Gaussian Process can also be obtained by taking the infinite limit of at\nleast one of the bond dimensions $\\alpha_{i}$ in the product of tensor nodes,\nand the proofs can be done with the same ideas in the proofs of the\ninfinite-width cases.) We calculate the mean function (mean vector) and the\ncovariance function (covariance matrix) of the finite dimensional distribution\nof the induced Gaussian Process by the infinite-width tensor network with a\ngeneral set-up. We study the properties of the covariance function and derive\nthe approximation of the covariance function when the integral in the\nexpectation operator is intractable. In the numerical experiments, we implement\nthe Gaussian Process corresponding to the infinite limit tensor networks and\nplot the sample paths of these models. We study the hyperparameters and plot\nthe sample path families in the induced Gaussian Process by varying the\nstandard deviations of the prior distributions. As expected, the parameters in\nthe prior distribution namely the hyper-parameters in the induced Gaussian\nProcess controls the characteristic lengthscales of the Gaussian Process.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:29:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.02344", "submitter": "Yufang Huang Dr.", "authors": "Yufang Huang, Kelly M. Axsom, John Lee, Lakshminarayanan Subramanian\n  and Yiye Zhang", "title": "DICE: Deep Significance Clustering for Outcome-Aware Stratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deep significance clustering (DICE), a framework for jointly\nperforming representation learning and clustering for \"outcome-aware\"\nstratification. DICE is intended to generate cluster membership that may be\nused to categorize a population by individual risk level for a targeted\noutcome. Following the representation learning and clustering steps, we embed\nthe objective function in DICE with a constraint which requires a statistically\nsignificant association between the outcome and cluster membership of learned\nrepresentations. DICE further includes a neural architecture search step to\nmaximize both the likelihood of representation learning and outcome\nclassification accuracy with cluster membership as the predictor. To\ndemonstrate its utility in medicine for patient risk-stratification, the\nperformance of DICE was evaluated using two datasets with different outcome\nratios extracted from real-world electronic health records. Outcomes are\ndefined as acute kidney injury (30.4\\%) among a cohort of COVID-19 patients,\nand discharge disposition (36.8\\%) among a cohort of heart failure patients,\nrespectively. Extensive results demonstrate that DICE has superior performance\nas measured by the difference in outcome distribution across clusters,\nSilhouette score, Calinski-Harabasz index, and Davies-Bouldin index for\nclustering, and Area under the ROC Curve (AUC) for outcome classification\ncompared to several baseline approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:06:52 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Huang", "Yufang", ""], ["Axsom", "Kelly M.", ""], ["Lee", "John", ""], ["Subramanian", "Lakshminarayanan", ""], ["Zhang", "Yiye", ""]]}, {"id": "2101.02442", "submitter": "Clement Leroy", "authors": "Cl\\'ement Leroy (INTUIDOC), Eric Anquetil (INTUIDOC), Nathalie Girard\n  (INTUIDOC)", "title": "Drift anticipation with forgetting to improve evolving fuzzy system", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020),\n  Jan 2021, Milan, Italy", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with a non-stationary stream of data requires for the analysis system\nto evolve its model (the parameters as well as the structure) over time. In\nparticular, concept drifts can occur, which makes it necessary to forget\nknowledge that has become obsolete. However, the forgetting is subjected to the\nstability-plasticity dilemma, that is, increasing forgetting improve reactivity\nof adapting to the new data while reducing the robustness of the system. Based\non a set of inference rules, Evolving Fuzzy Systems-EFS-have proven to be\neffective in solving the data stream learning problem. However tackling the\nstability-plasticity dilemma is still an open question. This paper proposes a\ncoherent method to integrate forgetting in Evolving Fuzzy System, based on the\nrecently introduced notion of concept drift anticipation. The forgetting is\napplied with two methods: an exponential forgetting of the premise part and a\ndeferred directional forgetting of the conclusion part of EFS to preserve the\ncoherence between both parts. The originality of the approach consists in\napplying the forgetting only in the anticipation module and in keeping the EFS\n(called principal system) learned without any forgetting. Then, when a drift is\ndetected in the stream, a selection mechanism is proposed to replace the\nobsolete parameters of the principal system with more suitable parameters of\nthe anticipation module. An evaluation of the proposed methods is carried out\non benchmark online datasets, with a comparison with state-of-the-art online\nclassifiers (Learn++.NSE, PENsemble, pclass) as well as with the original\nsystem using different forgetting strategies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:21:27 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Leroy", "Cl\u00e9ment", "", "INTUIDOC"], ["Anquetil", "Eric", "", "INTUIDOC"], ["Girard", "Nathalie", "", "INTUIDOC"]]}, {"id": "2101.02480", "submitter": "Tugdual Ceillier", "authors": "Alex Goupilleau, Tugdual Ceillier, Marie-Caroline Corbineau", "title": "Active learning for object detection in high-resolution satellite images", "comments": null, "journal-ref": "Conference on Artificial Intelligence for Defense, Dec 2020,\n  Rennes, France", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the term active learning regroups techniques that aim at\nselecting the most useful data to label from a large pool of unlabelled\nexamples. While supervised deep learning techniques have shown to be\nincreasingly efficient on many applications, they require a huge number of\nlabelled examples to reach operational performances. Therefore, the labelling\neffort linked to the creation of the datasets required is also increasing. When\nworking on defense-related remote sensing applications, labelling can be\nchallenging due to the large areas covered and often requires military experts\nwho are rare and whose time is primarily dedicated to operational needs.\nLimiting the labelling effort is thus of utmost importance. This study aims at\nreviewing the most relevant active learning techniques to be used for object\ndetection on very high resolution imagery and shows an example of the value of\nsuch techniques on a relevant operational use case: aircraft detection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:57:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Goupilleau", "Alex", ""], ["Ceillier", "Tugdual", ""], ["Corbineau", "Marie-Caroline", ""]]}, {"id": "2101.02534", "submitter": "Leon Moonen", "authors": "Moeen Ali Naqvi and Merve Astekin and Sehrish Malik and Leon Moonen", "title": "Adaptive Immunity for Software: Towards Autonomous Self-healing Systems", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing and code reviews are known techniques to improve the quality and\nrobustness of software. Unfortunately, the complexity of modern software\nsystems makes it impossible to anticipate all possible problems that can occur\nat runtime, which limits what issues can be found using testing and reviews.\nThus, it is of interest to consider autonomous self-healing software systems,\nwhich can automatically detect, diagnose, and contain unanticipated problems at\nruntime. Most research in this area has adopted a model-driven approach, where\nactual behavior is checked against a model specifying the intended behavior,\nand a controller takes action when the system behaves outside of the\nspecification. However, it is not easy to develop these specifications, nor to\nkeep them up-to-date as the system evolves. We pose that, with the recent\nadvances in machine learning, such models may be learned by observing the\nsystem. Moreover, we argue that artificial immune systems (AISs) are\nparticularly well-suited for building self-healing systems, because of their\nanomaly detection and diagnosis capabilities. We present the state-of-the-art\nin self-healing systems and in AISs, surveying some of the research directions\nthat have been considered up to now. To help advance the state-of-the-art, we\ndevelop a research agenda for building self-healing software systems using\nAISs, identifying required foundations, and promising research directions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:22:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Naqvi", "Moeen Ali", ""], ["Astekin", "Merve", ""], ["Malik", "Sehrish", ""], ["Moonen", "Leon", ""]]}, {"id": "2101.02729", "submitter": "Prabuddha Chakraborty", "authors": "Prabuddha Chakraborty and Swarup Bhunia", "title": "Neural Storage: A New Paradigm of Elastic Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage and retrieval of data in a computer memory plays a major role in\nsystem performance. Traditionally, computer memory organization is static -\ni.e., they do not change based on the application-specific characteristics in\nmemory access behaviour during system operation. Specifically, the association\nof a data block with a search pattern (or cues) as well as the granularity of a\nstored data do not evolve. Such a static nature of computer memory, we observe,\nnot only limits the amount of data we can store in a given physical storage,\nbut it also misses the opportunity for dramatic performance improvement in\nvarious applications. On the contrary, human memory is characterized by\nseemingly infinite plasticity in storing and retrieving data - as well as\ndynamically creating/updating the associations between data and corresponding\ncues. In this paper, we introduce Neural Storage (NS), a brain-inspired\nlearning memory paradigm that organizes the memory as a flexible neural memory\nnetwork. In NS, the network structure, strength of associations, and\ngranularity of the data adjust continuously during system operation, providing\nunprecedented plasticity and performance benefits. We present the associated\nstorage/retrieval/retention algorithms in NS, which integrate a formalized\nlearning process. Using a full-blown operational model, we demonstrate that NS\nachieves an order of magnitude improvement in memory access performance for two\nrepresentative applications when compared to traditional content-based memory.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:19:25 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Chakraborty", "Prabuddha", ""], ["Bhunia", "Swarup", ""]]}, {"id": "2101.02913", "submitter": "Yusheng Huang", "authors": "Yusheng Huang (1), Dong Chu (1), Joel Weijia Lai (2), Yong Deng (1),\n  Kang Hao Cheong (2) ((1) Institute of Fundamental and Frontier Science,\n  University of Electronic Science and Technology of China, Chengdu, China, (2)\n  Science and Math Cluster, Singapore University of Technology and Design\n  (SUTD), Singapore)", "title": "When does the Physarum Solver Distinguish the Shortest Path from other\n  Paths: the Transition Point and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physarum solver, also called the physarum polycephalum inspired algorithm\n(PPA), is a newly developed bio-inspired algorithm that has an inherent ability\nto find the shortest path in a given graph. Recent research has proposed\nmethods to develop this algorithm further by accelerating the original PPA\n(OPPA)'s path-finding process. However, when does the PPA ascertain that the\nshortest path has been found? Is there a point after which the PPA could\ndistinguish the shortest path from other paths? By innovatively proposing the\nconcept of the dominant path (D-Path), the exact moment, named the transition\npoint (T-Point), when the PPA finds the shortest path can be identified. Based\non the D-Path and T-Point, a newly accelerated PPA named OPPA-D using the\nproposed termination criterion is developed which is superior to all other\nbaseline algorithms according to the experiments conducted in this paper. The\nvalidity and the superiority of the proposed termination criterion is also\ndemonstrated. Furthermore, an evaluation method is proposed to provide new\ninsights for the comparison of different accelerated OPPAs. The breakthrough of\nthis paper lies in using D-path and T-point to terminate the OPPA. The novel\ntermination criterion reveals the actual performance of this OPPA. This OPPA is\nthe fastest algorithm, outperforming some so-called accelerated OPPAs.\nFurthermore, we explain why some existing works inappropriately claim to be\naccelerated algorithms is in fact a product of inappropriate termination\ncriterion, thus giving rise to the illusion that the method is accelerated.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:48:56 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Huang", "Yusheng", ""], ["Chu", "Dong", ""], ["Lai", "Joel Weijia", ""], ["Deng", "Yong", ""], ["Cheong", "Kang Hao", ""]]}, {"id": "2101.02932", "submitter": "Zhenzhong Wang", "authors": "Zhenzhong Wang and Haokai Hong and Kai Ye and Min Jiang and Kay Chen\n  Tan", "title": "Manifold Interpolation for Large-Scale Multi-Objective Optimization via\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multiobjective optimization problems (LSMOPs) are characterized\nas involving hundreds or even thousands of decision variables and multiple\nconflicting objectives. An excellent algorithm for solving LSMOPs should find\nPareto-optimal solutions with diversity and escape from local optima in the\nlarge-scale search space. Previous research has shown that these optimal\nsolutions are uniformly distributed on the manifold structure in the\nlow-dimensional space. However, traditional evolutionary algorithms for solving\nLSMOPs have some deficiencies in dealing with this structural manifold,\nresulting in poor diversity, local optima, and inefficient searches. In this\nwork, a generative adversarial network (GAN)-based manifold interpolation\nframework is proposed to learn the manifold and generate high-quality solutions\non this manifold, thereby improving the performance of evolutionary algorithms.\nWe compare the proposed algorithm with several state-of-the-art algorithms on\nlarge-scale multiobjective benchmark functions. Experimental results have\ndemonstrated the significant improvements achieved by this framework in solving\nLSMOPs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 09:38:38 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wang", "Zhenzhong", ""], ["Hong", "Haokai", ""], ["Ye", "Kai", ""], ["Jiang", "Min", ""], ["Tan", "Kay Chen", ""]]}, {"id": "2101.02966", "submitter": "Florian Stelzer", "authors": "Florian Stelzer (1, 2 and 3), Serhiy Yanchuk (1) ((1) Institute of\n  Mathematics, Technische Universit\\\"at Berlin, Germany, (2) Department of\n  Mathematics, Humboldt-Universit\\\"at zu Berlin, Germany, (3) Institute of\n  Computer Science, University of Tartu, Estonia)", "title": "Infinite-dimensional Folded-in-time Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.DS math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method recently introduced in arXiv:2011.10115 realizes a deep neural\nnetwork with just a single nonlinear element and delayed feedback. It is\napplicable for the description of physically implemented neural networks. In\nthis work, we present an infinite-dimensional generalization, which allows for\na more rigorous mathematical analysis and a higher flexibility in choosing the\nweight functions. Precisely speaking, the weights are described by Lebesgue\nintegrable functions instead of step functions. We also provide a functional\nback-propagation algorithm, which enables gradient descent training of the\nweights. In addition, with a slight modification, our concept realizes\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:30:50 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:57:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Stelzer", "Florian", "", "1, 2 and 3"], ["Yanchuk", "Serhiy", ""]]}, {"id": "2101.03095", "submitter": "Samuel Liu", "authors": "Samuel Liu, Christopher H. Bennett, Joseph S. Friedman, Matthew J.\n  Marinella, David Paydarfar, Jean Anne C. Incorvia", "title": "Controllable reset behavior in domain wall-magnetic tunnel junction\n  artificial neurons for task-adaptable computation", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": "10.1109/LMAG.2021.3069666", "report-no": null, "categories": "cond-mat.mes-hall cs.ET cs.NE physics.app-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing with spintronic devices has been of interest due to\nthe limitations of CMOS-driven von Neumann computing. Domain wall-magnetic\ntunnel junction (DW-MTJ) devices have been shown to be able to intrinsically\ncapture biological neuron behavior. Edgy-relaxed behavior, where a frequently\nfiring neuron experiences a lower action potential threshold, may provide\nadditional artificial neuronal functionality when executing repeated tasks. In\nthis study, we demonstrate that this behavior can be implemented in DW-MTJ\nartificial neurons via three alternative mechanisms: shape anisotropy, magnetic\nfield, and current-driven soft reset. Using micromagnetics and analytical\ndevice modeling to classify the Optdigits handwritten digit dataset, we show\nthat edgy-relaxed behavior improves both classification accuracy and\nclassification rate for ordered datasets while sacrificing little to no\naccuracy for a randomized dataset. This work establishes methods by which\nartificial spintronic neurons can be flexibly adapted to datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:50:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liu", "Samuel", ""], ["Bennett", "Christopher H.", ""], ["Friedman", "Joseph S.", ""], ["Marinella", "Matthew J.", ""], ["Paydarfar", "David", ""], ["Incorvia", "Jean Anne C.", ""]]}, {"id": "2101.03221", "submitter": "Stefano Martina", "authors": "Stefano Martina, Stefano Gherardini, Filippo Caruso", "title": "Machine learning approach for quantum non-Markovian noise classification", "comments": "14 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, machine learning and artificial neural network models are\nproposed for quantum noise classification in stochastic quantum dynamics. For\nthis purpose, we train and then validate support vector machine, multi-layer\nperceptron and recurrent neural network, models with different complexity and\naccuracy, to solve supervised binary classification problems. By exploiting the\nquantum random walk formalism, we demonstrate the high efficacy of such tools\nin classifying noisy quantum dynamics using data sets collected in a single\nrealisation of the quantum system evolution. In addition, we also show that for\na successful classification one just needs to measure, in a sequence of\ndiscrete time instants, the probabilities that the analysed quantum system is\nin one of the allowed positions or energy configurations, without any external\ndriving. Thus, neither measurements of quantum coherences nor sequences of\ncontrol pulses are required. Since in principle the training of the machine\nlearning models can be performed a-priori on synthetic data, our approach is\nexpected to find direct application in a vast number of experimental schemes\nand also for the noise benchmarking of the already available noisy\nintermediate-scale quantum devices.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:56:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Martina", "Stefano", ""], ["Gherardini", "Stefano", ""], ["Caruso", "Filippo", ""]]}, {"id": "2101.03419", "submitter": "Shiyu Duan", "authors": "Shiyu Duan and Jose C. Principe", "title": "Training Deep Architectures Without End-to-End Backpropagation: A Brief\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial paper surveys training alternatives to end-to-end\nbackpropagation (E2EBP) -- the de facto standard for training deep\narchitectures. Modular training refers to strictly local training without both\nthe forward and the backward pass, i.e., dividing a deep architecture into\nseveral nonoverlapping modules and training them separately without any\nend-to-end operation. Between the fully global E2EBP and the strictly local\nmodular training, there are \"weakly modular\" hybrids performing training\nwithout the backward pass only. These alternatives can match or surpass the\nperformance of E2EBP on challenging datasets such as ImageNet, and are gaining\nincreased attention primarily because they offer practical advantages over\nE2EBP, which will be enumerated herein. In particular, they allow for greater\nmodularity and transparency in deep learning workflows, aligning deep learning\nwith the mainstream computer science engineering that heavily exploits\nmodularization for scalability. Modular training has also revealed novel\ninsights about learning and has further implications on other important\nresearch domains. Specifically, it induces natural and effective solutions to\nsome important practical problems such as data efficiency and transferability\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:56:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 03:36:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Duan", "Shiyu", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.03867", "submitter": "Ahmad Asadi", "authors": "Mehran Taghian, Ahmad Asadi, Reza Safabakhsh", "title": "A Reinforcement Learning Based Encoder-Decoder Framework for Learning\n  Stock Trading Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of deep reinforcement learning (DRL) models have recently been\nproposed to learn profitable investment strategies. The rules learned by these\nmodels outperform the previous strategies specially in high frequency trading\nenvironments. However, it is shown that the quality of the extracted features\nfrom a long-term sequence of raw prices of the instruments greatly affects the\nperformance of the trading rules learned by these models. Employing a neural\nencoder-decoder structure to extract informative features from complex input\ntime-series has proved very effective in other popular tasks like neural\nmachine translation and video captioning in which the models face a similar\nproblem. The encoder-decoder framework extracts highly informative features\nfrom a long sequence of prices along with learning how to generate outputs\nbased on the extracted features. In this paper, a novel end-to-end model based\non the neural encoder-decoder framework combined with DRL is proposed to learn\nsingle instrument trading strategies from a long sequence of raw prices of the\ninstrument. The proposed model consists of an encoder which is a neural\nstructure responsible for learning informative features from the input\nsequence, and a decoder which is a DRL model responsible for learning\nprofitable strategies based on the features extracted by the encoder. The\nparameters of the encoder and the decoder structures are learned jointly, which\nenables the encoder to extract features fitted to the task of the decoder DRL.\nIn addition, the effects of different structures for the encoder and various\nforms of the input sequences on the performance of the learned strategies are\ninvestigated. Experimental results showed that the proposed model outperforms\nother state-of-the-art models in highly dynamic environments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:19:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Taghian", "Mehran", ""], ["Asadi", "Ahmad", ""], ["Safabakhsh", "Reza", ""]]}, {"id": "2101.03958", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey\n  Levine, Quoc V. Le, Honglak Lee, Aleksandra Faust", "title": "Evolving Reinforcement Learning Algorithms", "comments": "ICLR 2021 Oral. See project website at\n  https://sites.google.com/view/evolvingrl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for meta-learning reinforcement learning algorithms by\nsearching over the space of computational graphs which compute the loss\nfunction for a value-based model-free RL agent to optimize. The learned\nalgorithms are domain-agnostic and can generalize to new environments not seen\nduring training. Our method can both learn from scratch and bootstrap off known\nexisting algorithms, like DQN, enabling interpretable modifications which\nimprove performance. Learning from scratch on simple classical control and\ngridworld tasks, our method rediscovers the temporal-difference (TD) algorithm.\nBootstrapped from DQN, we highlight two learned algorithms which obtain good\ngeneralization performance over other classical control tasks, gridworld type\ntasks, and Atari games. The analysis of the learned algorithm behavior shows\nresemblance to recently proposed RL algorithms that address overestimation in\nvalue-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:55:07 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 19:41:47 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 22:53:58 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 16:35:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Miao", "Yingjie", ""], ["Peng", "Daiyi", ""], ["Real", "Esteban", ""], ["Levine", "Sergey", ""], ["Le", "Quoc V.", ""], ["Lee", "Honglak", ""], ["Faust", "Aleksandra", ""]]}, {"id": "2101.04354", "submitter": "Yeshwanth Venkatesha", "authors": "Karina Vasquez, Yeshwanth Venkatesha, Abhiroop Bhattacharjee, Abhishek\n  Moitra, Priyadarshini Panda", "title": "Activation Density based Mixed-Precision Quantization for Energy\n  Efficient Neural Networks", "comments": "Published in Design, Automation and Test in Europe (DATE) conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks gain widespread adoption in embedded devices, there is a\nneed for model compression techniques to facilitate deployment in\nresource-constrained environments. Quantization is one of the go-to methods\nyielding state-of-the-art model compression. Most approaches take a fully\ntrained model, apply different heuristics to determine the optimal\nbit-precision for different layers of the network, and retrain the network to\nregain any drop in accuracy. Based on Activation Density (AD)-the proportion of\nnon-zero activations in a layer-we propose an in-training quantization method.\nOur method calculates bit-width for each layer during training yielding a mixed\nprecision model with competitive accuracy. Since we train lower precision\nmodels during training, our approach yields the final quantized model at lower\ntraining complexity and also eliminates the need for re-training. We run\nexperiments on benchmark datasets like CIFAR-10, CIFAR-100, TinyImagenet on\nVGG19/ResNet18 architectures and report the accuracy and energy estimates for\nthe same. We achieve ~4.5x benefit in terms of estimated\nmultiply-and-accumulate (MAC) reduction while reducing the training complexity\nby 50% in our experiments. To further evaluate the energy benefits of our\nproposed method, we develop a mixed-precision scalable Process In Memory (PIM)\nhardware accelerator platform. The hardware platform incorporates shift-add\nfunctionality for handling multi-bit precision neural network models.\nEvaluating the quantized models obtained with our proposed method on the PIM\nplatform yields ~5x energy reduction compared to 16-bit models. Additionally,\nwe find that integrating AD based quantization with AD based pruning (both\nconducted during training) yields up to ~198x and ~44x energy reductions for\nVGG19 and ResNet18 architectures respectively on PIM platform compared to\nbaseline 16-bit precision, unpruned models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:01:44 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Vasquez", "Karina", ""], ["Venkatesha", "Yeshwanth", ""], ["Bhattacharjee", "Abhiroop", ""], ["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2101.04424", "submitter": "Manuel Chica Serrano", "authors": "M. Chica and J. Hernandez and C. Manrique-de-Lara-Pe\\~nate and R.\n  Chiong", "title": "An Evolutionary Game Model for Understanding Fraud in Consumption Taxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computational evolutionary game model to study and\nunderstand fraud dynamics in the consumption tax system. Players are\ncooperators if they correctly declare their value added tax (VAT), and are\ndefectors otherwise. Each player's payoff is influenced by the amount evaded\nand the subjective probability of being inspected by tax authorities. Since\ntransactions between companies must be declared by both the buyer and seller, a\nstrategy adopted by one influences the other's payoff. We study the model with\na well-mixed population and different scale-free networks. Model parameters\nwere calibrated using real-world data of VAT declarations by businesses\nregistered in the Canary Islands region of Spain. We analyzed several scenarios\nof audit probabilities for high and low transactions and their prevalence in\nthe population, as well as social rewards and penalties to find the most\nefficient policy to increase the proportion of cooperators. Two major insights\nwere found. First, increasing the subjective audit probability for low\ntransactions is more efficient than increasing this probability for high\ntransactions. Second, favoring social rewards for cooperators or alternative\npenalties for defectors can be effective policies, but their success depends on\nthe distribution of the audit probability for low and high transactions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:53:31 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Chica", "M.", ""], ["Hernandez", "J.", ""], ["Manrique-de-Lara-Pe\u00f1ate", "C.", ""], ["Chiong", "R.", ""]]}, {"id": "2101.04753", "submitter": "Jin-Kao Hao", "authors": "Zequn Wei and Jin-Kao Hao", "title": "A threshold search based memetic algorithm for the disjunctively\n  constrained knapsack problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disjunctively constrained knapsack problem consists in packing a subset\nof pairwisely compatible items in a capacity-constrained knapsack such that the\ntotal profit of the selected items is maximized while satisfying the knapsack\ncapacity. DCKP has numerous applications and is however computationally\nchallenging (NP-hard). In this work, we present a threshold search based\nmemetic algorithm for solving the DCKP that combines the memetic framework with\nthreshold search to find high quality solutions. Extensive computational\nassessments on two sets of 6340 benchmark instances in the literature\ndemonstrate that the proposed algorithm is highly competitive compared to the\nstate-of-the-art methods. In particular, we report 24 and 354 improved\nbest-known results (new lower bounds) for Set I (100 instances) and for Set II\n(6240 instances), respectively. We analyze the key algorithmic components and\nshed lights on their roles for the performance of the algorithm. The code of\nour algorithm will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:07:33 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wei", "Zequn", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "2101.04758", "submitter": "Muhammad Khalifa", "authors": "Muhammad Khalifa and Muhammad Abdul-Mageed and Khaled Shaalan", "title": "Self-Training Pre-Trained Language Models for Zero- and Few-Shot\n  Multi-Dialectal Arabic Sequence Labeling", "comments": "Accepted at EACL 2021 (Camera Ready Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A sufficient amount of annotated data is usually required to fine-tune\npre-trained language models for downstream tasks. Unfortunately, attaining\nlabeled data can be costly, especially for multiple language varieties and\ndialects. We propose to self-train pre-trained language models in zero- and\nfew-shot scenarios to improve performance on data-scarce varieties using only\nresources from data-rich ones. We demonstrate the utility of our approach in\nthe context of Arabic sequence labeling by using a language model fine-tuned on\nModern Standard Arabic (MSA) only to predict named entities (NE) and\npart-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show\nthat self-training is indeed powerful, improving zero-shot MSA-to-DA transfer\nby as large as \\texttildelow 10\\% F$_1$ (NER) and 2\\% accuracy (POS tagging).\nWe acquire even better performance in few-shot scenarios with limited amounts\nof labeled data. We conduct an ablation study and show that the performance\nboost observed directly results from the unlabeled DA examples used for\nself-training. Our work opens up opportunities for developing DA models\nexploiting only MSA resources and it can be extended to other languages and\ntasks. Our code and fine-tuned models can be accessed at\nhttps://github.com/mohammadKhalifa/zero-shot-arabic-dialects.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:29:30 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 00:00:36 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 20:48:15 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 23:36:04 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Khalifa", "Muhammad", ""], ["Abdul-Mageed", "Muhammad", ""], ["Shaalan", "Khaled", ""]]}, {"id": "2101.05457", "submitter": "Ka-Hou Chan", "authors": "Ka-Hou Chan, Sio-Kei Im and Wei Ke", "title": "A Multiple Classifier Approach for Concatenate-Designed Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a multiple classifier method to improve the\nperformance of concatenate-designed neural networks, such as ResNet and\nDenseNet, with the purpose to alleviate the pressure on the final classifier.\nWe give the design of the classifiers, which collects the features produced\nbetween the network sets, and present the constituent layers and the activation\nfunction for the classifiers, to calculate the classification score of each\nclassifier. We use the L2 normalization method to obtain the classifier score\ninstead of the Softmax normalization. We also determine the conditions that can\nenhance convergence. As a result, the proposed classifiers are able to improve\nthe accuracy in the experimental cases significantly, and show that the method\nnot only has better performance than the original models, but also produces\nfaster convergence. Moreover, our classifiers are general and can be applied to\nall classification related concatenate-designed network models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 04:32:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chan", "Ka-Hou", ""], ["Im", "Sio-Kei", ""], ["Ke", "Wei", ""]]}, {"id": "2101.05536", "submitter": "Axel Laborieux", "authors": "Axel Laborieux, Maxence Ernoult, Benjamin Scellier, Yoshua Bengio,\n  Julie Grollier and Damien Querlioz", "title": "Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing\n  its Gradient Estimator Bias", "comments": "NeurIPS 2020 Workshop : \"Beyond Backpropagation Novel Ideas for\n  Training Neural Architectures\". arXiv admin note: substantial text overlap\n  with arXiv:2006.03824", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Equilibrium Propagation (EP) is a biologically-inspired counterpart of\nBackpropagation Through Time (BPTT) which, owing to its strong theoretical\nguarantees and the locality in space of its learning rule, fosters the design\nof energy-efficient hardware dedicated to learning. In practice, however, EP\ndoes not scale to visual tasks harder than MNIST. In this work, we show that a\nbias in the gradient estimate of EP, inherent in the use of finite nudging, is\nresponsible for this phenomenon and that cancelling it allows training deep\nConvNets by EP, including architectures with distinct forward and backward\nconnections. These results highlight EP as a scalable approach to compute error\ngradients in deep neural networks, thereby motivating its hardware\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:23:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Laborieux", "Axel", ""], ["Ernoult", "Maxence", ""], ["Scellier", "Benjamin", ""], ["Bengio", "Yoshua", ""], ["Grollier", "Julie", ""], ["Querlioz", "Damien", ""]]}, {"id": "2101.05537", "submitter": "Stefano Massaroli", "authors": "Stefano Massaroli, Michael Poli, Federico Califano, Jinkyoo Park,\n  Atsushi Yamashita and Hajime Asama", "title": "Optimal Energy Shaping via Neural Approximators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.NE cs.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce optimal energy shaping as an enhancement of classical\npassivity-based control methods. A promising feature of passivity theory,\nalongside stability, has traditionally been claimed to be intuitive performance\ntuning along the execution of a given task. However, a systematic approach to\nadjust performance within a passive control framework has yet to be developed,\nas each method relies on few and problem-specific practical insights. Here, we\ncast the classic energy-shaping control design process in an optimal control\nframework; once a task-dependent performance metric is defined, an optimal\nsolution is systematically obtained through an iterative procedure relying on\nneural networks and gradient-based optimization. The proposed method is\nvalidated on state-regulation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:25:58 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Massaroli", "Stefano", ""], ["Poli", "Michael", ""], ["Califano", "Federico", ""], ["Park", "Jinkyoo", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""]]}, {"id": "2101.05652", "submitter": "Gustavo de Rosa", "authors": "Gustavo H. de Rosa, Jo\\~ao Paulo Papa, Xin-She Yang", "title": "A Nature-Inspired Feature Selection Approach based on Hypercomplex\n  Information", "comments": "17 pages, 7 figures", "journal-ref": "APPLIED SOFT COMPUTING; v. 94, SEP 2020", "doi": "10.1016/j.asoc.2020.106453", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection for a given model can be transformed into an optimization\ntask. The essential idea behind it is to find the most suitable subset of\nfeatures according to some criterion. Nature-inspired optimization can mitigate\nthis problem by producing compelling yet straightforward solutions when dealing\nwith complicated fitness functions. Additionally, new mathematical\nrepresentations, such as quaternions and octonions, are being used to handle\nhigher-dimensional spaces. In this context, we are introducing a meta-heuristic\noptimization framework in a hypercomplex-based feature selection, where\nhypercomplex numbers are mapped to real-valued solutions and then transferred\nonto a boolean hypercube by a sigmoid function. The intended hypercomplex\nfeature selection is tested for several meta-heuristic algorithms and\nhypercomplex representations, achieving results comparable to some\nstate-of-the-art approaches. The good results achieved by the proposed approach\nmake it a promising tool amongst feature selection research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:05:13 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["de Rosa", "Gustavo H.", ""], ["Papa", "Jo\u00e3o Paulo", ""], ["Yang", "Xin-She", ""]]}, {"id": "2101.05848", "submitter": "Guillermo Barrios Morales", "authors": "Guillermo B. Morales, Claudio R. Mirasso and Miguel C. Soriano", "title": "Unveiling the role of plasticity rules in reservoir computing", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.05.127", "report-no": null, "categories": "nlin.AO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Computing (RC) is an appealing approach in Machine Learning that\ncombines the high computational capabilities of Recurrent Neural Networks with\na fast and easy training method. Likewise, successful implementation of\nneuro-inspired plasticity rules into RC artificial networks has boosted the\nperformance of the original models. In this manuscript, we analyze the role\nthat plasticity rules play on the changes that lead to a better performance of\nRC. To this end, we implement synaptic and non-synaptic plasticity rules in a\nparadigmatic example of RC model: the Echo State Network. Testing on nonlinear\ntime series prediction tasks, we show evidence that improved performance in all\nplastic models are linked to a decrease of the pair-wise correlations in the\nreservoir, as well as a significant increase of individual neurons ability to\nseparate similar inputs in their activity space. Here we provide new insights\non this observed improvement through the study of different stages on the\nplastic learning. From the perspective of the reservoir dynamics, optimal\nperformance is found to occur close to the so-called edge of instability. Our\nresults also show that it is possible to combine different forms of plasticity\n(namely synaptic and non-synaptic rules) to further improve the performance on\nprediction tasks, obtaining better results than those achieved with\nsingle-plasticity models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:55:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Morales", "Guillermo B.", ""], ["Mirasso", "Claudio R.", ""], ["Soriano", "Miguel C.", ""]]}, {"id": "2101.05996", "submitter": "Mengyu Chen", "authors": "Mengyu Chen", "title": "Convolutional Neural Network with Pruning Method for Handwritten Digit\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN model is a popular method for imagery analysis, so it could be utilized\nto recognize handwritten digits based on MNIST datasets. For higher recognition\naccuracy, various CNN models with different fully connected layer sizes are\nexploited to figure out the relationship between the CNN fully connected layer\nsize and the recognition accuracy. Inspired by previous pruning work, we\nperformed pruning methods of distinctiveness on CNN models and compared the\npruning performance with NN models. For better pruning performances on CNN, the\neffect of angle threshold on the pruning performance was explored. The\nevaluation results show that: for the fully connected layer size, there is a\nthreshold, so that when the layer size increases, the recognition accuracy\ngrows if the layer size smaller than the threshold, and falls if the layer size\nlarger than the threshold; the performance of pruning performed on CNN is worse\nthan on NN; as pruning angle threshold increases, the fully connected layer\nsize and the recognition accuracy decreases. This paper also shows that for CNN\nmodels trained by the MNIST dataset, they are capable of handwritten digit\nrecognition and achieve the highest recognition accuracy with fully connected\nlayer size 400. In addition, for same dataset MNIST, CNN models work better\nthan big, deep, simple NN models in a published paper.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:25:13 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Mengyu", ""]]}, {"id": "2101.06006", "submitter": "Binxu Wang", "authors": "Binxu Wang, Carlos R. Ponce", "title": "The Geometry of Deep Generative Image Models and its Applications", "comments": "25 pages, 11 figures. Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Generative adversarial networks (GANs) have emerged as a powerful\nunsupervised method to model the statistical patterns of real-world data sets,\nsuch as natural images. These networks are trained to map random inputs in\ntheir latent space to new samples representative of the learned data. However,\nthe structure of the latent space is hard to intuit due to its high\ndimensionality and the non-linearity of the generator, which limits the\nusefulness of the models. Understanding the latent space requires a way to\nidentify input codes for existing real-world images (inversion), and a way to\nidentify directions with known image transformations (interpretability). Here,\nwe use a geometric framework to address both issues simultaneously. We develop\nan architecture-agnostic method to compute the Riemannian metric of the image\nmanifold created by GANs. The eigen-decomposition of the metric isolates axes\nthat account for different levels of image variability. An empirical analysis\nof several pretrained GANs shows that image variation around each position is\nconcentrated along surprisingly few major axes (the space is highly\nanisotropic) and the directions that create this large variation are similar at\ndifferent positions in the space (the space is homogeneous). We show that many\nof the top eigenvectors correspond to interpretable transforms in the image\nspace, with a substantial part of eigenspace corresponding to minor transforms\nwhich could be compressed out. This geometric understanding unifies key\nprevious results related to GAN interpretability. We show that the use of this\nmetric allows for more efficient optimization in the latent space (e.g. GAN\ninversion) and facilitates unsupervised discovery of interpretable axes. Our\nresults illustrate that defining the geometry of the GAN image manifold can\nserve as a general framework for understanding GANs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:57:33 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:24:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wang", "Binxu", ""], ["Ponce", "Carlos R.", ""]]}, {"id": "2101.06100", "submitter": "Tiago Ferreira", "authors": "Tiago A. E. Ferreira and Marios Mattheakis and Pavlos Protopapas", "title": "A New Artificial Neuron Proposal with Trainable Simultaneous Local and\n  Global Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The activation function plays a fundamental role in the artificial neural\nnetwork learning process. However, there is no obvious choice or procedure to\ndetermine the best activation function, which depends on the problem. This\nstudy proposes a new artificial neuron, named global-local neuron, with a\ntrainable activation function composed of two components, a global and a local.\nThe global component term used here is relative to a mathematical function to\ndescribe a general feature present in all problem domain. The local component\nis a function that can represent a localized behavior, like a transient or a\nperturbation. This new neuron can define the importance of each activation\nfunction component in the learning phase. Depending on the problem, it results\nin a purely global, or purely local, or a mixed global and local activation\nfunction after the training phase. Here, the trigonometric sine function was\nemployed for the global component and the hyperbolic tangent for the local\ncomponent. The proposed neuron was tested for problems where the target was a\npurely global function, or purely local function, or a composition of two\nglobal and local functions. Two classes of test problems were investigated,\nregression problems and differential equations solving. The experimental tests\ndemonstrated the Global-Local Neuron network's superior performance, compared\nwith simple neural networks with sine or hyperbolic tangent activation\nfunction, and with a hybrid network that combines these two simple neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 13:34:49 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ferreira", "Tiago A. E.", ""], ["Mattheakis", "Marios", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "2101.06213", "submitter": "Hsing-Chung Chen", "authors": "Hsing-Chung Chen, Karisma Trinanda Putra, Jerry Chun-WeiLin", "title": "A Novel Prediction Approach for Exploring PM2.5 Spatiotemporal\n  Propagation Based on Convolutional Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report-no: HCC-2021-01", "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spread of PM2.5 pollutants that endanger health is difficult to predict\nbecause it involves many atmospheric variables. These micron particles can\nspread rapidly from their source to residential areas, increasing the risk of\nrespiratory disease if exposed for long periods. The prediction system of PM2.5\npropagation provides more detailed and accurate information as an early warning\nsystem to reduce health impacts on the community. According to the idea of\ntransformative computing, the approach we propose in this paper allows\ncomputation on the dataset obtained from massive-scale PM2.5 sensor nodes via\nwireless sensor network. In the scheme, the deep learning model is implemented\non the server nodes to extract spatiotemporal features on these datasets. This\nresearch was conducted by using dataset of air quality monitoring systems in\nTaiwan. This study presents a new model based on the convolutional recursive\nneural network to generate the prediction map. In general, the model is able to\nprovide accurate predictive results by considering the bonds among measurement\nnodes in both spatially and temporally. Therefore, the particulate pollutant\npropagation of PM2.5 could be precisely monitored by using the model we propose\nin this paper.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:00:04 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Hsing-Chung", ""], ["Putra", "Karisma Trinanda", ""], ["Chun-WeiLin", "Jerry", ""]]}, {"id": "2101.06507", "submitter": "Jia Liu", "authors": "Jia Liu and Yaochu Jin", "title": "Multi-objective Search of Robust Neural Architectures against Multiple\n  Types of Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing deep learning models are vulnerable to adversarial examples\nthat are imperceptible to humans. To address this issue, various methods have\nbeen proposed to design network architectures that are robust to one particular\ntype of adversarial attacks. It is practically impossible, however, to predict\nbeforehand which type of attacks a machine learn model may suffer from. To\naddress this challenge, we propose to search for deep neural architectures that\nare robust to five types of well-known adversarial attacks using a\nmulti-objective evolutionary algorithm. To reduce the computational cost, a\nnormalized error rate of a randomly chosen attack is calculated as the\nrobustness for each newly generated neural architecture at each generation. All\nnon-dominated network architectures obtained by the proposed method are then\nfully trained against randomly chosen adversarial attacks and tested on two\nwidely used datasets. Our experimental results demonstrate the superiority of\noptimized neural architectures found by the proposed approach over\nstate-of-the-art networks that are widely used in the literature in terms of\nthe classification accuracy under different adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 19:38:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Jia", ""], ["Jin", "Yaochu", ""]]}, {"id": "2101.06558", "submitter": "Rahul Paropkari", "authors": "Rahul Arun Paropkari, Anurag Thantharate, Cory Beard", "title": "Deep-Mobility: A Deep Learning Approach for an Efficient and Reliable 5G\n  Handover", "comments": "This paper was accepted at the 29th ICCCN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  5G cellular networks are being deployed all over the world and this\narchitecture supports ultra-dense network (UDN) deployment. Small cells have a\nvery important role in providing 5G connectivity to the end users. Exponential\nincreases in devices, data and network demands make it mandatory for the\nservice providers to manage handovers better, to cater to the services that a\nuser desire. In contrast to any traditional handover improvement scheme, we\ndevelop a 'Deep-Mobility' model by implementing a deep learning neural network\n(DLNN) to manage network mobility, utilizing in-network deep learning and\nprediction. We use network key performance indicators (KPIs) to train our model\nto analyze network traffic and handover requirements. In this method, RF signal\nconditions are continuously observed and tracked using deep learning neural\nnetworks such as the Recurrent neural network (RNN) or Long Short-Term Memory\nnetwork (LSTM) and system level inputs are also considered in conjunction, to\ntake a collective decision for a handover. We can study multiple parameters and\ninteractions between system events along with the user mobility, which would\nthen trigger a handoff in any given scenario. Here, we show the fundamental\nmodeling approach and demonstrate usefulness of our model while investigating\nimpacts and sensitivities of certain KPIs from the user equipment (UE) and\nnetwork side.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:31:37 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 01:19:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Paropkari", "Rahul Arun", ""], ["Thantharate", "Anurag", ""], ["Beard", "Cory", ""]]}, {"id": "2101.06599", "submitter": "Pan Zibin", "authors": "Pan Zibin", "title": "Performance Analysis and Improvement of Parallel Differential Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential evolution (DE) is an effective global evolutionary optimization\nalgorithm using to solve global optimization problems mainly in a continuous\ndomain. In this field, researchers pay more attention to improving the\ncapability of DE to find better global solutions, however, the computational\nperformance of DE is also a very interesting aspect especially when the problem\nscale is quite large. Firstly, this paper analyzes the design of parallel\ncomputation of DE which can easily be executed in Math Kernel Library (MKL) and\nCompute Unified Device Architecture (CUDA). Then the essence of the exponential\ncrossover operator is described and we point out that it cannot be used for\nbetter parallel computation. Later, we propose a new exponential crossover\noperator (NEC) that can be executed parallelly with MKL/CUDA. Next, the\nextended experiments show that the new crossover operator can speed up DE\ngreatly. In the end, we test the new parallel DE structure, illustrating that\nthe former is much faster.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 05:57:12 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zibin", "Pan", ""]]}, {"id": "2101.06848", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge and Jose C. Principe", "title": "Faster Convergence in Deep-Predictive-Coding Networks to Learn Deeper\n  Representations", "comments": "Submitted to IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep-predictive-coding networks (DPCNs) are hierarchical, generative models.\nThey rely on feed-forward and feed-back connections to modulate latent feature\nrepresentations of stimuli in a dynamic and context-sensitive manner. A crucial\nelement of DPCNs is a forward-backward inference procedure to uncover sparse,\ninvariant features. However, this inference is a major computational\nbottleneck. It severely limits the network depth due to learning stagnation.\nHere, we prove why this bottleneck occurs. We then propose a new\nforward-inference strategy based on accelerated proximal gradients. This\nstrategy has faster theoretical convergence guarantees than the one used for\nDPCNs. It overcomes learning stagnation. We also demonstrate that it permits\nconstructing deep and wide predictive-coding networks. Such convolutional\nnetworks implement receptive fields that capture well the entire classes of\nobjects on which the networks are trained. This improves the feature\nrepresentations compared with our lab's previous non-convolutional and\nconvolutional DPCNs. It yields unsupervised object recognition that surpass\nconvolutional autoencoders and are on par with convolutional networks trained\nin a supervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:30:13 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 07:03:20 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 21:52:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.06887", "submitter": "Dmitry Krotov", "authors": "Yuchen Liang, Chaitanya K. Ryali, Benjamin Hoover, Leopold Grinberg,\n  Saket Navlakha, Mohammed J. Zaki, Dmitry Krotov", "title": "Can a Fruit Fly Learn Word Embeddings?", "comments": "Accepted for publication at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mushroom body of the fruit fly brain is one of the best studied systems\nin neuroscience. At its core it consists of a population of Kenyon cells, which\nreceive inputs from multiple sensory modalities. These cells are inhibited by\nthe anterior paired lateral neuron, thus creating a sparse high dimensional\nrepresentation of the inputs. In this work we study a mathematical\nformalization of this network motif and apply it to learning the correlational\nstructure between words and their context in a corpus of unstructured text, a\ncommon natural language processing (NLP) task. We show that this network can\nlearn semantic representations of words and can generate both static and\ncontext-dependent word embeddings. Unlike conventional methods (e.g., BERT,\nGloVe) that use dense representations for word embedding, our algorithm encodes\nsemantic meaning of words and their context in the form of sparse binary hash\ncodes. The quality of the learned representations is evaluated on word\nsimilarity analysis, word-sense disambiguation, and document classification. It\nis shown that not only can the fruit fly network motif achieve performance\ncomparable to existing methods in NLP, but, additionally, it uses only a\nfraction of the computational resources (shorter training time and smaller\nmemory footprint).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:41:50 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 19:50:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liang", "Yuchen", ""], ["Ryali", "Chaitanya K.", ""], ["Hoover", "Benjamin", ""], ["Grinberg", "Leopold", ""], ["Navlakha", "Saket", ""], ["Zaki", "Mohammed J.", ""], ["Krotov", "Dmitry", ""]]}, {"id": "2101.07001", "submitter": "Emmanouil Angelidis", "authors": "Emmanouil Angelidis, Emanuel Buchholz, Jonathan Patrick Arreguit\n  O'Neil, Alexis Roug\\`e, Terrence Stewart, Axel von Arnim, Alois Knoll, Auke\n  Ijspeert", "title": "A Spiking Central Pattern Generator for the control of a simulated\n  lamprey robot running on SpiNNaker and Loihi neuromorphic boards", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Central Pattern Generators (CPGs) models have been long used to investigate\nboth the neural mechanisms that underlie animal locomotion as well as a tool\nfor robotic research. In this work we propose a spiking CPG neural network and\nits implementation on neuromorphic hardware as a means to control a simulated\nlamprey model. To construct our CPG model, we employ the naturally emerging\ndynamical systems that arise through the use of recurrent neural populations in\nthe Neural Engineering Framework (NEF). We define the mathematical formulation\nbehind our model, which consists of a system of coupled abstract oscillators\nmodulated by high-level signals, capable of producing a variety of output\ngaits. We show that with this mathematical formulation of the Central Pattern\nGenerator model, the model can be turned into a Spiking Neural Network (SNN)\nthat can be easily simulated with Nengo, an SNN simulator. The spiking CPG\nmodel is then used to produce the swimming gaits of a simulated lamprey robot\nmodel in various scenarios. We show that by modifying the input to the network,\nwhich can be provided by sensory information, the robot can be controlled\ndynamically in direction and pace. The proposed methodology can be generalized\nto other types of CPGs suitable for both engineering applications and\nscientific research. We test our system on two neuromorphic platforms,\nSpiNNaker and Loihi. Finally, we show that this category of spiking algorithms\nshows a promising potential to exploit the theoretical advantages of\nneuromorphic hardware in terms of energy efficiency and computational speed.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 11:04:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Angelidis", "Emmanouil", ""], ["Buchholz", "Emanuel", ""], ["O'Neil", "Jonathan Patrick Arreguit", ""], ["Roug\u00e8", "Alexis", ""], ["Stewart", "Terrence", ""], ["von Arnim", "Axel", ""], ["Knoll", "Alois", ""], ["Ijspeert", "Auke", ""]]}, {"id": "2101.07259", "submitter": "Anuraganand Sharma Dr", "authors": "Anuraganand Sharma", "title": "Guided parallelized stochastic gradient descent for delay compensation", "comments": "This is a preprint version", "journal-ref": "Applied Soft Computing (2021)", "doi": "10.1016/j.asoc.2021.107084", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Stochastic gradient descent (SGD) algorithm and its variations have been\neffectively used to optimize neural network models. However, with the rapid\ngrowth of big data and deep learning, SGD is no longer the most suitable choice\ndue to its natural behavior of sequential optimization of the error function.\nThis has led to the development of parallel SGD algorithms, such as\nasynchronous SGD (ASGD) and synchronous SGD (SSGD) to train deep neural\nnetworks. However, it introduces a high variance due to the delay in parameter\n(weight) update. We address this delay in our proposed algorithm and try to\nminimize its impact. We employed guided SGD (gSGD) that encourages consistent\nexamples to steer the convergence by compensating the unpredictable deviation\ncaused by the delay. Its convergence rate is also similar to A/SSGD, however,\nsome additional (parallel) processing is required to compensate for the delay.\nThe experimental results demonstrate that our proposed approach has been able\nto mitigate the impact of delay for the quality of classification accuracy. The\nguided approach with SSGD clearly outperforms sequential SGD and even achieves\nthe accuracy close to sequential SGD for some benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:12:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Sharma", "Anuraganand", ""]]}, {"id": "2101.07312", "submitter": "Tobias Huber", "authors": "Tobias Huber, Benedikt Limmer, Elisabeth Andr\\'e", "title": "Benchmarking Perturbation-based Saliency Maps for Explaining Atari\n  Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years saw a plethora of work on explaining complex intelligent agents.\nOne example is the development of several algorithms that generate saliency\nmaps which show how much each pixel attributed to the agents' decision.\nHowever, most evaluations of such saliency maps focus on image classification\ntasks. As far as we know, there is no work that thoroughly compares different\nsaliency maps for Deep Reinforcement Learning agents. This paper compares four\nperturbation-based approaches to create saliency maps for Deep Reinforcement\nLearning agents trained on four different Atari 2600 games. All four approaches\nwork by perturbing parts of the input and measuring how much this affects the\nagent's output. The approaches are compared using three computational metrics:\ndependence on the learned parameters of the agent (sanity checks), faithfulness\nto the agent's reasoning (input degradation), and run-time. In particular,\nduring the sanity checks we find issues with two approaches and propose a\nsolution to fix one of those issues.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:57:52 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 09:02:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huber", "Tobias", ""], ["Limmer", "Benedikt", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2101.07367", "submitter": "Luke Metz", "authors": "Luke Metz, C. Daniel Freeman, Niru Maheswaranathan, Jascha\n  Sohl-Dickstein", "title": "Training Learned Optimizers with Randomly Initialized Learned Optimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned optimizers are increasingly effective, with performance exceeding\nthat of hand designed optimizers such as Adam~\\citep{kingma2014adam} on\nspecific tasks \\citep{metz2019understanding}. Despite the potential gains\navailable, in current work the meta-training (or `outer-training') of the\nlearned optimizer is performed by a hand-designed optimizer, or by an optimizer\ntrained by a hand-designed optimizer \\citep{metz2020tasks}. We show that a\npopulation of randomly initialized learned optimizers can be used to train\nthemselves from scratch in an online fashion, without resorting to a hand\ndesigned optimizer in any part of the process. A form of population based\ntraining is used to orchestrate this self-training. Although the randomly\ninitialized optimizers initially make slow progress, as they improve they\nexperience a positive feedback loop, and become rapidly more effective at\ntraining themselves. We believe feedback loops of this type, where an optimizer\nimproves itself, will be important and powerful in the future of machine\nlearning. These methods not only provide a path towards increased performance,\nbut more importantly relieve research and engineering effort.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:07:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Metz", "Luke", ""], ["Freeman", "C. Daniel", ""], ["Maheswaranathan", "Niru", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2101.07415", "submitter": "Xingyou Song", "authors": "Xingyou Song, Krzysztof Choromanski, Jack Parker-Holder, Yunhao Tang,\n  Daiyi Peng, Deepali Jain, Wenbo Gao, Aldo Pacchiano, Tamas Sarlos, Yuxiang\n  Yang", "title": "ES-ENAS: Controller-Based Architecture Search for Evolutionary\n  Reinforcement Learning", "comments": "17 pages. This is an updated version of a previous submission which\n  can be found at arXiv:1907.06511. See\n  https://github.com/google-research/google-research/tree/master/es_enas for\n  associated code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ES-ENAS, a simple yet general evolutionary joint optimization\nprocedure by combining continuous optimization via Evolutionary Strategies (ES)\nand combinatorial optimization via Efficient NAS (ENAS) in a highly scalable\nand intuitive way. Our main insight is noticing that ES is already a highly\ndistributed algorithm involving hundreds of forward passes which can not only\nbe used for training neural network weights, but also for jointly training a\nNAS controller, both in a blackbox fashion. By doing so, we also bridge the gap\nfrom NAS research in supervised learning settings to the reinforcement learning\nscenario through this relatively simple marriage between two different yet\ncommon lines of research. We demonstrate the utility and effectiveness of our\nmethod over a large search space by training highly combinatorial neural\nnetwork architectures for RL problems in continuous control, via edge pruning\nand quantization. We also incorporate a wide variety of popular techniques from\nmodern NAS literature including multiobjective optimization along with various\ncontroller methods, to showcase their promise in the RL field and discuss\npossible extensions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:19:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 23:48:45 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Song", "Xingyou", ""], ["Choromanski", "Krzysztof", ""], ["Parker-Holder", "Jack", ""], ["Tang", "Yunhao", ""], ["Peng", "Daiyi", ""], ["Jain", "Deepali", ""], ["Gao", "Wenbo", ""], ["Pacchiano", "Aldo", ""], ["Sarlos", "Tamas", ""], ["Yang", "Yuxiang", ""]]}, {"id": "2101.07430", "submitter": "An Chen", "authors": "An Chen, Zhigang Ren, Muyi Wang, Yongsheng Liang, Hanqing Liu, Wenhao\n  Du", "title": "A Surrogate-Assisted Variable Grouping Algorithm for General Large Scale\n  Global Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem decomposition plays a vital role when applying cooperative\ncoevolution (CC) to large scale global optimization problems. However, most\nlearning-based decomposition algorithms either only apply to additively\nseparable problems or face the issue of false separability detections.\nDirecting against these limitations, this study proposes a novel decomposition\nalgorithm called surrogate-assisted variable grouping (SVG). SVG first designs\na general-separability-oriented detection criterion according to whether the\noptimum of a variable changes with other variables. This criterion is\nconsistent with the separability definition and thus endows SVG with broad\napplicability and high accuracy. To reduce the fitness evaluation requirement,\nSVG seeks the optimum of a variable with the help of a surrogate model rather\nthan the original expensive high-dimensional model. Moreover, it converts the\nvariable grouping process into a dynamic-binary-tree search one, which\nfacilitates reutilizing historical separability detection information and thus\nreducing detection times. To evaluate the performance of SVG, a suite of\nbenchmark functions with up to 2000 dimensions, including additively and\nnon-additively separable ones, were designed. Experimental results on these\nfunctions indicate that, compared with six state-of-the-art decomposition\nalgorithms, SVG possesses broader applicability and competitive efficiency.\nFurthermore, it can significantly enhance the optimization performance of CC.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:57:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chen", "An", ""], ["Ren", "Zhigang", ""], ["Wang", "Muyi", ""], ["Liang", "Yongsheng", ""], ["Liu", "Hanqing", ""], ["Du", "Wenhao", ""]]}, {"id": "2101.07529", "submitter": "Mattijs Baert", "authors": "Mattijs Baert, Sam Leroux, Pieter Simoens", "title": "Intelligent Frame Selection as a Privacy-Friendlier Alternative to Face\n  Recognition", "comments": "accepted for AAAI 2021 Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread deployment of surveillance cameras for facial recognition\ngives rise to many privacy concerns. This study proposes a privacy-friendly\nalternative to large scale facial recognition. While there are multiple\ntechniques to preserve privacy, our work is based on the minimization principle\nwhich implies minimizing the amount of collected personal data. Instead of\nrunning facial recognition software on all video data, we propose to\nautomatically extract a high quality snapshot of each detected person without\nrevealing his or her identity. This snapshot is then encrypted and access is\nonly granted after legal authorization. We introduce a novel unsupervised face\nimage quality assessment method which is used to select the high quality\nsnapshots. For this, we train a variational autoencoder on high quality face\nimages from a publicly available dataset and use the reconstruction probability\nas a metric to estimate the quality of each face crop. We experimentally\nconfirm that the reconstruction probability can be used as biometric quality\npredictor. Unlike most previous studies, we do not rely on a manually defined\nface quality metric as everything is learned from data. Our face quality\nassessment method outperforms supervised, unsupervised and general image\nquality assessment methods on the task of improving face verification\nperformance by rejecting low quality images. The effectiveness of the whole\nsystem is validated qualitatively on still images and videos.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:31:42 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 13:29:00 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Baert", "Mattijs", ""], ["Leroux", "Sam", ""], ["Simoens", "Pieter", ""]]}, {"id": "2101.07540", "submitter": "Rafael Lahoz-Beltra", "authors": "A. Gargantilla Becerra, M. Guti\\'errez, R. Lahoz-Beltra", "title": "A synthetic biology approach for the design of genetic algorithms with\n  bacterial agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bacteria have been a source of inspiration for the design of evolutionary\nalgorithms. At the beginning of the 20th century synthetic biology was born, a\ndiscipline whose goal is the design of biological systems that do not exist in\nnature, for example, programmable synthetic bacteria. In this paper, we\nintroduce as a novelty the designing of evolutionary algorithms where all the\nsteps are conducted by synthetic bacteria. To this end, we designed a genetic\nalgorithm, which we have named BAGA, illustrating its utility solving simple\ninstances of optimization problems such as function optimization, 0/1 knapsack\nproblem, Hamiltonian path problem. The results obtained open the possibility of\nconceiving evolutionary algorithms inspired by principles, mechanisms and\ngenetic circuits from synthetic biology. In summary, we can conclude that\nsynthetic biology is a source of inspiration either for the design of\nevolutionary algorithms or for some of their steps, as shown by the results\nobtained in our simulation experiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:59:33 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Becerra", "A. Gargantilla", ""], ["Guti\u00e9rrez", "M.", ""], ["Lahoz-Beltra", "R.", ""]]}, {"id": "2101.07592", "submitter": "Axel Laborieux", "authors": "Axel Laborieux, Maxence Ernoult, Tifenn Hirtzlin and Damien Querlioz", "title": "Synaptic metaplasticity in binarized neural networks", "comments": "3 pages, 1 figure", "journal-ref": "Computational and Systems Neuroscience (Cosyne) 2021", "doi": "10.1038/s41467-021-22768-y", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike the brain, artificial neural networks, including state-of-the-art deep\nneural networks for computer vision, are subject to \"catastrophic forgetting\":\nthey rapidly forget the previous task when trained on a new one. Neuroscience\nsuggests that biological synapses avoid this issue through the process of\nsynaptic consolidation and metaplasticity: the plasticity itself changes upon\nrepeated synaptic events. In this work, we show that this concept of\nmetaplasticity can be transferred to a particular type of deep neural networks,\nbinarized neural networks, to reduce catastrophic forgetting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:32:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Laborieux", "Axel", ""], ["Ernoult", "Maxence", ""], ["Hirtzlin", "Tifenn", ""], ["Querlioz", "Damien", ""]]}, {"id": "2101.07627", "submitter": "Karol Gregor", "authors": "Karol Gregor, Frederic Besse", "title": "Self-Organizing Intelligent Matter: A blueprint for an AI generating\n  algorithm", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an artificial life framework aimed at facilitating the emergence\nof intelligent organisms. In this framework there is no explicit notion of an\nagent: instead there is an environment made of atomic elements. These elements\ncontain neural operations and interact through exchanges of information and\nthrough physics-like rules contained in the environment. We discuss how an\nevolutionary process can lead to the emergence of different organisms made of\nmany such atomic elements which can coexist and thrive in the environment. We\ndiscuss how this forms the basis of a general AI generating algorithm. We\nprovide a simplified implementation of such system and discuss what advances\nneed to be made to scale it up further.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:02:54 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gregor", "Karol", ""], ["Besse", "Frederic", ""]]}, {"id": "2101.07833", "submitter": "Melikasadat Emami", "authors": "Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep\n  Rangan, Alyson K. Fletcher", "title": "Implicit Bias of Linear RNNs", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary wisdom based on empirical studies suggests that standard\nrecurrent neural networks (RNNs) do not perform well on tasks requiring\nlong-term memory. However, precise reasoning for this behavior is still\nunknown. This paper provides a rigorous explanation of this property in the\nspecial case of linear RNNs. Although this work is limited to linear RNNs, even\nthese systems have traditionally been difficult to analyze due to their\nnon-linear parameterization. Using recently-developed kernel regime analysis,\nour main result shows that linear RNNs learned from random initializations are\nfunctionally equivalent to a certain weighted 1D-convolutional network.\nImportantly, the weightings in the equivalent model cause an implicit bias to\nelements with smaller time lags in the convolution and hence, shorter memory.\nThe degree of this bias depends on the variance of the transition kernel matrix\nat initialization and is related to the classic exploding and vanishing\ngradients problem. The theory is validated in both synthetic and real data\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:39:28 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Emami", "Melikasadat", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Pandit", "Parthe", ""], ["Rangan", "Sundeep", ""], ["Fletcher", "Alyson K.", ""]]}, {"id": "2101.07864", "submitter": "Chaeun Lee", "authors": "Chaeun Lee, Seyoung Kim", "title": "SEMULATOR: Emulating the Dynamics of Crossbar Array-based Analog Neural\n  System with Regression Neural Networks", "comments": "10 pages, 7 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks require tremendous amount of computation and memory,\nanalog computing with emerging memory devices is a promising alternative to\ndigital computing for edge devices. However, because of the increasing\nsimulation time for analog computing system, it has not been explored. To\novercome this issue, analytically approximated simulators are developed, but\nthese models are inaccurate and narrow down the options for peripheral circuits\nfor multiply-accumulate operation (MAC). In this sense, we propose a\nmethodology, SEMULATOR (SiMULATOR by Emulating the analog computing block)\nwhich uses a deep neural network to emulate the behavior of crossbar-based\nanalog computing system. With the proposed neural architecture, we\nexperimentally and theoretically shows that it emulates a MAC unit for neural\ncomputation. In addition, the simulation time is incomparably reduced when it\ncompared to the circuit simulators such as SPICE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:08:33 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lee", "Chaeun", ""], ["Kim", "Seyoung", ""]]}, {"id": "2101.07868", "submitter": "Jacob Schrum", "authors": "Kirby Steckel and Jacob Schrum", "title": "Illuminating the Space of Beatable Lode Runner Levels Produced By\n  Various Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are capable of generating convincing\nimitations of elements from a training set, but the distribution of elements in\nthe training set affects to difficulty of properly training the GAN and the\nquality of the outputs it produces. This paper looks at six different GANs\ntrained on different subsets of data from the game Lode Runner. The quality\ndiversity algorithm MAP-Elites was used to explore the set of quality levels\nthat could be produced by each GAN, where quality was defined as being beatable\nand having the longest solution path possible. Interestingly, a GAN trained on\nonly 20 levels generated the largest set of diverse beatable levels while a GAN\ntrained on 150 levels generated the smallest set of diverse beatable levels,\nthus challenging the notion that more is always better when training GANs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:41:42 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Steckel", "Kirby", ""], ["Schrum", "Jacob", ""]]}, {"id": "2101.08134", "submitter": "Mohamed Abdelfattah", "authors": "Mohamed S. Abdelfattah, Abhinav Mehrotra, {\\L}ukasz Dudziak, Nicholas\n  D. Lane", "title": "Zero-Cost Proxies for Lightweight NAS", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural Architecture Search (NAS) is quickly becoming the standard methodology\nto design neural network models. However, NAS is typically compute-intensive\nbecause multiple models need to be evaluated before choosing the best one. To\nreduce the computational power and time needed, a proxy task is often used for\nevaluating each model instead of full training. In this paper, we evaluate\nconventional reduced-training proxies and quantify how well they preserve\nranking between multiple models during search when compared with the rankings\nproduced by final trained accuracy. We propose a series of zero-cost proxies,\nbased on recent pruning literature, that use just a single minibatch of\ntraining data to compute a model's score. Our zero-cost proxies use 3 orders of\nmagnitude less computation but can match and even outperform conventional\nproxies. For example, Spearman's rank correlation coefficient between final\nvalidation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82,\ncompared to 0.61 for EcoNAS (a recently proposed reduced-training proxy).\nFinally, we use these zero-cost proxies to enhance existing NAS search\nalgorithms such as random search, reinforcement learning, evolutionary search\nand predictor-based search. For all search methodologies and across three\ndifferent NAS datasets, we are able to significantly improve sample efficiency,\nand thereby decrease computation, by using our zero-cost proxies. For example\non NAS-Bench-101, we achieved the same accuracy 4$\\times$ quicker than the best\nprevious result. Our code is made public at:\nhttps://github.com/mohsaied/zero-cost-nas.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:59:52 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:43:12 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Abdelfattah", "Mohamed S.", ""], ["Mehrotra", "Abhinav", ""], ["Dudziak", "\u0141ukasz", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2101.08286", "submitter": "Matthew Colbrook", "authors": "Matthew J. Colbrook, Vegard Antun, Anders C. Hansen", "title": "Can stable and accurate neural networks be computed? -- On the barriers\n  of deep learning and Smale's 18th problem", "comments": "14 pages + SI Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA cs.NE math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning (DL) has had unprecedented success and is now entering\nscientific computing with full force. However, current DL methods typically\nsuffer from instability, even when universal approximation properties guarantee\nthe existence of stable neural networks (NNs). We address this paradox by\ndemonstrating basic well-conditioned problems in scientific computing where one\ncan prove the existence of NNs with great approximation qualities, however,\nthere does not exist any algorithm, even randomised, that can train (or\ncompute) such a NN. For any positive integers $K > 2$ and $L$, there are cases\nwhere simultaneously: (a) no randomised training algorithm can compute a NN\ncorrect to $K$ digits with probability greater than $1/2$, (b) there exists a\ndeterministic training algorithm that computes a NN with $K-1$ correct digits,\nbut any such (even randomised) algorithm needs arbitrarily many training data,\n(c) there exists a deterministic training algorithm that computes a NN with\n$K-2$ correct digits using no more than $L$ training samples. These results\nimply a classification theory describing conditions under which (stable) NNs\nwith a given accuracy can be computed by an algorithm. We begin this theory by\nestablishing sufficient conditions for the existence of algorithms that compute\nstable NNs in inverse problems. We introduce Fast Iterative REstarted NETworks\n(FIRENETs), which we both prove and numerically verify are stable. Moreover, we\nprove that only $\\mathcal{O}(|\\log(\\epsilon)|)$ layers are needed for an\n$\\epsilon$-accurate solution to the inverse problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:04:17 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:09:49 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Colbrook", "Matthew J.", ""], ["Antun", "Vegard", ""], ["Hansen", "Anders C.", ""]]}, {"id": "2101.08552", "submitter": "Yi Chen", "authors": "Yi Chen, Aimin Zhou", "title": "Variable Division and Optimization for Constrained Multiobjective\n  Portfolio Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable division and optimization (D\\&O) is a frequently utilized algorithm\ndesign paradigm in Evolutionary Algorithms (EAs). A D\\&O EA divides a variable\ninto partial variables and then optimize them respectively. A complicated\nproblem is thus divided into simple subtasks. For example, a variable of\nportfolio problem can be divided into two partial variables, i.e. the selection\nof assets and the allocation of capital. Thereby, we optimize these two partial\nvariables respectively. There is no formal discussion about how are the partial\nvariables iteratively optimized and why can it work for both single- and\nmulti-objective problems in D\\&O. In this paper, this gap is filled. According\nto the discussion, an elitist selection method for partial variables in\nmultiobjective problems is developed. Then this method is incorporated into the\nDecomposition-Based Multiobjective Evolutionary Algorithm (D\\&O-MOEA/D). With\nthe help of a mathematical programming optimizer, it is achieved on the\nconstrained multiobjective portfolio problems. In the empirical study,\nD\\&O-MOEA/D is implemented for 20 instances and recent Chinese stock markets.\nThe results show the superiority and versatility of D\\&O-MOEA/D on large-scale\ninstances while the performance of it on small-scale problems is also not bad.\nThe former targets convergence towards the Pareto front and the latter helps\npromote diversity among the non-dominated solutions during the search process.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:08:23 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chen", "Yi", ""], ["Zhou", "Aimin", ""]]}, {"id": "2101.09181", "submitter": "Namig Guliyev", "authors": "Namig J. Guliyev, Vugar E. Ismailov", "title": "Approximation capability of two hidden layer feedforward neural networks\n  with fixed weights", "comments": "13 pages, 3 figures; this article uses the algorithm from\n  arXiv:1708.06219; for associated SageMath worksheet, see\n  https://sites.google.com/site/njguliyev/papers/tlfn", "journal-ref": "Neurocomputing, 316 (2018), 262-269", "doi": "10.1016/j.neucom.2018.07.075", "report-no": null, "categories": "cs.NE cs.IT cs.NA math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We algorithmically construct a two hidden layer feedforward neural network\n(TLFN) model with the weights fixed as the unit coordinate vectors of the\n$d$-dimensional Euclidean space and having $3d+2$ number of hidden neurons in\ntotal, which can approximate any continuous $d$-variable function with an\narbitrary precision. This result, in particular, shows an advantage of the TLFN\nmodel over the single hidden layer feedforward neural network (SLFN) model,\nsince SLFNs with fixed weights do not have the capability of approximating\nmultivariate functions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:04:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Guliyev", "Namig J.", ""], ["Ismailov", "Vugar E.", ""]]}, {"id": "2101.09300", "submitter": "Yingfang Yuan", "authors": "Yingfang Yuan and Wenjun Wang and George M. Coghill and Wei Pang", "title": "A Novel Genetic Algorithm with Hierarchical Evaluation Strategy for\n  Hyperparameter Optimisation of Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph representation of structured data can facilitate the extraction of\nstereoscopic features, and it has demonstrated excellent ability when working\nwith deep learning systems, the so-called Graph Neural Networks (GNNs).\nChoosing a promising architecture for constructing GNNs can be transferred to a\nhyperparameter optimisation problem, a very challenging task due to the size of\nthe underlying search space and high computational cost for evaluating\ncandidate GNNs. To address this issue, this research presents a novel genetic\nalgorithm with a hierarchical evaluation strategy (HESGA), which combines the\nfull evaluation of GNNs with a fast evaluation approach. By using full\nevaluation, a GNN is represented by a set of hyperparameter values and trained\non a specified dataset, and root mean square error (RMSE) will be used to\nmeasure the quality of the GNN represented by the set of hyperparameter values\n(for regression problems). While in the proposed fast evaluation process, the\ntraining will be interrupted at an early stage, the difference of RMSE values\nbetween the starting and interrupted epochs will be used as a fast score, which\nimplies the potential of the GNN being considered. To coordinate both types of\nevaluations, the proposed hierarchical strategy uses the fast evaluation in a\nlower level for recommending candidates to a higher level, where the full\nevaluation will act as a final assessor to maintain a group of elite\nindividuals. To validate the effectiveness of HESGA, we apply it to optimise\ntwo types of deep graph neural networks. The experimental results on three\nbenchmark datasets demonstrate its advantages compared to Bayesian\nhyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:19:59 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 11:38:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yuan", "Yingfang", ""], ["Wang", "Wenjun", ""], ["Coghill", "George M.", ""], ["Pang", "Wei", ""]]}, {"id": "2101.09492", "submitter": "Xuecan Yang", "authors": "Xuecan Yang, Sumanta Chaudhuri, Laurence Likforman, Lirida Naviner", "title": "MinConvNets: A new class of multiplication-less Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved unprecedented success in image\nclassification, recognition, or detection applications. However, their\nlarge-scale deployment in embedded devices is still limited by the huge\ncomputational requirements, i.e., millions of MAC operations per layer. In this\narticle, MinConvNets where the multiplications in the forward propagation are\napproximated by minimum comparator operations are introduced. Hardware\nimplementation of minimum operation is much simpler than multipliers. Firstly,\na methodology to find approximate operations based on statistical correlation\nis presented. We show that it is possible to replace multipliers by minimum\noperations in the forward propagation under certain constraints, i.e. given\nsimilar mean and variances of the feature and the weight vectors. A modified\ntraining method which guarantees the above constraints is proposed. And it is\nshown that equivalent precision can be achieved during inference with\nMinConvNets by using transfer learning from well trained exact CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 12:18:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Xuecan", ""], ["Chaudhuri", "Sumanta", ""], ["Likforman", "Laurence", ""], ["Naviner", "Lirida", ""]]}, {"id": "2101.09505", "submitter": "Youngmin Kim", "authors": "Youngmin Kim, Richard Allmendinger and Manuel L\\'opez-Ib\\'a\\~nez", "title": "Safe Learning and Optimization Techniques: Towards a Survey of the State\n  of the Art", "comments": "The final authenticated publication was made In: Heintz F., Milano\n  M., O'Sullivan B. (eds) Trustworthy AI - Integrating Learning, Optimization\n  and Reasoning. TAILOR 2020. Lecture Notes in Computer Science, vol 12641.\n  Springer, Cham. The final authenticated publication is available online at\n  \\<https://doi.org/10.1007/978-3-030-73959-1_12>", "journal-ref": null, "doi": "10.1007/978-3-030-73959-1_12", "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe learning and optimization deals with learning and optimization problems\nthat avoid, as much as possible, the evaluation of non-safe input points, which\nare solutions, policies, or strategies that cause an irrecoverable loss (e.g.,\nbreakage of a machine or equipment, or life threat). Although a comprehensive\nsurvey of safe reinforcement learning algorithms was published in 2015, a\nnumber of new algorithms have been proposed thereafter, and related works in\nactive learning and in optimization were not considered. This paper reviews\nthose algorithms from a number of domains including reinforcement learning,\nGaussian process regression and classification, evolutionary algorithms, and\nactive learning. We provide the fundamental concepts on which the reviewed\nalgorithms are based and a characterization of the individual algorithms. We\nconclude by explaining how the algorithms are connected and suggestions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 13:58:09 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 13:38:59 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 18:19:08 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kim", "Youngmin", ""], ["Allmendinger", "Richard", ""], ["L\u00f3pez-Ib\u00e1\u00f1ez", "Manuel", ""]]}, {"id": "2101.09530", "submitter": "Bhumika Mistry", "authors": "Bhumika Mistry, Katayoun Farrahi and Jonathon Hare", "title": "A Primer for Neural Arithmetic Logic Modules", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Arithmetic Logic Modules have become a growing area of interest,\nthough remain a niche field. These units are small neural networks which aim to\nachieve systematic generalisation in learning arithmetic operations such as {+,\n-, *, \\} while also being interpretive in their weights. This paper is the\nfirst in discussing the current state of progress of this field, explaining key\nworks, starting with the Neural Arithmetic Logic Unit (NALU). Focusing on the\nshortcomings of NALU, we provide an in-depth analysis to reason about design\nchoices of recent units. A cross-comparison between units is made on experiment\nsetups and findings, where we highlight inconsistencies in a fundamental\nexperiment causing the inability to directly compare across papers. We finish\nby providing a novel discussion of existing applications for NALU and research\ndirections requiring further exploration.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 16:09:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mistry", "Bhumika", ""], ["Farrahi", "Katayoun", ""], ["Hare", "Jonathon", ""]]}, {"id": "2101.09556", "submitter": "Yali Wang", "authors": "Yali Wang, Steffen Limmer, Markus Olhofer, Michael Emmerich, Thomas\n  Baeck", "title": "Automatic Preference Based Multi-objective Evolutionary Algorithm on\n  Vehicle Fleet Maintenance Scheduling Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A preference based multi-objective evolutionary algorithm is proposed for\ngenerating solutions in an automatically detected knee point region. It is\nnamed Automatic Preference based DI-MOEA (AP-DI-MOEA) where DI-MOEA stands for\nDiversity-Indicator based Multi-Objective Evolutionary Algorithm). AP-DI-MOEA\nhas two main characteristics: firstly, it generates the preference region\nautomatically during the optimization; secondly, it concentrates the solution\nset in this preference region. Moreover, the real-world vehicle fleet\nmaintenance scheduling optimization (VFMSO) problem is formulated, and a\ncustomized multi-objective evolutionary algorithm (MOEA) is proposed to\noptimize maintenance schedules of vehicle fleets based on the predicted failure\ndistribution of the components of cars. Furthermore, the customized MOEA for\nVFMSO is combined with AP-DI-MOEA to find maintenance schedules in the\nautomatically generated preference region. Experimental results on\nmulti-objective benchmark problems and our three-objective real-world\napplication problems show that the newly proposed algorithm can generate the\npreference region accurately and that it can obtain better solutions in the\npreference region. Especially, in many cases, under the same budget, the Pareto\noptimal solutions obtained by AP-DI-MOEA dominate solutions obtained by MOEAs\nthat pursue the entire Pareto front.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 18:48:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Yali", ""], ["Limmer", "Steffen", ""], ["Olhofer", "Markus", ""], ["Emmerich", "Michael", ""], ["Baeck", "Thomas", ""]]}, {"id": "2101.09571", "submitter": "Vadim Liventsev", "authors": "Vadim Liventsev, Aki H\\\"arm\\\"a and Milan Petkovi\\'c", "title": "BF++: a language for general-purpose program synthesis", "comments": "8+2 pages (paper+references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most state of the art decision systems based on Reinforcement Learning (RL)\nare data-driven black-box neural models, where it is often difficult to\nincorporate expert knowledge into the models or let experts review and validate\nthe learned decision mechanisms. Knowledge-insertion and model review are\nimportant requirements in many applications involving human health and safety.\nOne way to bridge the gap between data and knowledge driven systems is program\nsynthesis: replacing a neural network that outputs decisions with a symbolic\nprogram generated by a neural network or by means of genetic programming. We\npropose a new programming language, BF++, designed specifically for automatic\nprogramming of agents in a Partially Observable Markov Decision Process (POMDP)\nsetting and apply neural program synthesis to solve standard OpenAI Gym\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 19:44:44 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 12:25:25 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 20:24:02 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 13:01:09 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liventsev", "Vadim", ""], ["H\u00e4rm\u00e4", "Aki", ""], ["Petkovi\u0107", "Milan", ""]]}, {"id": "2101.09688", "submitter": "Pasquale Minervini", "authors": "Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van\n  Breugel, Pasquale Minervini", "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and\n  Fine-tuned Language Models", "comments": "Proceedings of the 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes two intuitive metrics, skew and stereotype, that quantify\nand analyse the gender bias present in contextual language models when tackling\nthe WinoBias pronoun resolution task. We find evidence that gender stereotype\ncorrelates approximately negatively with gender skew in out-of-the-box models,\nsuggesting that there is a trade-off between these two forms of bias. We\ninvestigate two methods to mitigate bias. The first approach is an online\nmethod which is effective at removing skew at the expense of stereotype. The\nsecond, inspired by previous work on ELMo, involves the fine-tuning of BERT\nusing an augmented gender-balanced dataset. We show that this reduces both skew\nand stereotype relative to its unaugmented fine-tuned counterpart. However, we\nfind that existing gender bias benchmarks do not fully probe professional bias\nas pronoun resolution may be obfuscated by cross-correlations from other\nmanifestations of gender prejudice. Our code is available online, at\nhttps://github.com/12kleingordon34/NLP_masters_project.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 10:57:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:17:41 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Manela", "Daniel de Vassimon", ""], ["Errington", "David", ""], ["Fisher", "Thomas", ""], ["van Breugel", "Boris", ""], ["Minervini", "Pasquale", ""]]}, {"id": "2101.09709", "submitter": "Juan Pedro Dominguez-Morales", "authors": "Pablo Lopez-Osorio, Alberto Patino-Saucedo, Juan P. Dominguez-Morales,\n  Horacio Rostro-Gonzalez, Fernando Perez-Pe\\~na", "title": "Neuromorphic adaptive spiking CPG towards bio-inspired locomotion of\n  legged robots", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, locomotion mechanisms exhibited by vertebrate animals have\nbeen the inspiration for the improvement in the performance of robotic systems.\nThese mechanisms include the adaptability of their locomotion to any change\nregistered in the environment through their biological sensors. In this regard,\nwe aim to replicate such kind of adaptability in legged robots through a\nSpiking Central Pattern Generator. This Spiking Central Pattern Generator\ngenerates different locomotion (rhythmic) patterns which are driven by an\nexternal stimulus, that is, the output of a Force Sensitive Resistor connected\nto the robot to provide feedback. The Spiking Central Pattern Generator\nconsists of a network of five populations of Leaky Integrate-and-Fire neurons\ndesigned with a specific topology in such a way that the rhythmic patterns can\nbe generated and driven by the aforementioned external stimulus. Therefore, the\nlocomotion of the end robotic platform (any-legged robot) can be adapted to the\nterrain by using any sensor as input. The Spiking Central Pattern Generator\nwith adaptive learning has been numerically validated at software and hardware\nlevel, using the Brian 2 simulator and the SpiNNaker neuromorphic platform for\nthe latest. In particular, our experiments clearly show an adaptation in the\noscillation frequencies between the spikes produced in the populations of the\nSpiking Central Pattern Generator while the input stimulus varies. To validate\nthe robustness and adaptability of the Spiking Central Pattern Generator, we\nhave performed several tests by variating the output of the sensor. These\nexperiments were carried out in Brian 2 and SpiNNaker; both implementations\nshowed a similar behavior with a Pearson correlation coefficient of 0.905.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:44:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lopez-Osorio", "Pablo", ""], ["Patino-Saucedo", "Alberto", ""], ["Dominguez-Morales", "Juan P.", ""], ["Rostro-Gonzalez", "Horacio", ""], ["Perez-Pe\u00f1a", "Fernando", ""]]}, {"id": "2101.09721", "submitter": "Fabio Ferreira", "authors": "Fabio Ferreira, Thomas Nierhoff, Frank Hutter", "title": "Learning Synthetic Environments for Reinforcement Learning with\n  Evolution Strategies", "comments": null, "journal-ref": "AAAI 2021 Meta-Learning Workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores learning agent-agnostic synthetic environments (SEs) for\nReinforcement Learning. SEs act as a proxy for target environments and allow\nagents to be trained more efficiently than when directly trained on the target\nenvironment. We formulate this as a bi-level optimization problem and represent\nan SE as a neural network. By using Natural Evolution Strategies and a\npopulation of SE parameter vectors, we train agents in the inner loop on\nevolving SEs while in the outer loop we use the performance on the target task\nas a score for meta-updating the SE population. We show empirically that our\nmethod is capable of learning SEs for two discrete-action-space tasks\n(CartPole-v0 and Acrobot-v1) that allow us to train agents more robustly and\nwith up to 60% fewer steps. Not only do we show in experiments with 4000\nevaluations that the SEs are robust against hyperparameter changes such as the\nlearning rate, batch sizes and network sizes, we also show that SEs trained\nwith DDQN agents transfer in limited ways to a discrete-action-space version of\nTD3 and very well to Dueling DDQN.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 14:16:13 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 18:53:35 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 15:03:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ferreira", "Fabio", ""], ["Nierhoff", "Thomas", ""], ["Hutter", "Frank", ""]]}, {"id": "2101.09835", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente, Johann Sienz", "title": "Particle Swarm Optimization: Development of a General-Purpose Optimizer", "comments": "6th ASMO UK / ISSMO conference. Oxford, 3rd-4th July 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Traditional methods present a very restrictive range of applications, mainly\nlimited by the features of the function to be optimized and of the constraint\nfunctions. In contrast, evolutionary algorithms present almost no restriction\nto the features of these functions, although the most appropriate\nconstraint-handling technique is still an open question. The particle swarm\noptimization (PSO) method is sometimes viewed as another evolutionary algorithm\nbecause of their many similarities, despite not being inspired by the same\nmetaphor. Namely, they evolve a population of individuals taking into\nconsideration previous experiences and using stochastic operators to introduce\nnew responses. The advantages of evolutionary algorithms with respect to\ntraditional methods have been greatly discussed in the literature for decades.\nWhile all such advantages are valid when comparing the PSO paradigm to\ntraditional methods, its main advantages with respect to evolutionary\nalgorithms consist of its noticeably lower computational cost and easier\nimplementation. In fact, the plain version can be programmed in a few lines of\ncode, involving no operator design and few parameters to be tuned. This paper\ndeals with three important aspects of the method: the influence of the\nparameters' tuning on the behaviour of the system; the design of stopping\ncriteria so that the reliability of the solution found can be somehow estimated\nand computational cost can be saved; and the development of appropriate\ntechniques to handle constraints, given that the original method is designed\nfor unconstrained optimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 00:35:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.09957", "submitter": "Johannes Lederer", "authors": "Johannes Lederer", "title": "Activation Functions in Artificial Neural Networks: A Systematic\n  Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions shape the outputs of artificial neurons and, therefore,\nare integral parts of neural networks in general and deep learning in\nparticular. Some activation functions, such as logistic and relu, have been\nused for many decades. But with deep learning becoming a mainstream research\ntopic, new activation functions have mushroomed, leading to confusion in both\ntheory and practice. This paper provides an analytic yet up-to-date overview of\npopular activation functions and their properties, which makes it a timely\nresource for anyone who studies or applies neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:55:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lederer", "Johannes", ""]]}, {"id": "2101.09974", "submitter": "Mauro Innocente", "authors": "M. S. Innocente, Ll. Torres, X. Cah\\'is, G. Barbeta, A. Catal\\'an", "title": "Optimal Flexural Design of FRP-Reinforced Concrete Beams Using a\n  Particle Swarm Optimizer", "comments": "Submitted to FRPRCS-8, University of Patras, Patras, Greece, July\n  16-18, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The design of the cross-section of an FRP-reinforced concrete beam is an\niterative process of estimating both its dimensions and the reinforcement\nratio, followed by the check of the compliance of a number of strength and\nserviceability constraints. The process continues until a suitable solution is\nfound. Since there are infinite solutions to the problem, it appears convenient\nto define some optimality criteria so as to measure the relative goodness of\nthe different solutions. This paper intends to develop a preliminary least-cost\nsection design model that follows the recommendations in the ACI 440.1 R-06,\nand uses a relatively new artificial intelligence technique called particle\nswarm optimization (PSO) to handle the optimization tasks. The latter is based\non the intelligence that emerges from the low-level interactions among a number\nof relatively non-intelligent individuals within a population.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:36:47 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Innocente", "M. S.", ""], ["Torres", "Ll.", ""], ["Cah\u00eds", "X.", ""], ["Barbeta", "G.", ""], ["Catal\u00e1n", "A.", ""]]}, {"id": "2101.10115", "submitter": "Iosu Rodr\\'iguez-Mart\\'inez", "authors": "Martin Pap\\v{c}o, Iosu Rodr\\'iguez-Mart\\'inez, Javier Fumanal-Idocin,\n  Abdulrahman H. Altalhi and Humberto Bustince", "title": "A fusion method for multi-valued data", "comments": null, "journal-ref": "Information Fusion, Volume 71, 2021, Pages 1-10", "doi": "10.1016/j.inffus.2021.01.001", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we propose an extension of the notion of deviation-based\naggregation function tailored to aggregate multidimensional data. Our objective\nis both to improve the results obtained by other methods that try to select the\nbest aggregation function for a particular set of data, such as penalty\nfunctions, and to reduce the temporal complexity required by such approaches.\nWe discuss how this notion can be defined and present three illustrative\nexamples of the applicability of our new proposal in areas where temporal\nconstraints can be strict, such as image processing, deep learning and decision\nmaking, obtaining favourable results in the process.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:27:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pap\u010do", "Martin", ""], ["Rodr\u00edguez-Mart\u00ednez", "Iosu", ""], ["Fumanal-Idocin", "Javier", ""], ["Altalhi", "Abdulrahman H.", ""], ["Bustince", "Humberto", ""]]}, {"id": "2101.10326", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente, Johann Sienz", "title": "A Study of the Fundamental Parameters of Particle Swarm Optimizers", "comments": "submitted to the 7th World Congress on Structural and\n  Multidisciplinary Optimization, COEX Seoul, 21 May - 25 May 2007, Korea.\n  arXiv admin note: substantial text overlap with arXiv:2101.09835", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The range of applications of traditional optimization methods are limited by\nthe features of the object variables, and of both the objective and the\nconstraint functions. In contrast, population-based algorithms whose\noptimization capabilities are emergent properties, such as evolutionary\nalgorithms and particle swarm optimization, present almost no restriction on\nthose features and can handle different optimization problems with few or no\nadaptations. Their main drawbacks consist of their comparatively higher\ncomputational cost and difficulty in handling equality constraints. The\nparticle swarm optimization method is sometimes viewed as an evolutionary\nalgorithm because of their many similarities, despite not being inspired by the\nsame metaphor: they evolve a population of individuals taking into account\nprevious experiences and using stochastic operators to introduce new responses.\nThe advantages of evolutionary algorithms with respect to traditional methods\nhave been greatly discussed in the literature for decades. While the particle\nswarm optimizers share such advantages, their main desirable features when\ncompared to evolutionary algorithms are their lower computational cost and\neasier implementation, involving no operator design and few parameters to be\ntuned. However, even slight modifications of these parameters greatly influence\nthe dynamics of the swarm. This paper deals with the effect of the settings of\nthe parameters of the particles' velocity update equation on the behaviour of\nthe system.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:18:34 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.10427", "submitter": "Nihal Acharya Adde", "authors": "Thilo Moshagen, Nihal Acharya Adde, Ajay Navilarekal Rajgopal", "title": "Finding hidden-feature depending laws inside a data set and classifying\n  it using Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.07332", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The logcosh loss function for neural networks has been developed to combine\nthe advantage of the absolute error loss function of not overweighting outliers\nwith the advantage of the mean square error of continuous derivative near the\nmean, which makes the last phase of learning easier. It is clear, and one\nexperiences it soon, that in the case of clustered data, an artificial neural\nnetwork with logcosh loss learns the bigger cluster rather than the mean of the\ntwo. Even more so, the ANN, when used for regression of a set-valued function,\nwill learn a value close to one of the choices, in other words, one branch of\nthe set-valued function, while a mean-square-error NN will learn the value in\nbetween. This work suggests a method that uses artificial neural networks with\nlogcosh loss to find the branches of set-valued mappings in parameter-outcome\nsample sets and classifies the samples according to those branches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:37:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Moshagen", "Thilo", ""], ["Adde", "Nihal Acharya", ""], ["Rajgopal", "Ajay Navilarekal", ""]]}, {"id": "2101.10453", "submitter": "Abhilash Singh", "authors": "Abhilash Singh, Sandeep Sharma, Jitenda Singh", "title": "Nature-Inspired Algorithms for Wireless Sensor Networks: A Comprehensive\n  Survey", "comments": null, "journal-ref": "Computer Science Review (2020) 100342", "doi": "10.1016/j.cosrev.2020.100342", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to solve the critical issues in Wireless Sensor Networks (WSNs),\nwith concern for limited sensor lifetime, nature-inspired algorithms are\nemerging as a suitable method. Getting optimal network coverage is one of those\nchallenging issues that need to be examined critically before any network\nsetup. Optimal network coverage not only minimizes the consumption of limited\nenergy of battery-driven sensors but also reduce the sensing of redundant\ninformation. In this paper, we focus on nature-inspired optimization algorithms\nconcerning the optimal coverage in WSNs. In the first half of the paper, we\nhave briefly discussed the taxonomy of the optimization algorithms along with\nthe problem domains in WSNs. In the second half of the paper, we have compared\nthe performance of two nature-inspired algorithms for getting optimal coverage\nin WSNs. The first one is a combined Improved Genetic Algorithm and Binary Ant\nColony Algorithm (IGABACA), and the second one is Lion Optimization (LO). The\nsimulation results confirm that LO gives better network coverage, and the\nconvergence rate of LO is faster than that of IGA-BACA. Further, we observed\nthat the optimal coverage is achieved at a lesser number of generations in LO\nas compared to IGA-BACA. This review will help researchers to explore the\napplications in this field as well as beyond this area. Keywords: Optimal\nCoverage, Bio-inspired Algorithm, Lion Optimization, WSNs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:30:04 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Singh", "Abhilash", ""], ["Sharma", "Sandeep", ""], ["Singh", "Jitenda", ""]]}, {"id": "2101.10460", "submitter": "Brian Quanz", "authors": "Nam Nguyen, Brian Quanz", "title": "Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate\n  Time Series Forecasting", "comments": "Accepted at AAAI 2021 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting of high dimensional multivariate time series is a\nnotoriously challenging task, both in terms of computational burden and\ndistribution modeling. Most previous work either makes simple distribution\nassumptions or abandons modeling cross-series correlations. A promising line of\nwork exploits scalable matrix factorization for latent-space forecasting, but\nis limited to linear embeddings, unable to model distributions, and not\ntrainable end-to-end when using deep learning forecasting. We introduce a novel\ntemporal latent auto-encoder method which enables nonlinear factorization of\nmultivariate time series, learned end-to-end with a temporal deep learning\nlatent space forecast model. By imposing a probabilistic latent space model,\ncomplex distributions of the input series are modeled via the decoder.\nExtensive experiments demonstrate that our model achieves state-of-the-art\nperformance on many popular multivariate datasets, with gains sometimes as high\nas $50\\%$ for several standard metrics.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:29:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Nguyen", "Nam", ""], ["Quanz", "Brian", ""]]}, {"id": "2101.10483", "submitter": "EPTCS", "authors": "Toby St Clere Smithe (University of Oxford)", "title": "Cyber Kittens, or Some First Steps Towards Categorical Cybernetics", "comments": "In Proceedings ACT 2020, arXiv:2101.07888. Includes a summary of\n  arXiv:2006.01631", "journal-ref": "EPTCS 333, 2021, pp. 108-124", "doi": "10.4204/EPTCS.333.8", "report-no": null, "categories": "cs.NE cs.GT math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a categorical notion of cybernetic system as a dynamical\nrealisation of a generalized open game, along with a coherence condition. We\nshow that this notion captures a wide class of cybernetic systems in\ncomputational neuroscience and statistical machine learning, exposes their\ncompositional structure, and gives an abstract justification for the\nbidirectional structure empirically observed in cortical circuits. Our\nconstruction is built on the observation that Bayesian updates compose\noptically, a fact which we prove along the way, via a fibred category of\nstate-dependent stochastic channels.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:04:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Smithe", "Toby St Clere", "", "University of Oxford"]]}, {"id": "2101.10901", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente", "title": "Population-Based Methods: PARTICLE SWARM OPTIMIZATION -- Development of\n  a General-Purpose Optimizer and Applications", "comments": "MSc Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This thesis is concerned with continuous, static, and single-objective\noptimization problems subject to inequality constraints. Nevertheless, some\nmethods to handle other kinds of problems are briefly reviewed. The particle\nswarm optimization paradigm was inspired by previous simulations of the\ncooperative behaviour observed in social beings. It is a bottom-up, randomly\nweighted, population-based method whose ability to optimize emerges from local,\nindividual-to-individual interactions. As opposed to traditional methods, it\ncan deal with different problems with few or no adaptation due to the fact that\nit does profit from problem-specific features of the problem at issue but\nperforms a parallel, cooperative exploration of the search-space by means of a\npopulation of individuals. The main goal of this thesis consists of developing\nan optimizer that can perform reasonably well on most problems. Hence, the\ninfluence of the settings of the algorithm's parameters on the behaviour of the\nsystem is studied, some general-purpose settings are sought, and some\nvariations to the canonical version are proposed aiming to turn it into a more\ngeneral-purpose optimizer. Since no termination condition is included in the\ncanonical version, this thesis is also concerned with the design of some\nstopping criteria which allow the iterative search to be terminated if further\nsignificant improvement is unlikely, or if a certain number of time-steps are\nreached. In addition, some constraint-handling techniques are incorporated into\nthe canonical algorithm to handle inequality constraints. Finally, the\ncapabilities of the proposed general-purpose optimizers are illustrated by\noptimizing a few benchmark problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:36:25 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Innocente", "Mauro S.", ""]]}, {"id": "2101.10933", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente, Johann Sienz", "title": "Constraint-Handling Techniques for Particle Swarm Optimization\n  Algorithms", "comments": "Preprint submitted to the 7th ASMO UK Conference on Engineering\n  Design Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Population-based methods can cope with a variety of different problems,\nincluding problems of remarkably higher complexity than those traditional\nmethods can handle. The main procedure consists of successively updating a\npopulation of candidate solutions, performing a parallel exploration instead of\ntraditional sequential exploration. While the origins of the PSO method are\nlinked to bird flock simulations, it is a stochastic optimization method in the\nsense that it relies on random coefficients to introduce creativity, and a\nbottom-up artificial intelligence-based approach in the sense that its\nintelligent behaviour emerges in a higher level than the individuals' rather\nthan deterministically programmed. As opposed to EAs, the PSO involves no\noperator design and few coefficients to be tuned. Since this paper does not\nintend to study such tuning, general-purpose settings are taken from previous\nstudies. The PSO algorithm requires the incorporation of some technique to\nhandle constraints. A popular one is the penalization method, which turns the\noriginal constrained problem into unconstrained by penalizing infeasible\nsolutions. Other techniques can be specifically designed for PSO. Since these\nstrategies present advantages and disadvantages when compared to one another,\nthere is no obvious best constraint-handling technique (CHT) for all problems.\nThe aim here is to develop and compare different CHTs suitable for PSOs, which\nare incorporated to an algorithm with general-purpose settings. The comparisons\nare performed keeping the remaining features of the algorithm the same, while\ncomparisons to other authors' results are offered as a frame of reference for\nthe optimizer as a whole. Thus, the penalization, preserving feasibility and\nbisection methods are discussed, implemented, and tested on two suites of\nbenchmark problems. Three neighbourhood sizes are also considered in the\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:49:10 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.10935", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente, Johann Sienz", "title": "Numerical Comparison of Neighbourhood Topologies in Particle Swarm\n  Optimization", "comments": "Preprint submitted to the 8th ASMO UK Conference on Engineering\n  Design Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Particle Swarm Optimization is a global optimizer in the sense that it has\nthe ability to escape poor local optima. However, if the spread of information\nwithin the population is not adequately performed, premature convergence may\noccur. The convergence speed and hence the reluctance of the algorithm to\ngetting trapped in suboptimal solutions are controlled by the settings of the\ncoefficients in the velocity update equation as well as by the neighbourhood\ntopology. The coefficients settings govern the trajectories of the particles\ntowards the good locations identified, whereas the neighbourhood topology\ncontrols the form and speed of spread of information within the population\n(i.e. the update of the social attractor). Numerous neighbourhood topologies\nhave been proposed and implemented in the literature. This paper offers a\nnumerical comparison of the performances exhibited by five different\nneighbourhood topologies combined with four different coefficients' settings\nwhen optimizing a set of benchmark unconstrained problems. Despite the optimum\ntopology being problem-dependent, it appears that dynamic neighbourhoods with\nthe number of interconnections increasing as the search progresses should be\npreferred for a non-problem-specific optimizer.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:23:55 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.10936", "submitter": "Mauro Innocente", "authors": "Carwyn Pelley, Mauro S. Innocente, Johann Sienz", "title": "Combining Particle Swarm Optimizer with SQP Local Search for Constrained\n  Optimization Problems", "comments": "Preprint submitted to the 8th ASMO UK Conference on Engineering\n  Design Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The combining of a General-Purpose Particle Swarm Optimizer (GP-PSO) with\nSequential Quadratic Programming (SQP) algorithm for constrained optimization\nproblems has been shown to be highly beneficial to the refinement, and in some\ncases, the success of finding a global optimum solution. It is shown that the\nlikely difference between leading algorithms are in their local search ability.\nA comparison with other leading optimizers on the tested benchmark suite,\nindicate the hybrid GP-PSO with implemented local search to compete along side\nother leading PSO algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:34:52 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Pelley", "Carwyn", ""], ["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.11096", "submitter": "Mauro Innocente", "authors": "Johann Sienz, Mauro S. Innocente", "title": "Particle Swarm Optimization: Fundamental Study and its Application to\n  Optimization and to Jetty Scheduling Problems", "comments": "Preprint submitted to Trends in Engineering Computational Technology.\n  arXiv admin note: text overlap with arXiv:2101.10933", "journal-ref": null, "doi": "10.4203/csets.20.6", "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The advantages of evolutionary algorithms with respect to traditional methods\nhave been greatly discussed in the literature. While particle swarm optimizers\nshare such advantages, they outperform evolutionary algorithms in that they\nrequire lower computational cost and easier implementation, involving no\noperator design and few coefficients to be tuned. However, even marginal\nvariations in the settings of these coefficients greatly influence the dynamics\nof the swarm. Since this paper does not intend to study their tuning,\ngeneral-purpose settings are taken from previous studies, and virtually the\nsame algorithm is used to optimize a variety of notably different problems.\nThus, following a review of the paradigm, the algorithm is tested on a set of\nbenchmark functions and engineering problems taken from the literature. Later,\ncomplementary lines of code are incorporated to adapt the method to\ncombinatorial optimization as it occurs in scheduling problems, and a real case\nis solved using the same optimizer with the same settings. The aim is to show\nthe flexibility and robustness of the approach, which can handle a wide variety\nof problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:06:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Sienz", "Johann", ""], ["Innocente", "Mauro S.", ""]]}, {"id": "2101.11186", "submitter": "Junjie Li", "authors": "Junjie Li, Junwei Zhang, Xiaoyu Gong, Shuai L\\\"u", "title": "Evolutionary Generative Adversarial Networks with Crossover Based\n  Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) is an adversarial model, and it has\nbeen demonstrated to be effective for various generative tasks. However, GAN\nand its variants also suffer from many training problems, such as mode collapse\nand gradient vanish. In this paper, we firstly propose a general crossover\noperator, which can be widely applied to GANs using evolutionary strategies.\nThen we design an evolutionary GAN framework C-GAN based on it. And we combine\nthe crossover operator with evolutionary generative adversarial networks (EGAN)\nto implement the evolutionary generative adversarial networks with crossover\n(CE-GAN). Under the premise that a variety of loss functions are used as\nmutation operators to generate mutation individuals, we evaluate the generated\nsamples and allow the mutation individuals to learn experiences from the output\nin a knowledge distillation manner, imitating the best output outcome,\nresulting in better offspring. Then, we greedily selected the best offspring as\nparents for subsequent training using discriminator as evaluator. Experiments\non real datasets demonstrate the effectiveness of CE-GAN and show that our\nmethod is competitive in terms of generated images quality and time efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:24:30 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:26:35 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Junjie", ""], ["Zhang", "Junwei", ""], ["Gong", "Xiaoyu", ""], ["L\u00fc", "Shuai", ""]]}, {"id": "2101.11249", "submitter": "Sai Sukruth Bezugam", "authors": "Sai Sukruth Bezugam, Swatilekha Majumdar, Chetan Ralekar and Tapan\n  Kumar Gandhi", "title": "Efficient Video Summarization Framework using EEG and Eye-tracking\n  Signals", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient video summarization framework that will give\na gist of the entire video in a few key-frames or video skims. Existing video\nsummarization frameworks are based on algorithms that utilize computer vision\nlow-level feature extraction or high-level domain level extraction. However,\nbeing the ultimate user of the summarized video, humans remain the most\nneglected aspect. Therefore, the proposed paper considers human's role in\nsummarization and introduces human visual attention-based summarization\ntechniques. To understand human attention behavior, we have designed and\nperformed experiments with human participants using electroencephalogram (EEG)\nand eye-tracking technology. The EEG and eye-tracking data obtained from the\nexperimentation are processed simultaneously and used to segment frames\ncontaining useful information from a considerable video volume. Thus, the frame\nsegmentation primarily relies on the cognitive judgments of human beings. Using\nour approach, a video is summarized by 96.5% while maintaining higher precision\nand high recall factors. The comparison with the state-of-the-art techniques\ndemonstrates that the proposed approach yields ceiling-level performance with\nreduced computational cost in summarising the videos.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:13:19 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bezugam", "Sai Sukruth", ""], ["Majumdar", "Swatilekha", ""], ["Ralekar", "Chetan", ""], ["Gandhi", "Tapan Kumar", ""]]}, {"id": "2101.11275", "submitter": "Yang Yu", "authors": "Yang Yu, Shangce Gao, Yirui Wang, Jiujun Cheng and Yuki Todo", "title": "ASBSO: An Improved Brain Storm Optimization With Flexible Search Length\n  and Memory-Based Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain storm optimization (BSO) is a newly proposed population-based\noptimization algorithm, which uses a logarithmic sigmoid transfer function to\nadjust its search range during the convergent process. However, this adjustment\nonly varies with the current iteration number and lacks of flexibility and\nvariety which makes a poor search effciency and robustness of BSO. To alleviate\nthis problem, an adaptive step length structure together with a success memory\nselection strategy is proposed to be incorporated into BSO. This proposed\nmethod, adaptive step length based on memory selection BSO, namely ASBSO,\napplies multiple step lengths to modify the generation process of new\nsolutions, thus supplying a flexible search according to corresponding problems\nand convergent periods. The novel memory mechanism, which is capable of\nevaluating and storing the degree of improvements of solutions, is used to\ndetermine the selection possibility of step lengths. A set of 57 benchmark\nfunctions are used to test ASBSO's search ability, and four real-world problems\nare adopted to show its application value. All these test results indicate the\nremarkable improvement in solution quality, scalability, and robustness of\nASBSO.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:11:41 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 03:08:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yu", "Yang", ""], ["Gao", "Shangce", ""], ["Wang", "Yirui", ""], ["Cheng", "Jiujun", ""], ["Todo", "Yuki", ""]]}, {"id": "2101.11439", "submitter": "Mauro Innocente", "authors": "Johann Sienz, Mauro S. Innocente", "title": "Individual and Social Behaviour in Particle Swarm Optimizers", "comments": "Preprint submitted to Developments and Applications in Engineering\n  Computational Technology", "journal-ref": null, "doi": "10.4203/csets.26.10", "report-no": null, "categories": "physics.soc-ph cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Three basic factors govern the individual behaviour of a particle: the\ninertia from its previous displacement; the attraction to its own best\nexperience; and the attraction to a given neighbour's best experience. The\nimportance awarded to each factor is controlled by three coefficients: the\ninertia; the individuality; and the sociality weights. The social behaviour is\nruled by the structure of the social network, which defines the neighbours that\nare to inform of their experiences to a given particle. This paper presents a\nstudy of the influence of different settings of the coefficients as well as of\nthe combined effect of different settings and different neighbourhood\ntopologies on the speed and form of convergence.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:56:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Sienz", "Johann", ""], ["Innocente", "Mauro S.", ""]]}, {"id": "2101.11441", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente, Johann Sienz", "title": "Pseudo-Adaptive Penalization to Handle Constraints in Particle Swarm\n  Optimizers", "comments": "Preprint submitted to Proceedings of the tenth International\n  Conference on Computational Structures Technology", "journal-ref": null, "doi": "10.4203/ccp.93.123", "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The penalization method is a popular technique to provide particle swarm\noptimizers with the ability to handle constraints. The downside is the need of\npenalization coefficients whose settings are problem-specific. While adaptive\ncoefficients can be found in the literature, a different adaptive scheme is\nproposed in this paper, where coefficients are kept constant. A pseudo-adaptive\nrelaxation of the tolerances for constraint violations while penalizing only\nviolations beyond such tolerances results in a pseudo-adaptive penalization. A\nparticle swarm optimizer is tested on a suite of benchmark problems for three\ntypes of tolerance relaxation: no relaxation; self-tuned initial relaxation\nwith deterministic decrease; and self-tuned initial relaxation with\npseudo-adaptive decrease. Other authors' results are offered as frames of\nreference.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:09:48 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.11717", "submitter": "Francois Malgouyres", "authors": "Adrien Gauffriau, Fran\\c{c}ois Malgouyres (IMT), M\\'elanie Ducoffe", "title": "Overestimation learning with guarantees", "comments": null, "journal-ref": "AAAI-21, workshop on safeAI, Feb 2021, Valence (Virtual), Spain", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a complete method that learns a neural network which is\nguaranteed to overestimate a reference function on a given domain. The neural\nnetwork can then be used as a surrogate for the reference function. The method\ninvolves two steps. In the first step, we construct an adaptive set of Majoring\nPoints. In the second step, we optimize a well-chosen neural network to\noverestimate the Majoring Points. In order to extend the guarantee on the\nMajoring Points to the whole domain, we necessarily have to make an assumption\non the reference function. In this study, we assume that the reference function\nis monotonic. We provide experiments on synthetic and real problems. The\nexperiments show that the density of the Majoring Points concentrate where the\nreference function varies. The learned over-estimations are both guaranteed to\noverestimate the reference function and are proven empirically to provide good\napproximations of it. Experiments on real data show that the method makes it\npossible to use the surrogate function in embedded systems for which an\nunderestimation is critical; when computing the reference function requires too\nmany resources.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:06:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Gauffriau", "Adrien", "", "IMT"], ["Malgouyres", "Fran\u00e7ois", "", "IMT"], ["Ducoffe", "M\u00e9lanie", ""]]}, {"id": "2101.11883", "submitter": "Vojtech Mrazek", "authors": "Michal Pinos and Vojtech Mrazek and Lukas Sekanina", "title": "Evolutionary Neural Architecture Search Supporting Approximate\n  Multipliers", "comments": "Accepted for publication at 24th European Conference on Genetic\n  Programming (EuroGP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in automated neural architecture search (NAS)\nmethods. They are employed to routinely deliver high-quality neural network\narchitectures for various challenging data sets and reduce the designer's\neffort. The NAS methods utilizing multi-objective evolutionary algorithms are\nespecially useful when the objective is not only to minimize the network error\nbut also to minimize the number of parameters (weights) or power consumption of\nthe inference phase. We propose a multi-objective NAS method based on Cartesian\ngenetic programming for evolving convolutional neural networks (CNN). The\nmethod allows approximate operations to be used in CNNs to reduce the power\nconsumption of a target hardware implementation. During the NAS process, a\nsuitable CNN architecture is evolved together with approximate multipliers to\ndeliver the best trade-offs between the accuracy, network size, and power\nconsumption. The most suitable approximate multipliers are automatically\nselected from a library of approximate multipliers. Evolved CNNs are compared\nwith common human-created CNNs of a similar complexity on the CIFAR-10\nbenchmark problem.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:26:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Pinos", "Michal", ""], ["Mrazek", "Vojtech", ""], ["Sekanina", "Lukas", ""]]}, {"id": "2101.11944", "submitter": "Mauro Innocente", "authors": "Mauro S. Innocente, Johann Sienz", "title": "Coefficients' Settings in Particle Swarm Optimization: Insight and\n  Guidelines", "comments": "Preprint submitted to E. Dvorkin, M. Goldschmit, & M. Storti (Eds.),\n  Mec\\'anica Computacional: Computational Intelligence Techniques for\n  Optimization and Data Modeling (B) (Vol. XXIX, pp. 9253-9269). Asociaci\\'on\n  Argentina de Mec\\'anica Computacional, Buenos Aires, Argentina, 2010. Open\n  access published version here:\n  https://cimec.org.ar/ojs/index.php/mc/article/view/3666", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Particle Swam Optimization is a population-based and gradient-free\noptimization method developed by mimicking social behaviour observed in nature.\nIts ability to optimize is not specifically implemented but emerges in the\nglobal level from local interactions. In its canonical version, there are three\nfactors that govern a particle's trajectory: 1) inertia from its previous\ndisplacement; 2) attraction to its best experience; and 3) attraction to a\ngiven neighbour's best experience. The importance given to each of these\nfactors is regulated by three coefficients: 1) the inertia; 2) the\nindividuality; and 3) the sociality weights. Their settings rule the trajectory\nof the particle when pulled by these two attractors. Different speeds and forms\nof convergence of a particle towards its attractor(s) take place for different\nsettings of the coefficients. A more general formulation is presented aiming\nfor a better control of the embedded randomness. Guidelines to select the\ncoefficients' settings to obtain the desired behaviour are offered. The\nconvergence speed of the algorithm also depends on the speed of spread of\ninformation within the swarm. The latter is governed by the structure of the\nneighbourhood, whose study is beyond the scope of this paper. The objective\nhere is to help understand the core of the PSO paradigm from the bottom up by\noffering some insight into the form of the particles' trajectories, and to\nprovide some guidelines as to how to decide upon the settings of the\ncoefficients in the particles' velocity update equation in the proposed\nformulation to obtain the type of behaviour desired for the problem at hand.\nGeneral-purpose settings are also suggested. The relationship between the\nproposed formulation and both the classical and constricted PSO formulations\nare also provided.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:49:45 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Innocente", "Mauro S.", ""], ["Sienz", "Johann", ""]]}, {"id": "2101.12037", "submitter": "Demetres Kostas", "authors": "Demetres Kostas, Stephane Aroca-Ouellette, Frank Rudzicz", "title": "BENDR: using transformers and a contrastive self-supervised learning\n  task to learn from massive amounts of EEG data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) used for brain-computer-interface (BCI)\nclassification are commonly expected to learn general features when trained\nacross a variety of contexts, such that these features could be fine-tuned to\nspecific contexts. While some success is found in such an approach, we suggest\nthat this interpretation is limited and an alternative would better leverage\nthe newly (publicly) available massive EEG datasets. We consider how to adapt\ntechniques and architectures used for language modelling (LM), that appear\ncapable of ingesting awesome amounts of data, towards the development of\nencephalography modelling (EM) with DNNs in the same vein. We specifically\nadapt an approach effectively used for automatic speech recognition, which\nsimilarly (to LMs) uses a self-supervised training objective to learn\ncompressed representations of raw data signals. After adaptation to EEG, we\nfind that a single pre-trained model is capable of modelling completely novel\nraw EEG sequences recorded with differing hardware, and different subjects\nperforming different tasks. Furthermore, both the internal representations of\nthis model and the entire architecture can be fine-tuned to a variety of\ndownstream BCI and EEG classification tasks, outperforming prior work in more\ntask-specific (sleep stage classification) self-supervision.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:54:01 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kostas", "Demetres", ""], ["Aroca-Ouellette", "Stephane", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2101.12054", "submitter": "Amirhossein Rajabi", "authors": "Amirhossein Rajabi and Carsten Witt", "title": "Stagnation Detection with Randomized Local Search", "comments": "24 pages. Full version of a paper appearing at EvoCOP 2021", "journal-ref": null, "doi": "10.1007/978-3-030-72904-2_10", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a mechanism called stagnation detection was proposed that\nautomatically adjusts the mutation rate of evolutionary algorithms when they\nencounter local optima. The so-called $SD-(1+1)EA$ introduced by Rajabi and\nWitt (GECCO 2020) adds stagnation detection to the classical $(1+1)EA$ with\nstandard bit mutation, which flips each bit independently with some mutation\nrate, and raises the mutation rate when the algorithm is likely to have\nencountered local optima.\n  In this paper, we investigate stagnation detection in the context of the\n$k$-bit flip operator of randomized local search that flips $k$ bits chosen\nuniformly at random and let stagnation detection adjust the parameter $k$. We\nobtain improved runtime results compared to the $SD-(1+1)EA$ amounting to a\nspeed-up of up to $e=2.71\\dots$ Moreover, we propose additional schemes that\nprevent infinite optimization times even if the algorithm misses a working\nchoice of $k$ due to unlucky events. Finally, we present an example where\nstandard bit mutation still outperforms the local $k$-bit flip with stagnation\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:11:32 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:29:08 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rajabi", "Amirhossein", ""], ["Witt", "Carsten", ""]]}, {"id": "2101.12083", "submitter": "Tao Fang", "authors": "Tao Fang, Yu Qi and Gang Pan", "title": "Reconstructing Perceptive Images from Brain Activity by Shape-Semantic\n  GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing seeing images from fMRI recordings is an absorbing research\narea in neuroscience and provides a potential brain-reading technology. The\nchallenge lies in that visual encoding in brain is highly complex and not fully\nrevealed. Inspired by the theory that visual features are hierarchically\nrepresented in cortex, we propose to break the complex visual signals into\nmulti-level components and decode each component separately. Specifically, we\ndecode shape and semantic representations from the lower and higher visual\ncortex respectively, and merge the shape and semantic information to images by\na generative adversarial network (Shape-Semantic GAN). This 'divide and\nconquer' strategy captures visual information more accurately. Experiments\ndemonstrate that Shape-Semantic GAN improves the reconstruction similarity and\nimage quality, and achieves the state-of-the-art image reconstruction\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:04:17 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Fang", "Tao", ""], ["Qi", "Yu", ""], ["Pan", "Gang", ""]]}, {"id": "2101.12700", "submitter": "Matthew Dale", "authors": "Matthew Dale, Richard F. L. Evans, Sarah Jenkins, Simon O'Keefe,\n  Angelika Sebald, Susan Stepney, Fernando Torre, Martin Trefzer", "title": "Reservoir Computing with Thin-film Ferromagnetic Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.mtrl-sci cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in artificial intelligence are driven by technologies inspired by\nthe brain, but these technologies are orders of magnitude less powerful and\nenergy efficient than biological systems. Inspired by the nonlinear dynamics of\nneural networks, new unconventional computing hardware has emerged with the\npotential for extreme parallelism and ultra-low power consumption. Physical\nreservoir computing demonstrates this with a variety of unconventional systems\nfrom optical-based to spintronic. Reservoir computers provide a nonlinear\nprojection of the task input into a high-dimensional feature space by\nexploiting the system's internal dynamics. A trained readout layer then\ncombines features to perform tasks, such as pattern recognition and time-series\nanalysis. Despite progress, achieving state-of-the-art performance without\nexternal signal processing to the reservoir remains challenging. Here we show,\nthrough simulation, that magnetic materials in thin-film geometries can realise\nreservoir computers with greater than or similar accuracy to digital recurrent\nneural networks. Our results reveal that basic spin properties of magnetic\nfilms generate the required nonlinear dynamics and memory to solve machine\nlearning tasks. Furthermore, we show that neuromorphic hardware can be reduced\nin size by removing the need for discrete neural components and external\nprocessing. The natural dynamics and nanoscale size of magnetic thin-films\npresent a new path towards fast energy-efficient computing with the potential\nto innovate portable smart devices, self driving vehicles, and robotics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:37:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Dale", "Matthew", ""], ["Evans", "Richard F. L.", ""], ["Jenkins", "Sarah", ""], ["O'Keefe", "Simon", ""], ["Sebald", "Angelika", ""], ["Stepney", "Susan", ""], ["Torre", "Fernando", ""], ["Trefzer", "Martin", ""]]}]