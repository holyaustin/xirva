[{"id": "1710.00175", "submitter": "Zhi-Zhong Liu", "authors": "Zhi-Zhong Liu, Yong Wang and Pei-Qiu Huang", "title": "A Many-Objective Evolutionary Algorithm with Angle-Based Selection and\n  Shift-Based Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary many-objective optimization has been gaining increasing\nattention from the evolutionary computation research community. Much effort has\nbeen devoted to addressing this issue by improving the scalability of\nmultiobjective evolutionary algorithms, such as Pareto-based,\ndecomposition-based, and indicator-based approaches. Different from current\nwork, we propose a novel algorithm in this paper called AnD, which consists of\nan angle-based selection strategy and a shift-based density estimation\nstrategy. These two strategies are employed in the environmental selection to\ndelete the poor individuals one by one. Specifically, the former is devised to\nfind a pair of individuals with the minimum vector angle, which means that\nthese two individuals share the most similar search direction. The latter,\nwhich takes both the diversity and convergence into account, is adopted to\ncompare these two individuals and to delete the worse one. AnD has a simple\nstructure, few parameters, and no complicated operators. The performance of AnD\nis compared with that of seven state-of-the-art many-objective evolutionary\nalgorithms on a variety of benchmark test problems with up to 15 objectives.\nThe experimental results suggest that AnD can achieve highly competitive\nperformance. In addition, we also verify that AnD can be readily extended to\nsolve constrained many-objective optimization problems.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 10:15:16 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Liu", "Zhi-Zhong", ""], ["Wang", "Yong", ""], ["Huang", "Pei-Qiu", ""]]}, {"id": "1710.00486", "submitter": "Divya Gopinath", "authors": "Divya Gopinath, Guy Katz, Corina S. Pasareanu, Clark Barrett", "title": "DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become widely used, obtaining remarkable results in\ndomains such as computer vision, speech recognition, natural language\nprocessing, audio recognition, social network filtering, machine translation,\nand bio-informatics, where they have produced results comparable to human\nexperts. However, these networks can be easily fooled by adversarial\nperturbations: minimal changes to correctly-classified inputs, that cause the\nnetwork to mis-classify them. This phenomenon represents a concern for both\nsafety and security, but it is currently unclear how to measure a network's\nrobustness against such perturbations. Existing techniques are limited to\nchecking robustness around a few individual input points, providing only very\nlimited guarantees. We propose a novel approach for automatically identifying\nsafe regions of the input space, within which the network is robust against\nadversarial perturbations. The approach is data-guided, relying on clustering\nto identify well-defined geometric regions as candidate safe regions. We then\nutilize verification techniques to confirm that these regions are safe or to\nprovide counter-examples showing that they are not safe. We also introduce the\nnotion of targeted robustness which, for a given target label and region,\nensures that a NN does not map any input in the region to the target label. We\nevaluated our technique on the MNIST dataset and on a neural network\nimplementation of a controller for the next-generation Airborne Collision\nAvoidance System for unmanned aircraft (ACAS Xu). For these networks, our\napproach identified multiple regions which were completely safe as well as some\nwhich were only safe for specific labels. It also discovered several\nadversarial perturbations of interest.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 05:09:52 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 19:29:12 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Gopinath", "Divya", ""], ["Katz", "Guy", ""], ["Pasareanu", "Corina S.", ""], ["Barrett", "Clark", ""]]}, {"id": "1710.00489", "submitter": "Arunkumar Byravan", "authors": "Arunkumar Byravan, Felix Leeb, Franziska Meier and Dieter Fox", "title": "SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning\n  and Control", "comments": "8 pages, Initial submission to IEEE International Conference on\n  Robotics and Automation (ICRA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an approach to deep visuomotor control using\nstructured deep dynamics models. Our deep dynamics model, a variant of\nSE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an\nencoder-decoder structure. Unlike prior work, our dynamics model is structured:\ngiven an input scene, our network explicitly learns to segment salient parts\nand predict their pose-embedding along with their motion modeled as a change in\nthe pose space due to the applied actions. We train our model using a pair of\npoint clouds separated by an action and show that given supervision only in the\nform of point-wise data associations between the frames our network is able to\nlearn a meaningful segmentation of the scene along with consistent poses. We\nfurther show that our model can be used for closed-loop control directly in the\nlearned low-dimensional pose space, where the actions are computed by\nminimizing error in the pose space using gradient-based methods, similar to\ntraditional model-based control. We present results on controlling a Baxter\nrobot from raw depth data in simulation and in the real world and compare\nagainst two baseline deep networks. Our method runs in real-time, achieves good\nprediction of scene dynamics and outperforms the baseline methods on multiple\ncontrol runs. Video results can be found at:\nhttps://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 05:18:12 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Byravan", "Arunkumar", ""], ["Leeb", "Felix", ""], ["Meier", "Franziska", ""], ["Fox", "Dieter", ""]]}, {"id": "1710.00641", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Philemon Brakel, Maurizio Omologo, Yoshua Bengio", "title": "Improving speech recognition by revising gated recurrent units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition is largely taking advantage of deep learning, showing that\nsubstantial benefits can be obtained by modern Recurrent Neural Networks\n(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which\ntypically reach state-of-the-art performance in many tasks thanks to their\nability to learn long-term dependencies and robustness to vanishing gradients.\nNevertheless, LSTMs have a rather complex design with three multiplicative\ngates, that might impair their efficient implementation. An attempt to simplify\nLSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just\ntwo multiplicative gates.\n  This paper builds on these efforts by further revising GRUs and proposing a\nsimplified architecture potentially more suitable for speech recognition. The\ncontribution of this work is two-fold. First, we suggest to remove the reset\ngate in the GRU design, resulting in a more efficient single-gate architecture.\nSecond, we propose to replace tanh with ReLU activations in the state update\nequations. Results show that, in our implementation, the revised architecture\nreduces the per-epoch training time with more than 30% and consistently\nimproves recognition performance across different tasks, input features, and\nnoisy conditions when compared to a standard GRU.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 12:40:50 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Brakel", "Philemon", ""], ["Omologo", "Maurizio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.00811", "submitter": "Aaron Tuor", "authors": "Aaron Tuor, Samuel Kaplan, Brian Hutchinson, Nicole Nichols, Sean\n  Robinson", "title": "Deep Learning for Unsupervised Insider Threat Detection in Structured\n  Cybersecurity Data Streams", "comments": "Proceedings of AI for Cyber Security Workshop at AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of an organization's computer network activity is a key component of\nearly detection and mitigation of insider threat, a growing concern for many\norganizations. Raw system logs are a prototypical example of streaming data\nthat can quickly scale beyond the cognitive power of a human analyst. As a\nprospective filter for the human analyst, we present an online unsupervised\ndeep learning approach to detect anomalous network activity from system logs in\nreal time. Our models decompose anomaly scores into the contributions of\nindividual user behavior features for increased interpretability to aid\nanalysts reviewing potential cases of insider threat. Using the CERT Insider\nThreat Dataset v6.2 and threat detection recall as our performance metric, our\nnovel deep and recurrent neural network models outperform Principal Component\nAnalysis, Support Vector Machine and Isolation Forest based anomaly detection\nbaselines. For our best model, the events labeled as insider threat activity in\nour dataset had an average anomaly score in the 95.53 percentile, demonstrating\nour approach's potential to greatly reduce analyst workloads.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 17:54:28 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 20:53:03 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Tuor", "Aaron", ""], ["Kaplan", "Samuel", ""], ["Hutchinson", "Brian", ""], ["Nichols", "Nicole", ""], ["Robinson", "Sean", ""]]}, {"id": "1710.01013", "submitter": "Emanuele Sansone", "authors": "Emanuele Sansone, Francesco G.B. De Natale", "title": "Training Feedforward Neural Networks with Standard Logistic Activations\n  is Feasible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training feedforward neural networks with standard logistic activations is\nconsidered difficult because of the intrinsic properties of these sigmoidal\nfunctions. This work aims at showing that these networks can be trained to\nachieve generalization performance comparable to those based on hyperbolic\ntangent activations. The solution consists on applying a set of conditions in\nparameter initialization, which have been derived from the study of the\nproperties of a single neuron from an information-theoretic perspective. The\nproposed initialization is validated through an extensive experimental\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 07:21:03 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Sansone", "Emanuele", ""], ["De Natale", "Francesco G. B.", ""]]}, {"id": "1710.01347", "submitter": "David Di Giorgio", "authors": "David Di Giorgio", "title": "Simple Cortex: A Model of Cells in the Sensory Nervous System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience research has produced many theories and computational neural\nmodels of sensory nervous systems. Notwithstanding many different perspectives\ntowards developing intelligent machines, artificial intelligence has ultimately\nbeen influenced by neuroscience. Therefore, this paper provides an introduction\nto biologically inspired machine intelligence by exploring the basic principles\nof sensation and perception as well as the structure and behavior of biological\nsensory nervous systems like the neocortex. Concepts like spike timing,\nsynaptic plasticity, inhibition, neural structure, and neural behavior are\napplied to a new model, Simple Cortex (SC). A software implementation of SC has\nbeen built and demonstrates fast observation, learning, and prediction of\nspatio-temporal sensory-motor patterns and sequences. Finally, this paper\nsuggests future areas of improvement and growth for Simple Cortex and other\nrelated machine intelligence models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 18:51:19 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Di Giorgio", "David", ""]]}, {"id": "1710.01916", "submitter": "Luiza Mici", "authors": "Luiza Mici, German I. Parisi, Stefan Wermter", "title": "A self-organizing neural network architecture for learning human-object\n  interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The visual recognition of transitive actions comprising human-object\ninteractions is a key component for artificial systems operating in natural\nenvironments. This challenging task requires jointly the recognition of\narticulated body actions as well as the extraction of semantic elements from\nthe scene such as the identity of the manipulated objects. In this paper, we\npresent a self-organizing neural network for the recognition of human-object\ninteractions from RGB-D videos. Our model consists of a hierarchy of\nGrow-When-Required (GWR) networks that learn prototypical representations of\nbody motion patterns and objects, accounting for the development of\naction-object mappings in an unsupervised fashion. We report experimental\nresults on a dataset of daily activities collected for the purpose of this\nstudy as well as on a publicly available benchmark dataset. In line with\nneurophysiological studies, our self-organizing architecture exhibits higher\nneural activation for congruent action-object pairs learned during training\nsessions with respect to synthetically created incongruent ones. We show that\nour unsupervised model shows competitive classification results on the\nbenchmark dataset with respect to strictly supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 08:40:24 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 09:00:41 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Mici", "Luiza", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""]]}, {"id": "1710.02094", "submitter": "Eileen Leary", "authors": "Jens B. Stephansen, Alexander N. Olesen, Mads Olsen, Aditya Ambati,\n  Eileen B. Leary, Hyatt E. Moore, Oscar Carrillo, Ling Lin, Fang Han, Han Yan,\n  Yun L. Sun, Yves Dauvilliers, Sabine Scholz, Lucie Barateau, Birgit Hogl,\n  Ambra Stefani, Seung Chul Hong, Tae Won Kim, Fabio Pizza, Giuseppe Plazzi,\n  Stefano Vandi, Elena Antelmi, Dimitri Perrin, Samuel T. Kuna, Paula K.\n  Schweitzer, Clete Kushida, Paul E. Peppard, Helge B.D. Sorensen, Poul Jennum,\n  Emmanuel Mignot", "title": "Neural network an1alysis of sleep stages enables efficient diagnosis of\n  narcolepsy", "comments": "21 pages (not including title or references), 6 figures (1a - 6c), 6\n  tables, 5 supplementary figures, 9 supplementary tables", "journal-ref": "Nature Communications volume 9, Article number: 5229 (2018)", "doi": "10.1038/s41467-018-07229-3", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of sleep for the diagnosis of sleep disorders such as Type-1\nNarcolepsy (T1N) currently requires visual inspection of polysomnography\nrecords by trained scoring technicians. Here, we used neural networks in\napproximately 3,000 normal and abnormal sleep recordings to automate sleep\nstage scoring, producing a hypnodensity graph - a probability distribution\nconveying more information than classical hypnograms. Accuracy of sleep stage\nscoring was validated in 70 subjects assessed by six scorers. The best model\nperformed better than any individual scorer (87% versus consensus). It also\nreliably scores sleep down to 5 instead of 30 second scoring epochs. A T1N\nmarker based on unusual sleep-stage overlaps achieved a specificity of 96% and\na sensitivity of 91%, validated in independent datasets. Addition of\nHLA-DQB1*06:02 typing increased specificity to 99%. Our method can reduce time\nspent in sleep clinics and automates T1N diagnosis. It also opens the\npossibility of diagnosing T1N using home sleep studies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 16:17:15 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 19:25:06 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Stephansen", "Jens B.", ""], ["Olesen", "Alexander N.", ""], ["Olsen", "Mads", ""], ["Ambati", "Aditya", ""], ["Leary", "Eileen B.", ""], ["Moore", "Hyatt E.", ""], ["Carrillo", "Oscar", ""], ["Lin", "Ling", ""], ["Han", "Fang", ""], ["Yan", "Han", ""], ["Sun", "Yun L.", ""], ["Dauvilliers", "Yves", ""], ["Scholz", "Sabine", ""], ["Barateau", "Lucie", ""], ["Hogl", "Birgit", ""], ["Stefani", "Ambra", ""], ["Hong", "Seung Chul", ""], ["Kim", "Tae Won", ""], ["Pizza", "Fabio", ""], ["Plazzi", "Giuseppe", ""], ["Vandi", "Stefano", ""], ["Antelmi", "Elena", ""], ["Perrin", "Dimitri", ""], ["Kuna", "Samuel T.", ""], ["Schweitzer", "Paula K.", ""], ["Kushida", "Clete", ""], ["Peppard", "Paul E.", ""], ["Sorensen", "Helge B. D.", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""]]}, {"id": "1710.02254", "submitter": "Chaitanya Ahuja", "authors": "Chaitanya Ahuja and Louis-Philippe Morency", "title": "Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency\n  for Sequence Modeling", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have shown remarkable success in modeling\nsequences. However low resource situations still adversely affect the\ngeneralizability of these models. We introduce a new family of models, called\nLattice Recurrent Units (LRU), to address the challenge of learning deep\nmulti-layer recurrent models with limited resources. LRU models achieve this\ngoal by creating distinct (but coupled) flow of information inside the units: a\nfirst flow along time dimension and a second flow along depth dimension. It\nalso offers a symmetry in how information can flow horizontally and vertically.\nWe analyze the effects of decoupling three different components of our LRU\nmodel: Reset Gate, Update Gate and Projected State. We evaluate this family on\nnew LRU models on computational convergence rates and statistical efficiency.\nOur experiments are performed on four publicly-available datasets, comparing\nwith Grid-LSTM and Recurrent Highway networks. Our results show that LRU has\nbetter empirical computational convergence rates and statistical efficiency\nvalues, along with learning more accurate language models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 01:52:14 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 05:11:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ahuja", "Chaitanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1710.02286", "submitter": "Lars Hertel", "authors": "Lars Hertel, Erhardt Barth, Thomas K\\\"aster, Thomas Martinetz", "title": "Deep Convolutional Neural Networks as Generic Feature Extractors", "comments": "4 pages, accepted version for publication in Proceedings of the IEEE\n  International Joint Conference on Neural Networks (IJCNN), July 2015,\n  Killarney, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing objects in natural images is an intricate problem involving\nmultiple conflicting objectives. Deep convolutional neural networks, trained on\nlarge datasets, achieve convincing results and are currently the\nstate-of-the-art approach for this task. However, the long time needed to train\nsuch deep networks is a major drawback. We tackled this problem by reusing a\npreviously trained network. For this purpose, we first trained a deep\nconvolutional network on the ILSVRC2012 dataset. We then maintained the learned\nconvolution kernels and only retrained the classification part on different\ndatasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100,\ncompared to the previous state-of-the-art result of 65.43 %. Furthermore, our\nfindings indicate that convolutional networks are able to learn generic feature\nextractors that can be used for different tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 06:42:11 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Hertel", "Lars", ""], ["Barth", "Erhardt", ""], ["K\u00e4ster", "Thomas", ""], ["Martinetz", "Thomas", ""]]}, {"id": "1710.02553", "submitter": "Juan Juli\\'an Merelo-Guerv\\'os Pr.", "authors": "Juan-Juli\\'an Merelo-Guerv\\'os", "title": "Artificial life, complex systems and cloud computing: a short review", "comments": "Short paper to support a Evolution in the cloud tutorial in ECAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cloud computing is the prevailing mode of designing, creating and deploying\ncomplex applications nowadays. Its underlying assumptions include distributed\ncomputing, but also new concepts that need to be incorporated in the different\nfields. In this short paper we will make a review of how the world of cloud\ncomputing has intersected the complex systems and artificial life field, and\nhow it has been used as inspiration for new models or implementation of new and\npowerful algorithms\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 18:05:13 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Merelo-Guerv\u00f3s", "Juan-Juli\u00e1n", ""]]}, {"id": "1710.03070", "submitter": "Brian DePasquale", "authors": "Brian DePasquale, Christopher J. Cueva, Kanaka Rajan, G. Sean Escola,\n  L.F. Abbott", "title": "full-FORCE: A Target-Based Method for Training Recurrent Networks", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0191527", "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained recurrent networks are powerful tools for modeling dynamic neural\ncomputations. We present a target-based method for modifying the full\nconnectivity matrix of a recurrent network to train it to perform tasks\ninvolving temporally complex input/output transformations. The method\nintroduces a second network during training to provide suitable \"target\"\ndynamics useful for performing the task. Because it exploits the full recurrent\nconnectivity, the method produces networks that perform tasks with fewer\nneurons and greater noise robustness than traditional least-squares (FORCE)\napproaches. In addition, we show how introducing additional input signals into\nthe target-generating network, which act as task hints, greatly extends the\nrange of tasks that can be learned and provides control over the complexity and\nnature of the dynamics of the trained, task-performing network.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 13:00:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["DePasquale", "Brian", ""], ["Cueva", "Christopher J.", ""], ["Rajan", "Kanaka", ""], ["Escola", "G. Sean", ""], ["Abbott", "L. F.", ""]]}, {"id": "1710.03414", "submitter": "Chao-Ming Wang", "authors": "Chao-Ming Wang", "title": "Network of Recurrent Neural Networks", "comments": "Under review as a conference paper at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe a class of systems theory based neural networks called \"Network\nOf Recurrent neural networks\" (NOR), which introduces a new structure level to\nRNN related models. In NOR, RNNs are viewed as the high-level neurons and are\nused to build the high-level layers. More specifically, we propose several\nmethodologies to design different NOR topologies according to the theory of\nsystem evolution. Then we carry experiments on three different tasks to\nevaluate our implementations. Experimental results show our models outperform\nsimple RNN remarkably under the same number of parameters, and sometimes\nachieve even better results than GRU and LSTM.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 06:14:58 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Wang", "Chao-Ming", ""]]}, {"id": "1710.03753", "submitter": "AbdElRahman ElSaid", "authors": "AbdElRahman ElSaid, Travis Desell, Fatima El Jamiy, James Higgins,\n  Brandon Wild", "title": "Optimizing Long Short-Term Memory Recurrent Neural Networks Using Ant\n  Colony Optimization to Predict Turbine Engine Vibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article expands on research that has been done to develop a recurrent\nneural network (RNN) capable of predicting aircraft engine vibrations using\nlong short-term memory (LSTM) neurons. LSTM RNNs can provide a more\ngeneralizable and robust method for prediction over analytical calculations of\nengine vibration, as analytical calculations must be solved iteratively based\non specific empirical engine parameters, making this approach ungeneralizable\nacross multiple engines. In initial work, multiple LSTM RNN architectures were\nproposed, evaluated and compared. This research improves the performance of the\nmost effective LSTM network design proposed in the previous work by using a\npromising neuroevolution method based on ant colony optimization (ACO) to\ndevelop and enhance the LSTM cell structure of the network. A parallelized\nversion of the ACO neuroevolution algorithm has been developed and the evolved\nLSTM RNNs were compared to the previously used fixed topology. The evolved\nnetworks were trained on a large database of flight data records obtained from\nan airline containing flights that suffered from excessive vibration. Results\nwere obtained using MPI (Message Passing Interface) on a high performance\ncomputing (HPC) cluster, evolving 1000 different LSTM cell structures using 168\ncores over 4 days. The new evolved LSTM cells showed an improvement of 1.35%,\nreducing prediction error from 5.51% to 4.17% when predicting excessive engine\nvibrations 10 seconds in the future, while at the same time dramatically\nreducing the number of weights from 21,170 to 11,810.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 14:09:22 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["ElSaid", "AbdElRahman", ""], ["Desell", "Travis", ""], ["Jamiy", "Fatima El", ""], ["Higgins", "James", ""], ["Wild", "Brandon", ""]]}, {"id": "1710.03981", "submitter": "Philippe Thomas", "authors": "Emmanuel Zimmermann (1), Hind Haouzi (1), Philippe Thomas (1), Andr\\'e\n  Thomas (1), Melanie Noyel ((1) CRAN)", "title": "A batching and scheduling optimisation for a cutting work-center:\n  Acta-Mobilier case study", "comments": "7th International Conference on Industrial Engineering and Systems\n  Management IESM'17, Oct 2017, Saarbr\\\"ucken, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to investigate an approach to group lots in\nbatches and to schedule these batches on Acta-Mobilier cutting work-center\nwhile taking into account numerous constraints and objectives. The specific\nbatching method was proposed to handle the Acta-Mobilier problem and a\nmathematical formalisation and genetic algorithm were proposed to deal with the\nscheduling problem. The proposed algorithm has been embedded in software to\noptimise production costs and emphasis the visual management on the production\nline. The application is currently being used in Acta-Mobilier plant and shows\nsignificant results\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 09:49:59 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Zimmermann", "Emmanuel", "", "CRAN"], ["Haouzi", "Hind", "", "CRAN"], ["Thomas", "Philippe", "", "CRAN"], ["Thomas", "Andr\u00e9", "", "CRAN"], ["Noyel", "Melanie", ""]]}, {"id": "1710.03996", "submitter": "Zhenhua Li", "authors": "Zhenhua Li, Qingfu Zhang", "title": "A Simple Yet Efficient Rank One Update for Covariance Matrix Adaptation", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient approximated rank one update for\ncovariance matrix adaptation evolution strategy (CMA-ES). It makes use of two\nevolution paths as simple as that of CMA-ES, while avoiding the computational\nmatrix decomposition. We analyze the algorithms' properties and behaviors. We\nexperimentally study the proposed algorithm's performances. It generally\noutperforms or performs competitively to the Cholesky CMA-ES.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 10:49:59 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 06:52:37 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 07:49:54 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Li", "Zhenhua", ""], ["Zhang", "Qingfu", ""]]}, {"id": "1710.04036", "submitter": "Yinyan Zhang", "authors": "Yinyan Zhang, Shuai Li, and Hongliang Guo", "title": "Porcellio scaber algorithm (PSA) for solving constrained optimization\n  problems", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend a bio-inspired algorithm called the porcellio scaber\nalgorithm (PSA) to solve constrained optimization problems, including a\nconstrained mixed discrete-continuous nonlinear optimization problem. Our\nextensive experiment results based on benchmark optimization problems show that\nthe PSA has a better performance than many existing methods or algorithms. The\nresults indicate that the PSA is a promising algorithm for constrained\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:38:59 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Zhang", "Yinyan", ""], ["Li", "Shuai", ""], ["Guo", "Hongliang", ""]]}, {"id": "1710.04110", "submitter": "Michael Mozer", "authors": "Michael C. Mozer, Denis Kazakov, Robert V. Lindsey", "title": "Discrete Event, Continuous Time RNNs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate recurrent neural network architectures for event-sequence\nprocessing. Event sequences, characterized by discrete observations stamped\nwith continuous-valued times of occurrence, are challenging due to the\npotentially wide dynamic range of relevant time scales as well as interactions\nbetween time scales. We describe four forms of inductive bias that should\nbenefit architectures for event sequences: temporal locality, position and\nscale homogeneity, and scale interdependence. We extend the popular gated\nrecurrent unit (GRU) architecture to incorporate these biases via intrinsic\ntemporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by\ninterpreting the gates of a GRU as selecting a time scale of memory, and the\nCT-GRU generalizes the GRU by incorporating multiple time scales of memory and\nperforming context-dependent selection of time scales for information storage\nand retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas\nthey serve as generic additional inputs to the GRU. Despite the very different\nmanner in which the two models consider time, their performance on eleven data\nsets we examined is essentially identical. Our surprising results point both to\nthe robustness of GRU and LSTM architectures for handling continuous time, and\nto the potency of incorporating continuous dynamics into neural architectures.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 15:20:51 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Mozer", "Michael C.", ""], ["Kazakov", "Denis", ""], ["Lindsey", "Robert V.", ""]]}, {"id": "1710.04211", "submitter": "Biswa Sengupta", "authors": "Alessandro Bay, Biswa Sengupta", "title": "StackSeq2Seq: Dual Encoder Seq2Seq Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely studied non-deterministic polynomial time (NP) hard problem lies in\nfinding a route between the two nodes of a graph. Often meta-heuristics\nalgorithms such as $A^{*}$ are employed on graphs with a large number of nodes.\nHere, we propose a deep recurrent neural network architecture based on the\nSequence-2-Sequence (Seq2Seq) model, widely used, for instance in text\ntranslation. Particularly, we illustrate that utilising a context vector that\nhas been learned from two different recurrent networks enables increased\naccuracies in learning the shortest route of a graph. Additionally, we show\nthat one can boost the performance of the Seq2Seq network by smoothing the loss\nfunction using a homotopy continuation of the decoder's loss function.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 11:22:26 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 21:47:46 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Bay", "Alessandro", ""], ["Sengupta", "Biswa", ""]]}, {"id": "1710.04404", "submitter": "Or Sharir", "authors": "Or Sharir and Amnon Shashua", "title": "Sum-Product-Quotient Networks", "comments": "Published as a conference paper at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel tractable generative model that extends Sum-Product\nNetworks (SPNs) and significantly boosts their power. We call it\nSum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate\nconditional distributions into the model by direct computation using quotient\nnodes, e.g. $P(A|B) = \\frac{P(A,B)}{P(B)}$. We provide sufficient conditions\nfor the tractability of SPQNs that generalize and relax the decomposable and\ncomplete tractability conditions of SPNs. These relaxed conditions give rise to\nan exponential boost to the expressive efficiency of our model, i.e. we prove\nthat there are distributions which SPQNs can compute efficiently but require\nSPNs to be of exponential size. Thus, we narrow the gap in expressivity between\ntractable graphical models and other Neural Network-based generative models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 08:18:07 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 16:11:53 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 23:34:53 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Sharir", "Or", ""], ["Shashua", "Amnon", ""]]}, {"id": "1710.04734", "submitter": "Nitin Rathi", "authors": "Nitin Rathi, Priyadarshini Panda, Kaushik Roy", "title": "STDP Based Pruning of Connections and Weight Quantization in Spiking\n  Neural Networks for Energy Efficient Recognition", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) with a large number of weights and varied\nweight distribution can be difficult to implement in emerging in-memory\ncomputing hardware due to the limitations on crossbar size (implementing dot\nproduct), the constrained number of conductance levels in non-CMOS devices and\nthe power budget. We present a sparse SNN topology where non-critical\nconnections are pruned to reduce the network size and the remaining critical\nsynapses are weight quantized to accommodate for limited conductance levels.\nPruning is based on the power law weight-dependent Spike Timing Dependent\nPlasticity (STDP) model; synapses between pre- and post-neuron with high spike\ncorrelation are retained, whereas synapses with low correlation or uncorrelated\nspiking activity are pruned. The weights of the retained connections are\nquantized to the available number of conductance levels. The process of pruning\nnon-critical connections and quantizing the weights of critical synapses is\nperformed at regular intervals during training. We evaluated our sparse and\nquantized network on MNIST dataset and on a subset of images from Caltech-101\ndataset. The compressed topology achieved a classification accuracy of 90.1%\n(91.6%) on the MNIST (Caltech-101) dataset with 3.1x (2.2x) and 4x (2.6x)\nimprovement in energy and area, respectively. The compressed topology is energy\nand area efficient while maintaining the same classification accuracy of a\n2-layer fully connected SNN topology.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 21:56:51 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Rathi", "Nitin", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1710.04748", "submitter": "Sebastian Risi", "authors": "Jakob Merrild, Mikkel Angaju Rasmussen and Sebastian Risi", "title": "HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments within memory-augmented neural networks have solved\nsequential problems requiring long-term memory, which are intractable for\ntraditional neural networks. However, current approaches still struggle to\nscale to large memory sizes and sequence lengths. In this paper we show how\naccess to memory can be encoded geometrically through a HyperNEAT-based Neural\nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT\nencoding allows for training on small memory vectors in a bit-vector copy task\nand then applying the knowledge gained from such training to speed up training\non larger size memory vectors. Additionally, we demonstrate that in some\ninstances, networks trained to copy bit-vectors of size 9 can be scaled to\nsizes of 1,000 without further training. While the task in this paper is\nsimple, these results could open up the problems amendable to networks with\nexternal memories to problems with larger memory vectors and theoretically\nunbounded memory sizes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 23:41:02 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Merrild", "Jakob", ""], ["Rasmussen", "Mikkel Angaju", ""], ["Risi", "Sebastian", ""]]}, {"id": "1710.04838", "submitter": "Davide Zambrano", "authors": "Davide Zambrano, Roeland Nusselder, H. Steven Scholte and Sander Bohte", "title": "Efficient Computation in Adaptive Artificial Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) are bio-inspired models of neural\ncomputation that have proven highly effective. Still, ANNs lack a natural\nnotion of time, and neural units in ANNs exchange analog values in a\nframe-based manner, a computationally and energetically inefficient form of\ncommunication. This contrasts sharply with biological neurons that communicate\nsparingly and efficiently using binary spikes. While artificial Spiking Neural\nNetworks (SNNs) can be constructed by replacing the units of an ANN with\nspiking neurons, the current performance is far from that of deep ANNs on hard\nbenchmarks and these SNNs use much higher firing rates compared to their\nbiological counterparts, limiting their efficiency. Here we show how spiking\nneurons that employ an efficient form of neural coding can be used to construct\nSNNs that match high-performance ANNs and exceed state-of-the-art in SNNs on\nimportant benchmarks, while requiring much lower average firing rates. For\nthis, we use spike-time coding based on the firing rate limiting adaptation\nphenomenon observed in biological spiking neurons. This phenomenon can be\ncaptured in adapting spiking neuron models, for which we derive the effective\ntransfer function. Neural units in ANNs trained with this transfer function can\nbe substituted directly with adaptive spiking neurons, and the resulting\nAdaptive SNNs (AdSNNs) can carry out inference in deep neural networks using up\nto an order of magnitude fewer spikes compared to previous SNNs. Adaptive\nspike-time coding additionally allows for the dynamic control of neural coding\nprecision: we show how a simple model of arousal in AdSNNs further halves the\naverage required firing rate and this notion naturally extends to other forms\nof attention. AdSNNs thus hold promise as a novel and efficient model for\nneural computation that naturally fits to temporally continuous and\nasynchronous applications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:29:50 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Zambrano", "Davide", ""], ["Nusselder", "Roeland", ""], ["Scholte", "H. Steven", ""], ["Bohte", "Sander", ""]]}, {"id": "1710.04874", "submitter": "Gregorz Dudek", "authors": "Grzegorz Dudek", "title": "A Method of Generating Random Weights and Biases in Feedforward Neural\n  Networks with Random Hidden Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with random hidden nodes have gained increasing interest from\nresearchers and practical applications. This is due to their unique features\nsuch as very fast training and universal approximation property. In these\nnetworks the weights and biases of hidden nodes determining the nonlinear\nfeature mapping are set randomly and are not learned. Appropriate selection of\nthe intervals from which weights and biases are selected is extremely\nimportant. This topic has not yet been sufficiently explored in the literature.\nIn this work a method of generating random weights and biases is proposed. This\nmethod generates the parameters of the hidden nodes in such a way that\nnonlinear fragments of the activation functions are located in the input space\nregions with data and can be used to construct the surface approximating a\nnonlinear target function. The weights and biases are dependent on the input\ndata range and activation function type. The proposed methods allows us to\ncontrol the generalization degree of the model. These all lead to improvement\nin approximation performance of the network. Several experiments show very\npromising results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 11:23:18 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Dudek", "Grzegorz", ""]]}, {"id": "1710.05189", "submitter": "Nicolas Rougier", "authors": "Nicolas P. Rougier", "title": "A graphical, scalable and intuitive method for the placement and the\n  connection of biological cells", "comments": "Corresponding code at https://github.com/rougier/spatial-computation", "journal-ref": null, "doi": "10.3389/fninf.2018.00012", "report-no": null, "categories": "cs.NE cs.GR q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a graphical method originating from the computer graphics domain\nthat is used for the arbitrary and intuitive placement of cells over a\ntwo-dimensional manifold. Using a bitmap image as input, where the color\nindicates the identity of the different structures and the alpha channel\nindicates the local cell density, this method guarantees a discrete\ndistribution of cell position respecting the local density function. This\nmethod scales to any number of cells, allows to specify several different\nstructures at once with arbitrary shapes and provides a scalable and versatile\nalternative to the more classical assumption of a uniform non-spatial\ndistribution. Furthermore, several connection schemes can be derived from the\npaired distances between cells using either an automatic mapping or a\nuser-defined local reference frame, providing new computational properties for\nthe underlying model. The method is illustrated on a discrete homogeneous\nneural field, on the distribution of cones and rods in the retina and on a\ncoronal view of the basal ganglia.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 14:10:39 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Rougier", "Nicolas P.", ""]]}, {"id": "1710.05311", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Vector Quantization using the Improved Differential Evolution Algorithm\n  for Image Compression", "comments": "11 pages", "journal-ref": null, "doi": "10.17605/OSF.IO/M9RNZ", "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector Quantization, VQ is a popular image compression technique with a\nsimple decoding architecture and high compression ratio. Codebook designing is\nthe most essential part in Vector Quantization. LindeBuzoGray, LBG is a\ntraditional method of generation of VQ Codebook which results in lower PSNR\nvalue. A Codebook affects the quality of image compression, so the choice of an\nappropriate codebook is a must. Several optimization techniques have been\nproposed for global codebook generation to enhance the quality of image\ncompression. In this paper, a novel algorithm called IDE-LBG is proposed which\nuses Improved Differential Evolution Algorithm coupled with LBG for generating\noptimum VQ Codebooks. The proposed IDE works better than the traditional DE\nwith modifications in the scaling factor and the boundary control mechanism.\nThe IDE generates better solutions by efficient exploration and exploitation of\nthe search space. Then the best optimal solution obtained by the IDE is\nprovided as the initial Codebook for the LBG. This approach produces an\nefficient Codebook with less computational time and the consequences include\nexcellent PSNR values and superior quality reconstructed images. It is observed\nthat the proposed IDE-LBG find better VQ Codebooks as compared to IPSO-LBG,\nBA-LBG and FA-LBG.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 10:31:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1710.05381", "submitter": "Mateusz Buda", "authors": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "title": "A systematic study of the class imbalance problem in convolutional\n  neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.07.011", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we systematically investigate the impact of class imbalance on\nclassification performance of convolutional neural networks (CNNs) and compare\nfrequently used methods to address the issue. Class imbalance is a common\nproblem that has been comprehensively studied in classical machine learning,\nyet very limited systematic research is available in the context of deep\nlearning. In our study, we use three benchmark datasets of increasing\ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of\nimbalance on classification and perform an extensive comparison of several\nmethods to address the issue: oversampling, undersampling, two-phase training,\nand thresholding that compensates for prior class probabilities. Our main\nevaluation metric is area under the receiver operating characteristic curve\n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is\nassociated with notable difficulties in the context of imbalanced data. Based\non results from our experiments we conclude that (i) the effect of class\nimbalance on classification performance is detrimental; (ii) the method of\naddressing class imbalance that emerged as dominant in almost all analyzed\nscenarios was oversampling; (iii) oversampling should be applied to the level\nthat completely eliminates the imbalance, whereas the optimal undersampling\nratio depends on the extent of imbalance; (iv) as opposed to some classical\nmachine learning models, oversampling does not cause overfitting of CNNs; (v)\nthresholding should be applied to compensate for prior class probabilities when\noverall number of properly classified cases is of interest.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 19:01:43 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 02:02:17 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Buda", "Mateusz", ""], ["Maki", "Atsuto", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1710.05468", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "title": "Generalization in Deep Learning", "comments": "To appear in Mathematics of Deep Learning, Cambridge University\n  Press. All previous results remain unchanged", "journal-ref": null, "doi": null, "report-no": "Massachusetts Institute of Technology (MIT), MIT-CSAIL-TR-2018-014", "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides theoretical insights into why and how deep learning can\ngeneralize well, despite its large capacity, complexity, possible algorithmic\ninstability, nonrobustness, and sharp minima, responding to an open question in\nthe literature. We also discuss approaches to provide non-vacuous\ngeneralization guarantees for deep learning. Based on theoretical observations,\nwe propose new open problems and discuss the limitations of our results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 02:21:24 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 19:44:43 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 23:39:50 GMT"}, {"version": "v4", "created": "Tue, 1 Jan 2019 00:07:45 GMT"}, {"version": "v5", "created": "Fri, 10 May 2019 18:41:13 GMT"}, {"version": "v6", "created": "Mon, 27 Jul 2020 23:01:04 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Kaelbling", "Leslie Pack", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.05941", "submitter": "Prajit Ramachandran", "authors": "Prajit Ramachandran, Barret Zoph, Quoc V. Le", "title": "Searching for Activation Functions", "comments": "Updated version of \"Swish: a Self-Gated Activation Function\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of activation functions in deep networks has a significant effect\non the training dynamics and task performance. Currently, the most successful\nand widely-used activation function is the Rectified Linear Unit (ReLU).\nAlthough various hand-designed alternatives to ReLU have been proposed, none\nhave managed to replace it due to inconsistent gains. In this work, we propose\nto leverage automatic search techniques to discover new activation functions.\nUsing a combination of exhaustive and reinforcement learning-based search, we\ndiscover multiple novel activation functions. We verify the effectiveness of\nthe searches by conducting an empirical evaluation with the best discovered\nactivation function. Our experiments show that the best discovered activation\nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends\nto work better than ReLU on deeper models across a number of challenging\ndatasets. For example, simply replacing ReLUs with Swish units improves top-1\nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for\nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it\neasy for practitioners to replace ReLUs with Swish units in any neural network.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:05:45 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 17:45:21 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Ramachandran", "Prajit", ""], ["Zoph", "Barret", ""], ["Le", "Quoc V.", ""]]}, {"id": "1710.06055", "submitter": "Tim Taylor", "authors": "Tim Taylor", "title": "Evolution in Virtual Worlds", "comments": "Author's final preprint", "journal-ref": "Chapter 32 of \"The Oxford Handbook of Virtuality\", Mark Grimshaw\n  (ed.), Oxford University Press, 2014. (ISBN 9780199826162)", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter discusses the possibility of instilling a virtual world with\nmechanisms for evolution and natural selection in order to generate rich\necosystems of complex organisms in a process akin to biological evolution. Some\nprevious work in the area is described, and successes and failures are\ndiscussed. The components of a more comprehensive framework for designing such\nworlds are mapped out, including the design of the individual organisms, the\nproperties and dynamics of the environmental medium in which they are evolving,\nand the representational relationship between organism and environment. Some of\nthe key issues discussed include how to allow organisms to evolve new\nstructures and functions with few restrictions, and how to create an\ninterconnectedness between organisms in order to generate drives for continuing\nevolutionary activity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 02:13:53 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Taylor", "Tim", ""]]}, {"id": "1710.06487", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "title": "Classification and Geometry of General Perceptual Manifolds", "comments": "24 pages, 12 figures, Supplementary Materials", "journal-ref": "Phys. Rev. X 8, 031003 (2018)", "doi": "10.1103/PhysRevX.8.031003", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual manifolds arise when a neural population responds to an ensemble\nof sensory signals associated with different physical features (e.g.,\norientation, pose, scale, location, and intensity) of the same perceptual\nobject. Object recognition and discrimination requires classifying the\nmanifolds in a manner that is insensitive to variability within a manifold. How\nneuronal systems give rise to invariant object classification and recognition\nis a fundamental problem in brain theory as well as in machine learning. Here\nwe study the ability of a readout network to classify objects from their\nperceptual manifold representations. We develop a statistical mechanical theory\nfor the linear classification of manifolds with arbitrary geometry revealing a\nremarkable relation to the mathematics of conic decomposition. Novel\ngeometrical measures of manifold radius and manifold dimension are introduced\nwhich can explain the classification capacity for manifolds of various\ngeometries. The general theory is demonstrated on a number of representative\nmanifolds, including L2 ellipsoids prototypical of strictly convex manifolds,\nL1 balls representing polytopes consisting of finite sample points, and\norientation manifolds which arise from neurons tuned to respond to a continuous\nangle variable, such as object orientation. The effects of label sparsity on\nthe classification capacity of manifolds are elucidated, revealing a scaling\nrelation between label sparsity and manifold radius. Theoretical predictions\nare corroborated by numerical simulations using recently developed algorithms\nto compute maximum margin solutions for manifold dichotomies. Our theory and\nits extensions provide a powerful and rich framework for applying statistical\nmechanics of linear classification to data arising from neuronal responses to\nobject stimuli, as well as to artificial deep networks trained for object\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 20:06:25 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 03:27:25 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 15:46:57 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Chung", "SueYeon", ""], ["Lee", "Daniel D.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1710.06770", "submitter": "Ramses Sala", "authors": "Ramses Sala, Niccolo Baldanzini, Marco Pierini", "title": "SQG-Differential Evolution for difficult optimization problems under a\n  tight function evaluation budget", "comments": null, "journal-ref": "International Workshop on Machine Learning, Optimization, and Big\n  Data. MOD 2017. Lecture Notes in Computer Science, vol 10710. (pp. 322-336)\n  Springer, Cham", "doi": "10.1007/978-3-319-72926-8_27", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of industrial engineering, it is important to integrate\nefficient computational optimization methods in the product development\nprocess. Some of the most challenging simulation-based engineering design\noptimization problems are characterized by: a large number of design variables,\nthe absence of analytical gradients, highly non-linear objectives and a limited\nfunction evaluation budget. Although a huge variety of different optimization\nalgorithms is available, the development and selection of efficient algorithms\nfor problems with these industrial relevant characteristics, remains a\nchallenge. In this communication, a hybrid variant of Differential Evolution\n(DE) is introduced which combines aspects of Stochastic Quasi-Gradient (SQG)\nmethods within the framework of DE, in order to improve optimization efficiency\non problems with the previously mentioned characteristics. The performance of\nthe resulting derivative-free algorithm is compared with other state-of-the-art\nDE variants on 25 commonly used benchmark functions, under tight function\nevaluation budget constraints of 1000 evaluations. The experimental results\nindicate that the new algorithm performs excellent on the 'difficult' (high\ndimensional, multi-modal, inseparable) test functions. The operations used in\nthe proposed mutation scheme, are computationally inexpensive, and can be\neasily implemented in existing differential evolution variants or other\npopulation-based optimization algorithms by a few lines of program code as an\nnon-invasive optional setting. Besides the applicability of the presented\nalgorithm by itself, the described concepts can serve as a useful and\ninteresting addition to the algorithmic operators in the frameworks of\nheuristics and evolutionary optimization and computing.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 15:12:11 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 17:52:53 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Sala", "Ramses", ""], ["Baldanzini", "Niccolo", ""], ["Pierini", "Marco", ""]]}, {"id": "1710.07031", "submitter": "Borko Bo\\v{s}kovi\\'c", "authors": "Borko Bo\\v{s}kovi\\'c and Janez Brest", "title": "Protein Folding Optimization using Differential Evolution Extended with\n  Local Search and Component Reinitialization", "comments": "22 pages, 8 figures, 10 tables, journal", "journal-ref": "Information Sciences, Volumes 454-455, 2018, Pages 178-199", "doi": "10.1016/j.ins.2018.04.072", "report-no": null, "categories": "cs.AI cs.NE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Differential Evolution algorithm for protein\nfolding optimization that is applied to a three-dimensional AB off-lattice\nmodel. The proposed algorithm includes two new mechanisms. A local search is\nused to improve convergence speed and to reduce the runtime complexity of the\nenergy calculation. For this purpose, a local movement is introduced within the\nlocal search. The designed evolutionary algorithm has fast convergence speed\nand, therefore, when it is trapped into the local optimum or a relatively good\nsolution is located, it is hard to locate a better similar solution. The\nsimilar solution is different from the good solution in only a few components.\nA component reinitialization method is designed to mitigate this problem. Both\nthe new mechanisms and the proposed algorithm were analyzed on well-known amino\nacid sequences that are used frequently in the literature. Experimental results\nshow that the employed new mechanisms improve the efficiency of our algorithm\nand that the proposed algorithm is superior to other state-of-the-art\nalgorithms. It obtained a hit ratio of 100% for sequences up to 18 monomers,\nwithin a budget of $10^{11}$ solution evaluations. New best-known solutions\nwere obtained for most of the sequences. The existence of the symmetric\nbest-known solutions is also demonstrated in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:07:51 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 06:56:02 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Bo\u0161kovi\u0107", "Borko", ""], ["Brest", "Janez", ""]]}, {"id": "1710.07354", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda and Narayan Srinivasa", "title": "Learning to Recognize Actions from Limited Training Examples Using a\n  Recurrent Spiking Neural Model", "comments": "13 figures (includes supplementary information)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in machine learning today is to build a model that\ncan learn from few examples. Here, we describe a reservoir based spiking neural\nmodel for learning to recognize actions with a limited number of labeled\nvideos. First, we propose a novel encoding, inspired by how microsaccades\ninfluence visual perception, to extract spike information from raw video data\nwhile preserving the temporal correlation across different frames. Using this\nencoding, we show that the reservoir generalizes its rich dynamical activity\ntoward signature action/movements enabling it to learn from few training\nexamples. We evaluate our approach on the UCF-101 dataset. Our experiments\ndemonstrate that our proposed reservoir achieves 81.3%/87% Top-1/Top-5\naccuracy, respectively, on the 101-class data while requiring just 8 video\nexamples per class for training. Our results establish a new benchmark for\naction recognition from limited video examples for spiking neural models while\nyielding competetive accuracy with respect to state-of-the-art non-spiking\nneural models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 21:07:02 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Srinivasa", "Narayan", ""]]}, {"id": "1710.07547", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Karl {\\O}yvind Mikalsen and Robert Jenssen", "title": "Learning compressed representations of blood samples time series with\n  missing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical measurements collected over time are naturally represented as\nmultivariate time series (MTS), which often contain missing data. An\nautoencoder can learn low dimensional vectorial representations of MTS that\npreserve important data characteristics, but cannot deal explicitly with\nmissing data. In this work, we propose a new framework that combines an\nautoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts\nfor missingness patterns in MTS. Via kernel alignment, we incorporate TCK in\nthe autoencoder to improve the learned representations in presence of missing\ndata. We consider a classification problem of MTS with missing values,\nrepresenting blood samples of patients with surgical site infection. With our\napproach, rather than with a standard autoencoder, we learn representations in\nlow dimensions that can be classified better.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:29:52 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Mikalsen", "Karl \u00d8yvind", ""], ["Jenssen", "Robert", ""]]}, {"id": "1710.07659", "submitter": "Andreas St\\\"ockel", "authors": "Andreas St\\\"ockel, Aaron R. Voelker, Chris Eliasmith", "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering\n  Framework", "comments": "24 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical model underlying the Neural Engineering Framework (NEF)\nexpresses neuronal input as a linear combination of synaptic currents. However,\nin biology, synapses are not perfect current sources and are thus nonlinear.\nDetailed synapse models are based on channel conductances instead of currents,\nwhich require independent handling of excitatory and inhibitory synapses. This,\nin particular, significantly affects the influence of inhibitory signals on the\nneuronal dynamics. In this technical report we first summarize the relevant\nportions of the NEF and conductance-based synapse models. We then discuss a\nna\\\"ive translation between populations of LIF neurons with current- and\nconductance-based synapses based on an estimation of an average membrane\npotential. Experiments show that this simple approach works relatively well for\nfeed-forward communication channels, yet performance degrades for NEF networks\ndescribing more complex dynamics, such as integration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 18:35:23 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["St\u00f6ckel", "Andreas", ""], ["Voelker", "Aaron R.", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1710.07829", "submitter": "Gerard Rinkus", "authors": "Rod Rinkus, Jasmin Leveille", "title": "Superposed Episodic and Semantic Memory via Sparse Distributed\n  Representation", "comments": "8 pages, 4 figures. Submitted to NIPS CIAI 2017 wkshp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abilities to perceive, learn, and use generalities, similarities,\nclasses, i.e., semantic memory (SM), is central to cognition. Machine learning\n(ML), neural network, and AI research has been primarily driven by tasks\nrequiring such abilities. However, another central facet of cognition,\nsingle-trial formation of permanent memories of experiences, i.e., episodic\nmemory (EM), has had relatively little focus. Only recently has EM-like\nfunctionality been added to Deep Learning (DL) models, e.g., Neural Turing\nMachine, Memory Networks. However, in these cases: a) EM is implemented as a\nseparate module, which entails substantial data movement (and so, time and\npower) between the DL net itself and EM; and b) individual items are stored\nlocalistically within the EM, precluding realizing the exponential\nrepresentational efficiency of distributed over localist coding. We describe\nSparsey, an unsupervised, hierarchical, spatial/spatiotemporal associative\nmemory model differing fundamentally from mainstream ML models, most crucially,\nin its use of sparse distributed representations (SDRs), or, cell assemblies,\nwhich admits an extremely efficient, single-trial learning algorithm that maps\ninput similarity into code space similarity (measured as intersection). SDRs of\nindividual inputs are stored in superposition and because similarity is\npreserved, the patterns of intersections over the assigned codes reflect the\nsimilarity, i.e., statistical, structure, of all orders, not simply pairwise,\nover the inputs. Thus, SM, i.e., a generative model, is built as a\ncomputationally free side effect of the act of storing episodic memory traces\nof individual inputs, either spatial patterns or sequences. We report initial\nresults on MNIST and on the Weizmann video event recognition benchmarks. While\nwe have not yet attained SOTA class accuracy, learning takes only minutes on a\nsingle CPU.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 17:07:59 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Rinkus", "Rod", ""], ["Leveille", "Jasmin", ""]]}, {"id": "1710.07913", "submitter": "Nicola Milano", "authors": "Nicola Milano, J\\^onata Tyska Carvalho, Stefano Nolfi", "title": "Moderate Environmental Variation Promotes the Evolution of Robust\n  Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous evolutionary studies demonstrated how evaluating evolving agents in\nvariable environmental conditions enable them to develop solutions that are\nrobust to environmental variation. We demonstrate how the robustness of the\nagents can be further improved by exposing them also to environmental\nvariations throughout generations. These two types of environmental variations\nplay partially distinct roles as demonstrated by the fact that agents evolved\nin environments that do not vary throughout generations display lower\nperformance than agents evolved in varying environments independently from the\namount of environmental variation experienced during evaluation. Moreover, our\nresults demonstrate that performance increases when the amount of variations\nintroduced during agents evaluation and the rate at which the environment\nvaries throughout generations are moderate. This is explained by the fact that\nthe probability to retain genetic variations, including non-neutral variations\nthat alter the behavior of the agents, increases when the environment varies\nthroughout generations but also when new environmental conditions persist over\ntime long enough to enable genetic accommodation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 09:34:49 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 21:31:11 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Milano", "Nicola", ""], ["Carvalho", "J\u00f4nata Tyska", ""], ["Nolfi", "Stefano", ""]]}, {"id": "1710.08177", "submitter": "Saikat  Chatterjee", "authors": "Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra,\n  Mikael Skoglund", "title": "Progressive Learning for Systematic Design of Large Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 10:06:15 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chatterjee", "Saikat", ""], ["Javid", "Alireza M.", ""], ["Sadeghi", "Mostafa", ""], ["Mitra", "Partha P.", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1710.08247", "submitter": "Tilman Wekel", "authors": "Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra\n  Malik, Silvio Savarese", "title": "Generic 3D Representation via Pose Estimation and Matching", "comments": "Published in ECCV16. See the project website\n  http://3drepresentation.stanford.edu/ and dataset website\n  https://github.com/amir32002/3D_Street_View", "journal-ref": "ECCV 2016 535-553", "doi": "10.1007/978-3-319-46487-9_33", "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though a large body of computer vision research has investigated developing\ngeneric semantic representations, efforts towards developing a similar\nrepresentation for 3D has been limited. In this paper, we learn a generic 3D\nrepresentation through solving a set of foundational proxy 3D tasks:\nobject-centric camera pose estimation and wide baseline feature matching. Our\nmethod is based upon the premise that by providing supervision over a set of\ncarefully selected foundational tasks, generalization to novel tasks and\nabstraction capabilities can be achieved. We empirically show that the internal\nrepresentation of a multi-task ConvNet trained to solve the above core problems\ngeneralizes to novel 3D tasks (e.g., scene layout estimation, object pose\nestimation, surface normal estimation) without the need for fine-tuning and\nshows traits of abstraction abilities (e.g., cross-modality pose estimation).\nIn the context of the core supervised tasks, we demonstrate our representation\nachieves state-of-the-art wide baseline feature matching results without\nrequiring apriori rectification (unlike SIFT and the majority of learned\nfeatures). We also show 6DOF camera pose estimation given a pair local image\npatches. The accuracy of both supervised tasks come comparable to humans.\nFinally, we contribute a large-scale dataset composed of object-centric street\nview scenes along with point correspondences and camera pose information, and\nconclude with a discussion on the learned representation and open research\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:01:05 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Zamir", "Amir R.", ""], ["Wekel", "Tilman", ""], ["Argrawal", "Pulkit", ""], ["Weil", "Colin", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1710.08266", "submitter": "Jean-Michel Loubes", "authors": "Thomas Epelbaum (IPHT), Fabrice Gamboa (IMT), Jean-Michel Loubes\n  (IMT), Jessica Martin", "title": "Deep Learning applied to Road Traffic Speed forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to\nforecast a regression model for time dependent data. These algorithm's are\ndesigned to handle Floating Car Data (FCD) historic speeds to predict road\ntraffic data. For this we aggregate the speeds into the network inputs in an\ninnovative way. We compare the RMSE thus obtained with the results of a simpler\nphysical model, and show that the latter achieves better RMSE accuracy. We also\npropose a new indicator, which evaluates the algorithms improvement when\ncompared to a benchmark prediction. We conclude by questioning the interest of\nusing deep learning methods for this specific regression task.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 14:27:38 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Epelbaum", "Thomas", "", "IPHT"], ["Gamboa", "Fabrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Martin", "Jessica", ""]]}, {"id": "1710.08382", "submitter": "Patrick Huembeli", "authors": "Patrick Huembeli, Alexandre Dauphin, Peter Wittek", "title": "Identifying Quantum Phase Transitions with Adversarial Neural Networks", "comments": "10 pages, 8 figures, computational appendix is available at\n  https://github.com/PatrickHuembeli/Adversarial-Domain-Adaptation-for-Identifying-Phase-Transitions", "journal-ref": "Phys. Rev. B 97, 134109 (2018)", "doi": "10.1103/PhysRevB.97.134109", "report-no": null, "categories": "cond-mat.stat-mech cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of phases of matter is a challenging task, especially in\nquantum mechanics, where the complexity of the ground state appears to grow\nexponentially with the size of the system. We address this problem with\nstate-of-the-art deep learning techniques: adversarial domain adaptation. We\nderive the phase diagram of the whole parameter space starting from a fixed and\nknown subspace using unsupervised learning. The input data set contains both\nlabeled and unlabeled data instances. The first kind is a system that admits an\naccurate analytical or numerical solution, and one can recover its phase\ndiagram. The second type is the physical system with an unknown phase diagram.\nAdversarial domain adaptation uses both types of data to create invariant\nfeature extracting layers in a deep learning architecture. Once these layers\nare trained, we can attach an unsupervised learner to the network to find phase\ntransitions. We show the success of this technique by applying it on several\nparadigmatic models: the Ising model with different temperatures, the\nBose-Hubbard model, and the SSH model with disorder. The input is the ground\nstate without any manual feature engineering, and the dimension of the\nparameter space is unrestricted. The method finds unknown transitions\nsuccessfully and predicts transition points in close agreement with standard\nmethods. This study opens the door to the classification of physical systems\nwhere the phases boundaries are complex such as the many-body localization\nproblem or the Bose glass phase.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 16:00:21 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 11:01:05 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Huembeli", "Patrick", ""], ["Dauphin", "Alexandre", ""], ["Wittek", "Peter", ""]]}, {"id": "1710.08904", "submitter": "Pei Cao", "authors": "Pei Cao, Shengli Zhang, Jiong Tang", "title": "Pre-Processing-Free Gear Fault Diagnosis Using Small Datasets with Deep\n  Convolutional Neural Network-Based Transfer Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2018.2837621", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early fault diagnosis in complex mechanical systems such as gearbox has\nalways been a great challenge, even with the recent development in deep neural\nnetworks. The performance of a classic fault diagnosis system predominantly\ndepends on the features extracted and the classifier subsequently applied.\nAlthough a large number of attempts have been made regarding feature extraction\ntechniques, the methods require great human involvements are heavily depend on\ndomain expertise and may thus be non-representative and biased from application\nto application. On the other hand, while the deep neural networks based\napproaches feature adaptive feature extractions and inherent classifications,\nthey usually require a substantial set of training data and thus hinder their\nusage for engineering applications with limited training data such as gearbox\nfault diagnosis. This paper develops a deep convolutional neural network-based\ntransfer learning approach that not only entertains pre-processing free\nadaptive feature extractions, but also requires only a small set of training\ndata. The proposed approach performs gear fault diagnosis using pre-processing\nfree raw accelerometer data and experiments with various sizes of training data\nwere conducted. The superiority of the proposed approach is revealed by\ncomparing the performance with other methods such as locally trained\nconvolution neural network and angle-frequency analysis based support vector\nmachine. The achieved accuracy indicates that the proposed approach is not only\nviable and robust, but also has the potential to be readily applicable to other\nfault diagnosis practices.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 17:39:00 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Cao", "Pei", ""], ["Zhang", "Shengli", ""], ["Tang", "Jiong", ""]]}, {"id": "1710.08961", "submitter": "Milad Makkie", "authors": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming\n  Liu", "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI\n  Big Data Analytics", "comments": "This work is submitted to SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, analyzing task-based fMRI (tfMRI) data has become an\nessential tool for understanding brain function and networks. However, due to\nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of\nground truth of underlying neural activities, modeling tfMRI data is hard and\nchallenging. Previously proposed data-modeling methods including Independent\nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly\nestablished model based on blind source separation under the strong assumption\nthat original fMRI signals could be linearly decomposed into time series\ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a\nlarge amount of tfMRI data from a variety of subjects has been shown to be very\ndemanding but yet challenging even with technological advances in computational\nhardware. Given the Convolutional Neural Network (CNN), a robust method for\nlearning high-level abstractions from low-level data such as tfMRI time series,\nin this work we propose a fast and scalable novel framework for distributed\ndeep Convolutional Autoencoder model. This model aims to both learn the complex\nhierarchical structure of the tfMRI data and to leverage the processing power\nof multiple GPUs in a distributed fashion. To implement such a model, we have\ncreated an enhanced processing pipeline on the top of Apache Spark and\nTensorflow library, leveraging from a very large cluster of GPU machines.\nExperimental data from applying the model on the Human Connectome Project (HCP)\nshow that the proposed model is efficient and scalable toward tfMRI big data\nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific\ninformation from massive fMRI big data in the future.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 19:35:51 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 07:46:58 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 21:31:55 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Makkie", "Milad", ""], ["Huang", "Heng", ""], ["Zhao", "Yu", ""], ["Vasilakos", "Athanasios V.", ""], ["Liu", "Tianming", ""]]}, {"id": "1710.09288", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Gregory D. Hager, Xiaohui Xie", "title": "Adversarial Deep Structured Nets for Mass Segmentation from Mammograms", "comments": "Accepted by ISBI2018. arXiv admin note: substantial text overlap with\n  arXiv:1612.05970", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mass segmentation provides effective morphological features which are\nimportant for mass diagnosis. In this work, we propose a novel end-to-end\nnetwork for mammographic mass segmentation which employs a fully convolutional\nnetwork (FCN) to model a potential function, followed by a CRF to perform\nstructured learning. Because the mass distribution varies greatly with pixel\nposition, the FCN is combined with a position priori. Further, we employ\nadversarial training to eliminate over-fitting due to the small sizes of\nmammogram datasets. Multi-scale FCN is employed to improve the segmentation\nperformance. Experimental results on two public datasets, INbreast and\nDDSM-BCRP, demonstrate that our end-to-end network achieves better performance\nthan state-of-the-art approaches.\n\\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git}\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 06:54:43 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 07:50:09 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhu", "Wentao", ""], ["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""], ["Hager", "Gregory D.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1710.09300", "submitter": "Filipe Alves Neto Verri", "authors": "Filipe Alves Neto Verri, Renato Tin\\'os, Liang Zhao", "title": "Feature learning in feature-sample networks using multi-objective\n  optimization", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": "10.1109/CEC.2018.8477891", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data and knowledge representation are fundamental concepts in machine\nlearning. The quality of the representation impacts the performance of the\nlearning model directly. Feature learning transforms or enhances raw data to\nstructures that are effectively exploited by those models. In recent years,\nseveral works have been using complex networks for data representation and\nanalysis. However, no feature learning method has been proposed for such\ncategory of techniques. Here, we present an unsupervised feature learning\nmechanism that works on datasets with binary features. First, the dataset is\nmapped into a feature--sample network. Then, a multi-objective optimization\nprocess selects a set of new vertices to produce an enhanced version of the\nnetwork. The new features depend on a nonlinear function of a combination of\npreexisting features. Effectively, the process projects the input data into a\nhigher-dimensional space. To solve the optimization problem, we design two\nmetaheuristics based on the lexicographic genetic algorithm and the improved\nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced\nnetwork contains more information and can be exploited to improve the\nperformance of machine learning methods. The advantages and disadvantages of\neach optimization strategy are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 15:18:27 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Verri", "Filipe Alves Neto", ""], ["Tin\u00f3s", "Renato", ""], ["Zhao", "Liang", ""]]}, {"id": "1710.09431", "submitter": "Yoav Levine", "authors": "Yoav Levine, Or Sharir, Alon Ziv and Amnon Shashua", "title": "On the Long-Term Memory of Deep Recurrent Networks", "comments": "An earlier version of this paper was accepted to the workshop track\n  of the 6th International Conference on Learning Representations (ICLR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key attribute that drives the unprecedented success of modern Recurrent\nNeural Networks (RNNs) on learning tasks which involve sequential data, is\ntheir ability to model intricate long-term temporal dependencies. However, a\nwell established measure of RNNs long-term memory capacity is lacking, and thus\nformal understanding of the effect of depth on their ability to correlate data\nthroughout time is limited. Specifically, existing depth efficiency results on\nconvolutional networks do not suffice in order to account for the success of\ndeep RNNs on data of varying lengths. In order to address this, we introduce a\nmeasure of the network's ability to support information flow across time,\nreferred to as the Start-End separation rank, which reflects the distance of\nthe function realized by the recurrent network from modeling no dependency\nbetween the beginning and end of the input sequence. We prove that deep\nrecurrent networks support Start-End separation ranks which are combinatorially\nhigher than those supported by their shallow counterparts. Thus, we establish\nthat depth brings forth an overwhelming advantage in the ability of recurrent\nnetworks to model long-term dependencies, and provide an exemplar of\nquantifying this key attribute which may be readily extended to other RNN\narchitectures of interest, e.g. variants of LSTM networks. We obtain our\nresults by considering a class of recurrent networks referred to as Recurrent\nArithmetic Circuits, which merge the hidden state with the input via the\nMultiplicative Integration operation, and empirically demonstrate the discussed\nphenomena on common RNNs. Finally, we employ the tool of quantum Tensor\nNetworks to gain additional graphic insight regarding the complexity brought\nforth by depth in recurrent networks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 19:34:33 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 13:33:04 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Levine", "Yoav", ""], ["Sharir", "Or", ""], ["Ziv", "Alon", ""], ["Shashua", "Amnon", ""]]}, {"id": "1710.09537", "submitter": "Li Jing", "authors": "Rumen Dangovski and Li Jing and Marin Soljacic", "title": "Rotational Unit of Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concepts of unitary evolution matrices and associative memory have\nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art\nperformance in a variety of sequential tasks. However, RNN still have a limited\ncapacity to manipulate long-term memory. To bypass this weakness the most\nsuccessful applications of RNN use external techniques such as attention\nmechanisms. In this paper we propose a novel RNN model that unifies the\nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM\nis its rotational operation, which is, naturally, a unitary matrix, providing\narchitectures with the power to learn long-term dependencies by overcoming the\nvanishing and exploding gradients problem. Moreover, the rotational unit also\nserves as associative memory. We evaluate our model on synthetic memorization,\nquestion answering and language modeling tasks. RUM learns the Copying Memory\ntask completely and improves the state-of-the-art result in the Recall task.\nRUM's performance in the bAbI Question Answering task is comparable to that of\nmodels with attention mechanism. We also improve the state-of-the-art result to\n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB)\ntask, which is to signify the applications of RUM to real-world sequential\ndata. The universality of our construction, at the core of RNN, establishes RUM\nas a promising approach to language modeling, speech recognition and machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 04:36:35 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Dangovski", "Rumen", ""], ["Jing", "Li", ""], ["Soljacic", "Marin", ""]]}, {"id": "1710.09574", "submitter": "Takashi Shinozaki", "authors": "Takashi Shinozaki", "title": "Biologically Inspired Feedforward Supervised Learning for Deep\n  Self-Organizing Map Networks", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)", "journal-ref": null, "doi": null, "report-no": "MLINI/2016/05", "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel deep neural network and its supervised\nlearning method that uses a feedforward supervisory signal. The method is\ninspired by the human visual system and performs human-like association-based\nlearning without any backward error propagation. The feedforward supervisory\nsignal that produces the correct result is preceded by the target signal and\nassociates its confirmed label with the classification result of the target\nsignal. It effectively uses a large amount of information from the feedforward\nsignal, and forms a continuous and rich learning representation. The method is\nvalidated using visual recognition tasks on the MNIST handwritten dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 07:56:16 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Shinozaki", "Takashi", ""]]}, {"id": "1710.09668", "submitter": "Bin Dong Dr.", "authors": "Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong", "title": "PDE-Net: Learning PDEs from Data", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an initial attempt to learn evolution PDEs from\ndata. Inspired by the latest development of neural network designs in deep\nlearning, we propose a new feed-forward deep network, called PDE-Net, to\nfulfill two objectives at the same time: to accurately predict dynamics of\ncomplex systems and to uncover the underlying hidden PDE models. The basic idea\nof the proposed PDE-Net is to learn differential operators by learning\nconvolution kernels (filters), and apply neural networks or other machine\nlearning methods to approximate the unknown nonlinear responses. Comparing with\nexisting approaches, which either assume the form of the nonlinear response is\nknown or fix certain finite difference approximations of differential\noperators, our approach has the most flexibility by learning both differential\noperators and the nonlinear responses. A special feature of the proposed\nPDE-Net is that all filters are properly constrained, which enables us to\neasily identify the governing PDE models while still maintaining the expressive\nand predictive power of the network. These constrains are carefully designed by\nfully exploiting the relation between the orders of differential operators and\nthe orders of sum rules of filters (an important concept originated from\nwavelet theory). We also discuss relations of the PDE-Net with some existing\nnetworks in computer vision such as Network-In-Network (NIN) and Residual\nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the\npotential to uncover the hidden PDE of the observed dynamics, and predict the\ndynamical behavior for a relatively long time, even in a noisy environment.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 12:50:45 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 07:22:36 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Long", "Zichao", ""], ["Lu", "Yiping", ""], ["Ma", "Xianzhong", ""], ["Dong", "Bin", ""]]}, {"id": "1710.09825", "submitter": "Carlo Lucibello", "authors": "Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello,\n  Luca Saglietti, Enzo Tartaglione, Riccardo Zecchina", "title": "On the role of synaptic stochasticity in training low-precision neural\n  networks", "comments": "7 pages + 14 pages of supplementary material", "journal-ref": "Phys. Rev. Lett. 120, 268103 (2018)", "doi": "10.1103/PhysRevLett.120.268103", "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochasticity and limited precision of synaptic weights in neural network\nmodels are key aspects of both biological and hardware modeling of learning\nprocesses. Here we show that a neural network model with stochastic binary\nweights naturally gives prominence to exponentially rare dense regions of\nsolutions with a number of desirable properties such as robustness and good\ngeneralization performance, while typical solutions are isolated and hard to\nfind. Binary solutions of the standard perceptron problem are obtained from a\nsimple gradient descent procedure on a set of real values parametrizing a\nprobability distribution over the binary synapses. Both analytical and\nnumerical results are presented. An algorithmic extension aimed at training\ndiscrete deep neural networks is also investigated.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:42:23 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 03:17:32 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Baldassi", "Carlo", ""], ["Gerace", "Federica", ""], ["Kappen", "Hilbert J.", ""], ["Lucibello", "Carlo", ""], ["Saglietti", "Luca", ""], ["Tartaglione", "Enzo", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1710.09867", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Stephen Clark, Karl Moritz Hermann, Phil Blunsom", "title": "Understanding Early Word Learning in Situated Artificial Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based systems can now learn to locate the referents of words\nand phrases in images, answer questions about visual scenes, and execute\nsymbolic instructions as first-person actors in partially-observable worlds. To\nachieve this so-called grounded language learning, models must overcome\nchallenges that infants face when learning their first words. While it is\nnotable that models with no meaningful prior knowledge overcome these\nobstacles, researchers currently lack a clear understanding of how they do so,\na problem that we attempt to address in this paper. For maximum control and\ngenerality, we focus on a simple neural network-based language learning agent,\ntrained via policy-gradient methods, which can interpret single-word\ninstructions in a simulated 3D world. Whilst the goal is not to explicitly\nmodel infant word learning, we take inspiration from experimental paradigms in\ndevelopmental psychology and apply some of these to the artificial agent,\nexploring the conditions under which established human biases and learning\neffects emerge. We further propose a novel method for visualising semantic\nrepresentations in the agent.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:48:20 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 17:43:34 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Hill", "Felix", ""], ["Clark", "Stephen", ""], ["Hermann", "Karl Moritz", ""], ["Blunsom", "Phil", ""]]}, {"id": "1710.09875", "submitter": "Jacob Carroll", "authors": "Jacob Carroll, Nils Carlson, and Garrett T. Kenyon", "title": "Phase Transitions in Image Denoising via Sparsely Coding Convolutional\n  Neural Networks", "comments": "4 pages, 3 figures, submitted to NIPS 2017 workshop: Advances in\n  Modeling and Learning Interactions from Complex Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.stat-mech cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are analogous in many ways to spin glasses, systems which are\nknown for their rich set of dynamics and equally complex phase diagrams. We\napply well-known techniques in the study of spin glasses to a convolutional\nsparsely encoding neural network and observe power law finite-size scaling\nbehavior in the sparsity and reconstruction error as the network denoises\n32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the\npresence of a continuous phase transition at a critical value of this sparsity.\nBy using the power law scaling relations inherent to finite-size scaling, we\ncan determine the optimal value of sparsity for any network size by tuning the\nsystem to the critical point and operate the system at the minimum denoising\nerror.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:09:54 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Carroll", "Jacob", ""], ["Carlson", "Nils", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1710.09934", "submitter": "William Severa", "authors": "William M. Severa, Jerilyn A. Timlin, Suraj Kholwadwala, Conrad D.\n  James, James B. Aimone", "title": "Data-driven Feature Sampling for Deep Hyperspectral Classification and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high dimensionality of hyperspectral imaging forces unique challenges in\nscope, size and processing requirements. Motivated by the potential for an\nin-the-field cell sorting detector, we examine a $\\textit{Synechocystis sp.}$\nPCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or\ndeplete cultures. We use deep learning techniques to both successfully classify\ncells and generate a mask segmenting the cells/condition from the background.\nFurther, we use the classification accuracy to guide a data-driven, iterative\nfeature selection method, allowing the design neural networks requiring 90%\nfewer input features with little accuracy degradation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 22:45:28 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Severa", "William M.", ""], ["Timlin", "Jerilyn A.", ""], ["Kholwadwala", "Suraj", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "1710.10196", "submitter": "Samuli Laine", "authors": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen", "title": "Progressive Growing of GANs for Improved Quality, Stability, and\n  Variation", "comments": "Final ICLR 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 15:28:35 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 14:39:27 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 15:33:34 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Laine", "Samuli", ""], ["Lehtinen", "Jaakko", ""]]}, {"id": "1710.10248", "submitter": "Vasily Pestun", "authors": "Vasily Pestun, Yiannis Vlassopoulos", "title": "Tensor network language model", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical model suitable for machine learning of systems\nwith long distance correlations such as natural languages. The model is based\non directed acyclic graph decorated by multi-linear tensor maps in the vertices\nand vector spaces in the edges, called tensor network. Such tensor networks\nhave been previously employed for effective numerical computation of the\nrenormalization group flow on the space of effective quantum field theories and\nlattice models of statistical mechanics. We provide explicit algebro-geometric\nanalysis of the parameter moduli space for tree graphs, discuss model\nproperties and applications such as statistical translation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 17:26:57 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 16:03:48 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Pestun", "Vasily", ""], ["Vlassopoulos", "Yiannis", ""]]}, {"id": "1710.10296", "submitter": "Yufeng Hao", "authors": "Yufeng Hao, Steven Quigley", "title": "The implementation of a Deep Recurrent Neural Network Language Model on\n  a Xilinx FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, FPGA has been increasingly applied to problems such as speech\nrecognition, machine learning, and cloud computation such as the Bing search\nengine used by Microsoft. This is due to FPGAs great parallel computation\ncapacity as well as low power consumption compared to general purpose\nprocessors. However, these applications mainly focus on large scale FPGA\nclusters which have an extreme processing power for executing massive matrix or\nconvolution operations but are unsuitable for portable or mobile applications.\nThis paper describes research on single-FPGA platform to explore the\napplications of FPGAs in these fields. In this project, we design a Deep\nRecurrent Neural Network (DRNN) Language Model (LM) and implement a hardware\naccelerator with AXI Stream interface on a PYNQ board which is equipped with a\nXILINX ZYNQ SOC XC7Z020 1CLG400C. The PYNQ has not only abundant programmable\nlogic resources but also a flexible embedded operation system, which makes it\nsuitable to be applied in the natural language processing field. We design the\nDRNN language model with Python and Theano, train the model on a CPU platform,\nand deploy the model on a PYNQ board to validate the model with Jupyter\nnotebook. Meanwhile, we design the hardware accelerator with Overlay, which is\na kind of hardware library on PYNQ, and verify the acceleration effect on the\nPYNQ board. Finally, we have found that the DRNN language model can be deployed\non the embedded system smoothly and the Overlay accelerator with AXI Stream\ninterface performs at 20 GOPS processing throughput, which constitutes a 70.5X\nand 2.75X speed up compared to the work in Ref.30 and Ref.31 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 07:34:48 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 22:14:09 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 13:51:15 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Hao", "Yufeng", ""], ["Quigley", "Steven", ""]]}, {"id": "1710.10304", "submitter": "Scott Reed", "authors": "Scott Reed, Yutian Chen, Thomas Paine, A\\\"aron van den Oord, S. M. Ali\n  Eslami, Danilo Rezende, Oriol Vinyals, Nando de Freitas", "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive models have shown state-of-the-art performance in density\nestimation for natural images on large-scale datasets such as ImageNet.\nHowever, such models require many thousands of gradient-based weight updates\nand unique image examples for training. Ideally, the models would rapidly learn\nvisual concepts from only a handful of examples, similar to the manner in which\nhumans learns across many vision tasks. In this paper, we show how 1) neural\nattention and 2) meta learning techniques can be used in combination with\nautoregressive models to enable effective few-shot density estimation. Our\nproposed modifications to PixelCNN result in state-of-the art few-shot density\nestimation on the Omniglot dataset. Furthermore, we visualize the learned\nattention policy and find that it learns intuitive algorithms for simple tasks\nsuch as image mirroring on ImageNet and handwriting on Omniglot without\nsupervision. Finally, we extend the model to natural images and demonstrate\nfew-shot image generation on the Stanford Online Products dataset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 18:58:51 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 19:42:35 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 04:30:30 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 18:00:49 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Reed", "Scott", ""], ["Chen", "Yutian", ""], ["Paine", "Thomas", ""], ["Oord", "A\u00e4ron van den", ""], ["Eslami", "S. M. Ali", ""], ["Rezende", "Danilo", ""], ["Vinyals", "Oriol", ""], ["de Freitas", "Nando", ""]]}, {"id": "1710.10355", "submitter": "Fernando Gama", "authors": "Fernando Gama, Geert Leus, Antonio G. Marques, Alejandro Ribeiro", "title": "Convolutional Neural Networks Via Node-Varying Graph Filters", "comments": "Submitted to DSW 2018 (IEEE Data Science Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are being applied to an increasing\nnumber of problems and fields due to their superior performance in\nclassification and regression tasks. Since two of the key operations that CNNs\nimplement are convolution and pooling, this type of networks is implicitly\ndesigned to act on data described by regular structures such as images.\nMotivated by the recent interest in processing signals defined in irregular\ndomains, we advocate a CNN architecture that operates on signals supported on\ngraphs. The proposed design replaces the classical convolution not with a\nnode-invariant graph filter (GF), which is the natural generalization of\nconvolution to graph domains, but with a node-varying GF. This filter extracts\ndifferent local features without increasing the output dimension of each layer\nand, as a result, bypasses the need for a pooling stage while involving only\nlocal operations. A second contribution is to replace the node-varying GF with\na hybrid node-varying GF, which is a new type of GF introduced in this paper.\nWhile the alternative architecture can still be run locally without requiring a\npooling stage, the number of trainable parameters is smaller and can be\nrendered independent of the data dimension. Tests are run on a synthetic source\nlocalization problem and on the 20NEWS dataset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 23:53:13 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 20:00:07 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Gama", "Fernando", ""], ["Leus", "Geert", ""], ["Marques", "Antonio G.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1710.10380", "submitter": "Shuai Tang", "authors": "Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia R. de Sa", "title": "Speeding up Context-based Sentence Representation Learning with\n  Non-autoregressive Convolutional Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context plays an important role in human language understanding, thus it may\nalso be useful for machines learning vector representations of language. In\nthis paper, we explore an asymmetric encoder-decoder structure for unsupervised\ncontext-based sentence representation learning. We carefully designed\nexperiments to show that neither an autoregressive decoder nor an RNN decoder\nis required. After that, we designed a model which still keeps an RNN as the\nencoder, while using a non-autoregressive convolutional decoder. We further\ncombine a suite of effective designs to significantly improve model efficiency\nwhile also achieving better performance. Our model is trained on two different\nlarge unlabelled corpora, and in both cases the transferability is evaluated on\na set of downstream NLP tasks. We empirically show that our model is simple and\nfast while producing rich sentence representations that excel in downstream\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 03:18:12 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 00:12:18 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 01:05:28 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Tang", "Shuai", ""], ["Jin", "Hailin", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["de Sa", "Virginia R.", ""]]}, {"id": "1710.10418", "submitter": "Tejas Krishna Reddy", "authors": "Tejas K, Ashok Reddy K, Pradeep Reddy D, Rajesh Kumar M", "title": "Efficient Licence Plate Detection By Unique Edge Detection Algorithm and\n  Smarter Interpretation Through IoT", "comments": "Paper has been submitted to SocPros17, 7th international conference\n  on soft computing and problem solving, Scopus indexed. If accepted paper will\n  be published in AISC series SPRINGER. Some of the extended/modified selected\n  quality papers will be published in a Special Issue of 'Swarm and\n  Evolutionary Computation journal, Elsevier (SCI). 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicles play a vital role in modern day transportation systems. Number plate\nprovides a standard means of identification for any vehicle. To serve this\npurpose, automatic licence plate recognition system was developed. This\nconsisted of four major steps: Pre-processing of the obtained image, extraction\nof licence plate region, segmentation and character recognition. In earlier\nresearch, direct application of Sobel edge detection algorithm or applying\nthreshold were used as key steps to extract the licence plate region, which\ndoes not produce effective results when the captured image is subjected to the\nhigh intensity of light. The use of morphological operations causes deformity\nin the characters during segmentation. We propose a novel algorithm to tackle\nthe mentioned issues through a unique edge detection algorithm. It is also a\ntedious task to create and update the database of required vehicles frequently.\nThis problem is solved by the use of Internet of things(IOT) where an online\ndatabase can be created and updated from any module instantly. Also, through\nIoT, we connect all the cameras in a geographical area to one server to create\na universal eye which drastically increases the probability of tracing a\nvehicle over having manual database attached to each camera for identification\npurpose.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 08:27:26 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["K", "Tejas", ""], ["K", "Ashok Reddy", ""], ["D", "Pradeep Reddy", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "1710.10451", "submitter": "Taejun Kim", "authors": "Taejun Kim, Jongpil Lee, Juhan Nam", "title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw\n  Waveforms", "comments": "Accepted for publication at ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that the end-to-end approach using convolutional neural\nnetwork (CNN) is effective in various types of machine learning tasks. For\naudio signals, the approach takes raw waveforms as input using an 1-D\nconvolution layer. In this paper, we improve the 1-D CNN architecture for music\nauto-tagging by adopting building blocks from state-of-the-art image\nclassification models, ResNets and SENets, and adding multi-level feature\naggregation to it. We compare different combinations of the modules in building\nCNN architectures. The results show that they achieve significant improvements\nover previous state-of-the-art models on the MagnaTagATune dataset and\ncomparable results on Million Song Dataset. Furthermore, we analyze and\nvisualize our model to show how the 1-D CNN operates.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 11:55:50 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 04:39:50 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Kim", "Taejun", ""], ["Lee", "Jongpil", ""], ["Nam", "Juhan", ""]]}, {"id": "1710.10460", "submitter": "Emmanuel Dauc\\'e", "authors": "Emmanuel Dauc\\'e", "title": "Toward predictive machine learning for active vision", "comments": "submitted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a comprehensive description of the active inference framework, as\nproposed by Friston (2010), under a machine-learning compliant perspective.\nStemming from a biological inspiration and the auto-encoding principles, the\nsketch of a cognitive architecture is proposed that should provide ways to\nimplement estimation-oriented control policies. Computer simulations illustrate\nthe effectiveness of the approach through a foveated inspection of the input\ndata. The pros and cons of the control policy are analyzed in detail, showing\ninteresting promises in terms of processing compression. Though optimizing\nfuture posterior entropy over the actions set is shown enough to attain locally\noptimal action selection, offline calculation using class-specific saliency\nmaps is shown better for it saves processing costs through saccades pathways\npre-processing, with a negligible effect on the recognition/compression rates.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 13:08:19 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 08:41:59 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 11:01:59 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Dauc\u00e9", "Emmanuel", ""]]}, {"id": "1710.10675", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Graham W. Taylor and Alexander Wong", "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided\n  Diagnosis of Diabetic Retinopathy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown\nconsiderable promise in recent years as a potential tool for improving clinical\ndecision support in medical oncology, particularly those based around the\nconcept of Discovery Radiomics, where radiomic sequencers are discovered\nthrough the analysis of medical imaging data. One of the main limitations with\ncurrent CAD approaches is that it is very difficult to gain insight or\nrationale as to how decisions are made, thus limiting their utility to\nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable\nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery\nRadiomics for the purpose of clinical decision support for diabetic\nretinopathy. Results: In addition to disease grading via the discovered deep\nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation\nof the decision-making process to provide better insight and understanding into\nthe decision-making process of the system. Conclusion: We demonstrate the\neffectiveness and utility of the proposed CLEAR-DR system of enhancing the\ninterpretability of diagnostic grading results for the application of diabetic\nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful\ntool to address the uninterpretability issue of current CAD systems, thus\nimproving their utility to clinicians.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 19:26:19 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kumar", "Devinder", ""], ["Taylor", "Graham W.", ""], ["Wong", "Alexander", ""]]}, {"id": "1710.10686", "submitter": "Vladimir Golkov", "authors": "Jan Kuka\\v{c}ka, Vladimir Golkov, Daniel Cremers", "title": "Regularization for Deep Learning: A Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is one of the crucial ingredients of deep learning, yet the\nterm regularization has various definitions, and regularization methods are\noften studied separately from each other. In our work we present a systematic,\nunifying taxonomy to categorize existing methods. We distinguish methods that\naffect data, network architectures, error terms, regularization terms, and\noptimization procedures. We do not provide all details about the listed\nmethods; instead, we present an overview of how the methods can be sorted into\nmeaningful categories and sub-categories. This helps revealing links and\nfundamental similarities between them. Finally, we include practical\nrecommendations both for users and for developers of new regularization\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 20:27:51 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kuka\u010dka", "Jan", ""], ["Golkov", "Vladimir", ""], ["Cremers", "Daniel", ""]]}, {"id": "1710.10704", "submitter": "Alireza Bagheri", "authors": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "title": "Training Probabilistic Spiking Neural Networks with First-to-spike\n  Decoding", "comments": "A shorter version will be published on Proc. IEEE ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at\nharnessing the energy efficiency of spike-domain processing by building on\ncomputing elements that operate on, and exchange, spikes. In this paper, the\nproblem of training a two-layer SNN is studied for the purpose of\nclassification, under a Generalized Linear Model (GLM) probabilistic neural\nmodel that was previously considered within the computational neuroscience\nliterature. Conventional classification rules for SNNs operate offline based on\nthe number of output spikes at each output neuron. In contrast, a novel\ntraining method is proposed here for a first-to-spike decoding rule, whereby\nthe SNN can perform an early classification decision once spike firing is\ndetected at an output neuron. Numerical results bring insights into the optimal\nparameter selection for the GLM neuron and on the accuracy-complexity trade-off\nperformance of conventional and first-to-spike decoding.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 22:13:53 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 07:41:15 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 04:49:44 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Bagheri", "Alireza", ""], ["Simeone", "Osvaldo", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1710.10724", "submitter": "Xiangyuan Jiang", "authors": "Xiangyuan Jiang, Shuai Li", "title": "BAS: Beetle Antennae Search Algorithm for Optimization Problems", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-heuristic algorithms have become very popular because of powerful\nperformance on the optimization problem. A new algorithm called beetle antennae\nsearch algorithm (BAS) is proposed in the paper inspired by the searching\nbehavior of longhorn beetles. The BAS algorithm imitates the function of\nantennae and the random walking mechanism of beetles in nature, and then two\nmain steps of detecting and searching are implemented. Finally, the algorithm\nis benchmarked on 2 well-known test functions, in which the numerical results\nvalidate the efficacy of the proposed BAS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 00:05:03 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Jiang", "Xiangyuan", ""], ["Li", "Shuai", ""]]}, {"id": "1710.10741", "submitter": "Yanan Sun", "authors": "Yanan Sun, Bing Xue, Mengjie Zhang and Gary G. Yen", "title": "Evolving Deep Convolutional Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary computation methods have been successfully applied to neural\nnetworks since two decades ago, while those methods cannot scale well to the\nmodern deep neural networks due to the complicated architectures and large\nquantities of connection weights. In this paper, we propose a new method using\ngenetic algorithms for evolving the architectures and connection weight\ninitialization values of a deep convolutional neural network to address image\nclassification problems. In the proposed algorithm, an efficient\nvariable-length gene encoding strategy is designed to represent the different\nbuilding blocks and the unpredictable optimal depth in convolutional neural\nnetworks. In addition, a new representation scheme is developed for effectively\ninitializing connection weights of deep convolutional neural networks, which is\nexpected to avoid networks getting stuck into local minima which is typically a\nmajor issue in the backward gradient-based optimization. Furthermore, a novel\nfitness evaluation method is proposed to speed up the heuristic search with\nsubstantially less computational resource. The proposed algorithm is examined\nand compared with 22 existing algorithms on nine widely used image\nclassification tasks, including the state-of-the-art methods. The experimental\nresults demonstrate the remarkable superiority of the proposed algorithm over\nthe state-of-the-art algorithms in terms of classification error rate and the\nnumber of parameters (weights).\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:04:07 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 00:54:22 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 23:23:51 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""], ["Yen", "Gary G.", ""]]}, {"id": "1710.10748", "submitter": "Zhenxing Cheng", "authors": "Zhenxing Cheng, Hu Wang", "title": "How we can control the crack to propagate along the specified path\n  feasibly?", "comments": "17 pages, 29 figures, Crack propagation path, Reanalysis solver, Back\n  propagation neural network, Particle swarm optimization, Extended finite\n  element method", "journal-ref": null, "doi": "10.1016/j.cma.2018.03.029", "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A controllable crack propagation (CCP) strategy is suggested. It is well\nknown that crack always leads the failure by crossing the critical domain in\nengineering structure. Therefore, the CCP method is proposed to control the\ncrack to propagate along the specified path, which is away from the critical\ndomain. To complete this strategy, two optimization methods are engaged.\nFirstly, a back propagation neural network (BPNN) assisted particle swarm\noptimization (PSO) is suggested. In this method, to improve the efficiency of\nCCP, the BPNN is used to build the metamodel instead of the forward evaluation.\nSecondly, the popular PSO is used. Considering the optimization iteration is a\ntime consuming process, an efficient reanalysis based extended finite element\nmethods (X-FEM) is used to substitute the complete X-FEM solver to calculate\nthe crack propagation path. Moreover, an adaptive subdomain partition strategy\nis suggested to improve the fitting accuracy between real crack and specified\npaths. Several typical numerical examples demonstrate that both optimization\nmethods can carry out the CCP. The selection of them should be determined by\nthe tradeoff between efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:55:46 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 02:22:55 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Cheng", "Zhenxing", ""], ["Wang", "Hu", ""]]}, {"id": "1710.10779", "submitter": "Cem Subakan", "authors": "Cem Subakan and Paris Smaragdis", "title": "Generative Adversarial Source Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative source separation methods such as non-negative matrix\nfactorization (NMF) or auto-encoders, rely on the assumption of an output\nprobability density. Generative Adversarial Networks (GANs) can learn data\ndistributions without needing a parametric assumption on the output density. We\nshow on a speech source separation experiment that, a multi-layer perceptron\ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders\ntrained with maximum likelihood, and variational auto-encoders in terms of\nsource to distortion ratio.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 05:42:25 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Subakan", "Cem", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1710.10944", "submitter": "Yuan Zeng", "authors": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen\n  Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks", "comments": "5 pages, 3 figures, Accepted by ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have shown great potential in many applications like speech\nrecognition, drug discovery, image classification, and object detection. Neural\nnetwork models are inspired by biological neural networks, but they are\noptimized to perform machine learning tasks on digital computers. The proposed\nwork explores the possibilities of using living neural networks in vitro as\nbasic computational elements for machine learning applications. A new\nsupervised STDP-based learning algorithm is proposed in this work, which\nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the\nMNIST benchmark for handwritten digit recognition.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 13:55:59 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 18:50:59 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 18:21:55 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zeng", "Yuan", ""], ["Devincentis", "Kevin", ""], ["Xiao", "Yao", ""], ["Ferdous", "Zubayer Ibne", ""], ["Guo", "Xiaochen", ""], ["Yan", "Zhiyuan", ""], ["Berdichevsky", "Yevgeny", ""]]}, {"id": "1710.11160", "submitter": "Vedran Dunjko", "authors": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "title": "Exponential improvements for quantum-accessible reinforcement learning", "comments": "14+13 pages, 4 figures; The updated version is simplified, and more\n  concisely presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computers can offer dramatic improvements over classical devices for\ndata analysis tasks such as prediction and classification. However, less is\nknown about the advantages that quantum computers may bring in the setting of\nreinforcement learning, where learning is achieved via interaction with a task\nenvironment. Here, we consider a special case of reinforcement learning, where\nthe task environment allows quantum access. In addition, we impose certain\n\"naturalness\" conditions on the task environment, which rule out the kinds of\noracle problems that are studied in quantum query complexity (and for which\nquantum speedups are well-known). Within this framework of quantum-accessible\nreinforcement learning environments, we demonstrate that quantum agents can\nachieve exponential improvements in learning efficiency, surpassing previous\nresults that showed only quadratic improvements. A key step in the proof is to\nconstruct task environments that encode well-known oracle problems, such as\nSimon's problem and Recursive Fourier Sampling, while satisfying the above\n\"naturalness\" conditions for reinforcement learning. Our results suggest that\nquantum agents may perform well in certain game-playing scenarios, where the\ngame has recursive structure, and the agent can learn by playing against\nitself.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 18:12:10 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 11:49:27 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 18:30:44 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Dunjko", "Vedran", ""], ["Liu", "Yi-Kai", ""], ["Wu", "Xingyao", ""], ["Taylor", "Jacob M.", ""]]}, {"id": "1710.11351", "submitter": "Keisuke Fukuda", "authors": "Takuya Akiba and Keisuke Fukuda and Shuji Suzuki", "title": "ChainerMN: Scalable Distributed Deep Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the keys for deep learning to have made a breakthrough in various\nfields was to utilize high computing powers centering around GPUs. Enabling the\nuse of further computing abilities by distributed processing is essential not\nonly to make the deep learning bigger and faster but also to tackle unsolved\nchallenges. We present the design, implementation, and evaluation of ChainerMN,\nthe distributed deep learning framework we have developed. We demonstrate that\nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet\ndataset up to 128 GPUs with the parallel efficiency of 90%.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 07:13:29 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Akiba", "Takuya", ""], ["Fukuda", "Keisuke", ""], ["Suzuki", "Shuji", ""]]}, {"id": "1710.11386", "submitter": "Yannic Kilcher", "authors": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "title": "Parametrizing filters of a CNN with a GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly agreed that the use of relevant invariances as a good\nstatistical bias is important in machine-learning. However, most approaches\nthat explicitly incorporate invariances into a model architecture only make use\nof very simple transformations, such as translations and rotations. Hence,\nthere is a need for methods to model and extract richer transformations that\ncapture much higher-level invariances. To that end, we introduce a tool\nallowing to parametrize the set of filters of a trained convolutional neural\nnetwork with the latent space of a generative adversarial network. We then show\nthat the method can capture highly non-linear invariances of the data by\nvisualizing their effect in the data space.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 09:24:39 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kilcher", "Yannic", ""], ["Becigneul", "Gary", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1710.11417", "submitter": "Gregory Farquhar", "authors": "Gregory Farquhar, Tim Rockt\\\"aschel, Maximilian Igl, Shimon Whiteson", "title": "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a\nsoftmax layer to form a stochastic policy network. Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\ntree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal. 2017) on multiple Atari games. Furthermore, we present ablation studies\nthat demonstrate the effect of different auxiliary losses on learning\ntransition models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 11:54:35 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 17:30:48 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Farquhar", "Gregory", ""], ["Rockt\u00e4schel", "Tim", ""], ["Igl", "Maximilian", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1710.11573", "submitter": "Abram Friesen", "authors": "Abram L. Friesen and Pedro Domingos", "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem", "comments": "14 pages (9 body, 5 pages of references and appendices)", "journal-ref": "In Proceedings of the International Conference on Learning\n  Representations (ICLR) 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks grow deeper and wider, learning networks with\nhard-threshold activations is becoming increasingly important, both for network\nquantization, which can drastically reduce time and energy requirements, and\nfor creating large integrated systems of deep networks, which may have\nnon-differentiable components and must avoid vanishing and exploding gradients\nfor effective learning. However, since gradient descent is not applicable to\nhard-threshold functions, it is not clear how to learn networks of them in a\nprincipled way. We address this problem by observing that setting targets for\nhard-threshold hidden units in order to minimize loss is a discrete\noptimization problem, and can be solved as such. The discrete optimization goal\nis to find a set of targets such that each unit, including the output, has a\nlinearly separable problem to solve. Given these targets, the network\ndecomposes into individual perceptrons, which can then be learned with standard\nconvex approaches. Based on this, we develop a recursive mini-batch algorithm\nfor learning deep hard-threshold networks that includes the popular but poorly\njustified straight-through estimator as a special case. Empirically, we show\nthat our algorithm improves classification accuracy in a number of settings,\nincluding for AlexNet and ResNet-18 on ImageNet, when compared to the\nstraight-through estimator.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:42:44 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 17:58:16 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 20:46:14 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Friesen", "Abram L.", ""], ["Domingos", "Pedro", ""]]}, {"id": "1710.11622", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Sergey Levine", "title": "Meta-Learning and Universality: Deep Representations and Gradient\n  Descent can Approximate any Learning Algorithm", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to learn is a powerful paradigm for enabling models to learn from\ndata more effectively and efficiently. A popular approach to meta-learning is\nto train a recurrent model to read in a training dataset as input and output\nthe parameters of a learned model, or output predictions for new test inputs.\nAlternatively, a more recent approach to meta-learning aims to acquire deep\nrepresentations that can be effectively fine-tuned, via standard gradient\ndescent, to new tasks. In this paper, we consider the meta-learning problem\nfrom the perspective of universality, formalizing the notion of learning\nalgorithm approximation and comparing the expressive power of the\naforementioned recurrent models to the more recent approaches that embed\ngradient descent into the meta-learner. In particular, we seek to answer the\nfollowing question: does deep representation combined with standard gradient\ndescent have sufficient capacity to approximate any learning algorithm? We find\nthat this is indeed true, and further find, in our experiments, that\ngradient-based meta-learning consistently leads to learning strategies that\ngeneralize more widely compared to those represented by recurrent models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:55:42 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 01:38:51 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 19:16:20 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}]