[{"id": "1311.0460", "submitter": "Xinyang Deng", "authors": "Xiaoge Zhang and Qi Liu and Yong Hu and Felix T.S. Chan and Sankaran\n  Mahadevan and Zili Zhang and Yong Deng", "title": "An Adaptive Amoeba Algorithm for Shortest Path Tree Computation in\n  Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an adaptive amoeba algorithm to address the shortest path\ntree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weight\nupdates consists of three categories: edge weight increases, edge weight\ndecreases, the mixture of them. Existing work on this problem solve this issue\nthrough analyzing the nodes influenced by the edge weight updates and recompute\nthese affected vertices. However, when the network becomes big, the process\nwill become complex. The proposed method can overcome the disadvantages of the\nexisting approaches. The most important feature of this algorithm is its\nadaptivity. When the edge weight changes, the proposed algorithm can recognize\nthe affected vertices and reconstruct them spontaneously. To evaluate the\nproposed adaptive amoeba algorithm, we compare it with the Label Setting\nalgorithm and Bellman-Ford algorithm. The comparison results demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 12:30:37 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Zhang", "Xiaoge", ""], ["Liu", "Qi", ""], ["Hu", "Yong", ""], ["Chan", "Felix T. S.", ""], ["Mahadevan", "Sankaran", ""], ["Zhang", "Zili", ""], ["Deng", "Yong", ""]]}, {"id": "1311.0598", "submitter": "Hiqmet Kamberaj Dr.", "authors": "Hiqmet Kamberaj", "title": "Q-Gaussian Swarm Quantum Particle Intelligence on Predicting Global\n  Minimum of Potential Energy Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a newly developed -Gaussian Swarm Quantum-like Particle\nOptimization (q-GSQPO) algorithm to determine the global minimum of the\npotential energy function. Swarm Quantum-like Particle Optimization (SQPO)\nalgorithms have been derived using different attractive potential fields to\nrepresent swarm particles moving in a quantum environment, where the one which\nuses a harmonic oscillator potential as attractive field is considered as an\nimproved version. In this paper, we propose a new SQPO that uses -Gaussian\nprobability density function for the attractive potential field (q-GSQPO)\nrather than Gaussian one (GSQPO) which corresponds to harmonic potential. The\nperformance of the q-GSQPO is compared against the GSQPO. The new algorithm\noutperforms the GSQPO on most of the time in convergence to the global optimum\nby increasing the efficiency of sampling the phase space and avoiding the\npremature convergence to local minima. Moreover, the computational efforts were\ncomparable for both algorithms. We tested the algorithm to determine the lowest\nenergy configurations of a particle moving in a 2, 5, 10, and 50 dimensional\nspaces.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 07:11:13 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Kamberaj", "Hiqmet", ""]]}, {"id": "1311.0701", "submitter": "Justin Bayer", "authors": "Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen,\n  Sebastian Urban, Patrick van der Smagt", "title": "On Fast Dropout and its Applicability to Recurrent Networks", "comments": "The experiments for the Penn Treebank corpus were erroneous and have\n  been stripped from this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are rich models for the processing of\nsequential data. Recent work on advancing the state of the art has been focused\non the optimization or modelling of RNNs, mostly motivated by adressing the\nproblems of the vanishing and exploding gradients. The control of overfitting\nhas seen considerably less attention. This paper contributes to that by\nanalyzing fast dropout, a recent regularization method for generalized linear\nmodels and neural networks from a back-propagation inspired perspective. We\nshow that fast dropout implements a quadratic form of an adaptive,\nper-parameter regularizer, which rewards large weights in the light of\nunderfitting, penalizes them for overconfident predictions and vanishes at\nminima of an unregularized training loss. The derivatives of that regularizer\nare exclusively based on the training error signal. One consequence of this is\nthe absense of a global weight attractor, which is particularly appealing for\nRNNs, since the dynamics are not biased towards a certain regime. We positively\ntest the hypothesis that this improves the performance of RNNs on four musical\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 13:56:23 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 09:53:52 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2013 15:28:07 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2014 20:49:56 GMT"}, {"version": "v5", "created": "Sun, 16 Feb 2014 10:21:01 GMT"}, {"version": "v6", "created": "Mon, 3 Mar 2014 14:48:37 GMT"}, {"version": "v7", "created": "Wed, 5 Mar 2014 19:32:29 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""], ["Korhammer", "Daniela", ""], ["Chen", "Nutan", ""], ["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1311.0966", "submitter": "Emre Neftci", "authors": "Emre Neftci, Srinjoy Das, Bruno Pedroni, Kenneth Kreutz-Delgado, and\n  Gert Cauwenberghs", "title": "Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems", "comments": "(Under review)", "journal-ref": null, "doi": "10.3389/fnins.2013.00272", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) and Deep Belief Networks have been\ndemonstrated to perform efficiently in a variety of applications, such as\ndimensionality reduction, feature learning, and classification. Their\nimplementation on neuromorphic hardware platforms emulating large-scale\nnetworks of spiking neurons can have significant advantages from the\nperspectives of scalability, power dissipation and real-time interfacing with\nthe environment. However the traditional RBM architecture and the commonly used\ntraining algorithm known as Contrastive Divergence (CD) are based on discrete\nupdates and exact arithmetics which do not directly map onto a dynamical neural\nsubstrate. Here, we present an event-driven variation of CD to train a RBM\nconstructed with Integrate & Fire (I&F) neurons, that is constrained by the\nlimitations of existing and near future neuromorphic hardware platforms. Our\nstrategy is based on neural sampling, which allows us to synthesize a spiking\nneural network that samples from a target Boltzmann distribution. The recurrent\nactivity of the network replaces the discrete steps of the CD algorithm, while\nSpike Time Dependent Plasticity (STDP) carries out the weight updates in an\nonline, asynchronous fashion. We demonstrate our approach by training an RBM\ncomposed of leaky I&F neurons with STDP synapses to learn a generative model of\nthe MNIST hand-written digit dataset, and by testing it in recognition,\ngeneration and cue integration tasks. Our results contribute to a machine\nlearning-driven approach for synthesizing networks of spiking neurons capable\nof carrying out practical, high-level functionality.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 04:53:11 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2013 19:45:07 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2013 07:04:28 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Neftci", "Emre", ""], ["Das", "Srinjoy", ""], ["Pedroni", "Bruno", ""], ["Kreutz-Delgado", "Kenneth", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1311.1090", "submitter": "Daniel Crespin Mr", "authors": "Daniel Crespin", "title": "Polyhedrons and Perceptrons Are Functionally Equivalent", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical definitions of polyhedrons and perceptron networks are\ndiscussed. The formalization of polyhedrons is done in a rather traditional\nway. For networks, previously proposed systems are developed. Perceptron\nnetworks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)\nare introduced. The main theme is that single output perceptron neural networks\nand characteristic functions of polyhedrons are one and the same class of\nfunctions. A rigorous formulation and proof that three layers suffice is\nobtained. The various constructions and results are among several steps\nrequired for algorithms that replace incremental and statistical learning with\nmore efficient, direct and exact geometric methods for calculation of\nperceptron architecture and weights.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 15:33:30 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Crespin", "Daniel", ""]]}, {"id": "1311.1294", "submitter": "Shaista Hussain", "authors": "Shaista Hussain, Arindam Basu, R. Wang and Tara Julia Hamilton", "title": "Delay Learning Architectures for Memory and Classification", "comments": "27 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neuromorphic spiking neural network, the DELTRON, that can\nremember and store patterns by changing the delays of every connection as\nopposed to modifying the weights. The advantage of this architecture over\ntraditional weight based ones is simpler hardware implementation without\nmultipliers or digital-analog converters (DACs) as well as being suited to\ntime-based computing. The name is derived due to similarity in the learning\nrule with an earlier architecture called Tempotron. The DELTRON can remember\nmore patterns than other delay-based networks by modifying a few delays to\nremember the most 'salient' or synchronous part of every spike pattern. We\npresent simulations of memory capacity and classification ability of the\nDELTRON for different random spatio-temporal spike patterns. The memory\ncapacity for noisy spike patterns and missing spikes are also shown. Finally,\nwe present SPICE simulation results of the core circuits involved in a\nreconfigurable mixed signal implementation of this architecture.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 06:10:30 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 07:26:37 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Hussain", "Shaista", ""], ["Basu", "Arindam", ""], ["Wang", "R.", ""], ["Hamilton", "Tara Julia", ""]]}, {"id": "1311.1761", "submitter": "Sergey Levine", "authors": "Sergey Levine", "title": "Exploring Deep and Recurrent Architectures for Optimal Control", "comments": "Appears in the Neural Information Processing Systems (NIPS 2013)\n  Workshop on Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated multilayer neural networks have achieved state of the art\nresults on multiple supervised tasks. However, successful applications of such\nmultilayer networks to control have so far been limited largely to the\nperception portion of the control pipeline. In this paper, we explore the\napplication of deep and recurrent neural networks to a continuous,\nhigh-dimensional locomotion task, where the network is used to represent a\ncontrol policy that maps the state of the system (represented by joint angles)\ndirectly to the torques at each joint. By using a recent reinforcement learning\nalgorithm called guided policy search, we can successfully train neural network\ncontrollers with thousands of parameters, allowing us to compare a variety of\narchitectures. We discuss the differences between the locomotion control task\nand previous supervised perception tasks, present experimental results\ncomparing various architectures, and discuss future directions in the\napplication of techniques from deep learning to the problem of optimal control.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:39:31 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Levine", "Sergey", ""]]}, {"id": "1311.1780", "submitter": "KyungHyun Cho", "authors": "Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu and Yoshua Bengio", "title": "Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks", "comments": "ECML/PKDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN).\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 18:30:37 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 03:32:43 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2013 18:32:42 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2014 22:55:24 GMT"}, {"version": "v5", "created": "Sat, 1 Feb 2014 18:17:38 GMT"}, {"version": "v6", "created": "Fri, 7 Feb 2014 18:55:42 GMT"}, {"version": "v7", "created": "Tue, 2 Sep 2014 00:53:40 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Pascanu", "Razvan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1311.2531", "submitter": "Tom Froese", "authors": "Tom Froese, Nathaniel Virgo and Takashi Ikegami", "title": "Motility at the origin of life: Its characterization and a model", "comments": "29 pages, 5 figures, Artificial Life", "journal-ref": null, "doi": "10.1162/ARTL_a_00096", "report-no": null, "categories": "cs.AI cs.NE nlin.AO nlin.PS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances in synthetic biology and artificial life, the origin\nof life is currently a hot topic of research. We review the literature and\nargue that the two traditionally competing \"replicator-first\" and\n\"metabolism-first\" approaches are merging into one integrated theory of\nindividuation and evolution. We contribute to the maturation of this more\ninclusive approach by highlighting some problematic assumptions that still lead\nto an impoverished conception of the phenomenon of life. In particular, we\nargue that the new consensus has so far failed to consider the relevance of\nintermediate timescales. We propose that an adequate theory of life must\naccount for the fact that all living beings are situated in at least four\ndistinct timescales, which are typically associated with metabolism, motility,\ndevelopment, and evolution. On this view, self-movement, adaptive behavior and\nmorphological changes could have already been present at the origin of life. In\norder to illustrate this possibility we analyze a minimal model of life-like\nphenomena, namely of precarious, individuated, dissipative structures that can\nbe found in simple reaction-diffusion systems. Based on our analysis we suggest\nthat processes in intermediate timescales could have already been operative in\nprebiotic systems. They may have facilitated and constrained changes occurring\nin the faster- and slower-paced timescales of chemical self-individuation and\nevolution by natural selection, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 18:58:46 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Froese", "Tom", ""], ["Virgo", "Nathaniel", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1311.2746", "submitter": "Emad Grais", "authors": "Emad M. Grais, Mehmet Umut Sen, Hakan Erdogan", "title": "Deep neural networks for single channel source separation", "comments": "5 pages, 2 figures, 2 tables, submitted to ICASSP2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach for single channel source separation (SCSS)\nusing a deep neural network (DNN) architecture is introduced. Unlike previous\nstudies in which DNN and other classifiers were used for classifying\ntime-frequency bins to obtain hard masks for each source, we use the DNN to\nclassify estimated source spectra to check for their validity during\nseparation. In the training stage, the training data for the source signals are\nused to train a DNN. In the separation stage, the trained DNN is utilized to\naid in estimation of each source in the mixed signal. Single channel source\nseparation problem is formulated as an energy minimization problem where each\nsource spectra estimate is encouraged to fit the trained DNN model and the\nmixed signal spectrum is encouraged to be written as a weighted sum of the\nestimated source spectra. The proposed approach works regardless of the energy\nscale differences between the source signals in the training and separation\nstages. Nonnegative matrix factorization (NMF) is used to initialize the DNN\nestimate for each source. The experimental results show that using DNN\ninitialized by NMF for source separation improves the quality of the separated\nsignal compared with using NMF for source separation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 12:03:40 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Grais", "Emad M.", ""], ["Sen", "Mehmet Umut", ""], ["Erdogan", "Hakan", ""]]}, {"id": "1311.3211", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel,\n  Karlheinz Meier", "title": "Stochastic inference with deterministic spiking neurons", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seemingly stochastic transient dynamics of neocortical circuits observed\nin vivo have been hypothesized to represent a signature of ongoing stochastic\ninference. In vitro neurons, on the other hand, exhibit a highly deterministic\nresponse to various types of stimulation. We show that an ensemble of\ndeterministic leaky integrate-and-fire neurons embedded in a spiking noisy\nenvironment can attain the correct firing statistics in order to sample from a\nwell-defined target distribution. We provide an analytical derivation of the\nactivation function on the single cell level; for recurrent networks, we\nexamine convergence towards stationarity in computer simulations and\ndemonstrate sample-based Bayesian inference in a mixed graphical model. This\nestablishes a rigorous link between deterministic neuron models and functional\nstochastic dynamics on the network level.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 17:04:41 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Bill", "Johannes", ""], ["Bytschok", "Ilja", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1311.3674", "submitter": "Hiroki Sayama", "authors": "Shelley D. Dionne, Hiroki Sayama, Francis J. Yammarino", "title": "Diversity and Social Network Structure in Collective Decision Making:\n  Evolutionary Perspectives with Agent-Based Simulations", "comments": "27 pages, 5 figures, 2 tables; accepted for publication in Complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NE cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective, especially group-based, managerial decision making is crucial in\norganizations. Using an evolutionary theoretic approach to collective decision\nmaking, agent-based simulations were conducted to investigate how human\ncollective decision making would be affected by the agents' diversity in\nproblem understanding and/or behavior in discussion, as well as by their social\nnetwork structure. Simulation results indicated that groups with consistent\nproblem understanding tended to produce higher utility values of ideas and\ndisplayed better decision convergence, but only if there was no group-level\nbias in collective problem understanding. Simulation results also indicated the\nimportance of balance between selection-oriented (i.e., exploitative) and\nvariation-oriented (i.e., explorative) behaviors in discussion to achieve\nquality final decisions. Expanding the group size and introducing non-trivial\nsocial network structure generally improved the quality of ideas at the cost of\ndecision convergence. Simulations with different social network topologies\nrevealed collective decision making on small-world networks with high local\nclustering tended to achieve highest decision quality more often than on random\nor scale-free networks. Implications of this evolutionary theory and simulation\napproach for future managerial research on collective, group, and multi-level\ndecision making are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 21:11:59 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 02:05:30 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 02:09:30 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Dionne", "Shelley D.", ""], ["Sayama", "Hiroki", ""], ["Yammarino", "Francis J.", ""]]}, {"id": "1311.3840", "submitter": "Mahmood A. Rashid", "authors": "Mahmood A. Rashid, M. A. Hakim Newton, Md. Tamjidul Hoque, and Abdul\n  Sattar", "title": "Mixing Energy Models in Genetic Algorithms for On-Lattice Protein\n  Structure Prediction", "comments": "Volume 2013, 15 pages, BioMed Research International, 2013", "journal-ref": null, "doi": "10.1155/2013/924137", "report-no": "Article ID 924137", "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein structure prediction (PSP) is computationally a very challenging\nproblem. The challenge largely comes from the fact that the energy function\nthat needs to be minimised in order to obtain the native structure of a given\nprotein is not clearly known. A high resolution 20x20 energy model could better\ncapture the behaviour of the actual energy function than a low resolution\nenergy model such as hydrophobic polar. However, the fine grained details of\nthe high resolution interaction energy matrix are often not very informative\nfor guiding the search. In contrast, a low resolution energy model could\neffectively bias the search towards certain promising directions. In this\npaper, we develop a genetic algorithm that mainly uses a high resolution energy\nmodel for protein structure evaluation but uses a low resolution HP energy\nmodel in focussing the search towards exploring structures that have\nhydrophobic cores. We experimentally show that this mixing of energy models\nleads to significant lower energy structures compared to the state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 13:09:19 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Rashid", "Mahmood A.", ""], ["Newton", "M. A. Hakim", ""], ["Hoque", "Md. Tamjidul", ""], ["Sattar", "Abdul", ""]]}, {"id": "1311.4088", "submitter": "Adel Alinezhad kolaei", "authors": "Adel Alinezhad Kolaei and Marzieh Ahmadzadeh", "title": "The Optimization of Running Queries in Relational Databases Using\n  ANT-Colony Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of optimizing queries is a cost-sensitive process and with respect\nto the number of associated tables in a query, its number of permutations grows\nexponentially. On one hand, in comparison with other operators in relational\ndatabase, join operator is the most difficult and complicated one in terms of\noptimization for reducing its runtime. Accordingly, various algorithms have so\nfar been proposed to solve this problem. On the other hand, the success of any\ndatabase management system (DBMS) means exploiting the query model. In the\ncurrent paper, the heuristic ant algorithm has been proposed to solve this\nproblem and improve the runtime of join operation. Experiments and observed\nresults reveal the efficiency of this algorithm compared to its similar\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 18:43:19 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Kolaei", "Adel Alinezhad", ""], ["Ahmadzadeh", "Marzieh", ""]]}, {"id": "1311.4987", "submitter": "Yang Yu", "authors": "Chao Qian and Yang Yu and Zhi-Hua Zhou", "title": "Analyzing Evolutionary Optimization in Noisy Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization tasks have to be handled in noisy environments, where we\ncannot obtain the exact evaluation of a solution but only a noisy one. For\nnoisy optimization tasks, evolutionary algorithms (EAs), a kind of stochastic\nmetaheuristic search algorithm, have been widely and successfully applied.\nPrevious work mainly focuses on empirical studying and designing EAs for noisy\noptimization, while, the theoretical counterpart has been little investigated.\nIn this paper, we investigate a largely ignored question, i.e., whether an\noptimization problem will always become harder for EAs in a noisy environment.\nWe prove that the answer is negative, with respect to the measurement of the\nexpected running time. The result implies that, for optimization tasks that\nhave already been quite hard to solve, the noise may not have a negative\neffect, and the easier a task the more negatively affected by the noise. On a\nrepresentative problem where the noise has a strong negative effect, we examine\ntwo commonly employed mechanisms in EAs dealing with noise, the re-evaluation\nand the threshold selection strategies. The analysis discloses that the two\nstrategies, however, both are not effective, i.e., they do not make the EA more\nnoise tolerant. We then find that a small modification of the threshold\nselection allows it to be proven as an effective strategy for dealing with the\nnoise in the problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 09:28:52 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Qian", "Chao", ""], ["Yu", "Yang", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1311.5763", "submitter": "Peter Sarlin", "authors": "Peter Sarlin", "title": "Automated and Weighted Self-Organizing Time Maps", "comments": "Preprint submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes schemes for automated and weighted Self-Organizing Time\nMaps (SOTMs). The SOTM provides means for a visual approach to evolutionary\nclustering, which aims at producing a sequence of clustering solutions. This\ntask we denote as visual dynamic clustering. The implication of an automated\nSOTM is not only a data-driven parametrization of the SOTM, but also the\nfeature of adjusting the training to the characteristics of the data at each\ntime step. The aim of the weighted SOTM is to improve learning from more\ntrustworthy or important data with an instance-varying weight. The schemes for\nautomated and weighted SOTMs are illustrated on two real-world datasets: (i)\ncountry-level risk indicators to measure the evolution of global imbalances,\nand (ii) credit applicant data to measure the evolution of firm-level credit\nrisks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 14:34:38 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Sarlin", "Peter", ""]]}, {"id": "1311.5829", "submitter": "Abdul Kadir", "authors": "Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa", "title": "Neural Network Application on Foliage Plant Identification", "comments": "8 pages", "journal-ref": "International Journal of Computer Applications Volume 29 No.9,\n  September 2011", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researches in leaf identification did not include color information\nas features. The main reason is caused by a fact that they used green colored\nleaves as samples. However, for foliage plants, plants with colorful leaves,\nfancy patterns in their leaves, and interesting plants with unique shape, color\nand also texture could not be neglected. For example, Epipremnum pinnatum\n'Aureum' and Epipremnum pinnatum 'Marble Queen' have similar patterns, same\nshape, but different colors. Combination of shape, color, texture features, and\nother attribute contained on the leaf is very useful in leaf identification. In\nthis research, Polar Fourier Transform and three kinds of geometric features\nwere used to represent shape features, color moments that consist of mean,\nstandard deviation, skewness were used to represent color features, texture\nfeatures are extracted from GLCMs, and vein features were added to improve\nperformance of the identification system. The identification system uses\nProbabilistic Neural Network (PNN) as a classifier. The result shows that the\nsystem gives average accuracy of 93.0833% for 60 kinds of foliage plants.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 08:02:20 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Kadir", "Abdul", ""], ["Nugroho", "Lukito Edi", ""], ["Susanto", "Adhi", ""], ["Santosa", "Paulus Insap", ""]]}, {"id": "1311.6091", "submitter": "Jianshu Chen", "authors": "Jianshu Chen and Li Deng", "title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained\n  by the Echo-State Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture of a recurrent neural network (RNN) with a\nfully-connected deep neural network (DNN) as its feature extractor. The RNN is\nequipped with both causal temporal prediction and non-causal look-ahead, via\nauto-regression (AR) and moving-average (MA), respectively. The focus of this\npaper is a primal-dual training method that formulates the learning of the RNN\nas a formal optimization problem with an inequality constraint that provides a\nsufficient condition for the stability of the network dynamics. Experimental\nresults demonstrate the effectiveness of this new method, which achieves 18.86%\nphone recognition error on the TIMIT benchmark for the core test set. The\nresult approaches the best result of 17.7%, which was obtained by using RNN\nwith long short-term memory (LSTM). The results also show that the proposed\nprimal-dual training method produces lower recognition errors than the popular\nRNN methods developed earlier based on the carefully tuned threshold parameter\nthat heuristically prevents the gradient from exploding.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 08:04:41 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2013 09:12:55 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 03:06:36 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Chen", "Jianshu", ""], ["Deng", "Li", ""]]}, {"id": "1311.6531", "submitter": "Va\\v{s}ek Chv\\'atal", "authors": "Va\\v{s}ek Chv\\'atal, Mark Goldsmith, and Nan Yang", "title": "Brains and pseudorandom generators", "comments": "This paper misinterprets the notion of a pseudorandom generator. For\n  this reason, it has been withdrawn by the authors. Its main result,\n  interpreted in terms of pseudorandom functions, reappears in arXiv:1603.01573\n  [math.DS]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CR cs.NE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model\nof the central nervous system; motivated by EEG recordings of normal brain\nactivity, Chv\\' atal and Goldsmith asked whether or not this model can be\nengineered to provide pseudorandom number generators. We supply evidence\nsuggesting that the answer is negative.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 01:03:30 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 15:13:30 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 08:52:47 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Chv\u00e1tal", "Va\u0161ek", ""], ["Goldsmith", "Mark", ""], ["Yang", "Nan", ""]]}, {"id": "1311.6881", "submitter": "Abhishek Pandey", "authors": "Abhishek Pandey, Anjna Jayant Deen and Rajeev Pandey (Dept. of CSE,\n  UIT-RGPV)", "title": "Color and Shape Content Based Image Classification using RBF Network and\n  PSO Technique: A Survey", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvement of the accuracy of image query retrieval used image\nclassification technique. Image classification is well known technique of\nsupervised learning. The improved method of image classification increases the\nworking efficiency of image query retrieval. For the improvements of\nclassification technique we used RBF neural network function for better\nprediction of feature used in image retrieval.Colour content is represented by\npixel values in image classification using radial base function(RBF) technique.\nThis approach provides better result compare to SVM technique in image\nrepresentation.Image is represented by matrix though RBF using pixel values of\ncolour intensity of image. Firstly we using RGB colour model. In this colour\nmodel we use red, green and blue colour intensity values in matrix.SVM with\npartical swarm optimization for image classification is implemented in content\nof images which provide better Results based on the proposed approach are found\nencouraging in terms of color image classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 07:14:25 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Pandey", "Abhishek", "", "Dept. of CSE,\n  UIT-RGPV"], ["Deen", "Anjna Jayant", "", "Dept. of CSE,\n  UIT-RGPV"], ["Pandey", "Rajeev", "", "Dept. of CSE,\n  UIT-RGPV"]]}, {"id": "1311.7213", "submitter": "Alireza Rezvanian", "authors": "Mohammad Soleimani-Pouri, Alireza Rezvanian, Mohammad Reza Meybodi", "title": "Finding a Maximum Clique using Ant Colony Optimization and Particle\n  Swarm Optimization in Social Networks", "comments": "4 pages, 3 figures, conference", "journal-ref": null, "doi": "10.1109/ASONAM.2012.20", "report-no": null, "categories": "cs.SI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction between users in online social networks plays a key role in\nsocial network analysis. One on important types of social group is full\nconnected relation between some users, which known as clique structure.\nTherefore finding a maximum clique is essential for some analysis. In this\npaper, we proposed a new method using ant colony optimization algorithm and\nparticle swarm optimization algorithm. In the proposed method, in order to\nattain better results, it is improved process of pheromone update by particle\nswarm optimization. Simulation results on popular standard social network\nbenchmarks in comparison standard ant colony optimization algorithm are shown a\nrelative enhancement of proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 05:43:32 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Soleimani-Pouri", "Mohammad", ""], ["Rezvanian", "Alireza", ""], ["Meybodi", "Mohammad Reza", ""]]}, {"id": "1311.7251", "submitter": "Michael Zibulevsky", "authors": "Joseph Shtok, Michael Zibulevsky and Michael Elad", "title": "Spatially-Adaptive Reconstruction in Computed Tomography using Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a supervised machine learning approach for boosting existing\nsignal and image recovery methods and demonstrate its efficacy on example of\nimage reconstruction in computed tomography. Our technique is based on a local\nnonlinear fusion of several image estimates, all obtained by applying a chosen\nreconstruction algorithm with different values of its control parameters.\nUsually such output images have different bias/variance trade-off. The fusion\nof the images is performed by feed-forward neural network trained on a set of\nknown examples. Numerical experiments show an improvement in reconstruction\nquality relatively to existing direct and iterative reconstruction methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 09:44:45 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Shtok", "Joseph", ""], ["Zibulevsky", "Michael", ""], ["Elad", "Michael", ""]]}]