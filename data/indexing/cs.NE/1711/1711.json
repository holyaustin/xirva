[{"id": "1711.00215", "submitter": "Bert Moons", "authors": "Bert Moons, Koen Goetschalckx, Nick Van Berckelaer, Marian Verhelst", "title": "Minimum Energy Quantized Neural Networks", "comments": "preprint for work presented at the 51st Asilomar Conference on\n  Signals, Systems and Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work targets the automated minimum-energy optimization of Quantized\nNeural Networks (QNNs) - networks using low precision weights and activations.\nThese networks are trained from scratch at an arbitrary fixed point precision.\nAt iso-accuracy, QNNs using fewer bits require deeper and wider network\narchitectures than networks using higher precision operators, while they\nrequire less complex arithmetic and less bits per weights. This fundamental\ntrade-off is analyzed and quantified to find the minimum energy QNN for any\nbenchmark and hence optimize energy-efficiency. To this end, the energy\nconsumption of inference is modeled for a generic hardware platform. This\nallows drawing several conclusions across different benchmarks. First, energy\nconsumption varies orders of magnitude at iso-accuracy depending on the number\nof bits used in the QNN. Second, in a typical system, BinaryNets or int4\nimplementations lead to the minimum energy solution, outperforming int8\nnetworks up to 2-10x at iso-accuracy. All code used for QNN training is\navailable from https://github.com/BertMoons.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 05:50:19 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 09:37:02 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Moons", "Bert", ""], ["Goetschalckx", "Koen", ""], ["Van Berckelaer", "Nick", ""], ["Verhelst", "Marian", ""]]}, {"id": "1711.00258", "submitter": "Yucen Luo", "authors": "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang", "title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning", "comments": "Accept as Spotlight in Computer Vision and Pattern Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed self-ensembling methods have achieved promising results\nin deep semi-supervised learning, which penalize inconsistent predictions of\nunlabeled data under different perturbations. However, they only consider\nadding perturbations to each single data point, while ignoring the connections\nbetween data samples. In this paper, we propose a novel method, called Smooth\nNeighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on\nthe predictions of the teacher model, i.e., the implicit self-ensemble of\nmodels. Then the graph serves as a similarity measure with respect to which the\nrepresentations of \"similar\" neighboring points are learned to be smooth on the\nlow-dimensional manifold. We achieve state-of-the-art results on\nsemi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for\nCIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,\nthe improvements are significant when the labels are fewer. For the\nnon-augmented MNIST with only 20 labels, the error rate is reduced from\nprevious 4.81% to 1.36%. Our method also shows robustness to noisy labels.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 09:10:07 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 14:53:05 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Luo", "Yucen", ""], ["Zhu", "Jun", ""], ["Li", "Mengxi", ""], ["Ren", "Yong", ""], ["Zhang", "Bo", ""]]}, {"id": "1711.00313", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with\n  Controlled Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires massive amounts of training data, but\nfor many tasks only limited labeled data is available. This makes weak\nsupervision attractive, using weak or noisy signals like the output of\nheuristic methods or user click-through data for training. In a semi-supervised\nsetting, we can use a large set of data with weak labels to pretrain a neural\nnetwork and then fine-tune the parameters with a small amount of data with true\nlabels. This feels intuitively sub-optimal as these two independent stages\nleave the model unaware about the varying label quality. What if we could\nsomehow inform the model about the label quality? In this paper, we propose a\nsemi-supervised learning method where we train two neural networks in a\nmulti-task fashion: a \"target network\" and a \"confidence network\". The target\nnetwork is optimized to perform a given task and is trained using a large set\nof unlabeled data that are weakly annotated. We propose to weight the gradient\nupdates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model. We evaluate our learning strategy on two different\ntasks: document ranking and sentiment classification. The results demonstrate\nthat our approach not only enhances the performance compared to the baselines\nbut also speeds up the learning process from weak labels.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:38:59 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:30:18 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Severyn", "Aliaksei", ""], ["Rothe", "Sascha", ""], ["Kamps", "Jaap", ""]]}, {"id": "1711.00436", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray\n  Kavukcuoglu", "title": "Hierarchical Representations for Efficient Architecture Search", "comments": "Accepted as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore efficient neural architecture search methods and show that a\nsimple yet powerful evolutionary algorithm can discover new architectures with\nexcellent performance. Our approach combines a novel hierarchical genetic\nrepresentation scheme that imitates the modularized design pattern commonly\nadopted by human experts, and an expressive search space that supports complex\ntopologies. Our algorithm efficiently discovers architectures that outperform a\nlarge number of manually designed models for image classification, obtaining\ntop-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which\nis competitive with the best existing neural architecture search approaches. We\nalso present results using random search, achieving 0.3% less top-1 accuracy on\nCIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36\nhours down to 1 hour.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:46:27 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:31:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Liu", "Hanxiao", ""], ["Simonyan", "Karen", ""], ["Vinyals", "Oriol", ""], ["Fernando", "Chrisantha", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1711.00482", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Dan Klein, Sergey Levine", "title": "Learning with Latent Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The named concepts and compositional operators present in natural language\nprovide a rich source of information about the kinds of abstractions humans use\nto navigate the world. Can this linguistic background knowledge improve the\ngenerality and efficiency of learned classifiers and control policies? This\npaper aims to show that using the space of natural language strings as a\nparameter space is an effective way to capture natural task structure. In a\npretraining phase, we learn a language interpretation model that transforms\ninputs (e.g. images) into outputs (e.g. labels) given natural language\ndescriptions. To learn a new concept (e.g. a classifier), we search directly in\nthe space of descriptions to minimize the interpreter's loss on training\nexamples. Crucially, our models do not require language data to learn these\nconcepts: language is used only in pretraining to impose structure on\nsubsequent learning. Results on image classification, text editing, and\nreinforcement learning show that, in all settings, models with a linguistic\nparameterization outperform those without.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:00:22 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Andreas", "Jacob", ""], ["Klein", "Dan", ""], ["Levine", "Sergey", ""]]}, {"id": "1711.00541", "submitter": "Yi Luo", "authors": "Yi Luo, Nima Mesgarani", "title": "TasNet: time-domain audio separation network for real-time,\n  single-channel speech separation", "comments": "Camera ready version for ICASSP 2018, Calgary, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robust speech processing in multi-talker environments requires effective\nspeech separation. Recent deep learning systems have made significant progress\ntoward solving this problem, yet it remains challenging particularly in\nreal-time, short latency applications. Most methods attempt to construct a mask\nfor each source in time-frequency representation of the mixture signal which is\nnot necessarily an optimal representation for speech separation. In addition,\ntime-frequency decomposition results in inherent problems such as\nphase/magnitude decoupling and long time window which is required to achieve\nsufficient frequency resolution. We propose Time-domain Audio Separation\nNetwork (TasNet) to overcome these limitations. We directly model the signal in\nthe time-domain using an encoder-decoder framework and perform the source\nseparation on nonnegative encoder outputs. This method removes the frequency\ndecomposition step and reduces the separation problem to estimation of source\nmasks on encoder outputs which is then synthesized by the decoder. Our system\noutperforms the current state-of-the-art causal and noncausal speech separation\nalgorithms, reduces the computational cost of speech separation, and\nsignificantly reduces the minimum required latency of the output. This makes\nTasNet suitable for applications where low-power, real-time implementation is\ndesirable such as in hearable and telecommunication devices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 21:19:22 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 02:25:29 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Luo", "Yi", ""], ["Mesgarani", "Nima", ""]]}, {"id": "1711.00549", "submitter": "Anjishnu Kumar", "authors": "Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam Tucker, Bjorn\n  Hoffmeister, Markus Dreyer, Stanislav Peshterliev, Ankur Gandhe, Denis\n  Filiminov, Ariya Rastrow, Christian Monson and Agnika Kumar", "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken\n  Language Understanding", "comments": "Published at the 1st Workshop on Conversational AI at NIPS 2017\n  (NIPS-WCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design of the machine learning architecture that\nunderlies the Alexa Skills Kit (ASK) a large scale Spoken Language\nUnderstanding (SLU) Software Development Kit (SDK) that enables developers to\nextend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the\ninfrastructure powers over 25,000 skills deployed through the ASK, as well as\nAWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability\nand a rapid iteration cycle for third party developers. It imposes inductive\nbiases that allow it to learn robust SLU models from extremely small and sparse\ndatasets and, in doing so, removes significant barriers to entry for software\ndevelopers and dialogue systems researchers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 22:10:11 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 09:19:37 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 00:37:00 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 13:58:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Kumar", "Anjishnu", ""], ["Gupta", "Arpit", ""], ["Chan", "Julian", ""], ["Tucker", "Sam", ""], ["Hoffmeister", "Bjorn", ""], ["Dreyer", "Markus", ""], ["Peshterliev", "Stanislav", ""], ["Gandhe", "Ankur", ""], ["Filiminov", "Denis", ""], ["Rastrow", "Ariya", ""], ["Monson", "Christian", ""], ["Kumar", "Agnika", ""]]}, {"id": "1711.00727", "submitter": "Wei Lyu", "authors": "Wei Lyu, Zhaoyang Zhang, Chunxu Jiao, Kangjian Qin, and Huazi Zhang", "title": "Performance Evaluation of Channel Decoding With Deep Neural Networks", "comments": "6 pages, 11 figures, Latex; typos corrected; IEEE ICC 2018 to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the demand of high data rate and low latency in fifth generation (5G),\ndeep neural network decoder (NND) has become a promising candidate due to its\ncapability of one-shot decoding and parallel computing. In this paper, three\ntypes of NND, i.e., multi-layer perceptron (MLP), convolution neural network\n(CNN) and recurrent neural network (RNN), are proposed with the same parameter\nmagnitude. The performance of these deep neural networks are evaluated through\nextensive simulation. Numerical results show that RNN has the best decoding\nperformance, yet at the price of the highest computational overhead. Moreover,\nwe find there exists a saturation length for each type of neural network, which\nis caused by their restricted learning abilities.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:21:02 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 08:43:37 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Lyu", "Wei", ""], ["Zhang", "Zhaoyang", ""], ["Jiao", "Chunxu", ""], ["Qin", "Kangjian", ""], ["Zhang", "Huazi", ""]]}, {"id": "1711.00913", "submitter": "Mohit Dubey", "authors": "Mohit Dubey, Garrett Kenyon, Nils Carlson, Austin Thresher", "title": "Does Phase Matter For Monaural Source Separation?", "comments": "4 pages, 2 figures, NIPS format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"cocktail party\" problem of fully separating multiple sources from a\nsingle channel audio waveform remains unsolved. Current biological\nunderstanding of neural encoding suggests that phase information is preserved\nand utilized at every stage of the auditory pathway. However, current\ncomputational approaches primarily discard phase information in order to mask\namplitude spectrograms of sound. In this paper, we seek to address whether\npreserving phase information in spectral representations of sound provides\nbetter results in monaural separation of vocals from a musical track by using a\nneurally plausible sparse generative model. Our results demonstrate that\npreserving phase information reduces artifacts in the separated tracks, as\nquantified by the signal to artifact ratio (GSAR). Furthermore, our proposed\nmethod achieves state-of-the-art performance for source separation, as\nquantified by a mean signal to interference ratio (GSIR) of 19.46.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 20:10:00 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Dubey", "Mohit", ""], ["Kenyon", "Garrett", ""], ["Carlson", "Nils", ""], ["Thresher", "Austin", ""]]}, {"id": "1711.00956", "submitter": "Chao Qian", "authors": "Chao Qian, Chao Bian, Wu Jiang, Ke Tang", "title": "Running Time Analysis of the (1+1)-EA for OneMax and LeadingOnes under\n  Bit-wise Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world optimization problems, the objective function evaluation\nis subject to noise, and we cannot obtain the exact objective value.\nEvolutionary algorithms (EAs), a type of general-purpose randomized\noptimization algorithm, have shown able to solve noisy optimization problems\nwell. However, previous theoretical analyses of EAs mainly focused on\nnoise-free optimization, which makes the theoretical understanding largely\ninsufficient. Meanwhile, the few existing theoretical studies under noise often\nconsidered the one-bit noise model, which flips a randomly chosen bit of a\nsolution before evaluation; while in many realistic applications, several bits\nof a solution can be changed simultaneously. In this paper, we study a natural\nextension of one-bit noise, the bit-wise noise model, which independently flips\neach bit of a solution with some probability. We analyze the running time of\nthe (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first\ntime, and derive the ranges of the noise level for polynomial and\nsuper-polynomial running time bounds. The analysis on LeadingOnes under\nbit-wise noise can be easily transferred to one-bit noise, and improves the\npreviously known results. Since our analysis discloses that the (1+1)-EA can be\nefficient only under low noise levels, we also study whether the sampling\nstrategy can bring robustness to noise. We prove that using sampling can\nsignificantly increase the largest noise level allowing a polynomial running\ntime, that is, sampling is robust to noise.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 22:00:21 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Qian", "Chao", ""], ["Bian", "Chao", ""], ["Jiang", "Wu", ""], ["Tang", "Ke", ""]]}, {"id": "1711.00970", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar, Ludwig Schmidt and Aleksander M\\k{a}dry", "title": "A Classification-Based Study of Covariate Shift in GAN Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic, and still largely unanswered, question in the context of Generative\nAdversarial Networks (GANs) is whether they are truly able to capture all the\nfundamental characteristics of the distributions they are trained on. In\nparticular, evaluating the diversity of GAN distributions is challenging and\nexisting methods provide only a partial understanding of this issue. In this\npaper, we develop quantitative and scalable tools for assessing the diversity\nof GAN distributions. Specifically, we take a classification-based perspective\nand view loss of diversity as a form of covariate shift introduced by GANs. We\nexamine two specific forms of such shift: mode collapse and boundary\ndistortion. In contrast to prior work, our methods need only minimal human\nsupervision and can be readily applied to state-of-the-art GANs on large,\ncanonical datasets. Examining popular GANs using our tools indicates that these\nGANs have significant problems in reproducing the more distributional\nproperties of their training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 23:13:39 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 14:41:30 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 16:46:20 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 02:07:59 GMT"}, {"version": "v5", "created": "Wed, 30 May 2018 03:23:47 GMT"}, {"version": "v6", "created": "Sat, 2 Jun 2018 03:49:25 GMT"}, {"version": "v7", "created": "Wed, 6 Jun 2018 02:51:11 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Santurkar", "Shibani", ""], ["Schmidt", "Ludwig", ""], ["M\u0105dry", "Aleksander", ""]]}, {"id": "1711.01201", "submitter": "Dhireesha Kudithipudi", "authors": "Dillon Graham, Seyed Hamed Fatemi Langroudi, Christopher Kanan, and\n  Dhireesha Kudithipudi", "title": "Convolutional Drift Networks for Video Classification", "comments": "Published in IEEE Rebooting Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing spatio-temporal data like video is a challenging task that requires\nprocessing visual and temporal information effectively. Convolutional Neural\nNetworks have shown promise as baseline fixed feature extractors through\ntransfer learning, a technique that helps minimize the training cost on visual\ninformation. Temporal information is often handled using hand-crafted features\nor Recurrent Neural Networks, but this can be overly specific or prohibitively\ncomplex. Building a fully trainable system that can efficiently analyze\nspatio-temporal data without hand-crafted features or complex training is an\nopen challenge. We present a new neural network architecture to address this\nchallenge, the Convolutional Drift Network (CDN). Our CDN architecture combines\nthe visual feature extraction power of deep Convolutional Neural Networks with\nthe intrinsically efficient temporal processing provided by Reservoir\nComputing. In this introductory paper on the CDN, we provide a very simple\nbaseline implementation tested on two egocentric (first-person) video activity\ndatasets.We achieve video-level activity classification results on-par with\nstate-of-the art methods. Notably, performance on this complex spatio-temporal\ntask was produced by only training a single feed-forward layer in the CDN.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 15:07:24 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Graham", "Dillon", ""], ["Langroudi", "Seyed Hamed Fatemi", ""], ["Kanan", "Christopher", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1711.01239", "submitter": "Clemens Rosenbaum", "authors": "Clemens Rosenbaum, Tim Klinger and Matthew Riemer", "title": "Routing Networks: Adaptive Selection of Non-linear Functions for\n  Multi-Task Learning", "comments": "Under Review at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) with neural networks leverages commonalities in\ntasks to improve performance, but often suffers from task interference which\nreduces the benefits of transfer. To address this issue we introduce the\nrouting network paradigm, a novel neural network and training algorithm. A\nrouting network is a kind of self-organizing neural network consisting of two\ncomponents: a router and a set of one or more function blocks. A function block\nmay be any neural network - for example a fully-connected or a convolutional\nlayer. Given an input the router makes a routing decision, choosing a function\nblock to apply and passing the output back to the router recursively,\nterminating when a fixed recursion depth is reached. In this way the routing\nnetwork dynamically composes different function blocks for each input. We\nemploy a collaborative multi-agent reinforcement learning (MARL) approach to\njointly train the router and function blocks. We evaluate our model against\ncross-stitch networks and shared-layer baselines on multi-task settings of the\nMNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a\nsignificant improvement in accuracy, with sharper convergence. In addition,\nrouting networks have nearly constant per-task training cost while cross-stitch\nnetworks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we\nobtain cross-stitch performance levels with an 85% reduction in training time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:07:51 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 14:53:00 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Rosenbaum", "Clemens", ""], ["Klinger", "Tim", ""], ["Riemer", "Matthew", ""]]}, {"id": "1711.01243", "submitter": "Mohammad Ghasemzadeh", "authors": "Mohammad Ghasemzadeh, Mohammad Samragh, Farinaz Koushanfar", "title": "ReBNet: Residual Binarized Neural Network", "comments": "To Appear In The 26th IEEE International Symposium on\n  Field-Programmable Custom Computing Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes ReBNet, an end-to-end framework for training\nreconfigurable binary neural networks on software and developing efficient\naccelerators for execution on FPGA. Binary neural networks offer an intriguing\nopportunity for deploying large-scale deep learning models on\nresource-constrained devices. Binarization reduces the memory footprint and\nreplaces the power-hungry matrix-multiplication with light-weight XnorPopcount\noperations. However, binary networks suffer from a degraded accuracy compared\nto their fixed-point counterparts. We show that the state-of-the-art methods\nfor optimizing binary networks accuracy, significantly increase the\nimplementation cost and complexity. To compensate for the degraded accuracy\nwhile adhering to the simplicity of binary networks, we devise the first\nreconfigurable scheme that can adjust the classification accuracy based on the\napplication. Our proposition improves the classification accuracy by\nrepresenting features with multiple levels of residual binarization. Unlike\nprevious methods, our approach does not exacerbate the area cost of the\nhardware accelerator. Instead, it provides a tradeoff between throughput and\naccuracy while the area overhead of multi-level binarization is negligible.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:12:15 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 00:45:57 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 20:58:01 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Ghasemzadeh", "Mohammad", ""], ["Samragh", "Mohammad", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1711.01343", "submitter": "Sourya Dey", "authors": "Sourya Dey, Yinan Shao, Keith M. Chugg and Peter A. Beerel", "title": "Accelerating Training of Deep Neural Networks via Sparse Edge Processing", "comments": "Presented at the 26th International Conference on Artificial Neural\n  Networks (ICANN) 2017 in Alghero, Italy", "journal-ref": "Proceedings Part 1 of ICANN 2017, pp 273-280. Lecture Notes in\n  Computer Science, vol 10613. Springer, Cham", "doi": "10.1007/978-3-319-68600-4_32", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reconfigurable hardware architecture for deep neural networks\n(DNNs) capable of online training and inference, which uses algorithmically\npre-determined, structured sparsity to significantly lower memory and\ncomputational requirements. This novel architecture introduces the notion of\nedge-processing to provide flexibility and combines junction pipelining and\noperational parallelization to speed up training. The overall effect is to\nreduce network complexity by factors up to 30x and training time by up to 35x\nrelative to GPUs, while maintaining high fidelity of inference results. This\nhas the potential to enable extensive parameter searches and development of the\nlargely unexplored theoretical foundation of DNNs. The architecture\nautomatically adapts itself to different network sizes given available hardware\nresources. As proof of concept, we show results obtained for different bit\nwidths.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:44:08 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Dey", "Sourya", ""], ["Shao", "Yinan", ""], ["Chugg", "Keith M.", ""], ["Beerel", "Peter A.", ""]]}, {"id": "1711.01416", "submitter": "Vasily Pestun", "authors": "Vasily Pestun, John Terilla, Yiannis Vlassopoulos", "title": "Language as a matrix product state", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical model for natural language that begins by\nconsidering language as a monoid, then representing it in complex matrices with\na compatible translation invariant probability measure. We interpret the\nprobability measure as arising via the Born rule from a translation invariant\nmatrix product state.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 09:11:18 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Pestun", "Vasily", ""], ["Terilla", "John", ""], ["Vlassopoulos", "Yiannis", ""]]}, {"id": "1711.01436", "submitter": "Magdalena Fuchs", "authors": "Magdalena Fuchs, Manuel Zimmer, Radu Grosu, Ramin M. Hasani", "title": "Searching for Biophysically Realistic Parameters for Dynamic Neuron\n  Models by Genetic Algorithms from Calcium Imaging Recording", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual Neurons in the nervous systems exploit various dynamics. To\ncapture these dynamics for single neurons, we tune the parameters of an\nelectrophysiological model of nerve cells, to fit experimental data obtained by\ncalcium imaging. A search for the biophysical parameters of this model is\nperformed by means of a genetic algorithm, where the model neuron is exposed to\na predefined input current representing overall inputs from other parts of the\nnervous system. The algorithm is then constrained for keeping the ion-channel\ncurrents within reasonable ranges, while producing the best fit to a calcium\nimaging time series of the AVA interneuron, from the brain of the soil-worm, C.\nelegans. Our settings enable us to project a set of biophysical parameters to\nthe the neuron kinetics observed in neuronal imaging.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 13:43:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Fuchs", "Magdalena", ""], ["Zimmer", "Manuel", ""], ["Grosu", "Radu", ""], ["Hasani", "Ramin M.", ""]]}, {"id": "1711.01559", "submitter": "Louis-Serge Bouchard", "authors": "K. Youssef, Louis-S. Bouchard, K.Z. Haigh, H. Krovi, J. Silovsky, C.P.\n  Vander Valk", "title": "Machine Learning Approach to RF Transmitter Identification", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development and widespread use of wireless devices in recent years\n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has\nbecome extremely crowded. In order to counter security threats posed by rogue\nor unknown transmitters, it is important to identify RF transmitters not by the\ndata content of the transmissions but based on the intrinsic physical\ncharacteristics of the transmitters. RF waveforms represent a particular\nchallenge because of the extremely high data rates involved and the potentially\nlarge number of transmitters present in a given location. These factors outline\nthe need for rapid fingerprinting and identification methods that go beyond the\ntraditional hand-engineered approaches. In this study, we investigate the use\nof machine learning (ML) strategies to the classification and identification\nproblems, and the use of wavelets to reduce the amount of data required. Four\ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional\nneural nets (CNN), support vector machines (SVM), and multi-stage training\n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method\npreconditioned by wavelets was by far the most accurate, achieving 100%\nclassification accuracy of transmitters, as tested using data originating from\n12 different transmitters. We discuss strategies for extension of MST to a much\nlarger number of transmitters.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 10:41:05 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 06:14:22 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Youssef", "K.", ""], ["Bouchard", "Louis-S.", ""], ["Haigh", "K. Z.", ""], ["Krovi", "H.", ""], ["Silovsky", "J.", ""], ["Valk", "C. P. Vander", ""]]}, {"id": "1711.01577", "submitter": "Zhen He", "authors": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence\n  Learning", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a popular approach to boosting the ability\nof Recurrent Neural Networks to store longer term temporal information. The\ncapacity of an LSTM network can be increased by widening and adding layers.\nHowever, usually the former introduces additional parameters, while the latter\nincreases the runtime. As an alternative we propose the Tensorized LSTM in\nwhich the hidden states are represented by tensors and updated via a\ncross-layer convolution. By increasing the tensor size, the network can be\nwidened efficiently without additional parameters since the parameters are\nshared across different locations in the tensor; by delaying the output, the\nnetwork can be deepened implicitly with little additional runtime since deep\ncomputations for each timestep are merged into temporal computations of the\nsequence. Experiments conducted on five challenging sequence learning tasks\nshow the potential of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:30:35 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:41:28 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 02:14:14 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["He", "Zhen", ""], ["Gao", "Shaobing", ""], ["Xiao", "Liang", ""], ["Liu", "Daxue", ""], ["He", "Hangen", ""], ["Barber", "David", ""]]}, {"id": "1711.02017", "submitter": "Xiaoliang Dai", "authors": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have begun to have a pervasive impact on various\napplications of machine learning. However, the problem of finding an optimal\nDNN architecture for large applications is challenging. Common approaches go\nfor deeper and larger DNN architectures but may incur substantial redundancy.\nTo address these problems, we introduce a network growth algorithm that\ncomplements network pruning to learn both weights and compact DNN architectures\nduring training. We propose a DNN synthesis tool (NeST) that combines both\nmethods to automate the generation of compact and accurate DNNs. NeST starts\nwith a randomly initialized sparse network called the seed architecture. It\niteratively tunes the architecture with gradient-based growth and\nmagnitude-based pruning of neurons and connections. Our experimental results\nshow that NeST yields accurate, yet very compact DNNs, with a wide range of\nseed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we\nreduce network parameters by 70.2x (74.3x) and floating-point operations\n(FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce\nnetwork parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively.\nNeST's grow-and-prune paradigm delivers significant additional parameter and\nFLOPs reduction relative to pruning-only methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:03:39 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 18:45:01 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 04:04:09 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Dai", "Xiaoliang", ""], ["Yin", "Hongxu", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1711.02114", "submitter": "Thiago Serra", "authors": "Thiago Serra and Christian Tjandraatmadja and Srikumar Ramalingam", "title": "Bounding and Counting Linear Regions of Deep Neural Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the complexity of deep neural networks (DNN) that represent\npiecewise linear (PWL) functions. In particular, we study the number of linear\nregions, i.e. pieces, that a PWL function represented by a DNN can attain, both\ntheoretically and empirically. We present (i) tighter upper and lower bounds\nfor the maximum number of linear regions on rectifier networks, which are exact\nfor inputs of dimension one; (ii) a first upper bound for multi-layer maxout\nnetworks; and (iii) a first method to perform exact enumeration or counting of\nthe number of regions by modeling the DNN with a mixed-integer linear\nformulation. These bounds come from leveraging the dimension of the space\ndefining each linear region. The results also indicate that a deep rectifier\nnetwork can only have more linear regions than every shallow counterpart with\nsame number of neurons if that number exceeds the dimension of the input.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:06:12 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 13:18:46 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 10:30:13 GMT"}, {"version": "v4", "created": "Sun, 16 Sep 2018 01:36:41 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Serra", "Thiago", ""], ["Tjandraatmadja", "Christian", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1711.02282", "submitter": "Nan Rosemary Ke", "authors": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio", "title": "Variational Walkback: Learning a Transition Operator as a Stochastic\n  Recurrent Net", "comments": "To appear at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to directly learn a stochastic transition operator\nwhose repeated application provides generated samples. Traditional undirected\ngraphical models approach this problem indirectly by learning a Markov chain\nmodel whose stationary distribution obeys detailed balance with respect to a\nparameterized energy function. The energy function is then modified so the\nmodel and data distributions match, with no guarantee on the number of steps\nrequired for the Markov chain to converge. Moreover, the detailed balance\ncondition is highly restrictive: energy based models corresponding to neural\nnetworks must have symmetric weights, unlike biological neural circuits. In\ncontrast, we develop a method for directly learning arbitrarily parameterized\ntransition operators capable of expressing non-equilibrium stationary\ndistributions that violate detailed balance, thereby enabling us to learn more\nbiologically plausible asymmetric neural networks and more general non-energy\nbased dynamical systems. The proposed training objective, which we derive via\nprincipled variational methods, encourages the transition operator to \"walk\nback\" in multi-step trajectories that start at data-points, as quickly as\npossible back to the original data points. We present a series of experimental\nresults illustrating the soundness of the proposed approach, Variational\nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating\nsuperior samples compared to earlier attempts to learn a transition operator.\nWe also show that although each rapid training trajectory is limited to a\nfinite but variable number of steps, our transition operator continues to\ngenerate good samples well past the length of such trajectories, thereby\ndemonstrating the match of its non-equilibrium stationary distribution to the\ndata distribution. Source Code: http://github.com/anirudh9119/walkback_nips17\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 04:45:13 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Goyal", "Anirudh", ""], ["Ke", "Nan Rosemary", ""], ["Ganguli", "Surya", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.02301", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V.\n  Le, Jon Kleinberg", "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?", "comments": "Accepted to ICML 2018, code opensourced at:\n  https://github.com/rubai5/ESS_Game", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has achieved many recent successes, but our\nunderstanding of its strengths and limitations is hampered by the lack of rich\nenvironments in which we can fully characterize optimal behavior, and\ncorrespondingly diagnose individual actions against such a characterization.\nHere we consider a family of combinatorial games, arising from work of Erdos,\nSelfridge, and Spencer, and we propose their use as environments for evaluating\nand comparing different approaches to reinforcement learning. These games have\na number of appealing features: they are challenging for current learning\napproaches, but they form (i) a low-dimensional, simply parametrized\nenvironment where (ii) there is a linear closed form solution for optimal\nbehavior from any state, and (iii) the difficulty of the game can be tuned by\nchanging environment parameters in an interpretable way. We use these\nErdos-Selfridge-Spencer games not only to compare different algorithms, but\ntest for generalization, make comparisons to supervised learning, analyse\nmultiagent play, and even develop a self play algorithm. Code can be found at:\nhttps://github.com/rubai5/ESS_Game\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:16:56 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 00:51:17 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 21:05:00 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 03:24:25 GMT"}, {"version": "v5", "created": "Fri, 29 Jun 2018 00:18:48 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Raghu", "Maithra", ""], ["Irpan", "Alex", ""], ["Andreas", "Jacob", ""], ["Kleinberg", "Robert", ""], ["Le", "Quoc V.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1711.02326", "submitter": "Nan Rosemary Ke", "authors": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas,\n  Laurent Charlin, Chris Pal, Yoshua Bengio", "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major drawback of backpropagation through time (BPTT) is the difficulty of\nlearning long-term dependencies, coming from having to propagate credit\ninformation backwards through every single step of the forward computation.\nThis makes BPTT both computationally impractical and biologically implausible.\nFor this reason, full backpropagation through time is rarely used on long\nsequences, and truncated backpropagation through time is used as a heuristic.\nHowever, this usually leads to biased estimates of the gradient in which longer\nterm dependencies are ignored. Addressing this issue, we propose an alternative\nalgorithm, Sparse Attentive Backtracking, which might also be related to\nprinciples used by brains to learn long-term dependencies. Sparse Attentive\nBacktracking learns an attention mechanism over the hidden states of the past\nand selectively backpropagates through paths with high attention weights. This\nallows the model to learn long term dependencies while only backtracking for a\nsmall number of time steps, not just from the recent past but also from\nattended relevant past states.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 07:52:12 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Bilaniuk", "Olexa", ""], ["Binas", "Jonathan", ""], ["Charlin", "Laurent", ""], ["Pal", "Chris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.02395", "submitter": "Xiangyuan Jiang", "authors": "Xiangyuan Jiang, Shuai Li", "title": "Beetle Antennae Search without Parameter Tuning (BAS-WPT) for\n  Multi-objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beetle antennae search (BAS) is an efficient meta-heuristic algorithm\ninspired by foraging behaviors of beetles. This algorithm includes several\nparameters for tuning and the existing results are limited to solve single\nobjective optimization. This work pushes forward the research on BAS by\nproviding one variant that releases the tuning parameters and is able to handle\nmulti-objective optimization. This new approach applies normalization to\nsimplify the original algorithm and uses a penalty function to exploit\ninfeasible solutions with low constraint violation to solve the constraint\noptimization problem. Extensive experimental studies are carried out and the\nresults reveal efficacy of the proposed approach to constraint handling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 11:07:05 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Jiang", "Xiangyuan", ""], ["Li", "Shuai", ""]]}, {"id": "1711.02448", "submitter": "Rui Ponte Costa", "authors": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de\n  Freitas and Tim P. Vogels", "title": "Cortical microcircuits as gated-recurrent neural networks", "comments": "To appear in Advances in Neural Information Processing Systems 30\n  (NIPS 2017). 13 pages, 2 figures (and 1 supp. figure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical circuits exhibit intricate recurrent architectures that are\nremarkably similar across different brain areas. Such stereotyped structure\nsuggests the existence of common computational principles. However, such\nprinciples have remained largely elusive. Inspired by gated-memory networks,\nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural\nnetwork in which information is gated through inhibitory cells that are\nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known\ncanonical excitatory-inhibitory cortical microcircuits. Our empirical\nevaluation across sequential image classification and language modelling tasks\nshows that subLSTM units can achieve similar performance to LSTM units. These\nresults suggest that cortical circuits can be optimised to solve complex\ncontextual problems and proposes a novel view on their computational function.\nOverall our work provides a step towards unifying recurrent networks as used in\nmachine learning with their biological counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 13:03:35 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 17:29:28 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Costa", "Rui Ponte", ""], ["Assael", "Yannis M.", ""], ["Shillingford", "Brendan", ""], ["de Freitas", "Nando", ""], ["Vogels", "Tim P.", ""]]}, {"id": "1711.02799", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, Bernhard\n  Sch\\\"olkopf", "title": "Fidelity-Weighted Learning", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires many training samples, but in practice\ntraining labels are expensive to obtain and may be of varying quality, as some\nmay be from trusted expert labelers while others might be from heuristics or\nother sources of weak supervision such as crowd-sourcing. This creates a\nfundamental quality versus-quantity trade-off in the learning process. Do we\nlearn from the small amount of high-quality data or the potentially large\namount of weakly-labeled data? We argue that if the learner could somehow know\nand take the label-quality into account when learning the data representation,\nwe could get the best of both worlds. To this end, we propose\n\"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher approach\nfor training deep neural networks using weakly-labeled data. FWL modulates the\nparameter updates to a student network (trained on the task we care about) on a\nper-sample basis according to the posterior confidence of its label-quality\nestimated by a teacher (who has access to the high-quality labels). Both\nstudent and teacher are learned from the data. We evaluate FWL on two tasks in\ninformation retrieval and natural language processing where we outperform\nstate-of-the-art alternative semi-supervised methods, indicating that our\napproach makes better use of strong and weak labels, and leads to better\ntask-dependent data representations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 02:05:11 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 14:27:21 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Mehrjou", "Arash", ""], ["Gouws", "Stephan", ""], ["Kamps", "Jaap", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1711.02833", "submitter": "Yuxiu Hua", "authors": "Yuxiu Hua, Zhifeng Zhao, Rongpeng Li, Xianfu Chen, Zhiming Liu,\n  Honggang Zhang", "title": "Traffic Prediction Based on Random Connectivity in Deep Learning with\n  Long Short-Term Memory", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic prediction plays an important role in evaluating the performance of\ntelecommunication networks and attracts intense research interests. A\nsignificant number of algorithms and models have been put forward to analyse\ntraffic data and make prediction. In the recent big data era, deep learning has\nbeen exploited to mine the profound information hidden in the data. In\nparticular, Long Short-Term Memory (LSTM), one kind of Recurrent Neural Network\n(RNN) schemes, has attracted a lot of attentions due to its capability of\nprocessing the long-range dependency embedded in the sequential traffic data.\nHowever, LSTM has considerable computational cost, which can not be tolerated\nin tasks with stringent latency requirement. In this paper, we propose a deep\nlearning model based on LSTM, called Random Connectivity LSTM (RCLSTM).\nCompared to the conventional LSTM, RCLSTM makes a notable breakthrough in the\nformation of neural network, which is that the neurons are connected in a\nstochastic manner rather than full connected. So, the RCLSTM, with certain\nintrinsic sparsity, have many neural connections absent (distinguished from the\nfull connectivity) and which leads to the reduction of the parameters to be\ntrained and the computational cost. We apply the RCLSTM to predict traffic and\nvalidate that the RCLSTM with even 35% neural connectivity still shows a\nsatisfactory performance. When we gradually add training samples, the\nperformance of RCLSTM becomes increasingly closer to the baseline LSTM.\nMoreover, for the input traffic sequences of enough length, the RCLSTM exhibits\neven superior prediction accuracy than the baseline LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:11:37 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 04:47:10 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Hua", "Yuxiu", ""], ["Zhao", "Zhifeng", ""], ["Li", "Rongpeng", ""], ["Chen", "Xianfu", ""], ["Liu", "Zhiming", ""], ["Zhang", "Honggang", ""]]}, {"id": "1711.03073", "submitter": "Anirbit Mukherjee", "authors": "Anirbit Mukherjee, Amitabh Basu", "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU\n  gates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the resurgence of neural networks in being able to solve complex\nlearning tasks we undertake a study of high depth networks using ReLU gates\nwhich implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the\nrole of depth in such neural networks by showing size lowerbounds against such\nnetwork architectures in parameter regimes hitherto unexplored. In particular\nwe show the following two main results about neural nets computing Boolean\nfunctions of input dimension $n$,\n  1. We use the method of random restrictions to show almost linear,\n$\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight\nunrestricted LTF-of-ReLU circuits to match the Andreev function on at least\n$\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon >\n\\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1\n2)$\n  2. We use the method of sign-rank to show exponential in dimension lower\nbounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$\nwith $\\xi < \\frac{1}{8}$ with some restrictions on the weights in the bottom\nmost layer. All other weights in these circuits are kept unrestricted. This in\nturns also implies the same lowerbounds for LTF circuits with the same\narchitecture and the same weight restrictions on their bottom most layer.\n  Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow\n\\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can\nnever represent no matter how large they are allowed to be.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 18:02:47 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 02:35:25 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Mukherjee", "Anirbit", ""], ["Basu", "Amitabh", ""]]}, {"id": "1711.03121", "submitter": "Daniel George", "authors": "Daniel George, E. A. Huerta", "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter\n  Estimation: Results with Advanced LIGO Data", "comments": "6 pages, 7 figures; First application of deep learning to real LIGO\n  events; Includes direct comparison against matched-filtering", "journal-ref": "Physics Letters B, 778 (2018) 64-70", "doi": "10.1016/j.physletb.2017.12.053", "report-no": null, "categories": "gr-qc astro-ph.HE astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Nobel-prize-winning detections of gravitational waves from merging\nblack holes and the subsequent detection of the collision of two neutron stars\nin coincidence with electromagnetic observations have inaugurated a new era of\nmultimessenger astrophysics. To enhance the scope of this emergent field of\nscience, we pioneered the use of deep learning with convolutional neural\nnetworks, that take time-series inputs, for rapid detection and\ncharacterization of gravitational wave signals. This approach, Deep Filtering,\nwas initially demonstrated using simulated LIGO noise. In this article, we\npresent the extension of Deep Filtering using real data from LIGO, for both\ndetection and parameter estimation of gravitational waves from binary black\nhole mergers using continuous data streams from multiple LIGO detectors. We\ndemonstrate for the first time that machine learning can detect and estimate\nthe true parameters of real events observed by LIGO. Our results show that Deep\nFiltering achieves similar sensitivities and lower errors compared to\nmatched-filtering while being far more computationally efficient and more\nresilient to glitches, allowing real-time processing of weak time-series\nsignals in non-stationary non-Gaussian noise with minimal resources, and also\nenables the detection of new classes of gravitational wave sources that may go\nunnoticed with existing detection algorithms. This unified framework for data\nanalysis is ideally suited to enable coincident detection campaigns of\ngravitational waves and their multimessenger counterparts in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:05:28 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["George", "Daniel", ""], ["Huerta", "E. A.", ""]]}, {"id": "1711.03129", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, William T Freeman,\n  Joshua B Tenenbaum", "title": "MarrNet: 3D Shape Reconstruction via 2.5D Sketches", "comments": "NIPS 2017. The first two authors contributed equally to this paper.\n  Project page: http://marrnet.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object reconstruction from a single image is a highly under-determined\nproblem, requiring strong prior knowledge of plausible 3D shapes. This\nintroduces challenges for learning-based approaches, as 3D object annotations\nare scarce in real images. Previous work chose to train on synthetic data with\nground truth 3D information, but suffered from domain adaptation when tested on\nreal data. In this work, we propose MarrNet, an end-to-end trainable model that\nsequentially estimates 2.5D sketches and 3D object shape. Our disentangled,\ntwo-step formulation has three advantages. First, compared to full 3D shape,\n2.5D sketches are much easier to be recovered from a 2D image; models that\nrecover 2.5D sketches are also more likely to transfer from synthetic to real\ndata. Second, for 3D reconstruction from 2.5D sketches, systems can learn\npurely from synthetic data. This is because we can easily render realistic 2.5D\nsketches without modeling object appearance variations in real images,\nincluding lighting, texture, etc. This further relieves the domain adaptation\nproblem. Third, we derive differentiable projective functions from 3D shape to\n2.5D sketches; the framework is therefore end-to-end trainable on real images,\nrequiring no human annotations. Our model achieves state-of-the-art performance\non 3D shape reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:29:01 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Wu", "Jiajun", ""], ["Wang", "Yifan", ""], ["Xue", "Tianfan", ""], ["Sun", "Xingyuan", ""], ["Freeman", "William T", ""], ["Tenenbaum", "Joshua B", ""]]}, {"id": "1711.03180", "submitter": "Sarah Hamilton", "authors": "Sarah Jane Hamilton and Andreas Hauptmann", "title": "Deep D-bar: Real time Electrical Impedance Tomography Imaging with Deep\n  Neural Networks", "comments": "11 pages, 13 figures", "journal-ref": "IEEE Transactions on Medical Imaging, 2018", "doi": "10.1109/TMI.2018.2828303", "report-no": null, "categories": "math.NA cs.NE math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical problem for Electrical Impedance Tomography (EIT) is a\nhighly nonlinear ill-posed inverse problem requiring carefully designed\nreconstruction procedures to ensure reliable image generation. D-bar methods\nare based on a rigorous mathematical analysis and provide robust direct\nreconstructions by using a low-pass filtering of the associated nonlinear\nFourier data. Similarly to low-pass filtering of linear Fourier data, only\nusing low frequencies in the image recovery process results in blurred images\nlacking sharp features such as clear organ boundaries. Convolutional Neural\nNetworks provide a powerful framework for post-processing such convolved direct\nreconstructions. In this study, we demonstrate that these CNN techniques lead\nto sharp and reliable reconstructions even for the highly nonlinear inverse\nproblem of EIT. The network is trained on data sets of simulated examples and\nthen applied to experimental data without the need to perform an additional\ntransfer training. Results for absolute EIT images are presented using\nexperimental EIT data from the ACT4 and KIT4 EIT systems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:46:40 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 00:18:17 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hamilton", "Sarah Jane", ""], ["Hauptmann", "Andreas", ""]]}, {"id": "1711.03357", "submitter": "Andrew Hallam", "authors": "Andrew Hallam, Edward Grant, Vid Stojevic, Simone Severini, Andrew G.\n  Green", "title": "Compact Neural Networks based on the Multiscale Entanglement\n  Renormalization Ansatz", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates a method for tensorizing neural networks based upon\nan efficient way of approximating scale invariant quantum states, the\nMulti-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a\nreplacement for the fully connected layers in a convolutional neural network\nand test this implementation on the CIFAR-10 and CIFAR-100 datasets. The\nproposed method outperforms factorization using tensor trains, providing\ngreater compression for the same level of accuracy and greater accuracy for the\nsame level of compression. We demonstrate MERA layers with 14000 times fewer\nparameters and a reduction in accuracy of less than 1% compared to the\nequivalent fully connected layers, scaling like O(N).\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:55:59 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 17:25:21 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 23:55:50 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Hallam", "Andrew", ""], ["Grant", "Edward", ""], ["Stojevic", "Vid", ""], ["Severini", "Simone", ""], ["Green", "Andrew G.", ""]]}, {"id": "1711.03417", "submitter": "Marcos Cardinot", "authors": "Marcos Cardinot, Josephine Griffith, Colm O'Riordan", "title": "A Further Analysis of The Role of Heterogeneity in Coevolutionary\n  Spatial Games", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 493,\n  1 March 2018, Pages 116-124, ISSN 0378-4371", "doi": "10.1016/j.physa.2017.10.035", "report-no": null, "categories": "physics.soc-ph cs.GT cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneity has been studied as one of the most common explanations of the\npuzzle of cooperation in social dilemmas. A large number of papers have been\npublished discussing the effects of increasing heterogeneity in structured\npopulations of agents, where it has been established that heterogeneity may\nfavour cooperative behaviour if it supports agents to locally coordinate their\nstrategies. In this paper, assuming an existing model of a heterogeneous\nweighted network, we aim to further this analysis by exploring the relationship\n(if any) between heterogeneity and cooperation. We adopt a weighted network\nwhich is fully populated by agents playing both the Prisoner's Dilemma or the\nOptional Prisoner's Dilemma games with coevolutionary rules, i.e., not only the\nstrategies but also the link weights evolve over time. Surprisingly, results\nshow that the heterogeneity of link weights (states) on their own does not\nalways promote cooperation; rather cooperation is actually favoured by the\nincrease in the number of overlapping states and not by the heterogeneity\nitself. We believe that these results can guide further research towards a more\naccurate analysis of the role of heterogeneity in social dilemmas.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:13:53 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Cardinot", "Marcos", ""], ["Griffith", "Josephine", ""], ["O'Riordan", "Colm", ""]]}, {"id": "1711.03467", "submitter": "Mathias Lechner", "authors": "Mathias Lechner, Radu Grosu, Ramin M. Hasani", "title": "Worm-level Control through Search-based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through natural evolution, nervous systems of organisms formed near-optimal\nstructures to express behavior. Here, we propose an effective way to create\ncontrol agents, by \\textit{re-purposing} the function of biological neural\ncircuit models, to govern similar real world applications. We model the\ntap-withdrawal (TW) neural circuit of the nematode, \\textit{C. elegans}, a\ncircuit responsible for the worm's reflexive response to external mechanical\ntouch stimulations, and learn its synaptic and neural parameters as a policy\nfor controlling the inverted pendulum problem. For reconfiguration of the\npurpose of the TW neural circuit, we manipulate a search-based reinforcement\nlearning. We show that our neural policy performs as good as existing\ntraditional control theory and machine learning approaches. A video\ndemonstration of the performance of our method can be accessed at\n\\url{https://youtu.be/o-Ia5IVyff8}.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 16:43:59 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Lechner", "Mathias", ""], ["Grosu", "Radu", ""], ["Hasani", "Ramin M.", ""]]}, {"id": "1711.03577", "submitter": "Chuyu Xiong", "authors": "Chuyu Xiong", "title": "What Really is Deep Learning Doing?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved a great success in many areas, from computer\nvision to natural language processing, to game playing, and much more. Yet,\nwhat deep learning is really doing is still an open question. There are a lot\nof works in this direction. For example, [5] tried to explain deep learning by\ngroup renormalization, and [6] tried to explain deep learning from the view of\nfunctional approximation. In order to address this very crucial question, here\nwe see deep learning from perspective of mechanical learning and learning\nmachine (see [1], [2]). From this particular angle, we can see deep learning\nmuch better and answer with confidence: What deep learning is really doing? why\nit works well, how it works, and how much data is necessary for learning. We\nalso will discuss advantages and disadvantages of deep learning at the end of\nthis work.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 23:00:13 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Xiong", "Chuyu", ""]]}, {"id": "1711.03637", "submitter": "Shruti Kulkarni", "authors": "Shruti R. Kulkarni, John M. Alexiades, Bipin Rajendran", "title": "Learning and Real-time Classification of Hand-written Digits With\n  Spiking Neural Networks", "comments": "4 pages, 4 figures, 1 table, accepted at ICECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel spiking neural network (SNN) for automated, real-time\nhandwritten digit classification and its implementation on a GP-GPU platform.\nInformation processing within the network, from feature extraction to\nclassification is implemented by mimicking the basic aspects of neuronal spike\ninitiation and propagation in the brain. The feature extraction layer of the\nSNN uses fixed synaptic weight maps to extract the key features of the image\nand the classifier layer uses the recently developed NormAD approximate\ngradient descent based supervised learning algorithm for spiking neural\nnetworks to adjust the synaptic weights. On the standard MNIST database images\nof handwritten digits, our network achieves an accuracy of 99.80% on the\ntraining set and 98.06% on the test set, with nearly 7x fewer parameters\ncompared to the state-of-the-art spiking networks. We further use this network\nin a GPU based user-interface system demonstrating real-time SNN simulation to\ninfer digits written by different users. On a test set of 500 such images, this\nreal-time platform achieves an accuracy exceeding 97% while making a prediction\nwithin an SNN emulation time of less than 100ms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:01:42 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Kulkarni", "Shruti R.", ""], ["Alexiades", "John M.", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1711.03640", "submitter": "Anakha Vasanthakumari Babu", "authors": "Anakha V Babu, Bipin Rajendran", "title": "Stochastic Deep Learning in Memristive Networks", "comments": "4 pages, 5 figures, accepted at ICECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of stochastically trained deep neural networks\n(DNNs) whose synaptic weights are implemented using emerging memristive devices\nthat exhibit limited dynamic range, resolution, and variability in their\nprogramming characteristics. We show that a key device parameter to optimize\nthe learning efficiency of DNNs is the variability in its programming\ncharacteristics. DNNs with such memristive synapses, even with dynamic range as\nlow as $15$ and only $32$ discrete levels, when trained based on stochastic\nupdates suffer less than $3\\%$ loss in accuracy compared to floating point\nsoftware baseline. We also study the performance of stochastic memristive DNNs\nwhen used as inference engines with noise corrupted data and find that if the\ndevice variability can be minimized, the relative degradation in performance\nfor the Stochastic DNN is better than that of the software baseline. Hence, our\nstudy presents a new optimization corner for memristive devices for building\nlarge noise-immune deep learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:09:36 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Babu", "Anakha V", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1711.04214", "submitter": "Amirhossein Tavanaei", "authors": "Amirhossein Tavanaei and Anthony S. Maida", "title": "BP-STDP: Approximating Backpropagation using Spike Timing Dependent\n  Plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of training spiking neural networks (SNNs) is a necessary\nprecondition to understanding computations within the brain, a field still in\nits infancy. Previous work has shown that supervised learning in multi-layer\nSNNs enables bio-inspired networks to recognize patterns of stimuli through\nhierarchical feature acquisition. Although gradient descent has shown\nimpressive performance in multi-layer (and deep) SNNs, it is generally not\nconsidered biologically plausible and is also computationally expensive. This\npaper proposes a novel supervised learning approach based on an event-based\nspike-timing-dependent plasticity (STDP) rule embedded in a network of\nintegrate-and-fire (IF) neurons. The proposed temporally local learning rule\nfollows the backpropagation weight change updates applied at each time step.\nThis approach enjoys benefits of both accurate gradient descent and temporally\nlocal, efficient STDP. Thus, this method is able to address some open questions\nregarding accurate and efficient computations that occur in the brain. The\nexperimental results on the XOR problem, the Iris data, and the MNIST dataset\ndemonstrate that the proposed SNN performs as successfully as the traditional\nNNs. Our approach also compares favorably with the state-of-the-art multi-layer\nSNNs.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 00:47:16 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 16:30:59 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Tavanaei", "Amirhossein", ""], ["Maida", "Anthony S.", ""]]}, {"id": "1711.04340", "submitter": "Antreas Antoniou Mr", "authors": "Antreas Antoniou, Amos Storkey and Harrison Edwards", "title": "Data Augmentation Generative Adversarial Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 19:17:57 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 16:46:40 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 23:26:15 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Antoniou", "Antreas", ""], ["Storkey", "Amos", ""], ["Edwards", "Harrison", ""]]}, {"id": "1711.04518", "submitter": "Marius St\\\"ark", "authors": "Marius St\\\"ark, Damian Backes, Christian Kehl", "title": "A Supervised Learning Concept for Reducing User Interaction in Passenger\n  Cars", "comments": "4 pages, 9 figures, concept only", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article an automation system for human-machine-interfaces (HMI) for\nsetpoint adjustment using supervised learning is presented. We use HMIs of\nmulti-modal thermal conditioning systems in passenger cars as example for a\ncomplex setpoint selection system. The goal is the reduction of interaction\ncomplexity up to full automation. The approach is not limited to climate\ncontrol applications but can be extended to other setpoint-based HMIs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:58:58 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["St\u00e4rk", "Marius", ""], ["Backes", "Damian", ""], ["Kehl", "Christian", ""]]}, {"id": "1711.04574", "submitter": "Edward Grefenstette", "authors": "Richard Evans and Edward Grefenstette", "title": "Learning Explanatory Rules from Noisy Data", "comments": "64 pages, to appear in Journal of Artificial Intelligence Research\n  (Special Track on Deep Learning, Knowledge Representation, and Reasoning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks are powerful function approximators capable of\nmodelling solutions to a wide variety of problems, both supervised and\nunsupervised. As their size and expressivity increases, so too does the\nvariance of the model, yielding a nearly ubiquitous overfitting problem.\nAlthough mitigated by a variety of model regularisation methods, the common\ncure is to seek large amounts of training data---which is not necessarily\neasily obtained---that sufficiently approximates the data distribution of the\ndomain we wish to test on. In contrast, logic programming methods such as\nInductive Logic Programming offer an extremely data-efficient process by which\nmodels can be trained to reason on symbolic domains. However, these methods are\nunable to deal with the variety of domains neural networks can be applied to:\nthey are not robust to noise in or mislabelling of inputs, and perhaps more\nimportantly, cannot be applied to non-symbolic domains where the data is\nambiguous, such as operating on raw pixels. In this paper, we propose a\nDifferentiable Inductive Logic framework, which can not only solve tasks which\ntraditional ILP systems are suited for, but shows a robustness to noise and\nerror in the training data which ILP cannot cope with. Furthermore, as it is\ntrained by backpropagation against a likelihood objective, it can be hybridised\nby connecting it with neural networks over ambiguous data in order to be\napplied to domains which ILP cannot address, while providing data efficiency\nand generalisation beyond what neural networks on their own can achieve.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 13:30:39 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 11:22:23 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Evans", "Richard", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1711.04759", "submitter": "Adenilton Jos\\'e da Silva", "authors": "Adenilton Jos\\'e da Silva and Rodolfo Luan F. de Oliveira", "title": "Neural Networks Architecture Evaluation in a Quantum Computer", "comments": null, "journal-ref": null, "doi": "10.1109/BRACIS.2017.33", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a quantum algorithm to evaluate neural networks\narchitectures named Quantum Neural Network Architecture Evaluation (QNNAE). The\nproposed algorithm is based on a quantum associative memory and the learning\nalgorithm for artificial neural networks. Unlike conventional algorithms for\nevaluating neural network architectures, QNNAE does not depend on\ninitialization of weights. The proposed algorithm has a binary output and\nresults in 0 with probability proportional to the performance of the network.\nAnd its computational cost is equal to the computational cost to train a neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:50:04 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["da Silva", "Adenilton Jos\u00e9", ""], ["de Oliveira", "Rodolfo Luan F.", ""]]}, {"id": "1711.04837", "submitter": "Zachary Lipton", "authors": "John Alberg, Zachary C. Lipton", "title": "Improving Factor-Based Quantitative Investing by Forecasting Company\n  Fundamentals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a periodic basis, publicly traded companies are required to report\nfundamentals: financial data such as revenue, operating income, debt, among\nothers. These data points provide some insight into the financial health of a\ncompany. Academic research has identified some factors, i.e. computed features\nof the reported data, that are known through retrospective analysis to\noutperform the market average. Two popular factors are the book value\nnormalized by market capitalization (book-to-market) and the operating income\nnormalized by the enterprise value (EBIT/EV). In this paper: we first show\nthrough simulation that if we could (clairvoyantly) select stocks using factors\ncalculated on future fundamentals (via oracle), then our portfolios would far\noutperform a standard factor approach. Motivated by this analysis, we train\ndeep neural networks to forecast future fundamentals based on a trailing\n5-years window. Quantitative analysis demonstrates a significant improvement in\nMSE over a naive strategy. Moreover, in retrospective analysis using an\nindustry-grade stock portfolio simulator (backtester), we show an improvement\nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:30:02 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 03:19:32 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Alberg", "John", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1711.04848", "submitter": "Lei Lin", "authors": "Lei Lin, John Handley, Adel Sadek", "title": "Reliability and Sharpness in Border Crossing Traffic Interval Prediction", "comments": "Presented at 2017 TRB Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Short-term traffic volume prediction models have been extensively studied in\nthe past few decades. However, most of the previous studies only focus on\nsingle-value prediction. Considering the uncertain and chaotic nature of the\ntransportation system, an accurate and reliable prediction interval with upper\nand lower bounds may be better than a single point value for transportation\nmanagement. In this paper, we introduce a neural network model called Extreme\nLearning Machine (ELM) for interval prediction of short-term traffic volume and\nimprove it with the heuristic particle swarm optimization algorithm (PSO). The\nhybrid PSO-ELM model can generate the prediction intervals under different\nconfidence levels and guarantee the quality by minimizing a multi-objective\nfunction which considers two criteria reliability and interval sharpness. The\nPSO-ELM models are built based on an hourly traffic dataset and compared with\nARMA and Kalman Filter models. The results show that ARMA models are the worst\nfor all confidence levels, and the PSO-ELM models are comparable with Kalman\nFilter from the aspects of reliability and narrowness of the intervals,\nalthough the parameters of PSO-ELM are fixed once the training is done while\nKalman Filter is updated in an online approach. Additionally, only the PSO-ELMs\nare able to produce intervals with coverage probabilities higher than or equal\nto the confidence levels. For the points outside of the prediction levels given\nby PSO-ELMs, they lie very close to the bounds.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:59:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lin", "Lei", ""], ["Handley", "John", ""], ["Sadek", "Adel", ""]]}, {"id": "1711.04988", "submitter": "Morad Behandish", "authors": "Morad Behandish and Zheng Yi Wu", "title": "Concurrent Pump Scheduling and Storage Level Optimization Using\n  Meta-Models and Evolutionary Algorithms", "comments": "12th International Conference on Computing and Control for the Water\n  Industry (CCWI'2013)", "journal-ref": "Procedia Engineering, 70, pp.103-112, 2014", "doi": "10.1016/j.proeng.2014.02.013", "report-no": "BENT-TR-14-04", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the growing computational power offered by the commodity\nhardware, fast pump scheduling of complex water distribution systems is still a\nchallenge. In this paper, the Artificial Neural Network (ANN) meta-modeling\ntechnique has been employed with a Genetic Algorithm (GA) for simultaneously\noptimizing the pump operation and the tank levels at the ends of the cycle. The\ngeneralized GA+ANN algorithm has been tested on a real system in the UK.\nComparing to the existing operation, the daily cost is reduced by about 10-15%,\nwhile the number of pump switches are kept below 4 switches-per-day. In\naddition, tank levels are optimized ensure a periodic behavior, which results\nin a predictable and stable performance over repeated cycles.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 08:02:55 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Behandish", "Morad", ""], ["Wu", "Zheng Yi", ""]]}, {"id": "1711.05101", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov, Frank Hutter", "title": "Decoupled Weight Decay Regularization", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L$_2$ regularization and weight decay regularization are equivalent for\nstandard stochastic gradient descent (when rescaled by the learning rate), but\nas we demonstrate this is \\emph{not} the case for adaptive gradient algorithms,\nsuch as Adam. While common implementations of these algorithms employ L$_2$\nregularization (often calling it \"weight decay\" in what may be misleading due\nto the inequivalence we expose), we propose a simple modification to recover\nthe original formulation of weight decay regularization by \\emph{decoupling}\nthe weight decay from the optimization steps taken w.r.t. the loss function. We\nprovide empirical evidence that our proposed modification (i) decouples the\noptimal choice of weight decay factor from the setting of the learning rate for\nboth standard SGD and Adam and (ii) substantially improves Adam's\ngeneralization performance, allowing it to compete with SGD with momentum on\nimage classification datasets (on which it was previously typically\noutperformed by the latter). Our proposed decoupled weight decay has already\nbeen adopted by many researchers, and the community has implemented it in\nTensorFlow and PyTorch; the complete source code for our experiments is\navailable at https://github.com/loshchil/AdamW-and-SGDW\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:24:06 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 14:03:35 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 21:01:49 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1711.05133", "submitter": "Daniel Brunner", "authors": "Julian Bueno, Sheler Maktoobi, Luc Froehly, Ingo Fischer, Maxime\n  Jacquot, Laurent Larger, Daniel Brunner", "title": "Reinforcement Learning in a large scale photonic Recurrent Neural\n  Network", "comments": null, "journal-ref": "Optica Vol. 5, Issue 6, pp. 756-760 (2018)", "doi": "10.1364/OPTICA.5.000756", "report-no": null, "categories": "cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photonic Neural Network implementations have been gaining considerable\nattention as a potentially disruptive future technology. Demonstrating learning\nin large scale neural networks is essential to establish photonic machine\nlearning substrates as viable information processing systems. Realizing\nphotonic Neural Networks with numerous nonlinear nodes in a fully parallel and\nefficient learning hardware was lacking so far. We demonstrate a network of up\nto 2500 diffractively coupled photonic nodes, forming a large scale Recurrent\nNeural Network. Using a Digital Micro Mirror Device, we realize reinforcement\nlearning. Our scheme is fully parallel, and the passive weights maximize energy\nefficiency and bandwidth. The computational output efficiently converges and we\nachieve very good performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:54:23 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 07:47:34 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bueno", "Julian", ""], ["Maktoobi", "Sheler", ""], ["Froehly", "Luc", ""], ["Fischer", "Ingo", ""], ["Jacquot", "Maxime", ""], ["Larger", "Laurent", ""], ["Brunner", "Daniel", ""]]}, {"id": "1711.05136", "submitter": "Guillaume Bellec", "authors": "Guillaume Bellec, David Kappel, Wolfgang Maass and Robert Legenstein", "title": "Deep Rewiring: Training very sparse deep networks", "comments": "Accepted for publication at ICLR 2018. 10 pages (12 with references,\n  24 with appendix), 4 Figures in the main text. Reviews are available at:\n  https://openreview.net/forum?id=BJ_wN01C- . This recent version contains\n  minor corrections in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:02:47 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 18:33:53 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 15:57:44 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 11:01:41 GMT"}, {"version": "v5", "created": "Tue, 7 Aug 2018 18:12:10 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Bellec", "Guillaume", ""], ["Kappel", "David", ""], ["Maass", "Wolfgang", ""], ["Legenstein", "Robert", ""]]}, {"id": "1711.05443", "submitter": "Zhiyuan Tang", "authors": "Miao Zhang, Xiaofei Kang, Yanqing Wang, Lantian Li, Zhiyuan Tang,\n  Haisheng Dai, Dong Wang", "title": "Human and Machine Speaker Recognition Based on Short Trivial Events", "comments": "ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trivial events are ubiquitous in human to human conversations, e.g., cough,\nlaugh and sniff. Compared to regular speech, these trivial events are usually\nshort and unclear, thus generally regarded as not speaker discriminative and so\nare largely ignored by present speaker recognition research. However, these\ntrivial events are highly valuable in some particular circumstances such as\nforensic examination, as they are less subjected to intentional change, so can\nbe used to discover the genuine speaker from disguised speech. In this paper,\nwe collect a trivial event speech database that involves 75 speakers and 6\ntypes of events, and report preliminary speaker recognition results on this\ndatabase, by both human listeners and machines. Particularly, the deep feature\nlearning technique recently proposed by our group is utilized to analyze and\nrecognize the trivial events, which leads to acceptable equal error rates\n(EERs) despite the extremely short durations (0.2-0.5 seconds) of these events.\nComparing different types of events, 'hmm' seems more speaker discriminative.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 08:21:20 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 10:27:00 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 04:13:27 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Zhang", "Miao", ""], ["Kang", "Xiaofei", ""], ["Wang", "Yanqing", ""], ["Li", "Lantian", ""], ["Tang", "Zhiyuan", ""], ["Dai", "Haisheng", ""], ["Wang", "Dong", ""]]}, {"id": "1711.05734", "submitter": "Francesco Conti", "authors": "Francesco Conti, Lukas Cavigelli, Gianna Paulin, Igor Susmelj, Luca\n  Benini", "title": "Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW\n  Accelerator for Near-Sensor Recurrent Neural Network Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are state-of-the-art in voice\nawareness/understanding and speech recognition. On-device computation of RNNs\non low-power mobile and wearable devices would be key to applications such as\nzero-latency voice-based human-machine interfaces. Here we present Chipmunk, a\nsmall (<1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC\n65 nm technology capable to operate at a measured peak efficiency up to 3.08\nGop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring\nin huge memory transfer overhead, multiple Chipmunk engines can cooperate to\nform a single systolic array. In this way, the Chipmunk architecture in a 75\ntiles configuration can achieve real-time phoneme extraction on a demanding RNN\ntopology proposed by Graves et al., consuming less than 13 mW of average power.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 10:15:44 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 21:43:55 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Conti", "Francesco", ""], ["Cavigelli", "Lukas", ""], ["Paulin", "Gianna", ""], ["Susmelj", "Igor", ""], ["Benini", "Luca", ""]]}, {"id": "1711.05747", "submitter": "Chris Donahue", "authors": "Chris Donahue, Bo Li, Rohit Prabhavalkar", "title": "Exploring Speech Enhancement with Generative Adversarial Networks for\n  Robust Speech Recognition", "comments": "Published as a conference paper at ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effectiveness of generative adversarial networks (GANs)\nfor speech enhancement, in the context of improving noise robustness of\nautomatic speech recognition (ASR) systems. Prior work demonstrates that GANs\ncan effectively suppress additive noise in raw waveform speech signals,\nimproving perceptual quality metrics; however this technique was not justified\nin the context of ASR. In this work, we conduct a detailed study to measure the\neffectiveness of GANs in enhancing speech contaminated by both additive and\nreverberant noise. Motivated by recent advances in image processing, we propose\noperating GANs on log-Mel filterbank spectra instead of waveforms, which\nrequires less computation and is more robust to reverberant noise. While GAN\nenhancement improves the performance of a clean-trained ASR system on noisy\nspeech, it falls short of the performance achieved by conventional multi-style\ntraining (MTR). By appending the GAN-enhanced features to the noisy inputs and\nretraining, we achieve a 7% WER improvement relative to the MTR system.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:00:07 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 00:48:59 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Donahue", "Chris", ""], ["Li", "Bo", ""], ["Prabhavalkar", "Rohit", ""]]}, {"id": "1711.05772", "submitter": "Jesse Engel", "authors": "Jesse Engel, Matthew Hoffman, Adam Roberts", "title": "Latent Constraints: Learning to Generate Conditionally from\n  Unconditional Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative neural networks have proven effective at both conditional and\nunconditional modeling of complex data distributions. Conditional generation\nenables interactive control, but creating new controls often requires expensive\nretraining. In this paper, we develop a method to condition generation without\nretraining the model. By post-hoc learning latent constraints, value functions\nthat identify regions in latent space that generate outputs with desired\nattributes, we can conditionally sample from these regions with gradient-based\noptimization or amortized actor functions. Combining attribute constraints with\na universal \"realism\" constraint, which enforces similarity to the data\ndistribution, we generate realistic conditional images from an unconditional\nvariational autoencoder. Further, using gradient-based optimization, we\ndemonstrate identity-preserving transformations that make the minimal\nadjustment in latent space to modify the attributes of an image. Finally, with\ndiscrete sequences of musical notes, we demonstrate zero-shot conditional\ngeneration, learning latent constraints in the absence of labeled data or a\ndifferentiable reward function. Code with dedicated cloud instance has been\nmade publicly available (https://goo.gl/STGMGx).\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:45:10 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 23:50:53 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Engel", "Jesse", ""], ["Hoffman", "Matthew", ""], ["Roberts", "Adam", ""]]}, {"id": "1711.05795", "submitter": "Shikhar Murty", "authors": "Shikhar Murty, Patrick Verga, Luke Vilnis, Andrew McCallum", "title": "Finer Grained Entity Typing with TypeNet", "comments": "Accepted at 6th Workshop on Automated Knowledge Base Construction\n  (AKBC) at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the challenging problem of entity typing over an extremely fine\ngrained set of types, wherein a single mention or entity can have many\nsimultaneous and often hierarchically-structured types. Despite the importance\nof the problem, there is a relative lack of resources in the form of\nfine-grained, deep type hierarchies aligned to existing knowledge bases. In\nresponse, we introduce TypeNet, a dataset of entity types consisting of over\n1941 types organized in a hierarchy, obtained by manually annotating a mapping\nfrom 1081 Freebase types to WordNet. We also experiment with several models\ncomparable to state-of-the-art systems and explore techniques to incorporate a\nstructure loss on the hierarchy with the standard mention typing loss, as a\nfirst step towards future research on this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 20:37:44 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Murty", "Shikhar", ""], ["Verga", "Patrick", ""], ["Vilnis", "Luke", ""], ["McCallum", "Andrew", ""]]}, {"id": "1711.05852", "submitter": "Asit Mishra", "authors": "Asit Mishra, Debbie Marr", "title": "Apprentice: Using Knowledge Distillation Techniques To Improve\n  Low-Precision Network Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning networks have achieved state-of-the-art accuracies on computer\nvision workloads like image classification and object detection. The performant\nsystems, however, typically involve big models with numerous parameters. Once\ntrained, a challenging aspect for such top performing models is deployment on\nresource constrained inference systems - the models (often deep networks or\nwide networks or both) are compute and memory intensive. Low-precision numerics\nand model compression using knowledge distillation are popular techniques to\nlower both the compute requirements and memory footprint of these deployed\nmodels. In this paper, we study the combination of these two techniques and\nshow that the performance of low-precision networks can be significantly\nimproved by using knowledge distillation techniques. Our approach, Apprentice,\nachieves state-of-the-art accuracies using ternary precision and 4-bit\nprecision for variants of ResNet architecture on ImageNet dataset. We present\nthree schemes using which one can apply knowledge distillation techniques to\nvarious stages of the train-and-deploy pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 23:45:59 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Mishra", "Asit", ""], ["Marr", "Debbie", ""]]}, {"id": "1711.05860", "submitter": "Yufeng Hao", "authors": "Yufeng Hao", "title": "A General Neural Network Hardware Architecture on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field Programmable Gate Arrays (FPGAs) plays an increasingly important role\nin data sampling and processing industries due to its highly parallel\narchitecture, low power consumption, and flexibility in custom algorithms.\nEspecially, in the artificial intelligence field, for training and implement\nthe neural networks and machine learning algorithms, high energy efficiency\nhardware implement and massively parallel computing capacity are heavily\ndemanded. Therefore, many global companies have applied FPGAs into AI and\nMachine learning fields such as autonomous driving and Automatic Spoken\nLanguage Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3].\nConsidering the FPGAs great potential in these fields, we tend to implement a\ngeneral neural network hardware architecture on XILINX ZU9CG System On Chip\n(SOC) platform [4], which contains abundant hardware resource and powerful\nprocessing capacity. The general neural network architecture on the FPGA SOC\nplatform can perform forward and backward algorithms in deep neural networks\n(DNN) with high performance and easily be adjusted according to the type and\nscale of the neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:17:58 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Hao", "Yufeng", ""]]}, {"id": "1711.05993", "submitter": "Mikhail Goykhman", "authors": "Mikhail Goykhman", "title": "On evolutionary selection of blackjack strategies", "comments": "Code is available here:\n  https://github.com/Goykhman/Blackjack_evolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the approach of evolutionary programming to the problem of\noptimization of the blackjack basic strategy. We demonstrate that the\npopulation of initially random blackjack strategies evolves and saturates to a\nprofitable performance in about one hundred generations. The resulting strategy\nresembles the known blackjack basic strategies in the specifics of its\nprescriptions, and has a similar performance. We also study evolution of the\npopulation of strategies initialized to the Thorp's basic strategy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 09:13:16 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Goykhman", "Mikhail", ""]]}, {"id": "1711.06006", "submitter": "Paulo Rauber", "authors": "Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, Juergen Schmidhuber", "title": "Hindsight policy gradients", "comments": "Accepted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reinforcement learning agent that needs to pursue different goals across\nepisodes requires a goal-conditional policy. In addition to their potential to\ngeneralize desirable behavior to unseen goals, such policies may also enable\nhigher-level planning based on subgoals. In sparse-reward environments, the\ncapacity to exploit information about the degree to which an arbitrary goal has\nbeen achieved while another goal was intended appears crucial to enable sample\nefficient learning. However, reinforcement learning agents have only recently\nbeen endowed with such capacity for hindsight. In this paper, we demonstrate\nhow hindsight can be introduced to policy gradient methods, generalizing this\nidea to a broad class of successful algorithms. Our experiments on a diverse\nselection of sparse-reward environments show that hindsight leads to a\nremarkable increase in sample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 10:05:31 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 14:11:06 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 10:46:44 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Rauber", "Paulo", ""], ["Ummadisingu", "Avinash", ""], ["Mutz", "Filipe", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1711.06317", "submitter": "Ehsan Hemmati", "authors": "Mansour Sheikhan, Ehsan Hemmati, Reza Shahnazi", "title": "GA-PSO-Optimized Neural-Based Control Scheme for Adaptive Congestion\n  Control to Improve Performance in Multimedia Applications", "comments": "arXiv admin note: text overlap with arXiv:1711.06356", "journal-ref": "Majlesi Journal of Electrical Engineering, [S.l.], v. 6, n. 1,\n  jan. 2012", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active queue control aims to improve the overall communication network\nthroughput while providing lower delay and small packet loss rate. The basic\nidea is to actively trigger packet dropping (or marking provided by explicit\ncongestion notification (ECN)) before buffer overflow. In this paper, two\nartificial neural networks (ANN)-based control schemes are proposed for\nadaptive queue control in TCP communication networks. The structure of these\ncontrollers is optimized using genetic algorithm (GA) and the output weights of\nANNs are optimized using particle swarm optimization (PSO) algorithm. The\ncontrollers are radial bias function (RBF)-based, but to improve the robustness\nof RBF controller, an error-integral term is added to RBF equation in the\nsecond scheme. Experimental results show that GA- PSO-optimized improved RBF\n(I-RBF) model controls network congestion effectively in terms of link\nutilization with a low packet loss rate and outperform Drop Tail,\nproportional-integral (PI), random exponential marking (REM), and adaptive\nrandom early detection (ARED) controllers.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 20:52:37 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Sheikhan", "Mansour", ""], ["Hemmati", "Ehsan", ""], ["Shahnazi", "Reza", ""]]}, {"id": "1711.06356", "submitter": "Ehsan Hemmati", "authors": "Mansour Sheikhan, Reza Shahnazi, Ehasn Hemmati", "title": "Adaptive active queue management controller for TCP communication\n  networks using PSO-RBF models", "comments": "arXiv admin note: text overlap with arXiv:1711.06317", "journal-ref": "Neural Computing and Applications, Volume 22, Issue 5, Pages\n  933-94, 2012", "doi": "10.1007/s00521-011-0786-0", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing performance degradations in end-to-end congestion control has been\none of the most active research areas in the last decade. Active queue\nmanagement (AQM) aims to improve the overall network throughput, while\nproviding lower delay and reduce packet loss and improving network. The basic\nidea is to actively trigger packet dropping (or marking provided by explicit\ncongestion notification (ECN)) before buffer overflow. Radial bias function\n(RBF)-based AQM controller is proposed in this paper. RBF controller is\nsuitable as an AQM scheme to control congestion in TCP communication networks\nsince it is nonlinear. Particle swarm optimization (PSO) algorithm is also\nemployed to derive RBF parameters such that the integrated-absolute error (IAE)\nis minimized. Furthermore, in order to improve the robustness of RBF\ncontroller, an error-integral term is added to RBF equation. The results of the\ncomparison with Drop Tail, adaptive random early detection (ARED), random\nexponential marking (REM), and proportional-integral (PI) controllers are\npresented. Integral-RBF has better performance not only in comparison with RBF\nbut also with ARED, REM and PI controllers in the case of link utilization\nwhile packet loss rate is small.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:46:16 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Sheikhan", "Mansour", ""], ["Shahnazi", "Reza", ""], ["Hemmati", "Ehasn", ""]]}, {"id": "1711.06379", "submitter": "Terrell Mundhenk", "authors": "T. Nathan Mundhenk, Daniel Ho and Barry Y. Chen", "title": "Improvements to context based self-supervised learning", "comments": "Accepted paper at CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00973", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a set of methods to improve on the results of self-supervised\nlearning using context. We start with a baseline of patch based arrangement\ncontext learning and go from there. Our methods address some overt problems\nsuch as chromatic aberration as well as other potential problems such as\nspatial skew and mid-level feature neglect. We prevent problems with testing\ngeneralization on common self-supervised benchmark tests by using different\ndatasets during our development. The results of our methods combined yield top\nscores on all standard self-supervised benchmarks, including classification and\ndetection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \"linear\ntests\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over\nour baseline method of between 4.0 to 7.1 percentage points on transfer\nlearning classification tests. We also show results on different standard\nnetwork architectures to demonstrate generalization as well as portability. All\ndata, models and programs are available at:\nhttps://gdo-datasci.llnl.gov/selfsupervised/.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 02:22:21 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 23:00:35 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 22:14:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mundhenk", "T. Nathan", ""], ["Ho", "Daniel", ""], ["Chen", "Barry Y.", ""]]}, {"id": "1711.06509", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Simone Scardapane, Sigurd L{\\o}kse, Robert\n  Jenssen", "title": "Bidirectional deep-readout echo state networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep architecture for the classification of multivariate time\nseries. By means of a recurrent and untrained reservoir we generate a vectorial\nrepresentation that embeds temporal relationships in the data. To improve the\nmemorization capability, we implement a bidirectional reservoir, whose last\nstate captures also past dependencies in the input. We apply dimensionality\nreduction to the final reservoir states to obtain compressed fixed size\nrepresentations of the time series. These are subsequently fed into a deep\nfeedforward network trained to perform the final classification. We test our\narchitecture on benchmark datasets and on a real-world use-case of blood\nsamples classification. Results show that our method performs better than a\nstandard echo state network and, at the same time, achieves results comparable\nto a fully-trained recurrent network, but with a faster training.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 12:29:29 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 19:47:11 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 19:15:42 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Scardapane", "Simone", ""], ["L\u00f8kse", "Sigurd", ""], ["Jenssen", "Robert", ""]]}, {"id": "1711.06516", "submitter": "Filippo Maria Bianchi", "authors": "Andreas Storvik Strauman, Filippo Maria Bianchi, Karl {\\O}yvind\n  Mikalsen, Michael Kampffmeyer, Cristina Soguero-Ruiz, Robert Jenssen", "title": "Classification of postoperative surgical site infections from blood\n  measurements with missing data using recurrent neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical measurements that can be represented as time series constitute an\nimportant fraction of the electronic health records and are often both\nuncertain and incomplete. Recurrent neural networks are a special class of\nneural networks that are particularly suitable to process time series data but,\nin their original formulation, cannot explicitly deal with missing data. In\nthis paper, we explore imputation strategies for handling missing values in\nclassifiers based on recurrent neural network (RNN) and apply a recently\nproposed recurrent architecture, the Gated Recurrent Unit with Decay,\nspecifically designed to handle missing data. We focus on the problem of\ndetecting surgical site infection in patients by analyzing time series of their\nblood sample measurements and we compare the results obtained with different\nRNN-based classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 12:52:10 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Strauman", "Andreas Storvik", ""], ["Bianchi", "Filippo Maria", ""], ["Mikalsen", "Karl \u00d8yvind", ""], ["Kampffmeyer", "Michael", ""], ["Soguero-Ruiz", "Cristina", ""], ["Jenssen", "Robert", ""]]}, {"id": "1711.06605", "submitter": "Francesco Corucci", "authors": "Francesco Corucci, Nick Cheney, Francesco Giorgio-Serchi, Josh Bongard\n  and Cecilia Laschi", "title": "Evolving soft locomotion in aquatic and terrestrial environments:\n  effects of material properties and environmental transitions", "comments": "37 pages, 22 figures, currently under review (journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing soft robots poses considerable challenges: automated design\napproaches may be particularly appealing in this field, as they promise to\noptimize complex multi-material machines with very little or no human\nintervention. Evolutionary soft robotics is concerned with the application of\noptimization algorithms inspired by natural evolution in order to let soft\nrobots (both morphologies and controllers) spontaneously evolve within\nphysically-realistic simulated environments, figuring out how to satisfy a set\nof objectives defined by human designers. In this paper a powerful evolutionary\nsystem is put in place in order to perform a broad investigation on the\nfree-form evolution of walking and swimming soft robots in different\nenvironments. Three sets of experiments are reported, tackling different\naspects of the evolution of soft locomotion. The first two sets explore the\neffects of different material properties on the evolution of terrestrial and\naquatic soft locomotion: particularly, we show how different materials lead to\nthe evolution of different morphologies, behaviors, and energy-performance\ntradeoffs. It is found that within our simplified physics world stiffer robots\nevolve more sophisticated and effective gaits and morphologies on land, while\nsofter ones tend to perform better in water. The third set of experiments\nstarts investigating the effect and potential benefits of major environmental\ntransitions (land - water) during evolution. Results provide interesting\nmorphological exaptation phenomena, and point out a potential asymmetry between\nland-water and water-land transitions: while the first type of transition\nappears to be detrimental, the second one seems to have some beneficial\neffects.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:01:27 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Corucci", "Francesco", ""], ["Cheney", "Nick", ""], ["Giorgio-Serchi", "Francesco", ""], ["Bongard", "Josh", ""], ["Laschi", "Cecilia", ""]]}, {"id": "1711.06673", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Neon2: Finding Local Minima via First-Order Oracles", "comments": "version 2 and 3 improve writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reduction for non-convex optimization that can (1) turn an\nstationary-point finding algorithm into an local-minimum finding one, and (2)\nreplace the Hessian-vector product computations with only gradient\ncomputations. It works both in the stochastic and the deterministic settings,\nwithout hurting the algorithm's performance.\n  As applications, our reduction turns Natasha2 into a first-order method\nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into\nalgorithms finding approximate local minima, outperforming some best known\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:59:01 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:34:18 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 07:58:25 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1711.06704", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin, Filip Radenovic, Jiri Matas", "title": "Repeatability Is Not Enough: Learning Affine Regions via\n  Discriminability", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for learning local affine-covariant regions is presented. We show\nthat maximizing geometric repeatability does not lead to local regions, a.k.a\nfeatures,that are reliably matched and this necessitates descriptor-based\nlearning. We explore factors that influence such learning and registration: the\nloss function, descriptor type, geometric parametrization and the trade-off\nbetween matchability and geometric accuracy and propose a novel hard\nnegative-constant loss function for learning of affine regions. The affine\nshape estimator -- AffNet -- trained with the hard negative-constant loss\noutperforms the state-of-the-art in bag-of-words image retrieval and wide\nbaseline stereo. The proposed training process does not require precisely\ngeometrically aligned patches.The source codes and trained weights are\navailable at https://github.com/ducha-aiki/affnet\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 19:37:28 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 09:51:54 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 15:16:50 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 10:01:42 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Radenovic", "Filip", ""], ["Matas", "Jiri", ""]]}, {"id": "1711.06756", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa, Vishwajith Ramesh, Gert Cauwenberghs", "title": "Deep supervised learning using local errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error backpropagation is a highly effective mechanism for learning\nhigh-quality hierarchical features in deep networks. Updating the features or\nweights in one layer, however, requires waiting for the propagation of error\nsignals from higher layers. Learning using delayed and non-local errors makes\nit hard to reconcile backpropagation with the learning mechanisms observed in\nbiological neural networks as it requires the neurons to maintain a memory of\nthe input long enough until the higher-layer errors arrive. In this paper, we\npropose an alternative learning mechanism where errors are generated locally in\neach layer using fixed, random auxiliary classifiers. Lower layers could thus\nbe trained independently of higher layers and training could either proceed\nlayer by layer, or simultaneously in all layers using local error information.\nWe address biological plausibility concerns such as weight symmetry\nrequirements and show that the proposed learning mechanism based on fixed,\nbroad, and random tuning of each neuron to the classification categories\noutperforms the biologically-motivated feedback alignment learning technique on\nthe MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard\nbackpropagation. Our approach highlights a potential biological mechanism for\nthe supervised, or task-dependent, learning of feature hierarchies. In\naddition, we show that it is well suited for learning deep networks in custom\nhardware where it can drastically reduce memory traffic and data communication\noverheads.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:48:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Mostafa", "Hesham", ""], ["Ramesh", "Vishwajith", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1711.06761", "submitter": "Matthew Riemer", "authors": "Matthew Riemer, Tim Klinger, Djallel Bouneffouf, Michele Franceschini", "title": "Scalable Recollections for Continual Lifelong Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the recent success of Deep Learning applied to a variety of single\ntasks, it is natural to consider more human-realistic settings. Perhaps the\nmost difficult of these settings is that of continual lifelong learning, where\nthe model must learn online over a continuous stream of non-stationary data. A\nsuccessful continual lifelong learning system must have three key capabilities:\nit must learn and adapt over time, it must not forget what it has learned, and\nit must be efficient in both training time and memory. Recent techniques have\nfocused their efforts primarily on the first two capabilities while questions\nof efficiency remain largely unexplored. In this paper, we consider the problem\nof efficient and effective storage of experiences over very large time-frames.\nIn particular we consider the case where typical experiences are O(n) bits and\nmemories are limited to O(k) bits for k << n. We present a novel scalable\narchitecture and training algorithm in this challenging domain and provide an\nextensive evaluation of its performance. Our results show that we can achieve\nconsiderable gains on top of state-of-the-art methods such as GEM.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:00:11 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 01:10:31 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 14:32:41 GMT"}, {"version": "v4", "created": "Thu, 20 Dec 2018 04:37:37 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Riemer", "Matthew", ""], ["Klinger", "Tim", ""], ["Bouneffouf", "Djallel", ""], ["Franceschini", "Michele", ""]]}, {"id": "1711.06763", "submitter": "Adam \\.Zychowski", "authors": "Adam \\.Zychowski, Abhishek Gupta, Jacek Ma\\'ndziuk, Yew Soon Ong", "title": "Addressing Expensive Multi-objective Games with Postponed Preference\n  Articulation via Memetic Co-evolution", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2018.05.012", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents algorithmic and empirical contributions demonstrating\nthat the convergence characteristics of a co-evolutionary approach to tackle\nMulti-Objective Games (MOGs) with postponed preference articulation can often\nbe hampered due to the possible emergence of the so-called Red Queen effect.\nAccordingly, it is hypothesized that the convergence characteristics can be\nsignificantly improved through the incorporation of memetics (local solution\nrefinements as a form of lifelong learning), as a promising means of mitigating\n(or at least suppressing) the Red Queen phenomenon by providing a guiding hand\nto the purely genetic mechanisms of co-evolution. Our practical motivation is\nto address MOGs of a time-sensitive nature that are characterized by\ncomputationally expensive evaluations, wherein there is a natural need to\nreduce the total number of true function evaluations consumed in achieving good\nquality solutions. To this end, we propose novel enhancements to\nco-evolutionary approaches for tackling MOGs, such that memetic local\nrefinements can be efficiently applied on evolved candidate strategies by\nsearching on computationally cheap surrogate payoff landscapes (that preserve\npostponed preference conditions). The efficacy of the proposal is demonstrated\non a suite of test MOGs that have been designed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:10:21 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["\u017bychowski", "Adam", ""], ["Gupta", "Abhishek", ""], ["Ma\u0144dziuk", "Jacek", ""], ["Ong", "Yew Soon", ""]]}, {"id": "1711.06764", "submitter": "Eli (Omid) David", "authors": "Sarit Chicotay, Eli David, Nathan S. Netanyahu", "title": "Image Registration of Very Large Images via Genetic Programming", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Workshop on Registration of Very Large Images, pp. 323-328, Columbus, OH,\n  June 2014", "doi": "10.1109/CVPRW.2014.56", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration (IR) is a fundamental task in image processing for\nmatching two or more images of the same scene taken at different times, from\ndifferent viewpoints and/or by different sensors. Due to the enormous diversity\nof IR applications, automatic IR remains a challenging problem to this day. A\nwide range of techniques has been developed for various data types and\nproblems. However, they might not handle effectively very large images, which\ngive rise usually to more complex transformations, e.g., deformations and\nvarious other distortions.\n  In this paper we present a genetic programming (GP)-based approach for IR,\nwhich could offer a significant advantage in dealing with very large images, as\nit does not make any prior assumptions about the transformation model. Thus, by\nincorporating certain generic building blocks into the proposed GP framework,\nwe hope to realize a large set of specialized transformations that should yield\naccurate registration of very large images.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:12:26 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 07:21:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chicotay", "Sarit", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06765", "submitter": "Eli (Omid) David", "authors": "Sarit Chicotay, Eli David, Nathan S. Netanyahu", "title": "A Two-Phase Genetic Algorithm for Image Registration", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pp.\n  189-190, Berlin, Germany, July 2017", "doi": "10.1145/3067695.3076017", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Registration (IR) is the process of aligning two (or more) images of\nthe same scene taken at different times, different viewpoints and/or by\ndifferent sensors. It is an important, crucial step in various image analysis\ntasks where multiple data sources are integrated/fused, in order to extract\nhigh-level information.\n  Registration methods usually assume a relevant transformation model for a\ngiven problem domain. The goal is to search for the \"optimal\" instance of the\ntransformation model assumed with respect to a similarity measure in question.\n  In this paper we present a novel genetic algorithm (GA)-based approach for\nIR. Since GA performs effective search in various optimization problems, it\ncould prove useful also for IR. Indeed, various GAs have been proposed for IR.\nHowever, most of them assume certain constraints, which simplify the\ntransformation model, restrict the search space or make additional\npreprocessing requirements. In contrast, we present a generalized GA-based\nsolution for an almost fully affine transformation model, which achieves\ncompetitive results without such limitations using a two-phase method and a\nmulti-objective optimization (MOO) approach.\n  We present good results for multiple dataset and demonstrate the robustness\nof our method in the presence of noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:15:19 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chicotay", "Sarit", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06766", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "Genetic Algorithm-Based Solver for Very Large Multiple Jigsaw Puzzles of\n  Unknown Dimensions and Piece Orientation", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1191-1198, Vancouver, Canada, July 2014", "doi": "10.1145/2576768.2598289", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first genetic algorithm (GA)-based solver for\njigsaw puzzles of unknown puzzle dimensions and unknown piece location and\norientation. Our solver uses a novel crossover technique, and sets a new\nstate-of-the-art in terms of the puzzle sizes solved and the accuracy obtained.\nThe results are significantly improved, even when compared to previous solvers\nassuming known puzzle dimensions. Moreover, the solver successfully contends\nwith a mixed bag of multiple puzzle pieces, assembling simultaneously all\npuzzles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:20 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06767", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "An Automatic Solver for Very Large Jigsaw Puzzles Using Genetic\n  Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06769", "journal-ref": "Genetic Programming and Evolvable Machines, Vol. 17, No. 3, pp.\n  291-313, September 2016", "doi": "10.1007/s10710-015-9258-0", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first effective genetic algorithm (GA)-based\njigsaw puzzle solver. We introduce a novel crossover procedure that merges two\n\"parent\" solutions to an improved \"child\" configuration by detecting,\nextracting, and combining correctly assembled puzzle segments. The solver\nproposed exhibits state-of-the-art performance, as far as handling previously\nattempted puzzles more accurately and efficiently, as well puzzle sizes that\nhave not been attempted before. The extended experimental results provided in\nthis paper include, among others, a thorough inspection of up to 30,745-piece\npuzzles (compared to previous attempts on 22,755-piece puzzles), using a\nconsiderably faster concurrent implementation of the algorithm. Furthermore, we\nexplore the impact of different phases of the novel crossover operator by\nexperimenting with several variants of the GA. Finally, we compare different\nfitness functions and their effect on the overall results of the GA-based\nsolver.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:23 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06768", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Generalized Genetic Algorithm-Based Solver for Very Large Jigsaw\n  Puzzles of Complex Types", "comments": null, "journal-ref": "AAAI Conference on Artificial Intelligence, pages 2839-2845,\n  Quebec City, Canada, July 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce new types of square-piece jigsaw puzzles, where in\naddition to the unknown location and orientation of each piece, a piece might\nalso need to be flipped. These puzzles, which are associated with a number of\nreal world problems, are considerably harder, from a computational standpoint.\nSpecifically, we present a novel generalized genetic algorithm (GA)-based\nsolver that can handle puzzle pieces of unknown location and orientation (Type\n2 puzzles) and (two-sided) puzzle pieces of unknown location, orientation, and\nface (Type 4 puzzles). To the best of our knowledge, our solver provides a new\nstate-of-the-art, solving previously attempted puzzles faster and far more\naccurately, handling puzzle sizes that have never been attempted before, and\nassembling the newly introduced two-sided puzzles automatically and\neffectively. This paper also presents, among other results, the most extensive\nset of experimental results, compiled as of yet, on Type 2 puzzles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:29 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06769", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06767", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  pages 1767-1774, Portland, OR, June 2013", "doi": "10.1109/CVPR.2013.231", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first effective automated, genetic algorithm\n(GA)-based jigsaw puzzle solver. We introduce a novel procedure of merging two\n\"parent\" solutions to an improved \"child\" solution by detecting, extracting,\nand combining correctly assembled puzzle segments. The solver proposed exhibits\nstate-of-the-art performance solving previously attempted puzzles faster and\nfar more accurately, and also puzzles of size never before attempted. Other\ncontributions include the creation of a benchmark of large images, previously\nunavailable. We share the data sets and all of our results for future testing\nand comparative evaluation of jigsaw puzzle solvers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:33 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06834", "submitter": "Radu Horaud P", "authors": "St\\'ephane Lathuili\\`ere, Benoit Mass\\'e, Pablo Mesejo and Radu Horaud", "title": "Neural Network Based Reinforcement Learning for Audio-Visual Gaze\n  Control in Human-Robot Interaction", "comments": "Paper submitted to Pattern Recognition Letters", "journal-ref": "Pattern Recognition Letters, vol. 118, 2019, 61-71", "doi": "10.1016/j.patrec.2018.05.023", "report-no": null, "categories": "cs.RO cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel neural network-based reinforcement learning\napproach for robot gaze control. Our approach enables a robot to learn and to\nadapt its gaze control strategy for human-robot interaction neither with the\nuse of external sensors nor with human supervision. The robot learns to focus\nits attention onto groups of people from its own audio-visual experiences,\nindependently of the number of people, of their positions and of their physical\nappearances. In particular, we use a recurrent neural network architecture in\ncombination with Q-learning to find an optimal action-selection policy; we\npre-train the network using a simulated environment that mimics realistic\nscenarios that involve speaking/silent participants, thus avoiding the need of\ntedious sessions of a robot interacting with people. Our experimental\nevaluation suggests that the proposed method is robust against parameter\nestimation, i.e. the parameter values yielded by the method do not have a\ndecisive impact on the performance. The best results are obtained when both\naudio and visual information is jointly used. Experiments with the Nao robot\nindicate that our framework is a step forward towards the autonomous learning\nof socially acceptable gaze behavior.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 09:19:47 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 15:07:23 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Lathuili\u00e8re", "St\u00e9phane", ""], ["Mass\u00e9", "Benoit", ""], ["Mesejo", "Pablo", ""], ["Horaud", "Radu", ""]]}, {"id": "1711.06839", "submitter": "Eli (Omid) David", "authors": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "title": "Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization", "comments": "Winner of Best Paper Award in GECCO 2008. arXiv admin note:\n  substantial text overlap with arXiv:1711.06840, arXiv:1711.06841", "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1469-1475, Atlanta, GA, July 2008", "doi": "10.1145/1389095.1389382", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate how genetic algorithms can be used to reverse\nengineer an evaluation function's parameters for computer chess. Our results\nshow that using an appropriate mentor, we can evolve a program that is on par\nwith top tournament-playing chess programs, outperforming a two-time World\nComputer Chess Champion. This performance gain is achieved by evolving a\nprogram with a smaller number of parameters in its evaluation function to mimic\nthe behavior of a superior mentor which uses a more extensive evaluation\nfunction. In principle, our mentor-assisted approach could be used in a wide\nrange of problems for which appropriate mentors are available.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:12:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["David", "Eli", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06840", "submitter": "Eli (Omid) David", "authors": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "title": "Simulating Human Grandmasters: Evolution and Coevolution of Evaluation\n  Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06839,\n  arXiv:1711.06841", "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1483-1489, Montreal, Canada, July 2009", "doi": "10.1145/1569901.1570100", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the use of genetic algorithms for evolving a\ngrandmaster-level evaluation function for a chess program. This is achieved by\ncombining supervised and unsupervised learning. In the supervised learning\nphase the organisms are evolved to mimic the behavior of human grandmasters,\nand in the unsupervised learning phase these evolved organisms are further\nimproved upon by means of coevolution.\n  While past attempts succeeded in creating a grandmaster-level program by\nmimicking the behavior of existing computer chess programs, this paper presents\nthe first successful attempt at evolving a state-of-the-art evaluation function\nby learning only from databases of games played by humans. Our results\ndemonstrate that the evolved program outperforms a two-time World Computer\nChess Champion.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:16:24 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["David", "Eli", ""], ["Herik", "H. Jaap van den", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06841", "submitter": "Eli (Omid) David", "authors": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "title": "Expert-Driven Genetic Algorithms for Simulating Evaluation Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06839,\n  arXiv:1711.06840", "journal-ref": "Genetic Programming and Evolvable Machines, Vol. 12, No. 1, pp.\n  5-22, March 2011", "doi": "10.1007/s10710-010-9103-4", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate how genetic algorithms can be used to reverse\nengineer an evaluation function's parameters for computer chess. Our results\nshow that using an appropriate expert (or mentor), we can evolve a program that\nis on par with top tournament-playing chess programs, outperforming a two-time\nWorld Computer Chess Champion. This performance gain is achieved by evolving a\nprogram that mimics the behavior of a superior expert. The resulting evaluation\nfunction of the evolved program consists of a much smaller number of parameters\nthan the expert's. The extended experimental results provided in this paper\ninclude a report of our successful participation in the 2008 World Computer\nChess Championship. In principle, our expert-driven approach could be used in a\nwide range of problems for which appropriate experts are available.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:22:49 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["David", "Eli", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.07128", "submitter": "Naveen Suda", "authors": "Yundong Zhang, Naveen Suda, Liangzhen Lai and Vikas Chandra", "title": "Hello Edge: Keyword Spotting on Microcontrollers", "comments": "Code available in github at\n  https://github.com/ARM-software/ML-KWS-for-MCU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword spotting (KWS) is a critical component for enabling speech based user\ninteractions on smart devices. It requires real-time response and high accuracy\nfor good user experience. Recently, neural networks have become an attractive\nchoice for KWS architecture because of their superior accuracy compared to\ntraditional speech processing algorithms. Due to its always-on nature, KWS\napplication has highly constrained power budget and typically runs on tiny\nmicrocontrollers with limited memory and compute capability. The design of\nneural network architecture for KWS must consider these constraints. In this\nwork, we perform neural network architecture evaluation and exploration for\nrunning KWS on resource-constrained microcontrollers. We train various neural\nnetwork architectures for keyword spotting published in literature to compare\ntheir accuracy and memory/compute requirements. We show that it is possible to\noptimize these neural network architectures to fit within the memory and\ncompute constraints of microcontrollers without sacrificing accuracy. We\nfurther explore the depthwise separable convolutional neural network (DS-CNN)\nand compare it against other neural network architectures. DS-CNN achieves an\naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number\nof parameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 03:19:03 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 23:54:52 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 19:24:55 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Zhang", "Yundong", ""], ["Suda", "Naveen", ""], ["Lai", "Liangzhen", ""], ["Chandra", "Vikas", ""]]}, {"id": "1711.07214", "submitter": "Chao Qian", "authors": "Chao Qian, Yang Yu, Ke Tang, Xin Yao, Zhi-Hua Zhou", "title": "Maximizing Non-monotone/Non-submodular Functions by Multi-objective\n  Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) are a kind of nature-inspired general-purpose\noptimization algorithm, and have shown empirically good performance in solving\nvarious real-word optimization problems. However, due to the highly randomized\nand complex behavior, the theoretical analysis of EAs is difficult and is an\nongoing challenge, which has attracted a lot of research attentions. During the\nlast two decades, promising results on the running time analysis (one essential\ntheoretical aspect) of EAs have been obtained, while most of them focused on\nisolated combinatorial optimization problems, which do not reflect the\ngeneral-purpose nature of EAs. To provide a general theoretical explanation of\nthe behavior of EAs, it is desirable to study the performance of EAs on a\ngeneral class of combinatorial optimization problems. To the best of our\nknowledge, this direction has been rarely touched and the only known result is\nthe provably good approximation guarantees of EAs for the problem class of\nmaximizing monotone submodular set functions with matroid constraints, which\nincludes many NP-hard combinatorial optimization problems. The aim of this work\nis to contribute to this line of research. As many combinatorial optimization\nproblems also involve non-monotone or non-submodular objective functions, we\nconsider these two general problem classes, maximizing non-monotone submodular\nfunctions without constraints and maximizing monotone non-submodular functions\nwith a size constraint. We prove that a simple multi-objective EA called GSEMO\ncan generally achieve good approximation guarantees in polynomial expected\nrunning time.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:21:19 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Qian", "Chao", ""], ["Yu", "Yang", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1711.07361", "submitter": "Kathleen Hamilton", "authors": "Kathleen E. Hamilton, Neena Imam, Travis S. Humble", "title": "Community detection with spiking neural networks for neuromorphic\n  hardware", "comments": "Conference paper presented at ORNL Neuromorphic Workshop 2017, 7\n  pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results related to the performance of an algorithm for community\ndetection which incorporates event-driven computation. We define a mapping\nwhich takes a graph G to a system of spiking neurons. Using a fully connected\nspiking neuron system, with both inhibitory and excitatory synaptic\nconnections, the firing patterns of neurons within the same community can be\ndistinguished from firing patterns of neurons in different communities. On a\nrandom graph with 128 vertices and known community structure we show that by\nusing binary decoding and a Hamming-distance based metric, individual\ncommunities can be identified from spike train similarities. Using bipolar\ndecoding and finite rate thresholding, we verify that inhibitory connections\nprevent the spread of spiking patterns.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:10:54 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Hamilton", "Kathleen E.", ""], ["Imam", "Neena", ""], ["Humble", "Travis S.", ""]]}, {"id": "1711.07459", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Francis Li, Brendan Chwyl, and Alexander Wong", "title": "SquishedNets: Squishing SqueezeNet further for edge device scenarios via\n  deep evolutionary synthesis", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have been shown in recent years to outperform\nother machine learning methods in a wide range of applications, one of the\nbiggest challenges with enabling deep neural networks for widespread deployment\non edge devices such as mobile and other consumer devices is high computational\nand memory requirements. Recently, there has been greater exploration into\nsmall deep neural network architectures that are more suitable for edge\ndevices, with one of the most popular architectures being SqueezeNet, with an\nincredibly small model size of 4.8MB. Taking further advantage of the notion\nthat many applications of machine learning on edge devices are often\ncharacterized by a low number of target classes, this study explores the\nutility of combining architectural modifications and an evolutionary synthesis\nstrategy for synthesizing even smaller deep neural architectures based on the\nmore recent SqueezeNet v1.1 macroarchitecture for applications with fewer\ntarget classes. In particular, architectural modifications are first made to\nSqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an\nevolutionary synthesis strategy is leveraged to synthesize more efficient deep\nneural networks based on this modified macroarchitecture. The resulting\nSquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller\nthan SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the\nSquishedNets are still able to achieve accuracies ranging from 81.2% to 77%,\nand able to process at speeds of 156 images/sec to as much as 256 images/sec on\na Nvidia Jetson TX1 embedded chip. These preliminary results show that a\ncombination of architectural modifications and an evolutionary synthesis\nstrategy can be a useful tool for producing very small deep neural network\narchitectures that are well-suited for edge device scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:50:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Chwyl", "Brendan", ""], ["Wong", "Alexander", ""]]}, {"id": "1711.07480", "submitter": "Franyell Silfa", "authors": "Franyell Silfa, Gem Dot, Jose-Maria Arnau, Antonio Gonzalez", "title": "E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks", "comments": null, "journal-ref": "PACT '18 Proceedings of the 27th International Conference on\n  Parallel Architectures and Compilation Techniques, Article No. 18, 2018", "doi": "10.1145/3243176.3243184", "report-no": "UPC-DAC-RR-2017-8", "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are a key technology for emerging\napplications such as automatic speech recognition, machine translation or image\ndescription. Long Short Term Memory (LSTM) networks are the most successful RNN\nimplementation, as they can learn long term dependencies to achieve high\naccuracy. Unfortunately, the recurrent nature of LSTM networks significantly\nconstrains the amount of parallelism and, hence, multicore CPUs and many-core\nGPUs exhibit poor efficiency for RNN inference. In this paper, we present\nE-PUR, an energy-efficient processing unit tailored to the requirements of LSTM\ncomputation. The main goal of E-PUR is to support large recurrent neural\nnetworks for low-power mobile devices. E-PUR provides an efficient hardware\nimplementation of LSTM networks that is flexible to support diverse\napplications. One of its main novelties is a technique that we call Maximizing\nWeight Locality (MWL), which improves the temporal locality of the memory\naccesses for fetching the synaptic weights, reducing the memory requirements by\na large extent. Our experimental results show that E-PUR achieves real-time\nperformance for different LSTM networks, while reducing energy consumption by\norders of magnitude with respect to general-purpose processors and GPUs, and it\nrequires a very small chip area. Compared to a modern mobile SoC, an NVIDIA\nTegra X1, E-PUR provides an average energy reduction of 92x.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:58:10 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Silfa", "Franyell", ""], ["Dot", "Gem", ""], ["Arnau", "Jose-Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "1711.07655", "submitter": "Eli (Omid) David", "authors": "Eli David, Iddo Greental", "title": "Genetic Algorithms for Evolving Deep Neural Networks", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1451-1452, Vancouver, Canada, July 2014", "doi": "10.1145/2598394.2602287", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods applying unsupervised learning to\ntrain deep layers of neural networks have achieved remarkable results in\nnumerous fields. In the past, many genetic algorithms based methods have been\nsuccessfully applied to training neural networks. In this paper, we extend\nprevious work and propose a GA-assisted method for deep learning. Our\nexperimental results indicate that this GA-assisted approach improves the\nperformance of a deep autoencoder, producing a sparser neural network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:23:32 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["David", "Eli", ""], ["Greental", "Iddo", ""]]}, {"id": "1711.07732", "submitter": "Zuozhu Liu", "authors": "Zuozhu Liu, Tony Q.S. Quek, Shaowei Lin", "title": "Variational Probability Flow for Biologically Plausible Training of Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for biologically plausible deep learning is driven, not just by the\ndesire to explain experimentally-observed properties of biological neural\nnetworks, but also by the hope of discovering more efficient methods for\ntraining artificial networks. In this paper, we propose a new algorithm named\nVariational Probably Flow (VPF), an extension of minimum probability flow for\ntraining binary Deep Boltzmann Machines (DBMs). We show that weight updates in\nVPF are local, depending only on the states and firing rates of the adjacent\nneurons. Unlike contrastive divergence, there is no need for Gibbs\nconfabulations; and unlike backpropagation, alternating feedforward and\nfeedback phases are not required. Moreover, the learning algorithm is effective\nfor training DBMs with intra-layer connections between the hidden nodes.\nExperiments with MNIST and Fashion MNIST demonstrate that VPF learns reasonable\nfeatures quickly, reconstructs corrupted images more accurately, and generates\nsamples with a high estimated log-likelihood. Lastly, we note that,\ninterestingly, if an asymmetric version of VPF exists, the weight updates\ndirectly explain experimental results in Spike-Timing-Dependent Plasticity\n(STDP).\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:49:05 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Liu", "Zuozhu", ""], ["Quek", "Tony Q. S.", ""], ["Lin", "Shaowei", ""]]}, {"id": "1711.07784", "submitter": "Davide Bacciu", "authors": "Davide Bacciu", "title": "Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces the Hidden Tree Markov Network (HTN), a\nneuro-probabilistic hybrid fusing the representation power of generative models\nfor trees with the incremental and discriminative learning capabilities of\nneural networks. We put forward a modular architecture in which multiple\ngenerative models of limited complexity are trained to learn structural feature\ndetectors whose outputs are then combined and integrated by neural layers at a\nlater stage. In this respect, the model is both deep, thanks to the unfolding\nof the generative models on the input structures, as well as wide, given the\npotentially large number of generative modules that can be trained in parallel.\nExperimental results show that the proposed approach can outperform\nstate-of-the-art syntactic kernels as well as generative kernels built on the\nsame probabilistic model as the HTN.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:50:34 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Bacciu", "Davide", ""]]}, {"id": "1711.07821", "submitter": "Wilfredo Ariel G\\'omez Bueno WAGomez", "authors": "Edson Florez, Nelson Diaz, Wilfredo Gomez, Lola Bautista, Dario\n  Delgado", "title": "Evaluation of bioinspired algorithms for the solution of the job\n  scheduling problem", "comments": "in Spanish", "journal-ref": "I+D Revista de Investigaciones 2017", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research we used bio-inspired metaheuristics, as artificial immune\nsystems and ant colony algorithms that are based on a number of characteristics\nand behaviors of living things that are interesting in the computer science\narea. This paper presents an evaluation of bio-inspired solutions to\ncombinatorial optimization problem, called the Job Shop Scheduling or planning\nwork, in a simple way the objective is to find a configuration or job stream\nthat has the least amount of time to be executed in machine settings. The\nperformance of the algorithms was characterized and evaluated for reference\ninstances of the job shop scheduling problem, comparing the quality of the\nsolutions obtained with respect to the best known solution of the most\neffective methods. The solutions were evaluated in two aspects, first in\nrelation of quality of solutions, taking as reference the makespan and secondly\nin relation of performance, taking the number evaluations performed by the\nalgorithm to obtain the best solution.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:57:14 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Florez", "Edson", ""], ["Diaz", "Nelson", ""], ["Gomez", "Wilfredo", ""], ["Bautista", "Lola", ""], ["Delgado", "Dario", ""]]}, {"id": "1711.07907", "submitter": "Ke Li Kl", "authors": "Ke Li, Renzhi Chen, Guangtao Fu, Xin Yao", "title": "Two-Archive Evolutionary Algorithm for Constrained Multi-Objective\n  Optimization", "comments": "26 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When solving constrained multi-objective optimization problems, an important\nissue is how to balance convergence, diversity and feasibility simultaneously.\nTo address this issue, this paper proposes a parameter-free constraint handling\ntechnique, two-archive evolutionary algorithm, for constrained multi-objective\noptimization. It maintains two co-evolving populations simultaneously: one,\ndenoted as convergence archive, is the driving force to push the population\ntoward the Pareto front; the other one, denoted as diversity archive, mainly\ntends to maintain the population diversity. In particular, to complement the\nbehavior of the convergence archive and provide as much diversified information\nas possible, the diversity archive aims at exploring areas under-exploited by\nthe convergence archive including the infeasible regions. To leverage the\ncomplementary effects of both archives, we develop a restricted mating\nselection mechanism that adaptively chooses appropriate mating parents from\nthem according to their evolution status. Comprehensive experiments on a series\nof benchmark problems and a real-world case study fully demonstrate the\ncompetitiveness of our proposed algorithm, comparing to five state-of-the-art\nconstrained evolutionary multi-objective optimizers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:55:01 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Li", "Ke", ""], ["Chen", "Renzhi", ""], ["Fu", "Guangtao", ""], ["Yao", "Xin", ""]]}, {"id": "1711.07966", "submitter": "Daniel George", "authors": "Daniel George and E. A. Huerta", "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter\n  Estimation with LIGO Data", "comments": "Camera-ready (final) version accepted to NIPS 2017 conference\n  workshop on Deep Learning for Physical Sciences and selected for contributed\n  talk. Also awarded 1st place at ACM SRC at SC17. Extended article:\n  arXiv:1711.03121", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc astro-ph.HE astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Nobel-prize-winning detections of gravitational waves from merging\nblack holes and the subsequent detection of the collision of two neutron stars\nin coincidence with electromagnetic observations have inaugurated a new era of\nmultimessenger astrophysics. To enhance the scope of this emergent science, we\nproposed the use of deep convolutional neural networks for the detection and\ncharacterization of gravitational wave signals in real-time. This method, Deep\nFiltering, was initially demonstrated using simulated LIGO noise. In this\narticle, we present the extension of Deep Filtering using real data from the\nfirst observing run of LIGO, for both detection and parameter estimation of\ngravitational waves from binary black hole mergers with continuous data streams\nfrom multiple LIGO detectors. We show for the first time that machine learning\ncan detect and estimate the true parameters of a real GW event observed by\nLIGO. Our comparisons show that Deep Filtering is far more computationally\nefficient than matched-filtering, while retaining similar sensitivity and lower\nerrors, allowing real-time processing of weak time-series signals in\nnon-stationary non-Gaussian noise, with minimal resources, and also enables the\ndetection of new classes of gravitational wave sources that may go unnoticed\nwith existing detection algorithms. This approach is uniquely suited to enable\ncoincident detection campaigns of gravitational waves and their multimessenger\ncounterparts in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:45:01 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 19:36:44 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["George", "Daniel", ""], ["Huerta", "E. A.", ""]]}, {"id": "1711.08336", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu", "title": "DeepSign: Deep Learning for Automatic Malware Signature Generation and\n  Classification", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), pages\n  1-8, Killarney, Ireland, July 2015", "doi": "10.1109/IJCNN.2015.7280815", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning based method for automatic malware\nsignature generation and classification. The method uses a deep belief network\n(DBN), implemented with a deep stack of denoising autoencoders, generating an\ninvariant compact representation of the malware behavior. While conventional\nsignature and token based methods for malware detection do not detect a\nmajority of new variants for existing malware, the results presented in this\npaper show that signatures generated by the DBN allow for an accurate\nclassification of new malware variants. Using a dataset containing hundreds of\nvariants for several major malware families, our method achieves 98.6%\nclassification accuracy using the signatures generated by the DBN. The\npresented method is completely agnostic to the type of malware behavior that is\nlogged (e.g., API calls and their parameters, registry entries, websites and\nports accessed, etc.), and can use any raw input from a sandbox to successfully\ntrain the deep neural network which is used to generate malware signatures.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:22:58 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 16:27:18 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08337", "submitter": "Eli (Omid) David", "authors": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "title": "Genetic Algorithms for Evolving Computer Chess Programs", "comments": "Winner of Gold Award in 11th Annual \"Humies\" Awards for\n  Human-Competitive Results. arXiv admin note: substantial text overlap with\n  arXiv:1711.06840, arXiv:1711.06841, arXiv:1711.06839", "journal-ref": "IEEE Transactions on Evolutionary Computation, Vol. 18, No. 5, pp.\n  779-789, September 2014", "doi": "10.1109/TEVC.2013.2285111", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the use of genetic algorithms for evolving: 1) a\ngrandmaster-level evaluation function, and 2) a search mechanism for a chess\nprogram, the parameter values of which are initialized randomly. The evaluation\nfunction of the program is evolved by learning from databases of (human)\ngrandmaster games. At first, the organisms are evolved to mimic the behavior of\nhuman grandmasters, and then these organisms are further improved upon by means\nof coevolution. The search mechanism is evolved by learning from tactical test\nsuites. Our results show that the evolved program outperforms a two-time world\ncomputer chess champion and is at par with the other leading computer chess\nprograms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:24:24 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["David", "Eli", ""], ["Herik", "H. Jaap van den", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08679", "submitter": "Doo Seok Jeong", "authors": "Guhyun Kim, Vladimir Kornijcuk, Dohun Kim, Inho Kim, Jaewook Kim, Hyo\n  Cheon Woo, Ji Hun Kim, Cheol Seong Hwang, Doo Seok Jeong", "title": "Markov chain Hebbian learning algorithm with ternary synaptic units", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of remarkable progress in machine learning techniques, the\nstate-of-the-art machine learning algorithms often keep machines from real-time\nlearning (online learning) due in part to computational complexity in parameter\noptimization. As an alternative, a learning algorithm to train a memory in real\ntime is proposed, which is named as the Markov chain Hebbian learning\nalgorithm. The algorithm pursues efficient memory use during training in that\n(i) the weight matrix has ternary elements (-1, 0, 1) and (ii) each update\nfollows a Markov chain--the upcoming update does not need past weight memory.\nThe algorithm was verified by two proof-of-concept tasks (handwritten digit\nrecognition and multiplication table memorization) in which numbers were taken\nas symbols. Particularly, the latter bases multiplication arithmetic on memory,\nwhich may be analogous to humans' mental arithmetic. The memory-based\nmultiplication arithmetic feasibly offers the basis of factorization,\nsupporting novel insight into the arithmetic.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:07:37 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Kim", "Guhyun", ""], ["Kornijcuk", "Vladimir", ""], ["Kim", "Dohun", ""], ["Kim", "Inho", ""], ["Kim", "Jaewook", ""], ["Woo", "Hyo Cheon", ""], ["Kim", "Ji Hun", ""], ["Hwang", "Cheol Seong", ""], ["Jeong", "Doo Seok", ""]]}, {"id": "1711.08681", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal\n  Deep Networks", "comments": "ISPRS Journal of Photogrammetry and Remote Sensing, Elsevier, A\n  Para{\\^i}tre", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate various methods to deal with semantic labeling\nof very high resolution multi-modal remote sensing data. Especially, we study\nhow deep fully convolutional networks can be adapted to deal with multi-modal\nand multi-scale remote sensing data for semantic labeling. Our contributions\nare threefold: a) we present an efficient multi-scale approach to leverage both\na large spatial context and the high resolution data, b) we investigate early\nand late fusion of Lidar and multispectral data, c) we validate our methods on\ntwo public datasets with state-of-the-art results. Our results indicate that\nlate fusion make it possible to recover errors steaming from ambiguous data,\nwhile early fusion allows for better joint-feature learning but at the cost of\nhigher sensitivity to missing data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:10:24 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1711.08762", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the\n  Jigsaw Puzzle Problem", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 170-178, Barcelona, Spain, September 2016", "doi": "10.1007/978-3-319-44781-0_21", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first deep neural network-based estimation metric\nfor the jigsaw puzzle problem. Given two puzzle piece edges, the neural network\npredicts whether or not they should be adjacent in the correct assembly of the\npuzzle, using nothing but the pixels of each piece. The proposed metric\nexhibits an extremely high precision even though no manual feature extraction\nis performed. When incorporated into an existing puzzle solver, the solution's\naccuracy increases significantly, achieving thereby a new state-of-the-art\nstandard.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:32:57 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08763", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu", "title": "DeepPainter: Painter Classification Using Deep Convolutional\n  Autoencoders", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 20-28, Barcelona, Spain, September 2016", "doi": "10.1007/978-3-319-44781-0_3", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the problem of painter classification, and propose\na novel approach based on deep convolutional autoencoder neural networks. While\nprevious approaches relied on image processing and manual feature extraction\nfrom paintings, our approach operates on the raw pixel level, without any\npreprocessing or manual feature extraction. We first train a deep convolutional\nautoencoder on a dataset of paintings, and subsequently use it to initialize a\nsupervised convolutional neural network for the classification phase.\n  The proposed approach substantially outperforms previous methods, improving\nthe previous state-of-the-art for the 3-painter classification problem from\n90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63%\nreduction in error rate.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:36:28 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.09389", "submitter": "Ashwin R Jadhav", "authors": "Ashwin R Jadhav, T.Shankar", "title": "Whale Optimization Based Energy-Efficient Cluster Head Selection\n  Algorithm for Wireless Sensor Networks", "comments": "22 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Sensor Network (WSN) consists of many individual sensors that are\ndeployed in the area of interest. These sensor nodes have major energy\nconstraints as they are small and their battery can't be replaced. They\ncollaborate together in order to gather, transmit and forward the sensed data\nto the base station. Consequently, data transmission is one of the biggest\nreasons for energy depletion in WSN. Clustering is one of the most effective\ntechniques for energy efficient data transmission in WSN. In this paper, an\nenergy efficient cluster head selection algorithm which is based on Whale\nOptimization Algorithm (WOA) called WOA-Clustering (WOA-C) is proposed.\nAccordingly, the proposed algorithm helps in selection of energy aware cluster\nheads based on a fitness function which considers the residual energy of the\nnode and the sum of energy of adjacent nodes. The proposed algorithm is\nevaluated for network lifetime, energy efficiency, throughput and overall\nstability. Furthermore, the performance of WOA-C is evaluated against other\nstandard contemporary routing protocols such as LEACH. Extensive simulations\nshow the superior performance of the proposed algorithm in terms of residual\nenergy, network lifetime and longer stability period.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 13:55:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Jadhav", "Ashwin R", ""], ["Shankar", "T.", ""]]}, {"id": "1711.09398", "submitter": "Ehsan Shojaedini", "authors": "Ehsan Shojaedini, Mahshid Majd, Reza Safabakhsh", "title": "Novel Adaptive Genetic Algorithm Sample Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sample consensus (RANSAC) is a successful algorithm in model fitting\napplications. It is vital to have strong exploration phase when there are an\nenormous amount of outliers within the dataset. Achieving a proper model is\nguaranteed by pure exploration strategy of RANSAC. However, finding the optimum\nresult requires exploitation. GASAC is an evolutionary paradigm to add\nexploitation capability to the algorithm. Although GASAC improves the results\nof RANSAC, it has a fixed strategy for balancing between exploration and\nexploitation. In this paper, a new paradigm is proposed based on genetic\nalgorithm with an adaptive strategy. We utilize an adaptive genetic operator to\nselect high fitness individuals as parents and mutate low fitness ones. In the\nmutation phase, a training method is used to gradually learn which gene is the\nbest replacement for the mutated gene. The proposed method adaptively balance\nbetween exploration and exploitation by learning about genes. During the final\nIterations, the algorithm draws on this information to improve the final\nresults. The proposed method is extensively evaluated on two set of\nexperiments. In all tests, our method outperformed the other methods in terms\nof both the number of inliers found and the speed of the algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 14:58:04 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Shojaedini", "Ehsan", ""], ["Majd", "Mahshid", ""], ["Safabakhsh", "Reza", ""]]}, {"id": "1711.09442", "submitter": "Lucas Lamata", "authors": "U. Alvarez-Rodriguez, M. Sanz, L. Lamata, E. Solano", "title": "Quantum Artificial Life in an IBM Quantum Computer", "comments": null, "journal-ref": "Scientific Reports 8, 14793 (2018)", "doi": "10.1038/s41598-018-33125-3", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first experimental realization of a quantum artificial life\nalgorithm in a quantum computer. The quantum biomimetic protocol encodes\ntailored quantum behaviors belonging to living systems, namely,\nself-replication, mutation, interaction between individuals, and death, into\nthe cloud quantum computer IBM ibmqx4. In this experiment, entanglement spreads\nthroughout generations of individuals, where genuine quantum information\nfeatures are inherited through genealogical networks. As a pioneering\nproof-of-principle, experimental data fits the ideal model with accuracy.\nThereafter, these and other models of quantum artificial life, for which no\nclassical device may predict its quantum supremacy evolution, can be further\nexplored in novel generations of quantum computers. Quantum biomimetics,\nquantum machine learning, and quantum artificial intelligence will move forward\nhand in hand through more elaborate levels of quantum complexity.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 19:44:41 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 09:46:56 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Alvarez-Rodriguez", "U.", ""], ["Sanz", "M.", ""], ["Lamata", "L.", ""], ["Solano", "E.", ""]]}, {"id": "1711.09561", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, John Kender and Zicheng Liu", "title": "HP-GAN: Probabilistic 3D human motion prediction via GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and understanding human motion dynamics has many applications,\nsuch as motion synthesis, augmented reality, security, and autonomous vehicles.\nDue to the recent success of generative adversarial networks (GAN), there has\nbeen much interest in probabilistic estimation and synthetic data generation\nusing deep neural network architectures and learning algorithms.\n  We propose a novel sequence-to-sequence model for probabilistic human motion\nprediction, trained with a modified version of improved Wasserstein generative\nadversarial networks (WGAN-GP), in which we use a custom loss function designed\nfor human motion prediction. Our model, which we call HP-GAN, learns a\nprobability density function of future human poses conditioned on previous\nposes. It predicts multiple sequences of possible future human poses, each from\nthe same input sequence but a different vector z drawn from a random\ndistribution. Furthermore, to quantify the quality of the non-deterministic\npredictions, we simultaneously train a motion-quality-assessment model that\nlearns the probability that a given skeleton sequence is a real human motion.\n  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and\nHuman3.6M. We train our model on both single and multiple action types. Its\npredictive power for long-term motion estimation is demonstrated by generating\nmultiple plausible futures of more than 30 frames from just 10 frames of input.\nWe show that most sequences generated from the same input have more than 50\\%\nprobabilities of being judged as a real human sequence. We will release all the\ncode used in this paper to Github.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 07:07:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Barsoum", "Emad", ""], ["Kender", "John", ""], ["Liu", "Zicheng", ""]]}, {"id": "1711.09663", "submitter": "Eli (Omid) David", "authors": "Ido Cohen, Eli David, Nathan S. Netanyahu, Noa Liscovitch, Gal Chechik", "title": "DeepBrain: Functional Representation of Neural In-Situ Hybridization\n  Images for Gene Ontology Classification Using Deep Convolutional Autoencoders", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 10614, pp. 287-296, Alghero, Italy, September, 2017", "doi": "10.1007/978-3-319-68612-7_33", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning-based method for learning a\nfunctional representation of mammalian neural images. The method uses a deep\nconvolutional denoising autoencoder (CDAE) for generating an invariant, compact\nrepresentation of in situ hybridization (ISH) images. While most existing\nmethods for bio-imaging analysis were not developed to handle images with\nhighly complex anatomical structures, the results presented in this paper show\nthat functional representation extracted by CDAE can help learn features of\nfunctional gene ontology categories for their classification in a highly\naccurate manner. Using this CDAE representation, our method outperforms the\nprevious state-of-the-art classification rate, by improving the average AUC\nfrom 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates\non input images that were downsampled significantly with respect to the\noriginal ones to make it computationally feasible.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:00:03 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Cohen", "Ido", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""], ["Liscovitch", "Noa", ""], ["Chechik", "Gal", ""]]}, {"id": "1711.09666", "submitter": "Eli (Omid) David", "authors": "Ishai Rosenberg, Guillaume Sicard, Eli David", "title": "DeepAPT: Nation-State APT Attribution Using End-to-End Deep Neural\n  Networks", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 10614, pp. 91-99, Alghero, Italy, September, 2017", "doi": "10.1007/978-3-319-68612-7_11", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years numerous advanced malware, aka advanced persistent threats\n(APT) are allegedly developed by nation-states. The task of attributing an APT\nto a specific nation-state is extremely challenging for several reasons. Each\nnation-state has usually more than a single cyber unit that develops such\nadvanced malware, rendering traditional authorship attribution algorithms\nuseless. Furthermore, those APTs use state-of-the-art evasion techniques,\nmaking feature extraction challenging. Finally, the dataset of such available\nAPTs is extremely small.\n  In this paper we describe how deep neural networks (DNN) could be\nsuccessfully employed for nation-state APT attribution. We use sandbox reports\n(recording the behavior of the APT when run dynamically) as raw input for the\nneural network, allowing the DNN to learn high level feature abstractions of\nthe APTs itself. Using a test set of 1,000 Chinese and Russian developed APTs,\nwe achieved an accuracy rate of 94.6%.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:04:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rosenberg", "Ishai", ""], ["Sicard", "Guillaume", ""], ["David", "Eli", ""]]}, {"id": "1711.09667", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu, Lior Wolf", "title": "DeepChess: End-to-End Deep Neural Network for Automatic Learning in\n  Chess", "comments": "Winner of Best Paper Award in ICANN 2016", "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 88-96, Barcelona, Spain, 2016", "doi": "10.1007/978-3-319-44781-0_11", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end learning method for chess, relying on deep neural\nnetworks. Without any a priori knowledge, in particular without any knowledge\nregarding the rules of chess, a deep neural network is trained using a\ncombination of unsupervised pretraining and supervised training. The\nunsupervised training extracts high level features from a given position, and\nthe supervised training learns to compare two chess positions and select the\nmore favorable one. The training relies entirely on datasets of several million\nchess games, and no further domain specific knowledge is incorporated.\n  The experiments show that the resulting neural network (referred to as\nDeepChess) is on a par with state-of-the-art chess playing programs, which have\nbeen developed through many years of manual feature selection and tuning.\nDeepChess is the first end-to-end machine learning-based method that results in\na grandmaster-level chess playing performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:04:57 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""], ["Wolf", "Lior", ""]]}, {"id": "1711.09846", "submitter": "Max Jaderberg", "authors": "Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M.\n  Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning,\n  Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu", "title": "Population Based Training of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks dominate the modern machine learning landscape, but their\ntraining and success still suffer from sensitivity to empirical choices of\nhyperparameters such as model architecture, loss function, and optimisation\nalgorithm. In this work we present \\emph{Population Based Training (PBT)}, a\nsimple asynchronous optimisation algorithm which effectively utilises a fixed\ncomputational budget to jointly optimise a population of models and their\nhyperparameters to maximise performance. Importantly, PBT discovers a schedule\nof hyperparameter settings rather than following the generally sub-optimal\nstrategy of trying to find a single fixed set to use for the whole course of\ntraining. With just a small modification to a typical distributed\nhyperparameter training framework, our method allows robust and reliable\ntraining of models. We demonstrate the effectiveness of PBT on deep\nreinforcement learning problems, showing faster wall-clock convergence and\nhigher final performance of agents by optimising over a suite of\nhyperparameters. In addition, we show the same method can be applied to\nsupervised learning for machine translation, where PBT is used to maximise the\nBLEU score directly, and also to training of Generative Adversarial Networks to\nmaximise the Inception score of generated images. In all cases PBT results in\nthe automatic discovery of hyperparameter schedules and model selection which\nresults in stable training and better final performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 17:33:27 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 16:16:21 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Jaderberg", "Max", ""], ["Dalibard", "Valentin", ""], ["Osindero", "Simon", ""], ["Czarnecki", "Wojciech M.", ""], ["Donahue", "Jeff", ""], ["Razavi", "Ali", ""], ["Vinyals", "Oriol", ""], ["Green", "Tim", ""], ["Dunning", "Iain", ""], ["Simonyan", "Karen", ""], ["Fernando", "Chrisantha", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1711.09869", "submitter": "Martin Simonovsky", "authors": "Loic Landrieu, Martin Simonovsky", "title": "Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs", "comments": "Accepted to CVPR 2018; camera ready version. Major updates to [v1]:\n  Improved performance on S3DIS (from +5.8 to +12.4 mIoU) and extended ablation\n  study in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning-based framework to tackle the challenge of\nsemantic segmentation of large-scale point clouds of millions of points. We\nargue that the organization of 3D point clouds can be efficiently captured by a\nstructure called superpoint graph (SPG), derived from a partition of the\nscanned scene into geometrically homogeneous elements. SPGs offer a compact yet\nrich representation of contextual relationships between object parts, which is\nthen exploited by a graph convolutional network. Our framework sets a new state\nof the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for\nboth Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the\nS3DIS dataset).\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:37:50 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 09:01:33 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Landrieu", "Loic", ""], ["Simonovsky", "Martin", ""]]}, {"id": "1711.09919", "submitter": "Daniel George", "authors": "Hongyu Shen, Daniel George, E. A. Huerta, Zhizhen Zhao", "title": "Denoising Gravitational Waves using Deep Learning with Recurrent\n  Denoising Autoencoders", "comments": "5 pages, 2 figures", "journal-ref": "ICASSP 2019", "doi": "10.1109/ICASSP.2019.8683061", "report-no": null, "categories": "gr-qc astro-ph.HE astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gravitational wave astronomy is a rapidly growing field of modern\nastrophysics, with observations being made frequently by the LIGO detectors.\nGravitational wave signals are often extremely weak and the data from the\ndetectors, such as LIGO, is contaminated with non-Gaussian and non-stationary\nnoise, often containing transient disturbances which can obscure real signals.\nTraditional denoising methods, such as principal component analysis and\ndictionary learning, are not optimal for dealing with this non-Gaussian noise,\nespecially for low signal-to-noise ratio gravitational wave signals.\nFurthermore, these methods are computationally expensive on large datasets. To\novercome these issues, we apply state-of-the-art signal processing techniques,\nbased on recent groundbreaking advancements in deep learning, to denoise\ngravitational wave signals embedded either in Gaussian noise or in real LIGO\nnoise. We introduce SMTDAE, a Staired Multi-Timestep Denoising Autoencoder,\nbased on sequence-to-sequence bi-directional Long-Short-Term-Memory recurrent\nneural networks. We demonstrate the advantages of using our unsupervised deep\nlearning approach and show that, after training only using simulated Gaussian\nnoise, SMTDAE achieves superior recovery performance for gravitational wave\nsignals embedded in real non-Gaussian LIGO noise.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 19:00:09 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Shen", "Hongyu", ""], ["George", "Daniel", ""], ["Huerta", "E. A.", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "1711.09958", "submitter": "Juan Quiroz", "authors": "Juan C. Quiroz, Amit Banerjee, Sushil J. Louis, Sergiu M. Dascalu", "title": "Collaborative Evolution of 3D Models", "comments": "Design Computing and Cognition 2014", "journal-ref": "Design Computing and Cognition (2014)", "doi": "10.1007/978-3-319-14956-1_28", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational model of creative design based on collaborative\ninteractive genetic algorithms. In our model, designers individually guide\ninteractive genetic algorithms (IGAs) to generate and explore potential design\nsolutions quickly. Collaboration is supported by allowing designers to share\nsolutions amongst each other while using IGAs, with the sharing of solutions\nadding variables to the search space. We present experiments on 3D modeling as\na case study, with designers creating model transformations individually and\ncollaboratively. The transformations were evaluated by participants in surveys\nand results show that individual and collaborative models were considered\nequally creative. However, the use of our collaborative IGAs model materially\nchanges resulting designs compared to individual IGAs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 20:05:38 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Quiroz", "Juan C.", ""], ["Banerjee", "Amit", ""], ["Louis", "Sushil J.", ""], ["Dascalu", "Sergiu M.", ""]]}, {"id": "1711.10067", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Nebojsa Jojic,\n  Jiashi Feng, Shuicheng Yan", "title": "WSNet: Compact and Efficient Networks Through Weight Sampling", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach and a novel architecture, termed WSNet, for\nlearning compact and efficient deep neural networks. Existing approaches\nconventionally learn full model parameters independently and then compress them\nvia ad hoc processing such as model pruning or filter factorization.\nAlternatively, WSNet proposes learning model parameters by sampling from a\ncompact set of learnable parameters, which naturally enforces {parameter\nsharing} throughout the learning process. We demonstrate that such a novel\nweight sampling approach (and induced WSNet) promotes both weights and\ncomputation sharing favorably. By employing this method, we can more\nefficiently learn much smaller networks with competitive performance compared\nto baseline networks with equal numbers of convolution filters. Specifically,\nwe consider learning compact and efficient 1D convolutional neural networks for\naudio classification. Extensive experiments on multiple audio classification\ndatasets verify the effectiveness of WSNet. Combined with weight quantization,\nthe resulted models are up to 180 times smaller and theoretically up to 16\ntimes faster than the well-established baselines, without noticeable\nperformance drop.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:43:20 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 05:11:35 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 13:41:19 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Jin", "Xiaojie", ""], ["Yang", "Yingzhen", ""], ["Xu", "Ning", ""], ["Yang", "Jianchao", ""], ["Jojic", "Nebojsa", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1711.10177", "submitter": "Kevin O'Regan", "authors": "Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov", "title": "Gradual Tuning: a better way of Fine Tuning the parameters of a Deep\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an alternative strategy for fine-tuning the\nparameters of a network. We named the technique Gradual Tuning. Once trained on\na first task, the network is fine-tuned on a second task by modifying a\nprogressively larger set of the network's parameters. We test Gradual Tuning on\ndifferent transfer learning tasks, using networks of different sizes trained\nwith different regularization techniques. The result shows that compared to the\nusual fine tuning, our approach significantly reduces catastrophic forgetting\nof the initial task, while still retaining comparable if not better performance\non the new task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:48:39 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Montone", "Guglielmo", ""], ["O'Regan", "J. Kevin", ""], ["Terekhov", "Alexander V.", ""]]}, {"id": "1711.10185", "submitter": "Kevin O'Regan", "authors": "Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov", "title": "Hyper-dimensional computing for a visual question-answering system that\n  is trainable end-to-end", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a system for visual question answering. Our\narchitecture is composed of two parts, the first part creates the logical\nknowledge base given the image. The second part evaluates questions against the\nknowledge base. Differently from previous work, the knowledge base is\nrepresented using hyper-dimensional computing. This choice has the advantage\nthat all the operations in the system, namely creating the knowledge base and\nevaluating the questions against it, are differentiable, thereby making the\nsystem easily trainable in an end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:00:55 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Montone", "Guglielmo", ""], ["O'Regan", "J. Kevin", ""], ["Terekhov", "Alexander V.", ""]]}, {"id": "1711.10204", "submitter": "Kevin O'Regan", "authors": "Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov", "title": "Block Neural Network Avoids Catastrophic Forgetting When Learning\n  Multiple Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work we propose a Deep Feed Forward network architecture which\ncan be trained according to a sequential learning paradigm, where tasks of\nincreasing difficulty are learned sequentially, yet avoiding catastrophic\nforgetting. The proposed architecture can re-use the features learned on\nprevious tasks in a new task when the old tasks and the new one are related.\nThe architecture needs fewer computational resources (neurons and connections)\nand less data for learning the new task than a network trained from scratch\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:47:51 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Montone", "Guglielmo", ""], ["O'Regan", "J. Kevin", ""], ["Terekhov", "Alexander V.", ""]]}, {"id": "1711.10354", "submitter": "Basheer Qolomany", "authors": "Basheer Qolomany, Majdi Maabreh, Ala Al-Fuqaha, Ajay Gupta, Driss\n  Benhaddou", "title": "Parameters Optimization of Deep Learning Models using Particle Swarm\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied in several fields such as machine\ntranslation, manufacturing, and pattern recognition. However, successful\napplication of deep learning depends upon appropriately setting its parameters\nto achieve high quality results. The number of hidden layers and the number of\nneurons in each layer of a deep machine learning network are two key\nparameters, which have main influence on the performance of the algorithm.\nManual parameter setting and grid search approaches somewhat ease the users\ntasks in setting these important parameters. Nonetheless, these two techniques\ncan be very time consuming. In this paper, we show that the Particle swarm\noptimization (PSO) technique holds great potential to optimize parameter\nsettings and thus saves valuable computational resources during the tuning\nprocess of deep learning models. Specifically, we use a dataset collected from\na Wi-Fi campus network to train deep learning models to predict the number of\noccupants and their locations. Our preliminary experiments indicate that PSO\nprovides an efficient approach for tuning the optimal number of hidden layers\nand the number of neurons in each layer of the deep learning algorithm when\ncompared to the grid search method. Our experiments illustrate that the\nexploration process of the landscape of configurations to find the optimal\nparameters is decreased by 77%-85%. In fact, the PSO yields even better\naccuracy results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:43:51 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Qolomany", "Basheer", ""], ["Maabreh", "Majdi", ""], ["Al-Fuqaha", "Ala", ""], ["Gupta", "Ajay", ""], ["Benhaddou", "Driss", ""]]}, {"id": "1711.10355", "submitter": "Basheer Qolomany", "authors": "Basheer Qolomany, Ala Al-Fuqaha, Driss Benhaddou, Ajay Gupta", "title": "Role of Deep LSTM Neural Networks And WiFi Networks in Support of\n  Occupancy Prediction in Smart Buildings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing how many people occupy a building, and where they are located, is a\nkey component of smart building services. Commercial, industrial and\nresidential buildings often incorporate systems used to determine occupancy.\nHowever, relatively simple sensor technology and control algorithms limit the\neffectiveness of smart building services. In this paper we propose to replace\nsensor technology with time series models that can predict the number of\noccupants at a given location and time. We use Wi-Fi data sets readily\navailable in abundance for smart building services and train Auto Regression\nIntegrating Moving Average (ARIMA) models and Long Short-Term Memory (LSTM)\ntime series models. As a use case scenario of smart building services, these\nmodels allow forecasting of the number of people at a given time and location\nin 15, 30 and 60 minutes time intervals at building as well as Access Point\n(AP) level. For LSTM, we build our models in two ways: a separate model for\nevery time scale, and a combined model for the three time scales. Our\nexperiments show that LSTM combined model reduced the computational resources\nwith respect to the number of neurons by 74.48 % for the AP level, and by 67.13\n% for the building level. Further, the root mean square error (RMSE) was\nreduced by 88.2% - 93.4% for LSTM in comparison to ARIMA for the building\nlevels models and by 80.9% - 87% for the AP level models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:44:11 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Qolomany", "Basheer", ""], ["Al-Fuqaha", "Ala", ""], ["Benhaddou", "Driss", ""], ["Gupta", "Ajay", ""]]}, {"id": "1711.10400", "submitter": "Simon Kohl", "authors": "Simon Kohl, David Bonekamp, Heinz-Peter Schlemmer, Kaneschka Yaqubi,\n  Markus Hohenfellner, Boris Hadaschik, Jan-Philipp Radtke, Klaus Maier-Hein", "title": "Adversarial Networks for Prostate Cancer Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large number of trainable parameters of deep neural networks renders them\ninherently data hungry. This characteristic heavily challenges the medical\nimaging community and to make things even worse, many imaging modalities are\nambiguous in nature leading to rater-dependant annotations that current loss\nformulations fail to capture. We propose employing adversarial training for\nsegmentation networks in order to alleviate aforementioned problems. We learn\nto segment aggressive prostate cancer utilizing challenging MRI images of 152\npatients and show that the proposed scheme is superior over the de facto\nstandard in terms of the detection sensitivity and the dice-score for\naggressive prostate cancer. The achieved relative gains are shown to be\nparticularly pronounced in the small dataset limit.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:53:33 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Kohl", "Simon", ""], ["Bonekamp", "David", ""], ["Schlemmer", "Heinz-Peter", ""], ["Yaqubi", "Kaneschka", ""], ["Hohenfellner", "Markus", ""], ["Hadaschik", "Boris", ""], ["Radtke", "Jan-Philipp", ""], ["Maier-Hein", "Klaus", ""]]}, {"id": "1711.10644", "submitter": "Seungkyun Hong", "authors": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "PSIque: Next Sequence Prediction of Satellite Images using a\n  Convolutional Sequence-to-Sequence Network", "comments": "Workshop on Deep Learning for Physical Sciences (DLPS 2017), NIPS\n  2017, Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting unseen weather phenomena is an important issue for disaster\nmanagement. In this paper, we suggest a model for a convolutional\nsequence-to-sequence autoencoder for predicting undiscovered weather situations\nfrom previous satellite images. We also propose a symmetric skip connection\nbetween encoder and decoder modules to produce more comprehensive image\npredictions. To examine our model performance, we conducted experiments for\neach suggested model to predict future satellite images from historical\nsatellite images. A specific combination of skip connection and\nsequence-to-sequence autoencoder was able to generate closest prediction from\nthe ground truth image.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:02:13 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:25:33 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hong", "Seungkyun", ""], ["Kim", "Seongchan", ""], ["Joh", "Minsu", ""], ["Song", "Sa-kwang", ""]]}, {"id": "1711.10746", "submitter": "Charles Martin", "authors": "Charles P. Martin and Jim Torresen", "title": "RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen\n  Interaction", "comments": null, "journal-ref": "Computational Intelligence in Music, Sound, Art and Design.\n  EvoMUSART 2018. Lecture Notes in Computer Science, vol 10783", "doi": "10.1007/978-3-319-77583-8_11", "report-no": null, "categories": "cs.HC cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoboJam is a machine-learning system for generating music that assists users\nof a touchscreen music app by performing responses to their short\nimprovisations. This system uses a recurrent artificial neural network to\ngenerate sequences of touchscreen interactions and absolute timings, rather\nthan high-level musical notes. To accomplish this, RoboJam's network uses a\nmixture density layer to predict appropriate touch interaction locations in\nspace and time. In this paper, we describe the design and implementation of\nRoboJam's network and how it has been integrated into a touchscreen music app.\nA preliminary evaluation analyses the system in terms of training, musical\ngeneration and user interaction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:48:06 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Martin", "Charles P.", ""], ["Torresen", "Jim", ""]]}, {"id": "1711.10761", "submitter": "Sam Leroux", "authors": "Sam Leroux, Steven Bohez, Tim Verbelen, Bert Vankeirsbilck, Pieter\n  Simoens, Bart Dhoedt", "title": "Transfer Learning with Binary Neural Networks", "comments": "Machine Learning on the Phone and other Consumer Devices, NIPS2017\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that it is possible to train deep neural networks\nwith low precision weights and activations. In the extreme case it is even\npossible to constrain the network to binary values. The costly floating point\nmultiplications are then reduced to fast logical operations. High end smart\nphones such as Google's Pixel 2 and Apple's iPhone X are already equipped with\nspecialised hardware for image processing and it is very likely that other\nfuture consumer hardware will also have dedicated accelerators for deep neural\nnetworks. Binary neural networks are attractive in this case because the\nlogical operations are very fast and efficient when implemented in hardware. We\npropose a transfer learning based architecture where we first train a binary\nnetwork on Imagenet and then retrain part of the network for different tasks\nwhile keeping most of the network fixed. The fixed binary part could be\nimplemented in a hardware accelerator while the last layers of the network are\nevaluated in software. We show that a single binary neural network trained on\nthe Imagenet dataset can indeed be used as a feature extractor for other\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:28:02 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Leroux", "Sam", ""], ["Bohez", "Steven", ""], ["Verbelen", "Tim", ""], ["Vankeirsbilck", "Bert", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1711.11023", "submitter": "Pawe{\\l} Budzianowski", "authors": "I\\~nigo Casanueva, Pawe{\\l} Budzianowski, Pei-Hao Su, Nikola\n  Mrk\\v{s}i\\'c, Tsung-Hsien Wen, Stefan Ultes, Lina Rojas-Barahona, Steve\n  Young, Milica Ga\\v{s}i\\'c", "title": "A Benchmarking Environment for Reinforcement Learning Based Task\n  Oriented Dialogue Management", "comments": "Accepted at the Deep Reinforcement Learning Symposium, 31st\n  Conference on Neural Information Processing Systems (NIPS 2017) Paper updated\n  with minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid\nthe significant effort needed to hand-craft the required dialogue flow, the\nDialogue Management (DM) module can be cast as a continuous Markov Decision\nProcess (MDP) and trained through Reinforcement Learning (RL). Several RL\nmodels have been investigated over recent years. However, the lack of a common\nbenchmarking framework makes it difficult to perform a fair comparison between\ndifferent models and their capability to generalise to different environments.\nTherefore, this paper proposes a set of challenging simulated environments for\ndialogue model development and evaluation. To provide some baselines, we\ninvestigate a number of representative parametric algorithms, namely deep\nreinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and\ncompare them to a non-parametric model, GP-SARSA. Both the environments and\npolicy models are implemented using the publicly available PyDial toolkit and\nreleased on-line, in order to establish a testbed framework for further\nexperiments and to facilitate experimental reproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:51:14 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 10:50:44 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Casanueva", "I\u00f1igo", ""], ["Budzianowski", "Pawe\u0142", ""], ["Su", "Pei-Hao", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Ultes", "Stefan", ""], ["Rojas-Barahona", "Lina", ""], ["Young", "Steve", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "1711.11059", "submitter": "Sebastian Urban", "authors": "Sebastian Urban, Marcus Basalla, Patrick van der Smagt", "title": "Gaussian Process Neurons Learn Stochastic Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose stochastic, non-parametric activation functions that are fully\nlearnable and individual to each neuron. Complexity and the risk of overfitting\nare controlled by placing a Gaussian process prior over these functions. The\nresult is the Gaussian process neuron, a probabilistic unit that can be used as\nthe basic building block for probabilistic graphical models that resemble the\nstructure of neural networks. The proposed model can intrinsically handle\nuncertainties in its inputs and self-estimate the confidence of its\npredictions. Using variational Bayesian inference and the central limit\ntheorem, a fully deterministic loss function is derived, allowing it to be\ntrained as efficiently as a conventional neural network using mini-batch\ngradient descent. The posterior distribution of activation functions is\ninferred from the training data alongside the weights of the network.\n  The proposed model favorably compares to deep Gaussian processes, both in\nmodel complexity and efficiency of inference. It can be directly applied to\nrecurrent or convolutional network structures, allowing its use in audio and\nimage processing tasks.\n  As an preliminary empirical evaluation we present experiments on regression\nand classification tasks, in which our model achieves performance comparable to\nor better than a Dropout regularized neural network with a fixed activation\nfunction. Experiments are ongoing and results will be added as they become\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:09:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Urban", "Sebastian", ""], ["Basalla", "Marcus", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1711.11240", "submitter": "Yudong Cao", "authors": "Yudong Cao and Gian Giacomo Guerreschi and Al\\'an Aspuru-Guzik", "title": "Quantum Neuron: an elementary building block for machine learning on\n  quantum computers", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even the most sophisticated artificial neural networks are built by\naggregating substantially identical units called neurons. A neuron receives\nmultiple signals, internally combines them, and applies a non-linear function\nto the resulting weighted sum. Several attempts to generalize neurons to the\nquantum regime have been proposed, but all proposals collided with the\ndifficulty of implementing non-linear activation functions, which is essential\nfor classical neurons, due to the linear nature of quantum mechanics. Here we\npropose a solution to this roadblock in the form of a small quantum circuit\nthat naturally simulates neurons with threshold activation. Our quantum circuit\ndefines a building block, the \"quantum neuron\", that can reproduce a variety of\nclassical neural network constructions while maintaining the ability to process\nsuperpositions of inputs and preserve quantum coherence and entanglement. In\nthe construction of feedforward networks of quantum neurons, we provide\nnumerical evidence that the network not only can learn a function when trained\nwith superposition of inputs and the corresponding output, but that this\ntraining suffices to learn the function on all individual inputs separately.\nWhen arranged to mimic Hopfield networks, quantum neural networks exhibit\nproperties of associative memory. Patterns are encoded using the simple Hebbian\nrule for the weights and we demonstrate attractor dynamics from corrupted\ninputs. Finally, the fact that our quantum model closely captures (traditional)\nneural network dynamics implies that the vast body of literature and results on\nneural networks becomes directly relevant in the context of quantum machine\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 05:38:52 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Cao", "Yudong", ""], ["Guerreschi", "Gian Giacomo", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1711.11486", "submitter": "Pawe{\\l} Budzianowski", "authors": "Christopher Tegho, Pawe{\\l} Budzianowski, Milica Ga\\v{s}i\\'c", "title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy\n  Optimisation", "comments": "Accepted at the Bayesian Deep Learning Workshop, 31st Conference on\n  Neural Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical dialogue management, the dialogue manager learns a policy that\nmaps a belief state to an action for the system to perform. Efficient\nexploration is key to successful policy optimisation. Current deep\nreinforcement learning methods are very promising but rely on epsilon-greedy\nexploration, thus subjecting the user to a random choice of action during\nlearning. Alternative approaches such as Gaussian Process SARSA (GPSARSA)\nestimate uncertainties and are sample efficient, leading to better user\nexperience, but on the expense of a greater computational complexity. This\npaper examines approaches to extract uncertainty estimates from deep Q-networks\n(DQN) in the context of dialogue management. We perform an extensive benchmark\nof deep Bayesian methods to extract uncertainty estimates, namely\nBayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and\nalpha-divergences, combining it with DQN algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:09:02 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Tegho", "Christopher", ""], ["Budzianowski", "Pawe\u0142", ""], ["Ga\u0161i\u0107", "Milica", ""]]}]