[{"id": "1705.00094", "submitter": "Marcos Cardinot", "authors": "Marcos Cardinot, Colm O'Riordan and Josephine Griffith", "title": "The Impact of Coevolution and Abstention on the Emergence of Cooperation", "comments": "To appear at Studies in Computational Intelligence (SCI), Springer,\n  2017", "journal-ref": "Studies in Computational Intelligence, 2019, vol 792. Springer", "doi": "10.1007/978-3-319-99283-9_6", "report-no": null, "categories": "cs.GT cs.AI cs.MA cs.NE math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the Coevolutionary Optional Prisoner's Dilemma (COPD)\ngame, which is a simple model to coevolve game strategy and link weights of\nagents playing the Optional Prisoner's Dilemma game. We consider a population\nof agents placed in a lattice grid with boundary conditions. A number of Monte\nCarlo simulations are performed to investigate the impacts of the COPD game on\nthe emergence of cooperation. Results show that the coevolutionary rules enable\ncooperators to survive and even dominate, with the presence of abstainers in\nthe population playing a key role in the protection of cooperators against\nexploitation from defectors. We observe that in adverse conditions such as when\nthe initial population of abstainers is too scarce/abundant, or when the\ntemptation to defect is very high, cooperation has no chance of emerging.\nHowever, when the simple coevolutionary rules are applied, cooperators\nflourish.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 23:15:11 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Cardinot", "Marcos", ""], ["O'Riordan", "Colm", ""], ["Griffith", "Josephine", ""]]}, {"id": "1705.00368", "submitter": "Miqing Li", "authors": "Miqing Li, Liangli Zhen, Xin Yao", "title": "How to Read Many-Objective Solution Sets in Parallel Coordinates", "comments": "18 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid development of evolutionary algorithms in handling many-objective\noptimization problems requires viable methods of visualizing a high-dimensional\nsolution set. Parallel coordinates which scale well to high-dimensional data\nare such a method, and have been frequently used in evolutionary many-objective\noptimization. However, the parallel coordinates plot is not as straightforward\nas the classic scatter plot to present the information contained in a solution\nset. In this paper, we make some observations of the parallel coordinates plot,\nin terms of comparing the quality of solution sets, understanding the shape and\ndistribution of a solution set, and reflecting the relation between objectives.\nWe hope that these observations could provide some guidelines as to the proper\nuse of parallel coordinates in evolutionary many-objective optimization.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 19:51:13 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Li", "Miqing", ""], ["Zhen", "Liangli", ""], ["Yao", "Xin", ""]]}, {"id": "1705.00557", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Samuel R. Bowman and David Sontag", "title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 09:15:35 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jernite", "Yacine", ""], ["Bowman", "Samuel R.", ""], ["Sontag", "David", ""]]}, {"id": "1705.00574", "submitter": "Alexey Romanov", "authors": "Alexey Romanov and Anna Rumshisky", "title": "Forced to Learn: Discovering Disentangled Representations Without\n  Exhaustive Labels", "comments": "Abstract accepted at ICLR 2017 Workshop:\n  https://openreview.net/pdf?id=SkCmfeSFg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a better representation with neural networks is a challenging\nproblem, which was tackled extensively from different prospectives in the past\nfew years. In this work, we focus on learning a representation that could be\nused for a clustering task and introduce two novel loss components that\nsubstantially improve the quality of produced clusters, are simple to apply to\nan arbitrary model and cost function, and do not require a complicated training\nprocedure. We evaluate them on two most common types of models, Recurrent\nNeural Networks and Convolutional Neural Networks, showing that the approach we\npropose consistently improves the quality of KMeans clustering in terms of\nAdjusted Mutual Information score and outperforms previously proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 16:03:25 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Romanov", "Alexey", ""], ["Rumshisky", "Anna", ""]]}, {"id": "1705.00594", "submitter": "Randal Olson", "authors": "Randal S. Olson, Moshe Sipper, William La Cava, Sharon Tartarone,\n  Steven Vitale, Weixuan Fu, Patryk Orzechowski, Ryan J. Urbanowicz, John H.\n  Holmes, Jason H. Moore", "title": "A System for Accessible Artificial Intelligence", "comments": "14 pages, 5 figures, submitted to Genetic Programming Theory and\n  Practice 2017 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While artificial intelligence (AI) has become widespread, many commercial AI\nsystems are not yet accessible to individual researchers nor the general public\ndue to the deep knowledge of the systems required to use them. We believe that\nAI has matured to the point where it should be an accessible technology for\neveryone. We present an ongoing project whose ultimate goal is to deliver an\nopen source, user-friendly AI system that is specialized for machine learning\nanalysis of complex data in the biomedical and health care domains. We discuss\nhow genetic programming can aid in this endeavor, and highlight specific\nexamples where genetic programming has automated machine learning analyses in\nprevious projects.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:11:48 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 17:14:14 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Olson", "Randal S.", ""], ["Sipper", "Moshe", ""], ["La Cava", "William", ""], ["Tartarone", "Sharon", ""], ["Vitale", "Steven", ""], ["Fu", "Weixuan", ""], ["Orzechowski", "Patryk", ""], ["Urbanowicz", "Ryan J.", ""], ["Holmes", "John H.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1705.01346", "submitter": "Danhao Zhu", "authors": "Danhao Zhu, Si Shen, Xin-Yu Dai and Jiajun Chen", "title": "Going Wider: Recurrent Neural Network With Parallel Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) has been widely applied for sequence modeling.\nIn RNN, the hidden states at current step are full connected to those at\nprevious step, thus the influence from less related features at previous step\nmay potentially decrease model's learning ability. We propose a simple\ntechnique called parallel cells (PCs) to enhance the learning ability of\nRecurrent Neural Network (RNN). In each layer, we run multiple small RNN cells\nrather than one single large cell. In this paper, we evaluate PCs on 2 tasks.\nOn language modeling task on PTB (Penn Tree Bank), our model outperforms state\nof art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English\ntranslation task, our model increases BLEU score for 0.39 points than baseline\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 10:22:22 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Zhu", "Danhao", ""], ["Shen", "Si", ""], ["Dai", "Xin-Yu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1705.01365", "submitter": "Dmitry Yarotsky", "authors": "Dmitry Yarotsky", "title": "Quantified advantage of discontinuous weight selection in approximations\n  with deep neural networks", "comments": "12 pages, submitted to J. Approx. Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximations of 1D Lipschitz functions by deep ReLU networks of\na fixed width. We prove that without the assumption of continuous weight\nselection the uniform approximation error is lower than with this assumption at\nleast by a factor logarithmic in the size of the network.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:29:28 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Yarotsky", "Dmitry", ""]]}, {"id": "1705.01462", "submitter": "Naveen Mellempudi", "authors": "Naveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das,\n  Bharat Kaul, Pradeep Dubey", "title": "Ternary Neural Networks with Fine-Grained Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel fine-grained quantization (FGQ) method to ternarize\npre-trained full precision models, while also constraining activations to 8 and\n4-bits. Using this method, we demonstrate a minimal loss in classification\naccuracy on state-of-the-art topologies without additional training. We provide\nan improved theoretical formulation that forms the basis for a higher quality\nsolution using FGQ. Our method involves ternarizing the original weight tensor\nin groups of $N$ weights. Using $N=4$, we achieve Top-1 accuracy within $3.7\\%$\nand $4.2\\%$ of the baseline full precision result for Resnet-101 and Resnet-50\nrespectively, while eliminating $75\\%$ of all multiplications. These results\nenable a full 8/4-bit inference pipeline, with best-reported accuracy using\nternary weights on ImageNet dataset, with a potential of $9\\times$ improvement\nin performance. Also, for smaller networks like AlexNet, FGQ achieves\nstate-of-the-art results. We further study the impact of group size on both\nperformance and accuracy. With a group size of $N=64$, we eliminate\n$\\approx99\\%$ of the multiplications; however, this introduces a noticeable\ndrop in accuracy, which necessitates fine tuning the parameters at lower\nprecision. We address this by fine-tuning Resnet-50 with 8-bit activations and\nternary weights at $N=64$, improving the Top-1 accuracy to within $4\\%$ of the\nfull precision result with $<30\\%$ additional training overhead. Our final\nquantized model can run on a full 8-bit compute pipeline using 2-bit weights\nand has the potential of up to $15\\times$ improvement in performance compared\nto baseline full-precision models.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:15:21 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 09:19:55 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 17:10:24 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Mellempudi", "Naveen", ""], ["Kundu", "Abhisek", ""], ["Mudigere", "Dheevatsa", ""], ["Das", "Dipankar", ""], ["Kaul", "Bharat", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1705.01502", "submitter": "Ran Rubin", "authors": "Ran Rubin, L.F. Abbott and Haim Sompolinsky", "title": "Balanced Excitation and Inhibition are Required for High-Capacity,\n  Noise-Robust Neuronal Selectivity", "comments": "Article and supplementary information", "journal-ref": "Proceedings of the National Academy of Sciences of the United\n  States of America, 114(41), 2017", "doi": "10.1073/pnas.1705841114", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons and networks in the cerebral cortex must operate reliably despite\nmultiple sources of noise. To evaluate the impact of both input and output\nnoise, we determine the robustness of single-neuron stimulus selective\nresponses, as well as the robustness of attractor states of networks of neurons\nperforming memory tasks. We find that robustness to output noise requires\nsynaptic connections to be in a balanced regime in which excitation and\ninhibition are strong and largely cancel each other. We evaluate the conditions\nrequired for this regime to exist and determine the properties of networks\noperating within it. A plausible synaptic plasticity rule for learning that\nbalances weight configurations is presented. Our theory predicts an optimal\nratio of the number of excitatory and inhibitory synapses for maximizing the\nencoding capacity of balanced networks for a given statistics of afferent\nactivations. Previous work has shown that balanced networks amplify\nspatio-temporal variability and account for observed asynchronous irregular\nstates. Here we present a novel type of balanced network that amplifies small\nchanges in the impinging signals, and emerges automatically from learning to\nperform neuronal and network functions robustly.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:38:01 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Rubin", "Ran", ""], ["Abbott", "L. F.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1705.01721", "submitter": "Elmar Langetepe", "authors": "Martin Kretschmer and Elmar Langetepe", "title": "Evolutionary learning of fire fighting strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic problem of enclosing an expanding fire can be modelled by a\ndiscrete variant in a grid graph. While the fire expands to all neighbouring\ncells in any time step, the fire fighter is allowed to block $c$ cells in the\naverage outside the fire in the same time interval. It was shown that the\nsuccess of the fire fighter is guaranteed for $c>1.5$ but no strategy can\nenclose the fire for $c\\leq 1.5$. For achieving such a critical threshold the\ncorrectness (sometimes even optimality) of strategies and lower bounds have\nbeen shown by integer programming or by direct but often very sophisticated\narguments. We investigate the problem whether it is possible to find or to\napproach such a threshold and/or optimal strategies by means of evolutionary\nalgorithms, i.e., we just try to learn successful strategies for different\nconstants $c$ and have a look at the outcome. The main general idea is that\nthis approach might give some insight in the power of evolutionary strategies\nfor similar geometrically motivated threshold questions. We investigate the\nvariant of protecting a highway with still unknown threshold and found\ninteresting strategic paradigms.\n  Keywords: Dynamic environments, fire fighting, evolutionary strategies,\nthreshold approximation\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 07:14:30 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Kretschmer", "Martin", ""], ["Langetepe", "Elmar", ""]]}, {"id": "1705.01809", "submitter": "Parth Sane", "authors": "Parth Sane, Ravindra Agrawal", "title": "Pixel Normalization from Numeric Data as Input to Neural Networks", "comments": "IEEE WiSPNET 2017 conference in Chennai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text to image transformation for input to neural networks requires\nintermediate steps. This paper attempts to present a new approach to pixel\nnormalization so as to convert textual data into image, suitable as input for\nneural networks. This method can be further improved by its Graphics Processing\nUnit (GPU) implementation to provide significant speedup in computational time.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 12:20:56 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Sane", "Parth", ""], ["Agrawal", "Ravindra", ""]]}, {"id": "1705.02042", "submitter": "James Aimone", "authors": "James B. Aimone", "title": "Exponential scaling of neural algorithms - a future beyond Moore's Law?", "comments": "Submitted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the brain has long been considered a potential inspiration for\nfuture computing, Moore's Law - the scaling property that has seen revolutions\nin technologies ranging from supercomputers to smart phones - has largely been\ndriven by advances in materials science. As the ability to miniaturize\ntransistors is coming to an end, there is increasing attention on new\napproaches to computation, including renewed enthusiasm around the potential of\nneural computation. This paper describes how recent advances in\nneurotechnologies, many of which have been aided by computing's rapid\nprogression over recent decades, are now reigniting this opportunity to bring\nneural computation insights into broader computing applications. As we\nunderstand more about the brain, our ability to motivate new computing\nparadigms with continue to progress. These new approaches to computing, which\nwe are already seeing in techniques such as deep learning and neuromorphic\nhardware, will themselves improve our ability to learn about the brain and\naccordingly can be projected to give rise to even further insights. This paper\nwill describe how this positive feedback has the potential to change the\ncomplexion of how computing sciences and neurosciences interact, and suggests\nthat the next form of exponential scaling in computing may emerge from our\nprogressive understanding of the brain.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 22:54:54 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 14:37:34 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Aimone", "James B.", ""]]}, {"id": "1705.02176", "submitter": "Liudmila Zhilyakova", "authors": "Nikolay Bazenkov, Varvara Dyakonova, Oleg Kuznetsov, Dmitri Sakharov,\n  Dmitry Vorontsov, Liudmila Zhilyakova", "title": "Discrete Modeling of Multi-Transmitter Neural Networks with Neuron\n  Competition", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel discrete model of central pattern generators (CPG),\nneuronal ensembles generating rhythmic activity. The model emphasizes the role\nof nonsynaptic interactions and the diversity of electrical properties in\nnervous systems. Neurons in the model release different neurotransmitters into\nthe shared extracellular space (ECS) so each neuron with the appropriate set of\nreceptors can receive signals from other neurons. We consider neurons,\ndiffering in their electrical activity, represented as finite-state machines\nfunctioning in discrete time steps. Discrete modeling is aimed to provide a\ncomputationally tractable and compact explanation of rhythmic pattern\ngeneration in nervous systems. The important feature of the model is the\nintroduced mechanism of neuronal competition which is shown to be responsible\nfor the generation of proper rhythms. The model is illustrated with two\nexamples: a half-center oscillator considered to be a basic mechanism of\nemerging rhythmic activity and the well-studied feeding network of a pond\nsnail. Future research will focus on the neuromodulatory effects ubiquitous in\nCPG networks and the whole nervous systems.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 11:54:29 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 18:09:35 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Bazenkov", "Nikolay", ""], ["Dyakonova", "Varvara", ""], ["Kuznetsov", "Oleg", ""], ["Sakharov", "Dmitri", ""], ["Vorontsov", "Dmitry", ""], ["Zhilyakova", "Liudmila", ""]]}, {"id": "1705.02302", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Or Sharir, Yoav Levine, Ronen Tamari, David Yakira, Amnon\n  Shashua", "title": "Analysis and Design of Convolutional Networks via Hierarchical Tensor\n  Decompositions", "comments": "Part of the Intel Collaborative Research Institute for Computational\n  Intelligence (ICRI-CI) Special Issue on Deep Learning Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The driving force behind convolutional networks - the most successful deep\nlearning architecture to date, is their expressive power. Despite its wide\nacceptance and vast empirical evidence, formal analyses supporting this belief\nare scarce. The primary notions for formally reasoning about expressiveness are\nefficiency and inductive bias. Expressive efficiency refers to the ability of a\nnetwork architecture to realize functions that require an alternative\narchitecture to be much larger. Inductive bias refers to the prioritization of\nsome functions over others given prior knowledge regarding a task at hand. In\nthis paper we overview a series of works written by the authors, that through\nan equivalence to hierarchical tensor decompositions, analyze the expressive\nefficiency and inductive bias of various convolutional network architectural\nfeatures (depth, width, strides and more). The results presented shed light on\nthe demonstrated effectiveness of convolutional networks, and in addition,\nprovide new tools for network design.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:09:58 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 16:54:48 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 06:15:41 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 19:07:18 GMT"}, {"version": "v5", "created": "Mon, 11 Jun 2018 06:38:42 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Cohen", "Nadav", ""], ["Sharir", "Or", ""], ["Levine", "Yoav", ""], ["Tamari", "Ronen", ""], ["Yakira", "David", ""], ["Shashua", "Amnon", ""]]}, {"id": "1705.02494", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji", "title": "Learning Distributed Representations of Texts and Entities from\n  Knowledge Base", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics, 5\n  (2017), 397-411", "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a neural network model that jointly learns distributed\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\nKB, we train our proposed model to predict entities that are relevant to the\ntext. Our model is designed to be generic with the ability to address various\nNLP tasks with ease. We train the model using a large corpus of texts and their\nentity annotations extracted from Wikipedia. We evaluated the model on three\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\nfactoid question answering) involving both unsupervised and supervised\nsettings. As a result, we achieved state-of-the-art results on all three of\nthese tasks. Our code and trained models are publicly available for further\nacademic research.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 15:11:30 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 16:59:08 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 15:27:55 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Yamada", "Ikuya", ""], ["Shindo", "Hiroyuki", ""], ["Takeda", "Hideaki", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "1705.02583", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang, Srinjoy Das, Ojash Neopane and Ken Kreutz-Delgado", "title": "A Design Methodology for Efficient Implementation of Deconvolutional\n  Neural Networks on an FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years deep learning algorithms have shown extremely high\nperformance on machine learning tasks such as image classification and speech\nrecognition. In support of such applications, various FPGA accelerator\narchitectures have been proposed for convolutional neural networks (CNNs) that\nenable high performance for classification tasks at lower power than CPU and\nGPU processors. However, to date, there has been little research on the use of\nFPGA implementations of deconvolutional neural networks (DCNNs). DCNNs, also\nknown as generative CNNs, encode high-dimensional probability distributions and\nhave been widely used for computer vision applications such as scene\ncompletion, scene segmentation, image creation, image denoising, and\nsuper-resolution imaging. We propose an FPGA architecture for deconvolutional\nnetworks built around an accelerator which effectively handles the complex\nmemory access patterns needed to perform strided deconvolutions, and that\nsupports convolution as well. We also develop a three-step design optimization\nmethod that systematically exploits statistical analysis, design space\nexploration and VLSI optimization. To verify our FPGA deconvolutional\naccelerator design methodology we train DCNNs offline on two representative\ndatasets using the generative adversarial network method (GAN) run on\nTensorflow, and then map these DCNNs to an FPGA DCNN-plus-accelerator\nimplementation to perform generative inference on a Xilinx Zynq-7000 FPGA. Our\nDCNN implementation achieves a peak performance density of 0.012 GOPs/DSP.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 09:18:44 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Zhang", "Xinyu", ""], ["Das", "Srinjoy", ""], ["Neopane", "Ojash", ""], ["Kreutz-Delgado", "Ken", ""]]}, {"id": "1705.02643", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Francesco Crecchi and Davide Morelli", "title": "DropIn: Making Reservoir Computing Neural Networks Robust to Missing\n  Inputs by Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel, principled approach to train recurrent neural\nnetworks from the Reservoir Computing family that are robust to missing part of\nthe input features at prediction time. By building on the ensembling properties\nof Dropout regularization, we propose a methodology, named DropIn, which\nefficiently trains a neural model as a committee machine of subnetworks, each\ncapable of predicting with a subset of the original input features. We discuss\nthe application of the DropIn methodology in the context of Reservoir Computing\nmodels and targeting applications characterized by input sources that are\nunreliable or prone to be disconnected, such as in pervasive wireless sensor\nnetworks and ambient intelligence. We provide an experimental assessment using\nreal-world data from such application domains, showing how the Dropin\nmethodology allows to maintain predictive performances comparable to those of a\nmodel without missing features, even when 20\\%-50\\% of the inputs are not\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 16:03:06 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bacciu", "Davide", ""], ["Crecchi", "Francesco", ""], ["Morelli", "Davide", ""]]}, {"id": "1705.02995", "submitter": "Zhezhi He", "authors": "Zhezhi He and Deliang Fan", "title": "Developing All-Skyrmion Spiking Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have proposed a revolutionary neuromorphic computing\nmethodology to implement All-Skyrmion Spiking Neural Network (AS-SNN). Such\nproposed methodology is based on our finding that skyrmion is a topological\nstable spin texture and its spatiotemporal motion along the magnetic nano-track\nintuitively interprets the pulse signal transmission between two interconnected\nneurons. In such design, spike train in SNN could be encoded as particle-like\nskyrmion train and further processed by the proposed skyrmion-synapse and\nskyrmion-neuron within the same magnetic nano-track to generate output skyrmion\nas post-spike. Then, both pre-neuron spikes and post-neuron spikes are encoded\nas particle-like skyrmions without conversion between charge and spin signals,\nwhich fundamentally differentiates our proposed design from other hybrid\nSpin-CMOS designs. The system level simulation shows 87.1% inference accuracy\nfor handwritten digit recognition task, while the energy dissipation is ~1\nfJ/per spike which is 3 orders smaller in comparison with CMOS based IBM\nTrueNorth system.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:54:31 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["He", "Zhezhi", ""], ["Fan", "Deliang", ""]]}, {"id": "1705.03151", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yixiang Chen, Lantian Li and Andrew Abel", "title": "Phonetic Temporal Neural Model for Language Identification", "comments": "Submitted to TASLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural models, particularly the LSTM-RNN model, have shown great\npotential for language identification (LID). However, the use of phonetic\ninformation has been largely overlooked by most existing neural LID methods,\nalthough this information has been used very successfully in conventional\nphonetic LID systems. We present a phonetic temporal neural model for LID,\nwhich is an LSTM-RNN LID system that accepts phonetic features produced by a\nphone-discriminative DNN as the input, rather than raw acoustic features. This\nnew model is similar to traditional phonetic LID methods, but the phonetic\nknowledge here is much richer: it is at the frame level and involves compacted\ninformation of all phones. Our experiments conducted on the Babel database and\nthe AP16-OLR database demonstrate that the temporal phonetic neural approach is\nvery effective, and significantly outperforms existing acoustic neural models.\nIt also outperforms the conventional i-vector approach on short utterances and\nin noisy conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:46:21 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:23:34 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 05:23:26 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Chen", "Yixiang", ""], ["Li", "Lantian", ""], ["Abel", "Andrew", ""]]}, {"id": "1705.03152", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yixiang Chen, Ying Shi, Lantian Li", "title": "Phone-aware Neural Language Identification", "comments": "arXiv admin note: text overlap with arXiv:1705.03151", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure acoustic neural models, particularly the LSTM-RNN model, have shown\ngreat potential in language identification (LID). However, the phonetic\ninformation has been largely overlooked by most of existing neural LID models,\nalthough this information has been used in the conventional phonetic LID\nsystems with a great success. We present a phone-aware neural LID architecture,\nwhich is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR\nsystem. By utilizing the phonetic knowledge, the LID performance can be\nsignificantly improved. Interestingly, even if the test language is not\ninvolved in the ASR training, the phonetic knowledge still presents a large\ncontribution. Our experiments conducted on four languages within the Babel\ncorpus demonstrated that the phone-aware approach is highly effective.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:47:22 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:43:40 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Chen", "Yixiang", ""], ["Shi", "Ying", ""], ["Li", "Lantian", ""]]}, {"id": "1705.03302", "submitter": "Iztok Fister", "authors": "Iztok Fister Jr. and Du\\v{s}an Fister and Suash Deb and Uro\\v{s}\n  Mlakar and Janez Brest and Iztok Fister", "title": "Making up for the deficit in a marathon run", "comments": "ISMSI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To predict the final result of an athlete in a marathon run thoroughly is the\neternal desire of each trainer. Usually, the achieved result is weaker than the\npredicted one due to the objective (e.g., environmental conditions) as well as\nsubjective factors (e.g., athlete's malaise). Therefore, making up for the\ndeficit between predicted and achieved results is the main ingredient of the\nanalysis performed by trainers after the competition. In the analysis, they\nsearch for parts of a marathon course where the athlete lost time. This paper\nproposes an automatic making up for the deficit by using a Differential\nEvolution algorithm. In this case study, the results that were obtained by a\nwearable sports-watch by an athlete in a real marathon are analyzed. The first\nexperiments with Differential Evolution show the possibility of using this\nmethod in the future.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 13:07:12 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Fister", "Iztok", "Jr."], ["Fister", "Du\u0161an", ""], ["Deb", "Suash", ""], ["Mlakar", "Uro\u0161", ""], ["Brest", "Janez", ""], ["Fister", "Iztok", ""]]}, {"id": "1705.03556", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, W. Bruce Croft", "title": "Relevance-based Word Embedding", "comments": "to appear in the proceedings of The 40th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a high-dimensional dense representation for vocabulary terms, also\nknown as a word embedding, has recently attracted much attention in natural\nlanguage processing and information retrieval tasks. The embedding vectors are\ntypically learned based on term proximity in a large corpus. This means that\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\naccurately predict adjacent word(s) for a given word or context. However, this\nobjective is not necessarily equivalent to the goal of many information\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\nrelevance instead of term proximity, syntactic, or even semantic similarity.\nThis is the motivation for developing unsupervised relevance-based word\nembedding models that learn word representations based on query-document\nrelevance information. In this paper, we propose two learning models with\ndifferent objective functions; one learns a relevance distribution over the\nvocabulary set for each query, and the other classifies each term as belonging\nto the relevant or non-relevant class for each query. To train our models, we\nused over six million unique queries and the top ranked documents retrieved in\nresponse to each query, which are assumed to be relevant to the query. We\nextrinsically evaluate our learned word representation models using two IR\ntasks: query expansion and query classification. Both query expansion\nexperiments on four TREC collections and query classification experiments on\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\nsignificantly outperform state-of-the-art proximity-based embedding models,\nsuch as word2vec and GloVe.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 22:11:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1705.03572", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "Discovery Radiomics via Evolutionary Deep Radiomic Sequencer Discovery\n  for Pathologically-Proven Lung Cancer Detection", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While lung cancer is the second most diagnosed form of cancer in men and\nwomen, a sufficiently early diagnosis can be pivotal in patient survival rates.\nImaging-based, or radiomics-driven, detection methods have been developed to\naid diagnosticians, but largely rely on hand-crafted features which may not\nfully encapsulate the differences between cancerous and healthy tissue.\nRecently, the concept of discovery radiomics was introduced, where custom\nabstract features are discovered from readily available imaging data. We\npropose a novel evolutionary deep radiomic sequencer discovery approach based\non evolutionary deep intelligence. Motivated by patient privacy concerns and\nthe idea of operational artificial intelligence, the evolutionary deep radiomic\nsequencer discovery approach organically evolves increasingly more efficient\ndeep radiomic sequencers that produce significantly more compact yet similarly\ndescriptive radiomic sequences over multiple generations. As a result, this\nframework improves operational efficiency and enables diagnosis to be run\nlocally at the radiologist's computer while maintaining detection accuracy. We\nevaluated the evolved deep radiomic sequencer (EDRS) discovered via the\nproposed evolutionary deep radiomic sequencer discovery framework against\nstate-of-the-art radiomics-driven and discovery radiomics methods using\nclinical lung CT data with pathologically-proven diagnostic data from the\nLIDC-IDRI dataset. The evolved deep radiomic sequencer shows improved\nsensitivity (93.42%), specificity (82.39%), and diagnostic accuracy (88.78%)\nrelative to previous radiomics approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 00:20:23 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 00:01:27 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chung", "Audrey G.", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1705.04058", "submitter": "Yongcheng Jing", "authors": "Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu,\n  Mingli Song", "title": "Neural Style Transfer: A Review", "comments": "Project page: https://github.com/ycjing/Neural-Style-Transfer-Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 08:08:44 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 09:21:36 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 13:25:07 GMT"}, {"version": "v4", "created": "Thu, 26 Apr 2018 12:35:19 GMT"}, {"version": "v5", "created": "Wed, 16 May 2018 11:59:51 GMT"}, {"version": "v6", "created": "Sun, 17 Jun 2018 08:40:41 GMT"}, {"version": "v7", "created": "Tue, 30 Oct 2018 09:48:05 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Jing", "Yongcheng", ""], ["Yang", "Yezhou", ""], ["Feng", "Zunlei", ""], ["Ye", "Jingwen", ""], ["Yu", "Yizhou", ""], ["Song", "Mingli", ""]]}, {"id": "1705.04288", "submitter": "Hokchhay Tann", "authors": "Hokchhay Tann, Soheil Hashemi, Iris Bahar, Sherief Reda", "title": "Hardware-Software Codesign of Accurate, Multiplier-free Deep Neural\n  Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Deep Neural Networks (DNNs) push the state-of-the-art in many machine\nlearning applications, they often require millions of expensive floating-point\noperations for each input classification. This computation overhead limits the\napplicability of DNNs to low-power, embedded platforms and incurs high cost in\ndata centers. This motivates recent interests in designing low-power,\nlow-latency DNNs based on fixed-point, ternary, or even binary data precision.\nWhile recent works in this area offer promising results, they often lead to\nlarge accuracy drops when compared to the floating-point networks. We propose a\nnovel approach to map floating-point based DNNs to 8-bit dynamic fixed-point\nnetworks with integer power-of-two weights with no change in network\narchitecture. Our dynamic fixed-point DNNs allow different radix points between\nlayers. During inference, power-of-two weights allow multiplications to be\nreplaced with arithmetic shifts, while the 8-bit fixed-point representation\nsimplifies both the buffer and adder design. In addition, we propose a hardware\naccelerator design to achieve low-power, low-latency inference with\ninsignificant degradation in accuracy. Using our custom accelerator design with\nthe CIFAR-10 and ImageNet datasets, we show that our method achieves\nsignificant power and energy savings while increasing the classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:01:44 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Tann", "Hokchhay", ""], ["Hashemi", "Soheil", ""], ["Bahar", "Iris", ""], ["Reda", "Sherief", ""]]}, {"id": "1705.04378", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Enrico Maiorino, Michael C. Kampffmeyer,\n  Antonello Rizzi, Robert Jenssen", "title": "An overview and comparative analysis of Recurrent Neural Networks for\n  Short Term Load Forecasting", "comments": "Springer Briefs in Computer Science (ISBN 978-3-319-70338-1), 2017", "journal-ref": null, "doi": "10.1007/978-3-319-70338-1", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key component in forecasting demand and consumption of resources in a\nsupply network is an accurate prediction of real-valued time series. Indeed,\nboth service interruptions and resource waste can be reduced with the\nimplementation of an effective forecasting system. Significant research has\nthus been devoted to the design and development of methodologies for short term\nload forecasting over the past decades. A class of mathematical models, called\nRecurrent Neural Networks, are nowadays gaining renewed interest among\nresearchers and they are replacing many practical implementation of the\nforecasting systems, previously based on static methods. Despite the undeniable\nexpressive power of these architectures, their recurrent nature complicates\ntheir understanding and poses challenges in the training procedures. Recently,\nnew important families of recurrent architectures have emerged and their\napplicability in the context of load forecasting has not been investigated\ncompletely yet. In this paper we perform a comparative study on the problem of\nShort-Term Load Forecast, by using different classes of state-of-the-art\nRecurrent Neural Networks. We test the reviewed models first on controlled\nsynthetic tasks and then on different real datasets, covering important\npractical cases of study. We provide a general overview of the most important\narchitectures and we define guidelines for configuring the recurrent networks\nto predict real-valued time series.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 21:17:00 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 22:54:01 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Maiorino", "Enrico", ""], ["Kampffmeyer", "Michael C.", ""], ["Rizzi", "Antonello", ""], ["Jenssen", "Robert", ""]]}, {"id": "1705.04536", "submitter": "Jack McKay Fletcher", "authors": "Jack McKay Fletcher and Thomas Wennekers", "title": "A natural approach to studying schema processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Building Block Hypothesis (BBH) states that adaptive systems combine good\npartial solutions (so-called building blocks) to find increasingly better\nsolutions. It is thought that Genetic Algorithms (GAs) implement the BBH.\nHowever, for GAs building blocks are semi-theoretical objects in that they are\nthought only to be implicitly exploited via the selection and crossover\noperations of a GA. In the current work, we discover a mathematical method to\nidentify the complete set of schemata present in a given population of a GA; as\nsuch a natural way to study schema processing (and thus the BBH) is revealed.\nWe demonstrate how this approach can be used both theoretically and\nexperimentally. Theoretically, we show that the search space for good schemata\nis a complete lattice and that each generation samples a complete sub-lattice\nof this search space. In addition, we show that combining schemata can only\nexplore a subset of the search space. Experimentally, we compare how well\ndifferent crossover methods combine building blocks. We find that for most\ncrossover methods approximately 25-35% of building blocks in a generation\nresult from the combination of the previous generation's building blocks. We\nalso find that an increase in the combination of building blocks does not lead\nto an increase in the efficiency of a GA. To complement this article, we\nintroduce an open source Python package called schematax, which allows one to\ncalculate the schemata present in a population using the methods described in\nthis article.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 12:23:27 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Fletcher", "Jack McKay", ""], ["Wennekers", "Thomas", ""]]}, {"id": "1705.04748", "submitter": "Syed Shakib Sarwar", "authors": "Syed Shakib Sarwar, Priyadarshini Panda, Kaushik Roy", "title": "Gabor Filter Assisted Energy Efficient Fast Learning Convolutional\n  Neural Networks", "comments": "Accepted in ISLPED 2017", "journal-ref": "EEE/ACM International Symposium on Low Power Electronics and\n  Design (ISLPED), Taipei, 2017, pp. 1-6", "doi": "10.1109/ISLPED.2017.8009202", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) are being increasingly used in computer\nvision for a wide range of classification and recognition problems. However,\ntraining these large networks demands high computational time and energy\nrequirements; hence, their energy-efficient implementation is of great\ninterest. In this work, we reduce the training complexity of CNNs by replacing\ncertain weight kernels of a CNN with Gabor filters. The convolutional layers\nuse the Gabor filters as fixed weight kernels, which extracts intrinsic\nfeatures, with regular trainable weight kernels. This combination creates a\nbalanced system that gives better training performance in terms of energy and\ntime, compared to the standalone CNN (without any Gabor kernels), in exchange\nfor tolerable accuracy degradation. We show that the accuracy degradation can\nbe mitigated by partially training the Gabor kernels, for a small fraction of\nthe total training cycles. We evaluated the proposed approach on 4 benchmark\napplications. Simple tasks like face detection and character recognition (MNIST\nand TiCH), were implemented using LeNet architecture. While a more complex task\nof object recognition (CIFAR10) was implemented on a state of the art deep CNN\n(Network in Network) architecture. The proposed approach yields 1.31-1.53x\nimprovement in training energy in comparison to conventional CNN\nimplementation. We also obtain improvement up to 1.4x in training time, up to\n2.23x in storage requirements, and up to 2.2x in memory access energy. The\naccuracy degradation suffered by the approximate implementations is within 0-3%\nof the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 20:50:51 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Sarwar", "Syed Shakib", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1705.05231", "submitter": "Qiang Lu", "authors": "Qiang Lu and Kyoung-Dae Kim", "title": "Autonomous and Connected Intersection Crossing Traffic Management using\n  Discrete-Time Occupancies Trajectory", "comments": "34 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a problem of safe and efficient intersection\ncrossing traffic management of autonomous and connected ground traffic. Toward\nthis objective, we propose an algorithm that is called the Discrete-time\noccupancies trajectory based Intersection traffic Coordination Algorithm\n(DICA). We first prove that the basic DICA is deadlock free and also starvation\nfree. Then, we show that the basic DICA has a computational complexity of\n$\\mathcal{O}(n^2 L_m^3)$ where $n$ is the number of vehicles granted to cross\nan intersection and $L_m$ is the maximum length of intersection crossing\nroutes.\n  To improve the overall computational efficiency of the algorithm, the basic\nDICA is enhanced by several computational approaches that are proposed in this\npaper. The enhanced algorithm has the computational complexity of\n$\\mathcal{O}(n^2 L_m \\log_2 L_m)$. The improved computational efficiency of the\nenhanced algorithm is validated through simulation using an open source traffic\nsimulator, called the Simulation of Urban MObility (SUMO). The overall\nthroughput as well as the computational efficiency of the enhanced algorithm\nare also compared with those of an optimized traffic light control.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 13:40:54 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Lu", "Qiang", ""], ["Kim", "Kyoung-Dae", ""]]}, {"id": "1705.05475", "submitter": "Tsung-Han Lin", "authors": "Ping Tak Peter Tang, Tsung-Han Lin, Mike Davies", "title": "Sparse Coding by Spiking Neural Networks: Convergence Theory and\n  Computational Results", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a spiking neural network (SNN), individual neurons operate autonomously\nand only communicate with other neurons sparingly and asynchronously via spike\nsignals. These characteristics render a massively parallel hardware\nimplementation of SNN a potentially powerful computer, albeit a non von Neumann\none. But can one guarantee that a SNN computer solves some important problems\nreliably? In this paper, we formulate a mathematical model of one SNN that can\nbe configured for a sparse coding problem for feature extraction. With a\nmoderate but well-defined assumption, we prove that the SNN indeed solves\nsparse coding. To the best of our knowledge, this is the first rigorous result\nof this kind.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 23:06:34 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Tang", "Ping Tak Peter", ""], ["Lin", "Tsung-Han", ""], ["Davies", "Mike", ""]]}, {"id": "1705.05487", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Peter Szolovits", "title": "NeuroNER: an easy-to-use program for named-entity recognition based on\n  neural networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named-entity recognition (NER) aims at identifying entities of interest in a\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\nexisting NER systems. However, ANNs remain challenging to use for non-expert\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\nrecognition tool based on ANNs. Users can annotate entities using a graphical\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 00:03:19 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.05502", "submitter": "David Rolnick", "authors": "David Rolnick (MIT), Max Tegmark (MIT)", "title": "The power of deeper networks for expressing natural functions", "comments": "Replaced to match version published at ICLR 2018. 14 pages, 2 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that neural networks are universal approximators, but that\ndeeper networks tend in practice to be more powerful than shallower ones. We\nshed light on this by proving that the total number of neurons $m$ required to\napproximate natural classes of multivariate polynomials of $n$ variables grows\nonly linearly with $n$ for deep neural networks, but grows exponentially when\nmerely a single hidden layer is allowed. We also provide evidence that when the\nnumber of hidden layers is increased from $1$ to $k$, the neuron requirement\ngrows exponentially not with $n$ but with $n^{1/k}$, suggesting that the\nminimum number of layers required for practical expressibility grows only\nlogarithmically with $n$.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 02:02:24 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 17:52:03 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Rolnick", "David", "", "MIT"], ["Tegmark", "Max", "", "MIT"]]}, {"id": "1705.05584", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Ajith Abraham, V\\'aclav Sn\\'a\\v{s}el", "title": "Metaheuristic Design of Feedforward Neural Networks: A Review of Two\n  Decades of Research", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence Volume 60,\n  April 2017, Pages 97 to 116", "doi": "10.1016/j.engappai.2017.01.013", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, the feedforward neural network (FNN) optimization\nhas been a key interest among the researchers and practitioners of multiple\ndisciplines. The FNN optimization is often viewed from the various\nperspectives: the optimization of weights, network architecture, activation\nnodes, learning parameters, learning environment, etc. Researchers adopted such\ndifferent viewpoints mainly to improve the FNN's generalization ability. The\ngradient-descent algorithm such as backpropagation has been widely applied to\noptimize the FNNs. Its success is evident from the FNN's application to\nnumerous real-world problems. However, due to the limitations of the\ngradient-based optimization methods, the metaheuristic algorithms including the\nevolutionary algorithms, swarm intelligence, etc., are still being widely\nexplored by the researchers aiming to obtain generalized FNN for a given\nproblem. This article attempts to summarize a broad spectrum of FNN\noptimization methodologies including conventional and metaheuristic approaches.\nThis article also tries to connect various research directions emerged out of\nthe FNN optimization practices, such as evolving neural network (NN),\ncooperative coevolution NN, complex-valued NN, deep learning, extreme learning\nmachine, quantum NN, etc. Additionally, it provides interesting research\nchallenges for future research to cope-up with the present information\nprocessing era.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:29:00 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Abraham", "Ajith", ""], ["Sn\u00e1\u0161el", "V\u00e1clav", ""]]}, {"id": "1705.05592", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Ajith Abraham, V\\'aclav Sn\\'a\\v{s}el", "title": "Ensemble of heterogeneous flexible neural trees using multiobjective\n  genetic programming", "comments": null, "journal-ref": "Applied Soft Computing, 2017, Volume 52 Pages 909 to 924", "doi": "10.1016/j.asoc.2016.09.035", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are inherently multiobjective in nature, where\napproximation error minimization and model's complexity simplification are two\nconflicting objectives. We proposed a multiobjective genetic programming (MOGP)\nfor creating a heterogeneous flexible neural tree (HFNT), tree-like flexible\nfeedforward neural network model. The functional heterogeneity in neural tree\nnodes was introduced to capture a better insight of data during learning\nbecause each input in a dataset possess different features. MOGP guided an\ninitial HFNT population towards Pareto-optimal solutions, where the final\npopulation was used for making an ensemble system. A diversity index measure\nalong with approximation error and complexity was introduced to maintain\ndiversity among the candidates in the population. Hence, the ensemble was\ncreated by using accurate, structurally simple, and diverse candidates from\nMOGP final population. Differential evolution algorithm was applied to\nfine-tune the underlying parameters of the selected candidates. A comprehensive\ntest over classification, regression, and time-series datasets proved the\nefficiency of the proposed algorithm over other available prediction methods.\nMoreover, the heterogeneous creation of HFNT proved to be efficient in making\nensemble system from the final population.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:40:42 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Abraham", "Ajith", ""], ["Sn\u00e1\u0161el", "V\u00e1clav", ""]]}, {"id": "1705.05981", "submitter": "Xin Zhou", "authors": "Qingxing Dong, Xin Zhou", "title": "A Continuous Opinion Dynamic Model in Co-evolving Networks--A Novel\n  Group Decision Approach", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion polarization is a ubiquitous phenomenon in opinion dynamics. In\ncontrast to the traditional consensus oriented group decision making (GDM)\nframework, this paper proposes a framework with the co-evolution of both\nopinions and relationship networks to improve the potential consensus level of\na group and help the group reach a stable state. Taking the bound of confidence\nand the degree of individual's persistence into consideration, the evolution of\nthe opinion is driven by the relationship among the group. Meanwhile, the\nantagonism or cooperation of individuals presented by the network topology also\nevolve according to the dynamic opinion distances. Opinions are convergent and\nthe stable state will be reached in this co-evolution mechanism. We further\nexplored this framework through simulation experiments. The simulation results\nverify the influence of the level of persistence on the time cost and indicate\nthe influence of group size, the initial topology of networks and the bound of\nconfidence on the number of opinion clusters.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 02:03:39 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Dong", "Qingxing", ""], ["Zhou", "Xin", ""]]}, {"id": "1705.06057", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (Palaiseau, OBELIX), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Joint Learning from Earth Observation and OpenStreetMap Data to Get\n  Faster Better Semantic Maps", "comments": null, "journal-ref": "EARTHVISION 2017 IEEE/ISPRS CVPR Workshop. Large Scale Computer\n  Vision for Remote Sensing Imagery, Jul 2017, Honolulu, United States. 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the use of OpenStreetMap data for semantic\nlabeling of Earth Observation images. Deep neural networks have been used in\nthe past for remote sensing data classification from various sensors, including\nmultispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has\nalready been used as ground truth data for training such networks, this\nabundant data source remains rarely exploited as an input information layer. In\nthis paper, we study different use cases and deep network architectures to\nleverage OpenStreetMap data for semantic labeling of aerial and satellite\nimages. Especially , we look into fusion based architectures and coarse-to-fine\nsegmentation to include the OpenStreetMap layer into multispectral-based deep\nfully convolutional networks. We illustrate how these methods can be\nsuccessfully used on two public datasets: ISPRS Potsdam and DFC2017. We show\nthat OpenStreetMap data can efficiently be integrated into the vision-based\ndeep learning models and that it significantly improves both the accuracy\nperformance and the convergence speed of the networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 09:07:08 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Audebert", "Nicolas", "", "Palaiseau, OBELIX"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1705.06270", "submitter": "Ankur Sinha PhD", "authors": "Ankur Sinha, Pekka Malo, Kalyanmoy Deb", "title": "A Review on Bilevel Optimization: From Classical to Evolutionary\n  Approaches and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilevel optimization is defined as a mathematical program, where an\noptimization problem contains another optimization problem as a constraint.\nThese problems have received significant attention from the mathematical\nprogramming community. Only limited work exists on bilevel problems using\nevolutionary computation techniques; however, recently there has been an\nincreasing interest due to the proliferation of practical applications and the\npotential of evolutionary algorithms in tackling these problems. This paper\nprovides a comprehensive review on bilevel optimization from the basic\nprinciples to solution strategies; both classical and evolutionary. A number of\npotential application problems are also discussed. To offer the readers\ninsights on the prominent developments in the field of bilevel optimization, we\nhave performed an automated text-analysis of an extended list of papers\npublished on bilevel optimization to date. This paper should motivate\nevolutionary computation researchers to pay more attention to this practical\nyet challenging area.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:43:13 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 06:19:13 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Sinha", "Ankur", ""], ["Malo", "Pekka", ""], ["Deb", "Kalyanmoy", ""]]}, {"id": "1705.06273", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "Transfer Learning for Named-Entity Recognition with Neural Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for named-entity recognition (NER). In order to achieve high\nperformances, ANNs need to be trained on a large labeled dataset. However,\nlabels might be difficult to obtain for the dataset on which the user wants to\nperform NER: label scarcity is particularly pronounced for patient note\nde-identification, which is an instance of NER. In this work, we analyze to\nwhat extent transfer learning may address this issue. In particular, we\ndemonstrate that transferring an ANN model trained on a large labeled dataset\nto another dataset with a limited number of labels improves upon the\nstate-of-the-art results on two different datasets for patient note\nde-identification.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:45:15 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.06614", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Karl Friston", "title": "Approximate Bayesian inference as a gauge theory", "comments": "Extended version published in PLoS Biology, ICML 2017 Computational\n  Biology Workshop (spotlight presentation)", "journal-ref": null, "doi": "10.1371/journal.pbio.1002400", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a published paper [Sengupta, 2016], we have proposed that the brain (and\nother self-organized biological and artificial systems) can be characterized\nvia the mathematical apparatus of a gauge theory. The picture that emerges from\nthis approach suggests that any biological system (from a neuron to an\norganism) can be cast as resolving uncertainty about its external milieu,\neither by changing its internal states or its relationship to the environment.\nUsing formal arguments, we have shown that a gauge theory for neuronal dynamics\n-- based on approximate Bayesian inference -- has the potential to shed new\nlight on phenomena that have thus far eluded a formal description, such as\nattention and the link between action and perception. Here, we describe the\ntechnical apparatus that enables such a variational inference on manifolds.\nParticularly, the novel contribution of this paper is an algorithm that utlizes\na Schild's ladder for parallel transport of sufficient statistics (means,\ncovariances, etc.) on a statistical manifold.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 13:13:28 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:18:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sengupta", "Biswa", ""], ["Friston", "Karl", ""]]}, {"id": "1705.06693", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov, Tobias Glasmachers, Hans-Georg Beyer", "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a popular\nmethod to deal with nonconvex and/or stochastic optimization problems when the\ngradient information is not available. Being based on the CMA-ES, the recently\nproposed Matrix Adaptation Evolution Strategy (MA-ES) provides a rather\nsurprising result that the covariance matrix and all associated operations\n(e.g., potentially unstable eigendecomposition) can be replaced in the CMA-ES\nby a updated transformation matrix without any loss of performance. In order to\nfurther simplify MA-ES and reduce its $\\mathcal{O}\\big(n^2\\big)$ time and\nstorage complexity to $\\mathcal{O}\\big(n\\log(n)\\big)$, we present the\nLimited-Memory Matrix Adaptation Evolution Strategy (LM-MA-ES) for efficient\nzeroth order large-scale optimization. The algorithm demonstrates\nstate-of-the-art performance on a set of established large-scale benchmarks. We\nexplore the algorithm on the problem of generating adversarial inputs for a\n(non-smooth) random forest classifier, demonstrating a surprising vulnerability\nof the classifier.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 16:53:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Glasmachers", "Tobias", ""], ["Beyer", "Hans-Georg", ""]]}, {"id": "1705.06778", "submitter": "Martin Mundt", "authors": "Martin Mundt, Tobias Weis, Kishore Konda and Visvanathan Ramesh", "title": "Building effective deep neural network architectures one feature at a\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful training of convolutional neural networks is often associated with\nsufficiently deep architectures composed of high amounts of features. These\nnetworks typically rely on a variety of regularization and pruning techniques\nto converge to less redundant states. We introduce a novel bottom-up approach\nto expand representations in fixed-depth architectures. These architectures\nstart from just a single feature per layer and greedily increase width of\nindividual layers to attain effective representational capacities needed for a\nspecific task. While network growth can rely on a family of metrics, we propose\na computationally efficient version based on feature time evolution and\ndemonstrate its potency in determining feature importance and a networks'\neffective capacity. We demonstrate how automatically expanded architectures\nconverge to similar topologies that benefit from lesser amount of parameters or\nimproved accuracy and exhibit systematic correspondence in representational\ncomplexity with the specified task. In contrast to conventional design patterns\nwith a typical monotonic increase in the amount of features with increased\ndepth, we observe that CNNs perform better when there is more learnable\nparameters in intermediate, with falloffs to earlier and later layers.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:40:37 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 21:59:52 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Mundt", "Martin", ""], ["Weis", "Tobias", ""], ["Konda", "Kishore", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1705.06820", "submitter": "Hongyang Gao", "authors": "Hongyang Gao and Hao Yuan and Zhengyang Wang and Shuiwang Ji", "title": "Pixel Deconvolutional Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolutional layers have been widely used in a variety of deep models for\nup-sampling, including encoder-decoder networks for semantic segmentation and\ndeep generative models for unsupervised learning. One of the key limitations of\ndeconvolutional operations is that they result in the so-called checkerboard\nproblem. This is caused by the fact that no direct relationship exists among\nadjacent pixels on the output feature map. To address this problem, we propose\nthe pixel deconvolutional layer (PixelDCL) to establish direct relationships\namong adjacent pixels on the up-sampled feature map. Our method is based on a\nfresh interpretation of the regular deconvolution operation. The resulting\nPixelDCL can be used to replace any deconvolutional layer in a plug-and-play\nmanner without compromising the fully trainable capabilities of original\nmodels. The proposed PixelDCL may result in slight decrease in efficiency, but\nthis can be overcome by an implementation trick. Experimental results on\nsemantic segmentation demonstrate that PixelDCL can consider spatial features\nsuch as edges and shapes and yields more accurate segmentation outputs than\ndeconvolutional layers. When used in image generation tasks, our PixelDCL can\nlargely overcome the checkerboard problem suffered by regular deconvolution\noperations.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:31:26 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 17:17:07 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 18:17:31 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 02:53:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gao", "Hongyang", ""], ["Yuan", "Hao", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06821", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Hao Yuan, Shuiwang Ji", "title": "Spatial Variational Auto-Encoding via Matrix-Variate Normal\n  Distributions", "comments": "Accepted by SDM2019. Code is publicly available at\n  https://github.com/divelab/svae", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of variational auto-encoders (VAEs) resembles that of\ntraditional auto-encoder models in which spatial information is supposed to be\nexplicitly encoded in the latent space. However, the latent variables in VAEs\nare vectors, which can be interpreted as multiple feature maps of size 1x1.\nSuch representations can only convey spatial information implicitly when\ncoupled with powerful decoders. In this work, we propose spatial VAEs that use\nfeature maps of larger size as latent variables to explicitly capture spatial\ninformation. This is achieved by allowing the latent variables to be sampled\nfrom matrix-variate normal (MVN) distributions whose parameters are computed\nfrom the encoder network. To increase dependencies among locations on latent\nfeature maps and reduce the number of parameters, we further propose spatial\nVAEs via low-rank MVN distributions. Experimental results show that the\nproposed spatial VAEs outperform original VAEs in capturing rich structural and\nspatial information.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:32:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 17:48:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Zhengyang", ""], ["Yuan", "Hao", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06824", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Shuiwang Ji", "title": "Learning Convolutional Text Representations for Visual Question\n  Answering", "comments": "Conference paper at SDM 2018. https://github.com/divelab/svae", "journal-ref": "In proceedings of the 2018 SIAM International Conference on Data\n  Mining (pp. 594-602). 2018", "doi": "10.1137/1.9781611975321.67", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:51:44 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 17:38:50 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06942", "submitter": "Akhilesh Jaiswal", "authors": "Akhilesh Jaiswal, Amogh Agrawal, Priyadarshini Panda, Kaushik Roy", "title": "Voltage-Driven Domain-Wall Motion based Neuro-Synaptic Devices for\n  Dynamic On-line Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional von-Neumann computing models have achieved remarkable feats for\nthe past few decades. However, they fail to deliver the required efficiency for\ncertain basic tasks like image and speech recognition when compared to\nbiological systems. As such, taking cues from biological systems, novel\ncomputing paradigms are being explored for efficient hardware implementations\nof recognition/classification tasks. The basic building blocks of such\nneuromorphic systems are neurons and synapses. Towards that end, we propose a\nleaky-integrate-fire (LIF) neuron and a programmable non-volatile synapse using\ndomain wall motion induced by magneto-electric effect. Due to a strong elastic\npinning between the ferro-magnetic domain wall (FM-DW) and the underlying\nferro-electric domain wall (FE-DW), the FM-DW gets dragged by the FE-DW on\napplication of a voltage pulse. The fact that FE materials are insulators\nallows for pure voltage-driven FM-DW motion, which in turn can be used to mimic\nthe behaviors of biological spiking neurons and synapses. The voltage driven\nnature of the proposed devices allows energy-efficient operation. A detailed\ndevice to system level simulation framework based on micromagnetic simulations\nhas been developed to analyze the feasibility of the proposed neuro-synaptic\ndevices. We also demonstrate that the energy-efficient voltage-controlled\nbehavior of the proposed devices make them suitable for dynamic on-line and\nlifelong learning in spiking neural networks (SNNs).\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 11:37:04 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 03:47:06 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Jaiswal", "Akhilesh", ""], ["Agrawal", "Amogh", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1705.06963", "submitter": "Catherine Schuman", "authors": "Catherine D. Schuman, Thomas E. Potok, Robert M. Patton, J. Douglas\n  Birdwell, Mark E. Dean, Garrett S. Rose, and James S. Plank", "title": "A Survey of Neuromorphic Computing and Neural Networks in Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing has come to refer to a variety of brain-inspired\ncomputers, devices, and models that contrast the pervasive von Neumann computer\narchitecture. This biologically inspired approach has created highly connected\nsynthetic neurons and synapses that can be used to model neuroscience theories\nas well as solve challenging machine learning problems. The promise of the\ntechnology is to create a brain-like ability to learn and adapt, but the\ntechnical challenges are significant, starting with an accurate neuroscience\nmodel of how the brain works, to finding materials and engineering\nbreakthroughs to build devices to support these models, to creating a\nprogramming framework so the systems can learn, to creating applications with\nbrain-like capabilities. In this work, we provide a comprehensive survey of the\nresearch and motivations for neuromorphic computing over its history. We begin\nwith a 35-year review of the motivations and drivers of neuromorphic computing,\nthen look at the major research areas of the field, which we define as\nneuro-inspired models, algorithms and learning approaches, hardware and\ndevices, supporting systems, and finally applications. We conclude with a broad\ndiscussion on the major research topics that need to be addressed in the coming\nyears to see the promise of neuromorphic computing fulfilled. The goals of this\nwork are to provide an exhaustive review of the research conducted in\nneuromorphic computing since the inception of the term, and to motivate further\nwork by illuminating gaps in the field where new research is needed.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 12:41:32 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Schuman", "Catherine D.", ""], ["Potok", "Thomas E.", ""], ["Patton", "Robert M.", ""], ["Birdwell", "J. Douglas", ""], ["Dean", "Mark E.", ""], ["Rose", "Garrett S.", ""], ["Plank", "James S.", ""]]}, {"id": "1705.06966", "submitter": "Carlos Garcia Cordero", "authors": "Carlos Garcia Cordero", "title": "Parameter Adaptation and Criticality in Particle Swarm Optimization", "comments": "70 pages, MSc. Thesis in Artificial Intelligence, University of\n  Edinburgh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generality is one of the main advantages of heuristic algorithms, as such,\nmultiple parameters are exposed to the user with the objective of allowing them\nto shape the algorithms to their specific needs. Parameter selection,\ntherefore, becomes an intrinsic problem of every heuristic algorithm. Selecting\ngood parameter values relies not only on knowledge related to the problem at\nhand, but to the algorithms themselves. This research explores the usage of\nself-organized criticality to reduce user interaction in the process of\nselecting suitable parameters for particle swarm optimization (PSO) heuristics.\nA particle swarm variant (named Adaptive PSO) with self-organized criticality\nis developed and benchmarked against the standard PSO. Criticality is observed\nin the dynamic behaviour of this swarm and excellent results are observed in\nthe long run. In contrast with the standard PSO, the Adaptive PSO does not\nstagnate at any point in time, balancing the concepts of exploration and\nexploitation better. A software platform for experimenting with particle\nswarms, called PSO Laboratory, is also developed. This software is used to test\nthe standard PSO as well as all other PSO variants developed in the process of\ncreating the Adaptive PSO. As the software is intended to be of aid to future\nand related research, special attention has been put in the development of a\nfriendly graphical user interface. Particle swarms are executed in real time,\nallowing users to experiment by changing parameters on-the-fly.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 12:53:07 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Cordero", "Carlos Garcia", ""]]}, {"id": "1705.07149", "submitter": "Tsung-Han Lin", "authors": "Tsung-Han Lin", "title": "Local Information with Feedback Perturbation Suffices for Dictionary\n  Learning in Neural Circuits", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the sparse coding principle can successfully model information\nprocessing in sensory neural systems, it remains unclear how learning can be\naccomplished under neural architectural constraints. Feasible learning rules\nmust rely solely on synaptically local information in order to be implemented\non spatially distributed neurons. We describe a neural network with spiking\nneurons that can address the aforementioned fundamental challenge and solve the\nL1-minimizing dictionary learning problem, representing the first model able to\ndo so. Our major innovation is to introduce feedback synapses to create a\npathway to turn the seemingly non-local information into local ones. The\nresulting network encodes the error signal needed for learning as the change of\nnetwork steady states caused by feedback, and operates akin to the classical\nstochastic gradient descent method.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:06:27 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Lin", "Tsung-Han", ""]]}, {"id": "1705.07175", "submitter": "Fabrizio Pedersoli", "authors": "Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi", "title": "Espresso: Efficient Forward Propagation for BCNNs", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications scenarios for which the computational performance\nand memory footprint of the prediction phase of Deep Neural Networks (DNNs)\nneeds to be optimized. Binary Neural Networks (BDNNs) have been shown to be an\neffective way of achieving this objective. In this paper, we show how\nConvolutional Neural Networks (CNNs) can be implemented using binary\nrepresentations. Espresso is a compact, yet powerful library written in C/CUDA\nthat features all the functionalities required for the forward propagation of\nCNNs, in a binary file less than 400KB, without any external dependencies.\nAlthough it is mainly designed to take advantage of massive GPU parallelism,\nEspresso also provides an equivalent CPU implementation for CNNs. Espresso\nprovides special convolutional and dense layers for BCNNs, leveraging\nbit-packing and bit-wise computations for efficient execution. These techniques\nprovide a speed-up of matrix-multiplication routines, and at the same time,\nreduce memory usage when storing parameters and activations. We experimentally\nshow that Espresso is significantly faster than existing implementations of\noptimized binary neural networks ($\\approx$ 2 orders of magnitude). Espresso is\nreleased under the Apache 2.0 license and is available at\nhttp://github.com/fpeder/espresso.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:29:42 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 17:59:16 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Pedersoli", "Fabrizio", ""], ["Tzanetakis", "George", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1705.07215", "submitter": "Naveen Kodali", "authors": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "title": "On Convergence and Stability of GANs", "comments": "Analysis of convergence and mode collapse by studying GAN training\n  process as regret minimization. Some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GT cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose studying GAN training dynamics as regret minimization, which is in\ncontrast to the popular view that there is consistent minimization of a\ndivergence between real and generated distributions. We analyze the convergence\nof GAN training from this new point of view to understand why mode collapse\nhappens. We hypothesize the existence of undesirable local equilibria in this\nnon-convex game to be responsible for mode collapse. We observe that these\nlocal equilibria often exhibit sharp gradients of the discriminator function\naround some real data points. We demonstrate that these degenerate local\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\nthat DRAGAN enables faster training, achieves improved stability with fewer\nmode collapses, and leads to generator networks with better modeling\nperformance across a variety of architectures and objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:41:56 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 15:13:01 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 00:51:40 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 21:47:51 GMT"}, {"version": "v5", "created": "Sun, 10 Dec 2017 15:24:13 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Kodali", "Naveen", ""], ["Abernethy", "Jacob", ""], ["Hays", "James", ""], ["Kira", "Zsolt", ""]]}, {"id": "1705.07241", "submitter": "Roby Velez", "authors": "Roby Velez and Jeff Clune", "title": "Diffusion-based neuromodulation can eliminate catastrophic forgetting in\n  simple neural networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0187736", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-term goal of AI is to produce agents that can learn a diversity of\nskills throughout their lifetimes and continuously improve those skills via\nexperience. A longstanding obstacle towards that goal is catastrophic\nforgetting, which is when learning new information erases previously learned\ninformation. Catastrophic forgetting occurs in artificial neural networks\n(ANNs), which have fueled most recent advances in AI. A recent paper proposed\nthat catastrophic forgetting in ANNs can be reduced by promoting modularity,\nwhich can limit forgetting by isolating task information to specific clusters\nof nodes and connections (functional modules). While the prior work did show\nthat modular ANNs suffered less from catastrophic forgetting, it was not able\nto produce ANNs that possessed task-specific functional modules, thereby\nleaving the main theory regarding modularity and forgetting untested. We\nintroduce diffusion-based neuromodulation, which simulates the release of\ndiffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up\nor down regulate) learning in a spatial region. On the simple diagnostic\nproblem from the prior work, diffusion-based neuromodulation 1) induces\ntask-specific learning in groups of nodes and connections (task-specific\nlocalized learning), which 2) produces functional modules for each subtask, and\n3) yields higher performance by eliminating catastrophic forgetting. Overall,\nour results suggest that diffusion-based neuromodulation promotes task-specific\nlocalized learning and functional modularity, which can help solve the\nchallenging, but important problem of catastrophic forgetting.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 01:04:33 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 22:45:17 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 21:06:52 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Velez", "Roby", ""], ["Clune", "Jeff", ""]]}, {"id": "1705.07262", "submitter": "Daniel Hein", "authors": "Daniel Hein, Steffen Udluft, Michel Tokic, Alexander Hentschel, Thomas\n  A. Runkler, Volkmar Sterzing", "title": "Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences", "comments": null, "journal-ref": "2017 International Joint Conference on Neural Networks (IJCNN),\n  Anchorage, AK, 2017, pp. 4214-4221", "doi": "10.1109/IJCNN.2017.7966389", "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:31:52 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 15:34:21 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Hein", "Daniel", ""], ["Udluft", "Steffen", ""], ["Tokic", "Michel", ""], ["Hentschel", "Alexander", ""], ["Runkler", "Thomas A.", ""], ["Sterzing", "Volkmar", ""]]}, {"id": "1705.07278", "submitter": "Biswa Sengupta", "authors": "Gerald K Cooray and Richard Rosch and Torsten Baldeweg and Louis\n  Lemieux and Karl Friston and Biswa Sengupta", "title": "Bayesian Belief Updating of Spatiotemporal Seizure Dynamics", "comments": "ICML 2017 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epileptic seizure activity shows complicated dynamics in both space and time.\nTo understand the evolution and propagation of seizures spatially extended sets\nof data need to be analysed. We have previously described an efficient\nfiltering scheme using variational Laplace that can be used in the Dynamic\nCausal Modelling (DCM) framework [Friston, 2003] to estimate the temporal\ndynamics of seizures recorded using either invasive or non-invasive electrical\nrecordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial\ndifferential equation -- in contrast to the ordinary differential equation used\nin our previous work on temporal estimation of seizure dynamics [Cooray, 2016].\nWe provide the requisite theoretical background for the method and test the\nensuing scheme on simulated seizure activity data and empirical invasive ECoG\ndata. The method provides a framework to assimilate the spatial and temporal\ndynamics of seizure activity, an aspect of great physiological and clinical\nimportance.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 08:06:05 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:27:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cooray", "Gerald K", ""], ["Rosch", "Richard", ""], ["Baldeweg", "Torsten", ""], ["Lemieux", "Louis", ""], ["Friston", "Karl", ""], ["Sengupta", "Biswa", ""]]}, {"id": "1705.07492", "submitter": "Hakan Ayral", "authors": "Hakan Ayral and Song\\\"ul Albayrak", "title": "Parallel and in-process compilation of individuals for genetic\n  programming on GPU", "comments": "Submitted to PeerJ Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three approaches to implement genetic programming on GPU hardware are\ncompilation, interpretation and direct generation of machine code. The compiled\napproach is known to have a prohibitive overhead compared to other two. This\npaper investigates methods to accelerate compilation of individuals for genetic\nprogramming on GPU hardware. We apply in-process compilation to minimize the\ncompilation overhead at each generation; and we investigate ways to parallelize\nin-process compilation. In-process compilation doesn't lend itself to trivial\nparallelization with threads; we propose a multiprocess parallelization using\nmemory sharing and operating systems interprocess communication primitives.\nWith parallelized compilation we achieve further reductions on compilation\noverhead. Another contribution of this work is the code framework we built in\nC# for the experiments. The framework makes it possible to build arbitrary\ngrammatical genetic programming experiments that run on GPU with minimal extra\ncoding effort, and is available as open source.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 19:19:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ayral", "Hakan", ""], ["Albayrak", "Song\u00fcl", ""]]}, {"id": "1705.07565", "submitter": "Xin Dong", "authors": "Xin Dong, Shangyu Chen, Sinno Jialin Pan", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain\n  Surgeon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to develop slim and accurate deep neural networks has become crucial for\nreal- world applications, especially for those employed in embedded systems.\nThough previous work along this research line has shown some promising results,\nmost existing methods either fail to significantly compress a well-trained deep\nnetwork or require a heavy retraining process for the pruned deep network to\nre-boost its prediction performance. In this paper, we propose a new layer-wise\npruning method for deep neural networks. In our proposed method, parameters of\neach individual layer are pruned independently based on second order\nderivatives of a layer-wise error function with respect to the corresponding\nparameters. We prove that the final prediction performance drop after pruning\nis bounded by a linear combination of the reconstructed errors caused at each\nlayer. Therefore, there is a guarantee that one only needs to perform a light\nretraining process on the pruned network to resume its original prediction\nperformance. We conduct extensive experiments on benchmark datasets to\ndemonstrate the effectiveness of our pruning method compared with several\nstate-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:54:37 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 23:50:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Dong", "Xin", ""], ["Chen", "Shangyu", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1705.07706", "submitter": "Armand Vilalta", "authors": "Dario Garcia-Gasulla, Armand Vilalta, Ferran Par\\'es, Jonatan Moreno,\n  Eduard Ayguad\\'e, Jesus Labarta, Ulises Cort\\'es and Toyotaro Suzumura", "title": "An Out-of-the-box Full-network Embedding for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning for feature extraction can be used to exploit deep\nrepresentations in contexts where there is very few training data, where there\nare limited computational resources, or when tuning the hyper-parameters needed\nfor training is not an option. While previous contributions to feature\nextraction propose embeddings based on a single layer of the network, in this\npaper we propose a full-network embedding which successfully integrates\nconvolutional and fully connected features, coming from all layers of a deep\nconvolutional neural network. To do so, the embedding normalizes features in\nthe context of the problem, and discretizes their values to reduce noise and\nregularize the embedding space. Significantly, this also reduces the\ncomputational cost of processing the resultant representations. The proposed\nmethod is shown to outperform single layer embeddings on several image\nclassification tasks, while also being more robust to the choice of the\npre-trained model used for obtaining the initial features. The performance gap\nin classification accuracy between thoroughly tuned solutions and the\nfull-network embedding is also reduced, which makes of the proposed approach a\ncompetitive solution for a large set of applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:14:11 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Vilalta", "Armand", ""], ["Par\u00e9s", "Ferran", ""], ["Moreno", "Jonatan", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jesus", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1705.07755", "submitter": "Antonio Jose Jimeno Yepes", "authors": "Antonio Jimeno Yepes, Jianbin Tang, Benjamin Scott Mashford", "title": "Improving classification accuracy of feedforward neural networks for\n  spiking neuromorphic chips", "comments": "IJCAI-2017. arXiv admin note: text overlap with arXiv:1605.07740", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) achieve human level performance in many image\nanalytics tasks but DNNs are mostly deployed to GPU platforms that consume a\nconsiderable amount of power. New hardware platforms using lower precision\narithmetic achieve drastic reductions in power consumption. More recently,\nbrain-inspired spiking neuromorphic chips have achieved even lower power\nconsumption, on the order of milliwatts, while still offering real-time\nprocessing.\n  However, for deploying DNNs to energy efficient neuromorphic chips the\nincompatibility between continuous neurons and synaptic weights of traditional\nDNNs, discrete spiking neurons and synapses of neuromorphic chips need to be\novercome. Previous work has achieved this by training a network to learn\ncontinuous probabilities, before it is deployed to a neuromorphic architecture,\nsuch as IBM TrueNorth Neurosynaptic System, by random sampling these\nprobabilities.\n  The main contribution of this paper is a new learning algorithm that learns a\nTrueNorth configuration ready for deployment. We achieve this by training\ndirectly a binary hardware crossbar that accommodates the TrueNorth axon\nconfiguration constrains and we propose a different neuron model.\n  Results of our approach trained on electroencephalogram (EEG) data show a\nsignificant improvement with previous work (76% vs 86% accuracy) while\nmaintaining state of the art performance on the MNIST handwritten data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 12:59:14 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yepes", "Antonio Jimeno", ""], ["Tang", "Jianbin", ""], ["Mashford", "Benjamin Scott", ""]]}, {"id": "1705.07877", "submitter": "Chen Chen", "authors": "Chen Chen, Changtong Luo, Zonglin Jiang", "title": "Block building programming for symbolic regression", "comments": "Accepted for publication in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression that aims to detect underlying data-driven models has\nbecome increasingly important for industrial data analysis. For most existing\nalgorithms such as genetic programming (GP), the convergence speed might be too\nslow for large-scale problems with a large number of variables. This situation\nmay become even worse with increasing problem size. The aforementioned\ndifficulty makes symbolic regression limited in practical applications.\nFortunately, in many engineering problems, the independent variables in target\nmodels are separable or partially separable. This feature inspires us to\ndevelop a new approach, block building programming (BBP). BBP divides the\noriginal target function into several blocks, and further into factors. The\nfactors are then modeled by an optimization engine (e.g. GP). Under such\ncircumstances, BBP can make large reductions to the search space. The partition\nof separability is based on a special method, block and factor detection. Two\ndifferent optimization engines are applied to test the performance of BBP on a\nset of symbolic regression problems. Numerical results show that BBP has a good\ncapability of structure and coefficient optimization with high computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:42:10 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 15:50:09 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 14:57:15 GMT"}, {"version": "v4", "created": "Sat, 28 Oct 2017 15:19:37 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Chen", "Chen", ""], ["Luo", "Changtong", ""], ["Jiang", "Zonglin", ""]]}, {"id": "1705.07878", "submitter": "Wei Wen", "authors": "Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai\n  Li", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep\n  Learning", "comments": "NIPS 2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High network communication cost for synchronizing gradients and parameters is\nthe well-known bottleneck of distributed training. In this work, we propose\nTernGrad that uses ternary gradients to accelerate distributed deep learning in\ndata parallelism. Our approach requires only three numerical levels {-1,0,1},\nwhich can aggressively reduce the communication time. We mathematically prove\nthe convergence of TernGrad under the assumption of a bound on gradients.\nGuided by the bound, we propose layer-wise ternarizing and gradient clipping to\nimprove its convergence. Our experiments show that applying TernGrad on AlexNet\ndoes not incur any accuracy loss and can even improve accuracy. The accuracy\nloss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a\nperformance model is proposed to study the scalability of TernGrad. Experiments\nshow significant speed gains for various deep neural networks. Our source code\nis available.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:42:15 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 06:41:05 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 23:49:08 GMT"}, {"version": "v4", "created": "Mon, 18 Sep 2017 16:21:51 GMT"}, {"version": "v5", "created": "Tue, 31 Oct 2017 16:36:41 GMT"}, {"version": "v6", "created": "Fri, 29 Dec 2017 02:51:48 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Wen", "Wei", ""], ["Xu", "Cong", ""], ["Yan", "Feng", ""], ["Wu", "Chunpeng", ""], ["Wang", "Yandan", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1705.07904", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Akshay Balsubramani, Julian McAuley", "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial\n  Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for training generative adversarial networks that\njointly learns latent codes for both identities (e.g. individual humans) and\nobservations (e.g. specific photographs). By fixing the identity portion of the\nlatent codes, we can generate diverse images of the same subject, and by fixing\nthe observation portion, we can traverse the manifold of subjects while\nmaintaining contingent aspects such as lighting and pose. Our algorithm\nfeatures a pairwise training scheme in which each sample from the generator\nconsists of two images with a common identity code. Corresponding samples from\nthe real dataset consist of two distinct photographs of the same subject. In\norder to fool the discriminator, the generator must produce pairs that are\nphotorealistic, distinct, and appear to depict the same individual. We augment\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\npairwise training. Experiments with human judges and an off-the-shelf face\nverification system demonstrate our algorithm's ability to generate convincing,\nidentity-matched photographs.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:00:02 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:00:05 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:36:33 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["Balsubramani", "Akshay", ""], ["McAuley", "Julian", ""]]}, {"id": "1705.07962", "submitter": "Tony Beltramelli", "authors": "Tony Beltramelli", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transforming a graphical user interface screenshot created by a designer into\ncomputer code is a typical task conducted by a developer in order to build\ncustomized software, websites, and mobile applications. In this paper, we show\nthat deep learning methods can be leveraged to train a model end-to-end to\nautomatically generate code from a single input image with over 77% of accuracy\nfor three different platforms (i.e. iOS, Android and web-based technologies).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:32:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 11:27:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Beltramelli", "Tony", ""]]}, {"id": "1705.08014", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, O. Murat Onen, Wilfried Haensch", "title": "Training Deep Convolutional Neural Networks with Resistive Cross-Point\n  Devices", "comments": "22 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work we have detailed the requirements to obtain a maximal\nperformance benefit by implementing fully connected deep neural networks (DNN)\nin form of arrays of resistive devices for deep learning. This concept of\nResistive Processing Unit (RPU) devices we extend here towards convolutional\nneural networks (CNNs). We show how to map the convolutional layers to RPU\narrays such that the parallelism of the hardware can be fully utilized in all\nthree cycles of the backpropagation algorithm. We find that the noise and bound\nlimitations imposed due to analog nature of the computations performed on the\narrays effect the training accuracy of the CNNs. Noise and bound management\ntechniques are presented that mitigate these problems without introducing any\nadditional complexity in the analog circuits and can be addressed by the\ndigital circuits. In addition, we discuss digitally programmable update\nmanagement and device variability reduction techniques that can be used\nselectively for some of the layers in a CNN. We show that combination of all\nthose techniques enables a successful application of the RPU concept for\ntraining CNNs. The techniques discussed here are more general and can be\napplied beyond CNN architectures and therefore enables applicability of RPU\napproach for large class of neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:40:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Onen", "O. Murat", ""], ["Haensch", "Wilfried", ""]]}, {"id": "1705.08061", "submitter": "Changtong Luo", "authors": "Changtong Luo, Chen Chen, Zonglin Jiang", "title": "A divide and conquer method for symbolic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression aims to find a function that best explains the\nrelationship between independent variables and the objective value based on a\ngiven set of sample data. Genetic programming (GP) is usually considered as an\nappropriate method for the problem since it can optimize functional structure\nand coefficients simultaneously. However, the convergence speed of GP might be\ntoo slow for large scale problems that involve a large number of variables.\nFortunately, in many applications, the target function is separable or\npartially separable. This feature motivated us to develop a new method, divide\nand conquer (D&C), for symbolic regression, in which the target function is\ndivided into a number of sub-functions and the sub-functions are then\ndetermined by any of a GP algorithm. The separability is probed by a new\nproposed technique, Bi-Correlation test (BiCT). D&C powered GP has been tested\non some real-world applications, and the study shows that D&C can help GP to\nget the target function much more rapidly.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 02:53:28 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 04:08:56 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Luo", "Changtong", ""], ["Chen", "Chen", ""], ["Jiang", "Zonglin", ""]]}, {"id": "1705.08142", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders\n  S{\\o}gaard", "title": "Latent Multi-task Architecture Learning", "comments": "To appear in Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) allows deep neural networks to learn from related\ntasks by sharing parameters with other networks. In practice, however, MTL\ninvolves searching an enormous space of possible parameter sharing\narchitectures to find (a) the layers or subspaces that benefit from sharing,\n(b) the appropriate amount of sharing, and (c) the appropriate relative weights\nof the different task losses. Recent work has addressed each of the above\nproblems in isolation. In this work we present an approach that learns a latent\nmulti-task architecture that jointly addresses (a)--(c). We present experiments\non synthetic data and data from OntoNotes 5.0, including four different tasks\nand seven different domains. Our extension consistently outperforms previous\napproaches to learning latent architectures for multi-task problems and\nachieves up to 15% average error reductions over common approaches to MTL.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:58:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:05:37 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 10:30:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ruder", "Sebastian", ""], ["Bingel", "Joachim", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1705.08209", "submitter": "Corentin Tallec", "authors": "Corentin Tallec and Yann Ollivier", "title": "Unbiasing Truncated Backpropagation Through Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated Backpropagation Through Time (truncated BPTT) is a widespread\nmethod for learning recurrent computational graphs. Truncated BPTT keeps the\ncomputational benefits of Backpropagation Through Time (BPTT) while relieving\nthe need for a complete backtrack through the whole data sequence at every\nstep. However, truncation favors short-term dependencies: the gradient estimate\nof truncated BPTT is biased, so that it does not benefit from the convergence\nguarantees from stochastic gradient theory. We introduce Anticipated Reweighted\nTruncated Backpropagation (ARTBP), an algorithm that keeps the computational\nbenefits of truncated BPTT, while providing unbiasedness. ARTBP works by using\nvariable truncation lengths together with carefully chosen compensation factors\nin the backpropagation equation. We check the viability of ARTBP on two tasks.\nFirst, a simple synthetic task where careful balancing of temporal dependencies\nat different scales is needed: truncated BPTT displays unreliable performance,\nand in worst case scenarios, divergence, while ARTBP converges reliably.\nSecond, on Penn Treebank character-level language modelling, ARTBP slightly\noutperforms truncated BPTT.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:32:48 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tallec", "Corentin", ""], ["Ollivier", "Yann", ""]]}, {"id": "1705.08272", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Lubor Ladicky and Marc Pollefeys", "title": "Matching neural paths: transfer from recognition to correspondence\n  search", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks require finding per-part correspondences between\nobjects. In this work we focus on low-level correspondences - a highly\nambiguous matching problem. We propose to use a hierarchical semantic\nrepresentation of the objects, coming from a convolutional neural network, to\nsolve this ambiguity. Training it for low-level correspondence prediction\ndirectly might not be an option in some domains where the ground-truth\ncorrespondences are hard to obtain. We show how transfer from recognition can\nbe used to avoid such training. Our idea is to mark parts as \"matching\" if\ntheir features are close to each other at all the levels of convolutional\nfeature hierarchy (neural paths). Although the overall number of such paths is\nexponential in the number of layers, we propose a polynomial algorithm for\naggregating all of them in a single backward pass. The empirical validation is\ndone on the task of stereo correspondence and demonstrates that we achieve\ncompetitive results among the methods which do not use labeled target domain\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:40:35 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:04:30 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 22:33:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1705.08380", "submitter": "Katarzyna Schmidt Dr", "authors": "Katarzyna Schmidt and Oskar Wyszynski", "title": "An evolutionary strategy for DeltaE - E identification", "comments": null, "journal-ref": null, "doi": "10.1088/1748-0221/12/09/P09007", "report-no": null, "categories": "physics.ins-det cs.NE nucl-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present an automatic method for charge and mass\nidentification of charged nuclear fragments produced in heavy ion collisions at\nintermediate energies. The algorithm combines a generative model of DeltaE - E\nrelation and a Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES). The\nCMA-ES is a stochastic and derivative-free method employed to search parameter\nspace of the model by means of a fitness function. The article describes\ndetails of the method along with results of an application on simulated labeled\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:55:33 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 14:17:46 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Schmidt", "Katarzyna", ""], ["Wyszynski", "Oskar", ""]]}, {"id": "1705.08520", "submitter": "Horst Samulowitz", "authors": "Gonzalo Diaz, Achille Fokoue, Giacomo Nannicini, Horst Samulowitz", "title": "An effective algorithm for hyperparameter optimization of neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in designing neural network (NN) systems is to determine\nthe best structure and parameters for the network given the data for the\nmachine learning problem at hand. Examples of parameters are the number of\nlayers and nodes, the learning rates, and the dropout rates. Typically, these\nparameters are chosen based on heuristic rules and manually fine-tuned, which\nmay be very time-consuming, because evaluating the performance of a single\nparametrization of the NN may require several hours. This paper addresses the\nproblem of choosing appropriate parameters for the NN by formulating it as a\nbox-constrained mathematical optimization problem, and applying a\nderivative-free optimization tool that automatically and effectively searches\nthe parameter space. The optimization tool employs a radial basis function\nmodel of the objective function (the prediction accuracy of the NN) to\naccelerate the discovery of configurations yielding high accuracy. Candidate\nconfigurations explored by the algorithm are trained to a small number of\nepochs, and only the most promising candidates receive full training. The\nperformance of the proposed methodology is assessed on benchmark sets and in\nthe context of predicting drug-drug interactions, showing promising results.\nThe optimization tool used in this paper is open-source.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:17:44 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Diaz", "Gonzalo", ""], ["Fokoue", "Achille", ""], ["Nannicini", "Giacomo", ""], ["Samulowitz", "Horst", ""]]}, {"id": "1705.08550", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, and Xiaohui Xie", "title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification", "comments": "MICCAI 2017 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods rely on regions of interest (ROIs) which\nrequire great efforts to annotate. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\n(MIL) for labeling a set of instances/patches, we propose end-to-end trained\ndeep multi-instance networks for mass classification based on whole mammogram\nwithout the aforementioned ROIs. We explore three different schemes to\nconstruct deep multi-instance networks for whole mammogram classification.\nExperimental results on the INbreast dataset demonstrate the robustness of\nproposed networks compared to previous work using segmentation and detection\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:16:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Zhu", "Wentao", ""], ["Lou", "Qi", ""], ["Vang", "Yeeleng Scott", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1705.08557", "submitter": "Ankit Vani", "authors": "Ankit Vani, Yacine Jernite, David Sontag", "title": "Grounded Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Grounded Recurrent Neural Network (GRNN), a\nrecurrent neural network architecture for multi-label prediction which\nexplicitly ties labels to specific dimensions of the recurrent hidden state (we\ncall this process \"grounding\"). The approach is particularly well-suited for\nextracting large numbers of concepts from text. We apply the new model to\naddress an important problem in healthcare of understanding what medical\nconcepts are discussed in clinical text. Using a publicly available dataset\nderived from Intensive Care Units, we learn to label a patient's diagnoses and\nprocedures from their discharge summary. Our evaluation shows a clear advantage\nto using our proposed architecture over a variety of strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:17:49 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Vani", "Ankit", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""]]}, {"id": "1705.08639", "submitter": "Asier Mujika", "authors": "Asier Mujika, Florian Meier, Angelika Steger", "title": "Fast-Slow Recurrent Neural Networks", "comments": "Corrected minor typos in Figure 1 and Zoneout citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing sequential data of variable length is a major challenge in a wide\nrange of applications, such as speech recognition, language modeling,\ngenerative image modeling and machine translation. Here, we address this\nchallenge by proposing a novel recurrent neural network (RNN) architecture, the\nFast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both\nmultiscale RNNs and deep transition RNNs as it processes sequential data on\ndifferent timescales and learns complex transition functions from one time step\nto the next. We evaluate the FS-RNN on two character level language modeling\ndata sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of\nthe art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In\naddition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize\nWikipedia outperforming the best known compression algorithm with respect to\nthe BPC measure. We also present an empirical investigation of the learning and\nnetwork dynamics of the FS-RNN, which explains the improved performance\ncompared to other RNN architectures. Our approach is general as any kind of RNN\ncell is a possible building block for the FS-RNN architecture, and thus can be\nflexibly applied to different tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 07:41:03 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 20:00:18 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Mujika", "Asier", ""], ["Meier", "Florian", ""], ["Steger", "Angelika", ""]]}, {"id": "1705.08868", "submitter": "Aditya Grover", "authors": "Aditya Grover, Manik Dhar, Stefano Ermon", "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in\n  Generative Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning of probabilistic models has recently emerged as a\npromising alternative to maximum likelihood. Implicit models such as generative\nadversarial networks (GAN) often generate better samples compared to explicit\nmodels trained by maximum likelihood. Yet, GANs sidestep the characterization\nof an explicit density which makes quantitative evaluations challenging. To\nbridge this gap, we propose Flow-GANs, a generative adversarial network for\nwhich we can perform exact likelihood evaluation, thus supporting both\nadversarial and maximum likelihood training. When trained adversarially,\nFlow-GANs generate high-quality samples but attain extremely poor\nlog-likelihood scores, inferior even to a mixture model memorizing the training\ndata; the opposite is true when trained by maximum likelihood. Results on MNIST\nand CIFAR-10 demonstrate that hybrid training can attain high held-out\nlikelihoods while retaining visual fidelity in the generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:11:25 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 21:47:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Grover", "Aditya", ""], ["Dhar", "Manik", ""], ["Ermon", "Stefano", ""]]}, {"id": "1705.08881", "submitter": "Jun Li", "authors": "Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji", "title": "Dense Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of current deep learning methods for dense prediction is to\napply a model on a regular patch centered on each pixel to make pixel-wise\npredictions. These methods are limited in the sense that the patches are\ndetermined by network architecture instead of learned from data. In this work,\nwe propose the dense transformer networks, which can learn the shapes and sizes\nof patches from data. The dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted into each of\nthe encoder and decoder paths. The novelty of this work is that we provide\ntechnical solutions for learning the shapes and sizes of patches from data and\nefficiently restoring the spatial correspondence required for dense prediction.\nThe proposed dense transformer modules are differentiable, thus the entire\nnetwork can be trained. We apply the proposed networks on natural and\nbiological image segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:50:32 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:10:04 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Li", "Jun", ""], ["Chen", "Yongjun", ""], ["Cai", "Lei", ""], ["Davidson", "Ian", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.09037", "submitter": "Tao Lei", "authors": "Tao Lei, Wengong Jin, Regina Barzilay and Tommi Jaakkola", "title": "Deriving Neural Architectures from Sequence and Graph Kernels", "comments": "extended version of ICML 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 03:58:10 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 14:34:24 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 13:56:23 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lei", "Tao", ""], ["Jin", "Wengong", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1705.09137", "submitter": "Luke Godfrey", "authors": "Luke B. Godfrey and Michael S. Gashler", "title": "Neural Decomposition of Time-Series Data for Effective Generalization", "comments": "13 pages, 11 figures, IEEE TNNLS Preprint", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 29.7\n  (2018) 2973-2985", "doi": "10.1109/TNNLS.2017.2709324", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network technique for the analysis and extrapolation of\ntime-series data called Neural Decomposition (ND). Units with a sinusoidal\nactivation function are used to perform a Fourier-like decomposition of\ntraining samples into a sum of sinusoids, augmented by units with nonperiodic\nactivation functions to capture linear trends and other nonperiodic components.\nWe show how careful weight initialization can be combined with regularization\nto form a simple model that generalizes well. Our method generalizes\neffectively on the Mackey-Glass series, a dataset of unemployment rates as\nreported by the U.S. Department of Labor Statistics, a time-series of monthly\ninternational airline passengers, the monthly ozone concentration in downtown\nLos Angeles, and an unevenly sampled time-series of oxygen isotope measurements\nfrom a cave in north India. We find that ND outperforms popular time-series\nforecasting techniques including LSTM, echo state networks, ARIMA, SARIMA, SVR\nwith a radial basis function, and Gashler and Ashmore's model.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 11:55:37 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 14:02:59 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Godfrey", "Luke B.", ""], ["Gashler", "Michael S.", ""]]}, {"id": "1705.09279", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess,\n  Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh", "title": "Filtering Variational Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When used as a surrogate objective for maximum likelihood estimation in\nlatent variable models, the evidence lower bound (ELBO) produces\nstate-of-the-art results. Inspired by this, we consider the extension of the\nELBO to a family of lower bounds defined by a particle filter's estimator of\nthe marginal likelihood, the filtering variational objectives (FIVOs). FIVOs\ntake the same arguments as the ELBO, but can exploit a model's sequential\nstructure to form tighter bounds. We present results that relate the tightness\nof FIVO's bound to the variance of the particle filter's estimator by\nconsidering the generic case of bounds defined as log-transformed likelihood\nestimators. Experimentally, we show that training with FIVO results in\nsubstantial improvements over training the same model architecture with the\nELBO on sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:52:41 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 17:58:51 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 20:38:13 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Maddison", "Chris J.", ""], ["Lawson", "Dieterich", ""], ["Tucker", "George", ""], ["Heess", "Nicolas", ""], ["Norouzi", "Mohammad", ""], ["Mnih", "Andriy", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1705.09515", "submitter": "Yannick Esteve", "authors": "Edwin Simonnet (LIUM), Sahar Ghannay (LIUM), Nathalie Camelin (LIUM),\n  Yannick Est\\`eve (LIUM), Renato De Mori (LIA)", "title": "ASR error management for improving spoken language understanding", "comments": "Interspeech 2017, Aug 2017, Stockholm, Sweden. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of automatic speech recognition (ASR) error\ndetection and their use for improving spoken language understanding (SLU)\nsystems. In this study, the SLU task consists in automatically extracting, from\nASR transcriptions , semantic concepts and concept/values pairs in a e.g\ntouristic information system. An approach is proposed for enriching the set of\nsemantic labels with error specific labels and by using a recently proposed\nneural approach based on word embeddings to compute well calibrated ASR\nconfidence measures. Experimental results are reported showing that it is\npossible to decrease significantly the Concept/Value Error Rate with a state of\nthe art system, outperforming previously published results performance on the\nsame experimental data. It also shown that combining an SLU approach based on\nconditional random fields with a neural encoder/decoder attention based\narchitecture , it is possible to effectively identifying confidence islands and\nuncertain semantic output segments useful for deciding appropriate error\nhandling actions by the dialogue manager strategy .\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 10:34:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Simonnet", "Edwin", "", "LIUM"], ["Ghannay", "Sahar", "", "LIUM"], ["Camelin", "Nathalie", "", "LIUM"], ["Est\u00e8ve", "Yannick", "", "LIUM"], ["De Mori", "Renato", "", "LIA"]]}, {"id": "1705.09792", "submitter": "Chiheb Trabelsi", "authors": "Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep\n  Subramanian, Jo\\~ao Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua\n  Bengio, Christopher J Pal", "title": "Deep Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the vast majority of building blocks, techniques, and\narchitectures for deep learning are based on real-valued operations and\nrepresentations. However, recent work on recurrent neural networks and older\nfundamental theoretical analysis suggests that complex numbers could have a\nricher representational capacity and could also facilitate noise-robust memory\nretrieval mechanisms. Despite their attractive properties and potential for\nopening up entirely new neural architectures, complex-valued deep neural\nnetworks have been marginalized due to the absence of the building blocks\nrequired to design such models. In this work, we provide the key atomic\ncomponents for complex-valued deep neural networks and apply them to\nconvolutional feed-forward networks and convolutional LSTMs. More precisely, we\nrely on complex convolutions and present algorithms for complex\nbatch-normalization, complex weight initialization strategies for\ncomplex-valued neural nets and we use them in experiments with end-to-end\ntraining schemes. We demonstrate that such complex-valued models are\ncompetitive with their real-valued counterparts. We test deep complex models on\nseveral computer vision tasks, on music transcription using the MusicNet\ndataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve\nstate-of-the-art performance on these audio-related tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 09:04:55 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 08:38:38 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 22:42:47 GMT"}, {"version": "v4", "created": "Sun, 25 Feb 2018 23:42:06 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Trabelsi", "Chiheb", ""], ["Bilaniuk", "Olexa", ""], ["Zhang", "Ying", ""], ["Serdyuk", "Dmitriy", ""], ["Subramanian", "Sandeep", ""], ["Santos", "Jo\u00e3o Felipe", ""], ["Mehri", "Soroush", ""], ["Rostamzadeh", "Negar", ""], ["Bengio", "Yoshua", ""], ["Pal", "Christopher J", ""]]}, {"id": "1705.09864", "submitter": "Haojin Yang", "authors": "Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel", "title": "BMXNet: An Open-Source Binary Neural Network Implementation Based on\n  MXNet", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) can drastically reduce memory size and accesses\nby applying bit-wise operations instead of standard arithmetic operations.\nTherefore it could significantly improve the efficiency and lower the energy\nconsumption at runtime, which enables the application of state-of-the-art deep\nlearning models on low power devices. BMXNet is an open-source BNN library\nbased on MXNet, which supports both XNOR-Networks and Quantized Neural\nNetworks. The developed BNN layers can be seamlessly applied with other\nstandard library components and work in both GPU and CPU mode. BMXNet is\nmaintained and developed by the multimedia research group at Hasso Plattner\nInstitute and released under Apache license. Extensive experiments validate the\nefficiency and effectiveness of our implementation. The BMXNet library, several\nsample projects, and a collection of pre-trained binary deep models are\navailable for download at https://github.com/hpi-xnor\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 20:52:10 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Yang", "Haojin", ""], ["Fritzsche", "Martin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1705.10119", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shengyang Sun, Jun Zhu", "title": "Kernel Implicit Variational Inference", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in variational inference has paid much attention to the\nflexibility of variational posteriors. One promising direction is to use\nimplicit distributions, i.e., distributions without tractable densities as the\nvariational posterior. However, existing methods on implicit posteriors still\nface challenges of noisy estimation and computational infeasibility when\napplied to models with high-dimensional latent variables. In this paper, we\npresent a new approach named Kernel Implicit Variational Inference that\naddresses these challenges. As far as we know, for the first time implicit\nvariational inference is successfully applied to Bayesian neural networks,\nwhich shows promising results on both regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:11:35 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 13:49:00 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 15:45:59 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shi", "Jiaxin", ""], ["Sun", "Shengyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1705.10209", "submitter": "Micha{\\l} Zapotoczny", "authors": "Micha{\\l} Zapotoczny, Pawe{\\l} Rychlikowski, and Jan Chorowski", "title": "On Multilingual Training of Neural Dependency Parsers", "comments": "preprint accepted into the TSD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a recently proposed neural dependency parser can be improved by\njoint training on multiple languages from the same family. The parser is\nimplemented as a deep neural network whose only input is orthographic\nrepresentations of words. In order to successfully parse, the network has to\ndiscover how linguistically relevant concepts can be inferred from word\nspellings. We analyze the representations of characters and words that are\nlearned by the network to establish which properties of languages were\naccounted for. In particular we show that the parser has approximately learned\nto associate Latin characters with their Cyrillic counterparts and that it can\ngroup Polish and Russian words that have a similar grammatical function.\nFinally, we evaluate the parser on selected languages from the Universal\nDependencies dataset and show that it is competitive with other recently\nproposed state-of-the art methods, while having a simple structure.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:24:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zapotoczny", "Micha\u0142", ""], ["Rychlikowski", "Pawe\u0142", ""], ["Chorowski", "Jan", ""]]}, {"id": "1705.10229", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, Steve Young", "title": "Latent Intention Dialogue Models", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a dialogue agent that is capable of making autonomous decisions\nand communicating by natural language is one of the long-term goals of machine\nlearning research. Traditional approaches either rely on hand-crafting a small\nstate-action set for applying reinforcement learning that is not scalable or\nconstructing deterministic models for learning dialogue sentences that fail to\ncapture natural conversational variability. In this paper, we propose a Latent\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\nlearn underlying dialogue intentions in the framework of neural variational\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\ninterpreted as actions guiding the generation of machine responses, which can\nbe further refined autonomously by reinforcement learning. The experimental\nevaluation of LIDM shows that the model out-performs published benchmarks for\nboth corpus-based and human evaluation, demonstrating the effectiveness of\ndiscrete latent variable models for learning goal-oriented dialogues.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:01:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Miao", "Yishu", ""], ["Blunsom", "Phil", ""], ["Young", "Steve", ""]]}, {"id": "1705.10368", "submitter": "Jose Eduardo Novoa Ilic", "authors": "Jos\\'e Novoa, Josu\\'e Fredes and N\\'estor Becerra Yoma", "title": "DNN-based uncertainty estimation for weighted DNN-HMM ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, the uncertainty is defined as the mean square error between a\ngiven enhanced noisy observation vector and the corresponding clean one. Then,\na DNN is trained by using enhanced noisy observation vectors as input and the\nuncertainty as output with a training database. In testing, the DNN receives an\nenhanced noisy observation vector and delivers the estimated uncertainty. This\nuncertainty in employed in combination with a weighted DNN-HMM based speech\nrecognition system and compared with an existing estimation of the noise\ncancelling uncertainty variance based on an additive noise model. Experiments\nwere carried out with Aurora-4 task. Results with clean, multi-noise and\nmulti-condition training are presented.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:24:14 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Novoa", "Jos\u00e9", ""], ["Fredes", "Josu\u00e9", ""], ["Yoma", "N\u00e9stor Becerra", ""]]}, {"id": "1705.10694", "submitter": "Andreas Veit", "authors": "David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit", "title": "Deep Learning is Robust to Massive Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained on large supervised datasets have led to\nimpressive results in image classification and other tasks. However,\nwell-annotated datasets can be time-consuming and expensive to collect, lending\nincreased interest to larger but noisy datasets that are more easily obtained.\nIn this paper, we show that deep neural networks are capable of generalizing\nfrom training data for which true labels are massively outnumbered by incorrect\nlabels. We demonstrate remarkably high test performance after training on\ncorrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain\ntest accuracy above 90 percent even after each clean training example has been\ndiluted with 100 randomly-labeled examples. Such behavior holds across multiple\npatterns of label noise, even when erroneous labels are biased towards\nconfusing classes. We show that training in this regime requires a significant\nbut manageable increase in dataset size that is related to the factor by which\ncorrect labels have been diluted. Finally, we provide an analysis of our\nresults that shows how increasing noise decreases the effective batch size.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 02:02:56 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 16:51:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Rolnick", "David", ""], ["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10823", "submitter": "Otkrist Gupta", "authors": "Bowen Baker, Otkrist Gupta, Ramesh Raskar and Nikhil Naik", "title": "Accelerating Neural Architecture Search using Performance Prediction", "comments": "Submitted to International Conference on Learning Representations,\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for neural network hyperparameter optimization and meta-modeling are\ncomputationally expensive due to the need to train a large number of model\nconfigurations. In this paper, we show that standard frequentist regression\nmodels can predict the final performance of partially trained model\nconfigurations using features based on network architectures, hyperparameters,\nand time-series validation performance data. We empirically show that our\nperformance prediction models are much more effective than prominent Bayesian\ncounterparts, are simpler to implement, and are faster to train. Our models can\npredict final performance in both visual classification and language modeling\ndomains, are effective for predicting performance of drastically varying model\narchitectures, and can even generalize between model classes. Using these\nprediction models, we also propose an early stopping method for hyperparameter\noptimization and meta-modeling, which obtains a speedup of a factor up to 6x in\nboth hyperparameter optimization and meta-modeling. Finally, we empirically\nshow that our early stopping method can be seamlessly incorporated into both\nreinforcement learning-based architecture selection algorithms and bandit based\nsearch methods. Through extensive experimentation, we empirically show our\nperformance prediction models and early stopping algorithm are state-of-the-art\nin terms of prediction accuracy and speedup achieved while still identifying\nthe optimal model configurations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:00:53 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:09:35 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Baker", "Bowen", ""], ["Gupta", "Otkrist", ""], ["Raskar", "Ramesh", ""], ["Naik", "Nikhil", ""]]}, {"id": "1705.10854", "submitter": "Larissa Albantakis", "authors": "Larissa Albantakis", "title": "A Tale of Two Animats: What does it take to have goals?", "comments": "This article is a contribution to the FQXi 2016-2017 essay contest\n  \"Wandering Towards a Goal\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it take for a system, biological or not, to have goals? Here, this\nquestion is approached in the context of in silico artificial evolution. By\nexamining the informational and causal properties of artificial organisms\n('animats') controlled by small, adaptive neural networks (Markov Brains), this\nessay discusses necessary requirements for intrinsic information, autonomy, and\nmeaning. The focus lies on comparing two types of Markov Brains that evolved in\nthe same simple environment: one with purely feedforward connections between\nits elements, the other with an integrated set of elements that causally\nconstrain each other. While both types of brains 'process' information about\ntheir environment and are equally fit, only the integrated one forms a causally\nautonomous entity above a background of external influences. This suggests that\nto assess whether goals are meaningful for a system itself, it is important to\nunderstand what the system is, rather than what it does.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 20:19:17 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Albantakis", "Larissa", ""]]}, {"id": "1705.10929", "submitter": "Sandeep Subramanian", "authors": "Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal,\n  Aaron Courville", "title": "Adversarial Generation of Natural Language", "comments": "11 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:06:39 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rajeswar", "Sai", ""], ["Subramanian", "Sandeep", ""], ["Dutil", "Francis", ""], ["Pal", "Christopher", ""], ["Courville", "Aaron", ""]]}, {"id": "1705.10993", "submitter": "Julien Perez", "authors": "Julien Perez and Tomi Silander", "title": "Non-Markovian Control with Gated End-to-End Memory Policy Networks", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable environments present an important open challenge in the\ndomain of sequential control learning with delayed rewards. Despite numerous\nattempts during the two last decades, the majority of reinforcement learning\nalgorithms and associated approximate models, applied to this context, still\nassume Markovian state transitions. In this paper, we explore the use of a\nrecently proposed attention-based model, the Gated End-to-End Memory Network,\nfor sequential control. We call the resulting model the Gated End-to-End Memory\nPolicy Network. More precisely, we use a model-free value-based algorithm to\nlearn policies for partially observed domains using this memory-enhanced neural\nnetwork. This model is end-to-end learnable and it features unbounded memory.\nIndeed, because of its attention mechanism and associated non-parametric\nmemory, the proposed model allows us to define an attention mechanism over the\nobservation stream unlike recurrent models. We show encouraging results that\nillustrate the capability of our attention-based model in the context of the\ncontinuous-state non-stationary control problem of stock trading. We also\npresent an OpenAI Gym environment for simulated stock exchange and explain its\nrelevance as a benchmark for the field of non-Markovian decision process\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:00:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Perez", "Julien", ""], ["Silander", "Tomi", ""]]}, {"id": "1705.11040", "submitter": "Tim Rockt\\\"aschel", "authors": "Tim Rockt\\\"aschel and Sebastian Riedel", "title": "End-to-End Differentiable Proving", "comments": "NIPS 2017 camera-ready, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce neural networks for end-to-end differentiable proving of queries\nto knowledge bases by operating on dense vector representations of symbols.\nThese neural networks are constructed recursively by taking inspiration from\nthe backward chaining algorithm as used in Prolog. Specifically, we replace\nsymbolic unification with a differentiable computation on vector\nrepresentations of symbols using a radial basis function kernel, thereby\ncombining symbolic reasoning with learning subsymbolic vector representations.\nBy using gradient descent, the resulting neural network can be trained to infer\nfacts from a given incomplete knowledge base. It learns to (i) place\nrepresentations of similar symbols in close proximity in a vector space, (ii)\nmake use of such similarities to prove queries, (iii) induce logical rules, and\n(iv) use provided and induced logical rules for multi-hop reasoning. We\ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art\nneural link prediction model, on three out of four benchmark knowledge bases\nwhile at the same time inducing interpretable function-free first-order logic\nrules.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:40:57 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 00:24:04 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1705.11146", "submitter": "Friedemann Zenke", "authors": "Friedemann Zenke and Surya Ganguli", "title": "SuperSpike: Supervised learning in multi-layer spiking neural networks", "comments": null, "journal-ref": null, "doi": "10.1162/neco_a_01086", "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vast majority of computation in the brain is performed by spiking neural\nnetworks. Despite the ubiquity of such spiking, we currently lack an\nunderstanding of how biological spiking neural circuits learn and compute\nin-vivo, as well as how we can instantiate such capabilities in artificial\nspiking circuits in-silico. Here we revisit the problem of supervised learning\nin temporally coding multi-layer spiking neural networks. First, by using a\nsurrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based\nthree factor learning rule capable of training multi-layer networks of\ndeterministic integrate-and-fire neurons to perform nonlinear computations on\nspatiotemporal spike patterns. Second, inspired by recent results on feedback\nalignment, we compare the performance of our learning rule under different\ncredit assignment strategies for propagating output errors to hidden units.\nSpecifically, we test uniform, symmetric and random feedback, finding that\nsimpler tasks can be solved with any type of feedback, while more complex tasks\nrequire symmetric feedback. In summary, our results open the door to obtaining\na better scientific understanding of learning and computation in spiking neural\nnetworks by advancing our ability to train them to solve nonlinear problems\ninvolving transformations between different spatiotemporal spike-time patterns.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:31:26 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 15:08:04 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zenke", "Friedemann", ""], ["Ganguli", "Surya", ""]]}]