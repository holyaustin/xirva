[{"id": "1411.0161", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Entropy of Overcomplete Kernel Dictionaries", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In signal analysis and synthesis, linear approximation theory considers a\nlinear decomposition of any given signal in a set of atoms, collected into a\nso-called dictionary. Relevant sparse representations are obtained by relaxing\nthe orthogonality condition of the atoms, yielding overcomplete dictionaries\nwith an extended number of atoms. More generally than the linear decomposition,\novercomplete kernel dictionaries provide an elegant nonlinear extension by\ndefining the atoms through a mapping kernel function (e.g., the gaussian\nkernel). Models based on such kernel dictionaries are used in neural networks,\ngaussian processes and online learning with kernels.\n  The quality of an overcomplete dictionary is evaluated with a diversity\nmeasure the distance, the approximation, the coherence and the Babel measures.\nIn this paper, we develop a framework to examine overcomplete kernel\ndictionaries with the entropy from information theory. Indeed, a higher value\nof the entropy is associated to a further uniform spread of the atoms over the\nspace. For each of the aforementioned diversity measures, we derive lower\nbounds on the entropy. Several definitions of the entropy are examined, with an\nextensive analysis in both the input space and the mapped feature space.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 19:41:14 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1411.0217", "submitter": "Raka Jovanovic", "authors": "Raka Jovanovic, Sabre Kais, Fahhad H. Alharbi", "title": "Cuckoo Search Inspired Hybridization of the Nelder-Mead Simplex\n  Algorithm Applied to Optimization of Photovoltaic Cells", "comments": null, "journal-ref": null, "doi": "10.18576/amis/100314", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new hybridization of the Cuckoo Search (CS) is developed and applied to\noptimize multi-cell solar systems; namely multi-junction and split spectrum\ncells. The new approach consists of combining the CS with the Nelder-Mead\nmethod. More precisely, instead of using single solutions as nests for the CS,\nwe use the concept of a simplex which is used in the Nelder-Mead algorithm.\nThis makes it possible to use the flip operation introduces in the Nelder-Mead\nalgorithm instead of the Levy flight which is a standard part of the CS. In\nthis way, the hybridized algorithm becomes more robust and less sensitive to\nparameter tuning which exists in CS. The goal of our work was to optimize the\nperformance of multi-cell solar systems. Although the underlying problem\nconsists of the minimization of a function of a relatively small number of\nparameters, the difficulty comes from the fact that the evaluation of the\nfunction is complex and only a small number of evaluations is possible. In our\ntest, we show that the new method has a better performance when compared to\nsimilar but more compex hybridizations of Nelder-Mead algorithm using genetic\nalgorithms or particle swarm optimization on standard benchmark functions.\nFinally, we show that the new method outperforms some standard meta-heuristics\nfor the problem of interest.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 07:47:26 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Jovanovic", "Raka", ""], ["Kais", "Sabre", ""], ["Alharbi", "Fahhad H.", ""]]}, {"id": "1411.0247", "submitter": "Timothy Lillicrap", "authors": "Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J.\n  Akerman", "title": "Random feedback weights support learning in deep neural networks", "comments": "14 pages, 5 figures in main text; 13 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain processes information through many layers of neurons. This deep\narchitecture is representationally powerful, but it complicates learning by\nmaking it hard to identify the responsible neurons when a mistake is made. In\nmachine learning, the backpropagation algorithm assigns blame to a neuron by\ncomputing exactly how it contributed to an error. To do this, it multiplies\nerror signals by matrices consisting of all the synaptic weights on the\nneuron's axon and farther downstream. This operation requires a precisely\nchoreographed transport of synaptic weight information, which is thought to be\nimpossible in the brain. Here we present a surprisingly simple algorithm for\ndeep learning, which assigns blame by multiplying error signals by random\nsynaptic weights. We show that a network can learn to extract useful\ninformation from signals sent through these random feedback connections. In\nessence, the network learns to learn. We demonstrate that this new mechanism\nperforms as quickly and accurately as backpropagation on a variety of problems\nand describe the principles which underlie its function. Our demonstration\nprovides a plausible basis for how a neuron can be adapted using error signals\ngenerated at distal locations in the brain, and thus dispels long-held\nassumptions about the algorithmic constraints on learning in neural circuits.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 12:31:15 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Lillicrap", "Timothy P.", ""], ["Cownden", "Daniel", ""], ["Tweed", "Douglas B.", ""], ["Akerman", "Colin J.", ""]]}, {"id": "1411.1091", "submitter": "Jonathan Long", "authors": "Jonathan Long, Ning Zhang, Trevor Darrell", "title": "Do Convnets Learn Correspondence?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural nets (convnets) trained from massive labeled datasets\nhave substantially improved the state-of-the-art in image classification and\nobject detection. However, visual understanding requires establishing\ncorrespondence on a finer level than object category. Given their large pooling\nregions and training from whole-image labels, it is not clear that convnets\nderive their success from an accurate correspondence model which could be used\nfor precise localization. In this paper, we study the effectiveness of convnet\nactivation features for tasks requiring correspondence. We present evidence\nthat convnet features localize at a much finer scale than their receptive field\nsizes, that they can be used to perform intraclass alignment as well as\nconventional hand-engineered features, and that they outperform conventional\nfeatures in keypoint prediction on objects from PASCAL VOC 2011.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:35:55 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Long", "Jonathan", ""], ["Zhang", "Ning", ""], ["Darrell", "Trevor", ""]]}, {"id": "1411.1509", "submitter": "Zetao Chen", "authors": "Zetao Chen, Obadiah Lam, Adam Jacobson and Michael Milford", "title": "Convolutional Neural Network-based Place Recognition", "comments": "8 pages, 11 figures, this paper has been accepted by 2014\n  Australasian Conference on Robotics and Automation (ACRA 2014) to be held in\n  University of Melbourne, Dec 2~4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Convolutional Neural Networks (CNNs) have been shown to achieve\nstate-of-the-art performance on various classification tasks. In this paper, we\npresent for the first time a place recognition technique based on CNN models,\nby combining the powerful features learnt by CNNs with a spatial and sequential\nfilter. Applying the system to a 70 km benchmark place recognition dataset we\nachieve a 75% increase in recall at 100% precision, significantly outperforming\nall previous state of the art techniques. We also conduct a comprehensive\nperformance comparison of the utility of features from all 21 layers for place\nrecognition, both for the benchmark dataset and for a second dataset with more\nsignificant viewpoint changes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 07:03:15 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Chen", "Zetao", ""], ["Lam", "Obadiah", ""], ["Jacobson", "Adam", ""], ["Milford", "Michael", ""]]}, {"id": "1411.1792", "submitter": "Jason Yosinski", "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson", "title": "How transferable are features in deep neural networks?", "comments": "To appear in Advances in Neural Information Processing Systems 27\n  (NIPS 2014)", "journal-ref": "Advances in Neural Information Processing Systems 27, pages\n  3320-3328. Dec. 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep neural networks trained on natural images exhibit a curious\nphenomenon in common: on the first layer they learn features similar to Gabor\nfilters and color blobs. Such first-layer features appear not to be specific to\na particular dataset or task, but general in that they are applicable to many\ndatasets and tasks. Features must eventually transition from general to\nspecific by the last layer of the network, but this transition has not been\nstudied extensively. In this paper we experimentally quantify the generality\nversus specificity of neurons in each layer of a deep convolutional neural\nnetwork and report a few surprising results. Transferability is negatively\naffected by two distinct issues: (1) the specialization of higher layer neurons\nto their original task at the expense of performance on the target task, which\nwas expected, and (2) optimization difficulties related to splitting networks\nbetween co-adapted neurons, which was not expected. In an example network\ntrained on ImageNet, we demonstrate that either of these two issues may\ndominate, depending on whether features are transferred from the bottom,\nmiddle, or top of the network. We also document that the transferability of\nfeatures decreases as the distance between the base task and target task\nincreases, but that transferring features even from distant tasks can be better\nthan using random features. A final surprising result is that initializing a\nnetwork with transferred features from almost any number of layers can produce\na boost to generalization that lingers even after fine-tuning to the target\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 23:09:37 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Yosinski", "Jason", ""], ["Clune", "Jeff", ""], ["Bengio", "Yoshua", ""], ["Lipson", "Hod", ""]]}, {"id": "1411.2153", "submitter": "Simone Cirillo", "authors": "Simone Cirillo, Stefan Lloyd, Peter Nordin", "title": "Evolving intraday foreign exchange trading strategies utilizing multiple\n  instruments price series", "comments": "15 pages, 10 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Genetic Programming architecture for the generation of foreign\nexchange trading strategies. The system's principal features are the evolution\nof free-form strategies which do not rely on any prior models and the\nutilization of price series from multiple instruments as input data. This\nlatter feature constitutes an innovation with respect to previous works\ndocumented in literature. In this article we utilize Open, High, Low, Close bar\ndata at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPY\ncurrency pairs. We will test the implementation analyzing the in-sample and\nout-of-sample performance of strategies for trading the USD.JPY obtained across\nmultiple algorithm runs. We will also evaluate the differences between\nstrategies selected according to two different criteria: one relies on the\nfitness obtained on the training set only, the second one makes use of an\nadditional validation dataset. Strategy activity and trade accuracy are\nremarkably stable between in and out of sample results. From a profitability\naspect, the two criteria both result in strategies successful on out-of-sample\ndata but exhibiting different characteristics. The overall best performing\nout-of-sample strategy achieves a yearly return of 19%.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 19:22:55 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Cirillo", "Simone", ""], ["Lloyd", "Stefan", ""], ["Nordin", "Peter", ""]]}, {"id": "1411.2276", "submitter": "Matej Hoffmann", "authors": "Matej Hoffmann and Vincent C. M\\\"uller", "title": "Trade-Offs in Exploiting Body Morphology for Control: from Simple Bodies\n  and Model-Based Control to Complex Bodies with Model-Free Distributed Control\n  Schemes", "comments": null, "journal-ref": "Helmut Hauser; Rudolf M. F\\\"uchslin & Rolf Pfeifer, ed., 'E-book\n  on Opinions and Outlooks on Morphological Computation', 2014, pp. 185--194", "doi": null, "report-no": null, "categories": "cs.RO cs.NE cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Tailoring the design of robot bodies for control purposes is implicitly\nperformed by engineers, however, a methodology or set of tools is largely\nabsent and optimization of morphology (shape, material properties of robot\nbodies, etc.) is lagging behind the development of controllers. This has become\neven more prominent with the advent of compliant, deformable or \"soft\" bodies.\nThese carry substantial potential regarding their exploitation for\ncontrol---sometimes referred to as \"morphological computation\" in the sense of\noffloading computation needed for control to the body. Here, we will argue in\nfavor of a dynamical systems rather than computational perspective on the\nproblem. Then, we will look at the pros and cons of simple vs. complex bodies,\ncritically reviewing the attractive notion of \"soft\" bodies automatically\ntaking over control tasks. We will address another key dimension of the design\nspace---whether model-based control should be used and to what extent it is\nfeasible to develop faithful models for different morphologies.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 20:11:42 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Hoffmann", "Matej", ""], ["M\u00fcller", "Vincent C.", ""]]}, {"id": "1411.2821", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Philip\n  de Chazal, Tara Julia Hamilton", "title": "Turn Down that Noise: Synaptic Encoding of Afferent SNR in a Single\n  Spiking Neuron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We have added a simplified neuromorphic model of Spike Time Dependent\nPlasticity (STDP) to the Synapto-dendritic Kernel Adapting Neuron (SKAN). The\nresulting neuron model is the first to show synaptic encoding of afferent\nsignal to noise ratio in addition to the unsupervised learning of spatio\ntemporal spike patterns. The neuron model is particularly suitable for\nimplementation in digital neuromorphic hardware as it does not use any complex\nmathematical operations and uses a novel approach to achieve synaptic\nhomeostasis. The neurons noise compensation properties are characterized and\ntested on noise corrupted zeros digits of the MNIST handwritten dataset.\nResults show the simultaneously learning common patterns in its input data\nwhile dynamically weighing individual afferent channels based on their signal\nto noise ratio. Despite its simplicity the interesting behaviors of the neuron\nmodel and the resulting computational power may offer insights into biological\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 14:22:37 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Afshar", "Saeed", ""], ["George", "Libin", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""], ["de Chazal", "Philip", ""], ["Hamilton", "Tara Julia", ""]]}, {"id": "1411.2897", "submitter": "Hassan Ismkhan", "authors": "Hassan Ismkhan", "title": "Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic\n  Operator", "comments": "International Journal on Computational Science & Applications,\n  Volume: 4 - volume NO: 2 - Issue: April 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper research review Ant colony optimization (ACO) and Genetic\nAlgorithm (GA), both are two powerful meta-heuristics. This paper explains some\nmajor defects of these two algorithm at first then proposes a new model for ACO\nin which, artificial ants use a quick genetic operator and accelerate their\nactions in selecting next state. Experimental results show that proposed hybrid\nalgorithm is effective and its performance including speed and accuracy beats\nother version.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 17:42:26 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Ismkhan", "Hassan", ""]]}, {"id": "1411.3251", "submitter": "Dheevatsa Mudigere", "authors": "S.N. Omkar, Dheevatsa Mudigere, J Senthilnath, M. Vijaya Kumar", "title": "Identification of Helicopter Dynamics based on Flight Data using Nature\n  Inspired Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The complexity of helicopter flight dynamics makes modeling and helicopter\nsystem identification a very difficult task. Most of the traditional techniques\nrequire a model structure to be defined apriori and in case of helicopter\ndynamics, this is difficult due to its complexity and the interplay between\nvarious subsystems.To overcome this difficulty, non-parametric approaches are\ncommonly adopted for helicopter system identification. Artificial Neural\nNetwork are a widely used class of algorithms for non-parametric system\nidentification, among them, the Nonlinear Auto Regressive eXogeneous input\nnetwork (NARX) model is very popular, but it also necessitates some in depth\nknowledge regarding the system being modeled. There have been many approaches\nproposed to circumvent this and yet still retain the advantageous\ncharacteristics. In this paper we carry out an extensive study of one such\nnewly proposed approach using a modified NARX model with a two tiered,\nexternally driven recurrent neural network architecture. This is coupled with\nan outer optimization routine for evolving the order of the system. This\ngeneric architecture is comprehensively explored to ascertain its usability and\ncritically asses its potential. Different instantiations of this architecture,\nbased on nature inspired computational techniques (Artificial Bee Colony,\nArtificial Immune System and Particle Swarm Optimization) are evaluated and\ncritically compared in this paper. Simulations have been carried out for\nidentifying the longitudinally uncoupled dynamics. Results of identification\nindicate a quite close correlation between the actual and the predicted\nresponse of the helicopter for all the models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 17:29:49 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Omkar", "S. N.", ""], ["Mudigere", "Dheevatsa", ""], ["Senthilnath", "J", ""], ["Kumar", "M. Vijaya", ""]]}, {"id": "1411.3277", "submitter": "Hassan Ismkhan", "authors": "Hassan Ismkhan", "title": "Using Ants as a Genetic Crossover Operator in GLS to Solve STSP", "comments": "2010 International Conference of Soft Computing and Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two\noptimization algorithms that have been successfully applied to the Traveling\nSalesman Problem (TSP). In this paper we define new crossover operator then\nredefine ACAs ants as operate according to defined crossover operator then put\nforward our GLS that uses these ants to solve Symmetric TSP (STSP) instances.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 18:56:45 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Ismkhan", "Hassan", ""]]}, {"id": "1411.3806", "submitter": "Sandhya Bansal", "authors": "Sandhya Bansal, V. Katiyar", "title": "Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing\n  Problem with Time Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper fuzzy VRPTW with an uncertain travel time is considered.\nCredibility theory is used to model the problem and specifies a preference\nindex at which it is desired that the travel times to reach the customers fall\ninto their time windows. We propose the integration of fuzzy and ant colony\nsystem based evolutionary algorithm to solve the problem while preserving the\nconstraints. Computational results for certain benchmark problems having short\nand long time horizons are presented to show the effectiveness of the\nalgorithm. Comparison between different preferences indexes have been obtained\nto help the user in making suitable decisions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 06:11:48 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Bansal", "Sandhya", ""], ["Katiyar", "V.", ""]]}, {"id": "1411.3815", "submitter": "Mingmin Zhao", "authors": "Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee", "title": "Predictive Encoding of Contextual Relationships for Perceptual\n  Inference, Interpolation and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new neurally-inspired model that can learn to encode the global\nrelationship context of visual events across time and space and to use the\ncontextual information to modulate the analysis by synthesis process in a\npredictive coding framework. The model learns latent contextual representations\nby maximizing the predictability of visual events based on local and global\ncontextual information through both top-down and bottom-up processes. In\ncontrast to standard predictive coding models, the prediction error in this\nmodel is used to update the contextual representation but does not alter the\nfeedforward input for the next layer, and is thus more consistent with\nneurophysiological observations. We establish the computational feasibility of\nthis model by demonstrating its ability in several aspects. We show that our\nmodel can outperform state-of-art performances of gated Boltzmann machines\n(GBM) in estimation of contextual information. Our model can also interpolate\nmissing events or predict future events in image sequences while simultaneously\nestimating contextual information. We show it achieves state-of-art\nperformances in terms of prediction accuracy in a variety of tasks and\npossesses the ability to interpolate missing frames, a function that is lacking\nin GBM.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 07:38:45 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 00:08:05 GMT"}, {"version": "v3", "created": "Wed, 24 Dec 2014 12:05:56 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2015 12:50:42 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 17:52:12 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 15:57:36 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Zhao", "Mingmin", ""], ["Zhuang", "Chengxu", ""], ["Wang", "Yizhou", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1411.4116", "submitter": "Jack Cheng J", "authors": "Jianpeng Cheng, Dimitri Kartsaklis, Edward Grefenstette", "title": "Investigating the Role of Prior Disambiguation in Deep-learning\n  Compositional Models of Meaning", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to explore the effect of prior disambiguation on neural\nnetwork- based compositional models, with the hope that better semantic\nrepresentations for text compounds can be produced. We disambiguate the input\nword vectors before they are fed into a compositional deep net. A series of\nevaluations shows the positive effect of prior disambiguation for such deep\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 06:32:49 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Kartsaklis", "Dimitri", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1411.4148", "submitter": "Maumita Bhattacharya", "authors": "Maumita Bhattacharya", "title": "Diversity Handling In Evolutionary Landscape", "comments": "In the \"Proceedings of the International Workshop on Combinations of\n  Intelligent Methods and Applications (CIMA 2014)\", pp. 1-8, November'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search ability of an Evolutionary Algorithm (EA) depends on the variation\namong the individuals in the population. Maintaining an optimal level of\ndiversity in the EA population is imperative to ensure that progress of the EA\nsearch is unhindered by premature convergence to suboptimal solutions. Clearer\nunderstanding of the concept of population diversity, in the context of\nevolutionary search and premature convergence in particular, is the key to\ndesigning efficient EAs. To this end, this paper first presents a comprehensive\nanalysis of the EA population diversity issues. Next we present an\ninvestigation on a counter-niching EA technique that introduces and maintains\nconstructive diversity in the population. The proposed approach uses informed\ngenetic operations to reach promising, but un-explored or under-explored areas\nof the search space, while discouraging premature local convergence. Simulation\nruns on a number of standard benchmark test functions with Genetic Algorithm\n(GA) implementation shows promising results.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 14:19:22 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Bhattacharya", "Maumita", ""]]}, {"id": "1411.4246", "submitter": "Md Lisul Islam", "authors": "Md. Lisul Islam, Swakkhar Shatabda and M. Sohel Rahman", "title": "GreMuTRRR: A Novel Genetic Algorithm to Solve Distance Geometry Problem\n  for Protein Structures", "comments": "Accepted for publication in the 8th International Conference on\n  Electrical and Computer Engineering (ICECE 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear Magnetic Resonance (NMR) Spectroscopy is a widely used technique to\npredict the native structure of proteins. However, NMR machines are only able\nto report approximate and partial distances between pair of atoms. To build the\nprotein structure one has to solve the Euclidean distance geometry problem\ngiven the incomplete interval distance data produced by NMR machines. In this\npaper, we propose a new genetic algorithm for solving the Euclidean distance\ngeometry problem for protein structure prediction given sparse NMR data. Our\ngenetic algorithm uses a greedy mutation operator to intensify the search, a\ntwin removal technique for diversification in the population and a random\nrestart method to recover stagnation. On a standard set of benchmark dataset,\nour algorithm significantly outperforms standard genetic algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 11:26:06 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Islam", "Md. Lisul", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1411.4297", "submitter": "Rishita Kalyani", "authors": "Rishita Kalyani", "title": "Application of Multi-core Parallel Programming to a Combination of Ant\n  Colony Optimization and Genetic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Paper will deal with a combination of Ant Colony and Genetic Programming\nAlgorithm to optimize Travelling Salesmen problem (NP-Hard). However, the\ncomplexity of the algorithm requires considerable computational time and\nresources. Parallel implementation can reduce the computational time. In this\npaper, emphasis in the parallelizing section is given to Multi-core\narchitecture and Multi-Processor Systems which is developed and used almost\neverywhere today and hence, multi-core parallelization to the combination of\nalgorithm is achieved by OpenMP library by Intel Corporation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 15:58:47 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Kalyani", "Rishita", ""]]}, {"id": "1411.4379", "submitter": "Md Lisul Islam", "authors": "Md. Lisul Islam, Novia Nurain, Swakkhar Shatabda and M Sohel Rahman", "title": "FGPGA: An Efficient Genetic Approach for Producing Feasible Graph\n  Partitions", "comments": "Accepted in the 1st International Conference on Networking Systems\n  and Security 2015 (NSysS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partitioning, a well studied problem of parallel computing has many\napplications in diversified fields such as distributed computing, social\nnetwork analysis, data mining and many other domains. In this paper, we\nintroduce FGPGA, an efficient genetic approach for producing feasible graph\npartitions. Our method takes into account the heterogeneity and capacity\nconstraints of the partitions to ensure balanced partitioning. Such approach\nhas various applications in mobile cloud computing that include feasible\ndeployment of software applications on the more resourceful infrastructure in\nthe cloud instead of mobile hand set. Our proposed approach is light weight and\nhence suitable for use in cloud architecture. We ensure feasibility of the\npartitions generated by not allowing over-sized partitions to be generated\nduring the initialization and search. Our proposed method tested on standard\nbenchmark datasets significantly outperforms the state-of-the-art methods in\nterms of quality of partitions and feasibility of the solutions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 06:51:50 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Islam", "Md. Lisul", ""], ["Nurain", "Novia", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M Sohel", ""]]}, {"id": "1411.4565", "submitter": "Drona Pratap Chandu", "authors": "Drona Pratap Chandu", "title": "A Parallel Genetic Algorithm for Three Dimensional Bin Packing with\n  Heterogeneous Bins", "comments": "6 pages, 4 figures", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V17(1):33-38, Nov 2014", "doi": "10.14445/22312803/IJCTT-V17P108", "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a parallel genetic algorithm for three dimensional bin\npacking with heterogeneous bins using Hadoop Map-Reduce framework. The most\ncommon three dimensional bin packing problem which packs given set of boxes\ninto minimum number of equal sized bins is proven to be NP Hard. The variation\nof three dimensional bin packing problem that allows heterogeneous bin sizes\nand rotation of boxes is computationally more harder than common three\ndimensional bin packing problem. The proposed Map-Reduce implementation helps\nto run the genetic algorithm for three dimensional bin packing with\nheterogeneous bins on multiple machines parallely and computes the solution in\nrelatively short time.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:35:02 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Chandu", "Drona Pratap", ""]]}, {"id": "1411.4679", "submitter": "Subodh Paudel", "authors": "S. Paudel, M. Elmtiri, W.L. Kling, O. Le Corre, B. Lacarriere", "title": "Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand\n  Using Artificial Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.enbuild.2013.11.051", "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the building heating demand prediction model with\noccupancy profile and operational heating power level characteristics in short\ntime horizon (a couple of days) using artificial neural network. In addition,\nnovel pseudo dynamic transitional model is introduced, which consider time\ndependent attributes of operational power level characteristics and its effect\nin the overall model performance is outlined. Pseudo dynamic model is applied\nto a case study of French Institution building and compared its results with\nstatic and other pseudo dynamic neural network models. The results show the\ncoefficients of correlation in static and pseudo dynamic neural network model\nof 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning\nphase, and 0.61 and 0.85 during the prediction phase respectively. Further,\northogonal array design is applied to the pseudo dynamic model to check the\nschedule of occupancy profile and operational heating power level\ncharacteristics. The results show the new schedule and provide the robust\ndesign for pseudo dynamic model. Due to prediction in short time horizon, it\nfinds application for Energy Services Company (ESCOs) to manage the heating\nload for dynamic control of heat production system.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 21:40:36 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Paudel", "S.", ""], ["Elmtiri", "M.", ""], ["Kling", "W. L.", ""], ["Corre", "O. Le", ""], ["Lacarriere", "B.", ""]]}, {"id": "1411.4702", "submitter": "Michael Ferrier", "authors": "Michael R. Ferrier", "title": "Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal\n  Memory in Light of Frontal Cortical Function", "comments": "105 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of evidence points toward the existence of a common algorithm\nunderlying the processing of information throughout the cerebral cortex.\nSeveral hypothesized features of this cortical algorithm are reviewed,\nincluding sparse distributed representation, Bayesian inference, hierarchical\norganization composed of alternating template matching and pooling layers,\ntemporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is\na family of learning algorithms and corresponding theories of cortical function\nthat embodies these principles. HTM has previously been applied mainly to\nperceptual tasks typical of posterior cortex. In order to evaluate HTM as a\ncandidate model of cortical function, it is necessary also to investigate its\ncompatibility with the requirements of frontal cortical function. To this end,\na variety of models of frontal cortical function are reviewed and integrated,\nto arrive at the hypothesis that frontal functions including attention, working\nmemory and action selection depend largely upon the same basic algorithms as do\nposterior functions, with the notable additions of a mechanism for the active\nmaintenance of representations and of multiple cortico-striato-thalamo-cortical\nloops that allow communication between regions of frontal cortex to be gated in\nan adaptive manner. Computational models of this system are reviewed. Finally,\nthere is a discussion of how HTM can contribute to the understanding of frontal\ncortical function, and of what the requirements of frontal cortical function\nmean for the future development of HTM.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 00:38:30 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Ferrier", "Michael R.", ""]]}, {"id": "1411.4798", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Fabio L. Traversa, Chiara Ramella, Fabrizio Bonani, Massimiliano Di\n  Ventra", "title": "Memcomputing NP-complete problems in polynomial time using polynomial\n  resources and collective states", "comments": "We have corrected minor typos and improved the presentation", "journal-ref": "Science Advances, Vol. 1, no. 6, pag e1500031, year 2015", "doi": "10.1126/sciadv.1500031", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memcomputing is a novel non-Turing paradigm of computation that uses\ninteracting memory cells (memprocessors for short) to store and process\ninformation on the same physical platform. It was recently proved\nmathematically that memcomputing machines have the same computational power of\nnon-deterministic Turing machines. Therefore, they can solve NP-complete\nproblems in polynomial time and, using the appropriate architecture, with\nresources that only grow polynomially with the input size. The reason for this\ncomputational power stems from properties inspired by the brain and shared by\nany universal memcomputing machine, in particular intrinsic parallelism and\ninformation overhead, namely the capability of compressing information in the\ncollective state of the memprocessor network. Here, we show an experimental\ndemonstration of an actual memcomputing architecture that solves the\nNP-complete version of the subset-sum problem in only one step and is composed\nof a number of memprocessors that scales linearly with the size of the problem.\nWe have fabricated this architecture using standard microelectronic technology\nso that it can be easily realized in any laboratory setting. Even though the\nparticular machine presented here is eventually limited by noise--and will thus\nrequire error-correcting codes to scale to an arbitrary number of\nmemprocessors--it represents the first proof-of-concept of a machine capable of\nworking with the collective state of interacting memory cells, unlike the\npresent-day single-state machines built using the von Neumann architecture.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 10:35:17 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 11:30:43 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 22:41:52 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Traversa", "Fabio L.", ""], ["Ramella", "Chiara", ""], ["Bonani", "Fabrizio", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1411.5053", "submitter": "Vladimir Red'ko", "authors": "Vladimir G. Red'ko", "title": "Model of Interaction between Learning and Evolution", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of interaction between learning and evolutionary optimization is\ndesigned and investigated. The evolving population of modeled organisms is\nconsidered. The mechanism of the genetic assimilation of the acquired features\nduring a number of generations of Darwinian evolution is studied. It is shown\nthat the genetic assimilation takes place as follows: phenotypes of modeled\norganisms move towards the optimum at learning; then the selection takes place;\ngenotypes of selected organisms also move towards the optimum. The hiding\neffect is also studied; this effect means that strong learning can inhibit the\nevolutionary search for the optimal genotype. The mechanism of influence of the\nlearning load on the interaction between learning and evolution is analyzed. It\nis shown that the learning load can lead to a significant acceleration of\nevolution.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 22:13:48 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Red'ko", "Vladimir G.", ""]]}, {"id": "1411.5140", "submitter": "Qian Wang", "authors": "Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang", "title": "Attentional Neural Network: Feature Selection Using Cognitive Feedback", "comments": "Poster in Neural Information Processing Systems (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentional Neural Network is a new framework that integrates top-down\ncognitive bias and bottom-up feature extraction in one coherent architecture.\nThe top-down influence is especially effective when dealing with high noise or\ndifficult segmentation problems. Our system is modular and extensible. It is\nalso easy to train and cheap to run, and yet can accommodate complex behaviors.\nWe obtain classification accuracy better than or competitive with state of art\nresults on the MNIST variation dataset, and successfully disentangle overlaid\ndigits with high success rates. We view such a general purpose framework as an\nessential foundation for a larger system emulating the cognitive abilities of\nthe whole brain.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 08:33:28 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Wang", "Qian", ""], ["Zhang", "Jiaxing", ""], ["Song", "Sen", ""], ["Zhang", "Zheng", ""]]}, {"id": "1411.5458", "submitter": "Subhrajit Roy", "authors": "Subhrajit Roy, Amitava Banerjee and Arindam Basu", "title": "Liquid State Machine with Dendritically Enhanced Readout for Low-power,\n  Neuromorphic VLSI Implementations", "comments": "14 pages, 19 figures, Journal", "journal-ref": "IEEE Transactions on Biomedical Circuits and Systems, vol.8, no.5,\n  pp.681,695, Oct. 2014", "doi": "10.1109/TBCAS.2014.2362969", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a new neuro-inspired, hardware-friendly readout\nstage for the liquid state machine (LSM), a popular model for reservoir\ncomputing. Compared to the parallel perceptron architecture trained by the\np-delta algorithm, which is the state of the art in terms of performance of\nreadout stages, our readout architecture and learning algorithm can attain\nbetter performance with significantly less synaptic resources making it\nattractive for VLSI implementation. Inspired by the nonlinear properties of\ndendrites in biological neurons, our readout stage incorporates neurons having\nmultiple dendrites with a lumped nonlinearity. The number of synaptic\nconnections on each branch is significantly lower than the total number of\nconnections from the liquid neurons and the learning algorithm tries to find\nthe best 'combination' of input connections on each branch to reduce the error.\nHence, the learning involves network rewiring (NRW) of the readout network\nsimilar to structural plasticity observed in its biological counterparts. We\nshow that compared to a single perceptron using analog weights, this\narchitecture for the readout can attain, even by using the same number of\nbinary valued synapses, up to 3.3 times less error for a two-class spike train\nclassification problem and 2.4 times less error for an input rate approximation\ntask. Even with 60 times larger synapses, a group of 60 parallel perceptrons\ncannot attain the performance of the proposed dendritically enhanced readout.\nAn additional advantage of this method for hardware implementations is that the\n'choice' of connectivity can be easily implemented exploiting address event\nrepresentation (AER) protocols commonly used in current neuromorphic systems\nwhere the connection matrix is stored in memory. Also, due to the use of binary\nsynapses, our proposed method is more robust against statistical variations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 07:05:28 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Roy", "Subhrajit", ""], ["Banerjee", "Amitava", ""], ["Basu", "Arindam", ""]]}, {"id": "1411.5731", "submitter": "Suleyman Cetintas", "authors": "Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li", "title": "Visual Sentiment Prediction with Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images have become one of the most popular types of media through which users\nconvey their emotions within online social networks. Although vast amount of\nresearch is devoted to sentiment analysis of textual data, there has been very\nlimited work that focuses on analyzing sentiment of image data. In this work,\nwe propose a novel visual sentiment prediction framework that performs image\nunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, the\nproposed sentiment prediction framework performs transfer learning from a CNN\nwith millions of parameters, which is pre-trained on large-scale data for\nobject recognition. Experiments conducted on two real-world datasets from\nTwitter and Tumblr demonstrate the effectiveness of the proposed visual\nsentiment analysis framework.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:39:43 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Xu", "Can", ""], ["Cetintas", "Suleyman", ""], ["Lee", "Kuang-Chih", ""], ["Li", "Li-Jia", ""]]}, {"id": "1411.5737", "submitter": "Steven Damelin Dr", "authors": "S. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu", "title": "Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications\n  to Clustering and Biclustering", "comments": "Accepted in Math.Model.Nat.Phenom", "journal-ref": "Math.Model.Nat.Phenom. Vol. 10, No 3, 2015, pp. 206-211", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance\nDif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory\nto do clustering on high dimensional data. We describe some applications of\nthis method and some problems for future research.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 01:21:17 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 00:16:54 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 19:56:55 GMT"}, {"version": "v4", "created": "Wed, 18 Feb 2015 00:08:47 GMT"}, {"version": "v5", "created": "Tue, 6 Oct 2015 00:03:43 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Damelin", "S. B.", ""], ["Gu", "Y.", ""], ["Wunsch", "D. C.", "II"], ["Xu", "R.", ""]]}, {"id": "1411.5881", "submitter": "Shaista Hussain", "authors": "Shaista Hussain, Shih-Chii Liu and Arindam Basu", "title": "Hardware-Amenable Structural Learning for Spike-based Pattern\n  Classification using a Simple Model of Active Dendrites", "comments": "Accepted for publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spike-based model which employs neurons with\nfunctionally distinct dendritic compartments for classifying high dimensional\nbinary patterns. The synaptic inputs arriving on each dendritic subunit are\nnonlinearly processed before being linearly integrated at the soma, giving the\nneuron a capacity to perform a large number of input-output mappings. The model\nutilizes sparse synaptic connectivity; where each synapse takes a binary value.\nThe optimal connection pattern of a neuron is learned by using a simple\nhardware-friendly, margin enhancing learning algorithm inspired by the\nmechanism of structural plasticity in biological neurons. The learning\nalgorithm groups correlated synaptic inputs on the same dendritic branch. Since\nthe learning results in modified connection patterns, it can be incorporated\ninto current event-based neuromorphic systems with little overhead. This work\nalso presents a branch-specific spike-based version of this structural\nplasticity rule. The proposed model is evaluated on benchmark binary\nclassification problems and its performance is compared against that achieved\nusing Support Vector Machine (SVM) and Extreme Learning Machine (ELM)\ntechniques. Our proposed method attains comparable performance while utilizing\n10 to 50% less computational resources than the other reported techniques.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 07:50:06 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 08:03:27 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Hussain", "Shaista", ""], ["Liu", "Shih-Chii", ""], ["Basu", "Arindam", ""]]}, {"id": "1411.5908", "submitter": "Karel Lenc", "authors": "Karel Lenc, Andrea Vedaldi", "title": "Understanding image representations by measuring their equivariance and\n  equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of image representations such as histograms of\noriented gradients and deep Convolutional Neural Networks (CNN), our\ntheoretical understanding of them remains limited. Aiming at filling this gap,\nwe investigate three key mathematical properties of representations:\nequivariance, invariance, and equivalence. Equivariance studies how\ntransformations of the input image are encoded by the representation,\ninvariance being a special case where a transformation has no effect.\nEquivalence studies whether two representations, for example two different\nparametrisations of a CNN, capture the same visual information or not. A number\nof methods to establish these properties empirically are proposed, including\nintroducing transformation and stitching layers in CNNs. These methods are then\napplied to popular representations to reveal insightful aspects of their\nstructure, including clarifying at which layers in a CNN certain geometric\ninvariances are achieved. While the focus of the paper is theoretical, direct\napplications to structured-output regression are demonstrated too.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 15:14:42 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2015 18:35:37 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1411.5928", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko and\n  Thomas Brox", "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks", "comments": "v4: final PAMI version. New architecture figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train generative 'up-convolutional' neural networks which are able to\ngenerate images of objects given object style, viewpoint, and color. We train\nthe networks on rendered 3D models of chairs, tables, and cars. Our experiments\nshow that the networks do not merely learn all images by heart, but rather find\na meaningful representation of 3D models allowing them to assess the similarity\nof different models, interpolate between given views to generate the missing\nones, extrapolate views, and invent new objects not present in the training set\nby recombining training instances, or even two different object classes.\nMoreover, we show that such generative networks can be used to find\ncorrespondences between different objects from the dataset, outperforming\nexisting approaches on this task.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 16:01:04 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 12:31:49 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 09:49:23 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 20:53:43 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Springenberg", "Jost Tobias", ""], ["Tatarchenko", "Maxim", ""], ["Brox", "Thomas", ""]]}, {"id": "1411.6191", "submitter": "David Balduzzi", "authors": "David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann", "title": "Kickback cuts Backprop's red-tape: Biologically plausible credit\n  assignment in neural networks", "comments": "7 pages. To appear, AAAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error backpropagation is an extremely effective algorithm for assigning\ncredit in artificial neural networks. However, weight updates under Backprop\ndepend on lengthy recursive computations and require separate output and error\nmessages -- features not shared by biological neurons, that are perhaps\nunnecessary. In this paper, we revisit Backprop and the credit assignment\nproblem. We first decompose Backprop into a collection of interacting learning\nalgorithms; provide regret bounds on the performance of these sub-algorithms;\nand factorize Backprop's error signals. Using these results, we derive a new\ncredit assignment algorithm for nonparametric regression, Kickback, that is\nsignificantly simpler than Backprop. Finally, we provide a sufficient condition\nfor Kickback to follow error gradients, and show that Kickback matches\nBackprop's performance on real-world regression benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 04:58:22 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Balduzzi", "David", ""], ["Vanchinathan", "Hastagiri", ""], ["Buhmann", "Joachim", ""]]}, {"id": "1411.6369", "submitter": "Yichong Xu", "authors": "Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang", "title": "Scale-Invariant Convolutional Neural Networks", "comments": "This paper is submitted for CVPR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though convolutional neural networks (CNN) has achieved near-human\nperformance in various computer vision tasks, its ability to tolerate scale\nvariations is limited. The popular practise is making the model bigger first,\nand then train it with data augmentation using extensive scale-jittering. In\nthis paper, we propose a scaleinvariant convolutional neural network (SiCNN), a\nmodeldesigned to incorporate multi-scale feature exaction and classification\ninto the network structure. SiCNN uses a multi-column architecture, with each\ncolumn focusing on a particular scale. Unlike previous multi-column strategies,\nthese columns share the same set of filter parameters by a scale transformation\namong them. This design deals with scale variation without blowing up the model\nsize. Experimental results show that SiCNN detects features at various scales,\nand the classification result exhibits strong robustness against object scale\nvariations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:21 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Xu", "Yichong", ""], ["Xiao", "Tianjun", ""], ["Zhang", "Jiaxing", ""], ["Yang", "Kuiyuan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1411.6757", "submitter": "N. Michael Mayer", "authors": "Norbert Michael Mayer", "title": "Echo State Condition at the Critical Point", "comments": null, "journal-ref": "Entropy 2017, 19(1), 3", "doi": "10.3390/e19010003", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent networks with transfer functions that fulfill the Lipschitz\ncontinuity with K=1 may be echo state networks if certain limitations on the\nrecurrent connectivity are applied. It has been shown that it is sufficient if\nthe largest singular value of the recurrent connectivity is smaller than 1. The\nmain achievement of this paper is a proof under which conditions the network is\nan echo state network even if the largest singular value is one. It turns out\nthat in this critical case the exact shape of the transfer function plays a\ndecisive role in determining whether the network still fulfills the echo state\ncondition. In addition, several examples with one neuron networks are outlined\nto illustrate effects of critical connectivity. Moreover, within the manuscript\na mathematical definition for a critical echo state network is suggested.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 08:09:43 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 05:16:24 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 07:32:15 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2015 10:04:41 GMT"}, {"version": "v5", "created": "Thu, 26 Nov 2015 07:06:20 GMT"}, {"version": "v6", "created": "Mon, 22 Feb 2016 09:21:12 GMT"}, {"version": "v7", "created": "Tue, 29 Mar 2016 10:33:01 GMT"}, {"version": "v8", "created": "Wed, 26 Oct 2016 11:37:34 GMT"}, {"version": "v9", "created": "Mon, 26 Dec 2016 05:06:40 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Mayer", "Norbert Michael", ""]]}, {"id": "1411.6768", "submitter": "Yuriy Parzhin", "authors": "Yuri Parzhin", "title": "Hypotheses of neural code and the information model of the\n  neuron-detector", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of neural code solving. On the basis of the\nformulated hypotheses the information model of a neuron-detector is suggested,\nthe detector being one of the basic elements of an artificial neural network\n(ANN). The paper subjects the connectionist paradigm of ANN building to\ncriticism and suggests a new presentation paradigm for ANN building and\nneuroelements (NE) learning. The adequacy of the suggested model is proved by\nthe fact that is does not contradict the modern propositions of neuropsychology\nand neurophysiology.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 08:52:14 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Parzhin", "Yuri", ""]]}, {"id": "1411.6912", "submitter": "Julien Hubert", "authors": "Julien Hubert and Takashi Ikegami", "title": "Short-Term Memory Through Persistent Activity: Evolution of\n  Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memories in the brain are separated in two categories: short-term and\nlong-term memories. Long-term memories remain for a lifetime, while short-term\nones exist from a few milliseconds to a few minutes. Within short-term memory\nstudies, there is debate about what neural structure could implement it.\nIndeed, mechanisms responsible for long-term memories appear inadequate for the\ntask. Instead, it has been proposed that short-term memories could be sustained\nby the persistent activity of a group of neurons. In this work, we explore what\ntopology could sustain short-term memories, not by designing a model from\nspecific hypotheses, but through Darwinian evolution in order to obtain new\ninsights into its implementation. We evolved 10 networks capable of retaining\ninformation for a fixed duration between 2 and 11s. Our main finding has been\nthat the evolution naturally created two functional modules in the network: one\nwhich sustains the information containing primarily excitatory neurons, while\nthe other, which is responsible for forgetting, was composed mainly of\ninhibitory neurons. This demonstrates how the balance between inhibition and\nexcitation plays an important role in cognition.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 16:32:14 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Hubert", "Julien", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1411.6998", "submitter": "Diego Arenas", "authors": "Diego Arenas (IFSTTAR/COSYS/ESTAS, LAMIH), Remy Chevirer\n  (IFSTTAR/COSYS/ESTAS), Said Hanafi (LAMIH), Joaquin Rodriguez\n  (IFSTTAR/COSYS/ESTAS)", "title": "Solving the Periodic Timetabling Problem using a Genetic Algorithm", "comments": "XVIII Congreso Panamericano de Ingenier\\'ia de Transito, Transporte y\n  Logistica (PANAM 2014), Jun 2014, Santander, Spain.\n  http://www.panam2014.unican.es", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In railway operations, a timetable is established to determine the departure\nand arrival times for the trains or other rolling stock at the different\nstations or relevant points inside the rail network or a subset of this\nnetwork. The elaboration of this timetable is done to respond to the commercial\nrequirements for both passenger and freight traffic, but also it must respect a\nset of security and capacity constraints associated with the railway network,\nrolling stock and legislation. Combining these requirements and constraints, as\nwell as the important number of trains and schedules to plan, makes the\npreparation of a feasible timetable a complex and time-consuming process, that\nnormally takes several months to be completed. This article addresses the\nproblem of generating periodic timetables, which means that the involved trains\noperate in a recurrent pattern. For instance, the trains belonging to the same\ntrain line, depart from some station every 15 minutes or one hour. To tackle\nthe problem, we present a constraint-based model suitable for this kind of\nproblem. Then, we propose a genetic algorithm, allowing a rapid generation of\nfeasible periodic timetables. Finally, two case studies are presented, the\nfirst, describing a sub-set of the Netherlands rail network, and the second a\nlarge portion of the Nord-pas-de-Calais regional rail network, both of them are\nthen solved using our algorithm and the results are presented and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 15:24:25 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Arenas", "Diego", "", "IFSTTAR/COSYS/ESTAS, LAMIH"], ["Chevirer", "Remy", "", "IFSTTAR/COSYS/ESTAS"], ["Hanafi", "Said", "", "LAMIH"], ["Rodriguez", "Joaquin", "", "IFSTTAR/COSYS/ESTAS"]]}, {"id": "1411.7494", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter", "title": "An Evolutionary Optimization Approach to Risk Parity Portfolio Selection", "comments": null, "journal-ref": "Lecture Notes in Computer Science Volume 9028: 279-288. 2015", "doi": "10.1007/978-3-319-16549-3_23", "report-no": null, "categories": "q-fin.PM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an evolutionary optimization approach to solve the\nrisk parity portfolio selection problem. While there exist convex optimization\napproaches to solve this problem when long-only portfolios are considered, the\noptimization problem becomes non-trivial in the long-short case. To solve this\nproblem, we propose a genetic algorithm as well as a local search heuristic.\nThis algorithmic framework is able to compute solutions successfully. Numerical\nresults using real-world data substantiate the practicability of the approach\npresented in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 08:26:09 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 09:33:57 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Hochreiter", "Ronald", ""]]}, {"id": "1411.7542", "submitter": "Malte Probst", "authors": "Malte Probst and Franz Rothlauf and J\\\"orn Grahl", "title": "Scalability of using Restricted Boltzmann Machines for Combinatorial\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of Distribution Algorithms (EDAs) require flexible probability\nmodels that can be efficiently learned and sampled. Restricted Boltzmann\nMachines (RBMs) are generative neural networks with these desired properties.\nWe integrate an RBM into an EDA and evaluate the performance of this system in\nsolving combinatorial optimization problems with a single objective. We assess\nhow the number of fitness evaluations and the CPU time scale with problem size\nand with problem complexity. The results are compared to the Bayesian\nOptimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires\nlarger population sizes and a larger number of fitness evaluations, it\noutperforms BOA in terms of CPU times, in particular if the problem is large or\ncomplex. RBM-EDA requires less time for model building than BOA. These results\nhighlight the potential of using generative neural networks for combinatorial\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 10:49:19 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Probst", "Malte", ""], ["Rothlauf", "Franz", ""], ["Grahl", "J\u00f6rn", ""]]}, {"id": "1411.7612", "submitter": "Drona Pratap Chandu", "authors": "Drona Pratap Chandu", "title": "A Parallel Genetic Algorithm for Generalized Vertex Cover Problem", "comments": "4 pages, 3 figures, ISSN: 0975-9646. arXiv admin note: substantial\n  text overlap with arXiv:1411.4565", "journal-ref": "International Journal of Computer Science and Information\n  Technologies (IJCSIT), Vol. 5 (6) , 2014, 7686-7689", "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a parallel genetic algorithm for generalised vertex cover\nproblem (GVCP) using Hadoop Map-Reduce framework. The proposed Map-Reduce\nimplementation helps to run the genetic algorithm for generalized vertex cover\nproblem (GVCP) on multiple machines parallely and computes the solution in\nrelatively short time.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 14:39:43 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Chandu", "Drona Pratap", ""]]}, {"id": "1411.7783", "submitter": "Harri Valpola", "authors": "Harri Valpola", "title": "From neural PCA to deep unsupervised learning", "comments": "A revised version of an article that has been accepted for\n  publication in Advances in Independent Component Analysis and Learning\n  Machines (2015), edited by Ella Bingham, Samuel Kaski, Jorma Laaksonen and\n  Jouko Lampinen", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network supporting deep unsupervised learning is presented. The network is\nan autoencoder with lateral shortcut connections from the encoder to decoder at\neach level of the hierarchy. The lateral shortcut connections allow the higher\nlevels of the hierarchy to focus on abstract invariant features. While standard\nautoencoders are analogous to latent variable models with a single layer of\nstochastic variables, the proposed network is analogous to hierarchical latent\nvariables models. Learning combines denoising autoencoder and denoising sources\nseparation frameworks. Each layer of the network contributes to the cost\nfunction a term which measures the distance of the representations produced by\nthe encoder and the decoder. Since training signals originate from all levels\nof the network, all layers can learn efficiently even in deep networks. The\nspeedup offered by cost terms from higher levels of the hierarchy and the\nability to learn invariant features are demonstrated in experiments.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 09:03:24 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 12:58:05 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Valpola", "Harri", ""]]}, {"id": "1411.7806", "submitter": "Martin Hole\\v{n}a", "authors": "Luk\\'a\\v{s} Bajer, Martin Hole\\v{n}a", "title": "Two Gaussian Approaches to Black-Box Optomization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Outline of several strategies for using Gaussian processes as surrogate\nmodels for the covariance matrix adaptation evolution strategy (CMA-ES).\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 10:39:24 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Bajer", "Luk\u00e1\u0161", ""], ["Hole\u0148a", "Martin", ""]]}]