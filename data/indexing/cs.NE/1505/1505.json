[{"id": "1505.00075", "submitter": "Shaoqiu Zheng", "authors": "Shaoqiu Zheng, Junzhi Li, Andreas Janecek, Ying Tan", "title": "A Cooperative Framework for Fireworks Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cooperative framework for fireworks algorithm (CoFFWA).\nA detailed analysis of existing fireworks algorithm (FWA) and its recently\ndeveloped variants has revealed that (i) the selection strategy lead to the\ncontribution of the firework with the best fitness (core firework) for the\noptimization overwhelms the contributions of the rest of fireworks (non-core\nfireworks) in the explosion operator, (ii) the Gaussian mutation operator is\nnot as effective as it is designed to be. To overcome these limitations, the\nCoFFWA is proposed, which can greatly enhance the exploitation ability of\nnon-core fireworks by using independent selection operator and increase the\nexploration capacity by crowdness-avoiding cooperative strategy among the\nfireworks. Experimental results on the CEC2013 benchmark functions suggest that\nCoFFWA outperforms the state-of-the-art FWA variants, artificial bee colony,\ndifferential evolution, the standard particle swarm optimization (SPSO) in 2007\nand the most recent SPSO in 2011 in term of convergence performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 02:56:42 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Zheng", "Shaoqiu", ""], ["Li", "Junzhi", ""], ["Janecek", "Andreas", ""], ["Tan", "Ying", ""]]}, {"id": "1505.00384", "submitter": "Abhinav Tushar", "authors": "Abhinav Tushar", "title": "Making Sense of Hidden Layer Information in Deep Networks by Learning\n  Hierarchical Targets", "comments": "Updated to add a note with commentary on original (v1) submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an architecture for deep neural networks with hidden\nlayer branches that learn targets of lower hierarchy than final layer targets.\nThe branches provide a channel for enforcing useful information in hidden layer\nwhich helps in attaining better accuracy, both for the final layer and hidden\nlayers. The shared layers modify their weights using the gradients of all cost\nfunctions higher than the branching layer. This model provides a flexible\ninference system with many levels of targets which is modular and can be used\nefficiently in situations requiring different levels of results according to\ncomplexity. This paper applies the idea to a text classification task on 20\nNewsgroups data set with two level of hierarchical targets and a comparison is\nmade with training without the use of hidden layer branches.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 00:58:38 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 07:31:47 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Tushar", "Abhinav", ""]]}, {"id": "1505.00387", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Klaus Greff, J\\\"urgen Schmidhuber", "title": "Highway Networks", "comments": "6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop.\n  Full paper is at arXiv:1507.06228", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is plenty of theoretical and empirical evidence that depth of neural\nnetworks is a crucial ingredient for their success. However, network training\nbecomes more difficult with increasing depth and training of very deep networks\nremains an open problem. In this extended abstract, we introduce a new\narchitecture designed to ease gradient-based training of very deep networks. We\nrefer to networks with this architecture as highway networks, since they allow\nunimpeded information flow across several layers on \"information highways\". The\narchitecture is characterized by the use of gating units which learn to\nregulate the flow of information through a network. Highway networks with\nhundreds of layers can be trained directly using stochastic gradient descent\nand with a variety of activation functions, opening up the possibility of\nstudying extremely deep and efficient architectures.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 01:56:57 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 18:15:15 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Greff", "Klaus", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1505.00444", "submitter": "Stephen Luttrell", "authors": "Stephen Luttrell", "title": "Some Theoretical Properties of a Network of Discretely Firing Neurons", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimising a network of discretely firing neurons is\naddressed. An objective function is introduced which measures the average\nnumber of bits that are needed for the network to encode its state. When this\nis minimised, it is shown that this leads to a number of results, such as\ntopographic mappings, piecewise linear dependence on the input of the\nprobability of a neuron firing, and factorial encoder networks.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 16:11:26 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Luttrell", "Stephen", ""]]}, {"id": "1505.00936", "submitter": "Kai Lars Polsterer", "authors": "Nikolaos Gianniotis, Dennis K\\\"ugler, Peter Tino, Kai Polsterer,\n  Ranjeev Misra", "title": "Autoencoding Time Series for Visualisation", "comments": "Published in ESANN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for the visualisation of time series. To that end we\nemploy echo state networks to convert time series into a suitable vector\nrepresentation which is capable of capturing the latent dynamics of the time\nseries. Subsequently, the obtained vector representations are put through an\nautoencoder and the visualisation is constructed using the activations of the\nbottleneck. The crux of the work lies with defining an objective function that\nquantifies the reconstruction error of these representations in a principled\nmanner. We demonstrate the method on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 09:45:20 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Gianniotis", "Nikolaos", ""], ["K\u00fcgler", "Dennis", ""], ["Tino", "Peter", ""], ["Polsterer", "Kai", ""], ["Misra", "Ranjeev", ""]]}, {"id": "1505.01474", "submitter": "Robyn Ffrancon", "authors": "Robyn Ffrancon", "title": "Retaining Experience and Growing Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, when genetic programming (GP) is used for function synthesis any\nvaluable experience gained by the system is lost from one problem to the next,\neven when the problems are closely related. With the aim of developing a system\nwhich retains beneficial experience from problem to problem, this paper\nintroduces the novel Node-by-Node Growth Solver (NNGS) algorithm which features\na component, called the controller, which can be adapted and improved for use\nacross a set of related problems. NNGS grows a single solution tree from root\nto leaves. Using semantic backpropagation and acting locally on each node in\nturn, the algorithm employs the controller to assign subsequent child nodes\nuntil a fully formed solution is generated.\n  The aim of this paper is to pave a path towards the use of a neural network\nas the controller component and also, separately, towards the use of meta-GP as\na mechanism for improving the controller component. A proof-of-concept\ncontroller is discussed which demonstrates the success and potential of the\nNNGS algorithm. In this case, the controller constitutes a set of hand written\nrules which can be used to deterministically and greedily solve standard\nBoolean function synthesis benchmarks. Even before employing machine learning\nto improve the controller, the algorithm vastly outperforms other well known\nrecent algorithms on run times, maintains comparable solution sizes, and has a\n100% success rate on all Boolean function synthesis benchmarks tested so far.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 19:40:26 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Ffrancon", "Robyn", ""]]}, {"id": "1505.01504", "submitter": "Hui Jiang", "authors": "Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai", "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its\n  Application to Neural Network Language Models", "comments": "7 pages, 4 figures, Technical report (A shorter version will appear\n  in ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the new fixed-size ordinally-forgetting encoding\n(FOFE) method, which can almost uniquely encode any variable-length sequence of\nwords into a fixed-size representation. FOFE can model the word order in a\nsequence using a simple ordinally-forgetting mechanism according to the\npositions of words. In this work, we have applied FOFE to feedforward neural\nnetwork language models (FNN-LMs). Experimental results have shown that without\nusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform\nnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 20:14:25 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 18:41:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhang", "Shiliang", ""], ["Jiang", "Hui", ""], ["Xu", "Mingbin", ""], ["Hou", "Junfeng", ""], ["Dai", "Lirong", ""]]}, {"id": "1505.01596", "submitter": "Pulkit Agrawal", "authors": "Pulkit Agrawal, Joao Carreira, Jitendra Malik", "title": "Learning to See by Moving", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant paradigm for feature learning in computer vision relies on\ntraining neural networks for the task of object recognition using millions of\nhand labelled images. Is it possible to learn useful features for a diverse set\nof visual tasks using any other form of supervision? In biology, living\norganisms developed the ability of visual perception for the purpose of moving\nand acting in the world. Drawing inspiration from this observation, in this\nwork we investigate if the awareness of egomotion can be used as a supervisory\nsignal for feature learning. As opposed to the knowledge of class labels,\ninformation about egomotion is freely available to mobile agents. We show that\ngiven the same number of training images, features learnt using egomotion as\nsupervision compare favourably to features learnt using class-label as\nsupervision on visual tasks of scene recognition, object recognition, visual\nodometry and keypoint matching.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 06:03:01 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 16:59:36 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Agrawal", "Pulkit", ""], ["Carreira", "Joao", ""], ["Malik", "Jitendra", ""]]}, {"id": "1505.01749", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "Object detection via a multi-region & semantic segmentation-aware CNN\n  model", "comments": "Extended technical report -- short version to appear at ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an object detection system that relies on a multi-region deep\nconvolutional neural network (CNN) that also encodes semantic\nsegmentation-aware features. The resulting CNN-based representation aims at\ncapturing a diverse set of discriminative appearance factors and exhibits\nlocalization sensitivity that is essential for accurate object localization. We\nexploit the above properties of our recognition module by integrating it on an\niterative localization mechanism that alternates between scoring a box proposal\nand refining its location with a deep CNN regression model. Thanks to the\nefficient use of our modules, we detect objects with very high localization\naccuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we\nachieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published\nwork by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 15:42:07 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 16:49:44 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 22:24:42 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1505.01887", "submitter": "Francisco Chicano", "authors": "Darrell Whitley, Renato Tin\\'os and Francisco Chicano", "title": "Optimal Neuron Selection: NK Echo State Networks for Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the NK Echo State Network. The problem of learning in\nthe NK Echo State Network is reduced to the problem of optimizing a special\nform of a Spin Glass Problem known as an NK Landscape. No weight adjustment is\nused; all learning is accomplished by spinning up (turning on) or spinning down\n(turning off) neurons in order to find a combination of neurons that work\ntogether to achieve the desired computation. For special types of NK\nLandscapes, an exact global solution can be obtained in polynomial time using\ndynamic programming. The NK Echo State Network is applied to a reinforcement\nlearning problem requiring a recurrent network: balancing two poles on a cart\ngiven no velocity information. Empirical results shows that the NK Echo State\nNetwork learns very rapidly and yields very good generalization.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 22:58:28 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Whitley", "Darrell", ""], ["Tin\u00f3s", "Renato", ""], ["Chicano", "Francisco", ""]]}, {"id": "1505.01980", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Evolving Boolean Networks with RNA Editing", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.4793,\n  arXiv:1303.7220", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.MN q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The editing of transcribed RNA by other molecules such that the form of the\nfinal product differs from that specified in the corresponding DNA sequence is\nubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory\nnetwork model to explore aspects of RNA editing. In particular, it is shown how\ndynamically altering expressed sequences via a guide RNA-inspired mechanism can\nbe selected for by simulated evolution under various single and multicellular\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 10:20:06 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1505.02142", "submitter": "Sebastian Billaudelle", "authors": "Sebastian Billaudelle, Subutai Ahmad", "title": "Porting HTM Models to the Heidelberg Neuromorphic Computing Platform", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Temporal Memory (HTM) is a computational theory of machine\nintelligence based on a detailed study of the neocortex. The Heidelberg\nNeuromorphic Computing Platform, developed as part of the Human Brain Project\n(HBP), is a mixed-signal (analog and digital) large-scale platform for modeling\nnetworks of spiking neurons. In this paper we present the first effort in\nporting HTM networks to this platform. We describe a framework for simulating\nkey HTM operations using spiking network models. We then describe specific\nspatial pooling and temporal memory implementations, as well as simulations\ndemonstrating that the fundamental properties are maintained. We discuss issues\nin implementing the full set of plasticity rules using Spike-Timing Dependent\nPlasticity (STDP), and rough place and route calculations. Although further\nwork is required, our initial studies indicate that it should be possible to\nrun large-scale HTM networks (including plasticity rules) efficiently on the\nHeidelberg platform. More generally the exercise of porting high level HTM\nalgorithms to biophysical neuron models promises to be a fruitful area of\ninvestigation for future studies.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 19:18:07 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 13:23:40 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Billaudelle", "Sebastian", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1505.02462", "submitter": "Taichi Kiwaki Mr", "authors": "Taichi Kiwaki", "title": "Soft-Deep Boltzmann Machines", "comments": "Major revision after bug fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a layered Boltzmann machine (BM) that can better exploit the\nadvantages of a distributed representation. It is widely believed that deep BMs\n(DBMs) have far greater representational power than its shallow counterpart,\nrestricted Boltzmann machines (RBMs). However, this expectation on the\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\nfashion. In this paper, we provide both theoretical and empirical evidences\nthat the representational power of DBMs can be actually rather limited in\ntaking advantages of distributed representations. We propose an approximate\nmeasure for the representational power of a BM regarding to the efficiency of a\ndistributed representation. With this measure, we show a surprising fact that\nDBMs can make inefficient use of distributed representations. Based on these\nobservations, we propose an alternative BM architecture, which we dub soft-deep\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\noutperform several state-of-the-art models, including DBMs, in generative tasks\non binarized MNIST and Caltech-101 silhouettes.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 00:54:43 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 18:58:39 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 01:41:46 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kiwaki", "Taichi", ""]]}, {"id": "1505.02495", "submitter": "Chetan Singh Thakur", "authors": "Chetan Singh Thakur, Runchun Wang, Saeed Afshar, Gregory Cohen, Tara\n  Julia Hamilton, Jonathan Tapson and Andre van Schaik", "title": "An Online Learning Algorithm for Neuromorphic Hardware Implementation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a sign-based online learning (SOL) algorithm for a neuromorphic\nhardware framework called Trainable Analogue Block (TAB). The TAB framework\nutilises the principles of neural population coding, implying that it encodes\nthe input stimulus using a large pool of nonlinear neurons. The SOL algorithm\nis a simple weight update rule that employs the sign of the hidden layer\nactivation and the sign of the output error, which is the difference between\nthe target output and the predicted output. The SOL algorithm is easily\nimplementable in hardware, and can be used in any artificial neural network\nframework that learns weights by minimising a convex cost function. We show\nthat the TAB framework can be trained for various regression tasks using the\nSOL algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 06:23:15 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 14:43:57 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Thakur", "Chetan Singh", ""], ["Wang", "Runchun", ""], ["Afshar", "Saeed", ""], ["Cohen", "Gregory", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""]]}, {"id": "1505.02581", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub Mikolaj Tomczak", "title": "Improving neural networks with bunches of neurons modeled by Kumaraswamy\n  units: Preliminary study", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved state-of-the-art results in many\nmachine learning problems, e.g., speech recognition or object recognition.\nHitherto, work on rectified linear units (ReLU) provides empirical and\ntheoretical evidence on performance increase of neural networks comparing to\ntypically used sigmoid activation function. In this paper, we investigate a new\nmanner of improving neural networks by introducing a bunch of copies of the\nsame neuron modeled by the generalized Kumaraswamy distribution. As a result,\nwe propose novel non-linear activation function which we refer to as\nKumaraswamy unit which is closely related to ReLU. In the experimental study\nwith MNIST image corpora we evaluate the Kumaraswamy unit applied to\nsingle-layer (shallow) neural network and report a significant drop in test\nclassification error and test cross-entropy in comparison to sigmoid unit, ReLU\nand Noisy ReLU.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 12:14:40 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Tomczak", "Jakub Mikolaj", ""]]}, {"id": "1505.03236", "submitter": "Jensi", "authors": "R. Jensi and G. Wiselin Jiji", "title": "Hybrid data clustering approach using K-Means and Flower Pollination\n  Algorithm", "comments": "11 pages, Journal. Advanced Computational Intelligence: An\n  International Journal (ACII), Vol.2, No.2, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a technique for clustering set of objects into known\nnumber of groups. Several approaches are widely applied to data clustering so\nthat objects within the clusters are similar and objects in different clusters\nare far away from each other. K-Means, is one of the familiar center based\nclustering algorithms since implementation is very easy and fast convergence.\nHowever, K-Means algorithm suffers from initialization, hence trapped in local\noptima. Flower Pollination Algorithm (FPA) is the global optimization\ntechnique, which avoids trapping in local optimum solution. In this paper, a\nnovel hybrid data clustering approach using Flower Pollination Algorithm and\nK-Means (FPAKM) is proposed. The proposed algorithm results are compared with\nK-Means and FPA on eight datasets. From the experimental results, FPAKM is\nbetter than FPA and K-Means.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 04:24:50 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Jensi", "R.", ""], ["Jiji", "G. Wiselin", ""]]}, {"id": "1505.03654", "submitter": "Sho Sonoda", "authors": "Sho Sonoda and Noboru Murata", "title": "Neural Network with Unbounded Activation Functions is Universal\n  Approximator", "comments": "under review; first revised version", "journal-ref": "Applied and Computational Harmonic Analysis, 43(2):233-268, 2017", "doi": "10.1016/j.acha.2015.12.005", "report-no": null, "categories": "cs.NE cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an investigation of the approximation property of neural\nnetworks with unbounded activation functions, such as the rectified linear unit\n(ReLU), which is the new de-facto standard of deep learning. The ReLU network\ncan be analyzed by the ridgelet transform with respect to Lizorkin\ndistributions. By showing three reconstruction formulas by using the Fourier\nslice theorem, the Radon transform, and Parseval's relation, it is shown that a\nneural network with unbounded activation functions still satisfies the\nuniversal approximation property. As an additional consequence, the ridgelet\ntransform, or the backprojection filter in the Radon domain, is what the\nnetwork learns after backpropagation. Subject to a constructive admissibility\ncondition, the trained network can be obtained by simply discretizing the\nridgelet transform, without backpropagation. Numerical examples not only\nsupport the consistency of the admissibility condition but also imply that some\nnon-admissible cases result in low-pass filtering.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 09:03:19 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 21:07:19 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Sonoda", "Sho", ""], ["Murata", "Noboru", ""]]}, {"id": "1505.03703", "submitter": "Yanhai Gan", "authors": "Yanhai Gan, Jun Liu, Junyu Dong, Guoqiang Zhong", "title": "A PCA-Based Convolutional Network", "comments": "8 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised deep learning model, called\nPCA-based Convolutional Network (PCN). The architecture of PCN is composed of\nseveral feature extraction stages and a nonlinear output stage. Particularly,\neach feature extraction stage includes two layers: a convolutional layer and a\nfeature pooling layer. In the convolutional layer, the filter banks are simply\nlearned by PCA. In the nonlinear output stage, binary hashing is applied. For\nthe higher convolutional layers, the filter banks are learned from the feature\nmaps that were obtained in the previous stage. To test PCN, we conducted\nextensive experiments on some challenging tasks, including handwritten digits\nrecognition, face recognition and texture classification. The results show that\nPCN performs competitive with or even better than state-of-the-art deep\nlearning models. More importantly, since there is no back propagation for\nsupervised finetuning, PCN is much more efficient than existing deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 12:35:19 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Gan", "Yanhai", ""], ["Liu", "Jun", ""], ["Dong", "Junyu", ""], ["Zhong", "Guoqiang", ""]]}, {"id": "1505.03917", "submitter": "Jascha A. Schewtschenko", "authors": "Jascha A. Schewtschenko", "title": "General Riemannian SOM", "comments": "175 pages, 46 figures, Diploma thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kohonen's Self-Organizing Maps (SOMs) have proven to be a successful\ndata-reduction method to identify the intrinsic lower-dimensional sub-manifold\nof a data set that is scattered in the higher-dimensional feature space.\nMotivated by the possibly non-Euclidian nature of the feature space and of the\nintrinsic geometry of the data set, we extend the definition of classic SOMs to\nobtain the General Riemannian SOM (GRiSOM). We additionally provide an\nimplementation as a proof-of-concept for geometries with constant curvature. We\nfurthermore perform the analytic and numerical analysis of the stability limits\nof certain (GRi)SOM configurations covering the different possible regular\ntessellation of the map space in each geometry. A deviation between the\nnumerical and analytic stability limit has been observed for the square and\nhexagonal Euclidean maps for very small neighbourhoods in the map space as well\nas agreement in case of longer-ranged relations between the map nodes.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 23:21:40 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Schewtschenko", "Jascha A.", ""]]}, {"id": "1505.04150", "submitter": "Zhipeng Wang", "authors": "Zhipeng Wang and Mingbo Cai", "title": "Reinforcement Learning applied to Single Neuron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper extends the reinforcement learning ideas into the multi-agents\nsystem, which is far more complicated than the previously studied single-agent\nsystem. We studied two different multi-agents systems. One is the\nfully-connected neural network consists of multiple single neurons. Another one\nis the simplified mechanical arm system which is controlled by multiple\nneurons. We suppose that each neuron is like an agent and it can do Gibbs\nsampling of the posterior probability of stimulus features. The policy is\noptimized in a way that the cumulative global rewards are maximized. The\nalgorithm for the second system is based on the same idea but we incorporate\nthe physics model into the constraints. The simulation results show that for\nthe first system our algorithm converges well. For the second system it does\nnot converge well in a reasonable simulation time length. In summary, we took\nthe initial endeavor to study the reinforcement learning for multi-agents\nsystem. The computational complexity is always an issue and significant amount\nof works have to be done in order to better understand the problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 18:36:20 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Wang", "Zhipeng", ""], ["Cai", "Mingbo", ""]]}, {"id": "1505.04211", "submitter": "John Loverich", "authors": "John Loverich", "title": "Discontinuous Piecewise Polynomial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial neural network is presented based on the idea of connections\nbetween units that are only active for a specific range of input values and\nzero outside that range (and so are not evaluated outside the active range).\nThe connection function is represented by a polynomial with compact support.\nThe finite range of activation allows for great activation sparsity in the\nnetwork and means that theoretically you are able to add computational power to\nthe network without increasing the computational time required to evaluate the\nnetwork for a given input. The polynomial order ranges from first to fifth\norder. Unit dropout is used for regularization and a parameter free weight\nupdate is used. Better performance is obtained by moving from piecewise linear\nconnections to piecewise quadratic, even better performance can be obtained by\nmoving to higher order polynomials. The algorithm is tested on the MAGIC Gamma\nray data set as well as the MNIST data set.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:21:39 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 18:58:11 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Loverich", "John", ""]]}, {"id": "1505.04357", "submitter": "Gerard Howard", "authors": "Gerard David Howard, Larry Bull, Ben de Lacy Costello, Andrew\n  Adamatzky, Ella Gale", "title": "Evolving Spiking Networks with Variable Resistive Memories", "comments": "27 pages", "journal-ref": "Evolutionary Computation, Spring 2014, Vol. 22, No. 1, Pages\n  79-103 Posted Online February 7, 2014", "doi": "10.1162/EVCO_a_00103", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing is a brainlike information processing paradigm that\nrequires adaptive learning mechanisms. A spiking neuro-evolutionary system is\nused for this purpose; plastic resistive memories are implemented as synapses\nin spiking neural networks. The evolutionary design process exploits parameter\nself-adaptation and allows the topology and synaptic weights to be evolved for\neach network in an autonomous manner. Variable resistive memories are the focus\nof this research; each synapse has its own conductance profile which modifies\nthe plastic behaviour of the device and may be altered during evolution. These\nvariable resistive networks are evaluated on a noisy robotic dynamic-reward\nscenario against two static resistive memories and a system containing standard\nconnections only. Results indicate that the extra behavioural degrees of\nfreedom available to the networks incorporating variable resistive memories\nenable them to outperform the comparative synapse types.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 05:23:07 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Howard", "Gerard David", ""], ["Bull", "Larry", ""], ["Costello", "Ben de Lacy", ""], ["Adamatzky", "Andrew", ""], ["Gale", "Ella", ""]]}, {"id": "1505.04518", "submitter": "Christopher Marriott", "authors": "Chris Marriott and Jobran Chebib", "title": "Emergence-focused design in complex system simulation", "comments": "European Conference on Artificial Life 2015 - York, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergence is a phenomenon taken for granted in science but also still not\nwell understood. We have developed a model of artificial genetic evolution\nintended to allow for emergence on genetic, population and social levels. We\npresent the details of the current state of our environment, agent, and\nreproductive models. In developing our models we have relied on a principle of\nusing non-linear systems to model as many systems as possible including\nmutation and recombination, gene-environment interaction, agent metabolism,\nagent survival, resource gathering and sexual reproduction. In this paper we\nreview the genetic dynamics that have emerged in our system including\ngenotype-phenotype divergence, genetic drift, pseudogenes, and gene\nduplication. We conclude that emergence-focused design in complex system\nsimulation is necessary to reproduce the multilevel emergence seen in the\nnatural world.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 05:42:38 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Marriott", "Chris", ""], ["Chebib", "Jobran", ""]]}, {"id": "1505.04618", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio", "title": "Fractally-organized Connectionist Networks: Conjectures and Preliminary\n  Results", "comments": "Draft of an invited paper for PEWET (1st Workshop on PErvasive WEb\n  Technologies, trends and challenges),\n  http://www.irpps.cnr.it/en/events/call-for-papers-pewet-pervasive-web-technologies-trends-and-challenges", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strict interpretation of connectionism mandates complex networks of simple\ncomponents. The question here is, is this simplicity to be interpreted in\nabsolute terms? I conjecture that absolute simplicity might not be an essential\nattribute of connectionism, and that it may be effectively exchanged with a\nrequirement for relative simplicity, namely simplicity with respect to the\ncurrent organizational level. In this paper I provide some elements to the\nanalysis of the above question. In particular I conjecture that fractally\norganized connectionist networks may provide a convenient means to achive what\nLeibniz calls an \"art of complication\", namely an effective way to encapsulate\ncomplexity and practically extend the applicability of connectionism to domains\nsuch as sociotechnical system modeling and design. Preliminary evidence to my\nclaim is brought by considering the design of the software architecture\ndesigned for the telemonitoring service of Flemish project \"Little Sister\".\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:04:40 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["De Florio", "Vincenzo", ""]]}, {"id": "1505.04630", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang and Zhiyong Zhang", "title": "Recurrent Neural Network Training with Dark Knowledge Transfer", "comments": "ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472809", "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),\nhave gained much attention in automatic speech recognition (ASR). Although some\nsuccessful stories have been reported, training RNNs remains highly\nchallenging, especially with limited training data. Recent research found that\na well-trained model can be used as a teacher to train other child models, by\nusing the predictions generated by the teacher model as supervision. This\nknowledge transfer learning has been employed to train simple neural nets with\na complex one, so that the final performance can reach a level that is\ninfeasible to obtain by regular training. In this paper, we employ the\nknowledge transfer learning approach to train RNNs (precisely LSTM) using a\ndeep neural network (DNN) model as the teacher. This is different from most of\nthe existing research on knowledge transfer learning, since the teacher (DNN)\nis assumed to be weaker than the child (RNN); however, our experiments on an\nASR task showed that it works fairly well: without applying any tricks on the\nlearning scheme, this approach can train RNNs successfully even with limited\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:26:02 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:58:15 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 03:31:51 GMT"}, {"version": "v4", "created": "Sat, 12 Mar 2016 07:51:36 GMT"}, {"version": "v5", "created": "Sun, 8 May 2016 12:40:35 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1505.04771", "submitter": "Eric Malmi", "authors": "Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides\n  Gionis", "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation", "comments": "This is a pre-print of an article appearing at KDD'16", "journal-ref": null, "doi": "10.1145/2939672.2939679", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing rap lyrics requires both creativity to construct a meaningful,\ninteresting story and lyrical skills to produce complex rhyme patterns, which\nform the cornerstone of good flow. We present a rap lyrics generation method\nthat captures both of these aspects. First, we develop a prediction model to\nidentify the next line of existing lyrics from a set of candidate next lines.\nThis model is based on two machine-learning techniques: the RankSVM algorithm\nand a deep neural network model with a novel structure. Results show that the\nprediction model can identify the true next line among 299 randomly selected\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\nSecond, we employ the prediction model to combine lines from existing songs,\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\nshows that in terms of quantitative rhyme density, the method outperforms the\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\nonline tool called DeepBeat, and the performance of the tool has been assessed\nby analyzing its usage logs. This analysis shows that machine-learned rankings\ncorrelate with user preferences.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:35:21 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 20:51:02 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Malmi", "Eric", ""], ["Takala", "Pyry", ""], ["Toivonen", "Hannu", ""], ["Raiko", "Tapani", ""], ["Gionis", "Aristides", ""]]}, {"id": "1505.05401", "submitter": "Alfredo Braunstein", "authors": "Carlo Baldassi and Alfredo Braunstein", "title": "A Max-Sum algorithm for training discrete neural networks", "comments": null, "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment 2015, no.\n  8, P08008", "doi": "10.1088/1742-5468/2015/08/P08008", "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient learning algorithm for the problem of training neural\nnetworks with discrete synapses, a well-known hard (NP-complete) discrete\noptimization problem. The algorithm is a variant of the so-called Max-Sum (MS)\nalgorithm. In particular, we show how, for bounded integer weights with $q$\ndistinct states and independent concave a priori distribution (e.g. $l_{1}$\nregularization), the algorithm's time complexity can be made to scale as\n$O\\left(N\\log N\\right)$ per node update, thus putting it on par with\nalternative schemes, such as Belief Propagation (BP), without resorting to\napproximations. Two special cases are of particular interest: binary synapses\n$W\\in\\{-1,1\\}$ and ternary synapses $W\\in\\{-1,0,1\\}$ with $l_{0}$\nregularization. The algorithm we present performs as well as BP on binary\nperceptron learning problems, and may be better suited to address the problem\non fully-connected two-layer networks, since inherent symmetries in two layer\nnetworks are naturally broken using the MS approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 14:34:23 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 11:49:49 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Baldassi", "Carlo", ""], ["Braunstein", "Alfredo", ""]]}, {"id": "1505.05667", "submitter": "Xipeng Qiu", "authors": "Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem to model all the nodes (words or\nphrases) in a dependency tree with the dense representations. We propose a\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\nand compositional-semantic representations of phrases and words in a dependency\ntree. Different with the original recursive neural network, we introduce the\nconvolution and pooling layers, which can model a variety of compositions by\nthe feature maps and choose the most informative compositions by the pooling\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\nof candidate dependency parsing trees. The experiments show that RCNN is very\neffective to improve the state-of-the-art dependency parsing on both English\nand Chinese datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:23:10 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zhu", "Chenxi", ""], ["Qiu", "Xipeng", ""], ["Chen", "Xinchi", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1505.05969", "submitter": "Jonathan Huang", "authors": "Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran\n  Sahami, Leonidas Guibas", "title": "Learning Program Embeddings to Propagate Feedback on Student Code", "comments": "Accepted to International Conference on Machine Learning (ICML 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing feedback, both assessing final work and giving hints to stuck\nstudents, is difficult for open-ended assignments in massive online classes\nwhich can range from thousands to millions of students. We introduce a neural\nnetwork method to encode programs as a linear mapping from an embedded\nprecondition space to an embedded postcondition space and propose an algorithm\nfor feedback at scale using these linear maps as features. We apply our\nalgorithm to assessments from the Code.org Hour of Code and Stanford\nUniversity's CS1 course, where we propagate human comments on student\nassignments to orders of magnitude more submissions.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 07:03:45 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Piech", "Chris", ""], ["Huang", "Jonathan", ""], ["Nguyen", "Andy", ""], ["Phulsuksombati", "Mike", ""], ["Sahami", "Mehran", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1505.06250", "submitter": "George Toderici", "authors": "Balakrishnan Varadarajan and George Toderici and Sudheendra\n  Vijayanarasimhan and Apostol Natsev", "title": "Efficient Large Scale Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification has advanced tremendously over the recent years. A large\npart of the improvements in video classification had to do with the work done\nby the image classification community and the use of deep convolutional\nnetworks (CNNs) which produce competitive results with hand- crafted motion\nfeatures. These networks were adapted to use video frames in various ways and\nhave yielded state of the art classification results. We present two methods\nthat build on this work, and scale it up to work with millions of videos and\nhundreds of thousands of classes while maintaining a low computational cost. In\nthe context of large scale video processing, training CNNs on video frames is\nextremely time consuming, due to the large number of frames involved. We\npropose to avoid this problem by training CNNs on either YouTube thumbnails or\nFlickr images, and then using these networks' outputs as features for other\nhigher level classifiers. We discuss the challenges of achieving this and\npropose two models for frame-level and video-level classification. The first is\na highly efficient mixture of experts while the latter is based on long short\nterm memory neural networks. We present results on the Sports-1M video dataset\n(1 million videos, 487 classes) and on a new dataset which has 12 million\nvideos and 150,000 labels.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 23:45:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Varadarajan", "Balakrishnan", ""], ["Toderici", "George", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Natsev", "Apostol", ""]]}, {"id": "1505.06353", "submitter": "Henok Mengistu S", "authors": "Henok Mengistu, Joost Huizinga, Jean-Baptiste Mouret, and Jeff Clune", "title": "The evolutionary origins of hierarchy", "comments": "32 pages", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004829", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical organization -- the recursive composition of sub-modules -- is\nubiquitous in biological networks, including neural, metabolic, ecological, and\ngenetic regulatory networks, and in human-made systems, such as large\norganizations and the Internet. To date, most research on hierarchy in networks\nhas been limited to quantifying this property. However, an open, important\nquestion in evolutionary biology is why hierarchical organization evolves in\nthe first place. It has recently been shown that modularity evolves because of\nthe presence of a cost for network connections. Here we investigate whether\nsuch connection costs also tend to cause a hierarchical organization of such\nmodules. In computational simulations, we find that networks without a\nconnection cost do not evolve to be hierarchical, even when the task has a\nhierarchical structure. However, with a connection cost, networks evolve to be\nboth modular and hierarchical, and these networks exhibit higher overall\nperformance and evolvability (i.e. faster adaptation to new environments).\nAdditional analyses confirm that hierarchy independently improves adaptability\nafter controlling for modularity. Overall, our results suggest that the same\nforce--the cost of connections--promotes the evolution of both hierarchy and\nmodularity, and that these properties are important drivers of network\nperformance and adaptability. In addition to shedding light on the emergence of\nhierarchy across the many domains in which it appears, these findings will also\naccelerate future research into evolving more complex, intelligent\ncomputational brains in the fields of artificial intelligence and robotics.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 17:16:32 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Mengistu", "Henok", ""], ["Huizinga", "Joost", ""], ["Mouret", "Jean-Baptiste", ""], ["Clune", "Jeff", ""]]}, {"id": "1505.06427", "submitter": "Lantian Li Mr.", "authors": "Lantian Li and Dong Wang and Zhiyong Zhang and Thomas Fang Zheng", "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that deep neural networks (DNNs) can be used to extract\ndeep speaker vectors (d-vectors) that preserve speaker characteristics and can\nbe used in speaker verification. This new method has been tested on\ntext-dependent speaker verification tasks, and improvement was reported when\ncombined with the conventional i-vector method.\n  This paper extends the d-vector approach to semi text-independent speaker\nverification tasks, i.e., the text of the speech is in a limited set of short\nphrases. We explore various settings of the DNN structure used for d-vector\nextraction, and present a phone-dependent training which employs the posterior\nfeatures obtained from an ASR system. The experimental results show that it is\npossible to apply d-vectors on semi text-independent speaker recognition, and\nthe phone-dependent training improves system performance.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 11:22:40 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1505.06605", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu", "title": "Expresso : A user-friendly GUI for Designing, Training and Exploring\n  Convolutional Neural Networks", "comments": "Project page : http://val.serc.iisc.ernet.in/expresso/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a view to provide a user-friendly interface for designing, training and\ndeveloping deep learning frameworks, we have developed Expresso, a GUI tool\nwritten in Python. Expresso is built atop Caffe, the open-source, prize-winning\nframework popularly used to develop Convolutional Neural Networks. Expresso\nprovides a convenient wizard-like graphical interface which guides the user\nthrough various common scenarios -- data import, construction and training of\ndeep networks, performing various experiments, analyzing and visualizing the\nresults of these experiments. The multi-threaded nature of Expresso enables\nconcurrent execution and notification of events related to the aforementioned\nscenarios. The GUI sub-components and inter-component interfaces in Expresso\nhave been designed with extensibility in mind. We believe Expresso's\nflexibility and ease of use will come in handy to researchers, newcomers and\nseasoned alike, in their explorations related to deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 12:12:30 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 07:06:35 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1505.06795", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Jingming Dong and Stefano Soatto", "title": "An Empirical Evaluation of Current Convolutional Architectures' Ability\n  to Manage Nuisance Location and Scale Variability", "comments": "10 pages, 5 figures, 3 tables -- CVPR 2016, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical study to test the ability of Convolutional Neural\nNetworks (CNNs) to reduce the effects of nuisance transformations of the input\ndata, such as location, scale and aspect ratio. We isolate factors by adopting\na common convolutional architecture either deployed globally on the image to\ncompute class posterior distributions, or restricted locally to compute class\nconditional distributions given location, scale and aspect ratios of bounding\nboxes determined by proposal heuristics. In theory, averaging the latter should\nyield inferior performance compared to proper marginalization. Yet empirical\nevidence suggests the converse, leading us to conclude that - at the current\nlevel of complexity of convolutional architectures and scale of the data sets\nused to train them - CNNs are not very effective at marginalizing nuisance\nvariability. We also quantify the effects of context on the overall\nclassification task and its impact on the performance of CNNs, and propose\nimproved sampling techniques for heuristic proposal schemes that improve\nend-to-end performance to state-of-the-art levels. We test our hypothesis on a\nclassification task using the ImageNet Challenge benchmark and on a\nwide-baseline matching task using the Oxford and Fischer's datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:11:11 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 05:20:40 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Dong", "Jingming", ""], ["Soatto", "Stefano", ""]]}, {"id": "1505.06798", "submitter": "Kaiming He", "authors": "Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun", "title": "Accelerating Very Deep Convolutional Networks for Classification and\n  Detection", "comments": "TPAMI, accepted. arXiv admin note: substantial text overlap with\n  arXiv:1411.4229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to accelerate the test-time computation of convolutional\nneural networks (CNNs), especially very deep CNNs that have substantially\nimpacted the computer vision community. Unlike previous methods that are\ndesigned for approximating linear filters or linear responses, our method takes\nthe nonlinear units into account. We develop an effective solution to the\nresulting nonlinear optimization problem without the need of stochastic\ngradient descent (SGD). More importantly, while previous methods mainly focus\non optimizing one or two layers, our nonlinear method enables an asymmetric\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\nshows a graceful accuracy degradation for object detection when plugged into\nthe Fast R-CNN detector.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:30:59 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 06:16:59 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zou", "Jianhua", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1505.06800", "submitter": "Baochang Zhang", "authors": "Lei Wang, Baochang Zhang", "title": "Boosting-like Deep Learning For Pedestrian Detection", "comments": "9 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes boosting-like deep learning (BDL) framework for\npedestrian detection. Due to overtraining on the limited training samples,\noverfitting is a major problem of deep learning. We incorporate a boosting-like\ntechnique into deep learning to weigh the training samples, and thus prevent\novertraining in the iterative process. We theoretically give the details of\nderivation of our algorithm, and report the experimental results on open data\nsets showing that BDL achieves a better stable performance than the\nstate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the\naverage miss rate compared with ACF and JointDeep on the largest Caltech\nbenchmark dataset, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:52:52 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Wang", "Lei", ""], ["Zhang", "Baochang", ""]]}, {"id": "1505.07376", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker and Matthias Bethge", "title": "Texture Synthesis Using Convolutional Neural Networks", "comments": "Revision for NIPS 2015 Camera Ready. In line with reviewer's comments\n  we now focus on the texture model and texture synthesis performance. We limit\n  the relationship of our texture model to the ventral stream and its potential\n  use for neuroscience to the discussion of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we introduce a new model of natural textures based on the feature spaces\nof convolutional neural networks optimised for object recognition. Samples from\nthe model are of high perceptual quality demonstrating the generative power of\nneural networks trained in a purely discriminative fashion. Within the model,\ntextures are represented by the correlations between feature maps in several\nlayers of the network. We show that across layers the texture representations\nincreasingly capture the statistical properties of natural images while making\nobject information more and more explicit. The model provides a new tool to\ngenerate stimuli for neuroscience and might offer insights into the deep\nrepresentations learned by convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 15:29:52 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 23:10:38 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 13:55:09 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1505.07427", "submitter": "Alex Kendall", "authors": "Alex Kendall, Matthew Grimes and Roberto Cipolla", "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera\n  Relocalization", "comments": "9 pages, 13 figures; Corrected numerical error in orientation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust and real-time monocular six degree of freedom\nrelocalization system. Our system trains a convolutional neural network to\nregress the 6-DOF camera pose from a single RGB image in an end-to-end manner\nwith no need of additional engineering or graph optimisation. The algorithm can\noperate indoors and outdoors in real time, taking 5ms per frame to compute. It\nobtains approximately 2m and 6 degree accuracy for large scale outdoor scenes\nand 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23\nlayer deep convnet, demonstrating that convnets can be used to solve\ncomplicated out of image plane regression problems. This was made possible by\nleveraging transfer learning from large scale classification data. We show the\nconvnet localizes from high level features and is robust to difficult lighting,\nmotion blur and different camera intrinsics where point based SIFT registration\nfails. Furthermore we show how the pose feature that is produced generalizes to\nother scenes allowing us to regress pose with only a few dozen training\nexamples. PoseNet code, dataset and an online demonstration is available on our\nproject webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 18:18:42 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 11:52:30 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 10:10:01 GMT"}, {"version": "v4", "created": "Thu, 18 Feb 2016 13:52:18 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Kendall", "Alex", ""], ["Grimes", "Matthew", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1505.07814", "submitter": "Xinyu Wu", "authors": "Xinyu Wu, Vishal Saxena, Kehan Zhu, Sakkarapani Balagopal", "title": "A CMOS Spiking Neuron for Brain-Inspired Neural Networks with Resistive\n  Synapses and In-Situ Learning", "comments": null, "journal-ref": "IEEE Transactions on Circuits and Systems II: Express Briefs,\n  62(11), 1088-1092, 2015", "doi": "10.1109/TCSII.2015.2456372", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nanoscale resistive memories are expected to fuel dense integration of\nelectronic synapses for large-scale neuromorphic system. To realize such a\nbrain-inspired computing chip, a compact CMOS spiking neuron that performs\nin-situ learning and computing while driving a large number of resistive\nsynapses is desired. This work presents a novel leaky integrate-and-fire neuron\ndesign which implements the dual-mode operation of current integration and\nsynaptic drive, with a single opamp and enables in-situ learning with crossbar\nresistive synapses. The proposed design was implemented in a 0.18 $\\mu$m CMOS\ntechnology. Measurements show neuron's ability to drive a thousand resistive\nsynapses, and demonstrate an in-situ associative learning. The neuron circuit\noccupies a small area of 0.01 mm$^2$ and has an energy-efficiency of 9.3\npJ$/$spike$/$synapse.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 19:30:32 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 18:50:33 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Wu", "Xinyu", ""], ["Saxena", "Vishal", ""], ["Zhu", "Kehan", ""], ["Balagopal", "Sakkarapani", ""]]}, {"id": "1505.07818", "submitter": "Pascal Germain", "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\n  Larochelle, Fran\\c{c}ois Laviolette, Mario Marchand, Victor Lempitsky", "title": "Domain-Adversarial Training of Neural Networks", "comments": "Published in JMLR: http://jmlr.org/papers/v17/15-239.html", "journal-ref": "Journal of Machine Learning Research 2016, vol. 17, p. 1-35", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation learning approach for domain adaptation, in\nwhich data at training and test time come from similar but different\ndistributions. Our approach is directly inspired by the theory on domain\nadaptation suggesting that, for effective domain transfer to be achieved,\npredictions must be made based on features that cannot discriminate between the\ntraining (source) and test (target) domains. The approach implements this idea\nin the context of neural network architectures that are trained on labeled data\nfrom the source domain and unlabeled data from the target domain (no labeled\ntarget-domain data is necessary). As the training progresses, the approach\npromotes the emergence of features that are (i) discriminative for the main\nlearning task on the source domain and (ii) indiscriminate with respect to the\nshift between the domains. We show that this adaptation behaviour can be\nachieved in almost any feed-forward model by augmenting it with few standard\nlayers and a new gradient reversal layer. The resulting augmented architecture\ncan be trained using standard backpropagation and stochastic gradient descent,\nand can thus be implemented with little effort using any of the deep learning\npackages. We demonstrate the success of our approach for two distinct\nclassification problems (document sentiment analysis and image classification),\nwhere state-of-the-art domain adaptation performance on standard benchmarks is\nachieved. We also validate the approach for descriptor learning task in the\ncontext of person re-identification application.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 19:34:53 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 13:32:12 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 16:57:53 GMT"}, {"version": "v4", "created": "Thu, 26 May 2016 19:56:08 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Ustinova", "Evgeniya", ""], ["Ajakan", "Hana", ""], ["Germain", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1505.08075", "submitter": "Chris Dyer", "authors": "Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A.\n  Smith", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "comments": "Proceedings of ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:58:12 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Dyer", "Chris", ""], ["Ballesteros", "Miguel", ""], ["Ling", "Wang", ""], ["Matthews", "Austin", ""], ["Smith", "Noah A.", ""]]}]