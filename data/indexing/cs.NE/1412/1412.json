[{"id": "1412.0307", "submitter": "Markus Wagner", "authors": "Tobias Friedrich, Markus Wagner", "title": "Seeding the Initial Population of Multi-Objective Evolutionary\n  Algorithms: A Computational Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most experimental studies initialize the population of evolutionary\nalgorithms with random genotypes. In practice, however, optimizers are\ntypically seeded with good candidate solutions either previously known or\ncreated according to some problem-specific method. This \"seeding\" has been\nstudied extensively for single-objective problems. For multi-objective\nproblems, however, very little literature is available on the approaches to\nseeding and their individual benefits and disadvantages. In this article, we\nare trying to narrow this gap via a comprehensive computational study on common\nreal-valued test functions. We investigate the effect of two seeding techniques\nfor five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8\nobjectives. We observe that some functions (e.g., DTLZ4 and the LZ family)\nbenefit significantly from seeding, while others (e.g., WFG) profit less. The\nadvantage of seeding also depends on the examined algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 23:37:11 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Friedrich", "Tobias", ""], ["Wagner", "Markus", ""]]}, {"id": "1412.0595", "submitter": "Naresh Balaji", "authors": "Naresh Balaji, Esin Yavuz, Thomas Nowotny", "title": "Scalability and Optimization Strategies for GPU Enhanced Neural Networks\n  (GeNN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of spiking neural networks has been traditionally done on\nhigh-performance supercomputers or large-scale clusters. Utilizing the parallel\nnature of neural network computation algorithms, GeNN (GPU Enhanced Neural\nNetwork) provides a simulation environment that performs on General Purpose\nNVIDIA GPUs with a code generation based approach. GeNN allows the users to\ndesign and simulate neural networks by specifying the populations of neurons at\ndifferent stages, their synapse connection densities and the model of\nindividual neurons. In this report we describe work on how to scale synaptic\nweights based on the configuration of the user-defined network to ensure\nsufficient spiking and subsequent effective learning. We also discuss\noptimization strategies particular to GPU computing: sparse representation of\nsynapse connections and occupancy based block-size determination.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:12:54 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Balaji", "Naresh", ""], ["Yavuz", "Esin", ""], ["Nowotny", "Thomas", ""]]}, {"id": "1412.0650", "submitter": "Igor L. Markov", "authors": "Igor L. Markov", "title": "A review of \"Mem-computing NP-complete problems in polynomial time using\n  polynomial resources\" (arXiv:1411.4798)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reviewed paper describes an analog device that empirically solves small\ninstances of the NP-complete Subset Sum Problem (SSP). The authors claim that\nthis device can solve the SSP in polynomial time using polynomial space, in\nprinciple, and observe no exponential scaling in resource requirements. We\npoint out that (a) the properties ascribed by the authors to their device are\ninsufficient to solve NP-complete problems in poly-time, (b) runtime analysis\noffered does not cover the spectral measurement step, (c) the overall technique\nrequires exponentially increasing resources when scaled up because of the\nspectral measurement step.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 11:01:20 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 02:02:43 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2015 19:20:00 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Markov", "Igor L.", ""]]}, {"id": "1412.1285", "submitter": "Steven Frank", "authors": "Steven A. Frank", "title": "The inductive theory of natural selection: summary and synthesis", "comments": "Version 2: Changed title. Noted that condensed and simplified version\n  of this manuscript will be published as book chapter with original title \"The\n  inductive theory of natural selection.\" See footnote on title page of pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of natural selection has two forms. Deductive theory describes how\npopulations change over time. One starts with an initial population and some\nrules for change. From those assumptions, one calculates the future state of\nthe population. Deductive theory predicts how populations adapt to\nenvironmental challenge. Inductive theory describes the causes of change in\npopulations. One starts with a given amount of change. One then assigns\ndifferent parts of the total change to particular causes. Inductive theory\nanalyzes alternative causal models for how populations have adapted to\nenvironmental challenge. This chapter emphasizes the inductive analysis of\ncause.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 11:49:52 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 23:21:25 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Frank", "Steven A.", ""]]}, {"id": "1412.1602", "submitter": "Jan Chorowski", "authors": "Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio", "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent\n  NN: First Results", "comments": "As accepted to: Deep Learning and Representation Learning Workshop,\n  NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We replace the Hidden Markov Model (HMM) which is traditionally used in in\ncontinuous speech recognition with a bi-directional recurrent neural network\nencoder coupled to a recurrent neural network decoder that directly emits a\nstream of phonemes. The alignment between the input and output sequences is\nestablished using an attention mechanism: the decoder emits each symbol based\non a context created with a subset of input symbols elected by the attention\nmechanism. We report initial results demonstrating that this new approach\nachieves phoneme error rates that are comparable to the state-of-the-art\nHMM-based decoders, on the TIMIT dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 10:00:19 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Chorowski", "Jan", ""], ["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.1628", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon", "title": "Fisher Kernel for Deep Neural Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to image representation based on low-level local descriptors, deep\nneural activations of Convolutional Neural Networks (CNNs) are richer in\nmid-level representation, but poorer in geometric invariance properties. In\nthis paper, we present a straightforward framework for better image\nrepresentation by combining the two approaches. To take advantages of both\nrepresentations, we propose an efficient method to extract a fair amount of\nmulti-scale dense local activations from a pre-trained CNN. We then aggregate\nthe activations by Fisher kernel framework, which has been modified with a\nsimple scale-wise normalization essential to make it suitable for CNN\nactivations. Replacing the direct use of a single activation vector with our\nrepresentation demonstrates significant performance improvements: +17.76 (Acc.)\non MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that\nour proposal can be used as a primary image representation for better\nperformances in visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 11:30:57 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:16:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1412.1897", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Jason Yosinski, Jeff Clune", "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for\n  Unrecognizable Images", "comments": "To appear at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 05:29:43 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 22:27:00 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 16:41:04 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 23:12:56 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1412.2186", "submitter": "Babatunji Omoniwa", "authors": "Hasan M. H. Owda, Babatunji Omoniwa, Ahmad R. Shahid, Sheikh Ziauddin", "title": "Using Artificial Neural Network Techniques for Prediction of Electric\n  Energy Consumption", "comments": "10 pages, 5 figures, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Due to imprecision and uncertainties in predicting real world problems,\nartificial neural network (ANN) techniques have become increasingly useful for\nmodeling and optimization. This paper presents an artificial neural network\napproach for forecasting electric energy consumption. For effective planning\nand operation of power systems, optimal forecasting tools are needed for energy\noperators to maximize profit and also to provide maximum satisfaction to energy\nconsumers. Monthly data for electric energy consumed in the Gaza strip was\ncollected from year 1994 to 2013. Data was trained and the proposed model was\nvalidated using 2-Fold and K-Fold cross validation techniques. The model has\nbeen tested with actual energy consumption data and yields satisfactory\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 00:31:22 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Owda", "Hasan M. H.", ""], ["Omoniwa", "Babatunji", ""], ["Shahid", "Ahmad R.", ""], ["Ziauddin", "Sheikh", ""]]}, {"id": "1412.2620", "submitter": "Gundram Leifert", "authors": "G. Leifert, T. Strau{\\ss}, T. Gr\\\"uning, R. Labahn (University of\n  Rostock)", "title": "Cells in Multidimensional Recurrent Neural Networks", "comments": null, "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-37", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transcription of handwritten text on images is one task in machine\nlearning and one solution to solve it is using multi-dimensional recurrent\nneural networks (MDRNN) with connectionist temporal classification (CTC). The\nRNNs can contain special units, the long short-term memory (LSTM) cells. They\nare able to learn long term dependencies but they get unstable when the\ndimension is chosen greater than one. We defined some useful and necessary\nproperties for the one-dimensional LSTM cell and extend them in the\nmulti-dimensional case. Thereby we introduce several new cells with better\nstability. We present a method to design cells using the theory of linear shift\ninvariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT\nand Rimes database, where we can improve the recognition rate compared to the\nLSTM cell. So each application where the LSTM cells in MDRNNs are used could be\nimproved by substituting them by the new developed cells.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:47:45 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 12:26:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Leifert", "G.", "", "University of\n  Rostock"], ["Strau\u00df", "T.", "", "University of\n  Rostock"], ["Gr\u00fcning", "T.", "", "University of\n  Rostock"], ["Labahn", "R.", "", "University of\n  Rostock"]]}, {"id": "1412.2693", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar", "title": "Provable Methods for Training Neural Networks with Sparse Connectivity", "comments": "Accepted for presentation at Neural Information Processing\n  Systems(NIPS) 2014 Deep Learning workshop and Accepted as a workshop\n  contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel guaranteed approaches for training feedforward neural\nnetworks with sparse connectivity. We leverage on the techniques developed\npreviously for learning linear networks and show that they can also be\neffectively adopted to learn non-linear networks. We operate on the moments\ninvolving label and the score function of the input, and show that their\nfactorization provably yields the weight matrix of the first layer of a deep\nnetwork under mild conditions. In practice, the output of our method can be\nemployed as effective initializers for gradient descent.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:45:22 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:40:26 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 11:38:16 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2015 11:23:38 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1412.3121", "submitter": "Seungwhan Moon", "authors": "Seungwhan Moon and Suyoun Kim and Haohan Wang", "title": "Multimodal Transfer Deep Learning with Applications in Audio-Visual\n  Recognition", "comments": "6 pages, MMML workshop at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transfer deep learning (TDL) framework that can transfer the\nknowledge obtained from a single-modal neural network to a network with a\ndifferent modality. Specifically, we show that we can leverage speech data to\nfine-tune the network trained for video recognition, given an initial set of\naudio-video parallel dataset within the same semantics. Our approach first\nlearns the analogy-preserving embeddings between the abstract representations\nlearned from intermediate layers of each network, allowing for semantics-level\ntransfer between the source and target modalities. We then apply our neural\nnetwork operation that fine-tunes the target network with the additional\nknowledge transferred from the source network, while keeping the topology of\nthe target network unchanged. While we present an audio-visual recognition task\nas an application of our approach, our framework is flexible and thus can work\nwith any multimodal dataset, or with any already-existing deep networks that\nshare the common underlying semantics. In this work in progress report, we aim\nto provide comprehensive results of different configurations of the proposed\napproach on two widely used audio-visual datasets, and we discuss potential\napplications of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 21:12:19 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 19:56:41 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Moon", "Seungwhan", ""], ["Kim", "Suyoun", ""], ["Wang", "Haohan", ""]]}, {"id": "1412.3191", "submitter": "I-Ting Liu", "authors": "I-Ting Liu, Bhiksha Ramakrishnan", "title": "Bach in 2014: Music Composition with Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a framework for computer music composition that uses resilient\npropagation (RProp) and long short term memory (LSTM) recurrent neural network.\nIn this paper, we show that LSTM network learns the structure and\ncharacteristics of music pieces properly by demonstrating its ability to\nrecreate music. We also show that predicting existing music using RProp\noutperforms Back propagation through time (BPTT).\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 04:06:38 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 03:18:33 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Liu", "I-Ting", ""], ["Ramakrishnan", "Bhiksha", ""]]}, {"id": "1412.3409", "submitter": "Christopher Clark", "authors": "Christopher Clark and Amos Storkey", "title": "Teaching Deep Convolutional Neural Networks to Play Go", "comments": "9 pages, 8 figures, 5 tables. Corrected typos, minor adjustment to\n  table format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastering the game of Go has remained a long standing challenge to the field\nof AI. Modern computer Go systems rely on processing millions of possible\nfuture positions to play well, but intuitively a stronger and more 'humanlike'\nway to play the game would be to rely on pattern recognition abilities rather\nthen brute force computation. Following this sentiment, we train deep\nconvolutional neural networks to play Go by training them to predict the moves\nmade by expert Go players. To solve this problem we introduce a number of novel\ntechniques, including a method of tying weights in the network to 'hard code'\nsymmetries that are expect to exist in the target function, and demonstrate in\nan ablation study they considerably improve performance. Our final networks are\nable to achieve move prediction accuracies of 41.1% and 44.4% on two different\nGo datasets, surpassing previous state of the art on this task by significant\nmargins. Additionally, while previous move prediction programs have not yielded\nstrong Go playing programs, we show that the networks trained in this work\nacquired high levels of skill. Our convolutional neural networks can\nconsistently defeat the well known Go program GNU Go, indicating it is state of\nthe art among programs that do not use Monte Carlo Tree Search. It is also able\nto win some games against state of the art Go playing program Fuego while using\na fraction of the play time. This success at playing Go indicates high level\nprinciples of the game were learned.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:59:43 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 10:31:31 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Clark", "Christopher", ""], ["Storkey", "Amos", ""]]}, {"id": "1412.3489", "submitter": "Nathan Wiebe", "authors": "Nathan Wiebe, Ashish Kapoor, Krysta M. Svore", "title": "Quantum Deep Learning", "comments": "34 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has had a profound impact on machine learning\nand artificial intelligence. At the same time, algorithms for quantum computers\nhave been shown to efficiently solve some problems that are intractable on\nconventional, classical computers. We show that quantum computing not only\nreduces the time required to train a deep restricted Boltzmann machine, but\nalso provides a richer and more comprehensive framework for deep learning than\nclassical computing and leads to significant improvements in the optimization\nof the underlying objective function. Our quantum methods also permit efficient\ntraining of full Boltzmann machines and multi-layer, fully connected models and\ndo not have well known classical counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 23:05:16 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 00:20:28 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Wiebe", "Nathan", ""], ["Kapoor", "Ashish", ""], ["Svore", "Krysta M.", ""]]}, {"id": "1412.3555", "submitter": "Junyoung Chung", "authors": "Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence\n  Modeling", "comments": "Presented in NIPS 2014 Deep Learning and Representation Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare different types of recurrent units in recurrent\nneural networks (RNNs). Especially, we focus on more sophisticated units that\nimplement a gating mechanism, such as a long short-term memory (LSTM) unit and\na recently proposed gated recurrent unit (GRU). We evaluate these recurrent\nunits on the tasks of polyphonic music modeling and speech signal modeling. Our\nexperiments revealed that these advanced recurrent units are indeed better than\nmore traditional recurrent units such as tanh units. Also, we found GRU to be\ncomparable to LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 06:46:53 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Chung", "Junyoung", ""], ["Gulcehre", "Caglar", ""], ["Cho", "KyungHyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.3635", "submitter": "Maria Schuld", "authors": "Maria Schuld, Ilya Sinayskiy and Francesco Petruccione", "title": "Simulating a perceptron on a quantum computer", "comments": "11 pages, 6 figures, accepted by Physics Letters A", "journal-ref": "Physics Letters A, 379, pp. 660-663 (2015)", "doi": "10.1016/j.physleta.2014.11.061", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptrons are the basic computational unit of artificial neural networks,\nas they model the activation mechanism of an output neuron due to incoming\nsignals from its neighbours. As linear classifiers, they play an important role\nin the foundations of machine learning. In the context of the emerging field of\nquantum machine learning, several attempts have been made to develop a\ncorresponding unit using quantum information theory. Based on the quantum phase\nestimation algorithm, this paper introduces a quantum perceptron model\nimitating the step-activation function of a classical perceptron. This scheme\nrequires resources in $\\mathcal{O}(n)$ (where $n$ is the size of the input) and\npromises efficient applications for more complex structures such as trainable\nquantum neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 12:49:36 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Schuld", "Maria", ""], ["Sinayskiy", "Ilya", ""], ["Petruccione", "Francesco", ""]]}, {"id": "1412.3684", "submitter": "Soren Goyal", "authors": "Soren Goyal, Paul Benjamin", "title": "Object Recognition Using Deep Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of objects using Deep Neural Networks is an active area of\nresearch and many breakthroughs have been made in the last few years. The paper\nattempts to indicate how far this field has progressed. The paper briefly\ndescribes the history of research in Neural Networks and describe several of\nthe recent advances in this field. The performances of recently developed\nNeural Network Algorithm over benchmark datasets have been tabulated. Finally,\nsome the applications of this field have been provided.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:23:13 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Goyal", "Soren", ""], ["Benjamin", "Paul", ""]]}, {"id": "1412.3714", "submitter": "Jiwei Li", "authors": "Jiwei Li", "title": "Feature Weight Tuning for Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:35:27 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 00:57:57 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Li", "Jiwei", ""]]}, {"id": "1412.3717", "submitter": "Sergey Tarasenko", "authors": "Natalia Efremova and Sergey Tarasenko", "title": "Unsupervised Neural Architecture for Saliency Detection: Extended\n  Version", "comments": "10 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural network architecture for visual saliency\ndetections, which utilizes neurophysiologically plausible mechanisms for\nextraction of salient regions. The model has been significantly inspired by\nrecent findings from neurophysiology and aimed to simulate the bottom-up\nprocesses of human selective attention. Two types of features were analyzed:\ncolor and direction of maximum variance. The mechanism we employ for processing\nthose features is PCA, implemented by means of normalized Hebbian learning and\nthe waves of spikes. To evaluate performance of our model we have conducted\npsychological experiment. Comparison of simulation results with those of\nexperiment indicates good performance of our model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 11:09:01 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 12:30:14 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Efremova", "Natalia", ""], ["Tarasenko", "Sergey", ""]]}, {"id": "1412.3949", "submitter": "Gundram Leifert", "authors": "Tobias Strau{\\ss}, Tobias Gr\\\"uning, Gundram Leifert, Roger Labahn\n  (for the University of Rostock - CITlab)", "title": "CITlab ARGUS for historical handwritten documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CITlab's recognition system for the HTRtS competition attached to\nthe 14. International Conference on Frontiers in Handwriting Recognition, ICFHR\n2014. The task comprises the recognition of historical handwritten documents.\nThe core algorithms of our system are based on multi-dimensional recurrent\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\nsoftware modules behind that as well as the basic utility technologies are\nessentially powered by PLANET's ARGUS framework for intelligent text\nrecognition and image processing.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 11:11:30 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Strau\u00df", "Tobias", "", "for the University of Rostock - CITlab"], ["Gr\u00fcning", "Tobias", "", "for the University of Rostock - CITlab"], ["Leifert", "Gundram", "", "for the University of Rostock - CITlab"], ["Labahn", "Roger", "", "for the University of Rostock - CITlab"]]}, {"id": "1412.4210", "submitter": "Arunava Banerjee", "authors": "Arunava Banerjee", "title": "Learning Precise Spike Train to Spike Train Transformations in\n  Multilayer Feedforward Neuronal Networks", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a synaptic weight update rule for learning temporally precise spike\ntrain to spike train transformations in multilayer feedforward networks of\nspiking neurons. The framework, aimed at seamlessly generalizing error\nbackpropagation to the deterministic spiking neuron setting, is based strictly\non spike timing and avoids invoking concepts pertaining to spike rates or\nprobabilistic models of spiking. The derivation is founded on two innovations.\nFirst, an error functional is proposed that compares the spike train emitted by\nthe output neuron of the network to the desired spike train by way of their\nputative impact on a virtual postsynaptic neuron. This formulation sidesteps\nthe need for spike alignment and leads to closed form solutions for all\nquantities of interest. Second, virtual assignment of weights to spikes rather\nthan synapses enables a perturbation analysis of individual spike times and\nsynaptic weights of the output as well as all intermediate neurons in the\nnetwork, which yields the gradients of the error functional with respect to the\nsaid entities. Learning proceeds via a gradient descent mechanism that\nleverages these quantities. Simulation experiments demonstrate the efficacy of\nthe proposed learning framework. The experiments also highlight asymmetries\nbetween synapses on excitatory and inhibitory neurons.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 09:30:57 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 15:27:00 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Banerjee", "Arunava", ""]]}, {"id": "1412.4218", "submitter": "Hotat Lam", "authors": "Ho Tat Lam and Kwok Yip Szeto", "title": "Optimization of Reliability of Network of Given Connectivity using\n  Genetic Algorithm", "comments": "9 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability is one of the important measures of how well the system meets its\ndesign objective, and mathematically is the probability that a system will\nperform satisfactorily for at least a given period of time. When the system is\ndescribed by a connected network of N components (nodes) and their L connection\n(links), the reliability of the system becomes a difficult network design\nproblem which solutions are of great practical interest in science and\nengineering. This paper discusses the numerical method of finding the most\nreliable network for a given N and L using genetic algorithm. For a given\ntopology of the network, the reliability is numerically computed using\nadjacency matrix. For a search in the space of all possible topologies of the\nconnected network with N nodes and L links, genetic operators such as mutation\nand crossover are applied to the adjacency matrix through a string\nrepresentation. In the context of graphs, the mutation of strings in genetic\nalgorithm corresponds to the rewiring of graphs, while crossover corresponds to\nthe interchange of the sub-graphs. For small networks where the most reliable\nnetwork can be found by exhaustive search, genetic algorithm is very efficient.\nFor larger networks, our results not only demonstrate the efficiency of our\nalgorithm, but also suggest that the most reliable network will have high\nsymmetry.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 10:03:01 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Lam", "Ho Tat", ""], ["Szeto", "Kwok Yip", ""]]}, {"id": "1412.4314", "submitter": "Joseph Chee Chang", "authors": "Joseph Chee Chang and Chu-Cheng Lin", "title": "Recurrent-Neural-Network for Language Detection on Twitter\n  Code-Switching Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed language data is one of the difficult yet less explored domains of\nnatural language processing. Most research in fields like machine translation\nor sentiment analysis assume monolingual input. However, people who are capable\nof using more than one language often communicate using multiple languages at\nthe same time. Sociolinguists believe this \"code-switching\" phenomenon to be\nsocially motivated. For example, to express solidarity or to establish\nauthority. Most past work depend on external tools or resources, such as\npart-of-speech tagging, dictionary look-up, or named-entity recognizers to\nextract rich features for training machine learning models. In this paper, we\ntrain recurrent neural networks with only raw features, and use word embedding\nto automatically learn meaningful representations. Using the same\nmixed-language Twitter corpus, our system is able to outperform the best\nSVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in\naccuracy, or by 17% in error rate reduction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 05:34:25 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 15:53:22 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Chang", "Joseph Chee", ""], ["Lin", "Chu-Cheng", ""]]}, {"id": "1412.4446", "submitter": "Pascal Germain", "authors": "Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\\c{c}ois Laviolette,\n  Mario Marchand", "title": "Domain-Adversarial Neural Networks", "comments": "The first version of this paper was accepted at the \"Second Workshop\n  on Transfer and Multi-Task Learning: Theory meets Practice\" (NIPS 2014,\n  Montreal, Canada). See: https://sites.google.com/site/multitaskwsnips2014/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation learning algorithm suited to the context of\ndomain adaptation, in which data at training and test time come from similar\nbut different distributions. Our algorithm is directly inspired by theory on\ndomain adaptation suggesting that, for effective domain transfer to be\nachieved, predictions must be made based on a data representation that cannot\ndiscriminate between the training (source) and test (target) domains. We\npropose a training objective that implements this idea in the context of a\nneural network, whose hidden layer is trained to be predictive of the\nclassification task, but uninformative as to the domain of the input. Our\nexperiments on a sentiment analysis classification benchmark, where the target\ndomain data available at training time is unlabeled, show that our neural\nnetwork for domain adaption algorithm has better performance than either a\nstandard neural network or an SVM, even if trained on input features extracted\nwith the state-of-the-art marginalized stacked denoising autoencoders of Chen\net al. (2012).\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 02:16:07 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 17:52:03 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ajakan", "Hana", ""], ["Germain", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1412.4564", "submitter": "Karel Lenc", "authors": "Andrea Vedaldi, Karel Lenc", "title": "MatConvNet - Convolutional Neural Networks for MATLAB", "comments": "Updated for release v1.0-beta20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for\nMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.\nIt exposes the building blocks of CNNs as easy-to-use MATLAB functions,\nproviding routines for computing linear convolutions with filter banks, feature\npooling, and many more. In this manner, MatConvNet allows fast prototyping of\nnew CNN architectures; at the same time, it supports efficient computation on\nCPU and GPU allowing to train complex models on large datasets such as ImageNet\nILSVRC. This document provides an overview of CNNs and how they are implemented\nin MatConvNet and gives the technical details of each computational block in\nthe toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 12:23:35 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 15:35:25 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 14:31:06 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Vedaldi", "Andrea", ""], ["Lenc", "Karel", ""]]}, {"id": "1412.4690", "submitter": "Dominic Searson", "authors": "Dominic P. Searson", "title": "GPTIPS 2: an open-source software platform for symbolic data mining", "comments": "26 pages, accepted for publication in the Springer Handbook of\n  Genetic Programming Applications (2015, in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPTIPS is a free, open source MATLAB based software platform for symbolic\ndata mining (SDM). It uses a multigene variant of the biologically inspired\nmachine learning method of genetic programming (MGGP) as the engine that drives\nthe automatic model discovery process. Symbolic data mining is the process of\nextracting hidden, meaningful relationships from data in the form of symbolic\nequations. In contrast to other data-mining methods, the structural\ntransparency of the generated predictive equations can give new insights into\nthe physical systems or processes that generated the data. Furthermore, this\ntransparency makes the models very easy to deploy outside of MATLAB. The\nrationale behind GPTIPS is to reduce the technical barriers to using,\nunderstanding, visualising and deploying GP based symbolic models of data,\nwhilst at the same time remaining highly customisable and delivering robust\nnumerical performance for power users. In this chapter, notable new features of\nthe latest version of the software are discussed with these aims in mind.\nAdditionally, a simplified variant of the MGGP high level gene crossover\nmechanism is proposed. It is demonstrated that the new functionality of GPTIPS\n2 (a) facilitates the discovery of compact symbolic relationships from data\nusing multiple approaches, e.g. using novel gene-centric visualisation analysis\nto mitigate horizontal bloat and reduce complexity in multigene symbolic\nregression models (b) provides numerous methods for visualising the properties\nof symbolic models (c) emphasises the generation of graphically navigable\nlibraries of models that are optimal in terms of the Pareto trade off surface\nof model performance and complexity and (d) expedites real world applications\nby the simple, rapid and robust deployment of symbolic models outside the\nsoftware environment they were developed in.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 17:36:45 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 08:46:46 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Searson", "Dominic P.", ""]]}, {"id": "1412.4736", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "On the Inductive Bias of Dropout", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16, 3403-3454 (2015). (See\n  http://jmlr.org/papers/volume16/helmbold15a/helmbold15a.pdf.)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a simple but effective technique for learning in neural networks\nand other settings. A sound theoretical understanding of dropout is needed to\ndetermine when dropout should be applied and how to use it most effectively. In\nthis paper we continue the exploration of dropout as a regularizer pioneered by\nWager, et.al. We focus on linear classification where a convex proxy to the\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\nminimized. We show: (a) when the dropout-regularized criterion has a unique\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\nthe weights, and when it remains bounded, (c) that the dropout regularization\ncan be non-monotonic as individual weights increase from 0, and (d) that the\ndropout regularization penalty may not be convex. This last point is\nparticularly surprising because the combination of dropout regularization with\nany convex loss proxy is always a convex function.\n  In order to contrast dropout regularization with $L_2$ regularization, we\nformalize the notion of when different sources are more compatible with\ndifferent regularizers. We then exhibit distributions that are provably more\ncompatible with dropout regularization than $L_2$ regularization, and vice\nversa. These sources provide additional insight into how the inductive biases\nof dropout and $L_2$ regularization differ. We provide some similar results for\n$L_1$ regularization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:40:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 02:58:50 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:22:30 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 18:59:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1412.4864", "submitter": "Philip Bachman", "authors": "Philip Bachman and Ouais Alsharif and Doina Precup", "title": "Learning with Pseudo-Ensembles", "comments": "To appear in Advances in Neural Information Processing Systems 27\n  (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 02:55:05 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Bachman", "Philip", ""], ["Alsharif", "Ouais", ""], ["Precup", "Doina", ""]]}, {"id": "1412.4967", "submitter": "Anthony Knittel", "authors": "Anthony Knittel, Alan Blair", "title": "Sparse, guided feature connections in an Abstract Deep Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for developing a network of re-used features, where\nthe topology is formed using a coarse learning method, that allows\ngradient-descent fine tuning, known as an Abstract Deep Network (ADN). New\nfeatures are built based on observed co-occurrences, and the network is\nmaintained using a selection process related to evolutionary algorithms. This\nallows coarse ex- ploration of the problem space, effective for irregular\ndomains, while gradient descent allows pre- cise solutions. Accuracy on\nstandard UCI and Protein-Structure Prediction problems is comparable with\nbenchmark SVM and optimized GBML approaches, and shows scalability for\naddressing large problems. The discrete implementation is symbolic, allowing\ninterpretability, while the continuous method using fine-tuning shows improved\naccuracy. The binary multiplexer problem is explored, as an irregular domain\nthat does not support gradient descent learning, showing solution to the bench-\nmark 135-bit problem. A convolutional implementation is demonstrated on image\nclassification, showing an error-rate of 0.79% on the MNIST problem, without a\npre-defined topology. The ADN system provides a method for developing a very\nsparse, deep feature topology, based on observed relationships between\nfeatures, that is able to find solutions in irregular domains, and initialize a\nnetwork prior to gradient descent learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 12:04:35 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Knittel", "Anthony", ""], ["Blair", "Alan", ""]]}, {"id": "1412.5067", "submitter": "Anton Eremeev", "authors": "A. V. Eremeev, Ju. V. Kovalenko", "title": "Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling\n  Problem with Setups", "comments": "13 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform an experimental study of optimal recombination\noperator for makespan minimization problem on single machine with\nsequence-dependent setup times ($1|s_{vu}|C_{\\max}$). The computational\nexperiment on benchmark problems from TSPLIB library indicates practical\napplicability of optimal recombination in crossover operator of genetic\nalgorithm for $1|s_{vu}|C_{\\max}$.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 16:29:34 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Eremeev", "A. V.", ""], ["Kovalenko", "Ju. V.", ""]]}, {"id": "1412.5068", "submitter": "Shixiang Gu", "authors": "Shixiang Gu, Luca Rigazio", "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown deep neural networks (DNNs) to be highly susceptible to\nwell-designed, small perturbations at the input layer, or so-called adversarial\nexamples. Taking images as an example, such distortions are often\nimperceptible, but can result in 100% mis-classification for a state of the art\nDNN. We study the structure of adversarial examples and explore network\ntopology, pre-processing and training strategies to improve the robustness of\nDNNs. We perform various experiments to assess the removability of adversarial\nexamples by corrupting with additional noise and pre-processing with denoising\nautoencoders (DAEs). We find that DAEs can remove substantial amounts of the\nadversarial noise. How- ever, when stacking the DAE with the original DNN, the\nresulting network can again be attacked by new adversarial examples with even\nsmaller distortion. As a solution, we propose Deep Contractive Network, a model\nwith a new end-to-end training procedure that includes a smoothness penalty\ninspired by the contractive autoencoder (CAE). This increases the network\nrobustness to adversarial examples, without a significant performance penalty.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 23:03:49 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 16:35:05 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 14:14:24 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2015 21:43:29 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Gu", "Shixiang", ""], ["Rigazio", "Luca", ""]]}, {"id": "1412.5104", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa, Abhishek Sharma, David Jacobs", "title": "Locally Scale-Invariant Convolutional Neural Networks", "comments": "Deep Learning and Representation Learning Workshop: NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) have shown excellent results on many\nvisual classification tasks. With the exception of ImageNet, these datasets are\ncarefully crafted such that objects are well-aligned at similar scales.\nNaturally, the feature learning problem gets more challenging as the amount of\nvariation in the data increases, as the models have to learn to be invariant to\ncertain changes in appearance. Recent results on the ImageNet dataset show that\ngiven enough data, ConvNets can learn such invariances producing very\ndiscriminative features [1]. But could we do more: use less parameters, less\ndata, learn more discriminative features, if certain invariances were built\ninto the learning process? In this paper we present a simple model that allows\nConvNets to learn features in a locally scale-invariant manner without\nincreasing the number of model parameters. We show on a modified MNIST dataset\nthat when faced with scale variation, building in scale-invariance allows\nConvNets to learn more discriminative features with reduced chances of\nover-fitting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 18:09:34 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Sharma", "Abhishek", ""], ["Jacobs", "David", ""]]}, {"id": "1412.5244", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky, Richard Zemel", "title": "Learning unbiased features", "comments": "Published in NIPS 2014 Workshop on Transfer and Multitask Learning,\n  see http://nips.cc/Conferences/2014/Program/event.php?ID=4282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key element in transfer learning is representation learning; if\nrepresentations can be developed that expose the relevant factors underlying\nthe data, then new tasks and domains can be learned readily based on mappings\nof these salient factors. We propose that an important aim for these\nrepresentations are to be unbiased. Different forms of representation learning\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\nuseful approach to estimating the amount of bias in a representation comes from\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\ndistributions. We are not the first to suggest that MMD can be a useful\ncriterion in developing representations that apply across multiple domains or\ntasks [1]. However, in this paper we describe a number of novel applications of\nthis criterion that we have devised, all based on the idea of developing\nunbiased representations. These formulations include: a standard domain\nadaptation framework; a method of learning invariant representations; an\napproach based on noise-insensitive autoencoders; and a novel form of\ngenerative model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 02:47:22 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1412.5323", "submitter": "Bassam Alkindy Mr.", "authors": "Bassam AlKindy, Christophe Guyeux, Jean-Fran\\c{c}ois Couchot, Michel\n  Salomon, Jacques M. Bahi", "title": "Gene Similarity-based Approaches for Determining Core-Genes of\n  Chloroplasts", "comments": "4 pages, IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM 2014)", "journal-ref": null, "doi": "10.1109/BIBM.2014.6999130", "report-no": null, "categories": "cs.NE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational biology and bioinformatics, the manner to understand\nevolution processes within various related organisms paid a lot of attention\nthese last decades. However, accurate methodologies are still needed to\ndiscover genes content evolution. In a previous work, two novel approaches\nbased on sequence similarities and genes features have been proposed. More\nprecisely, we proposed to use genes names, sequence similarities, or both,\ninsured either from NCBI or from DOGMA annotation tools. Dogma has the\nadvantage to be an up-to-date accurate automatic tool specifically designed for\nchloroplasts, whereas NCBI possesses high quality human curated genes (together\nwith wrongly annotated ones). The key idea of the former proposal was to take\nthe best from these two tools. However, the first proposal was limited by name\nvariations and spelling errors on the NCBI side, leading to core trees of low\nquality. In this paper, these flaws are fixed by improving the comparison of\nNCBI and DOGMA results, and by relaxing constraints on gene names while adding\na stage of post-validation on gene sequences. The two stages of similarity\nmeasures, on names and sequences, are thus proposed for sequence clustering.\nThis improves results that can be obtained using either NCBI or DOGMA alone.\nResults obtained with this quality control test are further investigated and\ncompared with previously released ones, on both computational and biological\naspects, considering a set of 99 chloroplastic genomes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 10:27:53 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["AlKindy", "Bassam", ""], ["Guyeux", "Christophe", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Salomon", "Michel", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1412.5335", "submitter": "Gr\\'egoire Mesnil", "authors": "Gr\\'egoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 11:02:04 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:17:16 GMT"}, {"version": "v3", "created": "Fri, 19 Dec 2014 11:36:14 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 20:03:35 GMT"}, {"version": "v5", "created": "Wed, 4 Feb 2015 05:17:55 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 14:26:14 GMT"}, {"version": "v7", "created": "Wed, 27 May 2015 06:40:09 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Mesnil", "Gr\u00e9goire", ""], ["Mikolov", "Tomas", ""], ["Ranzato", "Marc'Aurelio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.5384", "submitter": "Paulo Matias", "authors": "Andre B. Perina, Marcilyanne M. Gois, Paulo Matias, Joao M. P.\n  Cardoso, Alexandre C. B. Delbem, Vanderlei Bonato", "title": "Representation of Evolutionary Algorithms in FPGA Cluster for Project of\n  Large-Scale Networks", "comments": "Preprint of a short paper published in the proceedings of ERAD-SP\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems are related to network projects, such as electric distribution,\ntelecommunication and others. Most of them can be represented by graphs, which\nmanipulate thousands or millions of nodes, becoming almost an impossible task\nto obtain real-time solutions. Many efficient solutions use Evolutionary\nAlgorithms (EA), where researches show that performance of EAs can be\nsubstantially raised by using an appropriate representation, such as the\nNode-Depth Encoding (NDE). The objective of this work was to partition an\nimplementation on single-FPGA (Field-Programmable Gate Array) based on NDE from\n512 nodes to a multi-FPGAs approach, expanding the system to 4096 nodes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 13:37:29 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Perina", "Andre B.", ""], ["Gois", "Marcilyanne M.", ""], ["Matias", "Paulo", ""], ["Cardoso", "Joao M. P.", ""], ["Delbem", "Alexandre C. B.", ""], ["Bonato", "Vanderlei", ""]]}, {"id": "1412.5474", "submitter": "Jonghoon Jin", "authors": "Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello", "title": "Flattened Convolutional Neural Networks for Feedforward Acceleration", "comments": "International Conference on Learning Representations (ICLR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present flattened convolutional neural networks that are designed for fast\nfeedforward execution. The redundancy of the parameters, especially weights of\nthe convolutional filters in convolutional neural networks has been extensively\nstudied and different heuristics have been proposed to construct a low rank\nbasis of the filters after training. In this work, we train flattened networks\nthat consist of consecutive sequence of one-dimensional filters across all\ndirections in 3D space to obtain comparable performance as conventional\nconvolutional networks. We tested flattened model on different datasets and\nfound that the flattened layer can effectively substitute for the 3D filters\nwithout loss of accuracy. The flattened convolution pipelines provide around\ntwo times speed-up during feedforward pass compared to the baseline model due\nto the significant reduction of learning parameters. Furthermore, the proposed\nmethod does not require efforts in manual tuning or post processing once the\nmodel is trained.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 16:48:54 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:36:05 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2015 01:40:08 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2015 05:50:23 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Jin", "Jonghoon", ""], ["Dundar", "Aysegul", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1412.5513", "submitter": "Engelbert Mephu Nguifo", "authors": "Cyrine Arouri, Engelbert Mephu Nguifo, Sabeur Aridhi, C\\'ecile\n  Roucelle, Gaelle Bonnet-Loosli, Norbert Tsopz\\'e", "title": "Towards a constructive multilayer perceptron for regression task using\n  non-parametric clustering. A case study of Photo-Z redshift reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of architecture of artificial neuron network (ANN) is still a\nchallenging task that users face every time. It greatly affects the accuracy of\nthe built network. In fact there is no optimal method that is applicable to\nvarious implementations at the same time. In this paper we propose a method to\nconstruct ANN based on clustering, that resolves the problems of random and ad\nhoc approaches for multilayer ANN architecture. Our method can be applied to\nregression problems. Experimental results obtained with different datasets,\nreveals the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 18:36:23 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Arouri", "Cyrine", ""], ["Nguifo", "Engelbert Mephu", ""], ["Aridhi", "Sabeur", ""], ["Roucelle", "C\u00e9cile", ""], ["Bonnet-Loosli", "Gaelle", ""], ["Tsopz\u00e9", "Norbert", ""]]}, {"id": "1412.5567", "submitter": "Awni Hannun", "authors": "Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos,\n  Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates and\n  Andrew Y. Ng", "title": "Deep Speech: Scaling up end-to-end speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 20:39:45 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 21:36:13 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Hannun", "Awni", ""], ["Case", "Carl", ""], ["Casper", "Jared", ""], ["Catanzaro", "Bryan", ""], ["Diamos", "Greg", ""], ["Elsen", "Erich", ""], ["Prenger", "Ryan", ""], ["Satheesh", "Sanjeev", ""], ["Sengupta", "Shubho", ""], ["Coates", "Adam", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1412.5661", "submitter": "Wanli Ouyang", "authors": "Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong\n  Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, Xiaoou Tang", "title": "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object\n  Detection", "comments": "CVPR15, arXiv admin note: substantial text overlap with\n  arXiv:1409.3505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose deformable deep convolutional neural networks for\ngeneric object detection. This new deep learning object detection framework has\ninnovations in multiple aspects. In the proposed new deep architecture, a new\ndeformation constrained pooling (def-pooling) layer models the deformation of\nobject parts with geometric constraint and penalty. A new pre-training strategy\nis proposed to learn feature representations more suitable for the object\ndetection task and with good generalization capability. By changing the net\nstructures, training strategies, adding and removing some key components in the\ndetection pipeline, a set of models with large diversity are obtained, which\nsignificantly improves the effectiveness of model averaging. The proposed\napproach improves the mean averaged precision obtained by RCNN\n\\cite{girshick2014rich}, which was the state-of-the-art, from 31\\% to 50.3\\% on\nthe ILSVRC2014 detection test set. It also outperforms the winner of\nILSVRC2014, GoogLeNet, by 6.1\\%. Detailed component-wise analysis is also\nprovided through extensive experimental evaluation, which provide a global view\nfor people to understand the deep learning object detection pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:41:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 03:24:08 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Zeng", "Xingyu", ""], ["Qiu", "Shi", ""], ["Luo", "Ping", ""], ["Tian", "Yonglong", ""], ["Li", "Hongsheng", ""], ["Yang", "Shuo", ""], ["Wang", "Zhe", ""], ["Loy", "Chen-Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1412.5710", "submitter": "Jiaqi Zhao", "authors": "Jiaqi Zhao, Vitor Basto Fernandes, Licheng Jiao, Iryna Yevseyeva, Asep\n  Maulana, Rui Li, Thomas B\\\"ack, and Michael T. M. Emmerich", "title": "Multiobjective Optimization of Classifiers by Means of 3-D Convex Hull\n  Based Evolutionary Algorithm", "comments": "32 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Finding a good classifier is a multiobjective optimization problem with\ndifferent error rates and the costs to be minimized. The receiver operating\ncharacteristic is widely used in the machine learning community to analyze the\nperformance of parametric classifiers or sets of Pareto optimal classifiers. In\norder to directly compare two sets of classifiers the area (or volume) under\nthe convex hull can be used as a scalar indicator for the performance of a set\nof classifiers in receiver operating characteristic space.\n  Recently, the convex hull based multiobjective genetic programming algorithm\nwas proposed and successfully applied to maximize the convex hull area for\nbinary classification problems. The contribution of this paper is to extend\nthis algorithm for dealing with higher dimensional problem formulations. In\nparticular, we discuss problems where parsimony (or classifier complexity) is\nstated as a third objective and multi-class classification with three different\ntrue classification rates to be maximized.\n  The design of the algorithm proposed in this paper is inspired by\nindicator-based evolutionary algorithms, where first a performance indicator\nfor a solution set is established and then a selection operator is designed\nthat complies with the performance indicator. In this case, the performance\nindicator will be the volume under the convex hull. The algorithm is tested and\nanalyzed in a proof of concept study on different benchmarks that are designed\nfor measuring its capability to capture relevant parts of a convex hull.\n  Further benchmark and application studies on email classification and feature\nselection round up the analysis and assess robustness and usefulness of the new\nalgorithm in real world settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 03:01:10 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Zhao", "Jiaqi", ""], ["Fernandes", "Vitor Basto", ""], ["Jiao", "Licheng", ""], ["Yevseyeva", "Iryna", ""], ["Maulana", "Asep", ""], ["Li", "Rui", ""], ["B\u00e4ck", "Thomas", ""], ["Emmerich", "Michael T. M.", ""]]}, {"id": "1412.5862", "submitter": "Wolfgang Maass Prof.", "authors": "Zeno Jonke, Stefan Habenschuss, Wolfgang Maass", "title": "A theoretical basis for efficient computations with noisy spiking\n  neurons", "comments": "main paper: 21 pages, 5 figures supplemental paper: 11 pages, no\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network of neurons in the brain apply - unlike processors in our current\ngeneration of computer hardware - an event-based processing strategy, where\nshort pulses (spikes) are emitted sparsely by neurons to signal the occurrence\nof an event at a particular point in time. Such spike-based computations\npromise to be substantially more power-efficient than traditional clocked\nprocessing schemes. However it turned out to be surprisingly difficult to\ndesign networks of spiking neurons that are able to carry out demanding\ncomputations. We present here a new theoretical framework for organizing\ncomputations of networks of spiking neurons. In particular, we show that a\nsuitable design enables them to solve hard constraint satisfaction problems\nfrom the domains of planning - optimization and verification - logical\ninference. The underlying design principles employ noise as a computational\nresource. Nevertheless the timing of spikes (rather than just spike rates)\nplays an essential role in the resulting computations. Furthermore, one can\ndemonstrate for the Traveling Salesman Problem a surprising computational\nadvantage of networks of spiking neurons compared with traditional artificial\nneural networks and Gibbs sampling. The identification of such advantage has\nbeen a well-known open problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 14:12:54 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Jonke", "Zeno", ""], ["Habenschuss", "Stefan", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1412.5896", "submitter": "Raja Giryes", "authors": "Raja Giryes and Guillermo Sapiro and Alex M. Bronstein", "title": "On the Stability of Deep Networks", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NE math.IT math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the properties of deep neural networks (DNN) with\nrandom weights. We formally prove that these networks perform a\ndistance-preserving embedding of the data. Based on this we then draw\nconclusions on the size of the training data and the networks' structure. A\nlonger version of this paper with more results and details can be found in\n(Giryes et al., 2015). In particular, we formally prove in the longer version\nthat DNN with random Gaussian weights perform a distance-preserving embedding\nof the data, with a special treatment for in-class and out-of-class data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:40:03 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 20:52:41 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 14:38:57 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1412.6012", "submitter": "Gundram Leifert", "authors": "Gundram Leifert, Tobias Gr\\\"uning, Tobias Strau{\\ss}, Roger Labahn\n  (for the University of Rostock - CITlab)", "title": "CITlab ARGUS for historical data tables", "comments": "arXiv admin note: text overlap with arXiv:1412.3949", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CITlab's recognition system for the ANWRESH-2014 competition\nattached to the 14. International Conference on Frontiers in Handwriting\nRecognition, ICFHR 2014. The task comprises word recognition from segmented\nhistorical documents. The core components of our system are based on\nmulti-dimensional recurrent neural networks (MDRNN) and connectionist temporal\nclassification (CTC). The software modules behind that as well as the basic\nutility technologies are essentially powered by PLANET's ARGUS framework for\nintelligent text recognition and image processing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 06:54:47 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Leifert", "Gundram", "", "for the University of Rostock - CITlab"], ["Gr\u00fcning", "Tobias", "", "for the University of Rostock - CITlab"], ["Strau\u00df", "Tobias", "", "for the University of Rostock - CITlab"], ["Labahn", "Roger", "", "for the University of Rostock - CITlab"]]}, {"id": "1412.6061", "submitter": "Gundram Leifert", "authors": "Gundram Leifert, Roger Labahn, Tobias Strau{\\ss} (University of\n  Rostock - CITlab)", "title": "CITlab ARGUS for Arabic Handwriting", "comments": "http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years it turned out that multidimensional recurrent neural\nnetworks (MDRNN) perform very well for offline handwriting recognition tasks\nlike the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and\ndictionary lookup, our ARGUS software completed this task with an error rate of\n26.27% in its primary setup.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 06:55:28 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Leifert", "Gundram", "", "University of\n  Rostock - CITlab"], ["Labahn", "Roger", "", "University of\n  Rostock - CITlab"], ["Strau\u00df", "Tobias", "", "University of\n  Rostock - CITlab"]]}, {"id": "1412.6093", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel and Raunaq Vohra", "title": "Learning Temporal Dependencies in Data Using a DBN-BLSTM", "comments": "6 pages, 2 figures, 1 table, ICLR 2015 conference track submission\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of deep learning, it has been used to solve various problems\nusing many different architectures. The application of such deep architectures\nto auditory data is also not uncommon. However, these architectures do not\nalways adequately consider the temporal dependencies in data. We thus propose a\nnew generic architecture called the Deep Belief Network - Bidirectional Long\nShort-Term Memory (DBN-BLSTM) network that models sequences by keeping track of\nthe temporal information while enabling deep representations in the data. We\ndemonstrate this new architecture by applying it to the task of music\ngeneration and obtain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 11:04:59 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 18:44:33 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""]]}, {"id": "1412.6115", "submitter": "Yunchao Gong", "authors": "Yunchao Gong and Liu Liu and Ming Yang and Lubomir Bourdev", "title": "Compressing Deep Convolutional Networks using Vector Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) has become the most promising method\nfor object recognition, repeatedly demonstrating record breaking results for\nimage classification and object detection in recent years. However, a very deep\nCNN generally involves many layers with millions of parameters, making the\nstorage of the network model to be extremely large. This prohibits the usage of\ndeep CNNs on resource limited hardware, especially cell phones or other\nembedded devices. In this paper, we tackle this model storage issue by\ninvestigating information theoretical vector quantization methods for\ncompressing the parameters of CNNs. In particular, we have found in terms of\ncompressing the most storage demanding dense connected layers, vector\nquantization methods have a clear gain over existing matrix factorization\nmethods. Simply applying k-means clustering to the weights or conducting\nproduct quantization can lead to a very good balance between model size and\nrecognition accuracy. For the 1000-category classification task in the ImageNet\nchallenge, we are able to achieve 16-24 times compression of the network with\nonly 1% loss of classification accuracy using the state-of-the-art CNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:09:01 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Gong", "Yunchao", ""], ["Liu", "Liu", ""], ["Yang", "Ming", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1412.6122", "submitter": "Subhash Kak", "authors": "Subhash Kak", "title": "Spread Unary Coding", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unary coding is useful but it is redundant in its standard form. Unary coding\ncan also be seen as spatial coding where the value of the number is determined\nby its place in an array. Motivated by biological finding that several neurons\nin the vicinity represent the same number, we propose a variant of unary\nnumeration in its spatial form, where each number is represented by several 1s.\nWe call this spread unary coding where the number of 1s used is the spread of\nthe code. Spread unary coding is associated with saturation of the Hamming\ndistance between code words.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 22:58:54 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kak", "Subhash", ""]]}, {"id": "1412.6144", "submitter": "Daniel Kovach Jr.", "authors": "Daniel Kovach", "title": "The Computational Theory of Intelligence: Applications to Genetic\n  Programming and Turing Machines", "comments": "Total of 5 figures. This paper was originally presented at RAMSA 2013\n  in Visakhaptnam, India in December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we continue the efforts of the Computational Theory of\nIntelligence (CTI) by extending concepts to include computational processes in\nterms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,\nand Hybrid Computational Intelligence processes are also introduced and\ndiscussed. We consider the ramifications of the assumptions of CTI with regard\nto the qualities of reproduction and virility. Applications to Biology,\nComputer Science and Cyber Security are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 23:12:05 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kovach", "Daniel", ""]]}, {"id": "1412.6145", "submitter": "Lenka Skanderova", "authors": "Lenka Skanderova and Tomas Fabian", "title": "Study of the Influence of the Number Normalization Scheme Used in Two\n  Chaotic Pseudo Random Number Generators Used as the Source of Randomness in\n  Differential Evolution", "comments": "Pre-print for Impact Factor Journal Natural Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many publications, authors showed that chaotic pseudo random number\ngenerators (PRNGs) may improve performance of the evolutionary algorithms. In\nthis paper, we use two chaotic maps Gingerbread man and Tinkerbell as the\nchaotic PRNGs instead of the classical PRNG in the differential evolution.\nNumbers generated by this maps are normalized to the unit interval by three\ndifferent methods -- operation modulo, straightforward number normalization\nwhere we know minimal and maximal generated number and arctangent of the two\nvariables $x$ and $y$, where numbers $x$ and $y$ are generated by the\nGingerbread man map and Tinkerbell map. The first goal of this paper is to show\nwhether the differential evolution convergence speed might be affected by the\nway how we normalize number generated by the chaotic map. The second goal is to\nfind out the influence of the probability distribution function of the selected\nchaotic PRNGs. The results mentioned below showed that the selected\nnormalization method may improve differential evolution convergence speed,\nespecially in the case of arctangent and straightforward number normalization,\nwhere we know the minimal and maximal generated numbers.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 14:44:15 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Skanderova", "Lenka", ""], ["Fabian", "Tomas", ""]]}, {"id": "1412.6181", "submitter": "Ran Gilad-Bachrach", "authors": "Pengtao Xie and Misha Bilenko and Tom Finley and Ran Gilad-Bachrach\n  and Kristin Lauter and Michael Naehrig", "title": "Crypto-Nets: Neural Networks over Encrypted Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem we address is the following: how can a user employ a predictive\nmodel that is held by a third party, without compromising private information.\nFor example, a hospital may wish to use a cloud service to predict the\nreadmission risk of a patient. However, due to regulations, the patient's\nmedical files cannot be revealed. The goal is to make an inference using the\nmodel, without jeopardizing the accuracy of the prediction or the privacy of\nthe data.\n  To achieve high accuracy, we use neural networks, which have been shown to\noutperform other learning models for many tasks. To achieve the privacy\nrequirements, we use homomorphic encryption in the following protocol: the data\nowner encrypts the data and sends the ciphertexts to the third party to obtain\na prediction from a trained model. The model operates on these ciphertexts and\nsends back the encrypted prediction. In this protocol, not only the data\nremains private, even the values predicted are available only to the data\nowner.\n  Using homomorphic encryption and modifications to the activation functions\nand training algorithms of neural networks, we show that it is protocol is\npossible and may be feasible. This method paves the way to build a secure\ncloud-based neural network prediction services without invading users' privacy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 23:38:54 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 06:16:14 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Xie", "Pengtao", ""], ["Bilenko", "Misha", ""], ["Finley", "Tom", ""], ["Gilad-Bachrach", "Ran", ""], ["Lauter", "Kristin", ""], ["Naehrig", "Michael", ""]]}, {"id": "1412.6249", "submitter": "Min Lin", "authors": "Min Lin, Shuo Li, Xuan Luo, Shuicheng Yan", "title": "Purine: A bi-graph based deep learning framework", "comments": "Submitted to ICLR 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 08:20:10 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 03:18:46 GMT"}, {"version": "v3", "created": "Tue, 20 Jan 2015 02:17:39 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2015 16:13:46 GMT"}, {"version": "v5", "created": "Thu, 16 Apr 2015 13:09:33 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Lin", "Min", ""], ["Li", "Shuo", ""], ["Luo", "Xuan", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1412.6257", "submitter": "Alexander Kalmanovich", "authors": "Alexander Kalmanovich and Gal Chechik", "title": "Gradual training of deep denoising auto encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked denoising auto encoders (DAEs) are well known to learn useful deep\nrepresentations, which can be used to improve supervised training by\ninitializing a deep network. We investigate a training scheme of a deep DAE,\nwhere DAE layers are gradually added and keep adapting as additional layers are\nadded. We show that in the regime of mid-sized datasets, this gradual training\nprovides a small but consistent improvement over stacked training in both\nreconstruction quality and classification error over stacked training on MNIST\nand CIFAR datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 09:30:33 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kalmanovich", "Alexander", ""], ["Chechik", "Gal", ""]]}, {"id": "1412.6296", "submitter": "Jifeng Dai", "authors": "Jifeng Dai, Yang Lu, Ying-Nian Wu", "title": "Generative Modeling of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural networks (CNNs) have proven to be a powerful tool\nfor discriminative learning. Recently researchers have also started to show\ninterest in the generative aspects of CNNs in order to gain a deeper\nunderstanding of what they have learned and how to further improve them. This\npaper investigates generative modeling of CNNs. The main contributions include:\n(1) We construct a generative model for the CNN in the form of exponential\ntilting of a reference distribution. (2) We propose a generative gradient for\npre-training CNNs by a non-parametric importance sampling scheme, which is\nfundamentally different from the commonly used discriminative gradient, and yet\nhas the same computational architecture and cost as the latter. (3) We propose\na generative visualization method for the CNNs by sampling from an explicit\nparametric image distribution. The proposed visualization method can directly\ndraw synthetic samples for any given node in a trained CNN by the Hamiltonian\nMonte Carlo (HMC) algorithm, without resorting to any extra hold-out images.\nExperiments on the challenging ImageNet benchmark show that the proposed\ngenerative gradient pre-training consistently helps improve the performances of\nCNNs, and the proposed generative visualization method generates meaningful and\nvaried samples of synthetic images from a large-scale deep CNN.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:34:37 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 15:07:06 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Dai", "Jifeng", ""], ["Lu", "Yang", ""], ["Wu", "Ying-Nian", ""]]}, {"id": "1412.6346", "submitter": "Dominic Searson", "authors": "Dominic P. Searson, Mark J. Willis, Allen Wright", "title": "Reverse Engineering Chemical Reaction Networks from Time Series Data", "comments": "36 pages. In: Dehmer, M., Varmuza, K., Bonchev, D, ed. Statistical\n  Modelling of Molecular Descriptors in QSAR/QSPR. Weinheim, Germany: Wiley-VCH\n  Verlag GmbH, 2012, pp.327-348", "journal-ref": null, "doi": "10.1002/9783527645121.ch12", "report-no": null, "categories": "cs.NE q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated inference of physically interpretable (bio)chemical reaction\nnetwork models from measured experimental data is a challenging problem whose\nsolution has significant commercial and academic ramifications. It is\ndemonstrated, using simulations, how sets of elementary reactions comprising\nchemical reaction networks, as well as their rate coefficients, may be\naccurately recovered from non-equilibrium time series concentration data, such\nas that obtained from laboratory scale reactors. A variant of an evolutionary\nalgorithm called differential evolution in conjunction with least squares\ntechniques is used to search the space of reaction networks in order to infer\nboth the reaction network topology and its rate parameters. Properties of the\nstoichiometric matrices of trial networks are used to bias the search towards\nphysically realisable solutions. No other information, such as chemical\ncharacterisation of the reactive species is required, although where available\nit may be used to improve the search process.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 13:54:54 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Searson", "Dominic P.", ""], ["Willis", "Mark J.", ""], ["Wright", "Allen", ""]]}, {"id": "1412.6464", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew\n  Marsza{\\l}ek, Dawid Po{\\l}ap and Marcin Wo\\'zniak", "title": "Simplified firefly algorithm for 2D image key-points search", "comments": "Published version on: 2014 IEEE Symposium on Computational\n  Intelligence for Human-like Intelligence", "journal-ref": "IEEE Symposium on Computational Intelligence for Human-like\n  Intelligence, pp. 118-125, 2014", "doi": "10.1109/CIHLI.2014.7013395", "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify an object, human eyes firstly search the field of view\nfor points or areas which have particular properties. These properties are used\nto recognise an image or an object. Then this process could be taken as a model\nto develop computer algorithms for images identification. This paper proposes\nthe idea of applying the simplified firefly algorithm to search for key-areas\nin 2D images. For a set of input test images the proposed version of firefly\nalgorithm has been examined. Research results are presented and discussed to\nshow the efficiency of this evolutionary computation method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:00:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""], ["Marsza\u0142ek", "Zbigniew", ""], ["Po\u0142ap", "Dawid", ""], ["Wo\u017aniak", "Marcin", ""]]}, {"id": "1412.6502", "submitter": "Siddharth Pramod", "authors": "Siddharth Pramod, Adam Page, Tinoosh Mohsenin and Tim Oates", "title": "Detecting Epileptic Seizures from EEG Data using Neural Networks", "comments": "This paper has been withdrawn by the authors due to an error\n  discovered in the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of neural networks trained with dropout in predicting\nepileptic seizures from electroencephalographic data (scalp EEG). The input to\nthe neural network is a 126 feature vector containing 9 features for each of\nthe 14 EEG channels obtained over 1-second, non-overlapping windows. The models\nin our experiments achieved high sensitivity and specificity on patient records\nnot used in the training process. This is demonstrated using\nleave-one-out-cross-validation across patient records, where we hold out one\npatient's record as the test set and use all other patients' records for\ntraining; repeating this procedure for all patients in the database.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:00:38 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 01:20:42 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 00:42:42 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2015 15:24:05 GMT"}, {"version": "v5", "created": "Thu, 6 Dec 2018 03:15:14 GMT"}, {"version": "v6", "created": "Mon, 4 Feb 2019 00:41:35 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Pramod", "Siddharth", ""], ["Page", "Adam", ""], ["Mohsenin", "Tinoosh", ""], ["Oates", "Tim", ""]]}, {"id": "1412.6544", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe", "title": "Qualitatively characterizing neural network optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks involves solving large-scale non-convex optimization\nproblems. This task has long been believed to be extremely difficult, with fear\nof local minima and other obstacles motivating a variety of schemes to improve\noptimization, such as unsupervised pretraining. However, modern neural networks\nare able to achieve negligible training error on complex tasks, using only\ndirect training with stochastic gradient descent. We introduce a simple\nanalysis technique to look for evidence that such networks are overcoming local\noptima. We find that, in fact, on a straight path from initialization to\nsolution, a variety of state of the art neural networks never encounter any\nsignificant obstacles.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:55:01 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 23:59:32 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 05:25:15 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 18:28:17 GMT"}, {"version": "v5", "created": "Fri, 20 Mar 2015 20:08:54 GMT"}, {"version": "v6", "created": "Thu, 21 May 2015 21:44:31 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Vinyals", "Oriol", ""], ["Saxe", "Andrew M.", ""]]}, {"id": "1412.6550", "submitter": "Adriana Romero", "authors": "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine\n  Chassang, Carlo Gatta and Yoshua Bengio", "title": "FitNets: Hints for Thin Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While depth tends to improve network performances, it also makes\ngradient-based training more difficult since deeper networks tend to be more\nnon-linear. The recently proposed knowledge distillation approach is aimed at\nobtaining small and fast-to-execute models, and it has shown that a student\nnetwork could imitate the soft output of a larger teacher network or ensemble\nof networks. In this paper, we extend this idea to allow the training of a\nstudent that is deeper and thinner than the teacher, using not only the outputs\nbut also the intermediate representations learned by the teacher as hints to\nimprove the training process and final performance of the student. Because the\nstudent intermediate hidden layer will generally be smaller than the teacher's\nintermediate hidden layer, additional parameters are introduced to map the\nstudent hidden layer to the prediction of the teacher hidden layer. This allows\none to train deeper students that can generalize better or run faster, a\ntrade-off that is controlled by the chosen student capacity. For example, on\nCIFAR-10, a deep student network with almost 10.4 times less parameters\noutperforms a larger, state-of-the-art teacher network.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 22:40:51 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 20:56:15 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 18:44:36 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2015 11:52:28 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Romero", "Adriana", ""], ["Ballas", "Nicolas", ""], ["Kahou", "Samira Ebrahimi", ""], ["Chassang", "Antoine", ""], ["Gatta", "Carlo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.6558", "submitter": "David Sussillo", "authors": "David Sussillo, L.F. Abbott", "title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 23:24:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 21:28:29 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 22:28:32 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Sussillo", "David", ""], ["Abbott", "L. F.", ""]]}, {"id": "1412.6563", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov", "title": "Self-informed neural network structure learning", "comments": "Updated with accepted workshop contribution header", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of large scale, multi-label visual recognition with a\nlarge number of possible classes. We propose a method for augmenting a trained\nneural network classifier with auxiliary capacity in a manner designed to\nsignificantly improve upon an already well-performing model, while minimally\nimpacting its computational footprint. Using the predictions of the network\nitself as a descriptor for assessing visual similarity, we define a\npartitioning of the label space into groups of visually similar entities. We\nthen augment the network with auxilliary hidden layer pathways with\nconnectivity only to these groups of label units. We report a significant\nimprovement in mean average precision on a large-scale object recognition task\nwith the augmented model, while increasing the number of multiply-adds by less\nthan 3%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:05:57 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 21:35:29 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Warde-Farley", "David", ""], ["Rabinovich", "Andrew", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1412.6564", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Aja Huang, Ilya Sutskever, David Silver", "title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "comments": "Minor edits and included captures in Figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game of Go is more challenging than other board games, due to the\ndifficulty of constructing a position or move evaluation function. In this\npaper we investigate whether deep convolutional networks can be used to\ndirectly represent and learn this knowledge. We train a large 12-layer\nconvolutional neural network by supervised learning from a database of human\nprofessional games. The network correctly predicts the expert move in 55% of\npositions, equalling the accuracy of a 6 dan human player. When the trained\nconvolutional network was used directly to play games of Go, without any\nsearch, it beat the traditional search program GnuGo in 97% of games, and\nmatched the performance of a state-of-the-art Monte-Carlo tree search that\nsimulates a million positions per move.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:31:30 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 19:03:34 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Maddison", "Chris J.", ""], ["Huang", "Aja", ""], ["Sutskever", "Ilya", ""], ["Silver", "David", ""]]}, {"id": "1412.6567", "submitter": "Pitoyo Hartono", "authors": "Thomas Trappenberg, Paul Hollensen and Pitoyo Hartono", "title": "Classifier with Hierarchical Topographical Maps as Internal\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we want to connect our previously proposed context-relevant\ntopographical maps with the deep learning community. Our architecture is a\nclassifier with hidden layers that are hierarchical two-dimensional\ntopographical maps. These maps differ from the conventional self-organizing\nmaps in that their organizations are influenced by the context of the data\nlabels in a top-down manner. In this way bottom-up and top-down learning are\ncombined in a biologically relevant representational learning setting. Compared\nto our previous work, we are here specifically elaborating the model in a more\nchallenging setting compared to our previous experiments and to advance more\nhidden representation layers to bring our discussions into the context of deep\nrepresentational learning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:58:18 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 08:16:18 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 05:11:10 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 01:12:25 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Trappenberg", "Thomas", ""], ["Hollensen", "Paul", ""], ["Hartono", "Pitoyo", ""]]}, {"id": "1412.6581", "submitter": "Joost van Amersfoort B.Sc.", "authors": "Otto Fabius, Joost R. van Amersfoort", "title": "Variational Recurrent Auto-Encoders", "comments": "Accepted at ICLR workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 02:07:07 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 15:04:29 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 11:21:39 GMT"}, {"version": "v4", "created": "Wed, 18 Feb 2015 15:42:44 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 14:01:18 GMT"}, {"version": "v6", "created": "Mon, 15 Jun 2015 12:35:11 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Fabius", "Otto", ""], ["van Amersfoort", "Joost R.", ""]]}, {"id": "1412.6583", "submitter": "Brian Cheung", "authors": "Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, Bruno A. Olshausen", "title": "Discovering Hidden Factors of Variation in Deep Networks", "comments": "Presented at International Conference on Learning Representations\n  2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enjoyed a great deal of success because of its ability to\nlearn useful features for tasks such as classification. But there has been less\nexploration in learning the factors of variation apart from the classification\nsignal. By augmenting autoencoders with simple regularization terms during\ntraining, we demonstrate that standard deep architectures can discover and\nexplicitly represent factors of variation beyond those relevant for\ncategorization. We introduce a cross-covariance penalty (XCov) as a method to\ndisentangle factors like handwriting style for digits and subject identity in\nfaces. We demonstrate this on the MNIST handwritten digit database, the Toronto\nFaces Database (TFD) and the Multi-PIE dataset by generating manipulated\ninstances of the data. Furthermore, we demonstrate these deep networks can\nextrapolate `hidden' variation in the supervised signal.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 02:52:03 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:41:40 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 17:15:02 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 06:47:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Cheung", "Brian", ""], ["Livezey", "Jesse A.", ""], ["Bansal", "Arjun K.", ""], ["Olshausen", "Bruno A.", ""]]}, {"id": "1412.6596", "submitter": "Scott Reed", "authors": "Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru\n  Erhan, Andrew Rabinovich", "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art deep learning systems for visual object recognition\nand detection use purely supervised training with regularization such as\ndropout to avoid overfitting. The performance depends critically on the amount\nof labeled examples, and in current practice the labels are assumed to be\nunambiguous and accurate. However, this assumption often does not hold; e.g. in\nrecognition, class labels may be missing; in detection, objects in the image\nmay not be localized; and in general, the labeling may be subjective. In this\nwork we propose a generic way to handle noisy and incomplete labeling by\naugmenting the prediction objective with a notion of consistency. We consider a\nprediction consistent if the same prediction is made given similar percepts,\nwhere the notion of similarity is between deep network features computed from\nthe input data. In experiments we demonstrate that our approach yields\nsubstantial robustness to label noise on several datasets. On MNIST handwritten\ndigits, we show that our model is robust to label corruption. On the Toronto\nFace Database, we show that our model handles well the case of subjective\nlabels in emotion recognition, achieving state-of-the- art results, and can\nalso benefit from unlabeled face images with no modification to our method. On\nthe ILSVRC2014 detection challenge data, we show that our approach extends to\nvery deep networks, high resolution images and structured outputs, and results\nin improved scalable detection.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:11:33 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 22:30:39 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 19:48:37 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Reed", "Scott", ""], ["Lee", "Honglak", ""], ["Anguelov", "Dragomir", ""], ["Szegedy", "Christian", ""], ["Erhan", "Dumitru", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1412.6597", "submitter": "Tom Paine", "authors": "Tom Le Paine, Pooya Khorrami, Wei Han, Thomas S. Huang", "title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances", "comments": "Accepted as a workshop contribution to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks perform well on object recognition because of a\nnumber of recent advances: rectified linear units (ReLUs), data augmentation,\ndropout, and large labelled datasets. Unsupervised data has been proposed as\nanother way to improve performance. Unfortunately, unsupervised pre-training is\nnot used by state-of-the-art methods leading to the following question: Is\nunsupervised pre-training still useful given recent advances? If so, when? We\nanswer this in three parts: we 1) develop an unsupervised method that\nincorporates ReLUs and recent unsupervised regularization techniques, 2)\nanalyze the benefits of unsupervised pre-training compared to data augmentation\nand dropout on CIFAR-10 while varying the ratio of unsupervised to supervised\nsamples, 3) verify our findings on STL-10. We discover unsupervised\npre-training, as expected, helps when the ratio of unsupervised to supervised\nsamples is high, and surprisingly, hurts when the ratio is low. We also use\nunsupervised pre-training with additional color augmentation to achieve near\nstate-of-the-art performance on STL-10.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:20:55 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 22:03:40 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 21:05:34 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:26:31 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Han", "Wei", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1412.6601", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri and Ilya Trofimov", "title": "Using Neural Networks for Click Prediction of Sponsored Search", "comments": "updated typos, and removed conference header", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored search is a multi-billion dollar industry and makes up a major\nsource of revenue for search engines (SE). click-through-rate (CTR) estimation\nplays a crucial role for ads selection, and greatly affects the SE revenue,\nadvertiser traffic and user experience. We propose a novel architecture for\nsolving CTR prediction problem by combining artificial neural networks (ANN)\nwith decision trees. First we compare ANN with respect to other popular machine\nlearning models being used for this task. Then we go on to combine ANN with\nMatrixNet (proprietary implementation of boosted trees) and evaluate the\nperformance of the system as a whole. The results show that our approach\nprovides significant improvement over existing models.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:44:00 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 08:15:40 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2015 23:30:51 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""], ["Trofimov", "Ilya", ""]]}, {"id": "1412.6610", "submitter": "Daniel Jiwoong  Im", "authors": "Daniel Jiwoong Im and Graham W. Taylor", "title": "Scoring and Classifying with Gated Auto-encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-encoders are perhaps the best-known non-probabilistic methods for\nrepresentation learning. They are conceptually simple and easy to train. Recent\ntheoretical work has shed light on their ability to capture manifold structure,\nand drawn connections to density modelling. This has motivated researchers to\nseek ways of auto-encoder scoring, which has furthered their use in\nclassification. Gated auto-encoders (GAEs) are an interesting and flexible\nextension of auto-encoders which can learn transformations among different\nimages or pixel covariances within images. However, they have been much less\nstudied, theoretically or empirically. In this work, we apply a dynamical\nsystems view to GAEs, deriving a scoring function, and drawing connections to\nRestricted Boltzmann Machines. On a set of deep learning benchmarks, we also\ndemonstrate their effectiveness for single and multi-label classification.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:46:05 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 18:05:21 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 16:35:39 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 18:15:16 GMT"}, {"version": "v5", "created": "Mon, 15 Jun 2015 00:25:47 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1412.6618", "submitter": "Martin Kiefel", "authors": "Martin Kiefel, Varun Jampani and Peter V. Gehler", "title": "Permutohedral Lattice CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convolutional layer that is able to process sparse\ninput features. As an example, for image recognition problems this allows an\nefficient filtering of signals that do not lie on a dense grid (like pixel\nposition), but of more general features (such as color values). The presented\nalgorithm makes use of the permutohedral lattice data structure. The\npermutohedral lattice was introduced to efficiently implement a bilateral\nfilter, a commonly used image processing operation. Its use allows for a\ngeneralization of the convolution type found in current (spatial) convolutional\nnetwork architectures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:08:54 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 14:16:58 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 11:26:34 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Kiefel", "Martin", ""], ["Jampani", "Varun", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1412.6621", "submitter": "Arnab Paul", "authors": "Arnab Paul, Suresh Venkatasubramanian", "title": "Why does Deep Learning work? - A perspective from Group Theory", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called pre-training: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:28:46 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 02:22:01 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 07:19:35 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Paul", "Arnab", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1412.6630", "submitter": "Jan Rudy", "authors": "Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor", "title": "Neural Network Regularization via Robust Weight Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is essential when training large neural networks. As deep\nneural networks can be mathematically interpreted as universal function\napproximators, they are effective at memorizing sampling noise in the training\ndata. This results in poor generalization to unseen data. Therefore, it is no\nsurprise that a new regularization technique, Dropout, was partially\nresponsible for the now-ubiquitous winning entry to ImageNet 2012 by the\nUniversity of Toronto. Currently, Dropout (and related methods such as\nDropConnect) are the most effective means of regularizing large neural\nnetworks. These amount to efficiently visiting a large number of related models\nat training time, while aggregating them to a single predictor at test time.\nThe proposed FaMe model aims to apply a similar strategy, yet learns a\nfactorization of each weight matrix such that the factors are robust to noise.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:59:14 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 13:28:46 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Rudy", "Jan", ""], ["Ding", "Weiguang", ""], ["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1412.6650", "submitter": "Alex Ter-Sarkisov", "authors": "Aram Ter-Sarkisov, Holger Schwenk, Loic Barrault and Fethi Bougares", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "comments": "accepted as workshop paper at ACL-IJCNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is today acknowledged that neural network language models outperform\nbackoff language models in applications like speech recognition or statistical\nmachine translation. However, training these models on large amounts of data\ncan take several days. We present efficient techniques to adapt a neural\nnetwork language model to new data. Instead of training a completely new model\nor relying on mixture approaches, we propose two new methods: continued\ntraining on resampled data or insertion of adaptation layers. We present\nexperimental results in an CAT environment where the post-edits of professional\ntranslators are used to improve an SMT system. Both methods are very fast and\nachieve significant improvements without overfitting the small adaptation data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 13:06:05 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 13:43:19 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 11:36:36 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 14:54:51 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Ter-Sarkisov", "Aram", ""], ["Schwenk", "Holger", ""], ["Barrault", "Loic", ""], ["Bougares", "Fethi", ""]]}, {"id": "1412.6749", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction", "comments": "Eighteen pages in all, but much of the central ideas are covered in\n  the first five and a half pages; most of the remaining pages are devoted to\n  straightforward mathematical derivations, and the presentation of three\n  algorithms. Manuscript contains no figures at this time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By drawing on ideas from optimisation theory, artificial neural networks\n(ANN), graph embeddings and sparse representations, I develop a novel\ntechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at\naddressing the feature extraction problem. The proposed method uses (preferably\ndeep) ANNs for projecting input attribute vectors to an output space wherein\npairwise distances are maximized for vectors belonging to different classes,\nbut minimized for those belonging to the same class, while simultaneously\nenforcing sparsity on the ANN outputs. The vectors that result from the\nprojection can then be used as features in any classifier of choice.\nMathematically, I formulate the proposed method as the minimisation of an\nobjective function which can be interpreted, in the ANN output space, as a\nnegative factor of the sum of the squares of the pair-wise distances between\noutput vectors belonging to different classes, added to a positive factor of\nthe sum of squares of the pair-wise distances between output vectors belonging\nto the same classes, plus sparsity and weight decay terms. To derive an\nalgorithm for minimizing the objective function via gradient descent, I use the\nmulti-variate version of the chain rule to obtain the partial derivatives of\nthe function with respect to ANN weights and biases, and find that each of the\nrequired partial derivatives can be expressed as a sum of six terms. As it\nturns out, four of those six terms can be computed using the standard back\npropagation algorithm; the fifth can be computed via a slight modification of\nthe standard backpropagation algorithm; while the sixth one can be computed via\nsimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora\nof digits and letters, the CMU PIE database of faces, the MNIST digits\ndatabase, and other standard machine learning databases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 09:28:05 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6806", "submitter": "Alexey Dosovitskiy", "authors": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin\n  Riedmiller", "title": "Striving for Simplicity: The All Convolutional Net", "comments": "accepted to ICLR-2015 workshop track; no changes other than style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 16:16:37 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 21:44:06 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 07:58:17 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1412.6830", "submitter": "Forest Agostinelli", "authors": "Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi", "title": "Learning Activation Functions to Improve Deep Neural Networks", "comments": "Accepted as a workshop paper contribution at the International\n  Conference on Learning Representations (ICLR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 20:20:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 21:44:41 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 08:05:02 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Agostinelli", "Forest", ""], ["Hoffman", "Matthew", ""], ["Sadowski", "Peter", ""], ["Baldi", "Pierre", ""]]}, {"id": "1412.6856", "submitter": "Bolei Zhou", "authors": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio\n  Torralba", "title": "Object Detectors Emerge in Deep Scene CNNs", "comments": "12 pages, ICLR 2015 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of new computational architectures for visual processing,\nsuch as convolutional neural networks (CNN) and access to image databases with\nmillions of labeled examples (e.g., ImageNet, Places), the state of the art in\ncomputer vision is advancing rapidly. One important factor for continued\nprogress is to understand the representations that are learned by the inner\nlayers of these deep architectures. Here we show that object detectors emerge\nfrom training CNNs to perform scene classification. As scenes are composed of\nobjects, the CNN for scene classification automatically discovers meaningful\nobjects detectors, representative of the learned scene categories. With object\ndetectors emerging as a result of learning to recognize scenes, our work\ndemonstrates that the same network can perform both scene recognition and\nobject localization in a single forward-pass, without ever having been\nexplicitly taught the notion of objects.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 01:14:01 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 19:06:41 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Zhou", "Bolei", ""], ["Khosla", "Aditya", ""], ["Lapedriza", "Agata", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1412.6857", "submitter": "Tyng-Luh Liu", "authors": "Jyh-Jing Hwang and Tyng-Luh Liu", "title": "Contour Detection Using Cost-Sensitive Convolutional Neural Networks", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. The main challenge lies\nin adapting a pre-trained per-image CNN model for yielding per-pixel image\nfeatures. We propose to base on the DenseNet architecture to achieve pixelwise\nfine-tuning and then consider a cost-sensitive strategy to further improve the\nlearning with a small dataset of edge and non-edge image patches. In the\nexperiment of contour detection, we look into the effectiveness of combining\nper-pixel features from different CNN layers and obtain comparable performances\nto the state-of-the-art on BSDS500.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 01:16:50 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 14:37:27 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 15:01:16 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 07:37:54 GMT"}, {"version": "v5", "created": "Tue, 12 May 2015 08:42:42 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1412.6885", "submitter": "Jun Yuan", "authors": "Jun Yuan, Bingbing Ni, Ashraf A.Kassim", "title": "Half-CNN: A General Framework for Whole-Image Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Network (CNN) has achieved great success in image\nclassification. The classification model can also be utilized at image or patch\nlevel for many other applications, such as object detection and segmentation.\nIn this paper, we propose a whole-image CNN regression model, by removing the\nfull connection layer and training the network with continuous feature maps.\nThis is a generic regression framework that fits many applications. We\ndemonstrate this method through two tasks: simultaneous face detection &\nsegmentation, and scene saliency prediction. The result is comparable with\nother models in the respective fields, using only a small scale network. Since\nthe regression model is trained on corresponding image / feature map pairs,\nthere are no requirements on uniform input size as opposed to the\nclassification model. Our framework avoids classifier design, a process that\nmay introduce too much manual intervention in model development. Yet, it is\nhighly correlated to the classification network and offers some in-deep review\nof CNN structures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 06:43:58 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Yuan", "Jun", ""], ["Ni", "Bingbing", ""], ["Kassim", "Ashraf A.", ""]]}, {"id": "1412.7003", "submitter": "Shin-ichi Maeda", "authors": "Shin-ichi Maeda", "title": "A Bayesian encourages dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is one of the key techniques to prevent the learning from\noverfitting. It is explained that dropout works as a kind of modified L2\nregularization. Here, we shed light on the dropout from Bayesian standpoint.\nBayesian interpretation enables us to optimize the dropout rate, which is\nbeneficial for learning of weight parameters and prediction after learning. The\nexperiment result also encourages the optimization of the dropout.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:46:26 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 06:17:16 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 06:32:47 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Maeda", "Shin-ichi", ""]]}, {"id": "1412.7006", "submitter": "Vivek Venugopalan", "authors": "Michael Giering, Vivek Venugopalan, Kishore Reddy", "title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural\n  Networks", "comments": "7 pages, double column, IEEE format, accepted at IEEE HPEC 2015", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322485", "report-no": "1214_v3", "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to simultaneously leverage multiple modes of sensor information\nis critical for perception of an automated vehicle's physical surroundings.\nSpatio-temporal alignment of registration of the incoming information is often\na prerequisite to analyzing the fused data. The persistence and reliability of\nmulti-modal registration is therefore the key to the stability of decision\nsupport systems ingesting the fused information. LiDAR-video systems like on\nthose many driverless cars are a common example of where keeping the LiDAR and\nvideo channels registered to common physical features is important. We develop\na deep learning method that takes multiple channels of heterogeneous data, to\ndetect the misalignment of the LiDAR-video inputs. A number of variations were\ntested on the Ford LiDAR-video driving test data set and will be discussed. To\nthe best of our knowledge the use of multi-modal deep convolutional neural\nnetworks for dynamic real-time LiDAR-video registration has not been presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:54:53 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 01:14:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Giering", "Michael", ""], ["Venugopalan", "Vivek", ""], ["Reddy", "Kishore", ""]]}, {"id": "1412.7007", "submitter": "Vivek Venugopalan", "authors": "Soumik Sarkar, Vivek Venugopalan, Kishore Reddy, Michael Giering,\n  Julian Ryde, Navdeep Jaitly", "title": "Occlusion Edge Detection in RGB-D Frames using Deep Convolutional\n  Networks", "comments": "7 pages, double column, IEEE HPEC 2015 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion edges in images which correspond to range discontinuity in the\nscene from the point of view of the observer are an important prerequisite for\nmany vision and mobile robot tasks. Although they can be extracted from range\ndata however extracting them from images and videos would be extremely\nbeneficial. We trained a deep convolutional neural network (CNN) to identify\nocclusion edges in images and videos with both RGB-D and RGB inputs. The use of\nCNN avoids hand-crafting of features for automatically isolating occlusion\nedges and distinguishing them from appearance edges. Other than quantitative\nocclusion edge detection results, qualitative results are provided to\ndemonstrate the trade-off between high resolution analysis and frame-level\ncomputation time which is critical for real-time robotics applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:55:17 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 12:50:48 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 01:07:23 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Sarkar", "Soumik", ""], ["Venugopalan", "Vivek", ""], ["Reddy", "Kishore", ""], ["Giering", "Michael", ""], ["Ryde", "Julian", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1412.7009", "submitter": "Jan Rudy", "authors": "Jan Rudy, Graham Taylor", "title": "Generative Class-conditional Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Bengio et al. (2013) proposes a sampling procedure for\ndenoising autoencoders which involves learning the transition operator of a\nMarkov chain. The transition operator is typically unimodal, which limits its\ncapacity to model complex data. In order to perform efficient sampling from\nconditional distributions, we extend this work, both theoretically and\nalgorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is\nable to generate convincing class-conditional samples when trained on both the\nMNIST and TFD datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:57:05 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 00:16:55 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 01:54:33 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Rudy", "Jan", ""], ["Taylor", "Graham", ""]]}, {"id": "1412.7024", "submitter": "Matthieu Courbariaux", "authors": "Matthieu Courbariaux, Yoshua Bengio and Jean-Pierre David", "title": "Training deep neural networks with low precision multiplications", "comments": "10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:22:45 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 18:05:12 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 00:26:12 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 22:52:43 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 01:00:44 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Courbariaux", "Matthieu", ""], ["Bengio", "Yoshua", ""], ["David", "Jean-Pierre", ""]]}, {"id": "1412.7028", "submitter": "Jo\\\"el Legrand", "authors": "Jo\\\"el Legrand and Ronan Collobert", "title": "Joint RNN-Based Greedy Parsing and Word Composition", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:40:31 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 17:39:39 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 15:04:34 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:57:49 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Legrand", "Jo\u00ebl", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.7054", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Andrea Frome, Esteban Real", "title": "Attention for Fine-Grained Categorization", "comments": "ICLR 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents experiments extending the work of Ba et al. (2014) on\nrecurrent neural models for attention into less constrained visual\nenvironments, specifically fine-grained categorization on the Stanford Dogs\ndata set. In this work we use an RNN of the same structure but substitute a\nmore powerful visual network and perform large-scale pre-training of the visual\nnetwork outside of the attention RNN. Most work in attention models to date\nfocuses on tasks with toy or more constrained visual environments, whereas we\npresent results for fine-grained categorization better than the\nstate-of-the-art GoogLeNet classification model. We show that our model learns\nto direct high resolution attention to the most discriminative regions without\nany spatial supervision such as bounding boxes, and it is able to discriminate\nfine-grained dog breeds moderately well even when given only an initial\nlow-resolution context image and narrow, inexpensive glimpses at faces and fur\npatterns. This and similar attention models have the major advantage of being\ntrained end-to-end, as opposed to other current detection and recognition\npipelines with hand-engineered components where information is lost. While our\nmodel is state-of-the-art, further work is needed to fully leverage the\nsequential input.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:06:07 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 00:15:45 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 01:45:56 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Sermanet", "Pierre", ""], ["Frome", "Andrea", ""], ["Real", "Esteban", ""]]}, {"id": "1412.7062", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin\n  Murphy and Alan L. Yuille", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully\n  Connected CRFs", "comments": "14 pages. Updated related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\nart performance in high level vision tasks, such as image classification and\nobject detection. This work brings together methods from DCNNs and\nprobabilistic graphical models for addressing the task of pixel-level\nclassification (also called \"semantic image segmentation\"). We show that\nresponses at the final layer of DCNNs are not sufficiently localized for\naccurate object segmentation. This is due to the very invariance properties\nthat make DCNNs good for high level tasks. We overcome this poor localization\nproperty of deep networks by combining the responses at the final DCNN layer\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\n\"DeepLab\" system is able to localize segment boundaries at a level of accuracy\nwhich is beyond previous methods. Quantitatively, our method sets the new\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\n71.6% IOU accuracy in the test set. We show how these results can be obtained\nefficiently: Careful network re-purposing and a novel application of the 'hole'\nalgorithm from the wavelet community allow dense computation of neural net\nresponses at 8 frames per second on a modern GPU.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:18:33 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 18:21:29 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 17:14:50 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 04:00:08 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Papandreou", "George", ""], ["Kokkinos", "Iasonas", ""], ["Murphy", "Kevin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1412.7063", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran", "title": "Diverse Embedding Neural Network Language Models", "comments": "Under review as workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:19:56 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 21:33:46 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 19:53:21 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 21:55:15 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 20:07:50 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1412.7091", "submitter": "Pascal Vincent", "authors": "Pascal Vincent, Alexandre de Br\\'ebisson, Xavier Bouthillier", "title": "Efficient Exact Gradient Update for training Deep Networks with Very\n  Large Sparse Targets", "comments": "15 pages technical report version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g. 200\n000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the D x d output weight matrix and computing the gradient needed for\nbackpropagation to previous layers. While efficient handling of large sparse\nnetwork inputs is trivial, the case of large sparse targets is not, and has\nthus so far been sidestepped with approximate alternatives such as hierarchical\nsoftmax or sampling-based approximations during training. In this work we\ndevelop an original algorithmic approach which, for a family of loss functions\nthat includes squared error and spherical softmax, can compute the exact loss,\ngradient update for the output weights, and gradient for backpropagation, all\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\ntwo orders of magnitude for typical sizes, for that critical part of the\ncomputations that often dominates the training time in this kind of network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 18:51:08 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 04:02:12 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2015 01:27:13 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Vincent", "Pascal", ""], ["de Br\u00e9bisson", "Alexandre", ""], ["Bouthillier", "Xavier", ""]]}, {"id": "1412.7110", "submitter": "Dimitri Palaz", "authors": "Dimitri Palaz, Mathew Magimai Doss and Ronan Collobert", "title": "Learning linearly separable features for speech recognition using\n  convolutional neural networks", "comments": "Final version for ICLR 2015 Workshop; Revisions according to reviews.\n  Revised Section 4.5. Add references and correct typos. Submitted for ICLR\n  2015 conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 19:46:01 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 13:46:10 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 10:44:21 GMT"}, {"version": "v4", "created": "Thu, 26 Feb 2015 19:51:35 GMT"}, {"version": "v5", "created": "Fri, 27 Feb 2015 16:31:32 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 08:29:14 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Palaz", "Dimitri", ""], ["Doss", "Mathew Magimai", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.7122", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko", "title": "Learning Deep Object Detectors from 3D Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced 3D CAD models are becoming easily accessible online, and can\npotentially generate an infinite number of training images for almost any\nobject category.We show that augmenting the training data of contemporary Deep\nConvolutional Neural Net (DCNN) models with such synthetic data can be\neffective, especially when real training data is limited or not well matched to\nthe target domain. Most freely available CAD models capture 3D shape but are\noften missing other low level cues, such as realistic object texture, pose, or\nbackground. In a detailed analysis, we use synthetic CAD-rendered images to\nprobe the ability of DCNN to learn without these cues, with surprising\nfindings. In particular, we show that when the DCNN is fine-tuned on the target\ndetection task, it exhibits a large degree of invariance to missing low-level\ncues, but, when pretrained on generic ImageNet classification, it learns better\nwhen the low-level cues are simulated. We show that our synthetic DCNN training\napproach significantly outperforms previous methods on the PASCAL VOC2007\ndataset when learning in the few-shot scenario and improves performance in a\ndomain shift scenario on the Office benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:10:31 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 23:44:24 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 17:56:07 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2015 01:01:39 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Peng", "Xingchao", ""], ["Sun", "Baochen", ""], ["Ali", "Karim", ""], ["Saenko", "Kate", ""]]}, {"id": "1412.7144", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Evan Shelhamer, Jonathan Long and Trevor Darrell", "title": "Fully Convolutional Multi-Class Multiple Instance Learning", "comments": "in ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) can reduce the need for costly annotation in\ntasks such as semantic segmentation by weakening the required degree of\nsupervision. We propose a novel MIL formulation of multi-class semantic\nsegmentation learning by a fully convolutional network. In this setting, we\nseek to learn a semantic segmentation model from just weak image-level labels.\nThe model is trained end-to-end to jointly optimize the representation while\ndisambiguating the pixel-image label assignment. Fully convolutional training\naccepts inputs of any size, does not need object proposal pre-processing, and\noffers a pixelwise loss map for selecting latent instances. Our multi-class MIL\nloss exploits the further supervision given by images with multiple labels. We\nevaluate this approach through preliminary experiments on the PASCAL VOC\nsegmentation challenge.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:49:54 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 01:17:59 GMT"}, {"version": "v3", "created": "Sat, 7 Feb 2015 02:12:26 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2015 05:31:10 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Shelhamer", "Evan", ""], ["Long", "Jonathan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.7149", "submitter": "Zichao Yang", "authors": "Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex\n  Smola, Le Song, Ziyu Wang", "title": "Deep Fried Convnets", "comments": "svd experiments included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully connected layers of a deep convolutional neural network typically\ncontain over 90% of the network parameters, and consume the majority of the\nmemory required to store the network parameters. Reducing the number of\nparameters while preserving essentially the same predictive performance is\ncritically important for operating deep neural networks in memory constrained\nenvironments such as GPUs or embedded devices.\n  In this paper we show how kernel methods, in particular a single Fastfood\nlayer, can be used to replace all fully connected layers in a deep\nconvolutional neural network. This novel Fastfood layer is also end-to-end\ntrainable in conjunction with convolutional layers, allowing us to combine them\ninto a new architecture, named deep fried convolutional networks, which\nsubstantially reduces the memory footprint of convolutional networks trained on\nMNIST and ImageNet with no drop in predictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:53:30 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:17:24 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 21:30:55 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2015 20:17:26 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Zichao", ""], ["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""], ["Smola", "Alex", ""], ["Song", "Le", ""], ["Wang", "Ziyu", ""]]}, {"id": "1412.7155", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Lisa Anne Hendricks, Trevor Darrell", "title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "comments": "4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, nested dropout was proposed as a method for ordering representation\nunits in autoencoders by their information content, without diminishing\nreconstruction cost. However, it has only been applied to training\nfully-connected autoencoders in an unsupervised setting. We explore the impact\nof nested dropout on the convolutional layers in a CNN trained by\nbackpropagation, investigating whether nested dropout can provide a simple and\nsystematic way to determine the optimal representation size with respect to the\ndesired accuracy and desired task and data complexity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:59:58 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 01:47:57 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 00:07:59 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 06:11:22 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Finn", "Chelsea", ""], ["Hendricks", "Lisa Anne", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.7185", "submitter": "Mehran Fasihozaman Langerudi", "authors": "Mehran Fasihozaman Langerudi", "title": "Parameter Selection In Particle Swarm Optimization For Transportation\n  Network Design Problem", "comments": "21 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In transportation planning and development, transport network design problem\nseeks to optimize specific objectives (e.g. total travel time) through choosing\namong a given set of projects while keeping consumption of resources (e.g.\nbudget) within their limits. Due to the numerous cases of choosing projects,\nsolving such a problem is very difficult and time-consuming. Based on particle\nswarm optimization (PSO) technique, a heuristic solution algorithm for the\nbi-level problem is designed. This paper evaluates the algorithm performance in\nthe response of changing certain basic PSO parameters.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:04:35 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 23:49:40 GMT"}, {"version": "v3", "created": "Tue, 3 Feb 2015 06:56:01 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Langerudi", "Mehran Fasihozaman", ""]]}, {"id": "1412.7190", "submitter": "Mathieu Aubry", "authors": "Francisco Massa, Mathieu Aubry, Renaud Marlet", "title": "Convolutional Neural Networks for joint object detection and pose\n  estimation: A comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the application of convolutional neural networks for\njointly detecting objects depicted in still images and estimating their 3D\npose. We identify different feature representations of oriented objects, and\nenergies that lead a network to learn this representations. The choice of the\nrepresentation is crucial since the pose of an object has a natural, continuous\nstructure while its category is a discrete variable. We evaluate the different\napproaches on the joint object detection and pose estimation task of the\nPascal3D+ benchmark using Average Viewpoint Precision. We show that a\nclassification approach on discretized viewpoints achieves state-of-the-art\nperformance for joint object detection and pose estimation, and significantly\noutperforms existing baselines on this benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:26:26 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 16:43:41 GMT"}, {"version": "v3", "created": "Sat, 7 Feb 2015 05:27:24 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 05:15:45 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Massa", "Francisco", ""], ["Aubry", "Mathieu", ""], ["Marlet", "Renaud", ""]]}, {"id": "1412.7193", "submitter": "Giljin Jang", "authors": "Giljin Jang, Han-Gyu Kim, Yung-Hwan Oh", "title": "Audio Source Separation Using a Deep Autoencoder", "comments": "3 pages, 4 figures, ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for unsupervised audio source\nseparation using a deep autoencoder. The characteristics of unknown source\nsignals mixed in the mixed input is automatically by properly configured\nautoencoders implemented by a network with many layers, and separated by\nclustering the coefficient vectors in the code layer. By investigating the\nweight vectors to the final target, representation layer, the primitive\ncomponents of the audio signals in the frequency domain are observed. By\nclustering the activation coefficients in the code layer, the previously\nunknown source signals are segregated. The original source sounds are then\nseparated and reconstructed by using code vectors which belong to different\nclusters. The restored sounds are not perfect but yield promising results for\nthe possibility in the success of many practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:38:06 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Jang", "Giljin", ""], ["Kim", "Han-Gyu", ""], ["Oh", "Yung-Hwan", ""]]}, {"id": "1412.7210", "submitter": "Antti Rasmus", "authors": "Antti Rasmus, Tapani Raiko, Harri Valpola", "title": "Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images", "comments": "Presentation at ICLR 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:36:15 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 12:17:51 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 20:56:43 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2015 15:49:16 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Raiko", "Tapani", ""], ["Valpola", "Harri", ""]]}, {"id": "1412.7259", "submitter": "Dong Wang", "authors": "Dong Wang and Xiaoyang Tan", "title": "Unsupervised Feature Learning with C-SVDDNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of learning feature representation\nfrom unlabeled data using a single-layer K-means network. A K-means network\nmaps the input data into a feature representation by finding the nearest\ncentroid for each input point, which has attracted researchers' great attention\nrecently due to its simplicity, effectiveness, and scalability. However, one\ndrawback of this feature mapping is that it tends to be unreliable when the\ntraining data contains noise. To address this issue, we propose a SVDD based\nfeature learning algorithm that describes the density and distribution of each\ncluster from K-means with an SVDD ball for more robust feature representation.\nFor this purpose, we present a new SVDD algorithm called C-SVDD that centers\nthe SVDD ball towards the mode of local density of each cluster, and we show\nthat the objective of C-SVDD can be solved very efficiently as a linear\nprogramming problem. Additionally, traditional unsupervised feature learning\nmethods usually take an average or sum of local representations to obtain\nglobal representation which ignore spatial relationship among them. To use\nspatial information we propose a global representation with a variant of SIFT\ndescriptor. The architecture is also extended with multiple receptive field\nscales and multiple pooling sizes. Extensive experiments on several popular\nobject recognition benchmarks, such as STL-10, MINST, Holiday and Copydays\nshows that the proposed C-SVDDNet method yields comparable or better\nperformance than that of the previous state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 05:56:50 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 05:31:03 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 09:50:54 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Wang", "Dong", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1412.7272", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Emre Neftci and Gert Cauwenberghs", "title": "Learning Non-deterministic Representations with Energy-based Ensembles", "comments": "9 pages, 3 figures, ICLR-15 workshop contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a generative model is to capture the distribution underlying the\ndata, typically through latent variables. After training, these variables are\noften used as a new representation, more effective than the original features\nin a variety of learning tasks. However, the representations constructed by\ncontemporary generative models are usually point-wise deterministic mappings\nfrom the original feature space. Thus, even with representations robust to\nclass-specific transformations, statistically driven models trained on them\nwould not be able to generalize when the labeled data is scarce. Inspired by\nthe stochasticity of the synaptic connections in the brain, we introduce\nEnergy-based Stochastic Ensembles. These ensembles can learn non-deterministic\nrepresentations, i.e., mappings from the feature space to a family of\ndistributions in the latent space. These mappings are encoded in a distribution\nover a (possibly infinite) collection of models. By conditionally sampling\nmodels from the ensemble, we obtain multiple representations for every input\nexample and effectively augment the data. We propose an algorithm similar to\ncontrastive divergence for training restricted Boltzmann stochastic ensembles.\nFinally, we demonstrate the concept of the stochastic representations on a\nsynthetic dataset as well as test them in the one-shot learning scenario on\nMNIST.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 07:06:55 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 10:04:49 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Neftci", "Emre", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1412.7419", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski and Yoshua Bengio", "title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient", "comments": "8 pages, 3 figures, ICLR workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:55:08 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 19:15:01 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 00:46:01 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2015 18:52:17 GMT"}, {"version": "v5", "created": "Sun, 1 Nov 2015 03:05:18 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.7479", "submitter": "Sudheendra Vijayanarasimhan", "authors": "Sudheendra Vijayanarasimhan and Jonathon Shlens and Rajat Monga and\n  Jay Yagnik", "title": "Deep Networks With Large Output Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been extremely successful at various image, speech,\nvideo recognition tasks because of their ability to model deep structures\nwithin the data. However, they are still prohibitively expensive to train and\napply for problems containing millions of classes in the output layer. Based on\nthe observation that the key computation common to most neural network layers\nis a vector/matrix product, we propose a fast locality-sensitive hashing\ntechnique to approximate the actual dot product enabling us to scale up the\ntraining and inference to millions of output classes. We evaluate our technique\non three diverse large-scale recognition tasks and show that our approach can\ntrain large-scale models at a faster rate (in terms of steps/total time)\ncompared to baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:22:59 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 18:45:36 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 01:12:58 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 19:53:21 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Vijayanarasimhan", "Sudheendra", ""], ["Shlens", "Jonathon", ""], ["Monga", "Rajat", ""], ["Yagnik", "Jay", ""]]}, {"id": "1412.7489", "submitter": "Yongxin Yang", "authors": "Yongxin Yang and Timothy M. Hospedales", "title": "A Unified Perspective on Multi-Domain and Multi-Task Learning", "comments": "9 pages, Accepted to ICLR 2015 Conference Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a new neural-network based perspective on\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\nas different ways of constructing semantic descriptors. Our interpretation\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\nfor a novel class can be constructed without training data. Moreover, it leads\nto a new and practically relevant problem setting of zero-shot domain\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\nfor an unseen domain can be generated by its semantic descriptor. Experiments\nacross this range of problems demonstrate that our framework outperforms a\nvariety of alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 19:50:21 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 15:10:46 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 15:29:50 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1412.7522", "submitter": "Orhan Firat", "authors": "Orhan Firat, Emre Aksan, Ilke Oztekin, Fatos T. Yarman Vural", "title": "Learning Deep Temporal Representations for Brain Decoding", "comments": "This paper has been withdrawn for a revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Functional magnetic resonance imaging produces high dimensional data, with a\nless then ideal number of labelled samples for brain decoding tasks (predicting\nbrain states). In this study, we propose a new deep temporal convolutional\nneural network architecture with spatial pooling for brain decoding which aims\nto reduce dimensionality of feature space along with improved classification\nperformance. Temporal representations (filters) for each layer of the\nconvolutional model are learned by leveraging unlabelled fMRI data in an\nunsupervised fashion with regularized autoencoders. Learned temporal\nrepresentations in multiple levels capture the regularities in the temporal\ndomain and are observed to be a rich bank of activation patterns which also\nexhibit similarities to the actual hemodynamic responses. Further, spatial\npooling layers in the convolutional architecture reduce the dimensionality\nwithout losing excessive information. By employing the proposed temporal\nconvolutional architecture with spatial pooling, raw input fMRI data is mapped\nto a non-linear, highly-expressive and low-dimensional feature space where the\nfinal classification is conducted. In addition, we propose a simple heuristic\napproach for hyper-parameter tuning when no validation data is available.\nProposed method is tested on a ten class recognition memory experiment with\nnine subjects. The results support the efficiency and potential of the proposed\nmodel, compared to the baseline multi-voxel pattern analysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 20:53:09 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 10:31:13 GMT"}, {"version": "v3", "created": "Thu, 25 Dec 2014 07:43:42 GMT"}, {"version": "v4", "created": "Mon, 12 Jan 2015 05:03:58 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Firat", "Orhan", ""], ["Aksan", "Emre", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1412.7525", "submitter": "Dong-Hyun Lee", "authors": "Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, Yoshua Bengio", "title": "Difference Target Propagation", "comments": "13 pages, 8 figures, Accepted in ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Back-propagation has been the workhorse of recent successes of deep learning\nbut it relies on infinitesimal effects (partial derivatives) in order to\nperform credit assignment. This could become a serious issue as one considers\ndeeper and more non-linear functions, e.g., consider the extreme case of\nnonlinearity where the relation between parameters and cost is actually\ndiscrete. Inspired by the biological implausibility of back-propagation, a few\napproaches have been proposed in the past that could play a similar credit\nassignment role. In this spirit, we explore a novel approach to credit\nassignment in deep networks that we call target propagation. The main idea is\nto compute targets rather than gradients, at each layer. Like gradients, they\nare propagated backwards. In a way that is related but different from\npreviously proposed proxies for back-propagation which rely on a backwards\nnetwork with symmetric weights, target propagation relies on auto-encoders at\neach layer. Unlike back-propagation, it can be applied even when units exchange\nstochastic bits rather than real numbers. We show that a linear correction for\nthe imperfectness of the auto-encoders, called difference target propagation,\nis very effective to make target propagation actually work, leading to results\ncomparable to back-propagation for deep networks with discrete and continuous\nunits and denoising auto-encoders and achieving state of the art for stochastic\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 20:57:59 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 16:54:57 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2015 01:01:54 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2015 07:05:40 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2015 02:30:41 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Lee", "Dong-Hyun", ""], ["Zhang", "Saizheng", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.7580", "submitter": "Nicolas Vasilache", "authors": "Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala,\n  Serkan Piantino, Yann LeCun", "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation", "comments": "Camera ready for ICLR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the performance profile of Convolutional Neural Network training\non the current generation of NVIDIA Graphics Processing Units. We introduce two\nnew Fast Fourier Transform convolution implementations: one based on NVIDIA's\ncuFFT library, and another based on a Facebook authored FFT implementation,\nfbfft, that provides significant speedups over cuFFT (over 1.5x) for whole\nCNNs. Both of these convolution implementations are available in open source,\nand are faster than NVIDIA's cuDNN implementation for many common convolutional\nlayers (up to 23.5x for some synthetic kernel configurations). We discuss\ndifferent performance regimes of convolutions, comparing areas where\nstraightforward time domain convolutions outperform Fourier frequency domain\nconvolutions. Details on algorithmic applications of NVIDIA GPU hardware\nspecifics in the implementation of fbfft are also provided.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 01:31:36 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 16:55:04 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 20:01:00 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Vasilache", "Nicolas", ""], ["Johnson", "Jeff", ""], ["Mathieu", "Michael", ""], ["Chintala", "Soumith", ""], ["Piantino", "Serkan", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.7659", "submitter": "Taco Cohen", "authors": "Taco S. Cohen and Max Welling", "title": "Transformation Properties of Learned Visual Representations", "comments": "T.S. Cohen & M. Welling, Transformation Properties of Learned Visual\n  Representations. In International Conference on Learning Representations\n  (ICLR), 2015", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a three-dimensional object moves relative to an observer, a change\noccurs on the observer's image plane and in the visual representation computed\nby a learned model. Starting with the idea that a good visual representation is\none that transforms linearly under scene motions, we show, using the theory of\ngroup representations, that any such representation is equivalent to a\ncombination of the elementary irreducible representations. We derive a striking\nrelationship between irreducibility and the statistical dependency structure of\nthe representation, by showing that under restricted conditions, irreducible\nrepresentations are decorrelated. Under partial observability, as induced by\nthe perspective projection of a scene onto the image plane, the motion group\ndoes not have a linear action on the space of images, so that it becomes\nnecessary to perform inference over a latent representation that does transform\nlinearly. This idea is demonstrated in a model of rotating NORB objects that\nemploys a latent representation of the non-commutative 3D rotation group SO(3).\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 13:19:20 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:46:00 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 21:20:04 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1412.7753", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu,\n  Marc'Aurelio Ranzato", "title": "Learning Longer Memory in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network is a powerful model that learns temporal patterns in\nsequential data. For a long time, it was believed that recurrent networks are\ndifficult to train using simple optimizers, such as stochastic gradient\ndescent, due to the so-called vanishing gradient problem. In this paper, we\nshow that learning longer term patterns in real data, such as in natural\nlanguage, is perfectly possible using gradient descent. This is achieved by\nusing a slight structural modification of the simple recurrent neural network\narchitecture. We encourage some of the hidden units to change their state\nslowly by making part of the recurrent weight matrix close to identity, thus\nforming kind of a longer term memory. We evaluate our model in language\nmodeling experiments, where we obtain similar performance to the much more\ncomplex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber,\n1997).\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 20:58:18 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 23:37:58 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Mikolov", "Tomas", ""], ["Joulin", "Armand", ""], ["Chopra", "Sumit", ""], ["Mathieu", "Michael", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1412.7755", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu", "title": "Multiple Object Recognition with Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based model for recognizing multiple objects in\nimages. The proposed model is a deep recurrent neural network trained with\nreinforcement learning to attend to the most relevant regions of the input\nimage. We show that the model learns to both localize and recognize multiple\nobjects despite being given only class labels during training. We evaluate the\nmodel on the challenging task of transcribing house number sequences from\nGoogle Street View images and show that it is both more accurate than the\nstate-of-the-art convolutional networks and uses fewer parameters and less\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 20:58:23 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 16:49:23 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Ba", "Jimmy", ""], ["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1412.7774", "submitter": "Jongwon Ha", "authors": "Chol Man Ho, Son Il Gwak, Song Ho Pak, Jong Won Ha", "title": "Improved Parameter Identification Method Based on Moving Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  To improve the problem that the parameter identification for fuzzy neural\nnetwork has many time complexities in calculating, an improved T-S fuzzy\ninference method and an parameter identification method for fuzzy neural\nnetwork are proposed. It mainly includes three parts. First, improved fuzzy\ninference method based on production term for T-S Fuzzy model is explained.\nThen, compared with existing Sugeno fuzzy inference based on Compositional\nrules and type-distance fuzzy inference method, the proposed fuzzy inference\nalgorithm has a less amount of complexity in calculating and the calculating\nprocess is simple. Next, a parameter identification method for FNN based on\nproduction inference is proposed. Finally, the proposed method is applied for\nthe precipitation forecast and security situation prediction. Test results\nshowed that the proposed method significantly improved the effectiveness of\nidentification, reduced the learning order, time complexity and learning error.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 01:11:11 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Ho", "Chol Man", ""], ["Gwak", "Son Il", ""], ["Pak", "Song Ho", ""], ["Ha", "Jong Won", ""]]}, {"id": "1412.7828", "submitter": "S{\\o}ren S{\\o}nderby", "authors": "S{\\o}ren Kaae S{\\o}nderby and Ole Winther", "title": "Protein Secondary Structure Prediction with Long Short Term Memory\n  Networks", "comments": "v2: adds larger network with slightly better results, update author\n  affiliations", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of protein secondary structure from the amino acid sequence is a\nclassical bioinformatics problem. Common methods use feed forward neural\nnetworks or SVMs combined with a sliding window, as these models does not\nnaturally handle sequential data. Recurrent neural networks are an\ngeneralization of the feed forward neural network that naturally handle\nsequential data. We use a bidirectional recurrent neural network with long\nshort term memory cells for prediction of secondary structure and evaluate\nusing the CB513 dataset. On the secondary structure 8-class problem we report\nbetter performance (0.674) than state of the art (0.664). Our model includes\nfeed forward networks between the long short term memory cells, a path that can\nbe further explored.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 14:27:42 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 19:44:17 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1412.7927", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel, Raunaq Vohra, and J.K. Sahoo", "title": "Polyphonic Music Generation by Modeling Temporal Dependencies Using a\n  RNN-DBN", "comments": "8 pages, A4, 1 figure, 1 table, ICANN 2014 oral presentation. arXiv\n  admin note: text overlap with arXiv:1206.6392 by other authors", "journal-ref": "Lecture Notes in Computer Science Volume 8681, 2014, pp 217-224", "doi": "10.1007/978-3-319-11179-7_28", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic technique to model temporal dependencies\nand sequences using a combination of a recurrent neural network and a Deep\nBelief Network. Our technique, RNN-DBN, is an amalgamation of the memory state\nof the RNN that allows it to provide temporal information and a multi-layer DBN\nthat helps in high level representation of the data. This makes RNN-DBNs ideal\nfor sequence generation. Further, the use of a DBN in conjunction with the RNN\nmakes this model capable of significantly more complex data representation than\nan RBM. We apply this technique to the task of polyphonic music generation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 11:08:42 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""], ["Sahoo", "J. K.", ""]]}, {"id": "1412.7955", "submitter": "Santosh Vempala", "authors": "Christos H. Papadimitriou and Santosh S. Vempala", "title": "Unsupervised Learning through Prediction in a Model of Cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a primitive called PJOIN, for \"predictive join,\" which combines\nand extends the operations JOIN and LINK, which Valiant proposed as the basis\nof a computational theory of cortex. We show that PJOIN can be implemented in\nValiant's model. We also show that, using PJOIN, certain reasonably complex\nlearning and pattern matching tasks can be performed, in a way that involves\nphenomena which have been observed in cognition and the brain, namely\nmemory-based prediction and downward traffic in the cortical hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 16:41:04 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Papadimitriou", "Christos H.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1412.8291", "submitter": "Christian Osendorfer", "authors": "Maximilian Karl and Christian Osendorfer", "title": "Improving approximate RPCA with a k-sparsity prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A process centric view of robust PCA (RPCA) allows its fast approximate\nimplementation based on a special form o a deep neural network with weights\nshared across all layers. However, empirically this fast approximation to RPCA\nfails to find representations that are parsemonious. We resolve these bad local\nminima by relaxing the elementwise L1 and L2 priors and instead utilize a\nstructure inducing k-sparsity prior. In a discriminative classification task\nthe newly learned representations outperform these from the original\napproximate RPCA formulation significantly.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 09:51:20 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Karl", "Maximilian", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1412.8307", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, Andr\\'e van\n  Schaik, and Jonathan Tapson", "title": "Fast, simple and accurate handwritten digit classification by training\n  shallow neural network classifiers with the 'extreme learning machine'\n  algorithm", "comments": "Accepted for publication; 9 pages of text, 6 figures and 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0134254", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in training deep (multi-layer) architectures have inspired a\nrenaissance in neural network use. For example, deep convolutional networks are\nbecoming the default option for difficult tasks on large datasets, such as\nimage and speech recognition. However, here we show that error rates below 1%\non the MNIST handwritten digit benchmark can be replicated with shallow\nnon-convolutional neural networks. This is achieved by training such networks\nusing the 'Extreme Learning Machine' (ELM) approach, which also enables a very\nrapid training time (~10 minutes). Adding distortions, as is common practise\nfor MNIST, reduces error rates even further. Our methods are also shown to be\ncapable of achieving less than 5.5% error rates on the NORB image database. To\nachieve these results, we introduce several enhancements to the standard ELM\nalgorithm, which individually and in combination can significantly improve\nperformance. The main innovation is to ensure each hidden-unit operates only on\na randomly sized and positioned patch of each image. This form of random\n`receptive field' sampling of the input ensures the input weight matrix is\nsparse, with about 90% of weights equal to zero. Furthermore, combining our\nmethods with a small number of iterations of a single-batch backpropagation\nmethod can significantly reduce the number of hidden-units required to achieve\na particular performance. Our close to state-of-the-art results for MNIST and\nNORB suggest that the ease of use and accuracy of the ELM algorithm for\ndesigning a single-hidden-layer neural network classifier should cause it to be\ngiven greater consideration either as a standalone method for simpler problems,\nor as the final classification stage in deep neural networks applied to more\ndifficult problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 11:14:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 08:28:03 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Tissera", "Migel D.", ""], ["Vladusich", "Tony", ""], ["van Schaik", "Andr\u00e9", ""], ["Tapson", "Jonathan", ""]]}, {"id": "1412.8341", "submitter": "Pavel H\\'ala", "authors": "Pavel H\\'ala", "title": "Spectral classification using convolutional neural networks", "comments": "71 pages, 50 figures, Master's thesis, Masaryk University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There is a great need for accurate and autonomous spectral classification\nmethods in astrophysics. This thesis is about training a convolutional neural\nnetwork (ConvNet) to recognize an object class (quasar, star or galaxy) from\none-dimension spectra only. Author developed several scripts and C programs for\ndatasets preparation, preprocessing and postprocessing of the data. EBLearn\nlibrary (developed by Pierre Sermanet and Yann LeCun) was used to create\nConvNets. Application on dataset of more than 60000 spectra yielded success\nrate of nearly 95%. This thesis conclusively proved great potential of\nconvolutional neural networks and deep learning methods in astrophysics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 13:47:06 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["H\u00e1la", "Pavel", ""]]}, {"id": "1412.8419", "submitter": "Pedro O. Pinheiro", "authors": "Remi Lebret and Pedro O. Pinheiro and Ronan Collobert", "title": "Simple Image Description Generator via a Linear Phrase-Based Approach", "comments": "Accepted as a workshop paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 18:43:10 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 05:09:13 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 03:53:26 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Lebret", "Remi", ""], ["Pinheiro", "Pedro O.", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.8534", "submitter": "Mehdi Sajjadi", "authors": "Mehdi Sajjadi, Mojtaba Seyedhosseini, Tolga Tasdizen", "title": "Disjunctive Normal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are powerful pattern classifiers; however, they\nhave been surpassed in accuracy by methods such as support vector machines and\nrandom forests that are also easier to use and faster to train.\nBackpropagation, which is used to train artificial neural networks, suffers\nfrom the herd effect problem which leads to long training times and limit\nclassification accuracy. We use the disjunctive normal form and approximate the\nboolean conjunction operations with products to construct a novel network\narchitecture. The proposed model can be trained by minimizing an error function\nand it allows an effective and intuitive initialization which solves the\nherd-effect problem associated with backpropagation. This leads to state-of-the\nart classification accuracy and fast training times. In addition, our model can\nbe jointly optimized with convolutional features in an unified structure\nleading to state-of-the-art results on computer vision problems with fast\nconvergence rates. A GPU implementation of LDNN with optional convolutional\nfeatures is also available\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 02:17:30 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Sajjadi", "Mehdi", ""], ["Seyedhosseini", "Mojtaba", ""], ["Tasdizen", "Tolga", ""]]}]