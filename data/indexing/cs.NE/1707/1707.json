[{"id": "1707.00081", "submitter": "Alexander Wong", "authors": "A. H. Karimi, M. J. Shafiee, A. Ghodsi, and A. Wong", "title": "Synthesizing Deep Neural Network Architectures using Biological Synaptic\n  Strength Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform an exploratory study on synthesizing deep neural\nnetworks using biological synaptic strength distributions, and the potential\ninfluence of different distributions on modelling performance particularly for\nthe scenario associated with small data sets. Surprisingly, a CNN with\nconvolutional layer synaptic strengths drawn from biologically-inspired\ndistributions such as log-normal or correlated center-surround distributions\nperformed relatively well suggesting a possibility for designing deep neural\nnetwork architectures that do not require many data samples to learn, and can\nsidestep current training procedures while maintaining or boosting modelling\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 01:30:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Karimi", "A. H.", ""], ["Shafiee", "M. J.", ""], ["Ghodsi", "A.", ""], ["Wong", "A.", ""]]}, {"id": "1707.00095", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Francis Li, and Alexander Wong", "title": "Exploring the Imposition of Synaptic Precision Restrictions For\n  Evolutionary Synthesis of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key contributing factor to incredible success of deep neural networks has\nbeen the significant rise on massively parallel computing devices allowing\nresearchers to greatly increase the size and depth of deep neural networks,\nleading to significant improvements in modeling accuracy. Although deeper,\nlarger, or complex deep neural networks have shown considerable promise, the\ncomputational complexity of such networks is a major barrier to utilization in\nresource-starved scenarios. We explore the synaptogenesis of deep neural\nnetworks in the formation of efficient deep neural network architectures within\nan evolutionary deep intelligence framework, where a probabilistic generative\nmodeling strategy is introduced to stochastically synthesize increasingly\nefficient yet effective offspring deep neural networks over generations,\nmimicking evolutionary processes such as heredity, random mutation, and natural\nselection in a probabilistic manner. In this study, we primarily explore the\nimposition of synaptic precision restrictions and its impact on the\nevolutionary synthesis of deep neural networks to synthesize more efficient\nnetwork architectures tailored for resource-starved scenarios. Experimental\nresults show significant improvements in synaptic efficiency (~10X decrease for\nGoogLeNet-based DetectNet) and inference speed (>5X increase for\nGoogLeNet-based DetectNet) while preserving modeling accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 04:56:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Wong", "Alexander", ""]]}, {"id": "1707.00160", "submitter": "Sebastian Ewert", "authors": "Sebastian Ewert, Mark B. Sandler", "title": "An Augmented Lagrangian Method for Piano Transcription using Equal\n  Loudness Thresholding and LSTM-based Decoding", "comments": null, "journal-ref": "Proceedings of the IEEE Workshop on Applications of Signal\n  Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, pp. 146-150,\n  2017", "doi": null, "report-no": null, "categories": "cs.SD cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal in automatic music transcription is to detect individual note\nevents in music recordings. An important variant is instrument-dependent music\ntranscription where methods can use calibration data for the instruments in\nuse. However, despite the additional information, results rarely exceed an\nf-measure of 80%. As a potential explanation, the transcription problem can be\nshown to be badly conditioned and thus relies on appropriate regularization. A\nrecently proposed method employs a mixture of simple, convex regularizers (to\nstabilize the parameter estimation process) and more complex terms (to\nencourage more meaningful structure). In this paper, we present two extensions\nto this method. First, we integrate a computational loudness model to better\ndifferentiate real from spurious note detections. Second, we employ\n(Bidirectional) Long Short Term Memory networks to re-weight the likelihood of\ndetected note constellations. Despite their simplicity, our two extensions lead\nto a drop of about 35% in note error rate compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 14:06:51 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 18:42:33 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 11:13:30 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Ewert", "Sebastian", ""], ["Sandler", "Mark B.", ""]]}, {"id": "1707.00300", "submitter": "Dianhui Wang", "authors": "Dianhui Wang, Caihao Cui", "title": "Stochastic Configuration Networks Ensemble for Large-Scale Data\n  Analytics", "comments": "20 pages, 7 figures, 9 tables; this paper has been submitted to\n  Information Sciences for publication in December 2016, and accepted on July\n  3, 2017", "journal-ref": null, "doi": "10.1016/j.ins.2017.07.003", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a fast decorrelated neuro-ensemble with heterogeneous\nfeatures for large-scale data analytics, where stochastic configuration\nnetworks (SCNs) are employed as base learner models and the well-known negative\ncorrelation learning (NCL) strategy is adopted to evaluate the output weights.\nBy feeding a large number of samples into the SCN base models, we obtain a huge\nsized linear equation system which is difficult to be solved by means of\ncomputing a pseudo-inverse used in the least squares method. Based on the group\nof heterogeneous features, the block Jacobi and Gauss-Seidel methods are\nemployed to iteratively evaluate the output weights, and a convergence analysis\nis given with a demonstration on the uniqueness of these iterative solutions.\nExperiments with comparisons on two large-scale datasets are carried out, and\nthe system robustness with respect to the regularizing factor used in NCL is\ngiven. Results indicate that the proposed ensemble learning techniques have\ngood potential for resolving large-scale data modelling problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 14:43:23 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 02:09:46 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Wang", "Dianhui", ""], ["Cui", "Caihao", ""]]}, {"id": "1707.00451", "submitter": "Luca Manzoni", "authors": "Mauro Castelli, Gianpiero Cattaneo, Luca Manzoni, Leonardo Vanneschi", "title": "A Distance Between Populations for n-Points Crossover in Genetic\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms (GAs) are an optimization technique that has been\nsuccessfully used on many real-world problems. There exist different approaches\nto their theoretical study. In this paper we complete a recently presented\napproach to model one-point crossover using pretopologies (or Cech topologies)\nin two ways. First, we extend it to the case of n-points crossover. Then, we\nexperimentally study how the distance distribution changes when the number of\ncrossover points increases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 09:04:52 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Castelli", "Mauro", ""], ["Cattaneo", "Gianpiero", ""], ["Manzoni", "Luca", ""], ["Vanneschi", "Leonardo", ""]]}, {"id": "1707.00561", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Paramartha Dutta, Atal Chaudhuri", "title": "Identifying hazardousness of sewer pipeline gas mixture using\n  classification methods: a comparative study", "comments": null, "journal-ref": "Neural Comput & Applic (2017) 28: 1343", "doi": "10.1007/s00521-016-2443-0", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we formulated a real-world problem related to sewer pipeline\ngas detection using the classification-based approaches. The primary goal of\nthis work was to identify the hazardousness of sewer pipeline to offer safe and\nnon-hazardous access to sewer pipeline workers so that the human fatalities,\nwhich occurs due to the toxic exposure of sewer gas components, can be avoided.\nThe dataset acquired through laboratory tests, experiments, and various\nliterature sources was organized to design a predictive model that was able to\nidentify/classify hazardous and non-hazardous situation of sewer pipeline. To\ndesign such prediction model, several classification algorithms were used and\ntheir performances were evaluated and compared, both empirically and\nstatistically, over the collected dataset. In addition, the performances of\nseveral ensemble methods were analyzed to understand the extent of improvement\noffered by these methods. The result of this comprehensive study showed that\nthe instance-based learning algorithm performed better than many other\nalgorithms such as multilayer perceptron, radial basis function network,\nsupport vector machine, reduced pruning tree. Similarly, it was observed that\nmulti-scheme ensemble approach enhanced the performance of base predictors.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:57:46 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Dutta", "Paramartha", ""], ["Chaudhuri", "Atal", ""]]}, {"id": "1707.00666", "submitter": "Daniel Hsu", "authors": "Daniel Hsu", "title": "Multi-period Time Series Modeling with Sparsity via Bayesian Variational\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we use augmented the hierarchical latent variable model to\nmodel multi-period time series, where the dynamics of time series are governed\nby factors or trends in multiple periods. Previous methods based on stacked\nrecurrent neural network (RNN) and deep belief network (DBN) models cannot\nmodel the tendencies in multiple periods, and no models for sequential data pay\nspecial attention to redundant input variables which have no or even negative\nimpact on prediction and modeling. Applying hierarchical latent variable model\nwith multiple transition periods, our proposed algorithm can capture\ndependencies in different temporal resolutions. Introducing Bayesian neural\nnetwork with Horseshoe prior as input network, we can discard the redundant\ninput variables in the optimization process, concurrently with the learning of\nother parts of the model. Based on experiments with both synthetic and\nreal-world data, we show that the proposed method significantly improves the\nmodeling and prediction performance on multi-period time series.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 17:35:09 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 15:31:23 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 21:15:56 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Hsu", "Daniel", ""]]}, {"id": "1707.00684", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Naoki Kuwata, Mizuha Homma, Takayuki Takahashi,\n  Yuki Nagahama, Marie Sano, Satoki Hasegawa, Ryuji Hirayama, Takashi Kakue,\n  Atsushi Shiraki, Naoki Takada, Tomoyoshi Ito", "title": "Deep-learning-based data page classification for holographic memory", "comments": null, "journal-ref": null, "doi": "10.1364/ao.56.007327", "report-no": null, "categories": "cs.CV cs.LG cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep-learning-based classification of data pages used in\nholographic memory. We numerically investigated the classification performance\nof a conventional multi-layer perceptron (MLP) and a deep neural network, under\nthe condition that reconstructed page data are contaminated by some noise and\nare randomly laterally shifted. The MLP was found to have a classification\naccuracy of 91.58%, whereas the deep neural network was able to classify data\npages at an accuracy of 99.98%. The accuracy of the deep neural network is two\norders of magnitude better than the MLP.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 05:47:37 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Kuwata", "Naoki", ""], ["Homma", "Mizuha", ""], ["Takahashi", "Takayuki", ""], ["Nagahama", "Yuki", ""], ["Sano", "Marie", ""], ["Hasegawa", "Satoki", ""], ["Hirayama", "Ryuji", ""], ["Kakue", "Takashi", ""], ["Shiraki", "Atsushi", ""], ["Takada", "Naoki", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1707.00703", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "Automated Problem Identification: Regression vs Classification via\n  Evolutionary Deep Networks", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regression or classification? This is perhaps the most basic question faced\nwhen tackling a new supervised learning problem. We present an Evolutionary\nDeep Learning (EDL) algorithm that automatically solves this by identifying the\nquestion type with high accuracy, along with a proposed deep architecture.\nTypically, a significant amount of human insight and preparation is required\nprior to executing machine learning algorithms. For example, when creating deep\nneural networks, the number of parameters must be selected in advance and\nfurthermore, a lot of these choices are made based upon pre-existing knowledge\nof the data such as the use of a categorical cross entropy loss function.\nHumans are able to study a dataset and decide whether it represents a\nclassification or a regression problem, and consequently make decisions which\nwill be applied to the execution of the neural network. We propose the\nAutomated Problem Identification (API) algorithm, which uses an evolutionary\nalgorithm interface to TensorFlow to manipulate a deep neural network to decide\nif a dataset represents a classification or a regression problem. We test API\non 16 different classification, regression and sentiment analysis datasets with\nup to 10,000 features and up to 17,000 unique target values. API achieves an\naverage accuracy of $96.3\\%$ in identifying the problem type without hardcoding\nany insights about the general characteristics of regression or classification\nproblems. For example, API successfully identifies classification problems even\nwith 1000 target values. Furthermore, the algorithm recommends which loss\nfunction to use and also recommends a neural network architecture. Our work is\ntherefore a step towards fully automated machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:00:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1707.00718", "submitter": "Iztok Fister", "authors": "Iztok Fister, Andres Iglesias, Suash Deb, Du\\v{s}an Fister, Iztok\n  Fister Jr", "title": "Modeling preference time in middle distance triathlons", "comments": "ISCBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling preference time in triathlons means predicting the intermediate\ntimes of particular sports disciplines by a given overall finish time in a\nspecific triathlon course for the athlete with the known personal best result.\nThis is a hard task for athletes and sport trainers due to a lot of different\nfactors that need to be taken into account, e.g., athlete's abilities, health,\nmental preparations and even their current sports form. So far, this process\nwas calculated manually without any specific software tools or using the\nartificial intelligence. This paper presents the new solution for modeling\npreference time in middle distance triathlons based on particle swarm\noptimization algorithm and archive of existing sports results. Initial results\nare presented, which suggest the usefulness of proposed approach, while remarks\nfor future improvements and use are also emphasized.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:13:44 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Fister", "Iztok", ""], ["Iglesias", "Andres", ""], ["Deb", "Suash", ""], ["Fister", "Du\u0161an", ""], ["Fister", "Iztok", "Jr"]]}, {"id": "1707.00750", "submitter": "Dhanesh Ramachandram", "authors": "Dhanesh Ramachandram, Michal Lisicki, Timothy J. Shields, Mohamed R.\n  Amer, Graham W. Taylor", "title": "Structure Optimization for Deep Multimodal Fusion Networks using\n  Graph-Induced Kernels", "comments": "Proceedings of the 25th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning, April 2017,\n  Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular testbed for deep learning has been multimodal recognition of human\nactivity or gesture involving diverse inputs such as video, audio, skeletal\npose and depth images. Deep learning architectures have excelled on such\nproblems due to their ability to combine modality representations at different\nlevels of nonlinear feature extraction. However, designing an optimal\narchitecture in which to fuse such learned representations has largely been a\nnon-trivial human engineering effort. We treat fusion structure optimization as\na hyper-parameter search and cast it as a discrete optimization problem under\nthe Bayesian optimization framework. We propose a novel graph-induced kernel to\ncompute structural similarities in the search space of tree-structured\nmultimodal architectures and demonstrate its effectiveness using two\nchallenging multimodal human activity recognition datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 20:32:29 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Ramachandram", "Dhanesh", ""], ["Lisicki", "Michal", ""], ["Shields", "Timothy J.", ""], ["Amer", "Mohamed R.", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1707.00884", "submitter": "Violaine Cuicheret-Retel", "authors": "S. Carbillet, V. Guicheret-Retel, F. Trivaudey, F. Richard, M.L.\n  Boubakar", "title": "Identification of non-linear behavior models with restricted or\n  redundant data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a new strategy for the identification of material\nparameters in the case of restricted or redundant data, based on a hybrid\napproach combining a genetic algorithm and the Levenberg-Marquardt method. The\nproposed methodology consists essentially in a statistically based topological\nanalysis of the search domain, after this one has been reduced by the analysis\nof the parameters ranges. This is used to identify the parameters of a model\nrepresenting the behavior of damaged elastic, visco-elastic, plastic and\nvisco-plastic composite laminates. Optimization of the experimental tests on\ntubular samples leads to the selective identification of these parameters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 09:56:24 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Carbillet", "S.", ""], ["Guicheret-Retel", "V.", ""], ["Trivaudey", "F.", ""], ["Richard", "F.", ""], ["Boubakar", "M. L.", ""]]}, {"id": "1707.01046", "submitter": "Luiz Otavio Vilas Boas Oliveira", "authors": "Luis F. Miranda, Luiz Otavio V. B. Oliveira, Joao Francisco B. S.\n  Martins, Gisele L. Pappa", "title": "How Noisy Data Affects Geometric Semantic Genetic Programming", "comments": "8 pages, In proceedings of Genetic and Evolutionary Computation\n  Conference (GECCO 2017), Berlin, Germany", "journal-ref": null, "doi": "10.1145/3071178.3071300", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is a consequence of acquiring and pre-processing data from the\nenvironment, and shows fluctuations from different sources---e.g., from\nsensors, signal processing technology or even human error. As a machine\nlearning technique, Genetic Programming (GP) is not immune to this problem,\nwhich the field has frequently addressed. Recently, Geometric Semantic Genetic\nProgramming (GSGP), a semantic-aware branch of GP, has shown robustness and\nhigh generalization capability. Researchers believe these characteristics may\nbe associated with a lower sensibility to noisy data. However, there is no\nsystematic study on this matter. This paper performs a deep analysis of the\nGSGP performance over the presence of noise. Using 15 synthetic datasets where\nnoise can be controlled, we added different ratios of noise to the data and\ncompared the results obtained with those of a canonical GP. The results show\nthat, as we increase the percentage of noisy instances, the generalization\nperformance degradation is more pronounced in GSGP than GP. However, in\ngeneral, GSGP is more robust to noise than GP in the presence of up to 10% of\nnoise, and presents no statistical difference for values higher than that in\nthe test bed.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 15:56:35 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Miranda", "Luis F.", ""], ["Oliveira", "Luiz Otavio V. B.", ""], ["Martins", "Joao Francisco B. S.", ""], ["Pappa", "Gisele L.", ""]]}, {"id": "1707.01209", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Model compression as constrained optimization, with application to\n  neural nets. Part I: general framework", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing neural nets is an active research problem, given the large size\nof state-of-the-art nets for tasks such as object recognition, and the\ncomputational limits imposed by mobile devices. We give a general formulation\nof model compression as constrained optimization. This includes many types of\ncompression: quantization, low-rank decomposition, pruning, lossless\ncompression and others. Then, we give a general algorithm to optimize this\nnonconvex problem based on the augmented Lagrangian and alternating\noptimization. This results in a \"learning-compression\" algorithm, which\nalternates a learning step of the uncompressed model, independent of the\ncompression type, with a compression step of the model parameters, independent\nof the learning task. This simple, efficient algorithm is guaranteed to find\nthe best compressed model for the task in a local sense under standard\nassumptions.\n  We present separately in several companion papers the development of this\ngeneral framework into specific algorithms for model compression based on\nquantization, pruning and other variations, including experimental results on\ncompressing neural nets and other models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 04:36:26 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1707.01213", "submitter": "Naiyan Wang", "authors": "Zehao Huang, Naiyan Wang", "title": "Data-Driven Sparse Structure Selection for Deep Neural Networks", "comments": "ECCV Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have liberated its extraordinary power on\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\nmodels into real-world applications due to their high computational complexity.\nHow can we design a compact and effective network without massive experiments\nand expert knowledge? In this paper, we propose a simple and effective\nframework to learn and prune deep models in an end-to-end manner. In our\nframework, a new type of parameter -- scaling factor is first introduced to\nscale the outputs of specific structures, such as neurons, groups or residual\nblocks. Then we add sparsity regularizations on these factors, and solve this\noptimization problem by a modified stochastic Accelerated Proximal Gradient\n(APG) method. By forcing some of the factors to zero, we can safely remove the\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\nwith other structure selection methods that may need thousands of trials or\niterative fine-tuning, our method is trained fully end-to-end in one training\npass without bells and whistles. We evaluate our method, Sparse Structure\nSelection with several state-of-the-art CNNs, and demonstrate very promising\nresults with adaptive depth and width selection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:21:50 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:38:02 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 05:14:37 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1707.01219", "submitter": "Naiyan Wang", "authors": "Zehao Huang, Naiyan Wang", "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep neural networks have demonstrated extraordinary power in various\napplications, their superior performances are at expense of high storage and\ncomputational costs. Consequently, the acceleration and compression of neural\nnetworks have attracted much attention recently. Knowledge Transfer (KT), which\naims at training a smaller student network by transferring knowledge from a\nlarger teacher model, is one of the popular solutions. In this paper, we\npropose a novel knowledge transfer method by treating it as a distribution\nmatching problem. Particularly, we match the distributions of neuron\nselectivity patterns between teacher and student networks. To achieve this\ngoal, we devise a new KT loss function by minimizing the Maximum Mean\nDiscrepancy (MMD) metric between these distributions. Combined with the\noriginal loss function, our method can significantly improve the performance of\nstudent networks. We validate the effectiveness of our method across several\ndatasets, and further combine it with other KT methods to explore the best\npossible results. Last but not least, we fine-tune the model to other tasks\nsuch as object detection. The results are also encouraging, which confirm the\ntransferability of the learned features.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:44:02 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:35:22 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1707.01220", "submitter": "Naiyan Wang", "authors": "Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample\n  Similarities Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid evolution of deep neural network architecture design\nin the past years. These latest progresses greatly facilitate the developments\nin various areas such as computer vision and natural language processing.\nHowever, along with the extraordinary performance, these state-of-the-art\nmodels also bring in expensive computational cost. Directly deploying these\nmodels into applications with real-time requirement is still infeasible.\nRecently, Hinton etal. have shown that the dark knowledge within a powerful\nteacher model can significantly help the training of a smaller and faster\nstudent network. These knowledge are vastly beneficial to improve the\ngeneralization ability of the student model. Inspired by their work, we\nintroduce a new type of knowledge -- cross sample similarities for model\ncompression and acceleration. This knowledge can be naturally derived from deep\nmetric learning model. To transfer them, we bring the \"learning to rank\"\ntechnique into deep metric learning formulation. We test our proposed DarkRank\nmethod on various metric learning tasks including pedestrian re-identification,\nimage retrieval and image clustering. The results are quite encouraging. Our\nmethod can improve over the baseline method by a large margin. Moreover, it is\nfully compatible with other existing methods. When combined, the performance\ncan be further boosted.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:47:11 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:44:44 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Chen", "Yuntao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1707.01357", "submitter": "Stefan Lattner", "authors": "Stefan Lattner and Maarten Grachten", "title": "Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\n  Rotation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-invariance in mapping codes learned by GAEs is a useful feature for\nvarious relation learning tasks. In this paper we show that the\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\nbe substantially improved by extending the standard GAE loss (symmetric\nreconstruction error) with a regularization term that penalizes the symmetric\ncross-reconstruction error. This error term involves reconstruction of pairs\nwith mapping codes obtained from other pairs exhibiting similar\ntransformations. Although this would principally require knowledge of the\ntransformations exhibited by training pairs, our experiments show that a\nbootstrapping approach can sidestep this issue, and that the regularization\nterm can effectively be used in an unsupervised setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:28:43 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""]]}, {"id": "1707.01395", "submitter": "Dmitriy Anisimov", "authors": "Dmitriy Anisimov, Tatiana Khanova", "title": "Towards lightweight convolutional neural networks for object detection", "comments": "Submitted to the International Workshop on Traffic and Street\n  Surveillance for Safety and Security (IWT4S) in conjunction with the 14th\n  IEEE International Conference on Advanced Video and Signal based Surveillance\n  (AVSS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose model with larger spatial size of feature maps and evaluate it on\nobject detection task. With the goal to choose the best feature extraction\nnetwork for our model we compare several popular lightweight networks. After\nthat we conduct a set of experiments with channels reduction algorithms in\norder to accelerate execution. Our vehicle detection models are accurate, fast\nand therefore suit for embedded visual applications. With only 1.5 GFLOPs our\nbest model gives 93.39 AP on validation subset of challenging DETRAC dataset.\nThe smallest of our models is the first to achieve real-time inference speed on\nCPU with reasonable accuracy drop to 91.43 AP.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:53:00 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 12:49:46 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 12:08:49 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Anisimov", "Dmitriy", ""], ["Khanova", "Tatiana", ""]]}, {"id": "1707.01429", "submitter": "Edward Frady", "authors": "E. Paxon Frady, Denis Kleyko, Friedrich T. Sommer", "title": "Theory of the superposition principle for randomized connectionist\n  representations in neural networks", "comments": "42 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand cognitive reasoning in the brain, it has been proposed that\nsymbols and compositions of symbols are represented by activity patterns\n(vectors) in a large population of neurons. Formal models implementing this\nidea [Plate 2003], [Kanerva 2009], [Gayler 2003], [Eliasmith 2012] include a\nreversible superposition operation for representing with a single vector an\nentire set of symbols or an ordered sequence of symbols. If the representation\nspace is high-dimensional, large sets of symbols can be superposed and\nindividually retrieved. However, crosstalk noise limits the accuracy of\nretrieval and information capacity. To understand information processing in the\nbrain and to design artificial neural systems for cognitive reasoning, a theory\nof this superposition operation is essential. Here, such a theory is presented.\nThe superposition operations in different existing models are mapped to linear\nneural networks with unitary recurrent matrices, in which retrieval accuracy\ncan be analyzed by a single equation. We show that networks representing\ninformation in superposition can achieve a channel capacity of about half a bit\nper neuron, a significant fraction of the total available entropy. Going beyond\nexisting models, superposition operations with recency effects are proposed\nthat avoid catastrophic forgetting when representing the history of infinite\ndata streams. These novel models correspond to recurrent networks with\nnon-unitary matrices or with nonlinear neurons, and can be analyzed and\noptimized with an extension of our theory.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 15:18:09 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Frady", "E. Paxon", ""], ["Kleyko", "Denis", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "1707.01446", "submitter": "Xerxes D. Arsiwalla", "authors": "Xerxes D. Arsiwalla, Pedro A.M. Mediano, Paul F.M.J. Verschure", "title": "Spectral Modes of Network Dynamics Reveal Increased Informational\n  Complexity Near Criticality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does the informational complexity of dynamical networked systems tell us\nabout intrinsic mechanisms and functions of these complex systems? Recent\ncomplexity measures such as integrated information have sought to\noperationalize this problem taking a whole-versus-parts perspective, wherein\none explicitly computes the amount of information generated by a network as a\nwhole over and above that generated by the sum of its parts during state\ntransitions. While several numerical schemes for estimating network integrated\ninformation exist, it is instructive to pursue an analytic approach that\ncomputes integrated information as a function of network weights. Our\nformulation of integrated information uses a Kullback-Leibler divergence\nbetween the multi-variate distribution on the set of network states versus the\ncorresponding factorized distribution over its parts. Implementing stochastic\nGaussian dynamics, we perform computations for several prototypical network\ntopologies. Our findings show increased informational complexity near\ncriticality, which remains consistent across network topologies. Spectral\ndecomposition of the system's dynamics reveals how informational complexity is\ngoverned by eigenmodes of both, the network's covariance and adjacency\nmatrices. We find that as the dynamics of the system approach criticality, high\nintegrated information is exclusively driven by the eigenmode corresponding to\nthe leading eigenvalue of the covariance matrix, while sub-leading modes get\nsuppressed. The implication of this result is that it might be favorable for\ncomplex dynamical networked systems such as the human brain or communication\nsystems to operate near criticality so that efficient information integration\nmight be achieved.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 15:48:48 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Arsiwalla", "Xerxes D.", ""], ["Mediano", "Pedro A. M.", ""], ["Verschure", "Paul F. M. J.", ""]]}, {"id": "1707.01495", "submitter": "Marcin Andrychowicz", "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel\n  Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba", "title": "Hindsight Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with sparse rewards is one of the biggest challenges in Reinforcement\nLearning (RL). We present a novel technique called Hindsight Experience Replay\nwhich allows sample-efficient learning from rewards which are sparse and binary\nand therefore avoid the need for complicated reward engineering. It can be\ncombined with an arbitrary off-policy RL algorithm and may be seen as a form of\nimplicit curriculum.\n  We demonstrate our approach on the task of manipulating objects with a\nrobotic arm. In particular, we run experiments on three different tasks:\npushing, sliding, and pick-and-place, in each case using only binary rewards\nindicating whether or not the task is completed. Our ablation studies show that\nHindsight Experience Replay is a crucial ingredient which makes training\npossible in these challenging environments. We show that our policies trained\non a physics simulation can be deployed on a physical robot and successfully\ncomplete the task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:55:53 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 18:35:33 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 10:04:20 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Andrychowicz", "Marcin", ""], ["Wolski", "Filip", ""], ["Ray", "Alex", ""], ["Schneider", "Jonas", ""], ["Fong", "Rachel", ""], ["Welinder", "Peter", ""], ["McGrew", "Bob", ""], ["Tobin", "Josh", ""], ["Abbeel", "Pieter", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1707.01555", "submitter": "Hongyu Guo", "authors": "Hongyu Guo", "title": "A Deep Network with Visual Text Composition Behavior", "comments": "accepted to ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While natural languages are compositional, how state-of-the-art neural models\nachieve compositionality is still unclear. We propose a deep network, which not\nonly achieves competitive accuracy for text classification, but also exhibits\ncompositional behavior. That is, while creating hierarchical representations of\na piece of text, such as a sentence, the lower layers of the network distribute\ntheir layer-specific attention weights to individual words. In contrast, the\nhigher layers compose meaningful phrases and clauses, whose lengths increase as\nthe networks get deeper until fully composing the sentence.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:37:23 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Guo", "Hongyu", ""]]}, {"id": "1707.01623", "submitter": "Ji-Sung Kim", "authors": "Ji-Sung Kim, Xin Gao, Andrey Rzhetsky", "title": "RIDDLE: Race and ethnicity Imputation from Disease history with Deep\n  LEarning", "comments": null, "journal-ref": "PLOS Computational Biology 14(4): e1006106 (2018)", "doi": "10.1371/journal.pcbi.1006106", "report-no": null, "categories": "q-bio.QM cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anonymized electronic medical records are an increasingly popular source of\nresearch data. However, these datasets often lack race and ethnicity\ninformation. This creates problems for researchers modeling human disease, as\nrace and ethnicity are powerful confounders for many health exposures and\ntreatment outcomes; race and ethnicity are closely linked to\npopulation-specific genetic variation. We showed that deep neural networks\ngenerate more accurate estimates for missing racial and ethnic information than\ncompeting methods (e.g., logistic regression, random forest). RIDDLE yielded\nsignificantly better classification performance across all metrics that were\nconsidered: accuracy, cross-entropy loss (error), and area under the curve for\nreceiver operating characteristic plots (all $p < 10^{-6}$). We made specific\nefforts to interpret the trained neural network models to identify, quantify,\nand visualize medical features which are predictive of race and ethnicity. We\nused these characterizations of informative features to perform a systematic\ncomparison of differential disease patterns by race and ethnicity. The fact\nthat clinical histories are informative for imputing race and ethnicity could\nreflect (1) a skewed distribution of blue- and white-collar professions across\nracial and ethnic groups, (2) uneven accessibility and subjective importance of\nprophylactic health, (3) possible variation in lifestyle, such as dietary\nhabits, and (4) differences in background genetic variation which predispose to\ndiseases.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 03:03:57 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 21:21:47 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kim", "Ji-Sung", ""], ["Gao", "Xin", ""], ["Rzhetsky", "Andrey", ""]]}, {"id": "1707.01642", "submitter": "Ruggero Micheletto", "authors": "Ruggero Micheletto and Ahyi Kim", "title": "An HTM based cortical algorithm for detection of seismic waves", "comments": "7 pages, 4 figures and one table. 7 Citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing seismic waves immediately is very important for the realization\nof efficient disaster prevention. Generally these systems consist of a network\nof seismic detectors that send real time data to a central server. The server\nelaborates the data and attempts to recognize the first signs of an earthquake.\nThe current problem with this approach is that it is subject to false alarms. A\ncritical trade-off exists between sensitivity of the system and error rate. To\novercame this problems, an artificial neural network based intelligent learning\nsystems can be used. However, conventional supervised ANN systems are difficult\nto train, CPU intensive and prone to false alarms. To surpass these problems,\nhere we attempt to use a next-generation unsupervised cortical algorithm HTM.\nThis novel approach does not learn particular waveforms, but adapts to\ncontinuously fed data reaching the ability to discriminate between normality\n(seismic sensor background noise in no-earthquake conditions) and anomaly\n(sensor response to a jitter or an earthquake). Main goal of this study is test\nthe ability of the HTM algorithm to be used to signal earthquakes automatically\nin a feasible disaster prevention system. We describe the methodology used and\ngive the first qualitative assessments of the recognition ability of the\nsystem. Our preliminary results show that the cortical algorithm used is very\nrobust to noise and that can successfully recognize synthetic earthquake-like\nsignals efficiently and reliably.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 05:24:12 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Micheletto", "Ruggero", ""], ["Kim", "Ahyi", ""]]}, {"id": "1707.01810", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Ajith Abraham, Vaclav Snasel", "title": "Simultaneous Optimization of Neural Network Weights and Active Nodes\n  using Metaheuristics", "comments": null, "journal-ref": null, "doi": "10.1109/HIS.2014.7086207", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of neural network (NN) significantly influenced by the transfer\nfunction used in its active nodes. It has been observed that the homogeneity in\nthe activation nodes does not provide the best solution. Therefore, the\ncustomizable transfer functions whose underlying parameters are subjected to\noptimization were used to provide heterogeneity to NN. For the experimental\npurpose, a meta-heuristic framework using a combined genotype representation of\nconnection weights and transfer function parameter was used. The performance of\nadaptive Logistic, Tangent-hyperbolic, Gaussian and Beta functions were\nanalyzed. In present research work, concise comparisons between different\ntransfer function and between the NN optimization algorithms are presented. The\ncomprehensive analysis of the results obtained over the benchmark dataset\nsuggests that the Artificial Bee Colony with adaptive transfer function\nprovides the best results in terms of classification accuracy over the particle\nswarm optimization and differential evolution.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:20:50 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Abraham", "Ajith", ""], ["Snasel", "Vaclav", ""]]}, {"id": "1707.01812", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Ajith Abraham, Vaclav Snasel", "title": "ACO for Continuous Function Optimization: A Performance Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/ISDA.2014.7066253", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the meta-heuristic algorithms often depends on their\nparameter settings. Appropriate tuning of the underlying parameters can\ndrastically improve the performance of a meta-heuristic. The Ant Colony\nOptimization (ACO), a population based meta-heuristic algorithm inspired by the\nforaging behavior of the ants, is no different. Fundamentally, the ACO depends\non the construction of new solutions, variable by variable basis using Gaussian\nsampling of the selected variables from an archive of solutions. A\ncomprehensive performance analysis of the underlying parameters such as:\nselection strategy, distance measure metric and pheromone evaporation rate of\nthe ACO suggests that the Roulette Wheel Selection strategy enhances the\nperformance of the ACO due to its ability to provide non-uniformity and\nadequate diversity in the selection of a solution. On the other hand, the\nSquared Euclidean distance-measure metric offers better performance than other\ndistance-measure metrics. It is observed from the analysis that the ACO is\nsensitive towards the evaporation rate. Experimental analysis between classical\nACO and other meta-heuristic suggested that the performance of the well-tuned\nACO surpasses its counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:27:33 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Abraham", "Ajith", ""], ["Snasel", "Vaclav", ""]]}, {"id": "1707.01821", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha and Paramartha Dutta and Atal Chaudhuri and Hiranmay\n  Saha", "title": "Convergence Analysis of Backpropagation Algorithm for Designing an\n  Intelligent System for Sensing Manhole Gases", "comments": null, "journal-ref": "Hybrid Soft Computing Approaches (2015) pp 215-236", "doi": "10.1007/978-81-322-2544-7_7", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human fatalities are reported due to the excessive proportional presence of\nhazardous gas components in the manhole, such as Hydrogen Sulfide, Ammonia,\nMethane, Carbon Dioxide, Nitrogen Oxide, Carbon Monoxide, etc. Hence,\npredetermination of these gases is imperative. A neural network (NN) based\nintelligent sensory system is proposed for the avoidance of such fatalities.\nBackpropagation (BP) was applied for the supervised training of the neural\nnetwork. A Gas sensor array consists of many sensor elements was employed for\nthe sensing manhole gases. Sensors in the sensor array are responsible for\nsensing their target gas components only. Therefore, the presence of multiple\ngases results in cross sensitivity. The cross sensitivity is a crucial issue to\nthis problem and it is viewed as pattern recognition and noise reduction\nproblem. Various performance parameters and complexity of the problem\ninfluences NN training. In present chapter the performance of BP algorithm on\nsuch a real life application problem was comprehensively studied, compared and\ncontrasted with the several other hybrid intelligent approaches both, in\ntheoretical and in the statistical sense.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:49:02 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Dutta", "Paramartha", ""], ["Chaudhuri", "Atal", ""], ["Saha", "Hiranmay", ""]]}, {"id": "1707.02469", "submitter": "Pau Vilimelis Aceituno", "authors": "Pau Vilimelis Aceituno, Yan Gang, Yang-Yu Liu", "title": "Tailoring Artificial Neural Networks for Optimal Learning", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important paradigms of recurrent neural networks, the echo\nstate network (ESN) has been applied to a wide range of fields, from robotics\nto medicine, finance, and language processing. A key feature of the ESN\nparadigm is its reservoir --- a directed and weighted network of neurons that\nprojects the input time series into a high dimensional space where linear\nregression or classification can be applied. Despite extensive studies, the\nimpact of the reservoir network on the ESN performance remains unclear.\nCombining tools from physics, dynamical systems and network science, we attempt\nto open the black box of ESN and offer insights to understand the behavior of\ngeneral artificial neural networks. Through spectral analysis of the reservoir\nnetwork we reveal a key factor that largely determines the ESN memory capacity\nand hence affects its performance. Moreover, we find that adding short loops to\nthe reservoir network can tailor ESN for specific tasks and optimize learning.\nWe validate our findings by applying ESN to forecast both synthetic and real\nbenchmark time series. Our results provide a new way to design task-specific\nESN. More importantly, it demonstrates the power of combining tools from\nphysics, dynamical systems and network science to offer new insights in\nunderstanding the mechanisms of general artificial neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 17:17:29 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 10:55:15 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 20:01:57 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 17:45:30 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Aceituno", "Pau Vilimelis", ""], ["Gang", "Yan", ""], ["Liu", "Yang-Yu", ""]]}, {"id": "1707.02533", "submitter": "Pramudita Satria Palar Dr.", "authors": "Pramudita Satria Palar and Koji Shimoyama", "title": "Exploiting Active Subspaces in Global Optimization: How Complex is your\n  Problem?", "comments": "The Genetic and Evolutionary Computation Conference (GECCO) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying optimization method to a real-world problem, the possession of\nprior knowledge and preliminary analysis on the landscape of a global\noptimization problem can give us an insight into the complexity of the problem.\nThis knowledge can better inform us in deciding what optimization method should\nbe used to tackle the problem. However, this analysis becomes problematic when\nthe dimensionality of the problem is high. This paper presents a framework to\ntake a deeper look at the global optimization problem to be tackled: by\nanalyzing the low-dimensional representation of the problem through discovering\nthe active subspaces of the given problem. The virtue of this is that the\nproblem's complexity can be visualized in a one or two-dimensional plot, thus\nallow one to get a better grip about the problem's difficulty. One could then\nhave a better idea regarding the complexity of their problem to determine the\nchoice of global optimizer or what surrogate-model type to be used.\nFurthermore, we also demonstrate how the active subspaces can be used to\nperform design exploration and analysis.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 07:02:15 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Palar", "Pramudita Satria", ""], ["Shimoyama", "Koji", ""]]}, {"id": "1707.02617", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "Deepest Neural Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that a long chain of perceptrons (that is, a multilayer\nperceptron, or MLP, with many hidden layers of width one) can be a universal\nclassifier. The classification procedure is not necessarily computationally\nefficient, but the technique throws some light on the kind of computations\npossible with narrow and deep MLPs.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 18:34:45 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "1707.02746", "submitter": "Nikolai Mishachev", "authors": "N. M. Mishachev", "title": "Backpropagation in matrix notation", "comments": "7 pages, Remark 6 added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we calculate the gradient of the network function in matrix\nnotation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 08:44:46 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 20:08:12 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Mishachev", "N. M.", ""]]}, {"id": "1707.03049", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa, Bruno Pedroni, Sadique Sheik, Gert Cauwenberghs", "title": "Hardware-efficient on-line learning through pipelined truncated-error\n  backpropagation in binary-state networks", "comments": "Now also consider 0/1 binary activations. Memory access statistics\n  reported", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs) trained using backpropagation are powerful\nlearning architectures that have achieved state-of-the-art performance in\nvarious benchmarks. Significant effort has been devoted to developing custom\nsilicon devices to accelerate inference in ANNs. Accelerating the training\nphase, however, has attracted relatively little attention. In this paper, we\ndescribe a hardware-efficient on-line learning technique for feedforward\nmulti-layer ANNs that is based on pipelined backpropagation. Learning is\nperformed in parallel with inference in the forward pass, removing the need for\nan explicit backward pass and requiring no extra weight lookup. By using binary\nstate variables in the feedforward network and ternary errors in\ntruncated-error backpropagation, the need for any multiplications in the\nforward and backward passes is removed, and memory requirements for the\npipelining are drastically reduced. Further reduction in addition operations\nowing to the sparsity in the forward neural and backpropagating error signal\npaths contributes to highly efficient hardware implementation. For\nproof-of-concept validation, we demonstrate on-line learning of MNIST\nhandwritten digit classification on a Spartan 6 FPGA interfacing with an\nexternal 1Gb DDR2 DRAM, that shows small degradation in test error performance\ncompared to an equivalently sized binary ANN trained off-line using standard\nback-propagation and exact errors. Our results highlight an attractive synergy\nbetween pipelined backpropagation and binary-state networks in substantially\nreducing computation and memory requirements, making pipelined on-line learning\npractical in deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 22:12:38 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 16:35:58 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Mostafa", "Hesham", ""], ["Pedroni", "Bruno", ""], ["Sheik", "Sadique", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1707.03093", "submitter": "Roberto Santana", "authors": "Roberto Santana", "title": "Gray-box optimization and factorized distribution algorithms: where two\n  worlds collide", "comments": "33 pages, 9 tables, 3 figures. This paper covers some of the topics\n  of the talk \"When the gray box was opened, model-based evolutionary\n  algorithms were already there\" presented in the Model-Based Evolutionary\n  Algorithms workshop on July 20, 2016, in Denver", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of gray-box optimization, in juxtaposition to black-box\noptimization, revolves about the idea of exploiting the problem structure to\nimplement more efficient evolutionary algorithms (EAs). Work on factorized\ndistribution algorithms (FDAs), whose factorizations are directly derived from\nthe problem structure, has also contributed to show how exploiting the problem\nstructure produces important gains in the efficiency of EAs. In this paper we\nanalyze the general question of using problem structure in EAs focusing on\nconfronting work done in gray-box optimization with related research\naccomplished in FDAs. This contrasted analysis helps us to identify, in current\nstudies on the use problem structure in EAs, two distinct analytical\ncharacterizations of how these algorithms work. Moreover, we claim that these\ntwo characterizations collide and compete at the time of providing a coherent\nframework to investigate this type of algorithms. To illustrate this claim, we\npresent a contrasted analysis of formalisms, questions, and results produced in\nFDAs and gray-box optimization. Common underlying principles in the two\napproaches, which are usually overlooked, are identified and discussed.\nBesides, an extensive review of previous research related to different uses of\nthe problem structure in EAs is presented. The paper also elaborates on some of\nthe questions that arise when extending the use of problem structure in EAs,\nsuch as the question of evolvability, high cardinality of the variables and\nlarge definition sets, constrained and multi-objective problems, etc. Finally,\nemergent approaches that exploit neural models to capture the problem structure\nare covered.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 01:14:14 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Santana", "Roberto", ""]]}, {"id": "1707.03141", "submitter": "Nikhil Mishra", "authors": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel", "title": "A Simple Neural Attentive Meta-Learner", "comments": "iclr 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in\nthe task. In response, recent work in meta-learning proposes training a\nmeta-learner on a distribution of similar tasks, in the hopes of generalization\nto novel but related tasks by learning a high-level strategy that captures the\nessence of the problem it is asked to solve. However, many recent meta-learning\napproaches are extensively hand-designed, either using architectures\nspecialized to a particular application, or hard-coding algorithmic components\nthat constrain how the meta-learner solves the task. We propose a class of\nsimple and generic meta-learner architectures that use a novel combination of\ntemporal convolutions and soft attention; the former to aggregate information\nfrom past experience and the latter to pinpoint specific pieces of information.\nIn the most extensive set of meta-learning experiments to date, we evaluate the\nresulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by significant margins.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 06:21:31 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 16:08:03 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 04:55:20 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mishra", "Nikhil", ""], ["Rohaninejad", "Mostafa", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1707.03374", "submitter": "Abhishek Gupta", "authors": "YuXuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine", "title": "Imitation from Observation: Learning to Imitate Behaviors from Raw Video\n  via Context Translation", "comments": "Accepted at ICRA 2018, Brisbane. YuXuan Liu and Abhishek Gupta had\n  equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is an effective approach for autonomous systems to acquire\ncontrol policies when an explicit reward function is unavailable, using\nsupervision provided as demonstrations from an expert, typically a human\noperator. However, standard imitation learning methods assume that the agent\nreceives examples of observation-action tuples that could be provided, for\ninstance, to a supervised learning algorithm. This stands in contrast to how\nhumans and animals imitate: we observe another person performing some behavior\nand then figure out which actions will realize that behavior, compensating for\nchanges in viewpoint, surroundings, object positions and types, and other\nfactors. We term this kind of imitation learning \"imitation-from-observation,\"\nand propose an imitation learning method based on video prediction with context\ntranslation and deep reinforcement learning. This lifts the assumption in\nimitation learning that the demonstration should consist of observations in the\nsame environment configuration, and enables a variety of interesting\napplications, including learning robotic skills that involve tool use simply by\nobserving videos of human tool use. Our experimental results show the\neffectiveness of our approach in learning a wide range of real-world robotic\ntasks modeled after common household chores from videos of a human\ndemonstrator, including sweeping, ladling almonds, pushing objects as well as a\nnumber of tasks in simulation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:23:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:00:13 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Liu", "YuXuan", ""], ["Gupta", "Abhishek", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1707.03502", "submitter": "Jindong Wang", "authors": "Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng and Lisha Hu", "title": "Deep Learning for Sensor-based Activity Recognition: A Survey", "comments": "10 pages, 2 figures, and 5 tables; submitted to Pattern Recognition\n  Letters (second revision)", "journal-ref": null, "doi": "10.1016/j.patrec.2018.02.010", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor-based activity recognition seeks the profound high-level knowledge\nabout human activities from multitudes of low-level sensor readings.\nConventional pattern recognition approaches have made tremendous progress in\nthe past years. However, those methods often heavily rely on heuristic\nhand-crafted feature extraction, which could hinder their generalization\nperformance. Additionally, existing methods are undermined for unsupervised and\nincremental learning tasks. Recently, the recent advancement of deep learning\nmakes it possible to perform automatic high-level feature extraction thus\nachieves promising performance in many areas. Since then, deep learning based\nmethods have been widely adopted for the sensor-based activity recognition\ntasks. This paper surveys the recent advance of deep learning based\nsensor-based activity recognition. We summarize existing literature from three\naspects: sensor modality, deep model, and application. We also present detailed\ninsights on existing work and propose grand challenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 00:21:04 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 03:11:15 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Hao", "Shuji", ""], ["Peng", "Xiaohui", ""], ["Hu", "Lisha", ""]]}, {"id": "1707.03604", "submitter": "Mrutyunjaya Panda", "authors": "Mrutyunjaya Panda", "title": "Elephant Search with Deep Learning for Microarray Data Analysis", "comments": "12 pages, 5 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though there is a plethora of research in Microarray gene expression\ndata analysis, still, it poses challenges for researchers to effectively and\nefficiently analyze the large yet complex expression of genes. The feature\n(gene) selection method is of paramount importance for understanding the\ndifferences in biological and non-biological variation between samples. In\norder to address this problem, a novel elephant search (ES) based optimization\nis proposed to select best gene expressions from the large volume of microarray\ndata. Further, a promising machine learning method is envisioned to leverage\nsuch high dimensional and complex microarray dataset for extracting hidden\npatterns inside to make a meaningful prediction and most accurate\nclassification. In particular, stochastic gradient descent based Deep learning\n(DL) with softmax activation function is then used on the reduced features\n(genes) for better classification of different samples according to their gene\nexpression levels. The experiments are carried out on nine most popular Cancer\nmicroarray gene selection datasets, obtained from UCI machine learning\nrepository. The empirical results obtained by the proposed elephant search\nbased deep learning (ESDL) approach are compared with most recent published\narticle for its suitability in future Bioinformatics research.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 09:04:04 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Panda", "Mrutyunjaya", ""]]}, {"id": "1707.03623", "submitter": "Yuri Parzhin", "authors": "Yuri Parzhin", "title": "The detector principle of constructing artificial neural networks as an\n  alternative to the connectionist paradigm", "comments": "22 pages, in Russian, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANN) are inadequate to biological neural\nnetworks. This inadequacy is manifested in the use of the obsolete model of the\nneuron and the connectionist paradigm of constructing ANN. The result of this\ninadequacy is the existence of many shortcomings of the ANN and the problems of\ntheir practical implementation. The alternative principle of ANN construction\nis proposed in the article. This principle was called the detector principle.\nThe basis of the detector principle is the consideration of the binding\nproperty of the input signals of a neuron. A new model of the neuron-detector,\na new approach to teaching ANN - counter training and a new approach to the\nformation of the ANN architecture are used in this principle.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 10:01:48 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Parzhin", "Yuri", ""]]}, {"id": "1707.03855", "submitter": "Dmitri Gavrilov", "authors": "Dmitri Gavrilov, Dmitri Strukov and Konstantin K. Likharev", "title": "Capacity, Fidelity, and Noise Tolerance of Associative Spatial-Temporal\n  Memories Based on Memristive Neuromorphic Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have calculated the key characteristics of associative\n(content-addressable) spatial-temporal memories based on neuromorphic networks\nwith restricted connectivity - \"CrossNets\". Such networks may be naturally\nimplemented in nanoelectronic hardware using hybrid CMOS/memristor circuits,\nwhich may feature extremely high energy efficiency, approaching that of\nbiological cortical circuits, at much higher operation speed. Our numerical\nsimulations, in some cases confirmed by analytical calculations, have shown\nthat the characteristics depend substantially on the method of information\nrecording into the memory. Of the four methods we have explored, two look\nespecially promising - one based on the quadratic programming, and the other\none being a specific discrete version of the gradient descent. The latter\nmethod provides a slightly lower memory capacity (at the same fidelity) then\nthe former one, but it allows local recording, which may be more readily\nimplemented in nanoelectronic hardware. Most importantly, at the synchronous\nretrieval, both methods provide a capacity higher than that of the well-known\nTernary Content-Addressable Memories with the same number of nonvolatile memory\ncells (e.g., memristors), though the input noise immunity of the CrossNet\nmemories is somewhat lower.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 18:26:37 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Gavrilov", "Dmitri", ""], ["Strukov", "Dmitri", ""], ["Likharev", "Konstantin K.", ""]]}, {"id": "1707.03902", "submitter": "Samuel James Alvernaz", "authors": "Samuel Alvernaz, Julian Togelius", "title": "Autoencoder-augmented Neuroevolution for Visual Doom Playing", "comments": "IEEE conference on Computational Intelligence and Games 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroevolution has proven effective at many reinforcement learning tasks, but\ndoes not seem to scale well to high-dimensional controller representations,\nwhich are needed for tasks where the input is raw pixel data. We propose a\nnovel method where we train an autoencoder to create a comparatively\nlow-dimensional representation of the environment observation, and then use\nCMA-ES to train neural network controllers acting on this input data. As the\nbehavior of the agent changes the nature of the input data, the autoencoder\ntraining progresses throughout evolution. We test this method in the VizDoom\nenvironment built on the classic FPS Doom, where it performs well on a\nhealth-pack gathering task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:46:21 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Alvernaz", "Samuel", ""], ["Togelius", "Julian", ""]]}, {"id": "1707.04035", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, Aurelio\n  Uncini", "title": "Kafnets: kernel-based non-parametric activation functions for neural\n  networks", "comments": "Preprint submitted to Neural Networks (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are generally built by interleaving (adaptable) linear layers\nwith (fixed) nonlinear activation functions. To increase their flexibility,\nseveral authors have proposed methods for adapting the activation functions\nthemselves, endowing them with varying degrees of flexibility. None of these\napproaches, however, have gained wide acceptance in practice, and research in\nthis topic remains open. In this paper, we introduce a novel family of flexible\nactivation functions that are based on an inexpensive kernel expansion at every\nneuron. Leveraging over several properties of kernel-based models, we propose\nmultiple variations for designing and initializing these kernel activation\nfunctions (KAFs), including a multidimensional scheme allowing to nonlinearly\ncombine information from different paths in the network. The resulting KAFs can\napproximate any mapping defined over a subset of the real line, either convex\nor nonconvex. Furthermore, they are smooth over their entire domain, linear in\ntheir parameters, and they can be regularized using any known scheme, including\nthe use of $\\ell_1$ penalties to enforce sparseness. To the best of our\nknowledge, no other known model satisfies all these properties simultaneously.\nIn addition, we provide a relatively complete overview on alternative\ntechniques for adapting the activation functions, which is currently lacking in\nthe literature. A large set of experiments validates our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:22:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 11:33:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Totaro", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1707.04319", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Yerlan Idelbayev", "title": "Model compression as constrained optimization, with application to\n  neural nets. Part II: quantization", "comments": "33 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deep neural net compression by quantization: given\na large, reference net, we want to quantize its real-valued weights using a\ncodebook with $K$ entries so that the training loss of the quantized net is\nminimal. The codebook can be optimally learned jointly with the net, or fixed,\nas for binarization or ternarization approaches. Previous work has quantized\nthe weights of the reference net, or incorporated rounding operations in the\nbackpropagation algorithm, but this has no guarantee of converging to a\nloss-optimal, quantized net. We describe a new approach based on the recently\nproposed framework of model compression as constrained optimization\n\\citep{Carreir17a}. This results in a simple iterative \"learning-compression\"\nalgorithm, which alternates a step that learns a net of continuous weights with\na step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed\nto converge to local optimum of the loss for quantized nets. We develop\nalgorithms for an adaptive codebook or a (partially) fixed codebook. The latter\nincludes binarization, ternarization, powers-of-two and other important\nparticular cases. We show experimentally that we can achieve much higher\ncompression rates than previous quantization work (even using just 1 bit per\nweight) with negligible loss degradation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:58:40 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Idelbayev", "Yerlan", ""]]}, {"id": "1707.04550", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Helcl and Jind\\v{r}ich Libovick\\'y", "title": "CUNI System for the WMT17 Multimodal Translation Task", "comments": "8 pages; Camera-ready submission to WMT17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our submissions to the WMT17 Multimodal\nTranslation Task. For Task 1 (multimodal translation), our best scoring system\nis a purely textual neural translation of the source image caption to the\ntarget language. The main feature of the system is the use of additional data\nthat was acquired by selecting similar sentences from parallel corpora and by\ndata synthesis with back-translation. For Task 2 (cross-lingual image\ncaptioning), our best submitted system generates an English caption which is\nthen translated by the best system used in Task 1. We also present negative\nresults, which are based on ideas that we believe have potential of making\nimprovements, but did not prove to be useful in our particular setup.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 15:58:47 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Helcl", "Jind\u0159ich", ""], ["Libovick\u00fd", "Jind\u0159ich", ""]]}, {"id": "1707.04619", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Simplified Long Short-term Memory Recurrent Neural Networks: part I", "comments": "4 pages, 6 figures, 5 tables. Part I of a three part publications\n  that will appear in IKE'17 - The 16th Int'l Conference on Information &\n  Knowledge Engineering The 2017 World Congress in Computer Science, Computer\n  Engineering & Applied Computing | CSCE'17, July 17-20, 2017, Las Vegas,\n  Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present five variants of the standard Long Short-term Memory (LSTM)\nrecurrent neural networks by uniformly reducing blocks of adaptive parameters\nin the gating mechanisms. For simplicity, we refer to these models as LSTM1,\nLSTM2, LSTM3, LSTM4, and LSTM5, respectively. Such parameter-reduced variants\nenable speeding up data training computations and would be more suitable for\nimplementations onto constrained embedded platforms. We comparatively evaluate\nand verify our five variant models on the classical MNIST dataset and\ndemonstrate that these variant models are comparable to a standard\nimplementation of the LSTM model while using less number of parameters.\nMoreover, we observe that in some cases the standard LSTM's accuracy\nperformance will drop after a number of epochs when using the ReLU\nnonlinearity; in contrast, however, LSTM3, LSTM4 and LSTM5 will retain their\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:46:59 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1707.04623", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Simplified Long Short-term Memory Recurrent Neural Networks: part II", "comments": "4 pages, 6 figures, 5 tables; this is part II of three-part work, all\n  to appear in IKE'17- The 16th Int'l Conference on Information & Knowledge\n  Engineering, in The 2017 World Congress in Computer Science Computer\n  Engineering & Applied Computing | CSCE'17 July 17-20, 2017, Las Vegas,\n  Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is part II of three-part work. Here, we present a second set of\ninter-related five variants of simplified Long Short-term Memory (LSTM)\nrecurrent neural networks by further reducing adaptive parameters. Two of these\nmodels have been introduced in part I of this work. We evaluate and verify our\nmodel variants on the benchmark MNIST dataset and assert that these models are\ncomparable to the base LSTM model while use progressively less number of\nparameters. Moreover, we observe that in case of using the ReLU activation, the\ntest accuracy performance of the standard LSTM will drop after a number of\nepochs when learning parameter become larger. However all of the new model\nvariants sustain their performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:59:18 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1707.04626", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Simplified Long Short-term Memory Recurrent Neural Networks: part III", "comments": "Here 5 pages (in the conference 4 pages), 10 figures, 5 tables; this\n  is part III of a three part work, all will appear in the IKE'17 - The 16th\n  Int'l Conference on Information & Knowledge Engineering. The 2017 World\n  Congress in Computer Science Computer Engineering & Applied Computing |\n  CSCE'17, July 17-20, 2017, Las Vegas, Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is part III of three-part work. In parts I and II, we have presented\neight variants for simplified Long Short Term Memory (LSTM) recurrent neural\nnetworks (RNNs). It is noted that fast computation, specially in constrained\ncomputing resources, are an important factor in processing big time-sequence\ndata. In this part III paper, we present and evaluate two new LSTM model\nvariants which dramatically reduce the computational load while retaining\ncomparable performance to the base (standard) LSTM RNNs. In these new variants,\nwe impose (Hadamard) pointwise state multiplications in the cell-memory network\nin addition to the gating signal networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 20:12:37 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1707.04678", "submitter": "Alexandros Tsaptsinos", "authors": "Alexandros Tsaptsinos", "title": "Lyrics-Based Music Genre Classification Using a Hierarchical Attention\n  Network", "comments": "8 pages, 4 figures, ISMIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music genre classification, especially using lyrics alone, remains a\nchallenging topic in Music Information Retrieval. In this study we apply\nrecurrent neural network models to classify a large dataset of intact song\nlyrics. As lyrics exhibit a hierarchical layer structure - in which words\ncombine to form lines, lines form segments, and segments form a complete song -\nwe adapt a hierarchical attention network (HAN) to exploit these layers and in\naddition learn the importance of the words, lines, and segments. We test the\nmodel over a 117-genre dataset and a reduced 20-genre dataset. Experimental\nresults show that the HAN outperforms both non-neural models and simpler neural\nmodels, whilst also classifying over a higher number of genres than previous\nresearch. Through the learning process we can also visualise which words or\nlines in a song the model believes are important to classifying the genre. As a\nresult the HAN provides insights, from a computational perspective, into\nlyrical structure and language features that differentiate musical genres.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 02:22:41 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tsaptsinos", "Alexandros", ""]]}, {"id": "1707.04780", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H.\n  Nguyen, Madeleine Gibescu, Antonio Liotta", "title": "Scalable Training of Artificial Neural Networks with Adaptive Sparse\n  Connectivity inspired by Network Science", "comments": "18 pages", "journal-ref": "Nature Communications, 2018", "doi": "10.1038/s41467-018-04316-3", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the success of deep learning in various domains, artificial neural\nnetworks are currently among the most used artificial intelligence methods.\nTaking inspiration from the network properties of biological neural networks\n(e.g. sparsity, scale-freeness), we argue that (contrary to general practice)\nartificial neural networks, too, should not have fully-connected layers. Here\nwe propose sparse evolutionary training of artificial neural networks, an\nalgorithm which evolves an initial sparse topology (Erd\\H{o}s-R\\'enyi random\ngraph) of two consecutive layers of neurons into a scale-free topology, during\nlearning. Our method replaces artificial neural networks fully-connected layers\nwith sparse ones before training, reducing quadratically the number of\nparameters, with no decrease in accuracy. We demonstrate our claims on\nrestricted Boltzmann machines, multi-layer perceptrons, and convolutional\nneural networks for unsupervised and supervised learning on 15 datasets. Our\napproach has the potential to enable artificial neural networks to scale up\nbeyond what is currently possible.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 19:46:25 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 12:55:55 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Mocanu", "Elena", ""], ["Stone", "Peter", ""], ["Nguyen", "Phuong H.", ""], ["Gibescu", "Madeleine", ""], ["Liotta", "Antonio", ""]]}, {"id": "1707.04853", "submitter": "Xu He", "authors": "Xu He and Herbert Jaeger", "title": "Overcoming Catastrophic Interference by Conceptors", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "Jacobs University Technical Report Nr 35", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic interference has been a major roadblock in the research of\ncontinual learning. Here we propose a variant of the back-propagation\nalgorithm, \"conceptor-aided back-prop\" (CAB), in which gradients are shielded\nby conceptors against degradation of previously learned tasks. Conceptors have\ntheir origin in reservoir computing, where they have been previously shown to\novercome catastrophic forgetting. CAB extends these results to deep feedforward\nnetworks. On the disjoint MNIST task CAB outperforms two other methods for\ncoping with catastrophic interference that have recently been proposed in the\ndeep learning field.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 10:12:13 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 21:37:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["He", "Xu", ""], ["Jaeger", "Herbert", ""]]}, {"id": "1707.05005", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan,\n  Lihui Chen, Yang Liu and Shantanu Jaiswal", "title": "graph2vec: Learning Distributed Representations of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CR cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on representation learning for graph structured data\npredominantly focus on learning distributed representations of graph\nsubstructures such as nodes and subgraphs. However, many graph analytics tasks\nsuch as graph classification and clustering require representing entire graphs\nas fixed length feature vectors. While the aforementioned approaches are\nnaturally unequipped to learn such representations, graph kernels remain as the\nmost effective way of obtaining them. However, these graph kernels use\nhandcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\nhampered by problems such as poor generalization. To address this limitation,\nin this work, we propose a neural embedding framework named graph2vec to learn\ndata-driven distributed representations of arbitrary sized graphs. graph2vec's\nembeddings are learnt in an unsupervised manner and are task agnostic. Hence,\nthey could be used for any downstream task such as graph classification,\nclustering and even seeding supervised representation learning approaches. Our\nexperiments on several benchmark and large real-world datasets show that\ngraph2vec achieves significant improvements in classification and clustering\naccuracies over substructure representation learning approaches and are\ncompetitive with state-of-the-art graph kernels.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 05:09:03 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Chandramohan", "Mahinthan", ""], ["Venkatesan", "Rajasekar", ""], ["Chen", "Lihui", ""], ["Liu", "Yang", ""], ["Jaiswal", "Shantanu", ""]]}, {"id": "1707.05173", "submitter": "Owain Evans", "authors": "William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans", "title": "Trial without Error: Towards Safe Reinforcement Learning via Human\n  Intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems are increasingly applied to complex tasks that involve interaction\nwith humans. During training, such systems are potentially dangerous, as they\nhaven't yet learned to avoid actions that could cause serious harm. How can an\nAI system explore and learn without making a single mistake that harms humans\nor otherwise causes serious damage? For model-free reinforcement learning,\nhaving a human \"in the loop\" and ready to intervene is currently the only way\nto prevent all catastrophes. We formalize human intervention for RL and show\nhow to reduce the human labor required by training a supervised learner to\nimitate the human's intervention decisions. We evaluate this scheme on Atari\ngames, with a Deep RL agent being overseen by a human for four hours. When the\nclass of catastrophes is simple, we are able to prevent all catastrophes\nwithout affecting the agent's learning (whereas an RL baseline fails due to\ncatastrophic forgetting). However, this scheme is less successful when\ncatastrophes are more complex: it reduces but does not eliminate catastrophes\nand the supervised learner fails on adversarial examples found by the agent.\nExtrapolating to more challenging environments, we show that our implementation\nwould not scale (due to the infeasible amount of human labor required). We\noutline extensions of the scheme that are necessary if we are to train\nmodel-free agents without a single catastrophe.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 14:13:40 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Saunders", "William", ""], ["Sastry", "Girish", ""], ["Stuhlmueller", "Andreas", ""], ["Evans", "Owain", ""]]}, {"id": "1707.05227", "submitter": "Marek Rei", "authors": "Marek Rei, Helen Yannakoudakis", "title": "Auxiliary Objectives for Neural Error Detection Models", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the utility of different auxiliary objectives and training\nstrategies within a neural sequence labeling approach to error detection in\nlearner writing. Auxiliary costs provide the model with additional linguistic\ninformation, allowing it to learn general-purpose compositional features that\ncan then be exploited for other objectives. Our experiments show that a joint\nlearning approach trained with parallel labels on in-domain data improves\nperformance over the previous best error detection system. While the resulting\nmodel has the same number of parameters, the additional objectives allow it to\nbe optimised more efficiently and achieve better performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:24:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Yannakoudakis", "Helen", ""]]}, {"id": "1707.05233", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Detecting Off-topic Responses to Visual Prompts", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for essay scoring have made great progress in recent years,\nachieving accuracies very close to human annotators. However, a known weakness\nof such automated scorers is not taking into account the semantic relevance of\nthe submitted text. While there is existing work on detecting answer relevance\ngiven a textual prompt, very little previous research has been done to\nincorporate visual writing prompts. We propose a neural architecture and\nseveral extensions for detecting off-topic responses to visual prompts and\nevaluate it on a dataset of texts written by language learners.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:31:20 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1707.05300", "submitter": "Carlos Florensa", "authors": "Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter\n  Abbeel", "title": "Reverse Curriculum Generation for Reinforcement Learning", "comments": "Published at the 1st Conference on Robot Learning (CoRL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many relevant tasks require an agent to reach a certain state, or to\nmanipulate objects into a desired configuration. For example, we might want a\nrobot to align and assemble a gear onto an axle or insert and turn a key in a\nlock. These goal-oriented tasks present a considerable challenge for\nreinforcement learning, since their natural reward function is sparse and\nprohibitive amounts of exploration are required to reach the goal and receive\nsome learning signal. Past approaches tackle these problems by exploiting\nexpert demonstrations or by manually designing a task-specific reward shaping\nfunction to guide the learning agent. Instead, we propose a method to learn\nthese tasks without requiring any prior knowledge other than obtaining a single\nstate in which the task is achieved. The robot is trained in reverse, gradually\nlearning to reach the goal from a set of start states increasingly far from the\ngoal. Our method automatically generates a curriculum of start states that\nadapts to the agent's performance, leading to efficient training on\ngoal-oriented tasks. We demonstrate our approach on difficult simulated\nnavigation and fine-grained manipulation problems, not solvable by\nstate-of-the-art reinforcement learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 17:53:54 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 02:46:26 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 10:10:17 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Florensa", "Carlos", ""], ["Held", "David", ""], ["Wulfmeier", "Markus", ""], ["Zhang", "Michael", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1707.05660", "submitter": "Gerard Rinkus", "authors": "Gerard J. Rinkus", "title": "Quantum Computation via Sparse Distributed Representation", "comments": "5 pages, 2 figs", "journal-ref": "NeuroQuantology 2012 10(2), 311-315", "doi": "10.14704/nq.2012.10.2.507", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum superposition says that any physical system simultaneously exists in\nall of its possible states, the number of which is exponential in the number of\nentities composing the system. The strength of presence of each possible state\nin the superposition, i.e., its probability of being observed, is represented\nby its probability amplitude coefficient. The assumption that these\ncoefficients must be represented physically disjointly from each other, i.e.,\nlocalistically, is nearly universal in the quantum theory/computing literature.\nAlternatively, these coefficients can be represented using sparse distributed\nrepresentations (SDR), wherein each coefficient is represented by small subset\nof an overall population of units, and the subsets can overlap. Specifically, I\nconsider an SDR model in which the overall population consists of Q WTA\nclusters, each with K binary units. Each coefficient is represented by a set of\nQ units, one per cluster. Thus, K^Q coefficients can be represented with KQ\nunits. Thus, the particular world state, X, whose coefficient's representation,\nR(X), is the set of Q units active at time t has the max probability and the\nprobability of every other state, Y_i, at time t, is measured by R(Y_i)'s\nintersection with R(X). Thus, R(X) simultaneously represents both the\nparticular state, X, and the probability distribution over all states. Thus,\nset intersection may be used to classically implement quantum superposition. If\nalgorithms exist for which the time it takes to store (learn) new\nrepresentations and to find the closest-matching stored representation\n(probabilistic inference) remains constant as additional representations are\nstored, this meets the criterion of quantum computing. Such an algorithm has\nalready been described: it achieves this \"quantum speed-up\" without esoteric\nhardware, and in fact, on a single-processor, classical (Von Neumann) computer.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 17:01:50 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Rinkus", "Gerard J.", ""]]}, {"id": "1707.05970", "submitter": "Ishai Rosenberg", "authors": "Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici", "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call\n  Based Malware Classifiers", "comments": "Accepted as a conference paper at RAID 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a black-box attack against API call based machine\nlearning malware classifiers, focusing on generating adversarial sequences\ncombining API calls and static features (e.g., printable strings) that will be\nmisclassified by the classifier without affecting the malware functionality. We\nshow that this attack is effective against many classifiers due to the\ntransferability principle between RNN variants, feed forward DNNs, and\ntraditional machine learning classifiers such as SVM. We also implement GADGET,\na software framework to convert any malware binary to a binary undetected by\nmalware classifiers, using the proposed attack, without access to the malware\nsource code.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 08:16:31 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 04:57:55 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 11:05:26 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 20:39:31 GMT"}, {"version": "v5", "created": "Sun, 24 Jun 2018 21:03:21 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rosenberg", "Ishai", ""], ["Shabtai", "Asaf", ""], ["Rokach", "Lior", ""], ["Elovici", "Yuval", ""]]}, {"id": "1707.06132", "submitter": "Joao Batista Monteiro Filho", "authors": "Joao Batista Monteiro FIlho, Isabela Maria Carneiro de Albuquerque,\n  Fernando Buarque de Lima Neto", "title": "Solving Mixed Model Workplace Time-dependent Assembly Line Balancing\n  Problem with FSS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing assembly lines, a family of optimization problems commonly known as\nAssembly Line Balancing Problem, is notoriously NP-Hard. They comprise a set of\nproblems of enormous practical interest to manufacturing industry due to the\nrelevant frequency of this type of production paradigm. For this reason, many\nresearchers on Computational Intelligence and Industrial Engineering have been\nconceiving algorithms for tackling different versions of assembly line\nbalancing problems utilizing different methodologies. In this article, it was\nproposed a problem version referred as Mixed Model Workplace Time-dependent\nAssembly Line Balancing Problem with the intention of including pressing issues\nof real assembly lines in the optimization problem, to which four versions were\nconceived. Heuristic search procedures were used, namely two Swarm Intelligence\nalgorithms from the Fish School Search family: the original version, named\n\"vanilla\", and a special variation including a stagnation avoidance routine.\nEither approaches solved the newly posed problem achieving good results when\ncompared to Particle Swarm Optimization algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 14:53:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["FIlho", "Joao Batista Monteiro", ""], ["de Albuquerque", "Isabela Maria Carneiro", ""], ["Neto", "Fernando Buarque de Lima", ""]]}, {"id": "1707.06169", "submitter": "Joao Batista Monteiro Filho", "authors": "Joao Batista Monteiro Filho, Isabela Maria Carneiro de Albuquerque,\n  Fernando Buarque de Lima Neto", "title": "Fish School Search Algorithm for Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the effectiveness of the application of niching\nable swarm metaheuristic approaches in order to solve constrained optimization\nproblems. Sub-swarms are used in order to allow the achievement of many\nfeasible regions to be exploited in terms of fitness function. The niching\napproach employed was wFSS, a version of the Fish School Search algorithm\ndevised specifically to deal with multi-modal search spaces. A base technique\nreferred as wrFSS was conceived and three variations applying different\nconstraint handling procedures were also proposed. Tests were performed in\nseven problems from CEC 2010 and a comparison with other approaches was carried\nout. Results show that the search strategy proposed is able to handle some\nheavily constrained problems and achieve results comparable to the\nstate-of-the-art algorithms. However, we also observed that the local search\noperator present in wFSS and inherited by wrFSS makes the fitness convergence\ndifficult when the feasible region presents some specific geometrical features.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:52:31 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Filho", "Joao Batista Monteiro", ""], ["de Albuquerque", "Isabela Maria Carneiro", ""], ["Neto", "Fernando Buarque de Lima", ""]]}, {"id": "1707.06170", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing,\n  Sebastien Racani\\`ere, David Reichert, Th\\'eophane Weber, Daan Wierstra,\n  Peter Battaglia", "title": "Learning model-based planning from scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional wisdom holds that model-based planning is a powerful approach to\nsequential decision-making. It is often very challenging in practice, however,\nbecause while a model can be used to evaluate a plan, it does not prescribe how\nto construct a plan. Here we introduce the \"Imagination-based Planner\", the\nfirst model-based, sequential decision-making agent that can learn to\nconstruct, evaluate, and execute plans. Before any action, it can perform a\nvariable number of imagination steps, which involve proposing an imagined\naction and evaluating it with its model-based imagination. All imagined actions\nand outcomes are aggregated, iteratively, into a \"plan context\" which\nconditions future real and imagined actions. The agent can even decide how to\nimagine: testing out alternative imagined actions, chaining sequences of\nactions together, or building a more complex \"imagination tree\" by navigating\nflexibly among the previously imagined states using a learned policy. And our\nagent can learn to plan economically, jointly optimizing for external rewards\nand computational costs associated with using its imagination. We show that our\narchitecture can learn to solve a challenging continuous control problem, and\nalso learn elaborate planning strategies in a discrete maze-solving task. Our\nwork opens a new direction toward learning the components of a model-based\nplanning system and how to use them.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:52:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Pascanu", "Razvan", ""], ["Li", "Yujia", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Buesing", "Lars", ""], ["Racani\u00e8re", "Sebastien", ""], ["Reichert", "David", ""], ["Weber", "Th\u00e9ophane", ""], ["Wierstra", "Daan", ""], ["Battaglia", "Peter", ""]]}, {"id": "1707.06185", "submitter": "Joao Batista Monteiro Filho", "authors": "Joao Batista Monteiro Filho, Isabela Maria Carneiro de Albuquerque,\n  Fernando Buarque de Lima Neto", "title": "Simultaneously Solving Mixed Model Assembly Line Balancing and\n  Sequencing problems with FSS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many assembly lines related optimization problems have been tackled by\nresearchers in the last decades due to its relevance for the decision makers\nwithin manufacturing industry. Many of theses problems, more specifically\nAssembly Lines Balancing and Sequencing problems, are known to be NP-Hard.\nTherefore, Computational Intelligence solution approaches have been conceived\nin order to provide practical use decision making tools. In this work, we\nproposed a simultaneous solution approach in order to tackle both Balancing and\nSequencing problems utilizing an effective meta-heuristic algorithm referred as\nFish School Search. Three different test instances were solved with the\noriginal and two modified versions of this algorithm and the results were\ncompared with Particle Swarm Optimization Algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 16:23:45 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Filho", "Joao Batista Monteiro", ""], ["de Albuquerque", "Isabela Maria Carneiro", ""], ["Neto", "Fernando Buarque de Lima", ""]]}, {"id": "1707.06260", "submitter": "Timothy O'Shea", "authors": "Timothy J. O'Shea, Kiran Karra, T. Charles Clancy", "title": "Learning Approximate Neural Estimators for Wireless Channel State\n  Information", "comments": "Under conference submission as of June 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation is a critical component of synchronization in wireless and signal\nprocessing systems. There is a rich body of work on estimator derivation,\noptimization, and statistical characterization from analytic system models\nwhich are used pervasively today. We explore an alternative approach to\nbuilding estimators which relies principally on approximate regression using\nlarge datasets and large computationally efficient artificial neural network\nmodels capable of learning non-linear function mappings which provide compact\nand accurate estimates. For single carrier PSK modulation, we explore the\naccuracy and computational complexity of such estimators compared with the\ncurrent gold-standard analytically derived alternatives. We compare performance\nin various wireless operating conditions and consider the trade offs between\nthe two different classes of systems. Our results show the learned estimators\ncan provide improvements in areas such as short-time estimation and estimation\nunder non-trivial real world channel conditions such as fading or other\nnon-linear hardware or propagation effects.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 18:49:41 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["O'Shea", "Timothy J.", ""], ["Karra", "Kiran", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1707.06381", "submitter": "Suhwan Lim", "authors": "Suhwan Lim, Jong-Ho Bae, Jai-Ho Eum, Sungtae Lee, Chul-Heung Kim,\n  Dongseok Kwon, Byung-Gook Park, Jong-Ho Lee", "title": "Adaptive Learning Rule for Hardware-based Deep Neural Networks Using\n  Electronic Synapse Devices", "comments": null, "journal-ref": "Neural Comput. Appl. (2018)", "doi": "10.1007/s00521-018-3659-y", "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning rule based on a back-propagation (BP)\nalgorithm that can be applied to a hardware-based deep neural network (HW-DNN)\nusing electronic devices that exhibit discrete and limited conductance\ncharacteristics. This adaptive learning rule, which enables forward, backward\npropagation, as well as weight updates in hardware, is helpful during the\nimplementation of power-efficient and high-speed deep neural networks. In\nsimulations using a three-layer perceptron network, we evaluate the learning\nperformance according to various conductance responses of electronic synapse\ndevices and weight-updating methods. It is shown that the learning accuracy is\ncomparable to that obtained when using a software-based BP algorithm when the\nelectronic synapse device has a linear conductance response with a high dynamic\nrange. Furthermore, the proposed unidirectional weight-updating method is\nsuitable for electronic synapse devices which have nonlinear and finite\nconductance responses. Because this weight-updating method can compensate the\ndemerit of asymmetric weight updates, we can obtain better accuracy compared to\nother methods. This adaptive learning rule, which can be applied to full\nhardware implementation, can also compensate the degradation of learning\naccuracy due to the probable device-to-device variation in an actual electronic\nsynapse device.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 06:10:36 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 11:42:23 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lim", "Suhwan", ""], ["Bae", "Jong-Ho", ""], ["Eum", "Jai-Ho", ""], ["Lee", "Sungtae", ""], ["Kim", "Chul-Heung", ""], ["Kwon", "Dongseok", ""], ["Park", "Byung-Gook", ""], ["Lee", "Jong-Ho", ""]]}, {"id": "1707.06474", "submitter": "Jonas Adler", "authors": "Jonas Adler and Ozan \\\"Oktem", "title": "Learned Primal-dual Reconstruction", "comments": "11 pages, 5 figures", "journal-ref": "IEEE Transactions on Medical Imaging (2018)", "doi": "10.1109/TMI.2018.2799231", "report-no": null, "categories": "math.OC cs.CV cs.NE math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Learned Primal-Dual algorithm for tomographic reconstruction.\nThe algorithm accounts for a (possibly non-linear) forward operator in a deep\nneural network by unrolling a proximal primal-dual optimization method, but\nwhere the proximal operators have been replaced with convolutional neural\nnetworks. The algorithm is trained end-to-end, working directly from raw\nmeasured data and it does not depend on any initial reconstruction such as FBP.\n  We compare performance of the proposed method on low dose CT reconstruction\nagainst FBP, TV, and deep learning based post-processing of FBP. For the\nShepp-Logan phantom we obtain >6dB PSNR improvement against all compared\nmethods. For human phantoms the corresponding improvement is 6.6dB over TV and\n2.2dB over learned post-processing along with a substantial improvement in the\nSSIM. Finally, our algorithm involves only ten forward-back-projection\ncomputations, making the method feasible for time critical clinical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:34:51 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 12:39:52 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 15:34:04 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Adler", "Jonas", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1707.06480", "submitter": "Zhenisbek Assylbekov", "authors": "Zhenisbek Assylbekov, Rustem Takhanov, Bagdat Myrzakhmetov and\n  Jonathan N. Washington", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware\n  Ones", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Syllabification does not seem to improve word-level RNN language modeling\nquality when compared to character-based segmentation. However, our best\nsyllable-aware language model, achieving performance comparable to the\ncompetitive character-aware model, has 18%-33% fewer parameters and is trained\n1.2-2.2 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:46:09 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Assylbekov", "Zhenisbek", ""], ["Takhanov", "Rustem", ""], ["Myrzakhmetov", "Bagdat", ""], ["Washington", "Jonathan N.", ""]]}, {"id": "1707.06600", "submitter": "Joel Leibo", "authors": "Julien Perolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie,\n  Karl Tuyls, Thore Graepel", "title": "A multi-agent reinforcement learning model of common-pool resource\n  appropriation", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanity faces numerous problems of common-pool resource appropriation. This\nclass of multi-agent social dilemma includes the problems of ensuring\nsustainable use of fresh water, common fisheries, grazing pastures, and\nirrigation systems. Abstract models of common-pool resource appropriation based\non non-cooperative game theory predict that self-interested agents will\ngenerally fail to find socially positive equilibria---a phenomenon called the\ntragedy of the commons. However, in reality, human societies are sometimes able\nto discover and implement stable cooperative solutions. Decades of behavioral\ngame theory research have sought to uncover aspects of human behavior that make\nthis possible. Most of that work was based on laboratory experiments where\nparticipants only make a single choice: how much to appropriate. Recognizing\nthe importance of spatial and temporal resource dynamics, a recent trend has\nbeen toward experiments in more complex real-time video game-like environments.\nHowever, standard methods of non-cooperative game theory can no longer be used\nto generate predictions for this case. Here we show that deep reinforcement\nlearning can be used instead. To that end, we study the emergent behavior of\ngroups of independently learning agents in a partially observed Markov game\nmodeling common-pool resource appropriation. Our experiments highlight the\nimportance of trial-and-error learning in common-pool resource appropriation\nand shed light on the relationship between exclusion, sustainability, and\ninequality.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 16:35:02 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 17:32:44 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Perolat", "Julien", ""], ["Leibo", "Joel Z.", ""], ["Zambaldi", "Vinicius", ""], ["Beattie", "Charles", ""], ["Tuyls", "Karl", ""], ["Graepel", "Thore", ""]]}, {"id": "1707.06729", "submitter": "Michael Arnold", "authors": "Michael Arnold", "title": "Predictive networking and optimization for flow-based networks", "comments": "A thesis submitted for the Master of Science Degree at The University\n  of Alabama in Huntsville", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) were used to classify neural network flows\nby flow size. After training the neural network was able to predict the size of\na flows with 87% accuracy with a Feed Forward Neural Network. This demonstrates\nthat flow based routers can prioritize candidate flows with a predicted large\nnumber of packets for priority insertion into hardware content-addressable\nmemory.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 00:50:46 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Arnold", "Michael", ""]]}, {"id": "1707.06841", "submitter": "Youmna Farag", "authors": "Youmna Farag, Marek Rei, Ted Briscoe", "title": "An Error-Oriented Approach to Word Embedding Pre-Training", "comments": "10 pages, 2 figures, 4 tables, BEA 2017", "journal-ref": "The 12th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel word embedding pre-training approach that exploits writing\nerrors in learners' scripts. We compare our method to previous models that tune\nthe embeddings based on script scores and the discrimination between correct\nand corrupt word contexts in addition to the generic commonly-used embeddings\npre-trained on large corpora. The comparison is achieved by using the\naforementioned models to bootstrap a neural network that learns to predict a\nholistic score for scripts. Furthermore, we investigate augmenting our model\nwith error corrections and monitor the impact on performance. Our results show\nthat our error-oriented approach outperforms other comparable ones which is\nfurther demonstrated when training on more data. Additionally, extending the\nmodel with corrections provides further performance gains when data sparsity is\nan issue.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:06:12 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Farag", "Youmna", ""], ["Rei", "Marek", ""], ["Briscoe", "Ted", ""]]}, {"id": "1707.06992", "submitter": "Hossein Hosseini", "authors": "S. Hossein Hosseini and Afshin Ebrahimi", "title": "Ideological Sublations: Resolution of Dialectic in Population-based\n  Optimization", "comments": "An antenna selection model for massive MIMO was considered at the\n  current version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population-based optimization algorithm was designed, inspired by two main\nthinking modes in philosophy, both based on dialectic concept and\nthesis-antithesis paradigm. They impose two different kinds of dialectics.\nIdealistic and materialistic antitheses are formulated as optimization models.\nBased on the models, the population is coordinated for dialectical\ninteractions. At the population-based context, the formulated optimization\nmodels are reduced to a simple detection problem for each thinker (particle).\nAccording to the assigned thinking mode to each thinker and her/his\nmeasurements of corresponding dialectic with other candidate particles, they\ndeterministically decide to interact with a thinker in maximum dialectic with\ntheir theses. The position of a thinker at maximum dialectic is known as an\navailable antithesis among the existing solutions. The dialectical interactions\nat each ideological community are distinguished by meaningful distributions of\nstep-sizes for each thinking mode. In fact, the thinking modes are regarded as\nexploration and exploitation elements of the proposed algorithm. The result is\na delicate balance without any requirement for adjustment of step-size\ncoefficients. Main parameter of the proposed algorithm is the number of\nparticles appointed to each thinking modes, or equivalently for each kind of\nmotions. An additional integer parameter is defined to boost the stability of\nthe final algorithm in some particular problems. The proposed algorithm is\nevaluated by a testbed of 12 single-objective continuous benchmark functions.\nMoreover, its performance and speed were highlighted in sparse reconstruction\nand antenna selection problems, at the context of compressed sensing and\nmassive MIMO, respectively. The results indicate fast and efficient performance\nin comparison with well-known evolutionary algorithms and dedicated\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:53:04 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 13:33:09 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hosseini", "S. Hossein", ""], ["Ebrahimi", "Afshin", ""]]}, {"id": "1707.07413", "submitter": "Sanjeev Satheesh", "authors": "Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates, Yashesh Gaur,\n  Yi Li, Hairong Liu, Sanjeev Satheesh, David Seetapun, Anuroop Sriram, Zhenyao\n  Zhu", "title": "Exploring Neural Transducers for End-to-End Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform an empirical comparison among the CTC,\nRNN-Transducer, and attention-based Seq2Seq models for end-to-end speech\nrecognition. We show that, without any language model, Seq2Seq and\nRNN-Transducer models both outperform the best reported CTC models with a\nlanguage model, on the popular Hub5'00 benchmark. On our internal diverse\ndataset, these trends continue - RNNTransducer models rescored with a language\nmodel after beam search outperform our best CTC models. These results simplify\nthe speech recognition pipeline so that decoding can now be expressed purely as\nneural network operations. We also study how the choice of encoder architecture\naffects the performance of the three models - when all encoder layers are\nforward only, and when encoders downsample the input representation\naggressively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 06:05:21 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Battenberg", "Eric", ""], ["Chen", "Jitong", ""], ["Child", "Rewon", ""], ["Coates", "Adam", ""], ["Gaur", "Yashesh", ""], ["Li", "Yi", ""], ["Liu", "Hairong", ""], ["Satheesh", "Sanjeev", ""], ["Seetapun", "David", ""], ["Sriram", "Anuroop", ""], ["Zhu", "Zhenyao", ""]]}, {"id": "1707.07465", "submitter": "Dario Garcia-Gasulla", "authors": "Dario Garcia-Gasulla, Armand Vilalta, Ferran Par\\'es, Jonatan Moreno,\n  Eduard Ayguad\\'e, Jesus Labarta, Ulises Cort\\'es, Toyotaro Suzumura", "title": "Building Graph Representations of Deep Vector Embeddings", "comments": "Accepted at the 2nd Workshop on Semantic Deep Learning (SemDeep-2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns stored within pre-trained deep neural networks compose large and\npowerful descriptive languages that can be used for many different purposes.\nTypically, deep network representations are implemented within vector embedding\nspaces, which enables the use of traditional machine learning algorithms on top\nof them. In this short paper we propose the construction of a graph embedding\nspace instead, introducing a methodology to transform the knowledge coded\nwithin a deep convolutional network into a topological space (i.e. a network).\nWe outline how such graph can hold data instances, data features, relations\nbetween instances and features, and relations among features. Finally, we\nintroduce some preliminary experiments to illustrate how the resultant graph\nembedding space can be exploited through graph analytics algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 10:20:12 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:52:07 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Vilalta", "Armand", ""], ["Par\u00e9s", "Ferran", ""], ["Moreno", "Jonatan", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jesus", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1707.07493", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman, Julia Kiseleva, Maarten de Rijke", "title": "Modeling Label Ambiguity for Neural List-Wise Learning to Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  List-wise learning to rank methods are considered to be the state-of-the-art.\nOne of the major problems with these methods is that the ambiguous nature of\nrelevance labels in learning to rank data is ignored. Ambiguity of relevance\nlabels refers to the phenomenon that multiple documents may be assigned the\nsame relevance label for a given query, so that no preference order should be\nlearned for those documents. In this paper we propose a novel sampling\ntechnique for computing a list-wise loss that can take into account this\nambiguity. We show the effectiveness of the proposed method by training a\n3-layer deep neural network. We compare our new loss function to two strong\nbaselines: ListNet and ListMLE. We show that our method generalizes better and\nsignificantly outperforms other methods on the validation and test sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 11:28:21 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Jagerman", "Rolf", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1707.07961", "submitter": "Daniel Hsu", "authors": "Daniel Hsu", "title": "Time Series Compression Based on Adaptive Piecewise Recurrent\n  Autoencoder", "comments": "arXiv admin note: text overlap with arXiv:1707.00666", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series account for a large proportion of the data stored in financial,\nmedical and scientific databases. The efficient storage of time series is\nimportant in practical applications. In this paper, we propose a novel\ncompression scheme for time series. The encoder and decoder are both composed\nby recurrent neural networks (RNN) such as long short-term memory (LSTM). There\nis an autoencoder between encoder and decoder, which encodes the hidden state\nand input together and decodes them at the decoder side. Moreover, we\npre-process the original time series by partitioning it into segments with\nvarious lengths which have similar total variation. The experimental study\nshows that the proposed algorithm can achieve competitive compression ratio on\nreal-world time series.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 15:55:24 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 15:28:26 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Hsu", "Daniel", ""]]}, {"id": "1707.08101", "submitter": "Andreas Eitel", "authors": "Andreas Eitel, Nico Hauff and Wolfram Burgard", "title": "Learning to Singulate Objects using a Push Proposal Network", "comments": "International Symposium on Robotics Research (ISRR) 2017, videos:\n  http://robotpush.cs.uni-freiburg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to act in unstructured environments, such as cluttered piles of\nobjects, poses a substantial challenge for manipulation robots. We present a\nnovel neural network-based approach that separates unknown objects in clutter\nby selecting favourable push actions. Our network is trained from data\ncollected through autonomous interaction of a PR2 robot with randomly organized\ntabletop scenes. The model is designed to propose meaningful push actions based\non over-segmented RGB-D images. We evaluate our approach by singulating up to 8\nunknown objects in clutter. We demonstrate that our method enables the robot to\nperform the task with a high success rate and a low number of required push\nactions. Our results based on real-world experiments show that our network is\nable to generalize to novel objects of various sizes and shapes, as well as to\narbitrary object configurations. Videos of our experiments can be viewed at\nhttp://robotpush.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:36:36 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 18:42:35 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Eitel", "Andreas", ""], ["Hauff", "Nico", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.08139", "submitter": "Jacob Andreas", "authors": "Jacob Andreas and Dan Klein", "title": "Analogs of Linguistic Structure in Deep Representations", "comments": "In EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the compositional structure of message vectors computed by a\ndeep network trained on a communication game. By comparing truth-conditional\nrepresentations of encoder-produced message vectors to human-produced referring\nexpressions, we are able to identify aligned (vector, utterance) pairs with the\nsame meaning. We then search for structured relationships among these aligned\npairs to discover simple vector space transformations corresponding to\nnegation, conjunction, and disjunction. Our results suggest that neural\nrepresentations are capable of spontaneously developing a \"syntax\" with\nfunctional analogues to qualitative properties of natural language.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:10:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Andreas", "Jacob", ""], ["Klein", "Dan", ""]]}, {"id": "1707.08167", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Sebastien Rouault", "title": "On The Robustness of a Neural Network", "comments": "36th IEEE International Symposium on Reliable Distributed Systems 26\n  - 29 September 2017. Hong Kong, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 19:22:55 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:18:24 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Rouault", "Sebastien", ""]]}, {"id": "1707.08169", "submitter": "Alex Budilovsky G", "authors": "Oleksiy Budilovsky, Golnaz Alipour, Andre Knoesen, Lisa Brown, Soheil\n  Ghiasi", "title": "A Data-Driven Approach to Pre-Operative Evaluation of Lung Cancer\n  Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the number one cause of cancer deaths. Many early stage lung\ncancer patients have resectable tumors; however, their cardiopulmonary function\nneeds to be properly evaluated before they are deemed operative candidates.\nConsequently, a subset of such patients is asked to undergo standard pulmonary\nfunction tests, such as cardiopulmonary exercise tests (CPET) or stair climbs,\nto have their pulmonary function evaluated. The standard tests are expensive,\nlabor intensive, and sometimes ineffective due to co-morbidities, such as\nlimited mobility. Recovering patients would benefit greatly from a device that\ncan be worn at home, is simple to use, and is relatively inexpensive. Using\nadvances in information technology, the goal is to design a continuous,\ninexpensive, mobile and patient-centric mechanism for evaluation of a patient's\npulmonary function. A light mobile mask is designed, fitted with CO2, O2, flow\nvolume, and accelerometer sensors and tested on 18 subjects performing 15\nminute exercises. The data collected from the device is stored in a cloud\nservice and machine learning algorithms are used to train and predict a user's\nactivity .Several classification techniques are compared - K Nearest Neighbor,\nRandom Forest, Support Vector Machine, Artificial Neural Network, and Naive\nBayes. One useful area of interest involves comparing a patient's predicted\nactivity levels, especially using only breath data, to that of a normal\nperson's, using the classification models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 22:18:08 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Budilovsky", "Oleksiy", ""], ["Alipour", "Golnaz", ""], ["Knoesen", "Andre", ""], ["Brown", "Lisa", ""], ["Ghiasi", "Soheil", ""]]}, {"id": "1707.08214", "submitter": "Fr\\'ederic Godin", "authors": "Fr\\'ederic Godin, Jonas Degrave, Joni Dambre, Wesley De Neve", "title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\n  Functions in Quasi-Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2018.09.006", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\nunbounded positive and negative image, can be used as a drop-in replacement for\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\nprone to the vanishing gradient problem, they are noise robust, and they induce\nsparse activations.\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\nlanguage modeling. Additionally, we evaluate on character-level language\nmodeling, showing that we are able to stack up to eight QRNN layers with\nDReLUs, thus making it possible to improve the current state-of-the-art in\ncharacter-level language modeling over shallow architectures based on LSTMs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:52:32 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 15:50:57 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Godin", "Fr\u00e9deric", ""], ["Degrave", "Jonas", ""], ["Dambre", "Joni", ""], ["De Neve", "Wesley", ""]]}, {"id": "1707.08265", "submitter": "Ting Pan", "authors": "Ting Pan", "title": "Dragon: A Computation Graph Virtual Machine Based Deep Learning\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has made a great progress for these years. However, it is still\ndifficult to master the implement of various models because different\nresearchers may release their code based on different frameworks or interfaces.\nIn this paper, we proposed a computation graph based framework which only aims\nto introduce well-known interfaces. It will help a lot when reproducing a newly\nmodel or transplanting models that were implemented by other frameworks.\nAdditionally, we implement numerous recent models covering both Computer Vision\nand Nature Language Processing. We demonstrate that our framework will not\nsuffer from model-starving because it is much easier to make full use of the\nworks that are already done.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 01:16:29 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Pan", "Ting", ""]]}, {"id": "1707.08767", "submitter": "Wenji Li", "authors": "Zhun Fan, Wenji Li, Xinye Cai, Han Huang, Yi Fang, Yugen You, Jiajie\n  Mo, Caimin Wei and Erik Goodman", "title": "An Improved Epsilon Constraint-handling Method in MOEA/D for CMOPs with\n  Large Infeasible Regions", "comments": "17 pages, 7 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an improved epsilon constraint-handling mechanism, and\ncombines it with a decomposition-based multi-objective evolutionary algorithm\n(MOEA/D) to solve constrained multi-objective optimization problems (CMOPs).\nThe proposed constrained multi-objective evolutionary algorithm (CMOEA) is\nnamed MOEA/D-IEpsilon. It adjusts the epsilon level dynamically according to\nthe ratio of feasible to total solutions (RFS) in the current population. In\norder to evaluate the performance of MOEA/D-IEpsilon, a new set of CMOPs with\ntwo and three objectives is designed, having large infeasible regions (relative\nto the feasible regions), and they are called LIR-CMOPs. Then the fourteen\nbenchmarks, including LIR-CMOP1-14, are used to test MOEA/D-IEpsilon and four\nother decomposition-based CMOEAs, including MOEA/D-Epsilon, MOEA/D-SR,\nMOEA/D-CDP and C-MOEA/D. The experimental results indicate that MOEA/D-IEpsilon\nis significantly better than the other four CMOEAs on all of the test\ninstances, which shows that MOEA/D-IEpsilon is more suitable for solving CMOPs\nwith large infeasible regions. Furthermore, a real-world problem, namely the\nrobot gripper optimization problem, is used to test the five CMOEAs. The\nexperimental results demonstrate that MOEA/D-IEpsilon also outperforms the\nother four CMOEAs on this problem.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:59:31 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Fan", "Zhun", ""], ["Li", "Wenji", ""], ["Cai", "Xinye", ""], ["Huang", "Han", ""], ["Fang", "Yi", ""], ["You", "Yugen", ""], ["Mo", "Jiajie", ""], ["Wei", "Caimin", ""], ["Goodman", "Erik", ""]]}, {"id": "1707.08776", "submitter": "Georgios Chasparis", "authors": "Georgios C. Chasparis, Michael Rossbory and Verena Haunschmid", "title": "An Evolutionary Stochastic-Local-Search Framework for One-Dimensional\n  Cutting-Stock Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an evolutionary stochastic-local-search (SLS) algorithm for\naddressing a generalized version of the so-called 1/V/D/R cutting-stock\nproblem. Cutting-stock problems are encountered often in industrial\nenvironments and the ability to address them efficiently usually results in\nlarge economic benefits. Traditionally linear-programming-based techniques have\nbeen utilized to address such problems, however their flexibility might be\nlimited when nonlinear constraints and objective functions are introduced. To\nthis end, this paper proposes an evolutionary SLS algorithm for addressing\none-dimensional cutting-stock problems. The contribution lies in the\nintroduction of a flexible structural framework of the optimization that may\naccommodate a large family of diversification strategies including a novel\nparallel pattern appropriate for SLS algorithms (not necessarily restricted to\ncutting-stock problems). We finally demonstrate through experiments in a\nreal-world manufacturing problem the benefit in cost reduction of the\nconsidered diversification strategies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 08:31:17 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Chasparis", "Georgios C.", ""], ["Rossbory", "Michael", ""], ["Haunschmid", "Verena", ""]]}, {"id": "1707.09068", "submitter": "Alberto Delm\\'as", "authors": "Alberto Delmas, Sayeh Sharify, Patrick Judd, Andreas Moshovos", "title": "Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep\n  Learning Networks by Exploiting Numerical Precision Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks\n(DNNs), is presented and evaluated on Convolutional Neural Networks. TRT\nexploits the variable per layer precision requirements of DNNs to deliver\nexecution time that is proportional to the precision p in bits used per layer\nfor convolutional and fully-connected layers. Prior art has demonstrated an\naccelerator with the same execution performance only for convolutional layers.\nExperiments on image classification CNNs show that on average across all\nnetworks studied, TRT outperforms a state-of-the-art bit-parallel accelerator\nby 1:90x without any loss in accuracy while it is 1:17x more energy efficient.\nTRT requires no network retraining while it enables trading off accuracy for\nadditional improvements in execution performance and energy efficiency. For\nexample, if a 1% relative loss in accuracy is acceptable, TRT is on average\n2:04x faster and 1:25x more energy efficient than a conventional bit-parallel\naccelerator. A Tartan configuration that processes 2-bits at time, requires\nless area than the 1-bit configuration, improves efficiency to 1:24x over the\nbit-parallel baseline while being 73% faster for convolutional layers and 60%\nfaster for fully-connected layers is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 22:56:13 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Delmas", "Alberto", ""], ["Sharify", "Sayeh", ""], ["Judd", "Patrick", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1707.09219", "submitter": "Isabeau Pr\\'emont-Schwarz", "authors": "Isabeau Pr\\'emont-Schwarz, Alexander Ilin, Tele Hotloo Hao, Antti\n  Rasmus, Rinu Boney, Harri Valpola", "title": "Recurrent Ladder Networks", "comments": "9 pages, 9 figures, 7-page appendix, fixed fig 9 (c)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recurrent extension of the Ladder networks whose structure is\nmotivated by the inference required in hierarchical latent variable models. We\ndemonstrate that the recurrent Ladder is able to handle a wide variety of\ncomplex learning tasks that benefit from iterative inference and temporal\nmodeling. The architecture shows close-to-optimal results on temporal modeling\nof video data, competitive results on music modeling, and improved perceptual\ngrouping based on higher order abstractions, such as stochastic textures and\nmotion cues. We present results for fully supervised, semi-supervised, and\nunsupervised tasks. The results suggest that the proposed architecture and\nprinciples are powerful tools for learning a hierarchy of abstractions,\nlearning iterative inference and handling temporal information.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 13:19:11 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 15:14:19 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 13:43:12 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 06:43:47 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Pr\u00e9mont-Schwarz", "Isabeau", ""], ["Ilin", "Alexander", ""], ["Hao", "Tele Hotloo", ""], ["Rasmus", "Antti", ""], ["Boney", "Rinu", ""], ["Valpola", "Harri", ""]]}, {"id": "1707.09872", "submitter": "Armand Vilalta", "authors": "Armand Vilalta, Dario Garcia-Gasulla, Ferran Par\\'es, Eduard\n  Ayguad\\'e, Jesus Labarta, Ulises Cort\\'es, Toyotaro Suzumura", "title": "Full-Network Embedding in a Multimodal Embedding Pipeline", "comments": "In 2nd Workshop on Semantic Deep Learning (SemDeep-2) at the 12th\n  International Conference on Computational Semantics (IWCS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art for image annotation and image retrieval tasks\nis obtained through deep neural networks, which combine an image representation\nand a text representation into a shared embedding space. In this paper we\nevaluate the impact of using the Full-Network embedding in this setting,\nreplacing the original image representation in a competitive multimodal\nembedding generation scheme. Unlike the one-layer image embeddings typically\nused by most approaches, the Full-Network embedding provides a multi-scale\nrepresentation of images, which results in richer characterizations. To measure\nthe influence of the Full-Network embedding, we evaluate its performance on\nthree different datasets, and compare the results with the original multimodal\nembedding generation scheme when using a one-layer image embedding, and with\nthe rest of the state-of-the-art. Results for image annotation and image\nretrieval tasks indicate that the Full-Network embedding is consistently\nsuperior to the one-layer embedding. These results motivate the integration of\nthe Full-Network embedding on any multimodal embedding generation scheme,\nsomething feasible thanks to the flexibility of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 10:27:33 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:11:42 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Vilalta", "Armand", ""], ["Garcia-Gasulla", "Dario", ""], ["Par\u00e9s", "Ferran", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jesus", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1707.09899", "submitter": "Ashwinkumar Ganesan", "authors": "Prutha Date, Ashwinkumar Ganesan, Tim Oates", "title": "Fashioning with Networks: Neural Style Transfer to Design Clothes", "comments": "ML4Fashion 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been highly successful in performing a\nhost of computer vision tasks such as object recognition, object detection,\nimage segmentation and texture synthesis. In 2015, Gatys et. al [7] show how\nthe style of a painter can be extracted from an image of the painting and\napplied to another normal photograph, thus recreating the photo in the style of\nthe painter. The method has been successfully applied to a wide range of images\nand has since spawned multiple applications and mobile apps. In this paper, the\nneural style transfer algorithm is applied to fashion so as to synthesize new\ncustom clothes. We construct an approach to personalize and generate new custom\nclothes based on a users preference and by learning the users fashion choices\nfrom a limited set of clothes from their closet. The approach is evaluated by\nanalyzing the generated images of clothes and how well they align with the\nusers fashion style.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:54:11 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Date", "Prutha", ""], ["Ganesan", "Ashwinkumar", ""], ["Oates", "Tim", ""]]}, {"id": "1707.09952", "submitter": "Matthew Marinella", "authors": "Matthew J. Marinella, Sapan Agarwal, Alexander Hsia, Isaac Richter,\n  Robin Jacobs-Gedrim, John Niroula, Steven J. Plimpton, Engin Ipek, Conrad D.\n  James", "title": "Multiscale Co-Design Analysis of Energy, Latency, Area, and Accuracy of\n  a ReRAM Analog Neural Training Accelerator", "comments": null, "journal-ref": null, "doi": "10.1109/JETCAS.2018.2796379", "report-no": null, "categories": "cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are an increasingly attractive algorithm for natural language\nprocessing and pattern recognition. Deep networks with >50M parameters are made\npossible by modern GPU clusters operating at <50 pJ per op and more recently,\nproduction accelerators capable of <5pJ per operation at the board level.\nHowever, with the slowing of CMOS scaling, new paradigms will be required to\nachieve the next several orders of magnitude in performance per watt gains.\nUsing an analog resistive memory (ReRAM) crossbar to perform key matrix\noperations in an accelerator is an attractive option. This work presents a\ndetailed design using a state of the art 14/16 nm PDK for of an analog crossbar\ncircuit block designed to process three key kernels required in training and\ninference of neural networks. A detailed circuit and device-level analysis of\nenergy, latency, area, and accuracy are given and compared to relevant designs\nusing standard digital ReRAM and SRAM operations. It is shown that the analog\naccelerator has a 270x energy and 540x latency advantage over a similar block\nutilizing only digital ReRAM and takes only 11 fJ per multiply and accumulate\n(MAC). Compared to an SRAM based accelerator, the energy is 430X better and\nlatency is 34X better. Although training accuracy is degraded in the analog\naccelerator, several options to improve this are presented. The possible gains\nover a similar digital-only version of this accelerator block suggest that\ncontinued optimization of analog resistive memories is valuable. This detailed\ncircuit and device analysis of a training accelerator may serve as a foundation\nfor further architecture-level studies.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 16:54:32 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 00:18:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Marinella", "Matthew J.", ""], ["Agarwal", "Sapan", ""], ["Hsia", "Alexander", ""], ["Richter", "Isaac", ""], ["Jacobs-Gedrim", "Robin", ""], ["Niroula", "John", ""], ["Plimpton", "Steven J.", ""], ["Ipek", "Engin", ""], ["James", "Conrad D.", ""]]}]