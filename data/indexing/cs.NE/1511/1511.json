[{"id": "1511.00221", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov", "title": "LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization", "comments": "to appear in Evolutionary Computation Journal, MIT Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is often\nconsidered to be the method of choice for continuous optimization when first-\nand/or second- order information is available. However, the use of L-BFGS can\nbe complicated in a black-box scenario where gradient information is not\navailable and therefore should be numerically estimated. The accuracy of this\nestimation, obtained by finite difference methods, is often problem-dependent\nthat may lead to premature convergence of the algorithm.\n  In this paper, we demonstrate an alternative to L-BFGS, the limited memory\nCovariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov\n(2014). The LM-CMA is a stochastic derivative-free algorithm for numerical\noptimization of non-linear, non-convex optimization problems. Inspired by the\nL-BFGS, the LM-CMA samples candidate solutions according to a covariance matrix\nreproduced from $m$ direction vectors selected during the optimization process.\nThe decomposition of the covariance matrix into Cholesky factors allows to\nreduce the memory complexity to $O(mn)$, where $n$ is the number of decision\nvariables. The time complexity of sampling one candidate solution is also\n$O(mn)$, but scales as only about 25 scalar-vector multiplications in practice.\nThe algorithm has an important property of invariance w.r.t. strictly\nincreasing transformations of the objective function, such transformations do\nnot compromise its ability to approach the optimum. The LM-CMA outperforms the\noriginal CMA-ES and its large scale versions on non-separable ill-conditioned\nproblems with a factor increasing with problem dimension. Invariance properties\nof the algorithm do not prevent it from demonstrating a comparable performance\nto L-BFGS on non-trivial large scale smooth and nonsmooth optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 09:52:58 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Loshchilov", "Ilya", ""]]}, {"id": "1511.00296", "submitter": "Matteo Smerlak", "authors": "Matteo Smerlak, Ahmed Youssef", "title": "Limiting fitness distributions in evolutionary dynamics", "comments": "15 pages + appendices", "journal-ref": "J. Theor. Biol. 416, 68-80 (2017)", "doi": "10.1016/j.jtbi.2017.01.005", "report-no": null, "categories": "q-bio.PE cond-mat.stat-mech cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Darwinian evolution can be modeled in general terms as a flow in the space of\nfitness (i.e. reproductive rate) distributions. In the diffusion approximation,\nTsimring et al. have showed that this flow admits \"fitness wave\" solutions:\nGaussian-shape fitness distributions moving towards higher fitness values at\nconstant speed. Here we show more generally that evolving fitness distributions\nare attracted to a one-parameter family of distributions with a fixed parabolic\nrelationship between skewness and kurtosis. Unlike fitness waves, this\nstatistical pattern encompasses both positive and negative (a.k.a. purifying)\nselection and is not restricted to rapidly adapting populations. Moreover we\nfind that the mean fitness of a population under the selection of pre-existing\nvariation is a power-law function of time, as observed in microbiological\nevolution experiments but at variance with fitness wave theory. At the\nconceptual level, our results can be viewed as the resolution of the \"dynamic\ninsufficiency\" of Fisher's fundamental theorem of natural selection. Our\npredictions are in good agreement with numerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 19:16:47 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 16:15:56 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Smerlak", "Matteo", ""], ["Youssef", "Ahmed", ""]]}, {"id": "1511.00363", "submitter": "Matthieu Courbariaux", "authors": "Matthieu Courbariaux, Yoshua Bengio and Jean-Pierre David", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during\n  propagations", "comments": "Accepted at NIPS 2015, 9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide\nrange of tasks, with the best results obtained with large training sets and\nlarge models. In the past, GPUs enabled these breakthroughs because of their\ngreater computational speed. In the future, faster computation at both training\nand test time is likely to be crucial for further progress and for consumer\napplications on low-power devices. As a result, there is much interest in\nresearch and development of dedicated hardware for Deep Learning (DL). Binary\nweights, i.e., weights which are constrained to only two possible values (e.g.\n-1 or 1), would bring great benefits to specialized DL hardware by replacing\nmany multiply-accumulate operations by simple accumulations, as multipliers are\nthe most space and power-hungry components of the digital implementation of\nneural networks. We introduce BinaryConnect, a method which consists in\ntraining a DNN with binary weights during the forward and backward\npropagations, while retaining precision of the stored weights in which\ngradients are accumulated. Like other dropout schemes, we show that\nBinaryConnect acts as regularizer and we obtain near state-of-the-art results\nwith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 02:50:05 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 23:31:09 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 13:11:45 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Courbariaux", "Matthieu", ""], ["Bengio", "Yoshua", ""], ["David", "Jean-Pierre", ""]]}, {"id": "1511.00540", "submitter": "Jonathan Binas", "authors": "Jonathan Binas, Giacomo Indiveri, Michael Pfeiffer", "title": "Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem\n  Solvers", "comments": "Accepted at ISCAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving constraint satisfaction problems (CSPs) is a notoriously expensive\ncomputational task. Recently, it has been proposed that efficient stochastic\nsolvers can be obtained through appropriately configured spiking neural\nnetworks performing Markov Chain Monte Carlo (MCMC) sampling. The possibility\nto run such models on massively parallel, low-power neuromorphic hardware holds\ngreat promise; however, previously proposed networks are based on\nprobabilistically spiking neurons, and thus rely on random number generators or\nexternal noise sources to achieve the necessary stochasticity, leading to\nsignificant overhead in the implementation. Here we show how stochasticity can\nbe achieved by implementing deterministic models of integrate and fire neurons\nusing subthreshold analog circuits that are affected by thermal noise. We\npresent an efficient implementation of spike-based CSP solvers using a\nreconfigurable neural network VLSI device, and the device's intrinsic noise as\na source of randomness. To illustrate the overall concept, we implement a\ngeneric Sudoku solver based on our approach and demonstrate its operation. We\nestablish a link between the neuron parameters and the system dynamics,\nallowing for a simple temperature control mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 15:16:43 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 09:17:17 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Binas", "Jonathan", ""], ["Indiveri", "Giacomo", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1511.00561", "submitter": "Alex Kendall", "authors": "Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 15:51:03 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 13:56:56 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2016 21:11:59 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Kendall", "Alex", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.00971", "submitter": "Diego Marron", "authors": "Diego Marr\\'on (dmarron@ac.upc.edu) and Jesse Read\n  (jesse.read@aalto.fi) and Albert Bifet (albert.bifet@telecom-paristech.fr)\n  and Nacho Navarro (nacho@ac.upc.edu)", "title": "Data Stream Classification using Random Feature Functions and Novel\n  Method Combinations", "comments": "20 pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data streams are being generated in a faster, bigger, and more\ncommonplace. In this scenario, Hoeffding Trees are an established method for\nclassification. Several extensions exist, including high-performing ensemble\nsetups such as online and leveraging bagging. Also, $k$-nearest neighbors is a\npopular choice, with most extensions dealing with the inherent performance\nlimitations over a potentially-infinite stream.\n  At the same time, gradient descent methods are becoming increasingly popular,\nowing in part to the successes of deep learning. Although deep neural networks\ncan learn incrementally, they have so far proved too sensitive to\nhyper-parameter options and initial conditions to be considered an effective\n`off-the-shelf' data-streams solution.\n  In this work, we look at combinations of Hoeffding-trees, nearest neighbour,\nand gradient descent methods with a streaming preprocessing approach in the\nform of a random feature functions filter for additional predictive power.\n  We further extend the investigation to implementing methods on GPUs, which we\ntest on some large real-world datasets, and show the benefits of using GPUs for\ndata-stream learning due to their high scalability.\n  Our empirical evaluation yields positive results for the novel approaches\nthat we experiment with, highlighting important issues, and shed light on\npromising future directions in approaches to data-stream classification.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 16:29:57 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Marr\u00f3n", "Diego", "", "dmarron@ac.upc.edu"], ["Read", "Jesse", "", "jesse.read@aalto.fi"], ["Bifet", "Albert", "", "albert.bifet@telecom-paristech.fr"], ["Navarro", "Nacho", "", "nacho@ac.upc.edu"]]}, {"id": "1511.01042", "submitter": "Junyoung Chung", "authors": "Junyoung Chung and Jacob Devlin and Hany Hassan Awadalla", "title": "Detecting Interrogative Utterances with Recurrent Neural Networks", "comments": "6 pages, accepted to NIPS 2015 Workshop on Machine Learning for\n  Spoken Language Understanding and Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore different neural network architectures that can\npredict if a speaker of a given utterance is asking a question or making a\nstatement. We com- pare the outcomes of regularization methods that are\npopularly used to train deep neural networks and study how different context\nfunctions can affect the classification performance. We also compare the\nefficacy of gated activation functions that are favorably used in recurrent\nneural networks and study how to combine multimodal inputs. We evaluate our\nmodels on two multimodal datasets: MSR-Skype and CALLHOME.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 19:26:16 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 03:54:19 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Chung", "Junyoung", ""], ["Devlin", "Jacob", ""], ["Awadalla", "Hany Hassan", ""]]}, {"id": "1511.01088", "submitter": "Juan Juli\\'an Merelo-Guerv\\'os Pr.", "authors": "Juan-J. Merelo, Pablo Garc\\'ia-S\\'anchez, Mario Garc\\'ia-Valdez,\n  Israel Blancas", "title": "There is no fast lunch: an examination of the running speed of\n  evolutionary algorithms in several languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is quite usual when an evolutionary algorithm tool or library uses a\nlanguage other than C, C++, Java or Matlab that a reviewer or the audience\nquestions its usefulness based on the speed of those other languages,\npurportedly slower than the aforementioned ones. Despite speed being not\neverything needed to design a useful evolutionary algorithm application, in\nthis paper we will measure the speed for several very basic evolutionary\nalgorithm operations in several languages which use different virtual machines\nand approaches, and prove that, in fact, there is no big difference in speed\nbetween interpreted and compiled languages, and that in some cases, interpreted\nlanguages such as JavaScript or Python can be faster than compiled languages\nsuch as Scala, making them worthy of use for evolutionary algorithm\nexperimentation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 20:34:24 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Merelo", "Juan-J.", ""], ["Garc\u00eda-S\u00e1nchez", "Pablo", ""], ["Garc\u00eda-Valdez", "Mario", ""], ["Blancas", "Israel", ""]]}, {"id": "1511.01427", "submitter": "Giovanni Carmantini", "authors": "Giovanni S Carmantini, Peter beim Graben, Mathieu Desroches, Serafim\n  Rodrigues", "title": "Turing Computation with Recurrent Artificial Neural Networks", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the results by Siegelmann & Sontag (1995) by providing a novel and\nparsimonious constructive mapping between Turing Machines and Recurrent\nArtificial Neural Networks, based on recent developments of Nonlinear Dynamical\nAutomata. The architecture of the resulting R-ANNs is simple and elegant,\nstemming from its transparent relation with the underlying NDAs. These\ncharacteristics yield promise for developments in machine learning methods and\nsymbolic computation with continuous time dynamical systems. A framework is\nprovided to directly program the R-ANNs from Turing Machine descriptions, in\nabsence of network training. At the same time, the network can potentially be\ntrained to perform algorithmic tasks, with exciting possibilities in the\nintegration of approaches akin to Google DeepMind's Neural Turing Machines.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 18:40:46 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Carmantini", "Giovanni S", ""], ["Graben", "Peter beim", ""], ["Desroches", "Mathieu", ""], ["Rodrigues", "Serafim", ""]]}, {"id": "1511.01865", "submitter": "Seyed Mostafa Kia", "authors": "Nastaran Mohammadian Rad, Andrea Bizzego, Seyed Mostafa Kia, Giuseppe\n  Jurman, Paola Venuti, Cesare Furlanello", "title": "Convolutional Neural Network for Stereotypical Motor Movement Detection\n  in Autism", "comments": "Presented at 5th NIPS Workshop on Machine Learning and Interpretation\n  in Neuroimaging (MLINI), 2015, (http://arxiv.org/html/1605.04435), Report-no:\n  MLINI/2015/13", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/13", "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism Spectrum Disorders (ASDs) are often associated with specific atypical\npostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have\na specific visibility. While the identification and the quantification of SMM\npatterns remain complex, its automation would provide support to accurate\ntuning of the intervention in the therapy of autism. Therefore, it is essential\nto develop automatic SMM detection systems in a real world setting, taking care\nof strong inter-subject and intra-subject variability. Wireless accelerometer\nsensing technology can provide a valid infrastructure for real-time SMM\ndetection, however such variability remains a problem also for machine learning\nmethods, in particular whenever handcrafted features extracted from\naccelerometer signal are considered. Here, we propose to employ the deep\nlearning paradigm in order to learn discriminating features from multi-sensor\naccelerometer signals. Our results provide preliminary evidence that feature\nlearning and transfer learning embedded in the deep architecture achieve higher\naccurate SMM detectors in longitudinal scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:36:33 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 21:02:02 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 19:11:34 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Rad", "Nastaran Mohammadian", ""], ["Bizzego", "Andrea", ""], ["Kia", "Seyed Mostafa", ""], ["Jurman", "Giuseppe", ""], ["Venuti", "Paola", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1511.02274", "submitter": "Zichao Yang", "authors": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola", "title": "Stacked Attention Networks for Image Question Answering", "comments": "test-dev/standard results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents stacked attention networks (SANs) that learn to answer\nnatural language questions from images. SANs use semantic representation of a\nquestion as query to search for the regions in an image that are related to the\nanswer. We argue that image question answering (QA) often requires multiple\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\nimage multiple times to infer the answer progressively. Experiments conducted\non four image QA data sets demonstrate that the proposed SANs significantly\noutperform previous state-of-the-art approaches. The visualization of the\nattention layers illustrates the progress that the SAN locates the relevant\nvisual clues that lead to the answer of the question layer-by-layer.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 00:43:32 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 20:37:49 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Yang", "Zichao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""], ["Smola", "Alex", ""]]}, {"id": "1511.02506", "submitter": "Yi-Hsiu Liao", "authors": "Yi-Hsiu Liao, Hung-yi Lee, Lin-shan Lee", "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition", "comments": "arXiv admin note: text overlap with arXiv:1506.01163", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the Structured Deep Neural Network (structured DNN)\nas a structured and deep learning framework. This approach can learn to find\nthe best structured object (such as a label sequence) given a structured input\n(such as a vector sequence) by globally considering the mapping relationships\nbetween the structures rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learn utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning approach. This approach was\nshown to beat structured SVM in preliminary experiments on TIMIT.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 17:08:54 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Liao", "Yi-Hsiu", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1511.02554", "submitter": "Hojjat Salehinejad", "authors": "Farhad Pouladi, Hojjat Salehinejad and Amir Mohammad Gilani", "title": "Deep Recurrent Neural Networks for Sequential Phenotype Prediction in\n  Genomics", "comments": "The articles is accepted at DeSE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyzing of modern biological data, we are often dealing with ill-posed\nproblems and missing data, mostly due to high dimensionality and\nmulticollinearity of the dataset. In this paper, we have proposed a system\nbased on matrix factorization (MF) and deep recurrent neural networks (DRNNs)\nfor genotype imputation and phenotype sequences prediction. In order to model\nthe long-term dependencies of phenotype data, the new Recurrent Linear Units\n(ReLU) learning strategy is utilized for the first time. The proposed model is\nimplemented for parallel processing on central processing units (CPUs) and\ngraphic processing units (GPUs). Performance of the proposed model is compared\nwith other training algorithms for learning long-term dependencies as well as\nthe sparse partial least square (SPLS) method on a set of genotype and\nphenotype data with 604 samples, 1980 single-nucleotide polymorphisms (SNPs),\nand two traits. The results demonstrate performance of the ReLU training\nalgorithm in learning long-term dependencies in RNNs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 02:11:00 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 20:48:34 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2016 03:30:10 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Pouladi", "Farhad", ""], ["Salehinejad", "Hojjat", ""], ["Gilani", "Amir Mohammad", ""]]}, {"id": "1511.02580", "submitter": "Zhouhan Lin", "authors": "Zhouhan Lin, Roland Memisevic, Kishore Konda", "title": "How far can we go without convolution: Improving fully-connected\n  networks", "comments": "10 pages, 11 figures, submitted for ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ways to improve the performance of fully connected networks. We\nfound that two approaches in particular have a strong effect on performance:\nlinear bottleneck layers and unsupervised pre-training using autoencoders\nwithout hidden unit biases. We show how both approaches can be related to\nimproving gradient flow and reducing sparsity in the network. We show that a\nfully connected network can yield approximately 70% classification accuracy on\nthe permutation-invariant CIFAR-10 task, which is much higher than the current\nstate-of-the-art. By adding deformations to the training data, the fully\nconnected network achieves 78% accuracy, which is just 10% short of a decent\nconvolutional network.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 06:56:24 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Lin", "Zhouhan", ""], ["Memisevic", "Roland", ""], ["Konda", "Kishore", ""]]}, {"id": "1511.02623", "submitter": "Pierre Bessiere", "authors": "Jacques Droulez and David Colliaux and Audrey Houillon and Pierre\n  Bessi\\`ere", "title": "Toward Biochemical Probabilistic Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Living organisms survive and multiply even though they have uncertain and\nincomplete information about their environment and imperfect models to predict\nthe consequences of their actions. Bayesian models have been proposed to face\nthis challenge. Indeed, Bayesian inference is a way to do optimal reasoning\nwhen only uncertain and incomplete information is available. Various\nperceptive, sensory-motor, and cognitive functions have been successfully\nmodeled this way. However, the biological mechanisms allowing animals and\nhumans to represent and to compute probability distributions are not known. It\nhas been proposed that neurons and assemblies of neurons could be the\nappropriate scale to search for clues to probabilistic reasoning. In contrast,\nin this paper, we propose that interacting populations of macromolecules and\ndiffusible messengers can perform probabilistic computation. This suggests that\nprobabilistic reasoning, based on cellular signaling pathways, is a fundamental\nskill of living organisms available to the simplest unicellular organisms as\nwell as the most complex brains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 10:26:57 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Droulez", "Jacques", ""], ["Colliaux", "David", ""], ["Houillon", "Audrey", ""], ["Bessi\u00e8re", "Pierre", ""]]}, {"id": "1511.02680", "submitter": "Alex Kendall", "authors": "Alex Kendall and Vijay Badrinarayanan and Roberto Cipolla", "title": "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder\n  Architectures for Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for probabilistic pixel-wise semantic\nsegmentation, which we term Bayesian SegNet. Semantic segmentation is an\nimportant tool for visual scene understanding and a meaningful measure of\nuncertainty is essential for decision making. Our contribution is a practical\nsystem which is able to predict pixel-wise class labels with a measure of model\nuncertainty. We achieve this by Monte Carlo sampling with dropout at test time\nto generate a posterior distribution of pixel class labels. In addition, we\nshow that modelling uncertainty improves segmentation performance by 2-3%\nacross a number of state of the art architectures such as SegNet, FCN and\nDilation Network, with no additional parametrisation. We also observe a\nsignificant improvement in performance for smaller datasets where modelling\nuncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN\nScene Understanding and outdoor CamVid driving scenes datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:00:21 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 22:04:21 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Kendall", "Alex", ""], ["Badrinarayanan", "Vijay", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.02799", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein", "title": "Neural Module Networks", "comments": "Corrects an error in the evaluation of the NMN-only ablation\n  experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is fundamentally compositional in nature---a\nquestion like \"where is the dog?\" shares substructure with questions like \"what\ncolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously\nexploit the representational capacity of deep networks and the compositional\nlinguistic structure of questions. We describe a procedure for constructing and\nlearning *neural module networks*, which compose collections of jointly-trained\nneural \"modules\" into deep networks for question answering. Our approach\ndecomposes questions into their linguistic substructures, and uses these\nstructures to dynamically instantiate modular networks (with reusable\ncomponents for recognizing dogs, classifying colors, etc.). The resulting\ncompound networks are jointly trained. We evaluate our approach on two\nchallenging datasets for visual question answering, achieving state-of-the-art\nresults on both the VQA natural image dataset and a new dataset of complex\nquestions about abstract shapes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:48:39 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 06:36:22 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 18:26:40 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 17:15:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Klein", "Dan", ""]]}, {"id": "1511.02954", "submitter": "Conrado Miranda", "authors": "Conrado S. Miranda and Fernando J. Von Zuben", "title": "Reducing the Training Time of Neural Networks by Partitioning", "comments": "Figure 2b has lower quality due to file size constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for pre-training neural networks that can\ndecrease the total training time for a neural network while maintaining the\nfinal performance, which motivates its use on deep neural networks. By\npartitioning the training task in multiple training subtasks with sub-models,\nwhich can be performed independently and in parallel, it is shown that the size\nof the sub-models reduces almost quadratically with the number of subtasks\ncreated, quickly scaling down the sub-models used for the pre-training. The\nsub-models are then merged to provide a pre-trained initial set of weights for\nthe original model. The proposed method is independent of the other aspects of\nthe training, such as architecture of the neural network, training method, and\nobjective, making it compatible with a wide range of existing approaches. The\nspeedup without loss of performance is validated experimentally on MNIST and on\nCIFAR10 data sets, also showing that even performing the subtasks sequentially\ncan decrease the training time. Moreover, we show that larger models may\npresent higher speedups and conjecture about the benefits of the method in\ndistributed learning systems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 01:20:51 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 17:18:06 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Miranda", "Conrado S.", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1511.03416", "submitter": "Yuke Zhu", "authors": "Yuke Zhu, Oliver Groth, Michael Bernstein and Li Fei-Fei", "title": "Visual7W: Grounded Question Answering in Images", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have seen great progress in basic perceptual tasks such as object\nrecognition and detection. However, AI models still fail to match humans in\nhigh-level vision tasks due to the lack of capacities for deeper reasoning.\nRecently the new task of visual question answering (QA) has been proposed to\nevaluate a model's capacity for deep image understanding. Previous works have\nestablished a loose, global association between QA sentences and images.\nHowever, many questions and answers, in practice, relate to local regions in\nthe images. We establish a semantic link between textual descriptions and image\nregions by object-level grounding. It enables a new type of QA with visual\nanswers, in addition to textual answers used in previous work. We study the\nvisual QA tasks in a grounded setting with a large collection of 7W\nmultiple-choice QA pairs. Furthermore, we evaluate human performance and\nseveral baseline models on the QA tasks. Finally, we propose a novel LSTM model\nwith spatial attention to tackle the 7W QA tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 08:29:14 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 21:53:55 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 19:37:20 GMT"}, {"version": "v4", "created": "Sat, 9 Apr 2016 07:18:10 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Zhu", "Yuke", ""], ["Groth", "Oliver", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.03483", "submitter": "Jun He", "authors": "Jun He", "title": "An Analytic Expression of Relative Approximation Error for a Class of\n  Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2016.7744345", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important question in evolutionary computation is how good solutions\nevolutionary algorithms can produce. This paper aims to provide an analytic\nanalysis of solution quality in terms of the relative approximation error,\nwhich is defined by the error between 1 and the approximation ratio of the\nsolution found by an evolutionary algorithm. Since evolutionary algorithms are\niterative methods, the relative approximation error is a function of\ngenerations. With the help of matrix analysis, it is possible to obtain an\nexact expression of such a function. In this paper, an analytic expression for\ncalculating the relative approximation error is presented for a class of\nevolutionary algorithms, that is, (1+1) strictly elitist evolution algorithms.\nFurthermore, analytic expressions of the fitness value and the average\nconvergence rate in each generation are also derived for this class of\nevolutionary algorithms. The approach is promising, and it can be extended to\nnon-elitist or population-based algorithms too.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 13:02:36 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 16:02:39 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 12:10:25 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["He", "Jun", ""]]}, {"id": "1511.03570", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Jason Morton", "title": "Dimension of Marginals of Kronecker Product Models", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE math.AG math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Kronecker product model is the set of visible marginal probability\ndistributions of an exponential family whose sufficient statistics matrix\nfactorizes as a Kronecker product of two matrices, one for the visible\nvariables and one for the hidden variables. We estimate the dimension of these\nmodels by the maximum rank of the Jacobian in the limit of large parameters.\nThe limit is described by the tropical morphism; a piecewise linear map with\npieces corresponding to slicings of the visible matrix by the normal fan of the\nhidden matrix. We obtain combinatorial conditions under which the model has the\nexpected dimension, equal to the minimum of the number of natural parameters\nand the dimension of the ambient probability simplex. Additionally, we prove\nthat the binary restricted Boltzmann machine always has the expected dimension.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 00:44:59 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Montufar", "Guido", ""], ["Morton", "Jason", ""]]}, {"id": "1511.03771", "submitter": "Sachin Talathi", "authors": "Sachin S. Talathi and Aniket Vartak", "title": "Improving performance of recurrent neural network with relu nonlinearity", "comments": "10 pages 6 figures; under consideration for publication with ICLR\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years significant progress has been made in successfully training\nrecurrent neural networks (RNNs) on sequence learning problems involving long\nrange temporal dependencies. The progress has been made on three fronts: (a)\nAlgorithmic improvements involving sophisticated optimization techniques, (b)\nnetwork design involving complex hidden layer nodes and specialized recurrent\nlayer connections and (c) weight initialization methods. In this paper, we\nfocus on recently proposed weight initialization with identity matrix for the\nrecurrent weights in a RNN. This initialization is specifically proposed for\nhidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple\ndynamical systems perspective on weight initialization process, which allows us\nto propose a modified weight initialization strategy. We show that this\ninitialization technique leads to successfully training RNNs composed of ReLUs.\nWe demonstrate that our proposal produces comparable or better solution for\nthree toy problems involving long range temporal structure: the addition\nproblem, the multiplication problem and the MNIST classification problem using\nsequence of pixels. In addition, we present results for a benchmark action\nrecognition problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 04:35:41 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 01:14:54 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 12:52:26 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Talathi", "Sachin S.", ""], ["Vartak", "Aniket", ""]]}, {"id": "1511.03908", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, Christian Wolf, Griffin Lacey, Lex Fridman, Deepak\n  Chandra, Brandon Barbello, Graham Taylor", "title": "Learning Human Identity from Motion Patterns", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale study exploring the capability of temporal deep\nneural networks to interpret natural human kinematics and introduce the first\nmethod for active biometric authentication with mobile inertial sensors. At\nGoogle, we have created a first-of-its-kind dataset of human movements,\npassively collected by 1500 volunteers using their smartphones daily over\nseveral months. We (1) compare several neural architectures for efficient\nlearning of temporal multi-modal data representations, (2) propose an optimized\nshift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the\ndiscriminatively-trained dynamic features in a probabilistic generative\nframework taking into account temporal characteristics. Our results demonstrate\nthat human kinematics convey important information about user identity and can\nserve as a valuable component of multi-modal authentication systems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 14:48:53 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 15:23:06 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 01:59:58 GMT"}, {"version": "v4", "created": "Thu, 21 Apr 2016 16:04:00 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Lacey", "Griffin", ""], ["Fridman", "Lex", ""], ["Chandra", "Deepak", ""], ["Barbello", "Brandon", ""], ["Taylor", "Graham", ""]]}, {"id": "1511.03979", "submitter": "Patrick McClure", "authors": "Patrick McClure, Nikolaus Kriegeskorte", "title": "Representational Distance Learning for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) provide useful models of visual representational\ntransformations. We present a method that enables a DNN (student) to learn from\nthe internal representational spaces of a reference model (teacher), which\ncould be another DNN or, in the future, a biological brain. Representational\nspaces of the student and the teacher are characterized by representational\ndistance matrices (RDMs). We propose representational distance learning (RDL),\na stochastic gradient descent method that drives the RDMs of the student to\napproximate the RDMs of the teacher. We demonstrate that RDL is competitive\nwith other transfer learning techniques for two publicly available benchmark\ncomputer vision datasets (MNIST and CIFAR-100), while allowing for\narchitectural differences between student and teacher. By pulling the student's\nRDMs towards those of the teacher, RDL significantly improved visual\nclassification performance when compared to baseline networks that did not use\ntransfer learning. In the future, RDL may enable combined supervised training\nof deep neural networks using task constraints (e.g. images and category\nlabels) and constraints from brain-activity measurements, so as to build models\nthat replicate the internal representational spaces of biological brains.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:35:03 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 13:58:48 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 17:19:03 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2015 17:10:25 GMT"}, {"version": "v5", "created": "Fri, 4 Dec 2015 20:45:15 GMT"}, {"version": "v6", "created": "Mon, 7 Nov 2016 18:37:05 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["McClure", "Patrick", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1511.03984", "submitter": "Run Wang", "authors": "Run Wang, Qiaoli Mo, Qian Zhang, Fudi Chen and Dazuo Yang", "title": "Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester\n  Using Artificial Neural Networks and Support Vector Machine", "comments": "32 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3\\b{eta}-O-phthalic ester of betulinic acid is of great importance in\nanticancer studies. However, the optimization of its reaction conditions\nrequires a large number of experimental works. To simplify the number of times\nof optimization in experimental works, here, we use artificial neural network\n(ANN) and support vector machine (SVM) models for the prediction of yields of\n3\\b{eta}-O-phthalic ester of betulinic acid synthesized by betulinic acid and\nphthalic anhydride using lipase as biocatalyst. General regression neural\nnetwork (GRNN), multilayer feed-forward neural network (MLFN) and the SVM\nmodels were trained based on experimental data. Four indicators were set as\nindependent variables, including time (h), temperature (C), amount of enzyme\n(mg) and molar ratio, while the yield of the 3\\b{eta}-O-phthalic ester of\nbetulinic acid was set as the dependent variable. Results show that the GRNN\nand SVM models have the best prediction results during the testing process,\nwith comparatively low RMS errors (4.01 and 4.23respectively) and short\ntraining times (both 1s). The prediction accuracy of the GRNN and SVM are both\n100% in testing process, under the tolerance of 30%.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:40:42 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wang", "Run", ""], ["Mo", "Qiaoli", ""], ["Zhang", "Qian", ""], ["Chen", "Fudi", ""], ["Yang", "Dazuo", ""]]}, {"id": "1511.04110", "submitter": "Ali Mollahosseini", "authors": "Ali Mollahosseini, David Chan, Mohammad H. Mahoor", "title": "Going Deeper in Facial Expression Recognition using Deep Neural Networks", "comments": "To be appear in IEEE Winter Conference on Applications of Computer\n  Vision (WACV), 2016 {Accepted in first round submission}", "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2016", "doi": "10.1109/WACV.2016.7477450", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Facial Expression Recognition (FER) has remained a challenging and\ninteresting problem. Despite efforts made in developing various methods for\nFER, existing approaches traditionally lack generalizability when applied to\nunseen images or those that are captured in wild setting. Most of the existing\napproaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where\nthe classifier's hyperparameters are tuned to give best recognition accuracies\nacross a single database, or a small collection of similar databases.\nNevertheless, the results are not significant when they are applied to novel\ndata. This paper proposes a deep neural network architecture to address the FER\nproblem across multiple well-known standard face datasets. Specifically, our\nnetwork consists of two convolutional layers each followed by max pooling and\nthen four Inception layers. The network is a single component architecture that\ntakes registered facial images as the input and classifies them into either of\nthe six basic or the neutral expressions. We conducted comprehensive\nexperiments on seven publically available facial expression databases, viz.\nMultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed\narchitecture are comparable to or better than the state-of-the-art methods and\nbetter than traditional convolutional neural networks and in both accuracy and\ntraining time.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 22:10:46 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Mollahosseini", "Ali", ""], ["Chan", "David", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1511.04143", "submitter": "Matthew Hausknecht", "authors": "Matthew Hausknecht and Peter Stone", "title": "Deep Reinforcement Learning in Parameterized Action Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that deep neural networks are capable of approximating\nboth value functions and policies in reinforcement learning domains featuring\ncontinuous state and action spaces. However, to the best of our knowledge no\nprevious work has succeeded at using deep neural networks in structured\n(parameterized) continuous action spaces. To fill this gap, this paper focuses\non learning within the domain of simulated RoboCup soccer, which features a\nsmall set of discrete action types, each of which is parameterized with\ncontinuous variables. The best learned agent can score goals more reliably than\nthe 2012 RoboCup champion agent. As such, this paper represents a successful\nextension of deep reinforcement learning to the class of parameterized action\nspace MDPs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 02:34:33 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 14:34:20 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 16:44:44 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2016 16:30:34 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Hausknecht", "Matthew", ""], ["Stone", "Peter", ""]]}, {"id": "1511.04306", "submitter": "Sebastian Stober", "authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen and Jessica A. Grahn", "title": "Deep Feature Learning for EEG Recordings", "comments": "submitted as conference paper for ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and compare several strategies for learning discriminative\nfeatures from electroencephalography (EEG) recordings using deep learning\ntechniques. EEG data are generally only available in small quantities, they are\nhigh-dimensional with a poor signal-to-noise ratio, and there is considerable\nvariability between individual subjects and recording sessions. Our proposed\ntechniques specifically address these challenges for feature learning.\nCross-trial encoding forces auto-encoders to focus on features that are stable\nacross trials. Similarity-constraint encoders learn features that allow to\ndistinguish between classes by demanding that two trials from the same class\nare more similar to each other than to trials from other classes. This\ntuple-based training approach is especially suitable for small datasets.\nHydra-nets allow for separate processing pathways adapting to subsets of a\ndataset and thus combine the advantages of individual feature learning (better\nadaptation of early, low-level processing) with group model training (better\ngeneralization of higher-level processing in deeper layers). This way, models\ncan, for instance, adapt to each subject individually to compensate for\ndifferences in spatial patterns due to anatomical differences or variance in\nelectrode positions. The different techniques are evaluated using the publicly\navailable OpenMIIR dataset of EEG recordings taken while participants listened\nto and imagined music.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 15:07:17 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:04:12 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 18:24:08 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 16:26:42 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Stober", "Sebastian", ""], ["Sternin", "Avital", ""], ["Owen", "Adrian M.", ""], ["Grahn", "Jessica A.", ""]]}, {"id": "1511.04348", "submitter": "Linnan Wang", "authors": "Linnan Wang, Wei Wu, Jianxiong Xiao, Yang Yi", "title": "Large Scale Artificial Neural Network Training Using Multi-GPUs", "comments": "SC 15 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for accelerating large scale Artificial Neural\nNetworks (ANN) training using multi-GPUs by reducing the forward and backward\npasses to matrix multiplication. We propose an out-of-core multi-GPU matrix\nmultiplication and integrate the algorithm with the ANN training. The\nexperiments demonstrate that our matrix multiplication algorithm achieves\nlinear speedup on multiple inhomogeneous GPUs. The full paper of this project\ncan be found at [1].\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 16:36:05 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Wang", "Linnan", ""], ["Wu", "Wei", ""], ["Xiao", "Jianxiong", ""], ["Yi", "Yang", ""]]}, {"id": "1511.04387", "submitter": "Daniel Karapetyan Dr", "authors": "Shahriar Asta, Daniel Karapetyan, Ahmed Kheiri, Ender \\\"Ozcan, Andrew\n  J. Parkes", "title": "Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode\n  Resource-constrained Multi-project Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-mode resource and precedence-constrained project scheduling is a\nwell-known challenging real-world optimisation problem. An important variant of\nthe problem requires scheduling of activities for multiple projects considering\navailability of local and global resources while respecting a range of\nconstraints. A critical aspect of the benchmarks addressed in this paper is\nthat the primary objective is to minimise the sum of the project completion\ntimes, with the usual makespan minimisation as a secondary objective. We\nobserve that this leads to an expected different overall structure of good\nsolutions and discuss the effects this has on the algorithm design. This paper\npresents a carefully designed hybrid of Monte-Carlo tree search, novel\nneighbourhood moves, memetic algorithms, and hyper-heuristic methods. The\nimplementation is also engineered to increase the speed with which iterations\nare performed, and to exploit the computing power of multicore machines.\nEmpirical evaluation shows that the resulting information-sharing\nmulti-component algorithm significantly outperforms other solvers on a set of\n\"hidden\" instances, i.e. instances not available at the algorithm design phase.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:17:32 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 16:43:16 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Asta", "Shahriar", ""], ["Karapetyan", "Daniel", ""], ["Kheiri", "Ahmed", ""], ["\u00d6zcan", "Ender", ""], ["Parkes", "Andrew J.", ""]]}, {"id": "1511.04401", "submitter": "Federico Raue", "authors": "Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki", "title": "Symbol Grounding Association in Multimodal Sequences with Missing\n  Elements", "comments": "Under review on Journal of Artificial Intelligence Research (JAIR) --\n  Special Track on Deep Learning, Knowledge Representation, and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend a symbolic association framework for being able to\nhandle missing elements in multimodal sequences. The general scope of the work\nis the symbolic associations of object-word mappings as it happens in language\ndevelopment in infants. In other words, two different representations of the\nsame abstract concepts can associate in both directions. This scenario has been\nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In\nthis work, we extend a recent approach for multimodal sequences (visual and\naudio) to also cope with missing elements in one or both modalities. Our method\nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based\non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We\npropose to include an extra step for the combination with the max operation for\nexploiting the common elements between both sequences. The motivation behind is\nthat the combination acts as a condition selector for choosing the best\nrepresentation from both LSTMs. We evaluated the proposed extension in the\nfollowing scenarios: missing elements in one modality (visual or audio) and\nmissing elements in both modalities (visual and sound). The performance of our\nextension reaches better results than the original model and similar results to\nindividual LSTM trained in each modality.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:59:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:59:02 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 11:36:59 GMT"}, {"version": "v4", "created": "Fri, 16 Dec 2016 14:17:02 GMT"}, {"version": "v5", "created": "Thu, 7 Dec 2017 10:14:23 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Raue", "Federico", ""], ["Dengel", "Andreas", ""], ["Breuel", "Thomas M.", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1511.04484", "submitter": "Emre Neftci", "authors": "Emre O. Neftci, Bruno U. Pedroni, Siddharth Joshi, Maruan Al-Shedivat,\n  Gert Cauwenberghs", "title": "Stochastic Synapses Enable Efficient Brain-Inspired Learning Machines", "comments": null, "journal-ref": "Frontiers in Neuroscience 10 (2016): 241", "doi": "10.3389/fnins.2016.00241", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that synaptic unreliability is a robust and\nsufficient mechanism for inducing the stochasticity observed in cortex. Here,\nwe introduce Synaptic Sampling Machines, a class of neural network models that\nuses synaptic stochasticity as a means to Monte Carlo sampling and unsupervised\nlearning. Similar to the original formulation of Boltzmann machines, these\nmodels can be viewed as a stochastic counterpart of Hopfield networks, but\nwhere stochasticity is induced by a random mask over the connections. Synaptic\nstochasticity plays the dual role of an efficient mechanism for sampling, and a\nregularizer during learning akin to DropConnect. A local synaptic plasticity\nrule implementing an event-driven form of contrastive divergence enables the\nlearning of generative models in an on-line fashion. Synaptic sampling machines\nperform equally well using discrete-timed artificial units (as in Hopfield\nnetworks) or continuous-timed leaky integrate & fire neurons. The learned\nrepresentations are remarkably sparse and robust to reductions in bit precision\nand synapse pruning: removal of more than 75% of the weakest connections\nfollowed by cursory re-learning causes a negligible performance loss on\nbenchmark classification tasks. The spiking neuron-based synaptic sampling\nmachines outperform existing spike-based unsupervised learners, while\npotentially offering substantial advantages in terms of power and complexity,\nand are thus promising models for on-line learning in brain-inspired hardware.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 00:27:37 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 05:58:49 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Neftci", "Emre O.", ""], ["Pedroni", "Bruno U.", ""], ["Joshi", "Siddharth", ""], ["Al-Shedivat", "Maruan", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1511.04508", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel and Xi Wu and Somesh Jha and\n  Ananthram Swami", "title": "Distillation as a Defense to Adversarial Perturbations against Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have been shown to perform extremely well on many\nclassical machine learning problems. However, recent studies have shown that\ndeep learning, like other machine learning techniques, is vulnerable to\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\nprovide adversary-selected outputs. Such attacks can seriously undermine the\nsecurity of the system supported by the DNN, sometimes with devastating\nconsequences. For example, autonomous vehicles can be crashed, illicit or\nillegal content can bypass content filters, or biometric authentication systems\ncan be manipulated to allow improper access. In this work, we introduce a\ndefensive mechanism called defensive distillation to reduce the effectiveness\nof adversarial samples on DNNs. We analytically investigate the\ngeneralizability and robustness properties granted by the use of defensive\ndistillation when training DNNs. We also empirically study the effectiveness of\nour defense mechanisms on two DNNs placed in adversarial settings. The study\nshows that defensive distillation can reduce effectiveness of sample creation\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\nexplained by the fact that distillation leads gradients used in adversarial\nsample creation to be reduced by a factor of 10^30. We also find that\ndistillation increases the average minimum number of features that need to be\nmodified to create adversarial samples by about 800% on one of the DNNs we\ntested.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 04:51:04 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 13:08:09 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""], ["Wu", "Xi", ""], ["Jha", "Somesh", ""], ["Swami", "Ananthram", ""]]}, {"id": "1511.04524", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuting Chen and Venkatesh Saligrama", "title": "Efficient Training of Very Deep Neural Networks for Supervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose training very deep neural networks (DNNs) for\nsupervised learning of hash codes. Existing methods in this context train\nrelatively \"shallow\" networks limited by the issues arising in back propagation\n(e.e. vanishing gradients) as well as computational efficiency. We propose a\nnovel and efficient training algorithm inspired by alternating direction method\nof multipliers (ADMM) that overcomes some of these limitations. Our method\ndecomposes the training process into independent layer-wise local updates\nthrough auxiliary variables. Empirically we observe that our training algorithm\nalways converges and its computational complexity is linearly proportional to\nthe number of edges in the networks. Empirically we manage to train DNNs with\n64 hidden layers and 1024 nodes per layer for supervised hashing in about 3\nhours using a single GPU. Our proposed very deep supervised hashing (VDSH)\nmethod significantly outperforms the state-of-the-art on several benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 07:35:01 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 21:49:21 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Zhang", "Ziming", ""], ["Chen", "Yuting", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1511.04561", "submitter": "Tim Dettmers", "authors": "Tim Dettmers", "title": "8-Bit Approximations for Parallelism in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation of practical deep learning data-products often requires\nparallelization across processors and computers to make deep learning feasible\non large data sets, but bottlenecks in communication bandwidth make it\ndifficult to attain good speedups through parallelism. Here we develop and test\n8-bit approximation algorithms which make better use of the available bandwidth\nby compressing 32-bit gradients and nonlinear activations to 8-bit\napproximations. We show that these approximations do not decrease predictive\nperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelism\nand provide a data transfer speedup of 2x relative to 32-bit parallelism. We\nbuild a predictive model for speedups based on our experimental data, verify\nits validity on known speedup data, and show that we can obtain a speedup of\n50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We\ncompare our data types with other methods and show that 8-bit approximations\nachieve state-of-the-art speedups for model parallelism. Thus 8-bit\napproximation is an efficient method to parallelize convolutional networks on\nvery large systems of GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 14:04:51 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 10:25:58 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 20:32:52 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 16:26:30 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Dettmers", "Tim", ""]]}, {"id": "1511.04664", "submitter": "Mohammad Abu Alsheikh", "authors": "Mohammad Abu Alsheikh, Ahmed Selim, Dusit Niyato, Linda Doyle, Shaowei\n  Lin, Hwee-Pink Tan", "title": "Deep Activity Recognition Models with Triaxial Accelerometers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread installation of accelerometers in almost all mobile\nphones and wearable devices, activity recognition using accelerometers is still\nimmature due to the poor recognition accuracy of existing recognition methods\nand the scarcity of labeled training data. We consider the problem of human\nactivity recognition using triaxial accelerometers and deep learning paradigms.\nThis paper shows that deep activity recognition models (a) provide better\nrecognition accuracy of human activities, (b) avoid the expensive design of\nhandcrafted features in existing systems, and (c) utilize the massive unlabeled\nacceleration samples for unsupervised feature extraction. Moreover, a hybrid\napproach of deep learning and hidden Markov models (DL-HMM) is presented for\nsequential activity recognition. This hybrid approach integrates the\nhierarchical representations of deep activity recognition models with the\nstochastic modeling of temporal sequences in the hidden Markov models. We show\nsubstantial recognition improvement on real world datasets over\nstate-of-the-art methods of human activity recognition using triaxial\naccelerometers.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 06:23:40 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 07:39:29 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Alsheikh", "Mohammad Abu", ""], ["Selim", "Ahmed", ""], ["Niyato", "Dusit", ""], ["Doyle", "Linda", ""], ["Lin", "Shaowei", ""], ["Tan", "Hwee-Pink", ""]]}, {"id": "1511.04855", "submitter": "Marc Chaumont", "authors": "Lionel Pibre, Pasquet J\\'er\\^ome, Dino Ienco, Marc Chaumont", "title": "Deep learning is a good steganalysis tool when embedding key is reused\n  for different images, even if there is a cover source-mismatch", "comments": "IS&T. Media Watermarking, Security, and Forensics, Part of IS&T\n  International Symposium on Electronic Imaging, EI'2016, Feb 2015, San\n  Fransisco, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the BOSS competition, in 2010, most steganalysis approaches use a\nlearning methodology involving two steps: feature extraction, such as the Rich\nModels (RM), for the image representation, and use of the Ensemble Classifier\n(EC) for the learning step. In 2015, Qian et al. have shown that the use of a\ndeep learning approach that jointly learns and computes the features, is very\npromising for the steganalysis. In this paper, we follow-up the study of Qian\net al., and show that, due to intrinsic joint minimization, the results\nobtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural\nNetwork (FNN), if well parameterized, surpass the conventional use of a RM with\nan EC. First, numerous experiments were conducted in order to find the best \"\nshape \" of the CNN. Second, experiments were carried out in the clairvoyant\nscenario in order to compare the CNN and FNN to an RM with an EC. The results\nshow more than 16% reduction in the classification error with our CNN or FNN.\nThird, experiments were also performed in a cover-source mismatch setting. The\nresults show that the CNN and FNN are naturally robust to the mismatch problem.\nIn Addition to the experiments, we provide discussions on the internal\nmechanisms of a CNN, and weave links with some previously stated ideas, in\norder to understand the impressive results we obtained.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 07:59:14 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 07:49:46 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Pibre", "Lionel", ""], ["J\u00e9r\u00f4me", "Pasquet", ""], ["Ienco", "Dino", ""], ["Chaumont", "Marc", ""]]}, {"id": "1511.04868", "submitter": "Navdeep Jaitly", "authors": "Navdeep Jaitly, David Sussillo, Quoc V. Le, Oriol Vinyals, Ilya\n  Sutskever and Samy Bengio", "title": "A Neural Transducer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have achieved impressive results on various\ntasks. However, they are unsuitable for tasks that require incremental\npredictions to be made as more data arrives or tasks that have long input\nsequences and output sequences. This is because they generate an output\nsequence conditioned on an entire input sequence. In this paper, we present a\nNeural Transducer that can make incremental predictions as more input arrives,\nwithout redoing the entire computation. Unlike sequence-to-sequence models, the\nNeural Transducer computes the next-step distribution conditioned on the\npartially observed input sequence and the partially generated sequence. At each\ntime step, the transducer can decide to emit zero to many output symbols. The\ndata can be processed using an encoder and presented as input to the\ntransducer. The discrete decision to emit a symbol at every time step makes it\ndifficult to learn with conventional backpropagation. It is however possible to\ntrain the transducer by using a dynamic programming algorithm to generate\ntarget discrete decisions. Our experiments show that the Neural Transducer\nworks well in settings where it is required to produce output predictions as\ndata come in. We also find that the Neural Transducer performs well for long\nsequences even when attention mechanisms are not used.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 08:53:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 19:56:58 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 19:27:14 GMT"}, {"version": "v4", "created": "Thu, 4 Aug 2016 23:31:46 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Jaitly", "Navdeep", ""], ["Sussillo", "David", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""], ["Sutskever", "Ilya", ""], ["Bengio", "Samy", ""]]}, {"id": "1511.04986", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Aleksandar Matic, Josep Luis Arcos, Alexandros\n  Karatzoglou", "title": "A genetic algorithm to discover flexible motifs with support", "comments": "9 pages, 8 figures, code available at\n  https://github.com/joansj/genmotif", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding repeated patterns or motifs in a time series is an important\nunsupervised task that has still a number of open issues, starting by the\ndefinition of motif. In this paper, we revise the notion of motif support,\ncharacterizing it as the number of patterns or repetitions that define a motif.\nWe then propose GENMOTIF, a genetic algorithm to discover motifs with support\nwhich, at the same time, is flexible enough to accommodate other motif\nspecifications and task characteristics. GENMOTIF is an anytime algorithm that\neasily adapts to many situations: searching in a range of segment lengths,\napplying uniform scaling, dealing with multiple dimensions, using different\nsimilarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: it\nhas only two intuitive parameters which, if set within reasonable bounds, do\nnot substantially affect its performance. We demonstrate the value of our\napproach in a number of synthetic and real-world settings, considering traffic\nvolume measurements, accelerometer signals, and telephone call records.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 15:14:56 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 15:26:45 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 10:12:05 GMT"}, {"version": "v4", "created": "Mon, 5 Dec 2016 13:21:21 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Matic", "Aleksandar", ""], ["Arcos", "Josep Luis", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1511.05042", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson and Pascal Vincent", "title": "An Exploration of Softmax Alternatives Belonging to the Spherical Loss\n  Family", "comments": "Published at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-class classification problem, it is standard to model the output\nof a neural network as a categorical distribution conditioned on the inputs.\nThe output must therefore be positive and sum to one, which is traditionally\nenforced by a softmax. This probabilistic mapping allows to use the maximum\nlikelihood principle, which leads to the well-known log-softmax loss. However\nthe choice of the softmax function seems somehow arbitrary as there are many\nother possible normalizing functions. It is thus unclear why the log-softmax\nloss would perform better than other loss alternatives. In particular Vincent\net al. (2015) recently introduced a class of loss functions, called the\nspherical family, for which there exists an efficient algorithm to compute the\nupdates of the output weights irrespective of the output size. In this paper,\nwe explore several loss functions from this family as possible alternatives to\nthe traditional log-softmax. In particular, we focus our investigation on\nspherical bounds of the log-softmax loss and on two spherical log-likelihood\nlosses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and\nthe log-Taylor Softmax that we introduce. Although these alternatives do not\nyield as good results as the log-softmax loss on two language modeling tasks,\nthey surprisingly outperform it in our experiments on MNIST and CIFAR-10,\nsuggesting that they might be relevant in a broad range of applications.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:15:51 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:36:50 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2016 13:22:44 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1511.05077", "submitter": "Zelda Mariet", "authors": "Zelda Mariet, Suvrit Sra", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point\n  Processes", "comments": "This paper appeared under the shorter title Diversity Networks at\n  ICLR 2016\n  (http://www.iclr.cc/doku.php?id=iclr2016:main#accepted_papers_conference_track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce Divnet, a flexible technique for learning networks with diverse\nneurons. Divnet models neuronal diversity by placing a Determinantal Point\nProcess (DPP) over neurons in a given layer. It uses this DPP to select a\nsubset of diverse neurons and subsequently fuses the redundant neurons into the\nselected ones. Compared with previous approaches, Divnet offers a more\nprincipled, flexible technique for capturing neuronal diversity and thus\nimplicitly enforcing regularization. This enables effective auto-tuning of\nnetwork architecture and leads to smaller network sizes without hurting\nperformance. Moreover, through its focus on diversity and neuron fusing, Divnet\nremains compatible with other procedures that seek to reduce memory footprints\nof networks. We present experimental results to corroborate our claims: for\npruning neural networks, Divnet is seen to be notably superior to competing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 18:28:10 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 02:22:30 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 17:55:06 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 18:14:16 GMT"}, {"version": "v5", "created": "Wed, 3 Feb 2016 16:37:39 GMT"}, {"version": "v6", "created": "Tue, 18 Apr 2017 20:33:53 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Mariet", "Zelda", ""], ["Sra", "Suvrit", ""]]}, {"id": "1511.05122", "submitter": "Sara Sabour", "authors": "Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet", "title": "Adversarial Manipulation of Deep Representations", "comments": "Accepted as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We show that the representation of an image in a deep neural network (DNN)\ncan be manipulated to mimic those of other natural images, with only minor,\nimperceptible perturbations to the original image. Previous methods for\ngenerating adversarial images focused on image perturbations designed to\nproduce erroneous class labels, while we concentrate on the internal layers of\nDNN representations. In this way our new class of adversarial images differs\nqualitatively from others. While the adversary is perceptually similar to one\nimage, its internal representation appears remarkably similar to a different\nimage, one from a different class, bearing little if any apparent similarity to\nthe input; they appear generic and consistent with the space of natural images.\nThis phenomenon raises questions about DNN representations, as well as the\nproperties of natural images themselves.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 20:48:20 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:00:44 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:56:44 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2015 21:03:14 GMT"}, {"version": "v5", "created": "Thu, 7 Jan 2016 20:59:55 GMT"}, {"version": "v6", "created": "Tue, 12 Jan 2016 20:51:51 GMT"}, {"version": "v7", "created": "Wed, 13 Jan 2016 20:57:33 GMT"}, {"version": "v8", "created": "Tue, 1 Mar 2016 20:51:06 GMT"}, {"version": "v9", "created": "Fri, 4 Mar 2016 20:21:24 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Sabour", "Sara", ""], ["Cao", "Yanshuai", ""], ["Faghri", "Fartash", ""], ["Fleet", "David J.", ""]]}, {"id": "1511.05234", "submitter": "Huijuan Xu", "authors": "Huijuan Xu and Kate Saenko", "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\n  Visual Question Answering", "comments": "include test-standard result on VQA full release (V1.0) dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 01:00:04 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 03:06:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Xu", "Huijuan", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05236", "submitter": "Patrick Judd", "authors": "Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor Aamodt,\n  Natalie Enright Jerger, Raquel Urtasun, Andreas Moshovos", "title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets", "comments": "Submitted to ICLR 2016, 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how using reduced precision data in Convolutional\nNeural Networks (CNNs) affects network accuracy during classification. More\nspecifically, this study considers networks where each layer may use different\nprecision data. Our key result is the observation that the tolerance of CNNs to\nreduced precision data not only varies across networks, a well established\nobservation, but also within networks. Tuning precision per layer is appealing\nas it could enable energy and performance improvements. In this paper we study\nhow error tolerance across layers varies and propose a method for finding a low\nprecision configuration for a network while maintaining high accuracy. A\ndiverse set of CNNs is analyzed showing that compared to a conventional\nimplementation using a 32-bit floating-point representation for all layers, and\nwith less than 1% loss in relative accuracy, the data footprint required by\nthese networks can be reduced by an average of 74% and up to 92%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 01:03:03 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 20:38:17 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2015 00:20:48 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 07:22:41 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Judd", "Patrick", ""], ["Albericio", "Jorge", ""], ["Hetherington", "Tayler", ""], ["Aamodt", "Tor", ""], ["Jerger", "Natalie Enright", ""], ["Urtasun", "Raquel", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1511.05298", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena", "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs", "comments": "CVPR 2016 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Recurrent Neural Network architectures, though remarkably capable at\nmodeling sequences, lack an intuitive high-level spatio-temporal structure.\nThat is while many problems in computer vision inherently have an underlying\nhigh-level structure and can benefit from it. Spatio-temporal graphs are a\npopular tool for imposing such high-level intuitions in the formulation of real\nworld problems. In this paper, we propose an approach for combining the power\nof high-level spatio-temporal graphs and sequence learning success of Recurrent\nNeural Networks~(RNNs). We develop a scalable method for casting an arbitrary\nspatio-temporal graph as a rich RNN mixture that is feedforward, fully\ndifferentiable, and jointly trainable. The proposed method is generic and\nprincipled as it can be used for transforming any spatio-temporal graph through\nemploying a certain set of well defined steps. The evaluations of the proposed\napproach on a diverse set of problems, ranging from modeling human motion to\nobject interactions, shows improvement over the state-of-the-art with a large\nmargin. We expect this method to empower new approaches to problem formulation\nthrough high-level spatio-temporal graphs and Recurrent Neural Networks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 07:49:58 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:26:23 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 19:00:24 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Jain", "Ashesh", ""], ["Zamir", "Amir R.", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1511.05432", "submitter": "Uri Shaham", "authors": "Uri Shaham, Yutaro Yamada, and Sahand Negahban", "title": "Understanding Adversarial Training: Increasing Local Stability of Neural\n  Nets through Robust Optimization", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2018.04.027", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for increasing local stability of Artificial\nNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through an\nalternating minimization-maximization procedure, in which the loss of the\nnetwork is minimized over perturbed examples that are generated at each\nparameter update. We show that adversarial training of ANNs is in fact\nrobustification of the network optimization, and that our proposed framework\ngeneralizes previous approaches for increasing local stability of ANNs.\nExperimental results reveal that our approach increases the robustness of the\nnetwork to existing adversarial examples, while making it harder to generate\nnew ones. Furthermore, our algorithm improves the accuracy of the network also\non the original test data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:14:57 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 16:35:50 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 19:05:27 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Shaham", "Uri", ""], ["Yamada", "Yutaro", ""], ["Negahban", "Sahand", ""]]}, {"id": "1511.05493", "submitter": "Yujia Li", "authors": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel", "title": "Gated Graph Sequence Neural Networks", "comments": "Published as a conference paper in ICLR 2016. Fixed a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-structured data appears frequently in domains including chemistry,\nnatural language semantics, social networks, and knowledge bases. In this work,\nwe study feature learning techniques for graph-structured inputs. Our starting\npoint is previous work on Graph Neural Networks (Scarselli et al., 2009), which\nwe modify to use gated recurrent units and modern optimization techniques and\nthen extend to output sequences. The result is a flexible and broadly useful\nclass of neural network models that has favorable inductive biases relative to\npurely sequence-based models (e.g., LSTMs) when the problem is\ngraph-structured. We demonstrate the capabilities on some simple AI (bAbI) and\ngraph algorithm learning tasks. We then show it achieves state-of-the-art\nperformance on a problem from program verification, in which subgraphs need to\nbe matched to abstract data structures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 18:10:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:03:02 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 21:55:01 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 21:36:00 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Yujia", ""], ["Tarlow", "Daniel", ""], ["Brockschmidt", "Marc", ""], ["Zemel", "Richard", ""]]}, {"id": "1511.05497", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas and R. Venkatesh Babu", "title": "Learning Neural Network Architectures using Backpropagation", "comments": "BMVC 2016 ; Title modified from 'Learning the Architecture of Deep\n  Neural Networks'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with millions of parameters are at the heart of many\nstate of the art machine learning models today. However, recent works have\nshown that models with much smaller number of parameters can also perform just\nas well. In this work, we introduce the problem of architecture-learning, i.e;\nlearning the architecture of a neural network along with weights. We introduce\na new trainable parameter called tri-state ReLU, which helps in eliminating\nunnecessary neurons. We also propose a smooth regularizer which encourages the\ntotal number of neurons after elimination to be small. The resulting objective\nis differentiable and simple to optimize. We experimentally validate our method\non both small and large networks, and show that it can learn models with a\nconsiderably small number of parameters without affecting prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 18:26:11 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 11:46:48 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1511.05520", "submitter": "Tian Wang", "authors": "Peter Li and Jiyuan Qian and Tian Wang", "title": "Automatic Instrument Recognition in Polyphonic Music Using Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods to tackle many music information retrieval tasks\ntypically follow a two-step architecture: feature engineering followed by a\nsimple learning algorithm. In these \"shallow\" architectures, feature\nengineering and learning are typically disjoint and unrelated. Additionally,\nfeature engineering is difficult, and typically depends on extensive domain\nexpertise.\n  In this paper, we present an application of convolutional neural networks for\nthe task of automatic musical instrument identification. In this model, feature\nextraction and learning algorithms are trained together in an end-to-end\nfashion. We show that a convolutional neural network trained on raw audio can\nachieve performance surpassing traditional methods that rely on hand-crafted\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 19:43:53 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Li", "Peter", ""], ["Qian", "Jiyuan", ""], ["Wang", "Tian", ""]]}, {"id": "1511.05547", "submitter": "Baochen Sun", "authors": "Baochen Sun, Jiashi Feng, Kate Saenko", "title": "Return of Frustratingly Easy Domain Adaptation", "comments": "Fixed typos. Full paper to appear in AAAI-16. Extended Abstract of\n  the full paper to appear in TASK-CV 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike human learning, machine learning often fails to handle changes between\ntraining (source) and test (target) input distributions. Such domain shifts,\ncommon in practical scenarios, severely damage the performance of conventional\nmachine learning methods. Supervised domain adaptation methods have been\nproposed for the case when the target data have labels, including some that\nperform very well despite being \"frustratingly easy\" to implement. However, in\npractice, the target domain is often unlabeled, requiring unsupervised\nadaptation. We propose a simple, effective, and efficient method for\nunsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL\nminimizes domain shift by aligning the second-order statistics of source and\ntarget distributions, without requiring any target labels. Even though it is\nextraordinarily simple--it can be implemented in four lines of Matlab\ncode--CORAL performs remarkably well in extensive evaluations on standard\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 20:53:26 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 05:39:43 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Sun", "Baochen", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05552", "submitter": "Andre Xian Ming Chang", "authors": "Andre Xian Ming Chang, Berin Martini and Eugenio Culurciello", "title": "Recurrent Neural Networks Hardware Implementation on FPGA", "comments": "7 pages, 8 figures, changed format, added figures, added references,\n  modified introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural Networks (RNNs) have the ability to retain memory and learn\ndata sequences. Due to the recurrent nature of RNNs, it is sometimes hard to\nparallelize all its computations on conventional hardware. CPUs do not\ncurrently offer large parallelism, while GPUs offer limited parallelism due to\nsequential components of RNN models. In this paper we present a hardware\nimplementation of Long-Short Term Memory (LSTM) recurrent network on the\nprogrammable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$\nlayers and $128$ hidden units in hardware and it has been tested using a\ncharacter level language model. The implementation is more than $21\\times$\nfaster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can\npotentially evolve to a RNN co-processor for future mobile devices.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 02:20:37 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 13:35:36 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 21:22:24 GMT"}, {"version": "v4", "created": "Fri, 4 Mar 2016 17:49:50 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Chang", "Andre Xian Ming", ""], ["Martini", "Berin", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1511.05607", "submitter": "Min Li", "authors": "Min Li, Sudeep Gaddam, Xiaolin Li, Yinan Zhao, Jingzhe Ma, Jian Ge", "title": "Identifying the Absorption Bump with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive interstellar dust grains provide significant insights to\nunderstand the formation and evolution of the stars, planetary systems, and the\ngalaxies, and may harbor the building blocks of life. One of the most effective\nway to analyze the dust is via their interaction with the light from background\nsources. The observed extinction curves and spectral features carry the size\nand composition information of dust. The broad absorption bump at 2175 Angstrom\nis the most prominent feature in the extinction curves. Traditionally,\nstatistical methods are applied to detect the existence of the absorption bump.\nThese methods require heavy preprocessing and the co-existence of other\nreference features to alleviate the influence from the noises. In this paper,\nwe apply Deep Learning techniques to detect the broad absorption bump. We\ndemonstrate the key steps for training the selected models and their results.\nThe success of Deep Learning based method inspires us to generalize a common\nmethodology for broader science discovery problems. We present our on-going\nwork to build the DeepDis system for such kind of applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 22:27:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 14:20:46 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Li", "Min", ""], ["Gaddam", "Sudeep", ""], ["Li", "Xiaolin", ""], ["Zhao", "Yinan", ""], ["Ma", "Jingzhe", ""], ["Ge", "Jian", ""]]}, {"id": "1511.05625", "submitter": "Roberto Santana", "authors": "Murilo Zangari de Souza, Roberto Santana, Aurora Trinidad Ramirez\n  Pozo, Alexander Mendiburu", "title": "MOEA/D-GM: Using probabilistic graphical models in MOEA/D for solving\n  combinatorial optimization problems", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms based on modeling the statistical dependencies\n(interactions) between the variables have been proposed to solve a wide range\nof complex problems. These algorithms learn and sample probabilistic graphical\nmodels able to encode and exploit the regularities of the problem. This paper\ninvestigates the effect of using probabilistic modeling techniques as a way to\nenhance the behavior of MOEA/D framework. MOEA/D is a decomposition based\nevolutionary algorithm that decomposes a multi-objective optimization problem\n(MOP) in a number of scalar single-objective subproblems and optimizes them in\na collaborative manner. MOEA/D framework has been widely used to solve several\nMOPs. The proposed algorithm, MOEA/D using probabilistic Graphical Models\n(MOEA/D-GM) is able to instantiate both univariate and multi-variate\nprobabilistic models for each subproblem. To validate the introduced framework\nalgorithm, an experimental study is conducted on a multi-objective version of\nthe deceptive function Trap5. The results show that the variant of the\nframework (MOEA/D-Tree), where tree models are learned from the matrices of the\nmutual information between the variables, is able to capture the structure of\nthe problem. MOEA/D-Tree is able to achieve significantly better results than\nboth MOEA/D using genetic operators and MOEA/D using univariate probability\nmodels, in terms of the approximation to the true Pareto front.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 00:04:35 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["de Souza", "Murilo Zangari", ""], ["Santana", "Roberto", ""], ["Pozo", "Aurora Trinidad Ramirez", ""], ["Mendiburu", "Alexander", ""]]}, {"id": "1511.05635", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Gustavo Carneiro", "title": "Competitive Multi-scale Convolution", "comments": null, "journal-ref": "Pattern Recognition 71 (2017), 94-105", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new deep convolutional neural network (ConvNet)\nmodule that promotes competition among a set of multi-scale convolutional\nfilters. This new module is inspired by the inception module, where we replace\nthe original collaborative pooling stage (consisting of a concatenation of the\nmulti-scale filter outputs) by a competitive pooling represented by a maxout\nactivation unit. This extension has the following two objectives: 1) the\nselection of the maximum response among the multi-scale filters prevents filter\nco-adaptation and allows the formation of multiple sub-networks within the same\nmodel, which has been shown to facilitate the training of complex learning\nproblems; and 2) the maxout unit reduces the dimensionality of the outputs from\nthe multi-scale filters. We show that the use of our proposed module in typical\ndeep ConvNets produces classification results that are either better than or\ncomparable to the state of the art on the following benchmark datasets: MNIST,\nCIFAR-10, CIFAR-100 and SVHN.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 01:19:00 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Liao", "Zhibin", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1511.05946", "submitter": "Marcin Moczulski", "authors": "Marcin Moczulski, Misha Denil, Jeremy Appleyard, Nando de Freitas", "title": "ACDC: A Structured Efficient Linear Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear layer is one of the most pervasive modules in deep learning\nrepresentations. However, it requires $O(N^2)$ parameters and $O(N^2)$\noperations. These costs can be prohibitive in mobile applications or prevent\nscaling in many domains. Here, we introduce a deep, differentiable,\nfully-connected neural network module composed of diagonal matrices of\nparameters, $\\mathbf{A}$ and $\\mathbf{D}$, and the discrete cosine transform\n$\\mathbf{C}$. The core module, structured as $\\mathbf{ACDC^{-1}}$, has $O(N)$\nparameters and incurs $O(N log N )$ operations. We present theoretical results\nshowing how deep cascades of ACDC layers approximate linear layers. ACDC is,\nhowever, a stand-alone module and can be used in combination with any other\ntypes of module. In our experiments, we show that it can indeed be successfully\ninterleaved with ReLU modules in convolutional neural networks for image\nrecognition. Our experiments also study critical factors in the training of\nthese structured modules, including initialization and depth. Finally, this\npaper also provides a connection between structured linear transforms used in\ndeep learning and the field of Fourier optics, illustrating how ACDC could in\nprinciple be implemented with lenses and diffractive elements.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:52:17 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 01:37:57 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2016 02:27:52 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 03:37:23 GMT"}, {"version": "v5", "created": "Sat, 19 Mar 2016 23:31:15 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["Appleyard", "Jeremy", ""], ["de Freitas", "Nando", ""]]}, {"id": "1511.06051", "submitter": "Robert Nishihara", "authors": "Philipp Moritz, Robert Nishihara, Ion Stoica, Michael I. Jordan", "title": "SparkNet: Training Deep Networks in Spark", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks is a time-consuming process, with networks for object\nrecognition often requiring multiple days to train. For this reason, leveraging\nthe resources of a cluster to speed up training is an important area of work.\nHowever, widely-popular batch-processing computational frameworks like\nMapReduce and Spark were not designed to support the asynchronous and\ncommunication-intensive workloads of existing distributed deep learning\nsystems. We introduce SparkNet, a framework for training deep networks in\nSpark. Our implementation includes a convenient interface for reading data from\nSpark RDDs, a Scala interface to the Caffe deep learning framework, and a\nlightweight multi-dimensional tensor library. Using a simple parallelization\nscheme for stochastic gradient descent, SparkNet scales well with the cluster\nsize and tolerates very high-latency communication. Furthermore, it is easy to\ndeploy and use with no parameter tuning, and it is compatible with existing\nCaffe models. We quantify the dependence of the speedup obtained by SparkNet on\nthe number of machines, the communication frequency, and the cluster's\ncommunication overhead, and we benchmark our system's performance on the\nImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 03:29:56 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 10:35:40 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 07:48:06 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2016 23:43:36 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Stoica", "Ion", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1511.06072", "submitter": "Sebastian Agethen", "authors": "Sebastian Agethen, Winston H. Hsu", "title": "Mediated Experts for Deep Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new supervised architecture termed Mediated Mixture-of-Experts\n(MMoE) that allows us to improve classification accuracy of Deep Convolutional\nNetworks (DCN). Our architecture achieves this with the help of expert\nnetworks: A network is trained on a disjoint subset of a given dataset and then\nrun in parallel to other experts during deployment. A mediator is employed if\nexperts contradict each other. This allows our framework to naturally support\nincremental learning, as adding new classes requires (re-)training of the new\nexpert only. We also propose two measures to control computational complexity:\nAn early-stopping mechanism halts experts that have low confidence in their\nprediction. The system allows to trade-off accuracy and complexity without\nfurther retraining. We also suggest to share low-level convolutional layers\nbetween experts in an effort to avoid computation of a near-duplicate feature\nset. We evaluate our system on a popular dataset and report improved accuracy\ncompared to a single model of same configuration.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 07:01:36 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Agethen", "Sebastian", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1511.06085", "submitter": "George Toderici", "authors": "George Toderici, Sean M. O'Malley, Sung Jin Hwang, Damien Vincent,\n  David Minnen, Shumeet Baluja, Michele Covell, Rahul Sukthankar", "title": "Variable Rate Image Compression with Recurrent Neural Networks", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of Internet traffic is now driven by requests from mobile\ndevices with relatively small screens and often stringent bandwidth\nrequirements. Due to these factors, it has become the norm for modern\ngraphics-heavy websites to transmit low-resolution, low-bytecount image\npreviews (thumbnails) as part of the initial page load process to improve\napparent page responsiveness. Increasing thumbnail compression beyond the\ncapabilities of existing codecs is therefore a current research focus, as any\nbyte savings will significantly enhance the experience of mobile device users.\nToward this end, we propose a general framework for variable-rate image\ncompression and a novel architecture based on convolutional and deconvolutional\nLSTM recurrent networks. Our models address the main issues that have prevented\nautoencoder neural networks from competing with existing image compression\nalgorithms: (1) our networks only need to be trained once (not per-image),\nregardless of input image dimensions and the desired compression rate; (2) our\nnetworks are progressive, meaning that the more bits are sent, the more\naccurate the image reconstruction; and (3) the proposed architecture is at\nleast as efficient as a standard purpose-trained autoencoder for a given number\nof bits. On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based\napproaches provide better visual quality than (headerless) JPEG, JPEG2000 and\nWebP, with a storage size that is reduced by 10% or more.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 07:50:46 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 01:44:51 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 02:43:40 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2016 20:57:42 GMT"}, {"version": "v5", "created": "Tue, 1 Mar 2016 22:13:44 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Toderici", "George", ""], ["O'Malley", "Sean M.", ""], ["Hwang", "Sung Jin", ""], ["Vincent", "Damien", ""], ["Minnen", "David", ""], ["Baluja", "Shumeet", ""], ["Covell", "Michele", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1511.06104", "submitter": "Sheng-Yi Bai", "authors": "Sheng-Yi Bai, Sebastian Agethen, Ting-Hsuan Chao, Winston Hsu", "title": "Semi-supervised Learning for Convolutional Neural Networks via Online\n  Graph Construction", "comments": "As the original submission of iclr is withdrawn, the arxiv submission\n  should be withdrawn as well", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent promising achievements of deep learning rely on the large amount\nof labeled data. Considering the abundance of data on the web, most of them do\nnot have labels at all. Therefore, it is important to improve generalization\nperformance using unlabeled data on supervised tasks with few labeled\ninstances. In this work, we revisit graph-based semi-supervised learning\nalgorithms and propose an online graph construction technique which suits deep\nconvolutional neural network better. We consider an EM-like algorithm for\nsemi-supervised learning on deep neural networks: In forward pass, the graph is\nconstructed based on the network output, and the graph is then used for loss\ncalculation to help update the network by back propagation in the backward\npass. We demonstrate the strength of our online approach compared to the\nconventional ones whose graph is constructed on static but not robust enough\nfeature representations beforehand.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 09:44:57 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 00:56:08 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bai", "Sheng-Yi", ""], ["Agethen", "Sebastian", ""], ["Chao", "Ting-Hsuan", ""], ["Hsu", "Winston", ""]]}, {"id": "1511.06248", "submitter": "Adam Erskine", "authors": "J. Michael Herrmann, Adam Erskine, Thomas Joyce", "title": "Critical Parameters in Particle Swarm Optimisation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle swarm optimisation is a metaheuristic algorithm which finds\nreasonable solutions in a wide range of applied problems if suitable parameters\nare used. We study the properties of the algorithm in the framework of random\ndynamical systems which, due to the quasi-linear swarm dynamics, yields\nanalytical results for the stability properties of the particles. Such\nconsiderations predict a relationship between the parameters of the algorithm\nthat marks the edge between convergent and divergent behaviours. Comparison\nwith simulations indicates that the algorithm performs best near this margin of\ninstability.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:47:01 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Herrmann", "J. Michael", ""], ["Erskine", "Adam", ""], ["Joyce", "Thomas", ""]]}, {"id": "1511.06279", "submitter": "Scott Reed", "authors": "Scott Reed and Nando de Freitas", "title": "Neural Programmer-Interpreters", "comments": "ICLR 2016 conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the neural programmer-interpreter (NPI): a recurrent and\ncompositional neural network that learns to represent and execute programs. NPI\nhas three learnable components: a task-agnostic recurrent core, a persistent\nkey-value program memory, and domain-specific encoders that enable a single NPI\nto operate in multiple perceptually diverse environments with distinct\naffordances. By learning to compose lower-level programs to express\nhigher-level programs, NPI reduces sample complexity and increases\ngeneralization ability compared to sequence-to-sequence LSTMs. The program\nmemory allows efficient learning of additional tasks by building on existing\nprograms. NPI can also harness the environment (e.g. a scratch pad with\nread-write pointers) to cache intermediate results of computation, lessening\nthe long-term memory burden on recurrent hidden units. In this work we train\nthe NPI with fully-supervised execution traces; each program has example\nsequences of calls to the immediate subprograms conditioned on the input.\nRather than training on a huge number of relatively weak labels, NPI learns\nfrom a small number of rich examples. We demonstrate the capability of our\nmodel to learn several types of compositional programs: addition, sorting, and\ncanonicalizing 3D models. Furthermore, a single NPI learns to execute these\nprograms and all 21 associated subprograms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:49:32 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 19:30:01 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 18:11:35 GMT"}, {"version": "v4", "created": "Mon, 29 Feb 2016 11:12:36 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Reed", "Scott", ""], ["de Freitas", "Nando", ""]]}, {"id": "1511.06314", "submitter": "Stefan Lee", "authors": "Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall,\n  and Dhruv Batra", "title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved state-of-the-art performance on a\nwide range of tasks. Most benchmarks are led by ensembles of these powerful\nlearners, but ensembling is typically treated as a post-hoc procedure\nimplemented by averaging independently trained models with model variation\ninduced by bagging or random initialization. In this paper, we rigorously treat\nensembling as a first-class problem to explicitly address the question: what\nare the best strategies to create an ensemble? We first compare a large number\nof ensembling strategies, and then propose and evaluate novel strategies, such\nas parameter sharing (through a new family of models we call TreeNets) as well\nas training under ensemble-aware and diversity-encouraging losses. We\ndemonstrate that TreeNets can improve ensemble performance and that diverse\nensembles can be trained end-to-end under a unified loss, achieving\nsignificantly higher \"oracle\" accuracies than classical ensembles.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:19:58 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Lee", "Stefan", ""], ["Purushwalkam", "Senthil", ""], ["Cogswell", "Michael", ""], ["Crandall", "David", ""], ["Batra", "Dhruv", ""]]}, {"id": "1511.06343", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov and Frank Hutter", "title": "Online Batch Selection for Faster Training of Neural Networks", "comments": "Workshop paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are commonly trained using stochastic non-convex\noptimization procedures, which are driven by gradient information estimated on\nfractions (batches) of the dataset. While it is commonly accepted that batch\nsize is an important parameter for offline tuning, the benefits of online\nselection of batches remain poorly understood. We investigate online batch\nselection strategies for two state-of-the-art methods of stochastic\ngradient-based optimization, AdaDelta and Adam. As the loss function to be\nminimized for the whole dataset is an aggregation of loss functions of\nindividual datapoints, intuitively, datapoints with the greatest loss should be\nconsidered (selected in a batch) more frequently. However, the limitations of\nthis intuition and the proper control of the selection pressure over time are\nopen questions. We propose a simple strategy where all datapoints are ranked\nw.r.t. their latest known loss value and the probability to be selected decays\nexponentially as a function of rank. Our experimental results on the MNIST\ndataset suggest that selecting batches speeds up both AdaDelta and Adam by a\nfactor of about 5.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:24:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 22:15:38 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2016 13:06:15 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2016 14:00:21 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1511.06348", "submitter": "Synho Do", "authors": "Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, Synho Do", "title": "How much data is needed to train a medical image deep learning system to\n  achieve necessary high accuracy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Convolutional Neural Networks (CNN) in natural image\nclassification systems has produced very impressive results. Combined with the\ninherent nature of medical images that make them ideal for deep-learning,\nfurther application of such systems to medical image classification holds much\npromise. However, the usefulness and potential impact of such a system can be\ncompletely negated if it does not reach a target accuracy. In this paper, we\npresent a study on determining the optimum size of the training data set\nnecessary to achieve high classification accuracy with low variance in medical\nimage classification systems. The CNN was applied to classify axial Computed\nTomography (CT) images into six anatomical classes. We trained the CNN using\nsix different sizes of training data set (5, 10, 20, 50, 100, and 200) and then\ntested the resulting system with a total of 6000 CT images. All images were\nacquired from the Massachusetts General Hospital (MGH) Picture Archiving and\nCommunication System (PACS). Using this data, we employ the learning curve\napproach to predict classification accuracy at a given training sample size.\nOur research will present a general methodology for determining the training\ndata set size necessary to achieve a certain target classification accuracy\nthat can be easily applied to other problems within such systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:38:43 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:08:10 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Cho", "Junghwan", ""], ["Lee", "Kyewook", ""], ["Shin", "Ellie", ""], ["Choy", "Garry", ""], ["Do", "Synho", ""]]}, {"id": "1511.06351", "submitter": "Andy Sarroff", "authors": "Andy M. Sarroff, Victor Shepardson, Michael A. Casey", "title": "Learning Representations Using Complex-Valued Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued neural networks (CVNNs) are an emerging field of research in\nneural networks due to their potential representational properties for audio,\nimage, and physiological signals. It is common in signal processing to\ntransform sequences of real values to the complex domain via a set of complex\nbasis functions, such as the Fourier transform. We show how CVNNs can be used\nto learn complex representations of real valued time-series data. We present\nmethods and results using a framework that can compose holomorphic and\nnon-holomorphic functions in a multi-layer network using a theoretical result\ncalled the Wirtinger derivative. We test our methods on a representation\nlearning task for real-valued signals, recurrent complex-valued networks and\ntheir real-valued counterparts. Our results show that recurrent complex-valued\nnetworks can perform as well as their real-valued counterparts while learning\nfilters that are representative of the domain of the data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:44:10 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Sarroff", "Andy M.", ""], ["Shepardson", "Victor", ""], ["Casey", "Michael A.", ""]]}, {"id": "1511.06392", "submitter": "Karol Kurach", "authors": "Karol Kurach, Marcin Andrychowicz, Ilya Sutskever", "title": "Neural Random-Access Machines", "comments": "ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and\n  appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and investigate a new neural network architecture\ncalled Neural Random Access Machine. It can manipulate and dereference pointers\nto an external variable-size random-access memory. The model is trained from\npure input-output examples using backpropagation.\n  We evaluate the new model on a number of simple algorithmic tasks whose\nsolutions require pointer manipulation and dereferencing. Our results show that\nthe proposed model can learn to solve algorithmic tasks of such type and is\ncapable of operating on simple data structures like linked-lists and binary\ntrees. For easier tasks, the learned solutions generalize to sequences of\narbitrary length. Moreover, memory access during inference can be done in a\nconstant time under some assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:36:28 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 10:27:06 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 21:29:07 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Kurach", "Karol", ""], ["Andrychowicz", "Marcin", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1511.06418", "submitter": "Klaus Greff", "authors": "Klaus Greff, Rupesh Kumar Srivastava, J\\\"urgen Schmidhuber", "title": "Binding via Reconstruction Clustering", "comments": "12 pages, plus 12 pages Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disentangled distributed representations of data are desirable for machine\nlearning, since they are more expressive and can generalize from fewer\nexamples. However, for complex data, the distributed representations of\nmultiple objects present in the same input can interfere and lead to\nambiguities, which is commonly referred to as the binding problem. We argue for\nthe importance of the binding problem to the field of representation learning,\nand develop a probabilistic framework that explicitly models inputs as a\ncomposition of multiple objects. We propose an unsupervised algorithm that uses\ndenoising autoencoders to dynamically bind features together in multi-object\ninputs through an Expectation-Maximization-like clustering process. The\neffectiveness of this method is demonstrated on artificially generated datasets\nof binary images, showing that it can even generalize to bind together new\nobjects never seen by the autoencoder during training.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:13:11 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 23:35:10 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 20:48:53 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 19:31:17 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Greff", "Klaus", ""], ["Srivastava", "Rupesh Kumar", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1511.06420", "submitter": "Ethan Caballero V", "authors": "Ethan Caballero", "title": "Skip-Thought Memory Networks", "comments": "Removed by arXiv administrators because submission violated the terms\n  of arXiv's license agreement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) is fundamental to natural language processing in that\nmost nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly\nsupervised memory network models that have been proposed so far struggle at\nanswering questions that involve relations among multiple entities (such as\nfacebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address\nthis problem of learning multi-argument multi-hop semantic relations for the\npurpose of QA, we propose a method that combines the jointly learned long-term\nread-write memory and attentive inference components of end-to-end memory\nnetworks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector\nrepresentations encoded by a Skip-Thought model (Kiros et al., 2015). This\nchoice to append Skip-Thought Vectors to the existing MemN2N framework is\nmotivated by the fact that Skip-Thought Vectors have been shown to accurately\nmodel multi-argument semantic relations (Kiros et al., 2015).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:15:46 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 02:30:16 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Caballero", "Ethan", ""]]}, {"id": "1511.06432", "submitter": "Nicolas Ballas", "authors": "Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville", "title": "Delving Deeper into Convolutional Networks for Learning Video\n  Representations", "comments": "ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to learn spatio-temporal features in videos from\nintermediate visual representations we call \"percepts\" using\nGated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts\nthat are extracted from all level of a deep convolutional network trained on\nthe large ImageNet dataset. While high-level percepts contain highly\ndiscriminative information, they tend to have a low-spatial resolution.\nLow-level percepts, on the other hand, preserve a higher spatial resolution\nfrom which we can model finer motion patterns. Using low-level percepts can\nleads to high-dimensionality video representations. To mitigate this effect and\ncontrol the model number of parameters, we introduce a variant of the GRU model\nthat leverages the convolution operations to enforce sparse connectivity of the\nmodel units and share parameters across the input spatial locations.\n  We empirically validate our approach on both Human Action Recognition and\nVideo Captioning tasks. In particular, we achieve results equivalent to\nstate-of-art on the YouTube2Text dataset using a simpler text-decoder model and\nwithout extra 3D CNN features.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:46:13 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 02:46:54 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:43:19 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 18:54:11 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Ballas", "Nicolas", ""], ["Yao", "Li", ""], ["Pal", "Chris", ""], ["Courville", "Aaron", ""]]}, {"id": "1511.06464", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky, Amar Shah, Yoshua Bengio", "title": "Unitary Evolution Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are notoriously difficult to train. When the\neigenvalues of the hidden to hidden weight matrix deviate from absolute value\n1, optimization becomes difficult due to the well studied issue of vanishing\nand exploding gradients, especially when trying to learn long-term\ndependencies. To circumvent this problem, we propose a new architecture that\nlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.\nThe challenge we address is that of parametrizing unitary matrices in a way\nthat does not require expensive computations (such as eigendecomposition) after\neach weight update. We construct an expressive unitary weight matrix by\ncomposing several structured matrices that act as building blocks with\nparameters to be learned. Optimization with this parameterization becomes\nfeasible only when considering hidden states in the complex domain. We\ndemonstrate the potential of this architecture by achieving state of the art\nresults in several hard tasks involving very long-term dependencies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:37:33 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 18:42:08 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 00:52:28 GMT"}, {"version": "v4", "created": "Wed, 25 May 2016 23:34:38 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Arjovsky", "Martin", ""], ["Shah", "Amar", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1511.06488", "submitter": "Sungho Shin", "authors": "Wonyong Sung, Sungho Shin, Kyuyeon Hwang", "title": "Resiliency of Deep Neural Networks under Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of deep neural network algorithms for hardware implementation\ncan be much lowered by optimizing the word-length of weights and signals.\nDirect quantization of floating-point weights, however, does not show good\nperformance when the number of bits assigned is small. Retraining of quantized\nnetworks has been developed to relieve this problem. In this work, the effects\nof retraining are analyzed for a feedforward deep neural network (FFDNN) and a\nconvolutional neural network (CNN). The network complexity is controlled to\nknow their effects on the resiliency of quantized networks by retraining. The\ncomplexity of the FFDNN is controlled by varying the unit size in each hidden\nlayer and the number of layers, while that of the CNN is done by modifying the\nfeature map configuration. We find that the performance gap between the\nfloating-point and the retrain-based ternary (+1, 0, -1) weight neural networks\nexists with a fair amount in 'complexity limited' networks, but the discrepancy\nalmost vanishes in fully complex networks whose capability is limited by the\ntraining data, rather than by the number of connections. This research shows\nthat highly complex DNNs have the capability of absorbing the effects of severe\nweight quantization through retraining, but connection limited networks are\nless resilient. This paper also presents the effective compression ratio to\nguide the trade-off between the network size and the precision when the\nhardware resource is limited.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 04:55:46 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 12:59:38 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 13:50:22 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Sung", "Wonyong", ""], ["Shin", "Sungho", ""], ["Hwang", "Kyuyeon", ""]]}, {"id": "1511.06499", "submitter": "Dustin Tran", "authors": "Dustin Tran, Rajesh Ranganath, David M. Blei", "title": "The Variational Gaussian Process", "comments": "Appears in International Conference on Learning Representations, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful tool for approximate inference, and it\nhas been recently applied for representation learning with deep generative\nmodels. We develop the variational Gaussian process (VGP), a Bayesian\nnonparametric variational family, which adapts its shape to match complex\nposterior distributions. The VGP generates approximate posterior samples by\ngenerating latent inputs and warping them through random non-linear mappings;\nthe distribution over random mappings is learned during inference, enabling the\ntransformed outputs to adapt to varying complexity. We prove a universal\napproximation theorem for the VGP, demonstrating its representative power for\nlearning any model. For inference we present a variational objective inspired\nby auto-encoders and perform black box inference over a wide class of models.\nThe VGP achieves new state-of-the-art results for unsupervised learning,\ninferring models such as the deep latent Gaussian model and the recently\nproposed DRAW.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 06:01:23 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:56:01 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 23:11:38 GMT"}, {"version": "v4", "created": "Sun, 17 Apr 2016 22:14:13 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1511.06586", "submitter": "Chee Seng Chan", "authors": "Ven Jyn Kok, Mei Kuan Lim, Chee Seng Chan", "title": "Crowd Behavior Analysis: A Review where Physics meets Biology", "comments": "Accepted in Neurocomputing, 31 pages, 180 references", "journal-ref": "Neurocomputing 177 (2016) 342-362", "doi": "10.1016/j.neucom.2015.11.021", "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the traits emerged in a mass gathering are often non-deliberative,\nthe act of mass impulse may lead to irre- vocable crowd disasters. The two-fold\nincrease of carnage in crowd since the past two decades has spurred significant\nadvances in the field of computer vision, towards effective and proactive crowd\nsurveillance. Computer vision stud- ies related to crowd are observed to\nresonate with the understanding of the emergent behavior in physics (complex\nsystems) and biology (animal swarm). These studies, which are inspired by\nbiology and physics, share surprisingly common insights, and interesting\ncontradictions. However, this aspect of discussion has not been fully explored.\nTherefore, this survey provides the readers with a review of the\nstate-of-the-art methods in crowd behavior analysis from the physics and\nbiologically inspired perspectives. We provide insights and comprehensive\ndiscussions for a broader understanding of the underlying prospect of blending\nphysics and biology studies in computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 13:19:44 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kok", "Ven Jyn", ""], ["Lim", "Mei Kuan", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1511.06603", "submitter": "Ghazal Zand", "authors": "Ghazal Zand, Mojtaba Taherkhani, Reza Safabakhsh", "title": "Exponential Natural Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Filter algorithm (PF) suffers from some problems such as the loss of\nparticle diversity, the need for large number of particles, and the costly\nselection of the importance density functions. In this paper, a novel\nExponential Natural Particle Filter (xNPF) is introduced to solve the above\nproblems. In this approach, a state transitional probability with the use of\nnatural gradient learning is proposed which balances exploration and\nexploitation more robustly. The results show that xNPF converges much closer to\nthe true target states than the other state of the art particle filter.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 14:08:33 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zand", "Ghazal", ""], ["Taherkhani", "Mojtaba", ""], ["Safabakhsh", "Reza", ""]]}, {"id": "1511.06744", "submitter": "Yani Ioannou", "authors": "Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla,\n  Antonio Criminisi", "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification", "comments": "Published as a conference paper at ICLR 2016. v3: updated ICLR\n  status. v2: Incorporated reviewer's feedback including: Amend Fig. 2 and 5\n  descriptions to explain that there are no ReLUs within the figures. Fix\n  headings of Table 5 - Fix typo in the sentence at bottom of page 6. Add ref.\n  to Predicting Parameters in Deep Learning. Fix Table 6, GMP-LR and GMP-LR-2x\n  had incorrect numbers of filters", "journal-ref": "International Conference on Learning Representations (ICLR), San\n  Juan, Puerto Rico, 2-4 May 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 20:14:28 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 17:07:02 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2016 21:23:19 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Ioannou", "Yani", ""], ["Robertson", "Duncan", ""], ["Shotton", "Jamie", ""], ["Cipolla", "Roberto", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1511.06827", "submitter": "Diogo Almeida", "authors": "Diogo Almeida, Nate Sauder", "title": "GradNets: Dynamic Interpolation Between Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, there is a fundamental trade-off between ease of\noptimization and expressive power. Neural Networks, in particular, have\nenormous expressive power and yet are notoriously challenging to train. The\nnature of that optimization challenge changes over the course of learning.\nTraditionally in deep learning, one makes a static trade-off between the needs\nof early and late optimization. In this paper, we investigate a novel\nframework, GradNets, for dynamically adapting architectures during training to\nget the benefits of both. For example, we can gradually transition from linear\nto non-linear networks, deterministic to stochastic computation, shallow to\ndeep architectures, or even simple downsampling to fully differentiable\nattention mechanisms. Benefits include increased accuracy, easier convergence\nwith more complex architectures, solutions to test-time execution of batch\nnormalization, and the ability to train networks of up to 200 layers.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 03:50:49 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Almeida", "Diogo", ""], ["Sauder", "Nate", ""]]}, {"id": "1511.06841", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Wonyong Sung", "title": "Online Sequence Training of Recurrent Neural Networks with Connectionist\n  Temporal Classification", "comments": "Final version: Kyuyeon Hwang and Wonyong Sung, \"Sequence to Sequence\n  Training of CTC-RNNs with Partial Windowing,\" Proceedings of The 33rd\n  International Conference on Machine Learning, pp. 2178-2187, 2016. URL:\n  http://www.jmlr.org/proceedings/papers/v48/hwanga16.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectionist temporal classification (CTC) based supervised sequence\ntraining of recurrent neural networks (RNNs) has shown great success in many\nmachine learning areas including end-to-end speech and handwritten character\nrecognition. For the CTC training, however, it is required to unroll (or\nunfold) the RNN by the length of an input sequence. This unrolling requires a\nlot of memory and hinders a small footprint implementation of online learning\nor adaptation. Furthermore, the length of training sequences is usually not\nuniform, which makes parallel training with multiple sequences inefficient on\nshared memory models such as graphics processing units (GPUs). In this work, we\nintroduce an expectation-maximization (EM) based online CTC algorithm that\nenables unidirectional RNNs to learn sequences that are longer than the amount\nof unrolling. The RNNs can also be trained to process an infinitely long input\nsequence without pre-segmentation or external reset. Moreover, the proposed\napproach allows efficient parallel training on GPUs. For evaluation, phoneme\nrecognition and end-to-end speech recognition examples are presented on the\nTIMIT and Wall Street Journal (WSJ) corpora, respectively. Our online model\nachieves 20.7% phoneme error rate (PER) on the very long input sequence that is\ngenerated by concatenating all 192 utterances in the TIMIT core test set. On\nWSJ, a network can be trained with only 64 times of unrolling while sacrificing\n4.5% relative word error rate (WER).\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 05:22:37 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 19:10:36 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2015 12:09:14 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 20:52:42 GMT"}, {"version": "v5", "created": "Thu, 2 Feb 2017 13:42:49 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1511.06909", "submitter": "Shihao Ji", "authors": "Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson\n  and Pradeep Dubey", "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very\n  Large Vocabularies", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BlackOut, an approximation algorithm to efficiently train massive\nrecurrent neural network language models (RNNLMs) with million word\nvocabularies. BlackOut is motivated by using a discriminative loss, and we\ndescribe a new sampling strategy which significantly reduces computation while\nimproving stability, sample efficiency, and rate of convergence. One way to\nunderstand BlackOut is to view it as an extension of the DropOut strategy to\nthe output layer, wherein we use a discriminative training loss and a weighted\nsampling scheme. We also establish close connections between BlackOut,\nimportance sampling, and noise contrastive estimation (NCE). Our experiments,\non the recently released one billion word language modeling benchmark,\ndemonstrate scalability and accuracy of BlackOut; we outperform the\nstate-of-the art, and achieve the lowest perplexity scores on this dataset.\nMoreover, unlike other established methods which typically require GPUs or CPU\nclusters, we show that a carefully implemented version of BlackOut requires\nonly 1-10 days on a single machine to train a RNNLM with a million word\nvocabulary and billions of parameters on one billion words. Although we\ndescribe BlackOut in the context of RNNLM training, it can be used to any\nnetworks with large softmax output layers.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 17:49:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 07:09:16 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 06:08:54 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 04:40:55 GMT"}, {"version": "v5", "created": "Wed, 6 Jan 2016 21:57:56 GMT"}, {"version": "v6", "created": "Sun, 21 Feb 2016 16:40:26 GMT"}, {"version": "v7", "created": "Thu, 31 Mar 2016 17:37:25 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Ji", "Shihao", ""], ["Vishwanathan", "S. V. N.", ""], ["Satish", "Nadathur", ""], ["Anderson", "Michael J.", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1511.06939", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos\n  Tikk", "title": "Session-based Recommendations with Recurrent Neural Networks", "comments": "Camera ready version (17th February, 2016) Affiliation update (29th\n  March, 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply recurrent neural networks (RNN) on a new domain, namely recommender\nsystems. Real-life recommender systems often face the problem of having to base\nrecommendations only on short session-based data (e.g. a small sportsware\nwebsite) instead of long user histories (as in the case of Netflix). In this\nsituation the frequently praised matrix factorization approaches are not\naccurate. This problem is usually overcome in practice by resorting to\nitem-to-item recommendations, i.e. recommending similar items. We argue that by\nmodeling the whole session, more accurate recommendations can be provided. We\ntherefore propose an RNN-based approach for session-based recommendations. Our\napproach also considers practical aspects of the task and introduces several\nmodifications to classic RNNs such as a ranking loss function that make it more\nviable for this specific problem. Experimental results on two data-sets show\nmarked improvements over widely used approaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 23:42:59 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:13:50 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 16:41:37 GMT"}, {"version": "v4", "created": "Tue, 29 Mar 2016 14:52:58 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Karatzoglou", "Alexandros", ""], ["Baltrunas", "Linas", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1511.06951", "submitter": "Leslie Smith", "authors": "Leslie N. Smith, Emily M. Hand, Timothy Doster", "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of dynamically growing a neural network during\ntraining. In particular, an untrainable deep network starts as a trainable\nshallow network and newly added layers are slowly, organically added during\ntraining, thereby increasing the network's depth. This is accomplished by a new\nlayer, which we call DropIn. The DropIn layer starts by passing the output from\na previous layer (effectively skipping over the newly added layers), then\nincreasingly including units from the new layers for both feedforward and\nbackpropagation. We show that deep networks, which are untrainable with\nconventional methods, will converge with DropIn layers interspersed in the\narchitecture. In addition, we demonstrate that DropIn provides regularization\nduring training in an analogous way as dropout. Experiments are described with\nthe MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset\nwith its architecture expanded from 3 to 11 layers, and on the ImageNet dataset\nwith the AlexNet architecture expanded to 13 layers and the VGG 16-layer\narchitecture.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 02:33:08 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Smith", "Leslie N.", ""], ["Hand", "Emily M.", ""], ["Doster", "Timothy", ""]]}, {"id": "1511.06987", "submitter": "Anton Eremeev", "authors": "Anton V. Eremeev", "title": "Evolutionary algorithms", "comments": "Outline of lectures course \"Evolutionary Algorithms\" (in Russian)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript contains an outline of lectures course \"Evolutionary\nAlgorithms\" read by the author. The course covers Canonic Genetic Algorithm and\nvarious other genetic algorithms as well as evolutionary strategies, genetic\nprogramming, tabu search and the class of evolutionary algorithms in general.\nSome facts, such as the Rotation Property of crossover, the Schemata Theorem,\nGA performance as a local search and \"almost surely\" convergence of\nevolutionary algorithms are given with complete proofs. The text is in Russian.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 10:05:33 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 16:44:02 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 07:06:47 GMT"}, {"version": "v4", "created": "Sat, 23 Nov 2019 17:33:20 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Eremeev", "Anton V.", ""]]}, {"id": "1511.07035", "submitter": "Lex Fridman", "authors": "Irman Abdi\\'c, Lex Fridman, Erik Marchi, Daniel E Brown, William\n  Angell, Bryan Reimer, Bj\\\"orn Schuller", "title": "Detecting Road Surface Wetness from Audio: A Deep Learning Approach", "comments": "Under review in IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a recurrent neural network architecture for automated road\nsurface wetness detection from audio of tire-surface interaction. The\nrobustness of our approach is evaluated on 785,826 bins of audio that span an\nextensive range of vehicle speeds, noises from the environment, road surface\ntypes, and pavement conditions including international roughness index (IRI)\nvalues from 25 in/mi to 1400 in/mi. The training and evaluation of the model\nare performed on different roads to minimize the impact of environmental and\nother external factors on the accuracy of the classification. We achieve an\nunweighted average recall (UAR) of 93.2% across all vehicle speeds including 0\nmph. The classifier still works at 0 mph because the discriminating signal is\npresent in the sound of other vehicles driving by.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 17:20:23 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 20:05:22 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Abdi\u0107", "Irman", ""], ["Fridman", "Lex", ""], ["Marchi", "Erik", ""], ["Brown", "Daniel E", ""], ["Angell", "William", ""], ["Reimer", "Bryan", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1511.07076", "submitter": "Andrei Zenkevich V", "authors": "D.V. Negrov, I.M. Karandashev, V.V. Shakirov, Yu.A. Matveyev, W.L.\n  Dunin-Barkowski and A.V. Zenkevich", "title": "An Approximate Backpropagation Learning Rule for Memristor Based Neural\n  Networks Using Synaptic Plasticity", "comments": "21 pages, 6 figures, 1 table, title changed, manuscript thoroughly\n  rewritten", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approximation to backpropagation algorithm for training deep\nneural networks, which is designed to work with synapses implemented with\nmemristors. The key idea is to represent the values of both the input signal\nand the backpropagated delta value with a series of pulses that trigger\nmultiple positive or negative updates of the synaptic weight, and to use the\nmin operation instead of the product of the two signals. In computational\nsimulations, we show that the proposed approximation to backpropagation is well\nconverged and may be suitable for memristor implementations of multilayer\nneural networks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 22:22:52 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 08:54:22 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Negrov", "D. V.", ""], ["Karandashev", "I. M.", ""], ["Shakirov", "V. V.", ""], ["Matveyev", "Yu. A.", ""], ["Dunin-Barkowski", "W. L.", ""], ["Zenkevich", "A. V.", ""]]}, {"id": "1511.07125", "submitter": "Patrick Gallagher", "authors": "Patrick W. Gallagher, Shuai Tang, Zhuowen Tu", "title": "What Happened to My Dog in That Network: Unraveling Top-down Generators\n  in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down information plays a central role in human perception, but plays\nrelatively little role in many current state-of-the-art deep networks, such as\nConvolutional Neural Networks (CNNs). This work seeks to explore a path by\nwhich top-down information can have a direct impact within current deep\nnetworks. We explore this path by learning and using \"generators\" corresponding\nto the network internal effects of three types of transformation (each a\nrestriction of a general affine transformation): rotation, scaling, and\ntranslation. We demonstrate how these learned generators can be used to\ntransfer top-down information to novel settings, as mediated by the \"feature\nflows\" that the transformations (and the associated generators) correspond to\ninside the network. Specifically, we explore three aspects: 1) using generators\nas part of a method for synthesizing transformed images --- given a previously\nunseen image, produce versions of that image corresponding to one or more\nspecified transformations, 2) \"zero-shot learning\" --- when provided with a\nfeature flow corresponding to the effect of a transformation of unknown amount,\nleverage learned generators as part of a method by which to perform an accurate\ncategorization of the amount of transformation, even for amounts never observed\nduring training, and 3) (inside-CNN) \"data augmentation\" --- improve the\nclassification performance of an existing network by using the learned\ngenerators to directly provide additional training \"inside the CNN\".\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 07:48:01 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Gallagher", "Patrick W.", ""], ["Tang", "Shuai", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1511.07401", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith\n  Chintala, Rob Fergus", "title": "MazeBase: A Sandbox for Learning from Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces MazeBase: an environment for simple 2D games, designed\nas a sandbox for machine learning approaches to reasoning and planning. Within\nit, we create 10 simple games embodying a range of algorithmic tasks (e.g.\nif-then statements or set negation). A variety of neural models (fully\nconnected, convolutional network, memory network) are deployed via\nreinforcement learning on these games, with and without a procedurally\ngenerated curriculum. Despite the tasks' simplicity, the performance of the\nmodels is far from optimal, suggesting directions for future development. We\nalso demonstrate the versatility of MazeBase by using it to emulate small\ncombat scenarios from StarCraft. Models trained on the MazeBase version can be\ndirectly applied to StarCraft, where they consistently beat the in-game AI.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:23:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 18:41:14 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Synnaeve", "Gabriel", ""], ["Chintala", "Soumith", ""], ["Fergus", "Rob", ""]]}, {"id": "1511.07528", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel and Somesh Jha and Matt\n  Fredrikson and Z. Berkay Celik and Ananthram Swami", "title": "The Limitations of Deep Learning in Adversarial Settings", "comments": "Accepted to the 1st IEEE European Symposium on Security & Privacy,\n  IEEE 2016. Saarbrucken, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a novel class of algorithms to craft adversarial samples based on a\nprecise understanding of the mapping between inputs and outputs of DNNs. In an\napplication to computer vision, we show that our algorithms can reliably\nproduce samples correctly classified by human subjects but misclassified in\nspecific targets by a DNN with a 97% adversarial success rate while only\nmodifying on average 4.02% of the input features per sample. We then evaluate\nthe vulnerability of different sample classes to adversarial perturbations by\ndefining a hardness measure. Finally, we describe preliminary work outlining\ndefenses against adversarial samples by defining a predictive measure of\ndistance between a benign input and a target classification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 01:07:08 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""], ["Jha", "Somesh", ""], ["Fredrikson", "Matt", ""], ["Celik", "Z. Berkay", ""], ["Swami", "Ananthram", ""]]}, {"id": "1511.07543", "submitter": "Yixuan Li", "authors": "Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson and John Hopcroft", "title": "Convergent Learning: Do different neural networks learn the same\n  representations?", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success in training deep neural networks have prompted active\ninvestigation into the features learned on their intermediate layers. Such\nresearch is difficult because it requires making sense of non-linear\ncomputations performed by millions of parameters, but valuable because it\nincreases our ability to understand current models and create improved versions\nof them. In this paper we investigate the extent to which neural networks\nexhibit what we call convergent learning, which is when the representations\nlearned by multiple nets converge to a set of features which are either\nindividually similar between networks or where subsets of features span similar\nlow-dimensional spaces. We propose a specific method of probing\nrepresentations: training multiple networks and then comparing and contrasting\ntheir individual, learned representations at the level of neurons or groups of\nneurons. We begin research into this question using three techniques to\napproximately align different neural networks on a feature level: a bipartite\nmatching approach that makes one-to-one assignments between neurons, a sparse\nprediction approach that finds one-to-many mappings, and a spectral clustering\napproach that finds many-to-many mappings. This initial investigation reveals a\nfew previously unknown properties of neural networks, and we argue that future\nresearch into the question of convergent learning will yield many more. The\ninsights described here include (1) that some features are learned reliably in\nmultiple networks, yet other features are not consistently learned; (2) that\nunits learn to span low-dimensional subspaces and, while these subspaces are\ncommon to multiple networks, the specific basis vectors learned are not; (3)\nthat the representation codes show evidence of being a mix between a local code\nand slightly, but not fully, distributed codes across multiple units.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 02:31:46 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 02:33:05 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2016 22:04:54 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Li", "Yixuan", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""], ["Lipson", "Hod", ""], ["Hopcroft", "John", ""]]}, {"id": "1511.07763", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "LocNet: Improving Localization Accuracy for Object Detection", "comments": "Extended technical report -- short version to appear as oral paper on\n  CVPR 2016. Code: https://github.com/gidariss/LocNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel object localization methodology with the purpose of\nboosting the localization accuracy of state-of-the-art object detection\nsystems. Our model, given a search region, aims at returning the bounding box\nof an object of interest inside this region. To accomplish its goal, it relies\non assigning conditional probabilities to each row and column of this region,\nwhere these probabilities provide useful information regarding the location of\nthe boundaries of the object inside the search region and allow the accurate\ninference of the object bounding box under a simple probabilistic framework.\n  For implementing our localization model, we make use of a convolutional\nneural network architecture that is properly adapted for this task, called\nLocNet. We show experimentally that LocNet achieves a very significant\nimprovement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and\nthat it can be very easily coupled with recent state-of-the-art object\ndetection systems, helping them to boost their performance. Finally, we\ndemonstrate that our detection approach can achieve high detection accuracy\neven when it is given as input a set of sliding windows, thus proving that it\nis independent of box proposal methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 15:42:01 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 15:09:15 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1511.07838", "submitter": "Amjad Almahairi", "authors": "Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo\n  Larochelle, Aaron Courville", "title": "Dynamic Capacity Networks", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Dynamic Capacity Network (DCN), a neural network that can\nadaptively assign its capacity across different portions of the input data.\nThis is achieved by combining modules of two types: low-capacity sub-networks\nand high-capacity sub-networks. The low-capacity sub-networks are applied\nacross most of the input, but also provide a guide to select a few portions of\nthe input on which to apply the high-capacity sub-networks. The selection is\nmade using a novel gradient-based attention mechanism, that efficiently\nidentifies input regions for which the DCN's output is most sensitive and to\nwhich we should devote more capacity. We focus our empirical evaluation on the\nCluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are\nable to drastically reduce the number of computations, compared to traditional\nconvolutional neural networks, while maintaining similar or even better\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:30:19 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 19:17:53 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 16:13:21 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 22:44:43 GMT"}, {"version": "v5", "created": "Tue, 9 Feb 2016 16:49:55 GMT"}, {"version": "v6", "created": "Wed, 6 Apr 2016 19:48:32 GMT"}, {"version": "v7", "created": "Sun, 22 May 2016 20:58:11 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Almahairi", "Amjad", ""], ["Ballas", "Nicolas", ""], ["Cooijmans", "Tim", ""], ["Zheng", "Yin", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1511.07860", "submitter": "Ryan Williams", "authors": "Daniel M. Kane and Ryan Williams", "title": "Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two\n  and Depth-Three Threshold Circuits", "comments": null, "journal-ref": "ACM Symposium on Theory of Computing (STOC), 2016", "doi": null, "report-no": null, "categories": "cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to formally understand the power of neural computing, we first need\nto crack the frontier of threshold circuits with two and three layers, a regime\nthat has been surprisingly intractable to analyze. We prove the first\nsuper-linear gate lower bounds and the first super-quadratic wire lower bounds\nfor depth-two linear threshold circuits with arbitrary weights, and depth-three\nmajority circuits computing an explicit function.\n  $\\bullet$ We prove that for all $\\epsilon\\gg \\sqrt{\\log(n)/n}$, the\nlinear-time computable Andreev's function cannot be computed on a\n$(1/2+\\epsilon)$-fraction of $n$-bit inputs by depth-two linear threshold\ncircuits of $o(\\epsilon^3 n^{3/2}/\\log^3 n)$ gates, nor can it be computed with\n$o(\\epsilon^{3} n^{5/2}/\\log^{7/2} n)$ wires. This establishes an average-case\n``size hierarchy'' for threshold circuits, as Andreev's function is computable\nby uniform depth-two circuits of $o(n^3)$ linear threshold gates, and by\nuniform depth-three circuits of $O(n)$ majority gates.\n  $\\bullet$ We present a new function in $P$ based on small-biased sets, which\nwe prove cannot be computed by a majority vote of depth-two linear threshold\ncircuits with $o(n^{3/2}/\\log^3 n)$ gates, nor with $o(n^{5/2}/\\log^{7/2}n)$\nwires.\n  $\\bullet$ We give tight average-case (gate and wire) complexity results for\ncomputing PARITY with depth-two threshold circuits; the answer turns out to be\nthe same as for depth-two majority circuits.\n  The key is a new random restriction lemma for linear threshold functions. Our\nmain analytical tool is the Littlewood-Offord Lemma from additive\ncombinatorics.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 20:45:51 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Kane", "Daniel M.", ""], ["Williams", "Ryan", ""]]}, {"id": "1511.07889", "submitter": "Nicholas Leonard", "authors": "Nicholas L\\'eonard, Sagar Waghmare, Yang Wang, Jin-Hwa Kim", "title": "rnn : Recurrent Library for Torch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rnn package provides components for implementing a wide range of\nRecurrent Neural Networks. It is built withing the framework of the Torch\ndistribution for use with the nn package. The components have evolved from 3\niterations, each adding to the flexibility and capability of the package. All\ncomponent modules inherit either the AbstractRecurrent or AbstractSequencer\nclasses. Strong unit testing, continued backwards compatibility and access to\nsupporting material are the principles followed during its development. The\npackage is compared against existing implementations of two published papers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 21:18:33 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 16:30:06 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["L\u00e9onard", "Nicholas", ""], ["Waghmare", "Sagar", ""], ["Wang", "Yang", ""], ["Kim", "Jin-Hwa", ""]]}, {"id": "1511.08228", "submitter": "{\\L}ukasz Kaiser", "authors": "{\\L}ukasz Kaiser and Ilya Sutskever", "title": "Neural GPUs Learn Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an algorithm from examples is a fundamental problem that has been\nwidely studied. Recently it has been addressed using neural networks, in\nparticular by Neural Turing Machines (NTMs). These are fully differentiable\ncomputers that use backpropagation to learn their own programming. Despite\ntheir appeal NTMs have a weakness that is caused by their sequential nature:\nthey are not parallel and are are hard to train due to their large depth when\nunfolded.\n  We present a neural network architecture to address this problem: the Neural\nGPU. It is based on a type of convolutional gated recurrent unit and, like the\nNTM, is computationally universal. Unlike the NTM, the Neural GPU is highly\nparallel which makes it easier to train and efficient to run.\n  An essential property of algorithms is their ability to handle inputs of\narbitrary size. We show that the Neural GPU can be trained on short instances\nof an algorithmic task and successfully generalize to long instances. We\nverified it on a number of tasks including long addition and long\nmultiplication of numbers represented in binary. We train the Neural GPU on\nnumbers with upto 20 bits and observe no errors whatsoever while testing it,\neven on much longer numbers.\n  To achieve these results we introduce a technique for training deep recurrent\nnetworks: parameter sharing relaxation. We also found a small amount of dropout\nand gradient noise to have a large positive effect on learning and\ngeneralization.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 21:17:43 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 14:44:27 GMT"}, {"version": "v3", "created": "Tue, 15 Mar 2016 00:20:54 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Kaiser", "\u0141ukasz", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1511.08277", "submitter": "Shengxian Wan", "authors": "Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi\n  Cheng", "title": "A Deep Architecture for Semantic Matching with Multiple Positional\n  Sentence Representations", "comments": "Accepted by AAAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching natural language sentences is central for many applications such as\ninformation retrieval and question answering. Existing deep models rely on a\nsingle sentence representation or multiple granularity representations for\nmatching. However, such methods cannot well capture the contextualized local\ninformation in the matching process. To tackle this problem, we present a new\ndeep architecture to match two sentences with multiple positional sentence\nrepresentations. Specifically, each positional sentence representation is a\nsentence representation at this position, generated by a bidirectional long\nshort term memory (Bi-LSTM). The matching score is finally produced by\naggregating interactions between these different positional sentence\nrepresentations, through $k$-Max pooling and a multi-layer perceptron. Our\nmodel has several advantages: (1) By using Bi-LSTM, rich context of the whole\nsentence is leveraged to capture the contextualized local information in each\npositional sentence representation; (2) By matching with multiple positional\nsentence representations, it is flexible to aggregate different important\ncontextualized local information in a sentence to support the matching; (3)\nExperiments on different tasks such as question answering and sentence\ncompletion demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:57:54 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Wan", "Shengxian", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Pang", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1511.08308", "submitter": "Eric Nichols", "authors": "Jason P.C. Chiu and Eric Nichols", "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition is a challenging task that has traditionally\nrequired large amounts of knowledge in the form of feature engineering and\nlexicons to achieve high performance. In this paper, we present a novel neural\nnetwork architecture that automatically detects word- and character-level\nfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminating\nthe need for most feature engineering. We also propose a novel method of\nencoding partial lexicon matches in neural networks and compare it to existing\napproaches. Extensive evaluation shows that, given only tokenized text and\npublicly available word embeddings, our system is competitive on the CoNLL-2003\ndataset and surpasses the previously reported state of the art performance on\nthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed\nfrom publicly-available sources, we establish new state of the art performance\nwith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing\nsystems that employ heavy feature engineering, proprietary lexicons, and rich\nentity linking information.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 07:40:33 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 09:23:52 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 06:25:57 GMT"}, {"version": "v4", "created": "Thu, 16 Jun 2016 06:15:49 GMT"}, {"version": "v5", "created": "Tue, 19 Jul 2016 05:02:51 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chiu", "Jason P. C.", ""], ["Nichols", "Eric", ""]]}, {"id": "1511.08366", "submitter": "Kristina Kapanova G", "authors": "K. G. Kapanova, I. Dimov, J. M. Sellier", "title": "On randomization of neural networks as a form of post-learning strategy", "comments": "15 pages, 26 figures", "journal-ref": null, "doi": "10.1007/s00500-015-1949-1", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today artificial neural networks are applied in various fields - engineering,\ndata analysis, robotics. While they represent a successful tool for a variety\nof relevant applications, mathematically speaking they are still far from being\nconclusive. In particular, they suffer from being unable to find the best\nconfiguration possible during the training process (local minimum problem). In\nthis paper, we focus on this issue and suggest a simple, but effective,\npost-learning strategy to allow the search for improved set of weights at a\nrelatively small extra computational cost. Therefore, we introduce a novel\ntechnique based on analogy with quantum effects occurring in nature as a way to\nimprove (and sometimes overcome) this problem. Several numerical experiments\nare presented to validate the approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 12:11:48 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kapanova", "K. G.", ""], ["Dimov", "I.", ""], ["Sellier", "J. M.", ""]]}, {"id": "1511.08400", "submitter": "David Krueger", "authors": "David Krueger, Roland Memisevic", "title": "Regularizing RNNs by Stabilizing Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We stabilize the activations of Recurrent Neural Networks (RNNs) by\npenalizing the squared distance between successive hidden states' norms.\n  This penalty term is an effective regularizer for RNNs including LSTMs and\nIRNNs, improving performance on character-level language modeling and phoneme\nrecognition, and outperforming weight noise and dropout.\n  We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme\nrecognition task for RNNs evaluated without beam search or an RNN transducer.\n  With this penalty term, IRNN can achieve similar performance to LSTM on\nlanguage modeling, although adding the penalty term to the LSTM results in\nsuperior performance.\n  Our penalty term also prevents the exponential growth of IRNN's activations\noutside of their training horizon, allowing them to generalize to much longer\nsequences.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:35:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 04:52:03 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2015 02:09:00 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 00:58:39 GMT"}, {"version": "v5", "created": "Fri, 5 Feb 2016 04:58:47 GMT"}, {"version": "v6", "created": "Wed, 2 Mar 2016 20:42:08 GMT"}, {"version": "v7", "created": "Tue, 26 Apr 2016 05:21:11 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Krueger", "David", ""], ["Memisevic", "Roland", ""]]}, {"id": "1511.08458", "submitter": "Keiron O'Shea", "authors": "Keiron O'Shea and Ryan Nash", "title": "An Introduction to Convolutional Neural Networks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of machine learning has taken a dramatic twist in recent times,\nwith the rise of the Artificial Neural Network (ANN). These biologically\ninspired computational models are able to far exceed the performance of\nprevious forms of artificial intelligence in common machine learning tasks. One\nof the most impressive forms of ANN architecture is that of the Convolutional\nNeural Network (CNN). CNNs are primarily used to solve difficult image-driven\npattern recognition tasks and with their precise yet simple architecture,\noffers a simplified method of getting started with ANNs.\n  This document provides a brief introduction to CNNs, discussing recently\npublished papers and newly formed techniques in developing these brilliantly\nfantastic image recognition models. This introduction assumes you are familiar\nwith the fundamentals of ANNs and machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 17:45:01 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 18:06:03 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["O'Shea", "Keiron", ""], ["Nash", "Ryan", ""]]}, {"id": "1511.08899", "submitter": "Mohamed Moustafa", "authors": "Mohamed Moustafa", "title": "Applying deep learning to classify pornographic images and videos", "comments": "PSIVT 2015, the final publication is available at link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is no secret that pornographic material is now a one-click-away from\neveryone, including children and minors. General social media networks are\nstriving to isolate adult images and videos from normal ones. Intelligent image\nanalysis methods can help to automatically detect and isolate questionable\nimages in media. Unfortunately, these methods require vast experience to design\nthe classifier including one or more of the popular computer vision feature\ndescriptors. We propose to build a classifier based on one of the recently\nflourishing deep learning techniques. Convolutional neural networks contain\nmany layers for both automatic features extraction and classification. The\nbenefit is an easier system to build (no need for hand-crafting features and\nclassifiers). Additionally, our experiments show that it is even more accurate\nthan the state of the art methods on the most recent benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 13:55:25 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Moustafa", "Mohamed", ""]]}, {"id": "1511.09249", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "On Learning to Think: Algorithmic Information Theory for Novel\n  Combinations of Reinforcement Learning Controllers and Recurrent Neural World\n  Models", "comments": "36 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1404.7828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the general problem of reinforcement learning (RL) in\npartially observable environments. In 2013, our large RL recurrent neural\nnetworks (RNNs) learned from scratch to drive simulated cars from\nhigh-dimensional video input. However, real brains are more powerful in many\nways. In particular, they learn a predictive model of their initially unknown\nenvironment, and somehow use it for abstract (e.g., hierarchical) planning and\nreasoning. Guided by algorithmic information theory, we describe RNN-based AIs\n(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending\nsequences of tasks, some of them provided by the user, others invented by the\nRNNAI itself in a curious, playful fashion, to improve its RNN-based world\nmodel. Unlike our previous model-building RNN-based RL machines dating back to\n1990, the RNNAI learns to actively query its model for abstract reasoning and\nplanning and decision making, essentially \"learning to think.\" The basic ideas\nof this report can be applied to many other cases where one RNN-like system\nexploits the algorithmic information content of another. They are taken from a\ngrant proposal submitted in Fall 2014, and also explain concepts such as\n\"mirror neurons.\" Experimental results will be described in separate papers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 11:35:26 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}, {"id": "1511.09337", "submitter": "Yu-An Chung", "authors": "Yu-An Chung, Hsuan-Tien Lin, Shao-Wen Yang", "title": "Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been one of the most prominent machine learning techniques\nnowadays, being the state-of-the-art on a broad range of applications where\nautomatic feature extraction is needed. Many such applications also demand\nvarying costs for different types of mis-classification errors, but it is not\nclear whether or how such cost information can be incorporated into deep\nlearning to improve performance. In this work, we propose a novel cost-aware\nalgorithm that takes into account the cost information into not only the\ntraining stage but also the pre-training stage of deep learning. The approach\nallows deep learning to conduct automatic feature extraction with the cost\ninformation effectively. Extensive experimental results demonstrate that the\nproposed approach outperforms other deep learning models that do not digest the\ncost information in the pre-training stage.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 14:54:28 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 07:30:13 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 04:00:11 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Chung", "Yu-An", ""], ["Lin", "Hsuan-Tien", ""], ["Yang", "Shao-Wen", ""]]}, {"id": "1511.09426", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "A Normative Theory of Adaptive Dimensionality Reduction in Neural\n  Networks", "comments": "Advances in Neural Information Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make sense of the world our brains must analyze high-dimensional datasets\nstreamed by our sensory organs. Because such analysis begins with\ndimensionality reduction, modelling early sensory processing requires\nbiologically plausible online dimensionality reduction algorithms. Recently, we\nderived such an algorithm, termed similarity matching, from a Multidimensional\nScaling (MDS) objective function. However, in the existing algorithm, the\nnumber of output dimensions is set a priori by the number of output neurons and\ncannot be changed. Because the number of informative dimensions in sensory\ninputs is variable there is a need for adaptive dimensionality reduction. Here,\nwe derive biologically plausible dimensionality reduction algorithms which\nadapt the number of output dimensions to the eigenspectrum of the input\ncovariance matrix. We formulate three objective functions which, in the offline\nsetting, are optimized by the projections of the input dataset onto its\nprincipal subspace scaled by the eigenvalues of the output covariance matrix.\nIn turn, the output eigenvalues are computed as i) soft-thresholded, ii)\nhard-thresholded, iii) equalized thresholded eigenvalues of the input\ncovariance matrix. In the online setting, we derive the three corresponding\nadaptive algorithms and map them onto the dynamics of neuronal activity in\nnetworks with biologically plausible local learning rules. Remarkably, in the\nlast two networks, neurons are divided into two classes which we identify with\nprincipal neurons and interneurons in biological circuits.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 18:45:30 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 18:44:23 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1511.09468", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "Optimization theory of Hebbian/anti-Hebbian networks for PCA and\n  whitening", "comments": "Annual Allerton Conference on Communication, Control, and Computing\n  (Allerton) 2015", "journal-ref": null, "doi": "10.1109/ALLERTON.2015.7447180", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyzing information streamed by sensory organs, our brains face\nchallenges similar to those solved in statistical signal processing. This\nsuggests that biologically plausible implementations of online signal\nprocessing algorithms may model neural computation. Here, we focus on such\nworkhorses of signal processing as Principal Component Analysis (PCA) and\nwhitening which maximize information transmission in the presence of noise. We\nadopt the similarity matching framework, recently developed for principal\nsubspace extraction, but modify the existing objective functions by adding a\ndecorrelating term. From the modified objective functions, we derive online PCA\nand whitening algorithms which are implementable by neural networks with local\nlearning rules, i.e. synaptic weight updates that depend on the activity of\nonly pre- and postsynaptic neurons. Our theory offers a principled model of\nneural computations and makes testable predictions such as the dropout of\nunderutilized neurons.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 20:52:39 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}]