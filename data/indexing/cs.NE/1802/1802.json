[{"id": "1802.00180", "submitter": "Dario  Izzo", "authors": "Dario Izzo and Christopher Sprague and Dharmesh Tailor", "title": "Machine learning and evolutionary techniques in interplanetary\n  trajectory design", "comments": "Submitted to as a book chapter for a Springer book on \"Optimization\n  in Space Engineering\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  After providing a brief historical overview on the synergies between\nartificial intelligence research, in the areas of evolutionary computations and\nmachine learning, and the optimal design of interplanetary trajectories, we\npropose and study the use of deep artificial neural networks to represent,\non-board, the optimal guidance profile of an interplanetary mission. The\nresults, limited to the chosen test case of an Earth-Mars orbital transfer,\nextend the findings made previously for landing scenarios and quadcopter\ndynamics, opening a new research area in interplanetary trajectory planning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 07:34:50 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 09:18:09 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Izzo", "Dario", ""], ["Sprague", "Christopher", ""], ["Tailor", "Dharmesh", ""]]}, {"id": "1802.00209", "submitter": "Ahmed Osman", "authors": "Ahmed Osman and Wojciech Samek", "title": "Dual Recurrent Attention Units for Visual Question Answering", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) requires AI models to comprehend data in two\ndomains, vision and text. Current state-of-the-art models use learned attention\nmechanisms to extract relevant information from the input domains to answer a\ncertain question. Thus, robust attention mechanisms are essential for powerful\nVQA models. In this paper, we propose a recurrent attention mechanism and show\nits benefits compared to the traditional convolutional approach. We perform two\nablation studies to evaluate recurrent attention. First, we introduce a\nbaseline VQA model with visual attention and test the performance difference\nbetween convolutional and recurrent attention on the VQA 2.0 dataset. Secondly,\nwe design an architecture for VQA which utilizes dual (textual and visual)\nRecurrent Attention Units (RAUs). Using this model, we show the effect of all\npossible combinations of recurrent and convolutional dual attention. Our single\nmodel outperforms the first place winner on the VQA 2016 challenge and to the\nbest of our knowledge, it is the second best performing single model on the VQA\n1.0 dataset. Furthermore, our model noticeably improves upon the winner of the\nVQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in\nstate-of-the-art models with our RAUs and show increased performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 09:35:33 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 16:27:26 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 13:41:21 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Osman", "Ahmed", ""], ["Samek", "Wojciech", ""]]}, {"id": "1802.00554", "submitter": "Andrew Lensen", "authors": "Andrew Lensen, Bing Xue, and Mengjie Zhang", "title": "Generating Redundant Features with Unsupervised Multi-Tree Genetic\n  Programming", "comments": "16 pages, preprint for EuroGP '18", "journal-ref": null, "doi": "10.1007/978-3-319-77553-1_6", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, feature selection has become an increasingly important area of\nresearch due to the surge in high-dimensional datasets in all areas of modern\nlife. A plethora of feature selection algorithms have been proposed, but it is\ndifficult to truly analyse the quality of a given algorithm. Ideally, an\nalgorithm would be evaluated by measuring how well it removes known bad\nfeatures. Acquiring datasets with such features is inherently difficult, and so\na common technique is to add synthetic bad features to an existing dataset.\nWhile adding noisy features is an easy task, it is very difficult to\nautomatically add complex, redundant features. This work proposes one of the\nfirst approaches to generating redundant features, using a novel genetic\nprogramming approach. Initial experiments show that our proposed method can\nautomatically create difficult, redundant features which have the potential to\nbe used for creating high-quality feature selection benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 04:19:04 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 06:35:56 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Lensen", "Andrew", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1802.00721", "submitter": "Phan Trung Hai Nguyen", "authors": "Per Kristian Lehre, Phan Trung Hai Nguyen", "title": "Improved Runtime Bounds for the Univariate Marginal Distribution\n  Algorithm via Anti-Concentration", "comments": "19 pages, 1 figure", "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO 2017). ACM, New York, NY, USA, 1383-1390", "doi": "10.1145/3071178.3071317", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional evolutionary algorithms which produce offspring via\ngenetic operators, Estimation of Distribution Algorithms (EDAs) sample\nsolutions from probabilistic models which are learned from selected\nindividuals. It is hoped that EDAs may improve optimisation performance on\nepistatic fitness landscapes by learning variable interactions. However, hardly\nany rigorous results are available to support claims about the performance of\nEDAs, even for fitness functions without epistasis. The expected runtime of the\nUnivariate Marginal Distribution Algorithm (UMDA) on OneMax was recently shown\nto be in $\\mathcal{O}\\left(n\\lambda\\log \\lambda\\right)$ by Dang and Lehre\n(GECCO 2015). Later, Krejca and Witt (FOGA 2017) proved the lower bound\n$\\Omega\\left(\\lambda\\sqrt{n}+n\\log n\\right)$ via an involved drift analysis.\n  We prove a $\\mathcal{O}\\left(n\\lambda\\right)$ bound, given some restrictions\non the population size. This implies the tight bound $\\Theta\\left(n\\log\nn\\right)$ when $\\lambda=\\mathcal{O}\\left(\\log n\\right)$, matching the runtime\nof classical EAs. Our analysis uses the level-based theorem and\nanti-concentration properties of the Poisson-Binomial distribution. We expect\nthat these generic methods will facilitate further analysis of EDAs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 15:11:33 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Lehre", "Per Kristian", ""], ["Nguyen", "Phan Trung Hai", ""]]}, {"id": "1802.00930", "submitter": "Dheevatsa Mudigere", "authors": "Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar,\n  Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan,\n  Bharat Kaul, Evangelos Georganas, Alexander Heinecke, Pradeep Dubey, Jesus\n  Corbal, Nikita Shustrov, Roma Dubtsov, Evarist Fomenko, Vadim Pirogov", "title": "Mixed Precision Training of Convolutional Neural Networks using Integer\n  Operations", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art (SOTA) for mixed precision training is dominated by\nvariants of low precision floating point operations, and in particular, FP16\naccumulating into FP32 Micikevicius et al. (2017). On the other hand, while a\nlot of research has also happened in the domain of low and mixed-precision\nInteger training, these works either present results for non-SOTA networks (for\ninstance only AlexNet for ImageNet-1K), or relatively small datasets (like\nCIFAR-10). In this work, we train state-of-the-art visual understanding neural\nnetworks on the ImageNet-1K dataset, with Integer operations on General Purpose\n(GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate\n(FMA) operations which take two pairs of INT16 operands and accumulate results\ninto an INT32 output.We propose a shared exponent representation of tensors and\ndevelop a Dynamic Fixed Point (DFP) scheme suitable for common neural network\noperations. The nuances of developing an efficient integer convolution kernel\nis examined, including methods to handle overflow of the INT32 accumulator. We\nimplement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and\nthese networks achieve or exceed SOTA accuracy within the same number of\niterations as their FP32 counterparts without any change in hyper-parameters\nand with a 1.8X improvement in end-to-end training throughput. To the best of\nour knowledge these results represent the first INT16 training results on GP\nhardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported\naccuracy using half-precision\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 07:01:48 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 18:02:58 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Das", "Dipankar", ""], ["Mellempudi", "Naveen", ""], ["Mudigere", "Dheevatsa", ""], ["Kalamkar", "Dhiraj", ""], ["Avancha", "Sasikanth", ""], ["Banerjee", "Kunal", ""], ["Sridharan", "Srinivas", ""], ["Vaidyanathan", "Karthik", ""], ["Kaul", "Bharat", ""], ["Georganas", "Evangelos", ""], ["Heinecke", "Alexander", ""], ["Dubey", "Pradeep", ""], ["Corbal", "Jesus", ""], ["Shustrov", "Nikita", ""], ["Dubtsov", "Roma", ""], ["Fomenko", "Evarist", ""], ["Pirogov", "Vadim", ""]]}, {"id": "1802.00938", "submitter": "Truyen Tran", "authors": "Asjad Khan, Hung Le, Kien Do, Truyen Tran, Aditya Ghose, Hoa Dam, and\n  Renuka Sindhgatta", "title": "Memory-Augmented Neural Networks for Predictive Process Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process analytics involves a sophisticated layer of data analytics built over\nthe traditional notion of process mining. The flexible execution of business\nprocess instances involves multiple critical decisions including what task to\nperform next and what resources to allocate to a task. In this paper, we\nexplore the application of deep learning techniques for solving various process\nanalytics related problems. Based on recent advances in the field we\nspecifically look at memory-augmented neural networks (MANN)s and adapt the\nlatest model to date, namely the Differential Neural Computer. We introduce two\nmodifications to account for a variety of tasks in predictive process\nanalytics: (i) separating the encoding phase and decoding phase, resulting dual\ncontrollers, one for each phase; (ii) implementing a write-protected policy for\nthe memory during the decoding phase. We demonstrate the feasibility and\nusefulness of our approach by solving a number of common process analytics\ntasks such as next activity prediction, time to completion and suffix\nprediction. We also introduce the notion of MANN based process analytics\nrecommendation machinery that once deployed can serve as an effective business\nprocess recommendation engine enabling organizations to answer various\nprescriptive process analytics related questions.Using real-world datasets, we\nbenchmark our results against those obtained from the state-of-art methods. We\nshow that MANNs based process analytics methods can acheive state-of-the-art\nperformance and have a lot of value to offer for enterprise specific process\nanlaytics applications.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 08:50:16 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Khan", "Asjad", ""], ["Le", "Hung", ""], ["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Ghose", "Aditya", ""], ["Dam", "Hoa", ""], ["Sindhgatta", "Renuka", ""]]}, {"id": "1802.00948", "submitter": "Truyen Tran", "authors": "Phuoc Nguyen, Truyen Tran, Svetha Venkatesh", "title": "Resset: A Recurrent Model for Sequence of Sets with Applications to\n  Electronic Medical Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern healthcare is ripe for disruption by AI. A game changer would be\nautomatic understanding the latent processes from electronic medical records,\nwhich are being collected for billions of people worldwide. However, these\nhealthcare processes are complicated by the interaction between at least three\ndynamic components: the illness which involves multiple diseases, the care\nwhich involves multiple treatments, and the recording practice which is biased\nand erroneous. Existing methods are inadequate in capturing the dynamic\nstructure of care. We propose Resset, an end-to-end recurrent model that reads\nmedical record and predicts future risk. The model adopts the algebraic view in\nthat discrete medical objects are embedded into continuous vectors lying in the\nsame space. We formulate the problem as modeling sequences of sets, a novel\nsetting that have rarely, if not, been addressed. Within Resset, the bag of\ndiseases recorded at each clinic visit is modeled as function of sets. The same\nhold for the bag of treatments. The interaction between the disease bag and the\ntreatment bag at a visit is modeled in several, one of which as residual of\ndiseases minus the treatments. Finally, the health trajectory, which is a\nsequence of visits, is modeled using a recurrent neural network. We report\nresults on over a hundred thousand hospital visits by patients suffered from\ntwo costly chronic diseases -- diabetes and mental health. Resset shows\npromises in multiple predictive tasks such as readmission prediction,\ntreatments recommendation and diseases progression.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 10:11:38 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Nguyen", "Phuoc", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1802.01016", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Yipeng Zhang, Geng Yuan, Ao Ren, Zhe Li, Jie Han,\n  Jingtong Hu, Yanzhi Wang", "title": "An Area and Energy Efficient Design of Domain-Wall Memory-Based Deep\n  Convolutional Neural Networks using Stochastic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent trend of wearable devices and Internet of Things (IoTs), it\nbecomes attractive to develop hardware-based deep convolutional neural networks\n(DCNNs) for embedded applications, which require low power/energy consumptions\nand small hardware footprints. Recent works demonstrated that the Stochastic\nComputing (SC) technique can radically simplify the hardware implementation of\narithmetic units and has the potential to satisfy the stringent power\nrequirements in embedded devices. However, in these works, the memory design\noptimization is neglected for weight storage, which will inevitably result in\nlarge hardware cost. Moreover, if conventional volatile SRAM or DRAM cells are\nutilized for weight storage, the weights need to be re-initialized whenever the\nDCNN platform is re-started.\n  In order to overcome these limitations, in this work we adopt an emerging\nnon-volatile Domain-Wall Memory (DWM), which can achieve ultra-high density, to\nreplace SRAM for weight storage in SC-based DCNNs. We propose DW-CNN, the first\ncomprehensive design optimization framework of DWM-based weight storage method.\nWe derive the optimal memory type, precision, and organization, as well as\nwhether to store binary or stochastic numbers. We present effective resource\nsharing scheme for DWM-based weight storage in the convolutional and\nfully-connected layers of SC-based DCNNs to achieve a desirable balance among\narea, power (energy) consumption, and application-level accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 19:33:40 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Ma", "Xiaolong", ""], ["Zhang", "Yipeng", ""], ["Yuan", "Geng", ""], ["Ren", "Ao", ""], ["Li", "Zhe", ""], ["Han", "Jie", ""], ["Hu", "Jingtong", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1802.01096", "submitter": "Nathalia Moraes do Nascimento", "authors": "Nathalia Nascimento, Carlos Lucena, Paulo Alencar and Donald Cowan", "title": "Software Engineers vs. Machine Learning Algorithms: An Empirical Study\n  Assessing Performance and Reuse Tasks", "comments": "22 pages. To be submitted to IEEE Transactions on Software\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several papers have recently contained reports on applying machine learning\n(ML) to the automation of software engineering (SE) tasks, such as project\nmanagement, modeling and development. However, there appear to be no approaches\ncomparing how software engineers fare against machine-learning algorithms as\napplied to specific software development tasks. Such a comparison is essential\nto gain insight into which tasks are better performed by humans and which by\nmachine learning and how cooperative work or human-in-the-loop processes can be\nimplemented more effectively. In this paper, we present an empirical study that\ncompares how software engineers and machine-learning algorithms perform and\nreuse tasks. The empirical study involves the synthesis of the control\nstructure of an autonomous streetlight application. Our approach consists of\nfour steps. First, we solved the problem using machine learning to determine\nspecific performance and reuse tasks. Second, we asked software engineers with\ndifferent domain knowledge levels to provide a solution to the same tasks.\nThird, we compared how software engineers fare against machine-learning\nalgorithms when accomplishing the performance and reuse tasks based on criteria\nsuch as energy consumption and safety. Finally, we analyzed the results to\nunderstand which tasks are better performed by either humans or algorithms so\nthat they can work together more effectively. Such an understanding and the\nresulting human-in-the-loop approaches, which take into account the strengths\nand weaknesses of humans and machine-learning algorithms, are fundamental not\nonly to provide a basis for cooperative work in support of software\nengineering, but also, in other areas.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 09:38:48 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 21:32:02 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Nascimento", "Nathalia", ""], ["Lucena", "Carlos", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1802.01353", "submitter": "Andrei Ivanov", "authors": "Andrei Ivanov, Alena Sholokhova, Sergei Andrianov, Roman\n  Konoplev-Esgenburg", "title": "Lie Transform--based Neural Networks for Dynamics Simulation and\n  Learning", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.NA math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the article, we discuss the architecture of the polynomial neural network\nthat corresponds to the matrix representation of Lie transform. The matrix form\nof Lie transform is an approximation of the general solution of the nonlinear\nsystem of ordinary differential equations. The proposed architecture can be\ntrained with small data sets, extrapolate predictions outside the training\ndata, and provide a possibility for interpretation. We provide a theoretical\nexplanation of the proposed architecture, as well as demonstrate it in several\napplications. We present the results of modeling and identification for both\nsimple and well-known dynamical systems, and more complicated examples from\nprice dynamics, chemistry, and accelerator physics. From a practical point of\nview, we describe the training of a Lie transform--based neural network with a\nsmall data set containing only 10 data points. We also demonstrate an\ninterpretation of the fitted neural network by converting it to a system of\ndifferential equations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 11:25:54 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 08:41:41 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ivanov", "Andrei", ""], ["Sholokhova", "Alena", ""], ["Andrianov", "Sergei", ""], ["Konoplev-Esgenburg", "Roman", ""]]}, {"id": "1802.01548", "submitter": "Esteban Real", "authors": "Esteban Real, Alok Aggarwal, Yanping Huang and Quoc V Le", "title": "Regularized Evolution for Image Classifier Architecture Search", "comments": "Accepted for publication at AAAI 2019, the Thirty-Third AAAI\n  Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effort devoted to hand-crafting neural network image classifiers has\nmotivated the use of architecture search to discover them automatically.\nAlthough evolutionary algorithms have been repeatedly applied to neural network\ntopologies, the image classifiers thus discovered have remained inferior to\nhuman-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that\nsurpasses hand-designs for the first time. To do this, we modify the tournament\nselection evolutionary algorithm by introducing an age property to favor the\nyounger genotypes. Matching size, AmoebaNet-A has comparable accuracy to\ncurrent state-of-the-art ImageNet models discovered with more complex\narchitecture-search methods. Scaled to larger size, AmoebaNet-A sets a new\nstate-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled\ncomparison against a well known reinforcement learning algorithm, we give\nevidence that evolution can obtain results faster with the same hardware,\nespecially at the earlier stages of the search. This is relevant when fewer\ncompute resources are available. Evolution is, thus, a simple method to\neffectively discover high-quality architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 18:20:52 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:24:29 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 00:10:00 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 06:21:47 GMT"}, {"version": "v5", "created": "Thu, 4 Oct 2018 00:11:37 GMT"}, {"version": "v6", "created": "Fri, 26 Oct 2018 05:56:00 GMT"}, {"version": "v7", "created": "Sat, 16 Feb 2019 23:28:16 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Real", "Esteban", ""], ["Aggarwal", "Alok", ""], ["Huang", "Yanping", ""], ["Le", "Quoc V", ""]]}, {"id": "1802.02006", "submitter": "Sayan Nag", "authors": "Uddalok Sarkar, Sayan Nag", "title": "An Adaptive Genetic Algorithm for Solving N-Queens Problem", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a Metaheuristic approach for solving the N-Queens Problem is\nintroduced to find the best possible solution in a reasonable amount of time.\nGenetic Algorithm is used with a novel fitness function as the Metaheuristic.\nThe aim of N-Queens Problem is to place N queens on an N x N chessboard, in a\nway so that no queen is in conflict with the others. Chromosome representation\nand genetic operations like Mutation and Crossover are described in detail.\nResults show that this approach yields promising and satisfactory results in\nless time compared to that obtained from the previous approaches for several\nlarge values of N.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:46:27 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Sarkar", "Uddalok", ""], ["Nag", "Sayan", ""]]}, {"id": "1802.02026", "submitter": "Piotr Antonik", "authors": "Piotr Antonik, Marc Haelterman, and Serge Massar", "title": "Brain-inspired photonic signal processor for periodic pattern generation\n  and chaotic system emulation", "comments": "16 pages, 18 figures", "journal-ref": "Phys. Rev. Applied 7, 054014 -- Published 24 May 2017", "doi": "10.1103/PhysRevApplied.7.054014", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a bio-inspired computing paradigm for processing\ntime-dependent signals. Its hardware implementations have received much\nattention because of their simplicity and remarkable performance on a series of\nbenchmark tasks. In previous experiments the output was uncoupled from the\nsystem and in most cases simply computed offline on a post-processing computer.\nHowever, numerical investigations have shown that feeding the output back into\nthe reservoir would open the possibility of long-horizon time series\nforecasting. Here we present a photonic reservoir computer with output\nfeedback, and demonstrate its capacity to generate periodic time series and to\nemulate chaotic systems. We study in detail the effect of experimental noise on\nsystem performance. In the case of chaotic systems, this leads us to introduce\nseveral metrics, based on standard signal processing techniques, to evaluate\nthe quality of the emulation. Our work significantly enlarges the range of\ntasks that can be solved by hardware reservoir computers, and therefore the\nrange of applications they could potentially tackle. It also raises novel\nquestions in nonlinear dynamics and chaos theory.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 16:08:32 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Antonik", "Piotr", ""], ["Haelterman", "Marc", ""], ["Massar", "Serge", ""]]}, {"id": "1802.02178", "submitter": "Ruizhou Ding", "authors": "Ruizhou Ding, Zeye Liu, Rongye Shi, Diana Marculescu, and R. D.\n  Blanton", "title": "LightNN: Filling the Gap between Conventional Deep Neural Networks and\n  Binarized Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application-specific integrated circuit (ASIC) implementations for Deep\nNeural Networks (DNNs) have been adopted in many systems because of their\nhigher classification speed. However, although they may be characterized by\nbetter accuracy, larger DNNs require significant energy and area, thereby\nlimiting their wide adoption. The energy consumption of DNNs is driven by both\nmemory accesses and computation. Binarized Neural Networks (BNNs), as a\ntrade-off between accuracy and energy consumption, can achieve great energy\nreduction, and have good accuracy for large DNNs due to its regularization\neffect. However, BNNs show poor accuracy when a smaller DNN configuration is\nadopted. In this paper, we propose a new DNN model, LightNN, which replaces the\nmultiplications to one shift or a constrained number of shifts and adds. For a\nfixed DNN configuration, LightNNs have better accuracy at a slight energy\nincrease than BNNs, yet are more energy efficient with only slightly less\naccuracy than conventional DNNs. Therefore, LightNNs provide more options for\nhardware designers to make trade-offs between accuracy and energy. Moreover,\nfor large DNN configurations, LightNNs have a regularization effect, making\nthem better in accuracy than conventional DNNs. These conclusions are verified\nby experiment using the MNIST and CIFAR-10 datasets for different DNN\nconfigurations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 21:34:39 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Ding", "Ruizhou", ""], ["Liu", "Zeye", ""], ["Shi", "Rongye", ""], ["Marculescu", "Diana", ""], ["Blanton", "R. D.", ""]]}, {"id": "1802.02195", "submitter": "Patrick Schwab", "authors": "Patrick Schwab, Djordje Miladinovic, Walter Karlen", "title": "Granger-causal Attentive Mixtures of Experts: Learning Important\n  Features with Neural Networks", "comments": "AAAI Conference on Artificial Intelligence 2019", "journal-ref": null, "doi": "10.1609/aaai.v33i01.33014846", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the importance of input features towards decisions made by\nmachine-learning models is essential to increase our understanding of both the\nmodels and the underlying data. Here, we present a new approach to estimating\nfeature importance with neural networks based on the idea of distributing the\nfeatures of interest among experts in an attentive mixture of experts (AME).\nAMEs use attentive gating networks trained with a Granger-causal objective to\nlearn to jointly produce accurate predictions as well as estimates of feature\nimportance in a single model. Our experiments show (i) that the feature\nimportance estimates provided by AMEs compare favourably to those provided by\nstate-of-the-art methods, (ii) that AMEs are significantly faster at estimating\nfeature importance than existing methods, and (iii) that the associations\ndiscovered by AMEs are consistent with those reported by domain experts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 20:21:30 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 11:18:22 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 12:49:54 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 12:15:56 GMT"}, {"version": "v5", "created": "Mon, 1 Oct 2018 12:36:39 GMT"}, {"version": "v6", "created": "Wed, 14 Nov 2018 23:57:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Schwab", "Patrick", ""], ["Miladinovic", "Djordje", ""], ["Karlen", "Walter", ""]]}, {"id": "1802.02203", "submitter": "Yang Hu Dr.", "authors": "Yang Hu, Guihua Wen, Huiqiang Liao, Changjun Wang, Dan Dai, Zhiwen Yu", "title": "Automatic construction of Chinese herbal prescription from tongue image\n  via CNNs and auxiliary latent therapy topics", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The tongue image provides important physical information of humans. It is of\ngreat importance for diagnoses and treatments in clinical medicine. Herbal\nprescriptions are simple, noninvasive and have low side effects. Thus, they are\nwidely applied in China. Studies on the automatic construction technology of\nherbal prescriptions based on tongue images have great significance for deep\nlearning to explore the relevance of tongue images for herbal prescriptions, it\ncan be applied to healthcare services in mobile medical systems. In order to\nadapt to the tongue image in a variety of photographic environments and\nconstruct herbal prescriptions, a neural network framework for prescription\nconstruction is designed. It includes single/double convolution channels and\nfully connected layers. Furthermore, it proposes the auxiliary therapy topic\nloss mechanism to model the therapy of Chinese doctors and alleviate the\ninterference of sparse output labels on the diversity of results. The\nexperiment use the real world tongue images and the corresponding prescriptions\nand the results can generate prescriptions that are close to the real samples,\nwhich verifies the feasibility of the proposed method for the automatic\nconstruction of herbal prescriptions from tongue images. Also, it provides a\nreference for automatic herbal prescription construction from more physical\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 08:21:04 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 05:43:28 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 08:39:35 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 09:37:56 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Hu", "Yang", ""], ["Wen", "Guihua", ""], ["Liao", "Huiqiang", ""], ["Wang", "Changjun", ""], ["Dai", "Dan", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1802.02271", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Universal Deep Neural Network Compression", "comments": "NeurIPS 2018 Workshop on Compact Deep Neural Network Representation\n  with Industrial Applications (CDNNRIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate lossy compression of deep neural networks\n(DNNs) by weight quantization and lossless source coding for memory-efficient\ndeployment. Whereas the previous work addressed non-universal scalar\nquantization and entropy coding of DNN weights, we for the first time introduce\nuniversal DNN compression by universal vector quantization and universal source\ncoding. In particular, we examine universal randomized lattice quantization of\nDNNs, which randomizes DNN weights by uniform random dithering before lattice\nquantization and can perform near-optimally on any source without relying on\nknowledge of its probability distribution. Moreover, we present a method of\nfine-tuning vector quantized DNNs to recover the performance loss after\nquantization. Our experimental results show that the proposed universal DNN\ncompression scheme compresses the 32-layer ResNet (trained on CIFAR-10) and the\nAlexNet (trained on ImageNet) with compression ratios of $47.1$ and $42.5$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 00:39:56 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 01:33:44 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1802.02342", "submitter": "Vishal Saxena", "authors": "Vishal Saxena, Xinyu Wu, and Kehan Zhu", "title": "Energy-Efficient CMOS Memristive Synapses for Mixed-Signal Neuromorphic\n  System-on-a-Chip", "comments": "This is a preprint of proceedings in IEEE International Symposium on\n  Circuits and Systems (ISCAS), May 2018. Copyright 2018 IEEE. Personal use is\n  permitted, but republication/redistribution requires IEEE permission. See\n  \\URL{http://www.ieee.org/publications\\_standards/publications/rights/index.html}\n  for more information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging non-volatile memory (NVM), or memristive, devices promise\nenergy-efficient realization of deep learning, when efficiently integrated with\nmixed-signal integrated circuits on a CMOS substrate. Even though several\nalgorithmic challenges need to be addressed to turn the vision of memristive\nNeuromorphic Systems-on-a-Chip (NeuSoCs) into reality, issues at the device and\ncircuit interface need immediate attention from the community. In this work, we\nperform energy-estimation of a NeuSoC system and predict the desirable circuit\nand device parameters for energy-efficiency optimization. Also, CMOS synapse\ncircuits based on the concept of CMOS memristor emulator are presented as a\nsystem prototyping methodology, while practical memristor devices are being\ndeveloped and integrated with general-purpose CMOS. The proposed mixed-signal\nmemristive synapse can be designed and fabricated using standard CMOS\ntechnologies and open doors to interesting applications in cognitive computing\ncircuits.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 08:20:51 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 09:32:18 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 09:33:38 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Saxena", "Vishal", ""], ["Wu", "Xinyu", ""], ["Zhu", "Kehan", ""]]}, {"id": "1802.02528", "submitter": "Rahul Parundekar", "authors": "Rahul Parundekar", "title": "Classification of Things in DBpedia using Deep Neural Networks", "comments": null, "journal-ref": "Classification of Things in DBpedia using Deep Neural Networks, R\n  Parundekar, International Journal of Web & Semantic Technology (IJWesT)\n  Vol.9, No.1, January 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web aims at representing knowledge about the real world at web\nscale - things, their attributes and relationships among them can be\nrepresented as nodes and edges in an inter-linked semantic graph. In the\npresence of noisy data, as is typical of data on the Semantic Web, a software\nAgent needs to be able to robustly infer one or more associated actionable\nclasses for the individuals in order to act automatically on it. We model this\nproblem as a multi-label classification task where we want to robustly identify\ntypes of the individuals in a semantic graph such as DBpedia, which we use as\nan exemplary dataset on the Semantic Web. Our approach first extracts multiple\nfeatures for the individuals using random walks and then performs multi-label\nclassification using fully-connected Neural Networks. Through systematic\nexploration and experimentation, we identify the effect of hyper-parameters of\nthe feature extraction and the fully-connected Neural Network structure on the\nclassification performance. Our final results show that our method performs\nbetter than state-of-the-art inferencing systems like SDtype and SLCN, from\nwhich we can conclude that random-walk-based feature extraction of individuals\nand their multi-label classification using Deep Neural Networks is a promising\nalternative to these systems for type classification of individuals on the\nSemantic Web. The main contribution of our work is to introduce a novel\napproach that allows us to use Deep Neural Networks to identify types of\nindividuals in a noisy semantic graph by extracting features using random walks\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:12:54 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Parundekar", "Rahul", ""]]}, {"id": "1802.02678", "submitter": "Charles Delahunt", "authors": "Charles B. Delahunt, Jeffrey A. Riffell, J. Nathan Kutz", "title": "Biological Mechanisms for Learning: A Computational Model of Olfactory\n  Learning in the Manduca sexta Moth, with Applications to Neural Nets", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insect olfactory system, which includes the antennal lobe (AL), mushroom\nbody (MB), and ancillary structures, is a relatively simple neural system\ncapable of learning. Its structural features, which are widespread in\nbiological neural systems, process olfactory stimuli through a cascade of\nnetworks where large dimension shifts occur from stage to stage and where\nsparsity and randomness play a critical role in coding. Learning is partly\nenabled by a neuromodulatory reward mechanism of octopamine stimulation of the\nAL, whose increased activity induces rewiring of the MB through Hebbian\nplasticity. Enforced sparsity in the MB focuses Hebbian growth on neurons that\nare the most important for the representation of the learned odor. Based upon\ncurrent biophysical knowledge, we have constructed an end-to-end computational\nmodel of the Manduca sexta moth olfactory system which includes the interaction\nof the AL and MB under octopamine stimulation. Our model is able to robustly\nlearn new odors, and our simulations of integrate-and-fire neurons match the\nstatistical features of in-vivo firing rate data. From a biological\nperspective, the model provides a valuable tool for examining the role of\nneuromodulators, like octopamine, in learning, and gives insight into critical\ninteractions between sparsity, Hebbian growth, and stimulation during learning.\nOur simulations also inform predictions about structural details of the\nolfactory system that are not currently well-characterized. From a machine\nlearning perspective, the model yields bio-inspired mechanisms that are\npotentially useful in constructing neural nets for rapid learning from very few\nsamples. These mechanisms include high-noise layers, sparse layers as noise\nfilters, and a biologically-plausible optimization method to train the network\nbased on octopamine stimulation, sparse layers, and Hebbian growth.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 00:16:31 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Delahunt", "Charles B.", ""], ["Riffell", "Jeffrey A.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1802.02844", "submitter": "Piotr Antonik", "authors": "Piotr Antonik, Marvyn Gulina, Ja\\\"el Pauwels, and Serge Massar", "title": "Using a reservoir computer to learn chaotic attractors, with\n  applications to chaos synchronisation and cryptography", "comments": "10 pages, 6 figures", "journal-ref": "Phys. Rev. E 98, 012215 (2018)", "doi": "10.1103/PhysRevE.98.012215", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the machine learning approach known as reservoir computing, it is\npossible to train one dynamical system to emulate another. We show that such\ntrained reservoir computers reproduce the properties of the attractor of the\nchaotic system sufficiently well to exhibit chaos synchronisation. That is, the\ntrained reservoir computer, weakly driven by the chaotic system, will\nsynchronise with the chaotic system. Conversely, the chaotic system, weakly\ndriven by a trained reservoir computer, will synchronise with the reservoir\ncomputer. We illustrate this behaviour on the Mackey-Glass and Lorenz systems.\nWe then show that trained reservoir computers can be used to crack chaos based\ncryptography and illustrate this on a chaos cryptosystem based on the\nMackey-Glass system. We conclude by discussing why reservoir computers are so\ngood at emulating chaotic systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 13:28:46 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 09:36:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Antonik", "Piotr", ""], ["Gulina", "Marvyn", ""], ["Pauwels", "Ja\u00ebl", ""], ["Massar", "Serge", ""]]}, {"id": "1802.02987", "submitter": "HyeonSeok Lee", "authors": "HyeonSeok Lee, Hyo Seon Park", "title": "A Generalization Method of Partitioned Activation Function for Complex\n  Number", "comments": "Complex Activation Function, Holomorphic, Phase-preserving,\n  real-complex interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI math.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method to convert real number partitioned activation function into complex\nnumber one is provided. The method has 4em variations; 1 has potential to get\nholomorphic activation, 2 has potential to conserve complex angle, and the last\n1 guarantees interaction between real and imaginary parts. The method has been\napplied to LReLU and SELU as examples. The complex number activation function\nis an building block of complex number ANN, which has potential to properly\ndeal with complex number problems. But the complex activation is not well\nestablished yet. Therefore, we propose a way to extend the partitioned real\nactivation to complex number.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 17:57:58 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Lee", "HyeonSeok", ""], ["Park", "Hyo Seon", ""]]}, {"id": "1802.03039", "submitter": "Akisato Kimura", "authors": "Akisato Kimura, Zoubin Ghahramani, Koh Takeuchi, Tomoharu Iwata,\n  Naonori Ueda", "title": "Few-shot learning of neural networks from scratch by pseudo example\n  optimization", "comments": "14 pages, 2 figures, will be presented at BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple but effective method for training neural\nnetworks with a limited amount of training data. Our approach inherits the idea\nof knowledge distillation that transfers knowledge from a deep or wide\nreference model to a shallow or narrow target model. The proposed method\nemploys this idea to mimic predictions of reference estimators that are more\nrobust against overfitting than the network we want to train. Different from\nalmost all the previous work for knowledge distillation that requires a large\namount of labeled training data, the proposed method requires only a small\namount of training data. Instead, we introduce pseudo training examples that\nare optimized as a part of model parameters. Experimental results for several\nbenchmark datasets demonstrate that the proposed method outperformed all the\nother baselines, such as naive training of the target model and standard\nknowledge distillation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 20:28:01 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:00:17 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 15:13:58 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Kimura", "Akisato", ""], ["Ghahramani", "Zoubin", ""], ["Takeuchi", "Koh", ""], ["Iwata", "Tomoharu", ""], ["Ueda", "Naonori", ""]]}, {"id": "1802.03155", "submitter": "Novanto Yudistira", "authors": "Aryo Pinandito, Novanto Yudistira, Fajar Pradana", "title": "Web-Based Implementation of Travelling Salesperson Problem Using Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is connected through the Internet. As the abundance of Internet\nusers connected into the Web and the popularity of cloud computing research,\nthe need of Artificial Intelligence (AI) is demanding. In this research,\nGenetic Algorithm (GA) as AI optimization method through natural selection and\ngenetic evolution is utilized. There are many applications of GA such as web\nmining, load balancing, routing, and scheduling or web service selection.\nHence, it is a challenging task to discover whether the code mainly server side\nand web based language technology affects the performance of GA. Travelling\nSalesperson Problem (TSP) as Non Polynomial-hard (NP-hard) problem is provided\nto be a problem domain to be solved by GA. While many scientists prefer Python\nin GA implementation, another popular high-level interpreter programming\nlanguage such as PHP (PHP Hypertext Preprocessor) and Ruby were benchmarked.\nLine of codes, file sizes, and performances based on GA implementation and\nruntime were found varies among these programming languages. Based on the\nresult, the use of Ruby in GA implementation is recommended.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:30:38 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Pinandito", "Aryo", ""], ["Yudistira", "Novanto", ""], ["Pradana", "Fajar", ""]]}, {"id": "1802.03209", "submitter": "Youhei Akimoto", "authors": "Youhei Akimoto, Anne Auger, Tobias Glasmachers", "title": "Drift Theory in Continuous Search Spaces: Expected Hitting Time of the\n  (1+1)-ES with 1/5 Success Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of the standard approach for proving runtime\nbounds in discrete domains---often referred to as drift analysis---in the\ncontext of optimization on a continuous domain. Using this framework we analyze\nthe (1+1) Evolution Strategy with one-fifth success rule on the sphere\nfunction. To deal with potential functions that are not lower-bounded, we\nformulate novel drift theorems. We then use the theorems to prove bounds on the\nexpected hitting time to reach a certain target fitness in finite dimension\n$d$. The bounds are akin to linear convergence. We then study the dependency of\nthe different terms on $d$ proving a convergence rate dependency of\n$\\Theta(1/d)$. Our results constitute the first non-asymptotic analysis for the\nalgorithm considered as well as the first explicit application of drift\nanalysis to a randomized search heuristic with continuous domain.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 11:28:50 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 12:23:44 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 09:24:05 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 23:00:06 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Akimoto", "Youhei", ""], ["Auger", "Anne", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "1802.03268", "submitter": "Hieu Pham", "authors": "Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean", "title": "Efficient Neural Architecture Search via Parameter Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:14:37 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 03:34:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Pham", "Hieu", ""], ["Guan", "Melody Y.", ""], ["Zoph", "Barret", ""], ["Le", "Quoc V.", ""], ["Dean", "Jeff", ""]]}, {"id": "1802.03308", "submitter": "Frieder Stolzenburg", "authors": "Frieder Stolzenburg, Sandra Litz, Olivia Michael, Oliver Obst", "title": "The Power of Linear Recurrent Neural Networks", "comments": "34 pages, 13 figures, 2 tables, revised implementation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are a powerful means to cope with time series. We\nshow how linear, i.e., linearly activated recurrent neural networks (LRNNs) can\napproximate any time-dependent function f(t) given by a number of function\nvalues. The approximation can effectively be learned by simply solving a linear\nequation system; no backpropagation or similar methods are needed. Furthermore,\nthe size of an LRNN can be reduced significantly in one step, after inspecting\nthe eigenvalues of the network transition matrix, by taking only the most\nrelevant components. Therefore, in contrast to others, we do not only learn\nnetwork weights but also the network architecture. LRNNs have interesting\nproperties: They end up in ellipse trajectories in the long run and allow the\nprediction of further values and compact representations of functions. We\ndemonstrate this by several experiments, among them multiple superimposed\noscillators (MSO), robotic soccer, and predicting stock prices. LRNNs\noutperform the previous state-of-the-art for the MSO task with a minimal number\nof units.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:35:41 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 06:28:35 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 11:51:16 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 14:17:57 GMT"}, {"version": "v5", "created": "Tue, 4 May 2021 11:34:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Stolzenburg", "Frieder", ""], ["Litz", "Sandra", ""], ["Michael", "Olivia", ""], ["Obst", "Oliver", ""]]}, {"id": "1802.03318", "submitter": "Alexander Wong", "authors": "Audrey G. Chung, Paul Fieguth, and Alexander Wong", "title": "Nature vs. Nurture: The Role of Environmental Resources in Evolutionary\n  Deep Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary deep intelligence synthesizes highly efficient deep neural\nnetworks architectures over successive generations. Inspired by the nature\nversus nurture debate, we propose a study to examine the role of external\nfactors on the network synthesis process by varying the availability of\nsimulated environmental resources. Experimental results were obtained for\nnetworks synthesized via asexual evolutionary synthesis (1-parent) and sexual\nevolutionary synthesis (2-parent, 3-parent, and 5-parent) using a 10% subset of\nthe MNIST dataset. Results show that a lower environmental factor model\nresulted in a more gradual loss in performance accuracy and decrease in storage\nsize. This potentially allows significantly reduced storage size with minimal\nto no drop in performance accuracy, and the best networks were synthesized\nusing the lowest environmental factor models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:58:58 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Chung", "Audrey G.", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1802.03480", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Nikos Komodakis", "title": "GraphVAE: Towards Generation of Small Graphs Using Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on graphs has become a popular research topic with many\napplications. However, past work has concentrated on learning graph embedding\ntasks, which is in contrast with advances in generative models for images and\ntext. Is it possible to transfer this progress to the domain of graphs? We\npropose to sidestep hurdles associated with linearization of such discrete\nstructures by having a decoder output a probabilistic fully-connected graph of\na predefined maximum size directly at once. Our method is formulated as a\nvariational autoencoder. We evaluate on the challenging task of molecule\ngeneration.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:57:46 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1802.03505", "submitter": "Emanuele Sansone", "authors": "Emanuele Sansone and Hafiz Tiomoko Ali and Sun Jiacheng", "title": "Coulomb Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the true density in high-dimensional feature spaces is a well-known\nproblem in machine learning. In this work, we consider generative autoencoders\nbased on maximum-mean discrepancy (MMD) and provide theoretical insights. In\nparticular, (i) we prove that MMD coupled with Coulomb kernels has optimal\nconvergence properties, which are similar to convex functionals, thus improving\nthe training of autoencoders, and (ii) we provide a probabilistic bound on the\ngeneralization performance, highlighting some fundamental conditions to achieve\nbetter generalization. We validate the theory on synthetic examples and on the\npopular dataset of celebrities' faces, showing that our model, called Coulomb\nautoencoders, outperform the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 02:37:31 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 15:18:59 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 14:18:52 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 12:32:58 GMT"}, {"version": "v5", "created": "Thu, 21 Feb 2019 12:07:34 GMT"}, {"version": "v6", "created": "Tue, 26 Nov 2019 10:25:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sansone", "Emanuele", ""], ["Ali", "Hafiz Tiomoko", ""], ["Jiacheng", "Sun", ""]]}, {"id": "1802.03608", "submitter": "Wenji Li", "authors": "Zhun Fan, Yi Fang, Wenji Li, Xinye Cai, Caimin Wei, Erik Goodman", "title": "MOEA/D with Angle-based Constrained Dominance Principle for Constrained\n  Multi-objective Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel constraint-handling mechanism named angle-based\nconstrained dominance principle (ACDP) embedded in a decomposition-based\nmulti-objective evolutionary algorithm (MOEA/D) to solve constrained\nmulti-objective optimization problems (CMOPs). To maintain the diversity of the\nworking population, ACDP utilizes the information of the angle of solutions to\nadjust the dominance relation of solutions during the evolutionary process.\nThis paper uses 14 benchmark instances to evaluate the performance of the\nMOEA/D with ACDP (MOEA/D-ACDP). Additionally, an engineering optimization\nproblem (which is I-beam optimization problem) is optimized. The proposed\nMOEA/D-ACDP, and four other decomposition-based CMOEAs, including C-MOEA/D,\nMOEA/D-CDP, MOEA/D-Epsilon and MOEA/D-SR are tested by the above benchmarks and\nthe engineering application. The experimental results manifest that MOEA/D-ACDP\nis significantly better than the other four CMOEAs on these test instances and\nthe real-world case, which indicates that ACDP is more effective for solving\nCMOPs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 15:12:03 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Fan", "Zhun", ""], ["Fang", "Yi", ""], ["Li", "Wenji", ""], ["Cai", "Xinye", ""], ["Wei", "Caimin", ""], ["Goodman", "Erik", ""]]}, {"id": "1802.03620", "submitter": "Dmitry Yarotsky", "authors": "Dmitry Yarotsky", "title": "Optimal approximation of continuous functions by very deep ReLU networks", "comments": "21 pages. In v2: more discussion of phase diagram, described\n  approximation for $p\\in(\\frac{1}{\\nu},\\frac{2}{\\nu})$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximations of general continuous functions on\nfinite-dimensional cubes by general deep ReLU neural networks and study the\napproximation rates with respect to the modulus of continuity of the function\nand the total number of weights $W$ in the network. We establish the complete\nphase diagram of feasible approximation rates and show that it includes two\ndistinct phases. One phase corresponds to slower approximations that can be\nachieved with constant-depth networks and continuous weight assignments. The\nother phase provides faster approximations at the cost of depths necessarily\ngrowing as a power law $L\\sim W^{\\alpha}, 0<\\alpha\\le 1,$ and with necessarily\ndiscontinuous weight assignments. In particular, we prove that constant-width\nfully-connected networks of depth $L\\sim W$ provide the fastest possible\napproximation rate $\\|f-\\widetilde f\\|_\\infty = O(\\omega_f(O(W^{-2/\\nu})))$\nthat cannot be achieved with less deep networks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 17:01:08 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 19:35:14 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Yarotsky", "Dmitry", ""]]}, {"id": "1802.03806", "submitter": "Jeff  (Jun) Zhang", "authors": "Jeff Zhang, Kartheek Rangineni, Zahra Ghodsi, Siddharth Garg", "title": "ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error\n  Resilience for Energy Efficient Deep Neural Network Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware accelerators are being increasingly deployed to boost the\nperformance and energy efficiency of deep neural network (DNN) inference. In\nthis paper we propose Thundervolt, a new framework that enables aggressive\nvoltage underscaling of high-performance DNN accelerators without compromising\nclassification accuracy even in the presence of high timing error rates. Using\npost-synthesis timing simulations of a DNN accelerator modeled on the Google\nTPU, we show that Thundervolt enables between 34%-57% energy savings on\nstate-of-the-art speech and image recognition benchmarks with less than 1% loss\nin classification accuracy and no performance loss. Further, we show that\nThundervolt is synergistic with and can further increase the energy efficiency\nof commonly used run-time DNN pruning techniques like Zero-Skip.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:51:59 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 04:10:47 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zhang", "Jeff", ""], ["Rangineni", "Kartheek", ""], ["Ghodsi", "Zahra", ""], ["Garg", "Siddharth", ""]]}, {"id": "1802.03891", "submitter": "Madhavun Candadai", "authors": "Madhavun Candadai and Eduardo Izquierdo", "title": "Multifunctionality in embodied agents: Three levels of neural reuse", "comments": "Accepted at Cognitive Science Conference, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain in conjunction with the body is able to adapt to new environments\nand perform multiple behaviors through reuse of neural resources and transfer\nof existing behavioral traits. Although mechanisms that underlie this ability\nare not well understood, they are largely attributed to neuromodulation. In\nthis work, we demonstrate that an agent can be multifunctional using the same\nsensory and motor systems across behaviors, in the absence of modulatory\nmechanisms. Further, we lay out the different levels at which neural reuse can\noccur through a dynamical filtering of the brain-body-environment system's\noperation: structural network, autonomous dynamics, and transient dynamics.\nNotably, transient dynamics reuse could only be explained by studying the\nbrain-body-environment system as a whole and not just the brain. The\nmultifunctional agent we present here demonstrates neural reuse at all three\nlevels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:41:18 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 17:44:36 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 01:44:56 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 00:43:18 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Candadai", "Madhavun", ""], ["Izquierdo", "Eduardo", ""]]}, {"id": "1802.03916", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Yu-Xiang Wang, Alex Smola", "title": "Detecting and Correcting for Label Shift with Black Box Predictors", "comments": "Published at the International Conference on Machine Learning (ICML)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with distribution shift between training and test set, we wish to\ndetect and quantify the shift, and to correct our classifiers without test set\nlabels. Motivated by medical diagnosis, where diseases (targets) cause symptoms\n(observations), we focus on label shift, where the label marginal $p(y)$\nchanges but the conditional $p(x| y)$ does not. We propose Black Box Shift\nEstimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits\narbitrary black box predictors to reduce dimensionality prior to shift\ncorrection. While better predictors give tighter estimates, BBSE works even\nwhen predictors are biased, inaccurate, or uncalibrated, so long as their\nconfusion matrices are invertible. We prove BBSE's consistency, bound its\nerror, and introduce a statistical test that uses BBSE to detect shift. We also\nleverage BBSE to correct classifiers. Experiments demonstrate accurate\nestimates and improved prediction, even on high-dimensional datasets of natural\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 07:16:03 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 17:23:51 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 12:51:37 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Wang", "Yu-Xiang", ""], ["Smola", "Alex", ""]]}, {"id": "1802.04051", "submitter": "Jaehun Kim", "authors": "Jaehun Kim (1), Juli\\'an Urbano (1), Cynthia C. S. Liem (1), Alan\n  Hanjalic (1) ((1) Delft University of Technology)", "title": "One Deep Music Representation to Rule Them All? : A comparative analysis\n  of different representation learning strategies", "comments": "This work has been accepted to \"Neural Computing and Applications:\n  Special Issue on Deep Learning for Music and Audio\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by the success of deploying deep learning in the fields of Computer\nVision and Natural Language Processing, this learning paradigm has also found\nits way into the field of Music Information Retrieval. In order to benefit from\ndeep learning in an effective, but also efficient manner, deep transfer\nlearning has become a common approach. In this approach, it is possible to\nreuse the output of a pre-trained neural network as the basis for a new\nlearning task. The underlying hypothesis is that if the initial and new\nlearning tasks show commonalities and are applied to the same type of input\ndata (e.g. music audio), the generated deep representation of the data is also\ninformative for the new task. Since, however, most of the networks used to\ngenerate deep representations are trained using a single initial learning\nsource, their representation is unlikely to be informative for all possible\nfuture tasks. In this paper, we present the results of our investigation of\nwhat are the most important factors to generate deep representations for the\ndata and learning tasks in the music domain. We conducted this investigation\nvia an extensive empirical study that involves multiple learning sources, as\nwell as multiple deep learning architectures with varying levels of information\nsharing between sources, in order to learn music representations. We then\nvalidate these representations considering multiple target datasets for\nevaluation. The results of our experiments yield several insights on how to\napproach the design of methods for learning widely deployable deep data\nrepresentations in the music domain.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:08:54 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 10:52:55 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 19:39:47 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 23:26:08 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Kim", "Jaehun", "", "Delft University of Technology"], ["Urbano", "Juli\u00e1n", "", "Delft University of Technology"], ["Liem", "Cynthia C. S.", "", "Delft University of Technology"], ["Hanjalic", "Alan", "", "Delft University of Technology"]]}, {"id": "1802.04364", "submitter": "Wengong Jin", "authors": "Wengong Jin, Regina Barzilay, Tommi Jaakkola", "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to automate the design of molecules based on specific chemical\nproperties. In computational terms, this task involves continuous embedding and\ngeneration of molecular graphs. Our primary contribution is the direct\nrealization of molecular graphs, a task previously approached by generating\nlinear SMILES strings instead of graphs. Our junction tree variational\nautoencoder generates molecular graphs in two phases, by first generating a\ntree-structured scaffold over chemical substructures, and then combining them\ninto a molecule with a graph message passing network. This approach allows us\nto incrementally expand molecules while maintaining chemical validity at every\nstep. We evaluate our model on multiple tasks ranging from molecular generation\nto optimization. Across these tasks, our model outperforms previous\nstate-of-the-art baselines by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:19:39 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 00:17:35 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 17:20:47 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 14:44:43 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Jin", "Wengong", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1802.04443", "submitter": "William Guss", "authors": "William H. Guss, Ruslan Salakhutdinov", "title": "On Characterizing the Capacity of Neural Networks using Algebraic\n  Topology", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.NE math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learnability of different neural architectures can be characterized\ndirectly by computable measures of data complexity. In this paper, we reframe\nthe problem of architecture selection as understanding how data determines the\nmost expressive and generalizable architectures suited to that data, beyond\ninductive bias. After suggesting algebraic topology as a measure for data\ncomplexity, we show that the power of a network to express the topological\ncomplexity of a dataset in its decision region is a strictly limiting factor in\nits ability to generalize. We then provide the first empirical characterization\nof the topological capacity of neural networks. Our empirical analysis shows\nthat at every level of dataset complexity, neural networks exhibit topological\nphase transitions. This observation allowed us to connect existing theory to\nempirically driven conjectures on the choice of architectures for\nfully-connected neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 02:32:10 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Guss", "William H.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1802.04491", "submitter": "Bin Han", "authors": "Bin Han, Lianghai Ji, Hans D. Schotten", "title": "Slice as an Evolutionary Service: Genetic Optimization for Inter-Slice\n  Resource Management in 5G Networks", "comments": "First version (v1) submitted to IEEE Access on 09-Feb-2018,\n  resubmission (v2) on 09-May-2018, acceptance (v3) on 08-June-2018", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2846543", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Fifth Generation (5G) mobile networks, the concept of\n\"Slice as a Service\" (SlaaS) promotes mobile network operators to flexibly\nshare infrastructures with mobile service providers and stakeholders. However,\nit also challenges with an emerging demand for efficient online algorithms to\noptimize the request-and-decision-based inter-slice resource management\nstrategy. Based on genetic algorithms, this paper presents a novel online\noptimizer that efficiently approaches towards the ideal slicing strategy with\nmaximized long-term network utility. The proposed method encodes slicing\nstrategies into binary sequences to cope with the request-and-decision\nmechanism. It requires no a priori knowledge about the traffic/utility models,\nand therefore supports heterogeneous slices, while providing solid\neffectiveness, good robustness against non-stationary service scenarios, and\nhigh scalability.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 07:44:13 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 09:57:34 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 12:51:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Han", "Bin", ""], ["Ji", "Lianghai", ""], ["Schotten", "Hans D.", ""]]}, {"id": "1802.04657", "submitter": "Jeff  (Jun) Zhang", "authors": "Jeff Zhang, Tianyu Gu, Kanad Basu, Siddharth Garg", "title": "Analyzing and Mitigating the Impact of Permanent Faults on a Systolic\n  Array Based Neural Network Accelerator", "comments": "To appear at IEEE VLSI Test Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their growing popularity and computational cost, deep neural networks\n(DNNs) are being targeted for hardware acceleration. A popular architecture for\nDNN acceleration, adopted by the Google Tensor Processing Unit (TPU), utilizes\na systolic array based matrix multiplication unit at its core. This paper deals\nwith the design of fault-tolerant, systolic array based DNN accelerators for\nhigh defect rate technologies. To this end, we empirically show that the\nclassification accuracy of a baseline TPU drops significantly even at extremely\nlow fault rates (as low as $0.006\\%$). We then propose two novel strategies,\nfault-aware pruning (FAP) and fault-aware pruning+retraining (FAP+T), that\nenable the TPU to operate at fault rates of up to $50\\%$, with negligible drop\nin classification accuracy (as low as $0.1\\%$) and no run-time performance\noverhead. The FAP+T does introduce a one-time retraining penalty per TPU chip\nbefore it is deployed, but we propose optimizations that reduce this one-time\npenalty to under 12 minutes. The penalty is then amortized over the entire\nlifetime of the TPU's operation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:51:35 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 06:19:12 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Zhang", "Jeff", ""], ["Gu", "Tianyu", ""], ["Basu", "Kanad", ""], ["Garg", "Siddharth", ""]]}, {"id": "1802.04741", "submitter": "Amir Bennatan", "authors": "Amir Bennatan, Yoni Choukroun and Pavel Kisilev", "title": "Deep Learning for Decoding of Linear Codes - A Syndrome-Based Approach", "comments": "The first two authors contributed equally to this work. A shortened\n  version was submitted to the ISIT 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for applying deep neural networks (DNN) to soft\ndecoding of linear codes at arbitrary block lengths. Unlike other approaches,\nour framework allows unconstrained DNN design, enabling the free application of\npowerful designs that were developed in other contexts. Our method is robust to\noverfitting that inhibits many competing methods, which follows from the\nexponentially large number of codewords required for their training. We achieve\nthis by transforming the channel output before feeding it to the network,\nextracting only the syndrome of the hard decisions and the channel output\nreliabilities. We prove analytically that this approach does not involve any\nintrinsic performance penalty, and guarantees the generalization of performance\nobtained during training. Our best results are obtained using a recurrent\nneural network (RNN) architecture combined with simple preprocessing by\npermutation. We provide simulation results that demonstrate performance that\nsometimes approaches that of the ordered statistics decoding (OSD) algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 17:06:40 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bennatan", "Amir", ""], ["Choukroun", "Yoni", ""], ["Kisilev", "Pavel", ""]]}, {"id": "1802.04855", "submitter": "Reza Bonyadi", "authors": "Mohammad Reza Bonyadi", "title": "A theoretical guideline for designing an effective adaptive particle\n  swarm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we theoretically investigate underlying assumptions that have\nbeen used for designing adaptive particle swarm optimization algorithms in the\npast years. We relate these assumptions to the movement patterns of particles\ncontrolled by coefficient values (inertia weight and acceleration coefficient)\nand introduce three factors, namely the autocorrelation of the particle\npositions, the average movement distance of the particle in each iteration, and\nthe focus of the search, that describe these movement patterns. We show how\nthese factors represent movement patterns of a particle within a swarm and how\nthey are affected by particle coefficients (i.e., inertia weight and\nacceleration coefficients). We derive equations that provide exact coefficient\nvalues to guarantee achieving a desired movement pattern defined by these three\nfactors within a swarm. We then relate these movements to the searching\ncapability of particles and provide guideline for designing potentially\nsuccessful adaptive methods to control coefficients in particle swarm. Finally,\nwe propose a new simple time adaptive particle swarm and compare its results\nwith previous adaptive particle swarm approaches. Our experiments show that the\ntheoretical findings indeed provide a beneficial guideline for successful\nadaptation of the coefficients in the particle swarm optimization algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 21:00:33 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Bonyadi", "Mohammad Reza", ""]]}, {"id": "1802.04899", "submitter": "Luiz Franca-Neto", "authors": "Luiz M Franca-Neto", "title": "Field-Programmable Deep Neural Network (DNN) Learning and Inference\n  accelerator: a concept", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accelerator is a specialized integrated circuit designed to perform\nspecific computations faster than if those were performed by CPU or GPU. A\nField-Programmable DNN learning and inference accelerator (FProg-DNN) using\nhybrid systolic and non-systolic techniques, distributed information-control\nand deep pipelined structure is proposed and its microarchitecture and\noperation presented here. Reconfigurability attends diverse DNN designs and\nallows for different number of workers to be assigned to different layers as a\nfunction of the relative difference in computational load among layers. The\ncomputational delay per layer is made roughly the same along pipelined\naccelerator structure. VGG-16 and recently proposed Inception Modules are used\nfor showing the flexibility of the FProg-DNN reconfigurability. Special\nstructures were also added for a combination of convolution layer, map\ncoincidence and feedback for state of the art learning with small set of\nexamples, which is the focus of a companion paper by the author (Franca-Neto,\n2018). The accelerator described is able to reconfigure from (1) allocating all\na DNN computations to a single worker in one extreme of sub-optimal performance\nto (2) optimally allocating workers per layer according to computational load\nin each DNN layer to be realized. Due the pipelined architecture, more than 50x\nspeedup is achieved relative to GPUs or TPUs. This speed-up is consequence of\nhiding the delay in transporting activation outputs from one layer to the next\nin a DNN behind the computations in the receiving layer. This FProg-DNN concept\nhas been simulated and validated at behavioral-functional level.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 00:02:08 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 06:23:53 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 00:56:28 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 22:41:33 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Franca-Neto", "Luiz M", ""]]}, {"id": "1802.04924", "submitter": "Zhihao Jia", "authors": "Zhihao Jia, Sina Lin, Charles R. Qi, Alex Aiken", "title": "Exploring Hidden Dimensions in Parallelizing Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed growth in the computational requirements\nfor training deep convolutional neural networks. Current approaches parallelize\ntraining onto multiple devices by applying a single parallelization strategy\n(e.g., data or model parallelism) to all layers in a network. Although easy to\nreason about, these approaches result in suboptimal runtime performance in\nlarge-scale distributed training, since different layers in a network may\nprefer different parallelization strategies. In this paper, we propose\nlayer-wise parallelism that allows each layer in a network to use an individual\nparallelization strategy. We jointly optimize how each layer is parallelized by\nsolving a graph search problem. Our evaluation shows that layer-wise\nparallelism outperforms state-of-the-art approaches by increasing training\nthroughput, reducing communication costs, achieving better scalability to\nmultiple GPUs, while maintaining original network accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 02:00:40 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 16:19:03 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Jia", "Zhihao", ""], ["Lin", "Sina", ""], ["Qi", "Charles R.", ""], ["Aiken", "Alex", ""]]}, {"id": "1802.04986", "submitter": "Viet Anh Phan Mr", "authors": "Anh Viet Phan, Minh Le Nguyen, Lam Thu Bui", "title": "Convolutional Neural Networks over Control Flow Graphs for Software\n  Defect Prediction", "comments": "presented at ICTAI 2017", "journal-ref": "ICTAI 2017", "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing defects in software components is unavoidable and leads to not only\na waste of time and money but also many serious consequences. To build\npredictive models, previous studies focus on manually extracting features or\nusing tree representations of programs, and exploiting different machine\nlearning algorithms. However, the performance of the models is not high since\nthe existing features and tree structures often fail to capture the semantics\nof programs. To explore deeply programs' semantics, this paper proposes to\nleverage precise graphs representing program execution flows, and deep neural\nnetworks for automatically learning defect features. Firstly, control flow\ngraphs are constructed from the assembly instructions obtained by compiling\nsource code; we thereafter apply multi-view multi-layer directed graph-based\nconvolutional neural networks (DGCNNs) to learn semantic features. The\nexperiments on four real-world datasets show that our method significantly\noutperforms the baselines including several other deep learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 08:32:05 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Phan", "Anh Viet", ""], ["Nguyen", "Minh Le", ""], ["Bui", "Lam Thu", ""]]}, {"id": "1802.05098", "submitter": "Jakob Foerster", "authors": "Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim\n  Rockt\\\"aschel, Eric P. Xing, Shimon Whiteson", "title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The score function estimator is widely used for estimating gradients of\nstochastic objectives in stochastic computation graphs (SCG), eg, in\nreinforcement learning and meta-learning. While deriving the first-order\ngradient estimators by differentiating a surrogate loss (SL) objective is\ncomputationally and conceptually simple, using the same approach for\nhigher-order derivatives is more challenging. Firstly, analytically deriving\nand implementing such estimators is laborious and not compliant with automatic\ndifferentiation. Secondly, repeatedly applying SL to construct new objectives\nfor each order derivative involves increasingly cumbersome graph manipulations.\nLastly, to match the first-order gradient under differentiation, SL treats part\nof the cost as a fixed sample, which we show leads to missing and wrong terms\nfor estimators of higher-order derivatives. To address all these shortcomings\nin a unified way, we introduce DiCE, which provides a single objective that can\nbe differentiated repeatedly, generating correct estimators of derivatives of\nany order in SCGs. Unlike SL, DiCE relies on automatic differentiation for\nperforming the requisite graph manipulations. We verify the correctness of DiCE\nboth through a proof and numerical evaluation of the DiCE derivative estimates.\nWe also use DiCE to propose and evaluate a novel approach for multi-agent\nlearning. Our code is available at https://www.github.com/alshedivat/lola.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:05:54 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 10:59:41 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 19:11:15 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Foerster", "Jakob", ""], ["Farquhar", "Gregory", ""], ["Al-Shedivat", "Maruan", ""], ["Rockt\u00e4schel", "Tim", ""], ["Xing", "Eric P.", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1802.05300", "submitter": "Mantas Mazeika", "authors": "Dan Hendrycks, Mantas Mazeika, Duncan Wilson, Kevin Gimpel", "title": "Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe\n  Noise", "comments": "NeurIPS 2018. PyTorch code available at\n  https://github.com/mmazeika/glc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing importance of massive datasets used for deep learning makes\nrobustness to label noise a critical property for classifiers to have. Sources\nof label noise include automatic labeling, non-expert labeling, and label\ncorruption by data poisoning adversaries. Numerous previous works assume that\nno source of labels can be trusted. We relax this assumption and assume that a\nsmall subset of the training data is trusted. This enables substantial label\ncorruption robustness performance gains. In addition, particularly severe label\nnoise can be combated by using a set of trusted data with clean labels. We\nutilize trusted data by proposing a loss correction technique that utilizes\ntrusted examples in a data-efficient manner to mitigate the effects of label\nnoise on deep neural network classifiers. Across vision and natural language\nprocessing tasks, we experiment with various label noises at several strengths,\nand show that our method significantly outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 19:48:50 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 19:07:19 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 17:41:23 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 19:55:36 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mazeika", "Mantas", ""], ["Wilson", "Duncan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1802.05324", "submitter": "Anh Tuan Nguyen", "authors": "Anh Tuan Nguyen, Jian Xu, Diu Khue Luu, Qi Zhao, and Zhi Yang", "title": "Advancing System Performance with Redundancy: From Biological to\n  Artificial Designs", "comments": null, "journal-ref": "Neural Computation, MIT Press, 2019", "doi": "10.1162/neco_a_01166", "report-no": null, "categories": "cs.IT cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy is a fundamental characteristic of many biological processes such\nas those in the genetic, visual, muscular and nervous system; yet its function\nhas not been fully understood. The conventional interpretation of redundancy is\nthat it serves as a fault-tolerance mechanism, which leads to redundancy's de\nfacto application in man-made systems for reliability enhancement. On the\ncontrary, our previous works have demonstrated an example where redundancy can\nbe engineered solely for enhancing other aspects of the system, namely accuracy\nand precision. This design was inspired by the binocular structure of the human\nvision which we believe may share a similar operation. In this paper, we\npresent a unified theory describing how such utilization of redundancy is\nfeasible through two complementary mechanisms: representational redundancy\n(RPR) and entangled redundancy (ETR). Besides the previous works, we point out\ntwo additional examples where our new understanding of redundancy can be\napplied to justify a system's superior performance. One is the human\nmusculoskeletal system (HMS) - a biological instance, and one is the deep\nresidual neural network (ResNet) - an artificial counterpart. We envision that\nour theory would provide a framework for the future development of bio-inspired\nredundant artificial systems as well as assist the studies of the fundamental\nmechanisms governing various biological processes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 21:13:38 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Nguyen", "Anh Tuan", ""], ["Xu", "Jian", ""], ["Luu", "Diu Khue", ""], ["Zhao", "Qi", ""], ["Yang", "Zhi", ""]]}, {"id": "1802.05405", "submitter": "Charles Delahunt", "authors": "Charles B. Delahunt, J. Nathan Kutz", "title": "Putting a bug in ML: The moth olfactory network learns to read MNIST", "comments": "23 pages, 7 figures. Version 3: 25 January 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to (i) characterize the learning architectures exploited in\nbiological neural networks for training on very few samples, and (ii) port\nthese algorithmic structures to a machine learning context. The Moth Olfactory\nNetwork is among the simplest biological neural systems that can learn, and its\narchitecture includes key structural elements and mechanisms widespread in\nbiological neural nets, such as cascaded networks, competitive inhibition, high\nintrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These\nstructural biological elements, in combination, enable rapid learning.\n  MothNet is a computational model of the Moth Olfactory Network, closely\naligned with the moth's known biophysics and with in vivo electrode data\ncollected from moths learning new odors. We assign this model the task of\nlearning to read the MNIST digits. We show that MothNet successfully learns to\nread given very few training samples (1 to 10 samples per class). In this\nfew-samples regime, it outperforms standard machine learning methods such as\nnearest-neighbors, support-vector machines, and neural networks (NNs), and\nmatches specialized one-shot transfer-learning methods but without the need for\npre-training. The MothNet architecture illustrates how algorithmic structures\nderived from biological brains can be used to build alternative NNs that may\navoid some of the learning rate limitations of current engineered NNs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 04:58:45 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 01:23:22 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 22:40:41 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Delahunt", "Charles B.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1802.05415", "submitter": "Sumeet Sohan Singh", "authors": "Sumeet S. Singh", "title": "Teaching Machines to Code: Neural Markup Generation with Visual\n  Attention", "comments": "For datasets, visualizations and ancillary material see:\n  https://untrix.github.io/i2l . For source code go to:\n  https://github.com/untrix/im2latex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural transducer model with visual attention that learns to\ngenerate LaTeX markup of a real-world math formula given its image. Applying\nsequence modeling and transduction techniques that have been very successful\nacross modalities such as natural language, image, handwriting, speech and\naudio; we construct an image-to-markup model that learns to produce\nsyntactically and semantically correct LaTeX markup code over 150 words long\nand achieves a BLEU score of 89%; improving upon the previous state-of-art for\nthe Im2Latex problem. We also demonstrate with heat-map visualization how\nattention helps in interpreting the model and can pinpoint (detect and\nlocalize) symbols on the image accurately despite having been trained without\nany bounding box data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 06:17:51 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 21:36:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Singh", "Sumeet S.", ""]]}, {"id": "1802.05448", "submitter": "Aneta Neumann", "authors": "Aneta Neumann, Wanru Gao, Carola Doerr, Frank Neumann, Markus Wagner", "title": "Discrepancy-based Evolutionary Diversity Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity plays a crucial role in evolutionary computation. While diversity\nhas been mainly used to prevent the population of an evolutionary algorithm\nfrom premature convergence, the use of evolutionary algorithms to obtain a\ndiverse set of solutions has gained increasing attention in recent years.\nDiversity optimization in terms of features on the underlying problem allows to\nobtain a better understanding of possible solutions to the problem at hand and\ncan be used for algorithm selection when dealing with combinatorial\noptimization problems such as the Traveling Salesperson Problem. We explore the\nuse of the star-discrepancy measure to guide the diversity optimization process\nof an evolutionary algorithm.\n  In our experimental investigations, we consider our discrepancy-based\ndiversity optimization approaches for evolving diverse sets of images as well\nas instances of the Traveling Salesperson problem where a local search is not\nable to find near optimal solutions. Our experimental investigations comparing\nthree diversity optimization approaches show that a discrepancy-based diversity\noptimization approach using a tie-breaking rule based on weighted differences\nto surrounding feature points provides the best results in terms of the star\ndiscrepancy measure.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:47:30 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Neumann", "Aneta", ""], ["Gao", "Wanru", ""], ["Doerr", "Carola", ""], ["Neumann", "Frank", ""], ["Wagner", "Markus", ""]]}, {"id": "1802.05480", "submitter": "Aneta Neumann", "authors": "Aneta Neumann, Christo Pyromallis, Bradley Alexander", "title": "Evolution of Images with Diversity and Constraints Using a Generator\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary search has been extensively used to generate artistic images.\nRaw images have high dimensionality which makes a direct search for an image\nchallenging. In previous work this problem has been addressed by using compact\nsymbolic encodings or by constraining images with priors. Recent developments\nin deep learning have enabled a generation of compelling artistic images using\ngenerative networks that encode images with lower-dimensional latent spaces. To\ndate this work has focused on the generation of images concordant with one or\nmore classes and transfer of artistic styles. There is currently no work which\nuses search in this latent space to generate images scoring high or low\naesthetic measures. In this paper we use evolutionary methods to search for\nimages in two datasets, faces and butterflies, and demonstrate the effect of\noptimising aesthetic feature scores in one or two dimensions. The work gives a\npreliminary indication of which feature measures promote the most interesting\nimages and how some of these measures interact.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 11:00:59 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Neumann", "Aneta", ""], ["Pyromallis", "Christo", ""], ["Alexander", "Bradley", ""]]}, {"id": "1802.05594", "submitter": "Beno\\^it Girard", "authors": "Lise Aubin, Mehdi Khamassi (ISIR), Beno\\^it Girard (ISIR)", "title": "Prioritized Sweeping Neural DynaQ with Multiple Predecessors, and\n  Hippocampal Replays", "comments": "Living Machines 2018 (Paris, France)", "journal-ref": null, "doi": "10.1007/978-3-319-95972-6_4", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During sleep and awake rest, the hippocampus replays sequences of place cells\nthat have been activated during prior experiences. These have been interpreted\nas a memory consolidation process, but recent results suggest a possible\ninterpretation in terms of reinforcement learning. The Dyna reinforcement\nlearning algorithms use off-line replays to improve learning. Under limited\nreplay budget, a prioritized sweeping approach, which requires a model of the\ntransitions to the predecessors, can be used to improve performance. We\ninvestigate whether such algorithms can explain the experimentally observed\nreplays. We propose a neural network version of prioritized sweeping\nQ-learning, for which we developed a growing multiple expert algorithm, able to\ncope with multiple predecessors. The resulting architecture is able to improve\nthe learning of simulated agents confronted to a navigation task. We predict\nthat, in animals, learning the world model should occur during rest periods,\nand that the corresponding replays should be shuffled.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 15:15:19 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 12:27:55 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Aubin", "Lise", "", "ISIR"], ["Khamassi", "Mehdi", "", "ISIR"], ["Girard", "Beno\u00eet", "", "ISIR"]]}, {"id": "1802.05642", "submitter": "David Balduzzi", "authors": "David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster,\n  Karl Tuyls, Thore Graepel", "title": "The Mechanics of n-Player Differentiable Games", "comments": "ICML 2018, final version", "journal-ref": "PMLR volume 80, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cornerstone underpinning deep learning is the guarantee that gradient\ndescent on an objective converges to local minima. Unfortunately, this\nguarantee fails in settings, such as generative adversarial nets, where there\nare multiple interacting losses. The behavior of gradient-based methods in\ngames is not well understood -- and is becoming increasingly important as\nadversarial and multi-objective architectures proliferate. In this paper, we\ndevelop new techniques to understand and control the dynamics in general games.\nThe key result is to decompose the second-order dynamics into two components.\nThe first is related to potential games, which reduce to gradient descent on an\nimplicit function; the second relates to Hamiltonian games, a new class of\ngames that obey a conservation law, akin to conservation laws in classical\nmechanical systems. The decomposition motivates Symplectic Gradient Adjustment\n(SGA), a new algorithm for finding stable fixed points in general games. Basic\nexperiments show SGA is competitive with recently proposed algorithms for\nfinding stable fixed points in GANs -- whilst at the same time being applicable\nto -- and having guarantees in -- much more general games.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:32:48 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 13:26:15 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Balduzzi", "David", ""], ["Racaniere", "Sebastien", ""], ["Martens", "James", ""], ["Foerster", "Jakob", ""], ["Tuyls", "Karl", ""], ["Graepel", "Thore", ""]]}, {"id": "1802.05668", "submitter": "Antonio Polino", "authors": "Antonio Polino, Razvan Pascanu, Dan Alistarh", "title": "Model compression via distillation and quantization", "comments": "21 pages, published as a conference paper at ICLR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) continue to make significant advances, solving\ntasks from image classification to translation or reinforcement learning. One\naspect of the field receiving considerable attention is efficiently executing\ndeep models in resource-constrained environments, such as mobile or embedded\ndevices. This paper focuses on this problem, and proposes two new compression\nmethods, which jointly leverage weight quantization and distillation of larger\nteacher networks into smaller student networks. The first method we propose is\ncalled quantized distillation and leverages distillation during the training\nprocess, by incorporating distillation loss, expressed with respect to the\nteacher, into the training of a student network whose weights are quantized to\na limited set of levels. The second method, differentiable quantization,\noptimizes the location of quantization points through stochastic gradient\ndescent, to better fit the behavior of the teacher model. We validate both\nmethods through experiments on convolutional and recurrent architectures. We\nshow that quantized shallow students can reach similar accuracy levels to\nfull-precision teacher models, while providing order of magnitude compression,\nand inference speedup that is linear in the depth reduction. In sum, our\nresults enable DNNs for resource-constrained environments to leverage\narchitecture and accuracy advances developed on more powerful devices.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 17:18:49 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Polino", "Antonio", ""], ["Pascanu", "Razvan", ""], ["Alistarh", "Dan", ""]]}, {"id": "1802.05686", "submitter": "Anh Tuan Nguyen", "authors": "Anh Tuan Nguyen, Jian Xu and Zhi Yang", "title": "A Bio-inspired Redundant Sensing Architecture", "comments": null, "journal-ref": "(2016) A Bio-inspired Redundant Sensing Architecture. Advances in\n  Neural Information Processing Systems (NIPS), Dec. 2016", "doi": null, "report-no": null, "categories": "cs.NE cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensing is the process of deriving signals from the environment that allows\nartificial systems to interact with the physical world. The Shannon theorem\nspecifies the maximum rate at which information can be acquired. However, this\nupper bound is hard to achieve in many man-made systems. The biological visual\nsystems, on the other hand, have highly efficient signal representation and\nprocessing mechanisms that allow precise sensing. In this work, we argue that\nredundancy is one of the critical characteristics for such superior\nperformance. We show architectural advantages by utilizing redundant sensing,\nincluding correction of mismatch error and significant precision enhancement.\nFor a proof-of-concept demonstration, we have designed a heuristic-based\nanalog-to-digital converter - a zero-dimensional quantizer. Through Monte Carlo\nsimulation with the error probabilistic distribution as a priori, the\nperformance approaching the Shannon limit is feasible. In actual measurements\nwithout knowing the error distribution, we observe at least 2-bit extra\nprecision. The results may also help explain biological processes including the\ndominance of binocular vision, the functional roles of the fixational eye\nmovements, and the structural mechanisms allowing hyperacuity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:03:55 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Nguyen", "Anh Tuan", ""], ["Xu", "Jian", ""], ["Yang", "Zhi", ""]]}, {"id": "1802.05698", "submitter": "Nikolai Andrianov", "authors": "Nikolai Andrianov", "title": "A Machine Learning Approach for Virtual Flow Metering and Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with robust and accurate forecasting of multiphase flow\nrates in wells and pipelines during oil and gas production. In practice, the\npossibility to physically measure the rates is often limited; besides, it is\ndesirable to estimate future values of multiphase rates based on the previous\nbehavior of the system. In this work, we demonstrate that a Long Short-Term\nMemory (LSTM) recurrent artificial network is able not only to accurately\nestimate the multiphase rates at current time (i.e., act as a virtual flow\nmeter), but also to forecast the rates for a sequence of future time instants.\nFor a synthetic severe slugging case, LSTM forecasts compare favorably with the\nresults of hydrodynamical modeling. LSTM results for a realistic noizy dataset\nof a variable rate well test show that the model can also successfully forecast\nmultiphase rates for a system with changing flow patterns.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:45:39 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Andrianov", "Nikolai", ""]]}, {"id": "1802.05825", "submitter": "Maria Yaneli Ameca Alducin", "authors": "Maria-Yaneli Ameca-Alducin, Maryam Hasani-Shoreh, Wilson Blaikie,\n  Frank Neumann, Efren Mezura-Montes", "title": "A Comparison of Constraint Handling Techniques for Dynamic Constrained\n  Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic constrained optimization problems (DCOPs) have gained researchers\nattention in recent years because a vast majority of real world problems change\nover time. There are studies about the effect of constrained handling\ntechniques in static optimization problems. However, there lacks any\nsubstantial study in the behavior of the most popular constraint handling\ntechniques when dealing with DCOPs. In this paper we study the four most\npopular used constraint handling techniques and apply a simple Differential\nEvolution (DE) algorithm coupled with a change detection mechanism to observe\nthe behavior of these techniques. These behaviors were analyzed using a common\nbenchmark to determine which techniques are suitable for the most prevalent\ntypes of DCOPs. For the purpose of analysis, common measures in static\nenvironments were adapted to suit dynamic environments. While an overall\nsuperior technique could not be determined, certain techniques outperformed\nothers in different aspects like rate of optimization or reliability of\nsolutions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 03:26:00 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Ameca-Alducin", "Maria-Yaneli", ""], ["Hasani-Shoreh", "Maryam", ""], ["Blaikie", "Wilson", ""], ["Neumann", "Frank", ""], ["Mezura-Montes", "Efren", ""]]}, {"id": "1802.05892", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg, Sergej Sizov", "title": "Neuroscientific User Models: The Source of Uncertain User Feedback and\n  Potentials for Improving Web Personalisation", "comments": "Bayesian Brain, Neural Coding, Human Uncertainty, Noise, User Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the neuroscientific theory of the Bayesian brain in\nthe light of adaptive web systems and content personalisation. In particular,\nwe elaborate on neural mechanisms of human decision-making and the origin of\nlacking reliability of user feedback, often denoted as noise or human\nuncertainty. To this end, we first introduce an adaptive model of cognitive\nagency in which populations of neurons provide an estimation for states of the\nworld. Subsequently, we present various so-called decoder functions with which\nneuronal activity can be translated into quantitative decisions. The interplay\nof the underlying cognition model and the chosen decoder function leads to\ndifferent model-based properties of decision processes. The goal of this paper\nis to promote novel user models and exploit them to naturally associate users\nto different clusters on the basis of their individual neural characteristics\nand thinking patterns. These user models might be able to turn the variability\nof user behaviour into additional information for improving web personalisation\nand its experience.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:08:30 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1802.05943", "submitter": "Yuhao Zhu", "authors": "Yuhao Zhu, Gu-Yeon Wei, David Brooks", "title": "Cloud No Longer a Silver Bullet, Edge to the Rescue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes the position that, while cognitive computing today relies\nheavily on the cloud, we will soon see a paradigm shift where cognitive\ncomputing primarily happens on network edges. The shift toward edge devices is\nfundamentally propelled both by technological constraints in data centers and\nwireless network infrastructures, as well as practical considerations such as\nprivacy and safety. The remainder of this paper lays out our view of how these\nconstraints will impact future cognitive computing. Bringing cognitive\ncomputing to edge devices opens up several new opportunities and challenges,\nsome of which demand new solutions and some of which require us to revisit\nentrenched techniques in light of new technologies. We close the paper with a\ncall to action for future research.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 06:18:54 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Zhu", "Yuhao", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "1802.05991", "submitter": "Jialin Liu Ph.D", "authors": "Simon M Lucas, Jialin Liu and Diego Perez-Liebana", "title": "The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation", "comments": "9 pages, 3 figures, 3 table. This is the final version of the article\n  accepted by WCCI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the N-Tuple Bandit Evolutionary Algorithm (NTBEA), an\noptimisation algorithm developed for noisy and expensive discrete\n(combinatorial) optimisation problems. The algorithm is applied to two\ngame-based hyper-parameter optimisation problems. The N-Tuple system directly\nmodels the statistics, approximating the fitness and number of evaluations of\neach modelled combination of parameters. The model is simple, efficient and\ninformative. Results show that the NTBEA significantly outperforms grid search\nand an estimation of distribution algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 15:49:10 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 10:32:58 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Lucas", "Simon M", ""], ["Liu", "Jialin", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "1802.06093", "submitter": "Phil Long", "authors": "Peter L. Bartlett, David P. Helmbold and Philip M. Long", "title": "Gradient descent with identity initialization efficiently learns\n  positive definite linear transformations by deep residual networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping\n$\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a\nfunction $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by\n$h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that\nlearn through gradient descent on the population quadratic loss in the case\nthat the distribution over the inputs is isotropic.\n  We provide polynomial bounds on the number of iterations for gradient descent\nto approximate the least squares matrix $\\Phi$, in the case where the initial\nhypothesis $\\Theta_1 = ... = \\Theta_L = I$ has excess loss bounded by a small\nenough constant. On the other hand, we show that gradient descent fails to\nconverge for $\\Phi$ whose distance from the identity is a larger constant, and\nwe show that some forms of regularization toward the identity in each layer do\nnot help.\n  If $\\Phi$ is symmetric positive definite, we show that an algorithm that\ninitializes $\\Theta_i = I$ learns an $\\epsilon$-approximation of $f$ using a\nnumber of updates polynomial in $L$, the condition number of $\\Phi$, and\n$\\log(d/\\epsilon)$. In contrast, we show that if the least squares matrix\n$\\Phi$ is symmetric and has a negative eigenvalue, then all members of a class\nof algorithms that perform gradient descent with identity initialization, and\noptionally regularize toward the identity in each layer, fail to converge.\n  We analyze an algorithm for the case that $\\Phi$ satisfies $u^{\\top} \\Phi u >\n0$ for all $u$, but may not be symmetric. This algorithm uses two regularizers:\none that maintains the invariant $u^{\\top} \\Theta_L \\Theta_{L-1} ... \\Theta_1 u\n> 0$ for all $u$, and another that \"balances\" $\\Theta_1, ..., \\Theta_L$ so that\nthey have the same singular values.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 19:24:29 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 15:48:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 16:10:40 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 16:46:26 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1802.06288", "submitter": "Karthik R", "authors": "R Karthik, Dhruv Tyagi, Amogh Raut, Soumya Saxena, Rajesh Kumar M", "title": "Implementation of Neural Network and feature extraction to classify ECG\n  signals", "comments": "SPRINGER LNEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a suitable and efficient implementation of a feature\nextraction algorithm (Pan Tompkins algorithm) on electrocardiography (ECG)\nsignals, for detection and classification of four cardiac diseases: Sleep\nApnea, Arrhythmia, Supraventricular Arrhythmia and Long Term Atrial\nFibrillation (AF) and differentiating them from the normal heart beat by using\npan Tompkins RR detection followed by feature extraction for classification\npurpose .The paper also presents a new approach towards signal classification\nusing the existing neural networks classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:42:04 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Karthik", "R", ""], ["Tyagi", "Dhruv", ""], ["Raut", "Amogh", ""], ["Saxena", "Soumya", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "1802.06360", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy (University of Sydney and Capital Markets\n  Cooperative Research Centre (CMCRC)), Aditya Krishna Menon (Data61/CSIRO and\n  the Australian National University), Sanjay Chawla (Qatar Computing Research\n  Institute (QCRI), HBKU)", "title": "Anomaly Detection using One-Class Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a one-class neural network (OC-NN) model to detect anomalies in\ncomplex data sets. OC-NN combines the ability of deep networks to extract a\nprogressively rich representation of data with the one-class objective of\ncreating a tight envelope around normal data. The OC-NN approach breaks new\nground for the following crucial reason: data representation in the hidden\nlayer is driven by the OC-NN objective and is thus customized for anomaly\ndetection. This is a departure from other approaches which use a hybrid\napproach of learning deep features using an autoencoder and then feeding the\nfeatures into a separate anomaly detection method like one-class SVM (OC-SVM).\nThe hybrid OC-SVM approach is sub-optimal because it is unable to influence\nrepresentational learning in the hidden layers. A comprehensive set of\nexperiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN\nperforms on par with state-of-the-art methods and outperformed conventional\nshallow methods in some scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 10:44:53 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 00:05:05 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Chalapathy", "Raghavendra", "", "University of Sydney and Capital Markets\n  Cooperative Research Centre"], ["Menon", "Aditya Krishna", "", "Data61/CSIRO and\n  the Australian National University"], ["Chawla", "Sanjay", "", "Qatar Computing Research\n  Institute"]]}, {"id": "1802.06367", "submitter": "Xingyu Liu", "authors": "Xingyu Liu, Jeff Pool, Song Han, William J. Dally", "title": "Efficient Sparse-Winograd Convolutional Neural Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are computationally intensive, which\nlimits their application on mobile devices. Their energy is dominated by the\nnumber of multiplies needed to perform the convolutions. Winograd's minimal\nfiltering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can\nreduce the operation count, but these two methods cannot be directly combined\n$-$ applying the Winograd transform fills in the sparsity in both the weights\nand the activations. We propose two modifications to Winograd-based CNNs to\nenable these methods to exploit sparsity. First, we move the ReLU operation\ninto the Winograd domain to increase the sparsity of the transformed\nactivations. Second, we prune the weights in the Winograd domain to exploit\nstatic weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet\ndatasets, our method reduces the number of multiplications by $10.4\\times$,\n$6.8\\times$ and $10.8\\times$ respectively with loss of accuracy less than\n$0.1\\%$, outperforming previous baselines by $2.0\\times$-$3.0\\times$. We also\nshow that moving ReLU to the Winograd domain allows more aggressive pruning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 12:29:05 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liu", "Xingyu", ""], ["Pool", "Jeff", ""], ["Han", "Song", ""], ["Dally", "William J.", ""]]}, {"id": "1802.06467", "submitter": "Adam Liska", "authors": "Adam Li\\v{s}ka, Germ\\'an Kruszewski, Marco Baroni", "title": "Memorize or generalize? Searching for a compositional RNN in a haystack", "comments": "AEGAP Workshop (ICML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are very powerful learning systems, but they do not readily\ngeneralize from one task to the other. This is partly due to the fact that they\ndo not learn in a compositional way, that is, by discovering skills that are\nshared by different tasks, and recombining them to solve new problems. In this\npaper, we explore the compositional generalization capabilities of recurrent\nneural networks (RNNs). We first propose the lookup table composition domain as\na simple setup to test compositional behaviour and show that it is\ntheoretically possible for a standard RNN to learn to behave compositionally in\nthis domain when trained with standard gradient descent and provided with\nadditional supervision. We then remove this additional supervision and perform\na search over a large number of model initializations to investigate the\nproportion of RNNs that can still converge to a compositional solution. We\ndiscover that a small but non-negligible proportion of RNNs do reach partial\ncompositional solutions even without special architectural constraints. This\nsuggests that a combination of gradient descent and evolutionary strategies\ndirectly favouring the minority models that developed more compositional\napproaches might suffice to lead standard RNNs towards compositional solutions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 23:15:26 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 18:55:27 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Li\u0161ka", "Adam", ""], ["Kruszewski", "Germ\u00e1n", ""], ["Baroni", "Marco", ""]]}, {"id": "1802.06488", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, Francis Li, Brendan Chwyl", "title": "Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network\n  for Real-time Embedded Object Detection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a major challenge in computer vision, involving both\nobject classification and object localization within a scene. While deep neural\nnetworks have been shown in recent years to yield very powerful techniques for\ntackling the challenge of object detection, one of the biggest challenges with\nenabling such object detection networks for widespread deployment on embedded\ndevices is high computational and memory requirements. Recently, there has been\nan increasing focus in exploring small deep neural network architectures for\nobject detection that are more suitable for embedded devices, such as Tiny YOLO\nand SqueezeDet. Inspired by the efficiency of the Fire microarchitecture\nintroduced in SqueezeNet and the object detection performance of the\nsingle-shot detection macroarchitecture introduced in SSD, this paper\nintroduces Tiny SSD, a single-shot detection deep convolutional neural network\nfor real-time embedded object detection that is composed of a highly optimized,\nnon-uniform Fire sub-network stack and a non-uniform sub-network stack of\nhighly optimized SSD-based auxiliary convolutional feature layers designed\nspecifically to minimize model size while maintaining object detection\nperformance. The resulting Tiny SSD possess a model size of 2.3MB (~26X smaller\nthan Tiny YOLO) while still achieving an mAP of 61.3% on VOC 2007 (~4.2% higher\nthan Tiny YOLO). These experimental results show that very small deep neural\nnetwork architectures can be designed for real-time object detection that are\nwell-suited for embedded scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 01:57:46 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Chwyl", "Brendan", ""]]}, {"id": "1802.06591", "submitter": "German I. Parisi", "authors": "Jonathan Tong, German I. Parisi, Stefan Wermter, Brigitte R\\\"oder", "title": "Closing the loop on multisensory interactions: A neural architecture for\n  multisensory causal inference and recalibration", "comments": "The code to implement the network model and simulations can be found\n  online at: https://github.com/jonathan-tong/multisensory-network-model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the brain receives input from multiple sensory systems, it is faced with\nthe question of whether it is appropriate to process the inputs in combination,\nas if they originated from the same event, or separately, as if they originated\nfrom distinct events. Furthermore, it must also have a mechanism through which\nit can keep sensory inputs calibrated to maintain the accuracy of its internal\nrepresentations. We have developed a neural network architecture capable of i)\napproximating optimal multisensory spatial integration, based on Bayesian\ncausal inference, and ii) recalibrating the spatial encoding of sensory\nsystems. The architecture is based on features of the dorsal processing\nhierarchy, including the spatial tuning properties of unisensory neurons and\nthe convergence of different sensory inputs onto multisensory neurons.\nFurthermore, we propose that these unisensory and multisensory neurons play\ndual roles in i) encoding spatial location as separate or integrated estimates\nand ii) accumulating evidence for the independence or relatedness of\nmultisensory stimuli. We further propose that top-down feedback connections\nspanning the dorsal pathway play key a role in recalibrating spatial encoding\nat the level of early unisensory cortices. Our proposed architecture provides\npossible explanations for a number of human electrophysiological and\nneuroimaging results and generates testable predictions linking neurophysiology\nwith behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 11:31:02 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 14:03:42 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 14:59:39 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Tong", "Jonathan", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""], ["R\u00f6der", "Brigitte", ""]]}, {"id": "1802.06920", "submitter": "John Mern", "authors": "John Mern, Jayesh K Gupta, Mykel Kochenderfer", "title": "Layer-wise synapse optimization for implementing neural networks on\n  general neuromorphic architectures", "comments": "Submitted to IEEE Symposium Series on Computational Intelligence\n  (SSCI) 2017", "journal-ref": null, "doi": "10.1109/SSCI.2017.8285202", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks (ANNs) can represent a wide range of complex\nfunctions. Implementing ANNs in Von Neumann computing systems, though, incurs a\nhigh energy cost due to the bottleneck created between CPU and memory.\nImplementation on neuromorphic systems may help to reduce energy demand.\nConventional ANNs must be converted into equivalent Spiking Neural Networks\n(SNNs) in order to be deployed on neuromorphic chips. This paper presents a way\nto perform this translation. We map the ANN weights to SNN synapses\nlayer-by-layer by forming a least-square-error approximation problem at each\nlayer.\n  An optimal set of synapse weights may then be found for a given choice of ANN\nactivation function and SNN neuron. Using an appropriate constrained solver, we\ncan generate SNNs compatible with digital, analog, or hybrid chip\narchitectures. We present an optimal node pruning method to allow SNN layer\nsizes to be set by the designer. To illustrate this process, we convert three\nANNs, including one convolutional network, to SNNs. In all three cases, a\nsimple linear program solver was used. The experiments show that the resulting\nnetworks maintain agreement with the original ANN and excellent performance on\nthe evaluation tasks. The networks were also reduced in size with little loss\nin task performance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 00:20:57 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Mern", "John", ""], ["Gupta", "Jayesh K", ""], ["Kochenderfer", "Mykel", ""]]}, {"id": "1802.06927", "submitter": "Nishant Desai", "authors": "Vinay Uday Prabhu, Nishant Desai, John Whaley", "title": "On Lyapunov exponents and adversarial perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we would like to disseminate a serendipitous discovery\ninvolving Lyapunov exponents of a 1-D time series and their use in serving as a\nfiltering defense tool against a specific kind of deep adversarial\nperturbation. To this end, we use the state-of-the-art CleverHans library to\ngenerate adversarial perturbations against a standard Convolutional Neural\nNetwork (CNN) architecture trained on the MNIST as well as the Fashion-MNIST\ndatasets. We empirically demonstrate how the Lyapunov exponents computed on the\nflattened 1-D vector representations of the images served as highly\ndiscriminative features that could be to pre-classify images as adversarial or\nlegitimate before feeding the image into the CNN for classification. We also\nexplore the issue of possible false-alarms when the input images are noisy in a\nnon-adversarial sense.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 01:09:22 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Prabhu", "Vinay Uday", ""], ["Desai", "Nishant", ""], ["Whaley", "John", ""]]}, {"id": "1802.06928", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Massimiliano Di Ventra and Fabio L. Traversa", "title": "Memcomputing: Leveraging memory and physics to compute efficiently", "comments": null, "journal-ref": null, "doi": "10.1063/1.5026506", "report-no": null, "categories": "cs.ET cs.CC cs.NE math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that physical phenomena may be of great help in computing\nsome difficult problems efficiently. A typical example is prime factorization\nthat may be solved in polynomial time by exploiting quantum entanglement on a\nquantum computer. There are, however, other types of (non-quantum) physical\nproperties that one may leverage to compute efficiently a wide range of hard\nproblems. In this perspective we discuss how to employ one such property,\nmemory (time non-locality), in a novel physics-based approach to computation:\nMemcomputing. In particular, we focus on digital memcomputing machines (DMMs)\nthat are scalable. DMMs can be realized with non-linear dynamical systems with\nmemory. The latter property allows the realization of a new type of Boolean\nlogic, one that is self-organizing. Self-organizing logic gates are\n\"terminal-agnostic\", namely they do not distinguish between input and output\nterminals. When appropriately assembled to represent a given\ncombinatorial/optimization problem, the corresponding self-organizing circuit\nconverges to the equilibrium points that express the solutions of the problem\nat hand. In doing so, DMMs take advantage of the long-range order that develops\nduring the transient dynamics. This collective dynamical behavior, reminiscent\nof a phase transition, or even the \"edge of chaos\", is mediated by families of\nclassical trajectories (instantons) that connect critical points of increasing\nstability in the system's phase space. The topological character of the\nsolution search renders DMMs robust against noise and structural disorder.\nSince DMMs are non-quantum systems described by ordinary differential\nequations, not only can they be built in hardware with available technology,\nthey can also be simulated efficiently on modern classical computers. As an\nexample, we will show the polynomial-time solution of the subset-sum problem\nfor the worst...\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 01:17:20 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 00:20:55 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Di Ventra", "Massimiliano", ""], ["Traversa", "Fabio L.", ""]]}, {"id": "1802.07008", "submitter": "Amin Fehri", "authors": "Amin Fehri (CMM), Santiago Velasco-Forero (CMM), Fernand Meyer (CMM)", "title": "Segmentation hi\\'erarchique faiblement supervis\\'ee", "comments": "in French", "journal-ref": "26e colloque GRETSI, Sep 2017, Juan-les-Pins, France", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning an image into a set of\nmeaningful regions according to some criteria. Hierarchical segmentation has\nemerged as a major trend in this regard as it favors the emergence of important\nregions at different scales. On the other hand, many methods allow us to have\nprior information on the position of structures of interest in the images. In\nthis paper, we present a versatile hierarchical segmentation method that takes\ninto account any prior spatial information and outputs a hierarchical\nsegmentation that emphasizes the contours or regions of interest while\npreserving the important structures in the image. An application of this method\nto the weakly-supervised segmentation problem is presented.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 08:42:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Fehri", "Amin", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Meyer", "Fernand", "", "CMM"]]}, {"id": "1802.07034", "submitter": "Christian Schulz", "authors": "Sonja Biedermann, Monika Henzinger, Christian Schulz, Bernhard\n  Schuster", "title": "Memetic Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common knowledge that there is no single best strategy for graph\nclustering, which justifies a plethora of existing approaches. In this paper,\nwe present a general memetic algorithm, VieClus, to tackle the graph clustering\nproblem. This algorithm can be adapted to optimize different objective\nfunctions. A key component of our contribution are natural recombine operators\nthat employ ensemble clusterings as well as multi-level techniques. Lastly, we\ncombine these techniques with a scalable communication protocol, producing a\nsystem that is able to compute high-quality solutions in a short amount of\ntime. We instantiate our scheme with local search for modularity and show that\nour algorithm successfully improves or reproduces all entries of the 10th\nDIMACS implementation~challenge under consideration using a small amount of\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:55:37 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Biedermann", "Sonja", ""], ["Henzinger", "Monika", ""], ["Schulz", "Christian", ""], ["Schuster", "Bernhard", ""]]}, {"id": "1802.07089", "submitter": "Qiuyuan Huang", "authors": "Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, Xiaodong He", "title": "Attentive Tensor Product Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new architecture - Attentive Tensor Product Learning\n(ATPL) - to represent grammatical structures in deep learning models. ATPL is a\nnew architecture to bridge this gap by exploiting Tensor Product\nRepresentations (TPR), a structured neural-symbolic model developed in\ncognitive science, aiming to integrate deep learning with explicit language\nstructures and rules. The key ideas of ATPL are: 1) unsupervised learning of\nrole-unbinding vectors of words via TPR-based deep neural network; 2) employing\nattention modules to compute TPR; and 3) integration of TPR with typical deep\nlearning architectures including Long Short-Term Memory (LSTM) and Feedforward\nNeural Network (FFNN). The novelty of our approach lies in its ability to\nextract the grammatical structure of a sentence by using role-unbinding\nvectors, which are obtained in an unsupervised manner. This ATPL approach is\napplied to 1) image captioning, 2) part of speech (POS) tagging, and 3)\nconstituency parsing of a sentence. Experimental results demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:42:07 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 05:14:18 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Huang", "Qiuyuan", ""], ["Deng", "Li", ""], ["Wu", "Dapeng", ""], ["Liu", "Chang", ""], ["He", "Xiaodong", ""]]}, {"id": "1802.07133", "submitter": "Hugo Jair  Escalante", "authors": "Lino Rodriguez-Coayahuitl, Alicia Morales-Reyes, Hugo Jair Escalante", "title": "Towards Deep Representation Learning with Genetic Programming", "comments": "EuroGP preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Programming (GP) is an evolutionary algorithm commonly used for\nmachine learning tasks. In this paper we present a method that allows GP to\ntransform the representation of a large-scale machine learning dataset into a\nmore compact representation, by means of processing features from the original\nrepresentation at individual level. We develop as a proof of concept of this\nmethod an autoencoder. We tested a preliminary version of our approach in a\nvariety of well-known machine learning image datasets. We speculate that this\nmethod, used in an iterative manner, can produce results competitive with\nstate-of-art deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 14:42:35 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Rodriguez-Coayahuitl", "Lino", ""], ["Morales-Reyes", "Alicia", ""], ["Escalante", "Hugo Jair", ""]]}, {"id": "1802.07239", "submitter": "Christos Kaplanis", "authors": "Christos Kaplanis, Murray Shanahan, Claudia Clopath", "title": "Continual Reinforcement Learning with Complex Synapses", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike humans, who are capable of continual learning over their lifetimes,\nartificial neural networks have long been known to suffer from a phenomenon\nknown as catastrophic forgetting, whereby new learning can lead to abrupt\nerasure of previously acquired knowledge. Whereas in a neural network the\nparameters are typically modelled as scalar values, an individual synapse in\nthe brain comprises a complex network of interacting biochemical components\nthat evolve at different timescales. In this paper, we show that by equipping\ntabular and deep reinforcement learning agents with a synaptic model that\nincorporates this biological complexity (Benna & Fusi, 2016), catastrophic\nforgetting can be mitigated at multiple timescales. In particular, we find that\nas well as enabling continual learning across sequential training of two simple\ntasks, it can also be used to overcome within-task forgetting by reducing the\nneed for an experience replay database.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 18:36:57 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 11:07:28 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Kaplanis", "Christos", ""], ["Shanahan", "Murray", ""], ["Clopath", "Claudia", ""]]}, {"id": "1802.07245", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey\n  Levine", "title": "Meta-Reinforcement Learning of Structured Exploration Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration is a fundamental challenge in reinforcement learning (RL). Many\nof the current exploration methods for deep RL use task-agnostic objectives,\nsuch as information gain or bonuses based on state visitation. However, many\npractical applications of RL involve learning more than a single task, and\nprior tasks can be used to inform how exploration should be performed in new\ntasks. In this work, we explore how prior tasks can inform an agent about how\nto explore effectively in new situations. We introduce a novel gradient-based\nfast adaptation algorithm -- model agnostic exploration with structured noise\n(MAESN) -- to learn exploration strategies from prior experience. The prior\nexperience is used both to initialize a policy and to acquire a latent\nexploration space that can inject structured stochasticity into a policy,\nproducing exploration strategies that are informed by prior knowledge and are\nmore effective than random action-space noise. We show that MAESN is more\neffective at learning exploration strategies when compared to prior meta-RL\nmethods, RL without learned exploration strategies, and task-agnostic\nexploration methods. We evaluate our method on a variety of simulated tasks:\nlocomotion with a wheeled robot, locomotion with a quadrupedal walker, and\nobject manipulation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 18:40:57 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Gupta", "Abhishek", ""], ["Mendonca", "Russell", ""], ["Liu", "YuXuan", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1802.07426", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Yoshua Bengio, Vikas Verma, Leslie Pack Kaelbling", "title": "Generalization in Machine Learning via Analytical Learning Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": "Massachusetts Institute of Technology (MIT), MIT-CSAIL-TR-2018-019", "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel measure-theoretic theory for machine learning\nthat does not require statistical assumptions. Based on this theory, a new\nregularization method in deep learning is derived and shown to outperform\nprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed\ntheory provides a theoretical basis for a family of practically successful\nregularization methods in deep learning. We discuss several consequences of our\nresults on one-shot learning, representation learning, deep learning, and\ncurriculum learning. Unlike statistical learning theory, the proposed learning\ntheory analyzes each problem instance individually via measure theory, rather\nthan a set of problem instances via statistics. As a result, it provides\ndifferent types of results and insights when compared to statistical learning\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 05:03:52 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:39:36 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 22:23:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Bengio", "Yoshua", ""], ["Verma", "Vikas", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1802.07697", "submitter": "Matthew Streeter", "authors": "Matthew Streeter", "title": "Approximation Algorithms for Cascading Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximation algorithm that takes a pool of pre-trained models\nas input and produces from it a cascaded model with similar accuracy but lower\naverage-case cost. Applied to state-of-the-art ImageNet classification models,\nthis yields up to a 2x reduction in floating point multiplications, and up to a\n6x reduction in average-case memory I/O. The auto-generated cascades exhibit\nintuitive properties, such as using lower-resolution input for easier images\nand requiring higher prediction confidence when using a computationally cheaper\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:56:05 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Streeter", "Matthew", ""]]}, {"id": "1802.08026", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Amir Hussain, Aurelio\n  Uncini", "title": "Complex-valued Neural Networks with Non-parametric Activation Functions", "comments": "Submitted to IEEE Transactions on Emerging Topics in Computational\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued neural networks (CVNNs) are a powerful modeling tool for\ndomains where data can be naturally interpreted in terms of complex numbers.\nHowever, several analytical properties of the complex domain (e.g.,\nholomorphicity) make the design of CVNNs a more challenging task than their\nreal counterpart. In this paper, we consider the problem of flexible activation\nfunctions (AFs) in the complex domain, i.e., AFs endowed with sufficient\ndegrees of freedom to adapt their shape given the training data. While this\nproblem has received considerable attention in the real case, a very limited\nliterature exists for CVNNs, where most activation functions are generally\ndeveloped in a split fashion (i.e., by considering the real and imaginary parts\nof the activation separately) or with simple phase-amplitude techniques.\nLeveraging over the recently proposed kernel activation functions (KAFs), and\nrelated advances in the design of complex-valued kernels, we propose the first\nfully complex, non-parametric activation function for CVNNs, which is based on\na kernel expansion with a fixed dictionary that can be implemented efficiently\non vectorized hardware. Several experiments on common use cases, including\nprediction and channel equalization, validate our proposal when compared to\nreal-valued neural networks and CVNNs with fixed activation functions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:11:22 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Hussain", "Amir", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1802.08217", "submitter": "Reza Moazzezi", "authors": "Reza Moazzezi", "title": "A new model for Cerebellar computation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard state space model is widely believed to account for the\ncerebellar computation in motor adaptation tasks [1]. Here we show that several\nrecent experiments [2-4] where the visual feedback is irrelevant to the motor\nresponse challenge the standard model. Furthermore, we propose a new model that\naccounts for the the results presented in [2-4]. According to this new model,\nlearning and forgetting are coupled and are error size dependent. We also show\nthat under reasonable assumptions, our proposed model is the only model that\naccounts for both the classical adaptation paradigm as well as the recent\nexperiments [2-4].\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:15:53 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Moazzezi", "Reza", ""]]}, {"id": "1802.08219", "submitter": "Nathaniel Thomas", "authors": "Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai\n  Kohlhoff, Patrick Riley", "title": "Tensor field networks: Rotation- and translation-equivariant neural\n  networks for 3D point clouds", "comments": "changes for NIPS submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tensor field neural networks, which are locally equivariant to\n3D rotations, translations, and permutations of points at every layer. 3D\nrotation equivariance removes the need for data augmentation to identify\nfeatures in arbitrary orientations. Our network uses filters built from\nspherical harmonics; due to the mathematical consequences of this filter\nchoice, each layer accepts as input (and guarantees as output) scalars,\nvectors, and higher-order tensors, in the geometric sense of these terms. We\ndemonstrate the capabilities of tensor field networks with tasks in geometry,\nphysics, and chemistry.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:17:31 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 18:58:16 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 20:09:34 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Thomas", "Nathaniel", ""], ["Smidt", "Tess", ""], ["Kearnes", "Steven", ""], ["Yang", "Lusann", ""], ["Li", "Li", ""], ["Kohlhoff", "Kai", ""], ["Riley", "Patrick", ""]]}, {"id": "1802.08370", "submitter": "Kanru Hua", "authors": "Kanru Hua", "title": "Do WaveNets Dream of Acoustic Waves?", "comments": "5 pages with 7 figures; submitted to Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Various sources have reported the WaveNet deep learning architecture being\nable to generate high-quality speech, but to our knowledge there haven't been\nstudies on the interpretation or visualization of trained WaveNets. This study\ninvestigates the possibility that WaveNet understands speech by unsupervisedly\nlearning an acoustically meaningful latent representation of the speech signals\nin its receptive field; we also attempt to interpret the mechanism by which the\nfeature extraction is performed. Suggested by singular value decomposition and\nlinear regression analysis on the activations and known acoustic features (e.g.\nF0), the key findings are (1) activations in the higher layers are highly\ncorrelated with spectral features; (2) WaveNet explicitly performs pitch\nextraction despite being trained to directly predict the next audio sample and\n(3) for the said feature analysis to take place, the latent signal\nrepresentation is converted back and forth between baseband and wideband\ncomponents.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:45:50 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Hua", "Kanru", ""]]}, {"id": "1802.08375", "submitter": "Zhenisbek Assylbekov", "authors": "Zhenisbek Assylbekov and Rustem Takhanov", "title": "Reusing Weights in Subword-aware Neural Language Models", "comments": "accepted to NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose several ways of reusing subword embeddings and other weights in\nsubword-aware neural language models. The proposed techniques do not benefit a\ncompetitive character-aware model, but some of them improve the performance of\nsyllable- and morpheme-aware models while showing significant reductions in\nmodel sizes. We discover a simple hands-on principle: in a multi-layer input\nembedding model, layers should be tied consecutively bottom-up if reused at\noutput. Our best morpheme-aware model with properly reused weights beats the\ncompetitive word-level model by a large margin across multiple languages and\nhas 20%-87% fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 03:44:15 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 04:53:59 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Assylbekov", "Zhenisbek", ""], ["Takhanov", "Rustem", ""]]}, {"id": "1802.08465", "submitter": "Francisco Javier Pulgar Rubio", "authors": "Francisco J. Pulgar, Francisco Charte, Antonio J. Rivera, Mar\\'ia J.\n  del Jesus", "title": "AEkNN: An AutoEncoder kNN-based classifier with built-in dimensionality\n  reduction", "comments": "35 pages, 13 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensionality, i.e. data having a large number of variables, tends to\nbe a challenge for most machine learning tasks, including classification. A\nclassifier usually builds a model representing how a set of inputs explain the\noutputs. The larger is the set of inputs and/or outputs, the more complex would\nbe that model. There is a family of classification algorithms, known as lazy\nlearning methods, which does not build a model. One of the best known members\nof this family is the kNN algorithm. Its strategy relies on searching a set of\nnearest neighbors, using the input variables as position vectors and computing\ndistances among them. These distances loss significance in high-dimensional\nspaces. Therefore kNN, as many other classifiers, tends to worse its\nperformance as the number of input variables grows.\n  In this work AEkNN, a new kNN-based algorithm with built-in dimensionality\nreduction, is presented. Aiming to obtain a new representation of the data,\nhaving a lower dimensionality but with more informational features, AEkNN\ninternally uses autoencoders. From this new feature vectors the computed\ndistances should be more significant, thus providing a way to choose better\nneighbors. A experimental evaluation of the new proposal is conducted,\nanalyzing several configurations and comparing them against the classical kNN\nalgorithm. The obtained conclusions demonstrate that AEkNN offers better\nresults in predictive and runtime performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 10:02:02 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 10:23:51 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Pulgar", "Francisco J.", ""], ["Charte", "Francisco", ""], ["Rivera", "Antonio J.", ""], ["del Jesus", "Mar\u00eda J.", ""]]}, {"id": "1802.08478", "submitter": "Wlodzislaw Duch", "authors": "Wlodzislaw Duch", "title": "Coloring black boxes: visualization of neural network decisions", "comments": "9 pages, 33 figures. Proc. of International Joint Conference on\n  Neural Networks (IJCNN) 2003, Vol. I, pp. 1735-1740", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are commonly regarded as black boxes performing\nincomprehensible functions. For classification problems networks provide maps\nfrom high dimensional feature space to K-dimensional image space. Images of\ntraining vector are projected on polygon vertices, providing visualization of\nnetwork function. Such visualization may show the dynamics of learning, allow\nfor comparison of different networks, display training vectors around which\npotential problems may arise, show differences due to regularization and\noptimization procedures, investigate stability of network classification under\nperturbation of original vectors, and place new data sample in relation to\ntraining data, allowing for estimation of confidence in classification of a\ngiven sample. An illustrative example for the three-class Wine data and\nfive-class Satimage data is described. The visualization method proposed here\nis applicable to any black box system that provides continuous outputs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 10:59:40 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Duch", "Wlodzislaw", ""]]}, {"id": "1802.08530", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell", "title": "Training wide residual networks for deployment using a single bit for\n  each weight", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": "ICLR 2018 - International Conference on Learning Representations,\n  Apr 2018, Vancouver, Canada. 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:54:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["McDonnell", "Mark D.", ""]]}, {"id": "1802.08535", "submitter": "Edward Grefenstette", "authors": "Richard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward\n  Grefenstette", "title": "Can Neural Networks Understand Logical Entailment?", "comments": "Published at ICLR 2018 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset of logical entailments for the purpose of\nmeasuring models' ability to capture and exploit the structure of logical\nexpressions against an entailment prediction task. We use this task to compare\na series of architectures which are ubiquitous in the sequence-processing\nliterature, in addition to a new model class---PossibleWorldNets---which\ncomputes entailment as a \"convolution over possible worlds\". Results show that\nconvolutional networks present the wrong inductive bias for this class of\nproblems relative to LSTM RNNs, tree-structured neural networks outperform LSTM\nRNNs due to their enhanced ability to exploit the syntax of logic, and\nPossibleWorldNets outperform all benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 14:04:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Evans", "Richard", ""], ["Saxton", "David", ""], ["Amos", "David", ""], ["Kohli", "Pushmeet", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1802.08567", "submitter": "Alireza Bagheri", "authors": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "title": "Adversarial Training for Probabilistic Spiking Neural Networks", "comments": "Submitted for possible publication. arXiv admin note: text overlap\n  with arXiv:1710.10704", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers trained using conventional empirical risk minimization or maximum\nlikelihood methods are known to suffer dramatic performance degradations when\ntested over examples adversarially selected based on knowledge of the\nclassifier's decision rule. Due to the prominence of Artificial Neural Networks\n(ANNs) as classifiers, their sensitivity to adversarial examples, as well as\nrobust training schemes, have been recently the subject of intense\ninvestigation. In this paper, for the first time, the sensitivity of spiking\nneural networks (SNNs), or third-generation neural networks, to adversarial\nexamples is studied. The study considers rate and time encoding, as well as\nrate and first-to-spike decoding. Furthermore, a robust training mechanism is\nproposed that is demonstrated to enhance the performance of SNNs under\nwhite-box attacks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 04:33:32 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 19:26:30 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Bagheri", "Alireza", ""], ["Simeone", "Osvaldo", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1802.08590", "submitter": "Andr\\'e R\\\"ohm", "authors": "Andr\\'e R\\\"ohm, Kathy L\\\"udge", "title": "Reservoir computing with simple oscillators: Virtual and real networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reservoir computing scheme is a machine learning mechanism which utilizes\nthe naturally occuring computational capabilities of dynamical systems. One\nimportant subset of systems that has proven powerful both in experiments and\ntheory are delay-systems. In this work, we investigate the reservoir computing\nperformance of hybrid network-delay systems systematically by evaluating the\nNARMA10 and the Sante Fe task.. We construct 'multiplexed networks' that can be\nseen as intermediate steps on the scale from classical networks to the 'virtual\nnetworks' of delay systems. We find that the delay approach can be extended to\nthe network case without loss of computational power, enabling the construction\nof faster reservoir computing substrates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:20:45 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["R\u00f6hm", "Andr\u00e9", ""], ["L\u00fcdge", "Kathy", ""]]}, {"id": "1802.08729", "submitter": "Waleed Alomoush", "authors": "Waleed Alomoush, Ayat Alrosan", "title": "Review: Metaheuristic Search-Based Fuzzy Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fuzzy clustering is a famous unsupervised learning method used to collecting\nsimilar data elements within cluster according to some similarity measurement.\nBut, clustering algorithms suffer from some drawbacks. Among the main weakness\nincluding, selecting the initial cluster centres and the appropriate clusters\nnumber is normally unknown. These weaknesses are considered the most\nchallenging tasks in clustering algorithms. This paper introduces a\ncomprehensive review of metahueristic search to solve fuzzy clustering\nalgorithms problems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 17:24:31 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Alomoush", "Waleed", ""], ["Alrosan", "Ayat", ""]]}, {"id": "1802.08760", "submitter": "Roman Novak", "authors": "Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington,\n  Jascha Sohl-Dickstein", "title": "Sensitivity and Generalization in Neural Networks: an Empirical Study", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice it is often found that large over-parameterized neural networks\ngeneralize better than their smaller counterparts, an observation that appears\nto conflict with classical notions of function complexity, which typically\nfavor smaller models. In this work, we investigate this tension between\ncomplexity and generalization through an extensive empirical exploration of two\nnatural metrics of complexity related to sensitivity to input perturbations.\nOur experiments survey thousands of models with various fully-connected\narchitectures, optimizers, and other hyper-parameters, as well as four\ndifferent image classification datasets.\n  We find that trained neural networks are more robust to input perturbations\nin the vicinity of the training data manifold, as measured by the norm of the\ninput-output Jacobian of the network, and that it correlates well with\ngeneralization. We further establish that factors associated with poor\ngeneralization $-$ such as full-batch training or using random labels $-$\ncorrespond to lower robustness, while factors associated with good\ngeneralization $-$ such as data augmentation and ReLU non-linearities $-$ give\nrise to more robust functions. Finally, we demonstrate how the input-output\nJacobian norm can be predictive of generalization at the level of individual\ntest points.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:11:07 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 23:45:21 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 18:01:43 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Novak", "Roman", ""], ["Bahri", "Yasaman", ""], ["Abolafia", "Daniel A.", ""], ["Pennington", "Jeffrey", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1802.08788", "submitter": "Yanan Sun", "authors": "Yanan Sun, Gary G. Yen, Zhang Yi", "title": "Improved Regularity Model-based EDA for Many-objective Optimization", "comments": "This paper has been accepted by IEEE Transactions on Evolutionary\n  Computation", "journal-ref": null, "doi": "10.1109/TEVC.2018.2794319", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of multi-objective evolutionary algorithms deteriorates\nappreciably in solving many-objective optimization problems which encompass\nmore than three objectives. One of the known rationales is the loss of\nselection pressure which leads to the selected parents not generating promising\noffspring towards Pareto-optimal front with diversity. Estimation of\ndistribution algorithms sample new solutions with a probabilistic model built\nfrom the statistics extracting over the existing solutions so as to mitigate\nthe adverse impact of genetic operators. In this paper, an improved\nregularity-based estimation of distribution algorithm is proposed to\neffectively tackle unconstrained many-objective optimization problems. In the\nproposed algorithm, \\emph{diversity repairing mechanism} is utilized to mend\nthe areas where need non-dominated solutions with a closer proximity to the\nPareto-optimal front. Then \\emph{favorable solutions} are generated by the\nmodel built from the regularity of the solutions surrounding a group of\nrepresentatives. These two steps collectively enhance the selection pressure\nwhich gives rise to the superior convergence of the proposed algorithm. In\naddition, dimension reduction technique is employed in the decision space to\nspeed up the estimation search of the proposed algorithm. Finally, by assigning\nthe Pareto-optimal solutions to the uniformly distributed reference vectors, a\nset of solutions with excellent diversity and convergence is obtained. To\nmeasure the performance, NSGA-III, GrEA, MOEA/D, HypE, MBN-EDA, and RM-MEDA are\nselected to perform comparison experiments over DTLZ and DTLZ$^-$ test suites\nwith $3$-, $5$-, $8$-, $10$-, and $15$-objective. Experimental results\nquantified by the selected performance metrics reveal that the proposed\nalgorithm shows considerable competitiveness in addressing unconstrained\nmany-objective optimization problems.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 03:08:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Sun", "Yanan", ""], ["Yen", "Gary G.", ""], ["Yi", "Zhang", ""]]}, {"id": "1802.08792", "submitter": "Yanan Sun", "authors": "Yanan Sun, Gary G. Yen, Zhang Yi", "title": "IGD Indicator-based Evolutionary Algorithm for Many-objective\n  Optimization Problems", "comments": "This paper has been accepted by IEEE Transactions on Evolutionary\n  Computation", "journal-ref": null, "doi": "10.1109/TEVC.2018.2791283", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverted Generational Distance (IGD) has been widely considered as a reliable\nperformance indicator to concurrently quantify the convergence and diversity of\nmulti- and many-objective evolutionary algorithms. In this paper, an IGD\nindicator-based evolutionary algorithm for solving many-objective optimization\nproblems (MaOPs) has been proposed. Specifically, the IGD indicator is employed\nin each generation to select the solutions with favorable convergence and\ndiversity. In addition, a computationally efficient dominance comparison method\nis designed to assign the rank values of solutions along with three newly\nproposed proximity distance assignments. Based on these two designs, the\nsolutions are selected from a global view by linear assignment mechanism to\nconcern the convergence and diversity simultaneously. In order to facilitate\nthe accuracy of the sampled reference points for the calculation of IGD\nindicator, we also propose an efficient decomposition-based nadir point\nestimation method for constructing the Utopian Pareto front which is regarded\nas the best approximate Pareto front for real-world MaOPs at the early stage of\nthe evolution. To evaluate the performance, a series of experiments is\nperformed on the proposed algorithm against a group of selected\nstate-of-the-art many-objective optimization algorithms over optimization\nproblems with $8$-, $15$-, and $20$-objective. Experimental results measured by\nthe chosen performance metrics indicate that the proposed algorithm is very\ncompetitive in addressing MaOPs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 03:50:33 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Sun", "Yanan", ""], ["Yen", "Gary G.", ""], ["Yi", "Zhang", ""]]}, {"id": "1802.08842", "submitter": "Patryk Chrabaszcz", "authors": "Patryk Chrabaszcz, Ilya Loshchilov, Frank Hutter", "title": "Back to Basics: Benchmarking Canonical Evolution Strategies for Playing\n  Atari", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution Strategies (ES) have recently been demonstrated to be a viable\nalternative to reinforcement learning (RL) algorithms on a set of challenging\ndeep RL problems, including Atari games and MuJoCo humanoid locomotion\nbenchmarks. While the ES algorithms in that work belonged to the specialized\nclass of natural evolution strategies (which resemble approximate gradient RL\nalgorithms, such as REINFORCE), we demonstrate that even a very basic canonical\nES algorithm can achieve the same or even better performance. This success of a\nbasic ES algorithm suggests that the state-of-the-art can be advanced further\nby integrating the many advances made in the field of ES in the last decades.\n  We also demonstrate qualitatively that ES algorithms have very different\nperformance characteristics than traditional RL algorithms: on some games, they\nlearn to exploit the environment and perform much better while on others they\ncan get stuck in suboptimal local minima. Combining their strengths with those\nof traditional RL algorithms is therefore likely to lead to new advances in the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 12:43:18 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Chrabaszcz", "Patryk", ""], ["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1802.08989", "submitter": "Yongsheng Liang", "authors": "Yongsheng Liang, Zhigang Ren, Xianghua Yao, Zuren Feng, An Chen", "title": "Enhancing Gaussian Estimation of Distribution Algorithm by Exploiting\n  Evolution Direction with Archive", "comments": "We modified some experiments in this manuscript that it has to be\n  rewritten. With the agreement of my co-authors, we would like to withdraw\n  this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a typical model-based evolutionary algorithm (EA), estimation of\ndistribution algorithm (EDA) possesses unique characteristics and has been\nwidely applied to global optimization. However, the common-used Gaussian EDA\n(GEDA) usually suffers from premature convergence which severely limits its\nsearch efficiency. This study first systematically analyses the reasons for the\ndeficiency of the traditional GEDA, then tries to enhance its performance by\nexploiting its evolution direction, and finally develops a new GEDA variant\nnamed EDA2. Instead of only utilizing some good solutions produced in the\ncurrent generation when estimating the Gaussian model, EDA2 preserves a certain\nnumber of high-quality solutions generated in previous generations into an\narchive and takes advantage of these historical solutions to assist estimating\nthe covariance matrix of Gaussian model. By this means, the evolution direction\ninformation hidden in the archive is naturally integrated into the estimated\nmodel which in turn can guide EDA2 towards more promising solution regions.\nMoreover, the new estimation method significantly reduces the population size\nof EDA2 since it needs fewer individuals in the current population for model\nestimation. As a result, a fast convergence can be achieved. To verify the\nefficiency of EDA2, we tested it on a variety of benchmark functions and\ncompared it with several state-of-the-art EAs, including IPOP-CMAES, AMaLGaM,\nthree high-powered DE algorithms, and a new PSO algorithm. The experimental\nresults demonstrate that EDA2 is efficient and competitive.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 11:26:02 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 07:11:46 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Liang", "Yongsheng", ""], ["Ren", "Zhigang", ""], ["Yao", "Xianghua", ""], ["Feng", "Zuren", ""], ["Chen", "An", ""]]}, {"id": "1802.09046", "submitter": "Hardik Meisheri", "authors": "Hardik Meisheri, Nagraj Ramrao, Suman Mitra", "title": "Multiclass Common Spatial Pattern for EEG based Brain Computer Interface\n  with Adaptive Learning Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Brain Computer Interface (BCI), data generated from Electroencephalogram\n(EEG) is non-stationary with low signal to noise ratio and contaminated with\nartifacts. Common Spatial Pattern (CSP) algorithm has been proved to be\neffective in BCI for extracting features in motor imagery tasks, but it is\nprone to overfitting. Many algorithms have been devised to regularize CSP for\ntwo class problem, however they have not been effective when applied to\nmulticlass CSP. Outliers present in data affect extracted CSP features and\nreduces performance of the system. In addition to this non-stationarity present\nin the features extracted from the CSP present a challenge in classification.\nWe propose a method to identify and remove artifact present in the data during\npre-processing stage, this helps in calculating eigenvectors which in turn\ngenerates better CSP features. To handle the non-stationarity, Self-Regulated\nInterval Type-2 Neuro-Fuzzy Inference System (SRIT2NFIS) was proposed in the\nliterature for two class EEG classification problem. This paper extends the\nSRIT2NFIS to multiclass using Joint Approximate Diagonalization (JAD). The\nresults on standard data set from BCI competition IV shows significant increase\nin the accuracies from the current state of the art methods for multiclass\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 17:18:31 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 04:04:47 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Meisheri", "Hardik", ""], ["Ramrao", "Nagraj", ""], ["Mitra", "Suman", ""]]}, {"id": "1802.09047", "submitter": "Anand Kumar Mukhopadhyay", "authors": "Anand Kumar Mukhopadhyay, Indrajit Chakrabarti, Arindam Basu, Mrigank\n  Sharad", "title": "Power efficient Spiking Neural Network Classifier based on memristive\n  crossbar network for spike sorting application", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper authors have presented a power efficient scheme for\nimplementing a spike sorting module. Spike sorting is an important application\nin the field of neural signal acquisition for implantable biomedical systems\nwhose function is to map the Neural-spikes (N-spikes) correctly to the neurons\nfrom which it originates. The accurate classification is a pre-requisite for\nthe succeeding systems needed in Brain-Machine-Interfaces (BMIs) to give better\nperformance. The primary design constraint to be satisfied for the spike sorter\nmodule is low power with good accuracy. There lies a trade-off in terms of\npower consumption between the on-chip and off-chip training of the N-spike\nfeatures. In the former case care has to be taken to make the computational\nunits power efficient whereas in the later the data rate of wireless\ntransmission should be minimized to reduce the power consumption due to the\ntransceivers. In this work a 2-step shared training scheme involving a K-means\nsorter and a Spiking Neural Network (SNN) is elaborated for on-chip training\nand classification. Also, a low power SNN classifier scheme using memristive\ncrossbar type architecture is compared with a fully digital implementation. The\nadvantage of the former classifier is that it is power efficient while\nproviding comparable accuracy as that of the digital implementation due to the\nrobustness of the SNN training algorithm which has a good tolerance for\nvariation in memristance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 17:34:43 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mukhopadhyay", "Anand Kumar", ""], ["Chakrabarti", "Indrajit", ""], ["Basu", "Arindam", ""], ["Sharad", "Mrigank", ""]]}, {"id": "1802.09121", "submitter": "Ryan Williams", "authors": "R. Ryan Williams", "title": "Limits on representing Boolean functions by linear combinations of\n  simple functions: thresholds, ReLUs, and low-degree polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of representing Boolean functions exactly by \"sparse\"\nlinear combinations (over $\\mathbb{R}$) of functions from some \"simple\" class\n${\\cal C}$. In particular, given ${\\cal C}$ we are interested in finding\nlow-complexity functions lacking sparse representations. When ${\\cal C}$ is the\nset of PARITY functions or the set of conjunctions, this sort of problem has a\nwell-understood answer, the problem becomes interesting when ${\\cal C}$ is\n\"overcomplete\" and the set of functions is not linearly independent. We focus\non the cases where ${\\cal C}$ is the set of linear threshold functions, the set\nof rectified linear units (ReLUs), and the set of low-degree polynomials over a\nfinite field, all of which are well-studied in different contexts.\n  We provide generic tools for proving lower bounds on representations of this\nkind. Applying these, we give several new lower bounds for \"semi-explicit\"\nBoolean functions. For example, we show there are functions in nondeterministic\nquasi-polynomial time that require super-polynomial size:\n  $\\bullet$ Depth-two neural networks with sign activation function, a special\ncase of depth-two threshold circuit lower bounds.\n  $\\bullet$ Depth-two neural networks with ReLU activation function.\n  $\\bullet$ $\\mathbb{R}$-linear combinations of $O(1)$-degree\n$\\mathbb{F}_p$-polynomials, for every prime $p$ (related to problems regarding\nHigher-Order \"Uncertainty Principles\"). We also obtain a function in $E^{NP}$\nrequiring $2^{\\Omega(n)}$ linear combinations.\n  $\\bullet$ $\\mathbb{R}$-linear combinations of $ACC \\circ THR$ circuits of\npolynomial size (further generalizing the recent lower bounds of Murray and the\nauthor).\n  (The above is a shortened abstract. For the full abstract, see the paper.)\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 01:30:21 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Williams", "R. Ryan", ""]]}, {"id": "1802.09332", "submitter": "Mahardhika Pratama Dr", "authors": "Wahyu Caesarendra, Mahardhika Pratama, Tegoeh Tjahjowidodo, Kiet\n  Tieud, and Buyung Kosasih", "title": "Parsimonious Network based on Fuzzy Inference System (PANFIS) for Time\n  Series Feature Prediction of Low Speed Slew Bearing Prognosis", "comments": "this paper is currently under review in Journal of Intelligence\n  Manufacturing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the utilization of rotating parts, e.g. bearings and gears,\nhas been continuously supporting the manufacturing line to produce consistent\noutput quality. Due to their critical role, the breakdown of these components\nmight significantly impact the production rate. A proper condition based\nmonitoring (CBM) is among a few ways to maintain and monitor the rotating\nsystems. Prognosis, as one of the major tasks in CBM that predicts and\nestimates the remaining useful life of the machine, has attracted significant\ninterest in decades. This paper presents a literature review on prognosis\napproaches from published papers in the last decade. The prognostic approaches\nare described comprehensively to provide a better idea on how to select an\nappropriate prognosis method for specific needs. An advanced predictive\nanalytics, namely Parsimonious Network Based on Fuzzy Inference System\n(PANFIS), was proposed and tested into the low speed slew bearing data. PANFIS\ndiffers itself from conventional prognostic approaches in which it supports for\nonline lifelong prognostics without the requirement of retraining or\nreconfiguration phase. The method is applied to normal-to-failure bearing\nvibration data collected for 139 days and to predict the time-domain features\nof vibration slew bearing signals. The performance of the proposed method is\ncompared to some established methods such as ANFIS, eTS, and Simp_eTS. From the\nresults, it is suggested that PANFIS offers outstanding performance compared to\nthose of other methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 04:31:27 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Caesarendra", "Wahyu", ""], ["Pratama", "Mahardhika", ""], ["Tjahjowidodo", "Tegoeh", ""], ["Tieud", "Kiet", ""], ["Kosasih", "Buyung", ""]]}, {"id": "1802.09405", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Danilo Comminiello, Aurelio\n  Uncini", "title": "Improving Graph Convolutional Networks with Non-Parametric Activation\n  Functions", "comments": "Submitted to EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are a class of neural networks that allow to\nefficiently perform inference on data that is associated to a graph structure,\nsuch as, e.g., citation networks or knowledge graphs. While several variants of\nGNNs have been proposed, they only consider simple nonlinear activation\nfunctions in their layers, such as rectifiers or squashing functions. In this\npaper, we investigate the use of graph convolutional networks (GCNs) when\ncombined with more complex activation functions, able to adapt from the\ntraining data. More specifically, we extend the recently proposed kernel\nactivation function, a non-parametric model which can be implemented easily,\ncan be regularized with standard $\\ell_p$-norms techniques, and is smooth over\nits entire domain. Our experimental evaluation shows that the proposed\narchitecture can significantly improve over its baseline, while similar\nimprovements cannot be obtained by simply increasing the depth or size of the\noriginal GCN.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:36:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Comminiello", "Danilo", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1802.09660", "submitter": "George Leu", "authors": "George Leu and Hussein Abbass", "title": "Computational Red Teaming in a Sudoku Solving Context: Neural Network\n  Based Skill Representation and Acquisition", "comments": null, "journal-ref": "Proceedings in Adaptation, Learning and Optimization, vol. 5,\n  Springer, 2016", "doi": "10.1007/978-3-319-27000-5_26", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an insight into the skill representation, where\nskill representation is seen as an essential part of the skill assessment stage\nin the Computational Red Teaming process. Skill representation is demonstrated\nin the context of Sudoku puzzle, for which the real human skills used in Sudoku\nsolving, along with their acquisition, are represented computationally in a\ncognitively plausible manner, by using feed-forward neural networks with\nback-propagation, and supervised learning. The neural network based skills are\nthen coupled with a hard-coded constraint propagation computational Sudoku\nsolver, in which the solving sequence is kept hard-coded, and the skills are\nrepresented through neural networks. The paper demonstrates that the modified\nsolver can achieve different levels of proficiency, depending on the amount of\nskills acquired through the neural networks. Results are encouraging for\ndeveloping more complex skill and skill acquisition models usable in general\nframeworks related to the skill assessment aspect of Computational Red Teaming.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:49:45 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Leu", "George", ""], ["Abbass", "Hussein", ""]]}, {"id": "1802.09703", "submitter": "Zhigang Ren", "authors": "Zhigang Ren, Yongsheng Liang, Aimin Zhang, Yang Yang, Zuren Feng, Lin\n  Wang", "title": "Boosting Cooperative Coevolution for Large Scale Optimization with a\n  Fine-Grained Computation Resource Allocation Strategy", "comments": "We changed the experiments in this manuscript that it has to be\n  rewritten. With the agreement of my co-authors, we would like to withdraw\n  this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative coevolution (CC) has shown great potential in solving large scale\noptimization problems (LSOPs). However, traditional CC algorithms often waste\npart of computation resource (CR) as they equally allocate CR among all the\nsubproblems. The recently developed contribution-based CC (CBCC) algorithms\nimprove the traditional ones to a certain extent by adaptively allocating CR\naccording to some heuristic rules. Different from existing works, this study\nexplicitly constructs a mathematical model for the CR allocation (CRA) problem\nin CC and proposes a novel fine-grained CRA (FCRA) strategy by fully\nconsidering both the theoretically optimal solution of the CRA model and the\nevolution characteristics of CC. FCRA takes a single iteration as a basic CRA\nunit and always selects the subproblem which is most likely to make the largest\ncontribution to the total fitness improvement to undergo a new iteration, where\nthe contribution of a subproblem at a new iteration is estimated according to\nits current contribution, current evolution status as well as the estimation\nfor its current contribution. We verified the efficiency of FCRA by combining\nit with SHADE which is an excellent differential evolution variant but has\nnever been employed in the CC framework. Experimental results on two benchmark\nsuites for LSOPs demonstrate that FCRA significantly outperforms existing CRA\nstrategies and the resultant CC algorithm is highly competitive in solving\nLSOPs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 03:30:30 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 02:12:47 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Ren", "Zhigang", ""], ["Liang", "Yongsheng", ""], ["Zhang", "Aimin", ""], ["Yang", "Yang", ""], ["Feng", "Zuren", ""], ["Wang", "Lin", ""]]}, {"id": "1802.09746", "submitter": "Zhigang Ren", "authors": "Zhigang Ren, Bei Pang, Yongsheng Liang, An Chen, Yipeng Zhang", "title": "Surrogate Model Assisted Cooperative Coevolution for Large Scale\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that cooperative coevolution (CC) can effectively deal with\nlarge scale optimization problems (LSOPs) through a divide-and-conquer\nstrategy. However, its performance is severely restricted by the current\ncontext-vector-based sub-solution evaluation method since this method needs to\naccess the original high dimensional simulation model when evaluating each\nsub-solution and thus requires many computation resources. To alleviate this\nissue, this study proposes a novel surrogate model assisted cooperative\ncoevolution (SACC) framework. SACC constructs a surrogate model for each\nsub-problem obtained via decomposition and employs it to evaluate corresponding\nsub-solutions. The original simulation model is only adopted to reevaluate some\ngood sub-solutions selected by surrogate models, and these real evaluated\nsub-solutions will be in turn employed to update surrogate models. By this\nmeans, the computation cost could be greatly reduced without significantly\nsacrificing evaluation quality. To show the efficiency of SACC, this study uses\nradial basis function (RBF) and success-history based adaptive differential\nevolution (SHADE) as surrogate model and optimizer, respectively. RBF and SHADE\nhave been proved to be effective on small and medium scale problems. This study\nfirst scales them up to LSOPs of 1000 dimensions under the SACC framework,\nwhere they are tailored to a certain extent for adapting to the characteristics\nof LSOP and SACC. Empirical studies on IEEE CEC 2010 benchmark functions\ndemonstrate that SACC significantly enhances the evaluation efficiency on\nsub-solutions, and even with much fewer computation resource, the resultant\nRBF-SHADE-SACC algorithm is able to find much better solutions than traditional\nCC algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:59:43 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Ren", "Zhigang", ""], ["Pang", "Bei", ""], ["Liang", "Yongsheng", ""], ["Chen", "An", ""], ["Zhang", "Yipeng", ""]]}, {"id": "1802.09816", "submitter": "Guillaume Charpiat", "authors": "Armand Zampieri (TITANE), Guillaume Charpiat (TAU), Yuliya Tarabalka\n  (TITANE)", "title": "Coarse to fine non-rigid registration: a chain of scale-specific neural\n  networks for multimodal image alignment with application to remote sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle here the problem of multimodal image non-rigid registration, which\nis of prime importance in remote sensing and medical imaging. The difficulties\nencountered by classical registration approaches include feature design and\nslow optimization by gradient descent. By analyzing these methods, we note the\nsignificance of the notion of scale. We design easy-to-train,\nfully-convolutional neural networks able to learn scale-specific features. Once\nchained appropriately, they perform global registration in linear time, getting\nrid of gradient descent schemes by predicting directly the deformation.We show\ntheir performance in terms of quality and speed through various tasks of remote\nsensing multimodal image alignment. In particular, we are able to register\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\nimages, and outperform current keypoint matching methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 10:47:06 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zampieri", "Armand", "", "TITANE"], ["Charpiat", "Guillaume", "", "TAU"], ["Tarabalka", "Yuliya", "", "TITANE"]]}, {"id": "1802.09913", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein, Sebastian Ruder, Anders S{\\o}gaard", "title": "Multi-task Learning of Pairwise Sequence Classification Tasks Over\n  Disparate Label Spaces", "comments": "To appear at NAACL 2018 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine multi-task learning and semi-supervised learning by inducing a\njoint embedding space between disparate label spaces and learning transfer\nfunctions between label embeddings, enabling us to jointly leverage unlabelled\ndata and auxiliary, annotated datasets. We evaluate our approach on a variety\nof sequence classification tasks with disparate label spaces. We outperform\nstrong single and multi-task baselines and achieve a new state-of-the-art for\ntopic-based sentiment analysis.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 14:38:43 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 07:34:47 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Ruder", "Sebastian", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1802.09941", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Torsten Hoefler", "title": "Demystifying Parallel and Distributed Deep Learning: An In-Depth\n  Concurrency Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are becoming an important tool in modern\ncomputing applications. Accelerating their training is a major challenge and\ntechniques range from distributed algorithms to low-level circuit design. In\nthis survey, we describe the problem from a theoretical perspective, followed\nby approaches for its parallelization. We present trends in DNN architectures\nand the resulting implications on parallelization strategies. We then review\nand model the different types of concurrency in DNNs: from the single operator,\nthrough parallelism in network inference and training, to distributed deep\nlearning. We discuss asynchronous stochastic optimization, distributed system\narchitectures, communication schemes, and neural architecture search. Based on\nthose approaches, we extrapolate potential directions for parallelism in deep\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 08:47:34 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 08:36:28 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1802.10203", "submitter": "Jiangjun Tang", "authors": "Jiangjun Tang and Hussein A. Abbass", "title": "Behavioral Learning of Aircraft Landing Sequencing Using a Society of\n  Probabilistic Finite State Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air Traffic Control (ATC) is a complex safety critical environment. A tower\ncontroller would be making many decisions in real-time to sequence aircraft.\nWhile some optimization tools exist to help the controller in some airports,\neven in these situations, the real sequence of the aircraft adopted by the\ncontroller is significantly different from the one proposed by the optimization\nalgorithm. This is due to the very dynamic nature of the environment. The\nobjective of this paper is to test the hypothesis that one can learn from the\nsequence adopted by the controller some strategies that can act as heuristics\nin decision support tools for aircraft sequencing. This aim is tested in this\npaper by attempting to learn sequences generated from a well-known sequencing\nmethod that is being used in the real world. The approach relies on a genetic\nalgorithm (GA) to learn these sequences using a society Probabilistic\nFinite-state Machines (PFSMs). Each PFSM learns a different sub-space; thus,\ndecomposing the learning problem into a group of agents that need to work\ntogether to learn the overall problem. Three sequence metrics (Levenshtein,\nHamming and Position distances) are compared as the fitness functions in GA. As\nthe results suggest, it is possible to learn the behavior of the\nalgorithm/heuristic that generated the original sequence from very limited\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 23:07:52 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Tang", "Jiangjun", ""], ["Abbass", "Hussein A.", ""]]}, {"id": "1802.10206", "submitter": "Jiangjun Tang", "authors": "Jiangjun Tang, George Leu and Hussein Abbass", "title": "Networking the Boids is More Robust Against Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm behavior using Boids-like models has been studied primarily using\nclose-proximity spatial sensory information (e.g. vision range). In this study,\nwe propose a novel approach in which the classic definition of\nboids\\textquoteright \\ neighborhood that relies on sensory perception and\nEuclidian space locality is replaced with graph-theoretic network-based\nproximity mimicking communication and social networks. We demonstrate that\nnetworking the boids leads to faster swarming and higher quality of the\nformation. We further investigate the effect of adversarial learning, whereby\nan observer attempts to reverse engineer the dynamics of the swarm through\nobserving its behavior. The results show that networking the swarm demonstrated\na more robust approach against adversarial learning than a local-proximity\nneighborhood structure.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 23:20:16 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Tang", "Jiangjun", ""], ["Leu", "George", ""], ["Abbass", "Hussein", ""]]}, {"id": "1802.10301", "submitter": "Vsevolod Avrutskiy", "authors": "V.I. Avrutskiy", "title": "Avoiding overfitting of multilayer perceptrons by training derivatives", "comments": null, "journal-ref": "Proceedings of the Future Technologies Conference (FTC) 2019.\n  Advances in Intelligent Systems and Computing, vol 1069", "doi": "10.1007/978-3-030-32520-6_12", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistance to overfitting is observed for neural networks trained with\nextended backpropagation algorithm. In addition to target values, its cost\nfunction uses derivatives of those up to the $4^{\\mathrm{th}}$ order. For\ncommon applications of neural networks, high order derivatives are not readily\navailable, so simpler cases are considered: training network to approximate\nanalytical function inside 2D and 5D domains and solving Poisson equation\ninside a 2D circle. For function approximation, the cost is a sum of squared\ndifferences between output and target as well as their derivatives with respect\nto the input. Differential equations are usually solved by putting a multilayer\nperceptron in place of unknown function and training its weights, so that\nequation holds within some margin of error. Commonly used cost is the\nequation's residual squared. Added terms are squared derivatives of said\nresidual with respect to the independent variables. To investigate overfitting,\nthe cost is minimized for points of regular grids with various spacing, and its\nroot mean is compared with its value on much denser test set. Fully connected\nperceptrons with six hidden layers and $2\\cdot10^{4}$, $1\\cdot10^{6}$ and\n$5\\cdot10^{6}$ weights in total are trained with Rprop until cost changes by\nless than 10% for last 1000 epochs, or when the $10000^{\\mathrm{th}}$ epoch is\nreached. Training the network with $5\\cdot10^{6}$ weights to represent simple\n2D function using 10 points with 8 extra derivatives in each produces cost test\nto train ratio of $1.5$, whereas for classical backpropagation in comparable\nconditions this ratio is $2\\cdot10^{4}$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 08:16:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Avrutskiy", "V. I.", ""]]}, {"id": "1802.10328", "submitter": "Tatsunori Taniai", "authors": "Tatsunori Taniai and Takanori Maehara", "title": "Neural Inverse Rendering for General Reflectance Photometric Stereo", "comments": "To appear in International Conference on Machine Learning 2018 (ICML\n  2018). 10 pages + 20 pages (appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel convolutional neural network architecture for photometric\nstereo (Woodham, 1980), a problem of recovering 3D object surface normals from\nmultiple images observed under varying illuminations. Despite its long history\nin computer vision, the problem still shows fundamental challenges for surfaces\nwith unknown general reflectance properties (BRDFs). Leveraging deep neural\nnetworks to learn complicated reflectance models is promising, but studies in\nthis direction are very limited due to difficulties in acquiring accurate\nground truth for training and also in designing networks invariant to\npermutation of input images. In order to address these challenges, we propose a\nphysics based unsupervised learning framework where surface normals and BRDFs\nare predicted by the network and fed into the rendering equation to synthesize\nobserved images. The network weights are optimized during testing by minimizing\nreconstruction loss between observed and synthesized images. Thus, our learning\nprocess does not require ground truth normals or even pre-training on external\nimages. Our method is shown to achieve the state-of-the-art performance on a\nchallenging real-world scene benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:47:20 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 09:15:37 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Taniai", "Tatsunori", ""], ["Maehara", "Takanori", ""]]}, {"id": "1802.10353", "submitter": "Sjoerd van Steenkiste", "authors": "Sjoerd van Steenkiste, Michael Chang, Klaus Greff, J\\\"urgen\n  Schmidhuber", "title": "Relational Neural Expectation Maximization: Unsupervised Discovery of\n  Objects and their Interactions", "comments": "Accepted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Common-sense physical reasoning is an essential ingredient for any\nintelligent agent operating in the real-world. For example, it can be used to\nsimulate the environment, or to infer the state of parts of the world that are\ncurrently unobserved. In order to match real-world conditions this causal\nknowledge must be learned without access to supervised data. To address this\nproblem we present a novel method that learns to discover objects and model\ntheir physical interactions from raw visual images in a purely\n\\emph{unsupervised} fashion. It incorporates prior knowledge about the\ncompositional nature of human perception to factor interactions between\nobject-pairs and learn efficiently. On videos of bouncing balls we show the\nsuperior modelling capabilities of our method compared to other unsupervised\nneural approaches that do not incorporate such prior knowledge. We demonstrate\nits ability to handle occlusion and show that it can extrapolate learned\nknowledge to scenes with different numbers of objects.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 10:55:36 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["van Steenkiste", "Sjoerd", ""], ["Chang", "Michael", ""], ["Greff", "Klaus", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1802.10393", "submitter": "Guilherme Wachs-Lopes", "authors": "Henrique X. Goulart, Guilherme A. Wachs-Lopes", "title": "A Bayesian Model for Activities Recommendation and Event Structure\n  Optimization Using Visitors Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In events that are composed by many activities, there is a problem that\ninvolves retrieve and management the information of visitors that are visiting\nthe activities. This management is crucial to find some activities that are\ndrawing attention of visitors; identify an ideal positioning for activities;\nwhich path is more frequented by visitors. In this work, these features are\nstudied using Complex Network theory. For the beginning, an artificial database\nwas generated to study the mentioned features. Secondly, this work shows a\nmethod to optimize the event structure that is better than a random method and\na recommendation system that achieves ~95% of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 12:59:43 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Goulart", "Henrique X.", ""], ["Wachs-Lopes", "Guilherme A.", ""]]}]