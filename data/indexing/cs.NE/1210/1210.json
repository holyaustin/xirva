[{"id": "1210.0118", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "Self-Delimiting Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "IDSIA-08-12", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-delimiting (SLIM) programs are a central concept of theoretical computer\nscience, particularly algorithmic information & probability theory, and\nasymptotically optimal program search (AOPS). To apply AOPS to (possibly\nrecurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typical\nSLIM NN have threshold activation functions. During a computational episode,\nactivations are spreading from input neurons through the SLIM NN until the\ncomputation activates a special halt neuron. Weights of the NN's used\nconnections define its program. Halting programs form a prefix code. The reset\nof the initial NN state does not cost more than the latest program execution.\nSince prefixes of SLIM programs influence their suffixes (weight changes\noccurring early in an episode influence which weights are considered later),\nSLIM NN learning algorithms (LAs) should execute weight changes online during\nactivation spreading. This can be achieved by applying AOPS to growing SLIM\nNNs. To efficiently teach a SLIM NN to solve many tasks, such as correctly\nclassifying many different patterns, or solving many different robot control\ntasks, each connection keeps a list of tasks it is used for. The lists may be\nefficiently updated during training. To evaluate the overall effect of\ncurrently tested weight changes, a SLIM NN LA needs to re-test performance only\non the efficiently computable union of tasks potentially affected by the\ncurrent weight changes. Future SLIM NNs will be implemented on 3-dimensional\nbrain-like multi-processor hardware. Their LAs will minimize task-specific\ntotal wire length of used connections, to encourage efficient solutions of\nsubtasks by subsets of neurons that are physically close. The novel class of\nSLIM NN LAs is currently being probed in ongoing experiments to be reported in\nseparate papers.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 15:38:53 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}, {"id": "1210.0477", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "Think Locally, Act Globally: Perfectly Balanced Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel local improvement scheme for the perfectly balanced graph\npartitioning problem. This scheme encodes local searches that are not\nrestricted to a balance constraint into a model allowing us to find\ncombinations of these searches maintaining balance by applying a negative cycle\ndetection algorithm. We combine this technique with an algorithm to balance\nunbalanced solutions and integrate it into a parallel multi-level evolutionary\nalgorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that is\nfast on the one hand and on the other hand is able to improve or reproduce most\nof the best known perfectly balanced partitioning results ever reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 17:30:47 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1210.1530", "submitter": "Tao Hu", "authors": "Tao Hu, Alexander Genkin and Dmitri B. Chklovskii", "title": "A network of spiking neurons for computing sparse representations in an\n  energy efficient way", "comments": "5 figures Early Access:\n  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00353", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing sparse redundant representations is an important problem both in\napplied mathematics and neuroscience. In many applications, this problem must\nbe solved in an energy efficient way. Here, we propose a hybrid distributed\nalgorithm (HDA), which solves this problem on a network of simple nodes\ncommunicating via low-bandwidth channels. HDA nodes perform both\ngradient-descent-like steps on analog internal variables and\ncoordinate-descent-like steps via quantized external variables communicated to\neach other. Interestingly, such operation is equivalent to a network of\nintegrate-and-fire neurons, suggesting that HDA may serve as a model of neural\ncomputation. We show that the numerical performance of HDA is on par with\nexisting algorithms. In the asymptotic regime the representation error of HDA\ndecays with time, t, as 1/t. HDA is stable against time-varying noise,\nspecifically, the representation error decays as 1/sqrt(t) for Gaussian white\nnoise.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 18:26:03 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Hu", "Tao", ""], ["Genkin", "Alexander", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1210.1983", "submitter": "Dorian Aur", "authors": "Dorian Aur", "title": "Reply to Comments on Neuroelectrodynamics: Where are the Real Conceptual\n  Pitfalls?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.AO physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The fundamental, powerful process of computation in the brain has been widely\nmisunderstood. The paper [1] associates the general failure to build\nintelligent thinking machines with current reductionist principles of temporal\ncoding and advocates for a change in paradigm regarding the brain analogy.\nSince fragments of information are stored in proteins which can shift between\nseveral structures to perform their function, the biological substrate is\nactively involved in physical computation. The intrinsic nonlinear dynamics of\naction potentials and synaptic activities maintain physical interactions within\nand between neurons in the brain. During these events the required information\nis exchanged between molecular structures (proteins) which store fragments of\ninformation and the generated electric flux which carries and integrates\ninformation in the brain. The entire process of physical interaction explains\nhow the brain actively creates or experiences meaning. This process of\ninteraction during an action potential generation can be simply seen as the\nmoment when the neuron solves a many-body problem. A neuroelectrodynamic theory\nshows that the neuron solves equations rather than exclusively computes\nfunctions. With the main focus on temporal patterns, the spike timing dogma\n(STD) has neglected important forms of computation which do occur inside\nneurons. In addition, artificial neural models have missed the most important\npart since the real super-computing power of the brain has its origins in\ncomputations that occur within neurons.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 18:19:11 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Aur", "Dorian", ""]]}, {"id": "1210.3210", "submitter": "Matthew Crossley", "authors": "Matthew Crossley, Andy Nisbet, Martyn Amos", "title": "Fitness Landscape-Based Characterisation of Nature-Inspired Algorithms", "comments": "10 pages, 1 figure, submitted to the 11th International Conference on\n  Adaptive and Natural Computing Algorithms", "journal-ref": null, "doi": "10.1007/978-3-642-37213-1_12", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant challenge in nature-inspired algorithmics is the identification\nof specific characteristics of problems that make them harder (or easier) to\nsolve using specific methods. The hope is that, by identifying these\ncharacteristics, we may more easily predict which algorithms are best-suited to\nproblems sharing certain features. Here, we approach this problem using fitness\nlandscape analysis. Techniques already exist for measuring the \"difficulty\" of\nspecific landscapes, but these are often designed solely with evolutionary\nalgorithms in mind, and are generally specific to discrete optimisation. In\nthis paper we develop an approach for comparing a wide range of continuous\noptimisation algorithms. Using a fitness landscape generation technique, we\ncompare six different nature-inspired algorithms and identify which methods\nperform best on landscapes exhibiting specific features.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 12:40:36 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 11:20:16 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Crossley", "Matthew", ""], ["Nisbet", "Andy", ""], ["Amos", "Martyn", ""]]}, {"id": "1210.3569", "submitter": "Matthew Luciw", "authors": "Sohrob Kazerounian, Matthew Luciw, Mathis Richter and Yulia\n  Sandamirskaya", "title": "Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics", "comments": "Sohrob Kazerounian, Matthew Luciw are Joint first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic neural algorithm called Dynamic Neural (DN)\nSARSA(\\lambda) for learning a behavioral sequence from delayed reward.\nDN-SARSA(\\lambda) combines Dynamic Field Theory models of behavioral sequence\nrepresentation, classical reinforcement learning, and a computational\nneuroscience model of working memory, called Item and Order working memory,\nwhich serves as an eligibility trace. DN-SARSA(\\lambda) is implemented on both\na simulated and real robot that must learn a specific rewarding sequence of\nelementary behaviors from exploration. Results show DN-SARSA(\\lambda) performs\non the level of the discrete SARSA(\\lambda), validating the feasibility of\ngeneral reinforcement learning without compromising neural dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 16:41:58 GMT"}, {"version": "v2", "created": "Tue, 14 May 2013 15:07:35 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Kazerounian", "Sohrob", ""], ["Luciw", "Matthew", ""], ["Richter", "Mathis", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1210.3652", "submitter": "Murphy Choy", "authors": "Murphy Choy, Michelle Cheong", "title": "A Flexible Mixed Integer Programming framework for Nurse Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nurse-scheduling model is developed using mixed integer\nprogramming model. It is deployed to a general care ward to replace and\nautomate the current manual approach for scheduling. The developed model\ndiffers from other similar studies in that it optimizes both hospitals\nrequirement as well as nurse preferences by allowing flexibility in the\ntransfer of nurses from different duties. The model also incorporated\nadditional policies which are part of the hospitals requirement but not part of\nthe legislations. Hospitals key primary mission is to ensure continuous ward\ncare service with appropriate number of nursing staffs and the right mix of\nnursing skills. The planning and scheduling is done to avoid additional non\nessential cost for hospital. Nurses preferences are taken into considerations\nsuch as the number of night shift and consecutive rest days. We will also\nreformulate problems from another paper which considers the penalty objective\nusing the model but without the flexible components. The models are built using\nAIMMS which solves the problem in very short amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 22:33:40 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Choy", "Murphy", ""], ["Cheong", "Michelle", ""]]}, {"id": "1210.3741", "submitter": "Tao Hu", "authors": "Tao Hu and Dmitri B. Chklovskii", "title": "Online computation of sparse representations of time varying stimuli\n  using a biologically motivated neural network", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural stimuli are highly redundant, possessing significant spatial and\ntemporal correlations. While sparse coding has been proposed as an efficient\nstrategy employed by neural systems to encode sensory stimuli, the underlying\nmechanisms are still not well understood. Most previous approaches model the\nneural dynamics by the sparse representation dictionary itself and compute the\nrepresentation coefficients offline. In reality, faced with the challenge of\nconstantly changing stimuli, neurons must compute the sparse representations\ndynamically in an online fashion. Here, we describe a leaky linearized Bregman\niteration (LLBI) algorithm which computes the time varying sparse\nrepresentations using a biologically motivated network of leaky rectifying\nneurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits\nthe temporal correlation of stimuli and demonstrate better performance both in\nrepresentation error and the smoothness of temporal evolution of sparse\ncoefficients.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2012 21:49:32 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Hu", "Tao", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1210.4021", "submitter": "Sebastien Verel", "authors": "Francisco Chicano, Fabio Daolio (ISI), Gabriela Ochoa, S\\'ebastien\n  Verel (INRIA Lille - Nord Europe), Marco Tomassini (ISI), Enrique Alba", "title": "Local Optima Networks, Landscape Autocorrelation and Heuristic Search\n  Performance", "comments": "Parallel Problem Solving from Nature - PPSN XII, Taormina : Italy\n  (2012)", "journal-ref": null, "doi": "10.1007/978-3-642-32964-7_34", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in fitness landscape analysis include the study of Local\nOptima Networks (LON) and applications of the Elementary Landscapes theory.\nThis paper represents a first step at combining these two tools to explore\ntheir ability to forecast the performance of search algorithms. We base our\nanalysis on the Quadratic Assignment Problem (QAP) and conduct a large\nstatistical study over 600 generated instances of different types. Our results\nreveal interesting links between the network measures, the autocorrelation\nmeasures and the performance of heuristic search algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 13:28:11 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Chicano", "Francisco", "", "ISI"], ["Daolio", "Fabio", "", "ISI"], ["Ochoa", "Gabriela", "", "INRIA Lille - Nord Europe"], ["Verel", "S\u00e9bastien", "", "INRIA Lille - Nord Europe"], ["Tomassini", "Marco", "", "ISI"], ["Alba", "Enrique", ""]]}, {"id": "1210.4145", "submitter": "Sacha Sokoloski", "authors": "Sacha Sokoloski", "title": "A Biologically Realistic Model of Saccadic Eye Control with\n  Probabilistic Population Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The posterior parietal cortex is believed to direct eye movements, especially\nin regards to target tracking tasks, and a number of debates exist over the\nprecise nature of the computations performed by the parietal cortex, with each\nside supported by different sets of biological evidence. In this paper I will\npresent my model which navigates a course between some of these debates,\ntowards the end of presenting a model which can explain some of the competing\ninterpretations among the data sets. In particular, rather than assuming that\nproprioception or efference copies form the key source of information for\ncomputing eye position information, I use a biological plausible implementation\nof a Kalman filter to optimally combine the two signals, and a simple gain\ncontrol mechanism in order to accommodate the latency of the proprioceptive\nsignal. Fitting within the Bayesian brain hypothesis, the result is a Bayes\noptimal solution to the eye control problem, with a range of data supporting\nclaims of biological plausibility.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 19:33:27 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Sokoloski", "Sacha", ""]]}, {"id": "1210.4502", "submitter": "Cm Pintea", "authors": "Camelia-M. Pintea, Cristian Pascan, Mara Hajdu-Macelaru", "title": "Comparing several heuristics for a packing problem", "comments": "5 figures, 2 tables; accepted: International Journal of Advanced\n  Intelligence Paradigms", "journal-ref": "Int J Advanced Intelligence Paradigms 4(3/4):268-277 (2012)", "doi": "10.1504/IJAIP.2012.052071", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing problems are in general NP-hard, even for simple cases. Since now\nthere are no highly efficient algorithms available for solving packing\nproblems. The two-dimensional bin packing problem is about packing all given\nrectangular items, into a minimum size rectangular bin, without overlapping.\nThe restriction is that the items cannot be rotated. The current paper is\ncomparing a greedy algorithm with a hybrid genetic algorithm in order to see\nwhich technique is better for the given problem. The algorithms are tested on\ndifferent sizes data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 11:08:40 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Pintea", "Camelia-M.", ""], ["Pascan", "Cristian", ""], ["Hajdu-Macelaru", "Mara", ""]]}, {"id": "1210.5474", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins and Aaron Courville and Yoshua Bengio", "title": "Disentangling Factors of Variation via Generative Entangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we propose a novel model family with the objective of learning to\ndisentangle the factors of variation in data. Our approach is based on the\nspike-and-slab restricted Boltzmann machine which we generalize to include\nhigher-order interactions among multiple latent variables. Seen from a\ngenerative perspective, the multiplicative interactions emulates the entangling\nof factors of variation. Inference in the model can be seen as disentangling\nthese generative factors. Unlike previous attempts at disentangling latent\nfactors, the proposed model is trained using no supervised information\nregarding the latent factors. We apply our model to the task of facial\nexpression classification.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 17:16:48 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1210.5500", "submitter": "Yasser Gonz\\'alez-Fern\\'andez", "authors": "Marta Soto, Yasser Gonz\\'alez-Fern\\'andez, Alberto Ochoa", "title": "Modeling with Copulas and Vines in Estimation of Distribution Algorithms", "comments": null, "journal-ref": "Investigaci\\'on Operacional, 36(1), 1-23", "doi": null, "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is studying the use of copulas and vines in the\noptimization with Estimation of Distribution Algorithms (EDAs). Two EDAs are\nbuilt around the multivariate product and normal copulas, and other two are\nbased on pair-copula decomposition of vine models. Empirically we study the\neffect of both marginal distributions and dependence structure separately, and\nshow that both aspects play a crucial role in the success of the optimization.\nThe results show that the use of copulas and vines opens new opportunities to a\nmore appropriate modeling of search distributions in EDAs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 19:03:11 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Soto", "Marta", ""], ["Gonz\u00e1lez-Fern\u00e1ndez", "Yasser", ""], ["Ochoa", "Alberto", ""]]}, {"id": "1210.6082", "submitter": "Richard Churchill", "authors": "Richard L. Churchill", "title": "Interplay: Dispersed Activation in Neural Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-point stimulation of a Hebbian neural network\nwith investigation of the interplay between the stimulus waves through the\nneurons of the network. Equilibrium of the resulting memory is achieved for\nrecall of specific memory data at a rate faster than single point stimulus. The\ninterplay of the intersecting stimuli appears to parallel the clarification\nprocess of recall in biological systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 22:59:58 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Churchill", "Richard L.", ""]]}, {"id": "1210.6119", "submitter": "Francis Cabarle", "authors": "Francis George C. Cabarle, Kelvin C. Bu\\~no, Henry N. Adorna", "title": "Time After Time: Notes on Delays In Spiking Neural P Systems", "comments": "11 pages, 9 figures, 4 lemmas, 1 theorem, preprint of Workshop on\n  Computation: Theory and Practice 2012 at DLSU, Manila together with UP\n  Diliman, DLSU, Tokyo Institute of Technology, and Osaka university", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural P systems, SNP systems for short, are biologically inspired\ncomputing devices based on how neurons perform computations. SNP systems use\nonly one type of symbol, the spike, in the computations. Information is encoded\nin the time differences of spikes or the multiplicity of spikes produced at\ncertain times. SNP systems with delays (associated with rules) and those\nwithout delays are two of several Turing complete SNP system variants in\nliterature. In this work we investigate how restricted forms of SNP systems\nwith delays can be simulated by SNP systems without delays. We show the\nsimulations for the following spike routing constructs: sequential, iteration,\njoin, and split.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 03:45:57 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Cabarle", "Francis George C.", ""], ["Bu\u00f1o", "Kelvin C.", ""], ["Adorna", "Henry N.", ""]]}, {"id": "1210.6230", "submitter": "Guillermo Ludue\\~na", "authors": "Guillermo A. Ludue\\~na and Claudius Gros", "title": "A Self-Organized Neural Comparator", "comments": null, "journal-ref": "G. A. Ludue\\~na and C. Gros, A self-organized neural comparator,\n  Neural Computation, 25, pp 1006 (2013)", "doi": "10.1162/NECO_a_00424", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms need generally the possibility to compare several streams\nof information. Neural learning architectures hence need a unit, a comparator,\nable to compare several inputs encoding either internal or external\ninformation, like for instance predictions and sensory readings. Without the\npossibility of comparing the values of prediction to actual sensory inputs,\nreward evaluation and supervised learning would not be possible.\n  Comparators are usually not implemented explicitly, necessary comparisons are\ncommonly performed by directly comparing one-to-one the respective activities.\nThis implies that the characteristics of the two input streams (like size and\nencoding) must be provided at the time of designing the system.\n  It is however plausible that biological comparators emerge from\nself-organizing, genetically encoded principles, which allow the system to\nadapt to the changes in the input and in the organism.\n  We propose an unsupervised neural circuitry, where the function of input\ncomparison emerges via self-organization only from the interaction of the\nsystem with the respective inputs, without external influence or supervision.\n  The proposed neural comparator adapts, unsupervised, according to the\ncorrelations present in the input streams. The system consists of a multilayer\nfeed-forward neural network which follows a local output minimization\n(anti-Hebbian) rule for adaptation of the synaptic weights.\n  The local output minimization allows the circuit to autonomously acquire the\ncapability of comparing the neural activities received from different neural\npopulations, which may differ in the size of the population and in the neural\nencoding used. The comparator is able to compare objects never encountered\nbefore in the sensory input streams and to evaluate a measure of their\nsimilarity, even when differently encoded.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 13:19:08 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2012 13:55:10 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Ludue\u00f1a", "Guillermo A.", ""], ["Gros", "Claudius", ""]]}, {"id": "1210.6465", "submitter": "Carola Winzen", "authors": "Benjamin Doerr and Carola Winzen", "title": "Black-Box Complexity: Breaking the $O(n \\log n)$ Barrier of LeadingOnes", "comments": "12 pages, to appear in the Proc. of Artificial Evolution 2011, LNCS\n  7401, Springer, 2012. For the unrestricted black-box complexity of\n  LeadingOnes there is now a tight $\\Theta(n \\log\\log n)$ bound, cf.\n  http://eccc.hpi-web.de/report/2012/087/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the unrestricted black-box complexity of the $n$-dimensional\nXOR- and permutation-invariant LeadingOnes function class is $O(n \\log (n) /\n\\log \\log n)$. This shows that the recent natural looking $O(n\\log n)$ bound is\nnot tight.\n  The black-box optimization algorithm leading to this bound can be implemented\nin a way that only 3-ary unbiased variation operators are used. Hence our bound\nis also valid for the unbiased black-box complexity recently introduced by\nLehre and Witt (GECCO 2010). The bound also remains valid if we impose the\nadditional restriction that the black-box algorithm does not have access to the\nobjective values but only to their relative order (ranking-based black-box\ncomplexity).\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 09:17:49 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Doerr", "Benjamin", ""], ["Winzen", "Carola", ""]]}, {"id": "1210.6511", "submitter": "Fabrice Rossi", "authors": "Marie Cottrell (SAMM), Madalina Olteanu (SAMM), Fabrice Rossi (SAMM),\n  Joseph Rynkiewicz (SAMM), Nathalie Villa-Vialaneix (SAMM)", "title": "Neural Networks for Complex Data", "comments": null, "journal-ref": "K\\\"unstliche Intelligenz 26, 4 (2012) 373-380", "doi": "10.1007/s13218-012-0207-2", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are simple and efficient machine learning tools.\nDefined originally in the traditional setting of simple vector data, neural\nnetwork models have evolved to address more and more difficulties of complex\nreal world problems, ranging from time evolving data to sophisticated data\nstructures such as graphs and functions. This paper summarizes advances on\nthose themes from the last decade, with a focus on results obtained by members\nof the SAMM team of Universit\\'e Paris 1\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 12:37:53 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Cottrell", "Marie", "", "SAMM"], ["Olteanu", "Madalina", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Rynkiewicz", "Joseph", "", "SAMM"], ["Villa-Vialaneix", "Nathalie", "", "SAMM"]]}, {"id": "1210.6539", "submitter": "Heiko Hamann", "authors": "Heiko Hamann", "title": "Towards Swarm Calculus: Urn Models of Collective Decisions and Universal\n  Properties of Swarm Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of general applicability are searched for in swarm intelligence with\nthe aim of gaining new insights about natural swarms and to develop design\nmethodologies for artificial swarms. An ideal solution could be a `swarm\ncalculus' that allows to calculate key features of swarms such as expected\nswarm performance and robustness based on only a few parameters. To work\ntowards this ideal, one needs to find methods and models with high degrees of\ngenerality. In this paper, we report two models that might be examples of\nexceptional generality. First, an abstract model is presented that describes\nswarm performance depending on swarm density based on the dichotomy between\ncooperation and interference. Typical swarm experiments are given as examples\nto show how the model fits to several different results. Second, we give an\nabstract model of collective decision making that is inspired by urn models.\nThe effects of positive feedback probability, that is increasing over time in a\ndecision making system, are understood by the help of a parameter that controls\nthe feedback based on the swarm's current consensus. Several applicable\nmethods, such as the description as Markov process, calculation of splitting\nprobabilities, mean first passage times, and measurements of positive feedback,\nare discussed and applications to artificial and natural swarms are reported.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 14:18:26 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2013 15:02:00 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2013 09:28:11 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Hamann", "Heiko", ""]]}, {"id": "1210.7956", "submitter": "Michel Owayjan", "authors": "Roger Achkar and Michel Owayjan", "title": "Implementation of a Vision System for a Landmine Detecting Robot Using\n  Artificial Neural Network", "comments": "20 pages, 14 figures, 4 tables", "journal-ref": "Achkar R, Owayjan M. Implementation of a Vision System for a\n  Landmine Detecting Robot Using Artificial Neural Network. International\n  Journal of Artificial Intelligence & Applications (IJAIA), Vol 3, No. 5, pp.\n  73-92, September 2012", "doi": "10.5121/ijaia.2012.3507", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmines, specifically anti-tank mines, cluster bombs, and unexploded\nordnance form a serious problem in many countries. Several landmine sweeping\ntechniques are used for minesweeping. This paper presents the design and the\nimplementation of the vision system of an autonomous robot for landmines\nlocalization. The proposed work develops state-of-the-art techniques in digital\nimage processing for pre-processing captured images of the contaminated area.\nAfter enhancement, Artificial Neural Network (ANN) is used in order to\nidentify, recognize and classify the landmines' make and model. The\nBack-Propagation algorithm is used for training the network. The proposed work\nproved to be able to identify and classify different types of landmines under\nvarious conditions (rotated landmine, partially covered landmine) with a\nsuccess rate of up to 90%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 10:41:30 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Achkar", "Roger", ""], ["Owayjan", "Michel", ""]]}, {"id": "1210.8124", "submitter": "Habib Dhahri", "authors": "Habib Dhahri, Mohamed Adel Alimi", "title": "Hierarchical Learning Algorithm for the Beta Basis Function Neural\n  Network", "comments": null, "journal-ref": "Third International Conference on Systems, Signals & Device, March\n  21-24, 2005 , Sousse, Tunisia", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a two-level learning method for the design of the Beta\nBasis Function Neural Network BBFNN. A Genetic Algorithm is employed at the\nupper level to construct BBFNN, while the key learning parameters :the width,\nthe centers and the Beta form are optimised using the gradient algorithm at the\nlower level. In order to demonstrate the effectiveness of this hierarchical\nlearning algorithm HLABBFNN, we need to validate our algorithm for the\napproximation of non-linear function.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 19:11:06 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Dhahri", "Habib", ""], ["Alimi", "Mohamed Adel", ""]]}, {"id": "1210.8442", "submitter": "Louis Shao", "authors": "Louis Yuanlong Shao", "title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On\n  Boltzmann Machines", "comments": "Submitted to International Conference of Learning Representation\n  (ICLR) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One conjecture in both deep learning and classical connectionist viewpoint is\nthat the biological brain implements certain kinds of deep networks as its\nback-end. However, to our knowledge, a detailed correspondence has not yet been\nset up, which is important if we want to bridge between neuroscience and\nmachine learning. Recent researches emphasized the biological plausibility of\nLinear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally\nplausible settings, the whole network is capable of representing any Boltzmann\nmachine and performing a semi-stochastic Bayesian inference algorithm lying\nbetween Gibbs sampling and variational inference.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 19:14:41 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2013 01:23:04 GMT"}, {"version": "v3", "created": "Sun, 27 Jan 2013 05:30:35 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Shao", "Louis Yuanlong", ""]]}]