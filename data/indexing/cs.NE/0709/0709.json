[{"id": "0709.0883", "submitter": "Joshua Herman J", "authors": "Joshua Jay Herman", "title": "Liquid State Machines in Adbiatic Quantum Computers for General\n  Computation", "comments": "Totally wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.NE", "license": null, "abstract": "  Major mistakes do not read\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2007 16:04:42 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2007 19:34:51 GMT"}, {"version": "v3", "created": "Sun, 9 Sep 2007 14:29:14 GMT"}, {"version": "v4", "created": "Fri, 21 Sep 2007 13:09:13 GMT"}, {"version": "v5", "created": "Fri, 8 Jul 2011 01:54:35 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Herman", "Joshua Jay", ""]]}, {"id": "0709.3427", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Amaury\n  Lendasse (CIS), Damien Fran\\c{c}ois (CESAME), Vincent Wertz (CESAME), Michel\n  Verleysen (DICE - MLG)", "title": "Mutual information for the selection of relevant variables in\n  spectrometric nonlinear modelling", "comments": null, "journal-ref": "Chemometrics and Intelligent Laboratory Systems / I Mathematical\n  Background Chemometrics Intell Lab Syst 80, 2 (2006) 215-226", "doi": "10.1016/j.chemolab.2005.06.010", "report-no": null, "categories": "cs.LG cs.NE stat.AP", "license": null, "abstract": "  Data from spectrophotometers form vectors of a large number of exploitable\nvariables. Building quantitative models using these variables most often\nrequires using a smaller set of variables than the initial one. Indeed, a too\nlarge number of input variables to a model results in a too large number of\nparameters, leading to overfitting and poor generalization abilities. In this\npaper, we suggest the use of the mutual information measure to select variables\nfrom the initial set. The mutual information measures the information content\nin input variables with respect to the model output, without making any\nassumption on the model that will be used; it is thus suitable for nonlinear\nmodelling. In addition, it leads to the selection of variables among the\ninitial set, and not to linear or nonlinear combinations of them. Without\ndecreasing the model performances compared to other variable projection\nmethods, it allows therefore a greater interpretability of the results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2007 12:49:47 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Lendasse", "Amaury", "", "CIS"], ["Fran\u00e7ois", "Damien", "", "CESAME"], ["Wertz", "Vincent", "", "CESAME"], ["Verleysen", "Michel", "", "DICE - MLG"]]}, {"id": "0709.3461", "submitter": "Fabrice Rossi", "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (INRIA Rocquencourt / INRIA\n  Sophia Antipolis), A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)", "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps", "comments": null, "journal-ref": "Neural Networks 19, 6-7 (2006) 855-863", "doi": "10.1016/j.neunet.2006.05.002", "report-no": null, "categories": "cs.NE cs.LG", "license": null, "abstract": "  In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2007 15:20:07 GMT"}], "update_date": "2007-09-24", "authors_parsed": [["Conan-Guez", "Brieuc", "", "LITA"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"], ["Golli", "A\u00efcha El", "", "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"]]}, {"id": "0709.3586", "submitter": "Fabrice Rossi", "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis),\n  Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (LITA), Yves Lechevallier (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)", "title": "Une adaptation des cartes auto-organisatrices pour des donn\\'ees\n  d\\'ecrites par un tableau de dissimilarit\\'es", "comments": null, "journal-ref": "Revue de Statistique Appliqu\\'ee LIV, 3 (2006) 33-64", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": null, "abstract": "  Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2007 15:53:54 GMT"}], "update_date": "2007-09-25", "authors_parsed": [["Golli", "A\u00efcha El", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Conan-Guez", "Brieuc", "", "LITA"], ["Lechevallier", "Yves", "", "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"]]}, {"id": "0709.3587", "submitter": "Fabrice Rossi", "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis), Fabrice Rossi\n  (INRIA Rocquencourt / INRIA Sophia Antipolis)", "title": "Self-organizing maps and symbolic data", "comments": null, "journal-ref": "Journal of Symbolic Data Analysis 2, 1 (2004)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": null, "abstract": "  In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2007 15:54:37 GMT"}], "update_date": "2007-09-25", "authors_parsed": [["Golli", "A\u00efcha El", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Conan-Guez", "Brieuc", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"]]}, {"id": "0709.3641", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE),\n  Nicolas Delannay (DICE - MLG), Brieuc Conan-Guez (INRIA Rocquencourt / INRIA\n  Sophia Antipolis, CEREMADE), Michel Verleysen (DICE - MLG)", "title": "Representation of Functional Data in Neural Networks", "comments": "Also available online from:\n  http://www.sciencedirect.com/science/journal/09252312", "journal-ref": "Neurocomputing 64 (2005) 183--210", "doi": "10.1016/j.neucom.2004.11.012", "report-no": null, "categories": "cs.NE", "license": null, "abstract": "  Functional Data Analysis (FDA) is an extension of traditional data analysis\nto functional data, for example spectra, temporal series, spatio-temporal\nimages, gesture recognition data, etc. Functional data are rarely known in\npractice; usually a regular or irregular sampling is known. For this reason,\nsome processing is needed in order to benefit from the smooth character of\nfunctional data in the analysis methods. This paper shows how to extend the\nRadial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models\nto functional data inputs, in particular when the latter are known through\nlists of input-output pairs. Various possibilities for functional processing\nare discussed, including the projection on smooth bases, Functional Principal\nComponent Analysis, functional centering and reduction, and the use of\ndifferential operators. It is shown how to incorporate these functional\nprocessing into the RBFN and MLP models. The functional approach is illustrated\non a benchmark of spectrometric data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2007 14:10:08 GMT"}], "update_date": "2007-09-25", "authors_parsed": [["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"], ["Delannay", "Nicolas", "", "DICE - MLG"], ["Conan-Guez", "Brieuc", "", "INRIA Rocquencourt / INRIA\n  Sophia Antipolis, CEREMADE"], ["Verleysen", "Michel", "", "DICE - MLG"]]}, {"id": "0709.3642", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE),\n  Brieuc Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE)", "title": "Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data\n  Analysis", "comments": "http://www.sciencedirect.com/science/journal/08936080", "journal-ref": "Neural Networks 18, 1 (2005) 45--60", "doi": "10.1016/j.neunet.2004.07.001", "report-no": null, "categories": "cs.NE", "license": null, "abstract": "  In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP)\nto functional inputs. We show that fundamental results for classical MLP can be\nextended to functional MLP. We obtain universal approximation results that show\nthe expressive power of functional MLP is comparable to that of numerical MLP.\nWe obtain consistency results which imply that the estimation of optimal\nparameters for functional MLP is statistically well defined. We finally show on\nsimulated and real world data that the proposed model performs in a very\nsatisfactory way.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2007 14:10:48 GMT"}], "update_date": "2007-09-25", "authors_parsed": [["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"], ["Conan-Guez", "Brieuc", "", "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"]]}, {"id": "0709.3965", "submitter": "Tshilidzi Marwala", "authors": "Greg Hulley and Tshilidzi Marwala", "title": "Evolving Classifiers: Methods for Incremental Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": null, "abstract": "  The ability of a classifier to take on new information and classes by\nevolving the classifier without it having to be fully retrained is known as\nincremental learning. Incremental learning has been successfully applied to\nmany classification problems, where the data is changing and is not all\navailable at once. In this paper there is a comparison between Learn++, which\nis one of the most recent incremental learning algorithms, and the new proposed\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\nshown good incremental learning capabilities on benchmark datasets on which the\nnew ILUGA method has been tested. ILUGA has also shown good incremental\nlearning ability using only a few classifiers and does not suffer from\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\nfor the difficult multi-class OCR dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2007 14:28:32 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2007 10:37:00 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Hulley", "Greg", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0709.4464", "submitter": "Jesus-Emeterio Navarro-Barrientos", "authors": "J.-Emeterio Navarro", "title": "Adaptive Investment Strategies For Periodic Environments", "comments": "Paper submitted to Advances in Complex Systems (November, 2007) 22\n  pages, 9 figures", "journal-ref": "Advances in Complex Systems Vol. 11, No. 5 (2008) 761-787", "doi": "10.1142/S0219525908001933", "report-no": null, "categories": "cs.CE cs.NE", "license": null, "abstract": "  In this paper, we present an adaptive investment strategy for environments\nwith periodic returns on investment. In our approach, we consider an investment\nmodel where the agent decides at every time step the proportion of wealth to\ninvest in a risky asset, keeping the rest of the budget in a risk-free asset.\nEvery investment is evaluated in the market via a stylized return on investment\nfunction (RoI), which is modeled by a stochastic process with unknown\nperiodicities and levels of noise. For comparison reasons, we present two\nreference strategies which represent the case of agents with zero-knowledge and\ncomplete-knowledge of the dynamics of the returns. We consider also an\ninvestment strategy based on technical analysis to forecast the next return by\nfitting a trend line to previous received returns. To account for the\nperformance of the different strategies, we perform some computer experiments\nto calculate the average budget that can be obtained with them over a certain\nnumber of time steps. To assure for fair comparisons, we first tune the\nparameters of each strategy. Afterwards, we compare the performance of these\nstrategies for RoIs with different periodicities and levels of noise.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2007 19:04:00 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2007 16:13:07 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Navarro", "J. -Emeterio", ""]]}]