[{"id": "0809.0490", "submitter": "Alexander Gorban", "authors": "A. N. Gorban, A. Y. Zinovyev", "title": "Principal Graphs and Manifolds", "comments": "36 pages, 6 figures, minor corrections", "journal-ref": "Handbook of Research on Machine Learning Applications and Trends:\n  Algorithms, Methods and Techniques, Ch. 2, Information Science Reference,\n  2009. 28-59", "doi": "10.4018/978-1-60566-766-9", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In many physical, statistical, biological and other investigations it is\ndesirable to approximate a system of points by objects of lower dimension\nand/or complexity. For this purpose, Karl Pearson invented principal component\nanalysis in 1901 and found 'lines and planes of closest fit to system of\npoints'. The famous k-means algorithm solves the approximation problem too, but\nby finite sets instead of lines and planes. This chapter gives a brief\npractical introduction into the methods of construction of general principal\nobjects, i.e. objects embedded in the 'middle' of the multidimensional data\nset. As a basis, the unifying framework of mean squared distance approximation\nof finite datasets is selected. Principal graphs and manifolds are constructed\nas generalisations of principal components and k-means principal points. For\nthis purpose, the family of expectation/maximisation algorithms with nearest\ngeneralisations is presented. Construction of principal graphs with controlled\ncomplexity is based on the graph grammar approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 18:04:53 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 13:23:08 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Gorban", "A. N.", ""], ["Zinovyev", "A. Y.", ""]]}, {"id": "0809.0835", "submitter": "Tobias Friedrich", "authors": "Karl Bringmann and Tobias Friedrich", "title": "Approximating the volume of unions and intersections of high-dimensional\n  geometric objects", "comments": "16 pages, To appear in Computational Geometry - Theory and\n  Applications", "journal-ref": "Computational Geometry: Theory and Applications, Vol. 43, No. 6-7,\n  pages 601-610, 2010", "doi": "10.1016/j.comgeo.2010.03.004", "report-no": null, "categories": "cs.CG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the computation of the volume of the union of high-dimensional\ngeometric objects. While showing that this problem is #P-hard already for very\nsimple bodies (i.e., axis-parallel boxes), we give a fast FPRAS for all objects\nwhere one can: (1) test whether a given point lies inside the object, (2)\nsample a point uniformly, (3) calculate the volume of the object in polynomial\ntime. All three oracles can be weak, that is, just approximate. This implies\nthat Klee's measure problem and the hypervolume indicator can be approximated\nefficiently even though they are #P-hard and hence cannot be solved exactly in\ntime polynomial in the number of dimensions unless P=NP. Our algorithm also\nallows to approximate efficiently the volume of the union of convex bodies\ngiven by weak membership oracles.\n  For the analogous problem of the intersection of high-dimensional geometric\nobjects we prove #P-hardness for boxes and show that there is no multiplicative\npolynomial-time $2^{d^{1-\\epsilon}}$-approximation for certain boxes unless\nNP=BPP, but give a simple additive polynomial-time $\\epsilon$-approximation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2008 16:14:09 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2010 03:26:40 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Bringmann", "Karl", ""], ["Friedrich", "Tobias", ""]]}, {"id": "0809.4296", "submitter": "Ueli Rutishauser", "authors": "Ueli Rutishauser, Rodney J. Douglas", "title": "State dependent computation using coupled recurrent networks", "comments": "32 pages, 10 figures. Neural computation (in press)", "journal-ref": "Neural computation, 21(2):478-509, 2009", "doi": "10.1162/neco.2008.03-08-734", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although conditional branching between possible behavioural states is a\nhallmark of intelligent behavior, very little is known about the neuronal\nmechanisms that support this processing. In a step toward solving this problem\nwe demonstrate by theoretical analysis and simulation how networks of richly\ninter-connected neurons, such as those observed in the superficial layers of\nthe neocortex, can embed reliable robust finite state machines. We show how a\nmulti-stable neuronal network containing a number of states can be created very\nsimply, by coupling two recurrent networks whose synaptic weights have been\nconfigured for soft winner-take-all (sWTA) performance. These two sWTAs have\nsimple, homogenous locally recurrent connectivity except for a small fraction\nof recurrent cross-connections between them, which are used to embed the\nrequired states. This coupling between the maps allows the network to continue\nto express the current state even after the input that elicted that state is\nwithdrawn. In addition, a small number of 'transition neurons' implement the\nnecessary input-driven transitions between the embedded states. We provide\nsimple rules to systematically design and construct neuronal state machines of\nthis kind. The significance of our finding is that it offers a method whereby\nthe cortex could construct networks supporting a broad range of sophisticated\nprocessing by applying only small specializations to the same generic neuronal\ncircuit.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2008 22:52:05 GMT"}], "update_date": "2012-01-16", "authors_parsed": [["Rutishauser", "Ueli", ""], ["Douglas", "Rodney J.", ""]]}, {"id": "0809.4622", "submitter": "Jeremy Fix", "authors": "J\\'er\\'emy Fix (INRIA Lorraine - Loria), Nicolas P. Rougier (INRIA\n  Lorraine - Loria, University of Colorado, Boulder), Fr\\'ed\\'eric Alexandre\n  (INRIA Lorraine - Loria)", "title": "A computational approach to the covert and overt deployment of spatial\n  attention", "comments": null, "journal-ref": "Dans NeuroComp 2008 : 2i\\`eme Conf\\'erence Fran\\c{c}aise de\n  Neurosciences Computationnelles (2008)", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular computational models of visual attention tend to neglect the\ninfluence of saccadic eye movements whereas it has been shown that the primates\nperform on average three of them per seconds and that the neural substrate for\nthe deployment of attention and the execution of an eye movement might\nconsiderably overlap. Here we propose a computational model in which the\ndeployment of attention with or without a subsequent eye movement emerges from\nlocal, distributed and numerical computations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2008 13:12:36 GMT"}], "update_date": "2008-09-29", "authors_parsed": [["Fix", "J\u00e9r\u00e9my", "", "INRIA Lorraine - Loria"], ["Rougier", "Nicolas P.", "", "INRIA\n  Lorraine - Loria, University of Colorado, Boulder"], ["Alexandre", "Fr\u00e9d\u00e9ric", "", "INRIA Lorraine - Loria"]]}, {"id": "0809.5087", "submitter": "Yuhua  Chen", "authors": "Yuhua Chen, Subhash Kak, Lei Wang", "title": "Hybrid Neural Network Architecture for On-Line Learning", "comments": "19 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to machine intelligence based on brain models have stressed the\nuse of neural networks for generalization. Here we propose the use of a hybrid\nneural network architecture that uses two kind of neural networks\nsimultaneously: (i) a surface learning agent that quickly adapt to new modes of\noperation; and, (ii) a deep learning agent that is very accurate within a\nspecific regime of operation. The two networks of the hybrid architecture\nperform complementary functions that improve the overall performance. The\nperformance of the hybrid architecture has been compared with that of\nback-propagation perceptrons and the CC and FC networks for chaotic time-series\nprediction, the CATS benchmark test, and smooth function approximation. It has\nbeen shown that the hybrid architecture provides a superior performance based\non the RMS error criterion.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2008 23:00:22 GMT"}], "update_date": "2008-10-01", "authors_parsed": [["Chen", "Yuhua", ""], ["Kak", "Subhash", ""], ["Wang", "Lei", ""]]}]