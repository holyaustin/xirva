[{"id": "2008.00053", "submitter": "Jacob Adamczyk", "authors": "Jacob Adamczyk", "title": "Neural Network Degeneration and its Relationship to the Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses the application of neural networks (NNs) as small\nsegments of the brain. The networks representing the biological connectome are\naltered both spatially and temporally. The degradation techniques applied here\nare \"weight degradation\", \"weight scrambling\", and variable activation\nfunction. These methods aim to shine light on the study of neurodegenerative\ndiseases such as Alzheimer's, Huntington's and Parkinson's disease as well as\nstrokes and brain tumors disrupting the flow of information in the brain's\nnetwork. Fundamental insights to memory loss and generalized learning\ndysfunction are gained by monitoring the network's error function during\nnetwork degradation. The biological significance of each facet is also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 19:42:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Adamczyk", "Jacob", ""]]}, {"id": "2008.00077", "submitter": "Matheus Nunes", "authors": "Matheus Nunes and Gisele L. Pappa", "title": "Neural Architecture Search in Graph Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-61377-8", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing analytical tasks over graph data has become increasingly\ninteresting due to the ubiquity and large availability of relational\ninformation. However, unlike images or sentences, there is no notion of\nsequence in networks. Nodes (and edges) follow no absolute order, and it is\nhard for traditional machine learning (ML) algorithms to recognize a pattern\nand generalize their predictions on this type of data. Graph Neural Networks\n(GNN) successfully tackled this problem. They became popular after the\ngeneralization of the convolution concept to the graph domain. However, they\npossess a large number of hyperparameters and their design and optimization is\ncurrently hand-made, based on heuristics or empirical intuition. Neural\nArchitecture Search (NAS) methods appear as an interesting solution to this\nproblem. In this direction, this paper compares two NAS methods for optimizing\nGNN: one based on reinforcement learning and a second based on evolutionary\nalgorithms. Results consider 7 datasets over two search spaces and show that\nboth methods obtain similar accuracies to a random search, raising the question\nof how many of the search space dimensions are actually relevant to the\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 21:04:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Nunes", "Matheus", ""], ["Pappa", "Gisele L.", ""]]}, {"id": "2008.00113", "submitter": "Shakila Khan Rumi", "authors": "Shakila Khan Rumi, Kyle K. Qin, Flora D. Salim", "title": "Multi-officer Routing for Patrolling High Risk Areas Jointly Learned\n  from Check-ins, Crime and Incident Response Data", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-crafted police patrol route design is vital in providing community\nsafety and security in the society. Previous works have largely focused on\npredicting crime events with historical crime data. The usage of large-scale\nmobility data collected from Location-Based Social Network, or check-ins, and\nPoint of Interests (POI) data for designing an effective police patrol is\nlargely understudied. Given that there are multiple police officers being on\nduty in a real-life situation, this makes the problem more complex to solve. In\nthis paper, we formulate the dynamic crime patrol planning problem for multiple\npolice officers using check-ins, crime, incident response data, and POI\ninformation. We propose a joint learning and non-random optimisation method for\nthe representation of possible solutions where multiple police officers patrol\nthe high crime risk areas simultaneously first rather than the low crime risk\nareas. Later, meta-heuristic Genetic Algorithm (GA) and Cuckoo Search (CS) are\nimplemented to find the optimal routes. The performance of the proposed\nsolution is verified and compared with several state-of-art methods using\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 23:33:14 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Rumi", "Shakila Khan", ""], ["Qin", "Kyle K.", ""], ["Salim", "Flora D.", ""]]}, {"id": "2008.00138", "submitter": "Alexander Wong", "authors": "Hossein Aboutalebi, Mohammad Javad Shafiee, Michelle Karg, Christian\n  Scharfenberger, and Alexander Wong", "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior studies have unveiled the vulnerability of the deep neural networks in\nthe context of adversarial machine learning, leading to great recent attention\ninto this area. One interesting question that has yet to be fully explored is\nthe bias-variance relationship of adversarial machine learning, which can\npotentially provide deeper insights into this behaviour. The notion of bias and\nvariance is one of the main approaches to analyze and evaluate the\ngeneralization and reliability of a machine learning model. Although it has\nbeen extensively used in other machine learning models, it is not well explored\nin the field of deep learning and it is even less explored in the area of\nadversarial machine learning.\n  In this study, we investigate the effect of adversarial machine learning on\nthe bias and variance of a trained deep neural network and analyze how\nadversarial perturbations can affect the generalization of a network. We derive\nthe bias-variance trade-off for both classification and regression applications\nbased on two main loss functions: (i) mean squared error (MSE), and (ii)\ncross-entropy. Furthermore, we perform quantitative analysis with both\nsimulated and real data to empirically evaluate consistency with the derived\nbias-variance tradeoffs. Our analysis sheds light on why the deep neural\nnetworks have poor performance under adversarial perturbation from a\nbias-variance point of view and how this type of perturbation would change the\nperformance of a network. Moreover, given these new theoretical findings, we\nintroduce a new adversarial machine learning algorithm with lower computational\ncomplexity than well-known adversarial machine learning strategies (e.g., PGD)\nwhile providing a high success rate in fooling deep neural networks in lower\nperturbation magnitudes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 00:58:54 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Aboutalebi", "Hossein", ""], ["Shafiee", "Mohammad Javad", ""], ["Karg", "Michelle", ""], ["Scharfenberger", "Christian", ""], ["Wong", "Alexander", ""]]}, {"id": "2008.00316", "submitter": "Colin Benjamin", "authors": "Abhisek Panda, Colin Benjamin", "title": "Order from chaos in quantum walks on cyclic graphs", "comments": "8 pages, 11 figures, revised with a new section on secure\n  encryption-decryption mechanism via combining chaotic quantum walks to yield\n  an ordered quantum walk. Accepted for publication in Physical Review A", "journal-ref": "Phys. Rev. A 104, 012204 (2021)", "doi": "10.1103/PhysRevA.104.012204", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.NE math.QA nlin.CD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It has been shown classically that combining two chaotic random walks can\nyield an ordered(periodic) walk. Our aim in this paper is to find a quantum\nanalog for this rather counter-intuitive result. We study chaotic and periodic\nnature of cyclic quantum walks and focus on a unique situation wherein a\nperiodic quantum walk on a 3-cycle graph is generated via a deterministic\ncombination of two chaotic quantum walks on the same graph. We extend our\nresults to even-numbered cyclic graphs, specifically a 4-cycle graph too. Our\nresults will be relevant in quantum cryptography and quantum chaos control.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 18:39:40 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 06:48:45 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 09:03:55 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Panda", "Abhisek", ""], ["Benjamin", "Colin", ""]]}, {"id": "2008.00317", "submitter": "Shashwat Shukla", "authors": "Shashwat Shukla, Rohan Pathak, Vivek Saraswat and Udayan Ganguly", "title": "Adaptive Chemotaxis for improved Contour Tracking using Spiking Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a Spiking Neural Network (SNN) for autonomous\nnavigation, inspired by the chemotaxis network of the worm Caenorhabditis\nelegans. In particular, we focus on the problem of contour tracking, wherein\nthe bot must reach and subsequently follow a desired concentration setpoint.\nPast schemes that used only klinokinesis can follow the contour efficiently but\ntake excessive time to reach the setpoint. We address this shortcoming by\nproposing a novel adaptive klinotaxis mechanism that builds upon a previously\nproposed gradient climbing circuit. We demonstrate how our klinotaxis circuit\ncan autonomously be configured to perform gradient ascent, gradient descent and\nsubsequently be disabled to seamlessly integrate with the aforementioned\nklinokinesis circuit. We also incorporate speed regulation (orthokinesis) to\nfurther improve contour tracking performance. Thus for the first time, we\npresent a model that successfully integrates klinokinesis, klinotaxis and\northokinesis. We demonstrate via contour tracking simulations that our proposed\nscheme achieves an 2.4x reduction in the time to reach the setpoint, along with\na simultaneous 8.7x reduction in average deviation from the setpoint.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 18:49:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Shukla", "Shashwat", ""], ["Pathak", "Rohan", ""], ["Saraswat", "Vivek", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2008.00386", "submitter": "Franck Dernoncourt", "authors": "Lidan Wang, Franck Dernoncourt, Trung Bui", "title": "Bayesian Optimization for Selecting Efficient Machine Learning Models", "comments": "Published at CIKM MoST-Rec 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The performance of many machine learning models depends on their\nhyper-parameter settings. Bayesian Optimization has become a successful tool\nfor hyper-parameter optimization of machine learning algorithms, which aims to\nidentify optimal hyper-parameters during an iterative sequential process.\nHowever, most of the Bayesian Optimization algorithms are designed to select\nmodels for effectiveness only and ignore the important issue of model training\nefficiency. Given that both model effectiveness and training time are important\nfor real-world applications, models selected for effectiveness may not meet the\nstrict training time requirements necessary to deploy in a production\nenvironment. In this work, we present a unified Bayesian Optimization framework\nfor jointly optimizing models for both prediction effectiveness and training\nefficiency. We propose an objective that captures the tradeoff between these\ntwo metrics and demonstrate how we can jointly optimize them in a principled\nBayesian Optimization framework. Experiments on model selection for\nrecommendation tasks indicate models selected this way significantly improves\nmodel training efficiency while maintaining strong effectiveness as compared to\nstate-of-the-art Bayesian Optimization algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 02:56:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Lidan", ""], ["Dernoncourt", "Franck", ""], ["Bui", "Trung", ""]]}, {"id": "2008.00395", "submitter": "Yujun Zheng", "authors": "Yu-Jun Zheng, Xin Chen, Tie-Er Gan, Min-Xia Zhang, Wei-Guo Sheng and\n  Ling Wang", "title": "Balancing Common Treatment and Epidemic Control in Medical Procurement\n  during COVID-19: Transform-and-Divide Evolutionary Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing common disease treatment and epidemic control is a key objective of\nmedical supplies procurement in hospitals during a pandemic such as COVID-19.\nThis problem can be formulated as a bi-objective optimization problem for\nsimultaneously optimizing the effects of common disease treatment and epidemic\ncontrol. However, due to the large number of supplies, difficulties in\nevaluating the effects, and the strict budget constraint, it is difficult for\nexisting evolutionary multiobjective algorithms to efficiently approximate the\nPareto front of the problem. In this paper, we present an approach that first\ntransforms the original high-dimensional, constrained multiobjective\noptimization problem to a low-dimensional, unconstrained multiobjective\noptimization problem, and then evaluates each solution to the transformed\nproblem by solving a set of simple single-objective optimization subproblems,\nsuch that the problem can be efficiently solved by existing evolutionary\nmultiobjective algorithms. We applied the transform-and-divide evolutionary\noptimization approach to six hospitals in Zhejiang Province, China, during the\npeak of COVID-19. Results showed that the proposed approach exhibits\nsignificantly better performance than that of directly solving the original\nproblem. Our study has also shown that transform-and-divide evolutionary\noptimization based on problem-specific knowledge can be an efficient solution\napproach to many other complex problems and, therefore, enlarge the application\nfield of evolutionary algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:47:34 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zheng", "Yu-Jun", ""], ["Chen", "Xin", ""], ["Gan", "Tie-Er", ""], ["Zhang", "Min-Xia", ""], ["Sheng", "Wei-Guo", ""], ["Wang", "Ling", ""]]}, {"id": "2008.00410", "submitter": "Yashesh Dhebar", "authors": "Yashesh Dhebar and Kalyanmoy Deb", "title": "Interpretable Rule Discovery Through Bilevel Optimization of Split-Rules\n  of Nonlinear Decision Trees for Classification Problems", "comments": "Total 26 pages and 30 figures. Main Paper: 12 pages, 12 figures.\n  Supplementary Document: 14 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For supervised classification problems involving design, control, other\npractical purposes, users are not only interested in finding a highly accurate\nclassifier, but they also demand that the obtained classifier be easily\ninterpretable. While the definition of interpretability of a classifier can\nvary from case to case, here, by a humanly interpretable classifier we restrict\nit to be expressed in simplistic mathematical terms. As a novel approach, we\nrepresent a classifier as an assembly of simple mathematical rules using a\nnon-linear decision tree (NLDT). Each conditional (non-terminal) node of the\ntree represents a non-linear mathematical rule (split-rule) involving features\nin order to partition the dataset in the given conditional node into two\nnon-overlapping subsets. This partitioning is intended to minimize the impurity\nof the resulting child nodes. By restricting the structure of split-rule at\neach conditional node and depth of the decision tree, the interpretability of\nthe classifier is assured. The non-linear split-rule at a given conditional\nnode is obtained using an evolutionary bilevel optimization algorithm, in which\nwhile the upper-level focuses on arriving at an interpretable structure of the\nsplit-rule, the lower-level achieves the most appropriate weights\n(coefficients) of individual constituents of the rule to minimize the net\nimpurity of two resulting child nodes. The performance of the proposed\nalgorithm is demonstrated on a number of controlled test problems, existing\nbenchmark problems, and industrial problems. Results on two to 500-feature\nproblems are encouraging and open up further scopes of applying the proposed\napproach to more challenging and complex classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 06:35:32 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dhebar", "Yashesh", ""], ["Deb", "Kalyanmoy", ""]]}, {"id": "2008.00539", "submitter": "Aaron Hein", "authors": "Aaron Hein, Casey Cole, Homayoun Valafar", "title": "An Investigation in Optimal Encoding of Protein Primary Sequence for\n  Structure Prediction by Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and the use of neural networks has increased precipitously\nover the past few years primarily due to the ever-increasing accessibility to\ndata and the growth of computation power. It has become increasingly easy to\nharness the power of machine learning for predictive tasks. Protein structure\nprediction is one area where neural networks are becoming increasingly popular\nand successful. Although very powerful, the use of ANN require selection of\nmost appropriate input/output encoding, architecture, and class to produce the\noptimal results. In this investigation we have explored and evaluated the\neffect of several conventional and newly proposed input encodings and selected\nan optimal architecture. We considered 11 variations of input encoding, 11\nalternative window sizes, and 7 different architectures. In total, we evaluated\n2,541 permutations in application to the training and testing of more than\n10,000 protein structures over the course of 3 months. Our investigations\nconcluded that one-hot encoding, the use of LSTMs, and window sizes of 9, 11,\nand 15 produce the optimal outcome. Through this optimization, we were able to\nimprove the quality of protein structure prediction by predicting the {\\phi}\ndihedrals to within 14{\\deg} - 16{\\deg} and {\\psi} dihedrals to within\n23{\\deg}- 25{\\deg}. This is a notable improvement compared to previously\nsimilar investigations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 18:57:01 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Hein", "Aaron", ""], ["Cole", "Casey", ""], ["Valafar", "Homayoun", ""]]}, {"id": "2008.00727", "submitter": "Alykhan Tejani", "authors": "Dalin Guo, Sofia Ira Ktena, Ferenc Huszar, Pranay Kumar Myana, Wenzhe\n  Shi, Alykhan Tejani", "title": "Deep Bayesian Bandits: Exploring in Online Personalized Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems trained in a continuous learning fashion are plagued by\nthe feedback loop problem, also known as algorithmic bias. This causes a newly\ntrained model to act greedily and favor items that have already been engaged by\nusers. This behavior is particularly harmful in personalised ads\nrecommendations, as it can also cause new campaigns to remain unexplored.\nExploration aims to address this limitation by providing new information about\nthe environment, which encompasses user preference, and can lead to higher\nlong-term reward. In this work, we formulate a display advertising recommender\nas a contextual bandit and implement exploration techniques that require\nsampling from the posterior distribution of click-through-rates in a\ncomputationally tractable manner. Traditional large-scale deep learning models\ndo not provide uncertainty estimates by default. We approximate these\nuncertainty measurements of the predictions by employing a bootstrapped model\nwith multiple heads and dropout units. We benchmark a number of different\nmodels in an offline simulation environment using a publicly available dataset\nof user-ads engagements. We test our proposed deep Bayesian bandits algorithm\nin the offline simulation and online AB setting with large-scale production\ntraffic, where we demonstrate a positive gain of our exploration model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:58:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Guo", "Dalin", ""], ["Ktena", "Sofia Ira", ""], ["Huszar", "Ferenc", ""], ["Myana", "Pranay Kumar", ""], ["Shi", "Wenzhe", ""], ["Tejani", "Alykhan", ""]]}, {"id": "2008.00741", "submitter": "Ivan Anokhin", "authors": "Ivan Anokhin, Dmitry Yarotsky", "title": "Low-loss connection of weight vectors: distribution-based approaches", "comments": "accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that sublevel sets of the loss surfaces of\noverparameterized networks are connected, exactly or approximately. We describe\nand compare experimentally a panel of methods used to connect two low-loss\npoints by a low-loss curve on this surface. Our methods vary in accuracy and\ncomplexity. Most of our methods are based on \"macroscopic\" distributional\nassumptions, and some are insensitive to the detailed properties of the points\nto be connected. Some methods require a prior training of a \"global connection\nmodel\" which can then be applied to any pair of points. The accuracy of the\nmethod generally correlates with its complexity and sensitivity to the endpoint\ndetail.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 09:42:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Anokhin", "Ivan", ""], ["Yarotsky", "Dmitry", ""]]}, {"id": "2008.00829", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz and Ghulam Mohiuddin Bhat", "title": "Deep Network Ensemble Learning applied to Image Classification using CNN\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional machine learning approaches may fail to perform satisfactorily\nwhen dealing with complex data. In this context, the importance of data mining\nevolves w.r.t. building an efficient knowledge discovery and mining framework.\nEnsemble learning is aimed at integration of fusion, modeling and mining of\ndata into a unified model. However, traditional ensemble learning methods are\ncomplex and have optimization or tuning problems. In this paper, we propose a\nsimple, sequential, efficient, ensemble learning approach using multiple deep\nnetworks. The deep network used in the ensembles is ResNet50. The model draws\ninspiration from binary decision/classification trees. The proposed approach is\ncompared against the baseline viz. the single classifier approach i.e. using a\nsingle multiclass ResNet50 on the ImageNet and Natural Images datasets. Our\napproach outperforms the baseline on all experiments on the ImageNet dataset.\nCode is available in https://github.com/mueedhafiz1982/CNNTreeEnsemble.git\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:58:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Bhat", "Ghulam Mohiuddin", ""]]}, {"id": "2008.00979", "submitter": "Sini\\v{s}a Dru\\v{z}eta", "authors": "Sini\\v{s}a Dru\\v{z}eta, Stefan Ivi\\'c", "title": "Anakatabatic Inertia: Particle-wise Adaptive Inertia for PSO", "comments": "6 pages, 5 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1906.02474", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the course of the development of Particle Swarm Optimization,\nparticle inertia has been established as an important aspect of the method for\nresearching possible method improvements. As a continuation of our previous\nresearch, we propose a novel generalized technique of inertia weight adaptation\nbased on individual particle's fitness improvement, called anakatabatic\ninertia. This technique allows for adapting inertia weight value for each\nparticle corresponding to the particle's increasing or decreasing fitness, i.e.\nconditioned by particle's ascending (anabatic) or descending (katabatic)\nmovement. The proposed inertia weight control framework was metaoptimized and\ntested on the 30 test functions of the CEC 2014 test suite. The conducted\nprocedure produced four anakatabatic models, two for each of the PSO methods\nused (Standard PSO and TVAC-PSO). The benchmark testing results show that using\nthe proposed anakatabatic inertia models reliably yield moderate improvements\nin accuracy of Standard PSO (final fitness minimum reduced up to 0.09 orders of\nmagnitude) and rather strong improvements for TVAC-PSO (final fitness minimum\nreduced up to 0.59 orders of magnitude), mostly without any adverse effects on\nthe method's performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 11:09:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dru\u017eeta", "Sini\u0161a", ""], ["Ivi\u0107", "Stefan", ""]]}, {"id": "2008.01004", "submitter": "Homayoun Valafar", "authors": "Faramarz Valafar, Homayoun Valafar, William S. York", "title": "Identification of 1H-NMR Spectra of Xyloglucan Oligosaccharides: A\n  Comparative Study of Artificial Neural Networks and Bayesian Classification\n  Using Nonparametric Density Estimation", "comments": "6 pages. Published in IEEE ICAI99", "journal-ref": "Published in IEEE ICAI 1999 549-553", "doi": null, "report-no": null, "categories": "q-bio.BM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proton nuclear magnetic resonance (1H-NMR) is a widely used tool for chemical\nstructural analysis. However, 1H-NMR spectra suffer from natural aberrations\nthat render computer-assisted automated identification of these spectra\ndifficult, and at times impossible. Previous efforts have successfully\nimplemented instrument dependent or conditional identification of these\nspectra. In this paper, we report the first instrument independent\ncomputer-assisted automated identification system for a group of complex\ncarbohydrates known as the xyloglucan oligosaccharides. The developed system is\nalso implemented on the world wide web (http://www.ccrc.uga.edu) as part of an\nidentification package called the CCRC-Net and is intended to recognize any\nsubmitted 1H-NMR spectrum of these structures with reasonable signal-to-noise\nratio, recorded on any 500 MHz NMR instrument. The system uses Artificial\nNeural Networks (ANNs) technology and is insensitive to the instrument and\nenvironment-dependent variations in 1H-NMR spectroscopy. In this paper,\ncomparative results of the ANN engine versus a multidimensional Bayes'\nclassifier is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:29:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Valafar", "Faramarz", ""], ["Valafar", "Homayoun", ""], ["York", "William S.", ""]]}, {"id": "2008.01007", "submitter": "Nina Kudryashova", "authors": "Nina Kudryashova, Theoklitos Amvrosiadis, Nathalie Dupuy, Nathalie\n  Rochefort, Arno Onken", "title": "Parametric Copula-GP model for analyzing multidimensional neuronal and\n  behavioral relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in current systems neuroscience is the analysis of\nhigh-dimensional neuronal and behavioral data that are characterized by\ndifferent statistics and timescales of the recorded variables. We propose a\nparametric copula model which separates the statistics of the individual\nvariables from their dependence structure, and escapes the curse of\ndimensionality by using vine copula constructions. We use a Bayesian framework\nwith Gaussian Process (GP) priors over copula parameters, conditioned on a\ncontinuous task-related variable. We validate the model on synthetic data and\ncompare its performance in estimating mutual information against the commonly\nused non-parametric algorithms.\n  Our model provides accurate information estimates when the dependencies in\nthe data match the parametric copulas used in our framework. When the exact\ndensity estimation with a parametric model is not possible, our Copula-GP model\nis still able to provide reasonable information estimates, close to the ground\ntruth and comparable to those obtained with a neural network estimator.\nFinally, we apply our framework to real neuronal and behavioral recordings\nobtained in awake mice. We demonstrate the ability of our framework to\n  1) produce accurate and interpretable bivariate models for the analysis of\ninter-neuronal noise correlations or behavioral modulations;\n  2) expand to more than 100 dimensions and measure information content in the\nwhole-population statistics. These results demonstrate that the Copula-GP\nframework is particularly useful for the analysis of complex multidimensional\nrelationships between neuronal, sensory and behavioral data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:44:29 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kudryashova", "Nina", ""], ["Amvrosiadis", "Theoklitos", ""], ["Dupuy", "Nathalie", ""], ["Rochefort", "Nathalie", ""], ["Onken", "Arno", ""]]}, {"id": "2008.01039", "submitter": "Stefanie Czischek", "authors": "Stefanie Czischek, Andreas Baumbach, Sebastian Billaudelle, Benjamin\n  Cramer, Lukas Kades, Jan M. Pawlowski, Markus K. Oberthaler, Johannes\n  Schemmel, Mihai A. Petrovici, Thomas Gasenzer, and Martin G\\\"arttner", "title": "Spiking neuromorphic chip learns entangled quantum states", "comments": "21 pages, 6 figures Submission to SciPost", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.dis-nn cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of quantum states with artificial neural networks has\ngained a lot of attention during the last years. Meanwhile, analog neuromorphic\nchips, inspired by structural and dynamical properties of the biological brain,\nshow a high energy efficiency in running artificial neural-network\narchitectures for the profit of generative applications. This encourages\nemploying such hardware systems as platforms for simulations of quantum\nsystems. Here we report on the realization of a prototype using the latest\nspike-based BrainScaleS hardware allowing us to represent few-qubit maximally\nentangled quantum states with high fidelities. Extracted Bell correlations for\npure and mixed two-qubit states convey that non-classical features are captured\nby the analog hardware, demonstrating an important building block for\nsimulating quantum systems with spiking neuromorphic chips.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:24:33 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 07:20:09 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 20:04:41 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Czischek", "Stefanie", ""], ["Baumbach", "Andreas", ""], ["Billaudelle", "Sebastian", ""], ["Cramer", "Benjamin", ""], ["Kades", "Lukas", ""], ["Pawlowski", "Jan M.", ""], ["Oberthaler", "Markus K.", ""], ["Schemmel", "Johannes", ""], ["Petrovici", "Mihai A.", ""], ["Gasenzer", "Thomas", ""], ["G\u00e4rttner", "Martin", ""]]}, {"id": "2008.01124", "submitter": "Jamal Toutouh", "authors": "Jamal Toutouh, Erik Hemberg, and Una-May O'Reilly", "title": "Analyzing the Components of Distributed Coevolutionary GAN Training", "comments": "Accepted as a full paper in Sixteenth International Conference on\n  Parallel Problem Solving from Nature (PPSN XVI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed coevolutionary Generative Adversarial Network (GAN) training has\nempirically shown success in overcoming GAN training pathologies. This is\nmainly due to diversity maintenance in the populations of generators and\ndiscriminators during the training process. The method studied here coevolves\nsub-populations on each cell of a spatial grid organized into overlapping Moore\nneighborhoods. We investigate the impact on the performance of two algorithm\ncomponents that influence the diversity during coevolution: the\nperformance-based selection/replacement inside each sub-population and the\ncommunication through migration of solutions (networks) among overlapping\nneighborhoods. In experiments on MNIST dataset, we find that the combination of\nthese two components provides the best generative models. In addition,\nmigrating solutions without applying selection in the sub-populations achieves\ncompetitive results, while selection without communication between cells\nreduces performance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 18:35:06 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Toutouh", "Jamal", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2008.01151", "submitter": "Kenneth Stewart", "authors": "Kenneth Stewart, Garrick Orchard, Sumit Bam Shrestha, Emre Neftci", "title": "Online Few-shot Gesture Learning on a Neuromorphic Processor", "comments": "10 pages, accepted by IEEE JETCAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Surrogate-gradient Online Error-triggered Learning (SOEL)\nsystem for online few-shot learning on neuromorphic processors. The SOEL\nlearning system uses a combination of transfer learning and principles of\ncomputational neuroscience and deep learning. We show that partially trained\ndeep Spiking Neural Networks (SNNs) implemented on neuromorphic hardware can\nrapidly adapt online to new classes of data within a domain. SOEL updates\ntrigger when an error occurs, enabling faster learning with fewer updates.\nUsing gesture recognition as a case study, we show SOEL can be used for online\nfew-shot learning of new classes of pre-recorded gesture data and rapid online\nlearning of new gestures from data streamed live from a Dynamic Active-pixel\nVision Sensor to an Intel Loihi neuromorphic research processor.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:39:54 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 05:13:05 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Stewart", "Kenneth", ""], ["Orchard", "Garrick", ""], ["Shrestha", "Sumit Bam", ""], ["Neftci", "Emre", ""]]}, {"id": "2008.01352", "submitter": "Jean-Yves Franceschi", "authors": "J\\'er\\'emie Don\\`a (MLIA), Jean-Yves Franceschi (MLIA), Sylvain\n  Lamprier (MLIA), Patrick Gallinari (MLIA)", "title": "PDE-Driven Spatiotemporal Disentanglement", "comments": null, "journal-ref": "The Ninth International Conference on Learning Representations,\n  International Conference on Representation Learning, May 2021, Vienne,\n  Austria", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work in the machine learning community addresses the problem\nof predicting high-dimensional spatiotemporal phenomena by leveraging specific\ntools from the differential equations theory. Following this direction, we\npropose in this article a novel and general paradigm for this task based on a\nresolution method for partial differential equations: the separation of\nvariables. This inspiration allows us to introduce a dynamical interpretation\nof spatiotemporal disentanglement. It induces a principled model based on\nlearning disentangled spatial and temporal representations of a phenomenon to\naccurately predict future observations. We experimentally demonstrate the\nperformance and broad applicability of our method against prior\nstate-of-the-art models on physical and synthetic video datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 06:10:30 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 09:18:31 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 09:44:40 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Don\u00e0", "J\u00e9r\u00e9mie", "", "MLIA"], ["Franceschi", "Jean-Yves", "", "MLIA"], ["Lamprier", "Sylvain", "", "MLIA"], ["Gallinari", "Patrick", "", "MLIA"]]}, {"id": "2008.01380", "submitter": "Ata Mahjoubfar", "authors": "Te-Yuan Liu, Ata Mahjoubfar, Daniel Prusinski, Luis Stevens", "title": "Neuromorphic Computing for Content-based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in context-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 3.2 times more energy-efficient compared with an Intel Core i7 CPU and\n12.5 times more energy-efficient compared with Nvidia T4 GPU for inference by a\nlightweight convolutional neural network without batching, while maintaining\nthe same level of matching accuracy. The study validates the longterm potential\nof neuromorphic computing in machine learning, as a complementary paradigm to\nthe existing Von Neumann architectures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:34:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Liu", "Te-Yuan", ""], ["Mahjoubfar", "Ata", ""], ["Prusinski", "Daniel", ""], ["Stevens", "Luis", ""]]}, {"id": "2008.01438", "submitter": "Andrey Ignatov", "authors": "Dmitry Ignatov and Andrey Ignatov", "title": "Controlling Information Capacity of Binary Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.07.033", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing popularity of deep learning technologies, high memory\nrequirements and power consumption are essentially limiting their application\nin mobile and IoT areas. While binary convolutional networks can alleviate\nthese problems, the limited bitwidth of weights is often leading to significant\ndegradation of prediction accuracy. In this paper, we present a method for\ntraining binary networks that maintains a stable predefined level of their\ninformation capacity throughout the training process by applying Shannon\nentropy based penalty to convolutional filters. The results of experiments\nconducted on SVHN, CIFAR and ImageNet datasets demonstrate that the proposed\napproach can statistically significantly improve the accuracy of binary\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:08:28 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ignatov", "Dmitry", ""], ["Ignatov", "Andrey", ""]]}, {"id": "2008.01446", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Constructing transient amplifiers for death-Birth updating: A case study\n  of cubic and quartic regular graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question of evolutionary dynamics on graphs is whether or not a\nmutation introduced in a population of residents survives and eventually even\nspreads to the whole population, or gets extinct. The outcome naturally depends\non the fitness of the mutant and the rules by which mutants and residents may\npropagate on the network, but arguably the most determining factor is the\nnetwork structure. Some structured networks are transient amplifiers. They\nincrease for a certain fitness range the fixation probability of beneficial\nmutations as compared to a well-mixed population. We study a perturbation\nmethods for identifying transient amplifiers for death-Birth updating. The\nmethod includes calculating the coalescence times of random walks on graphs and\nfinding the vertex with the largest remeeting time. If the graph is perturbed\nby removing an edge from this vertex, there is a certain likelihood that the\nresulting perturbed graph is a transient amplifier. We test all pairwise\nnonisomorphic cubic and quartic regular graphs up to a certain size and thus\ncover the whole structural range expressible by these graphs. We carry out a\nspectral analysis and show that the graphs from which the transient amplifiers\ncan be constructed share certain structural properties. The graphs are\npath-like, have low conductance and are rather easy to divide into subgraphs by\nremoving edges and/or vertices. This is connected with the subgraphs being\nidentical (or almost identical) building blocks and the frequent occurrence of\ncut and/or hinge vertices. Identifying spectral and structural properties may\npromote finding and designing such networks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:37:09 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "2008.01531", "submitter": "Maren Awiszus", "authors": "Maren Awiszus, Frederik Schubert, Bodo Rosenhahn", "title": "TOAD-GAN: Coherent Style Level Generation from a Single Example", "comments": "7 pages, 7 figures. AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present TOAD-GAN (Token-based One-shot Arbitrary Dimension\nGenerative Adversarial Network), a novel Procedural Content Generation (PCG)\nalgorithm that generates token-based video game levels. TOAD-GAN follows the\nSinGAN architecture and can be trained using only one example. We demonstrate\nits application for Super Mario Bros. levels and are able to generate new\nlevels of similar style in arbitrary sizes. We achieve state-of-the-art results\nin modeling the patterns of the training level and provide a comparison with\ndifferent baselines under several metrics. Additionally, we present an\nextension of the method that allows the user to control the generation process\nof certain token structures to ensure a coherent global level layout. We\nprovide this tool to the community to spur further research by publishing our\nsource code.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:44:50 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Awiszus", "Maren", ""], ["Schubert", "Frederik", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2008.01874", "submitter": "Mohit Prabhushankar", "authors": "Yutong Sun, Mohit Prabhushankar and Ghassan AlRegib", "title": "Implicit Saliency in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that existing recognition and localization deep\narchitectures, that have not been exposed to eye tracking data or any saliency\ndatasets, are capable of predicting the human visual saliency. We term this as\nimplicit saliency in deep neural networks. We calculate this implicit saliency\nusing expectancy-mismatch hypothesis in an unsupervised fashion. Our\nexperiments show that extracting saliency in this fashion provides comparable\nperformance when measured against the state-of-art supervised algorithms.\nAdditionally, the robustness outperforms those algorithms when we add large\nnoise to the input images. Also, we show that semantic features contribute more\nthan low-level features for human visual saliency detection.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 23:14:24 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sun", "Yutong", ""], ["Prabhushankar", "Mohit", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2008.01950", "submitter": "Keemin Sohn", "authors": "Gyeongjun Kim and Keemin Sohn", "title": "Area-wide traffic signal control based on a deep graph Q-Network (DGQN)\n  trained in an asynchronous manner", "comments": "34 pages, 10 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms have been widely applied in traffic\nsignal studies. There are, however, several problems in jointly controlling\ntraffic lights for a large transportation network. First, the action space\nexponentially explodes as the number of intersections to be jointly controlled\nincreases. Although a multi-agent RL algorithm has been used to solve the curse\nof dimensionality, this neither guaranteed a global optimum, nor could it break\nthe ties between joint actions. The problem was circumvented by revising the\noutput structure of a deep Q-network (DQN) within the framework of a\nsingle-agent RL algorithm. Second, when mapping traffic states into an action\nvalue, it is difficult to consider spatio-temporal correlations over a large\ntransportation network. A deep graph Q-network (DGQN) was devised to\nefficiently accommodate spatio-temporal dependencies on a large scale. Finally,\ntraining a RL model to jointly control traffic lights in a large transportation\nnetwork requires much time to converge. An asynchronous update methodology was\ndevised for a DGQN to quickly reach an optimal policy. Using these three\nremedies, a DGQN succeeded in jointly controlling the traffic lights in a large\ntransportation network in Seoul. This approach outperformed other\nstate-of-the-art RL algorithms as well as an actual fixed-signal operation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 06:13:58 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kim", "Gyeongjun", ""], ["Sohn", "Keemin", ""]]}, {"id": "2008.01967", "submitter": "Heye Zhang", "authors": "Jingyu Hao and Chengjia Wang and Heye Zhang and Guang Yang", "title": "Annealing Genetic GAN for Minority Oversampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key to overcome class imbalance problems is to capture the distribution\nof minority class accurately. Generative Adversarial Networks (GANs) have shown\nsome potentials to tackle class imbalance problems due to their capability of\nreproducing data distributions given ample training data samples. However, the\nscarce samples of one or more classes still pose a great challenge for GANs to\nlearn accurate distributions for the minority classes. In this work, we propose\nan Annealing Genetic GAN (AGGAN) method, which aims to reproduce the\ndistributions closest to the ones of the minority classes using only limited\ndata samples. Our AGGAN renovates the training of GANs as an evolutionary\nprocess that incorporates the mechanism of simulated annealing. In particular,\nthe generator uses different training strategies to generate multiple offspring\nand retain the best. Then, we use the Metropolis criterion in the simulated\nannealing to decide whether we should update the best offspring for the\ngenerator. As the Metropolis criterion allows a certain chance to accept the\nworse solutions, it enables our AGGAN steering away from the local optimum.\nAccording to both theoretical analysis and experimental studies on multiple\nimbalanced image datasets, we prove that the proposed training strategy can\nenable our AGGAN to reproduce the distributions of minority classes from scarce\nsamples and provide an effective and robust solution for the class imbalance\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 07:19:47 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Hao", "Jingyu", ""], ["Wang", "Chengjia", ""], ["Zhang", "Heye", ""], ["Yang", "Guang", ""]]}, {"id": "2008.02067", "submitter": "Homayoun Valafar", "authors": "Homayoun Valafar, Faramarz Valafar, Okan Ersoy", "title": "Parallel, Self Organizing, Consensus Neural Networks", "comments": "4 pages", "journal-ref": "Published in IEEE-IJCNN 1999 1225-1228", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new neural network architecture (PSCNN) is developed to improve performance\nand speed of such networks. The architecture has all the advantages of the\nprevious models such as self-organization and possesses some other superior\ncharacteristics such as input parallelism and decision making based on\nconsensus. Due to the properties of this network, it was studied with respect\nto implementation on a Parallel Processor (Ncube Machine) as well as a regular\nsequential machine. The architecture self organizes its own modules in a way to\nmaximize performance. Since it is completely parallel, both recall and learning\nprocedures are very fast. The performance of the network was compared to the\nBackpropagation networks in problems of language perception, remote sensing and\nbinary logic (Exclusive-Or). PSCNN showed superior performance in all cases\nstudied.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:02:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Valafar", "Homayoun", ""], ["Valafar", "Faramarz", ""], ["Ersoy", "Okan", ""]]}, {"id": "2008.02072", "submitter": "Homayoun Valafar", "authors": "Faramarz Valafar, Homayoun Valafar", "title": "A Comparative study of Artificial Neural Networks Using Reinforcement\n  learning and Multidimensional Bayesian Classification Using Parzen Density\n  Estimation for Identification of GC-EIMS Spectra of Partially Methylated\n  Alditol Acetates", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": "Published in IEEE-ICAI 1999 554-558", "categories": "eess.SP cs.LG cs.NE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study reports the development of a pattern recognition search engine for\na World Wide Web-based database of gas chromatography-electron impact mass\nspectra (GC-EIMS) of partially methylated Alditol Acetates (PMAAs). Here, we\nalso report comparative results for two pattern recognition techniques that\nwere employed for this study. The first technique is a statistical technique\nusing Bayesian classifiers and Parzen density estimators. The second technique\ninvolves an artificial neural network module trained with reinforcement\nlearning. We demonstrate here that both systems perform well in identifying\nspectra with small amounts of noise. Both system's performance degrades with\ndegrading signal-to-noise ratio (SNR). When dealing with partial spectra\n(missing data), the artificial neural network system performs better. The\ndeveloped system is implemented on the world wide web, and is intended to\nidentify PMAAs using submitted spectra of these molecules recorded on any\nGC-EIMS instrument. The system, therefore, is insensitive to instrument and\ncolumn dependent variations in GC-EIMS spectra.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:54:51 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Valafar", "Faramarz", ""], ["Valafar", "Homayoun", ""]]}, {"id": "2008.02116", "submitter": "J{\\o}rgen Nordmoen", "authors": "J{\\o}rgen Nordmoen, Frank Veenstra, Kai Olav Ellefsen and Kyrre Glette", "title": "Quality and Diversity in Evolutionary Modular Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Evolutionary Robotics a population of solutions is evolved to optimize\nrobots that solve a given task. However, in traditional Evolutionary\nAlgorithms, the population of solutions tends to converge to local optima when\nthe problem is complex or the search space is large, a problem known as\npremature convergence. Quality Diversity algorithms try to overcome premature\nconvergence by introducing additional measures that reward solutions for being\ndifferent while not necessarily performing better. In this paper we compare a\nsingle objective Evolutionary Algorithm with two diversity promoting search\nalgorithms; a Multi-Objective Evolutionary Algorithm and MAP-Elites a Quality\nDiversity algorithm, for the difficult problem of evolving control and\nmorphology in modular robotics. We compare their ability to produce high\nperforming solutions, in addition to analyze the evolved morphological\ndiversity. The results show that all three search algorithms are capable of\nevolving high performing individuals. However, the Quality Diversity algorithm\nis better adept at filling all niches with high-performing solutions. This\nconfirms that Quality Diversity algorithms are well suited for evolving modular\nrobots and can be an important means of generating repertoires of high\nperforming solutions that can be exploited both at design- and runtime.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 13:08:14 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Nordmoen", "J\u00f8rgen", ""], ["Veenstra", "Frank", ""], ["Ellefsen", "Kai Olav", ""], ["Glette", "Kyrre", ""]]}, {"id": "2008.02189", "submitter": "Anakha Vasanthakumari Babu", "authors": "Anakha V Babu, Osvaldo Simeone, Bipin Rajendran", "title": "SpinAPS: A High-Performance Spintronic Accelerator for Probabilistic\n  Spiking Neural Networks", "comments": "25 pages, 10 figures, Submitted to Elsevier Neural Networks for\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a high-performance and high-throughput hardware accelerator for\nprobabilistic Spiking Neural Networks (SNNs) based on Generalized Linear Model\n(GLM) neurons, that uses binary STT-RAM devices as synapses and digital CMOS\nlogic for neurons. The inference accelerator, termed \"SpinAPS\" for Spintronic\nAccelerator for Probabilistic SNNs, implements a principled direct learning\nrule for first-to-spike decoding without the need for conversion from\npre-trained ANNs. The proposed solution is shown to achieve comparable\nperformance with an equivalent ANN on handwritten digit and human activity\nrecognition benchmarks. The inference engine, SpinAPS, is shown through\nsoftware emulation tools to achieve 4x performance improvement in terms of\nGSOPS/W/mm2 when compared to an equivalent SRAM-based design. The architecture\nleverages probabilistic spiking neural networks that employ first-to-spike\ndecoding rule to make inference decisions at low latencies, achieving 75% of\nthe test performance in as few as 4 algorithmic time steps on the handwritten\ndigit benchmark. The accelerator also exhibits competitive performance with\nother memristor-based DNN/SNN accelerators and state-of-the-art GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:37:47 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Babu", "Anakha V", ""], ["Simeone", "Osvaldo", ""], ["Rajendran", "Bipin", ""]]}, {"id": "2008.02217", "submitter": "Hubert Ramsauer", "authors": "Hubert Ramsauer, Bernhard Sch\\\"afl, Johannes Lehner, Philipp Seidl,\n  Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena\n  Pavlovi\\'c, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp,\n  G\\\"unter Klambauer, Johannes Brandstetter, Sepp Hochreiter", "title": "Hopfield Networks is All You Need", "comments": "10 pages (+ appendix); 12 figures; Blog:\n  https://ml-jku.github.io/hopfield-layers/; GitHub:\n  https://github.com/ml-jku/hopfield-layers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a modern Hopfield network with continuous states and a\ncorresponding update rule. The new Hopfield network can store exponentially\n(with the dimension of the associative space) many patterns, retrieves the\npattern with one update, and has exponentially small retrieval errors. It has\nthree types of energy minima (fixed points of the update): (1) global fixed\npoint averaging over all patterns, (2) metastable states averaging over a\nsubset of patterns, and (3) fixed points which store a single pattern. The new\nupdate rule is equivalent to the attention mechanism used in transformers. This\nequivalence enables a characterization of the heads of transformer models.\nThese heads perform in the first layers preferably global averaging and in\nhigher layers partial averaging via metastable states. The new modern Hopfield\nnetwork can be integrated into deep learning architectures as layers to allow\nthe storage of and access to raw input data, intermediate results, or learned\nprototypes. These Hopfield layers enable new ways of deep learning, beyond\nfully-connected, convolutional, or recurrent networks, and provide pooling,\nmemory, association, and attention mechanisms. We demonstrate the broad\napplicability of the Hopfield layers across various domains. Hopfield layers\nimproved state-of-the-art on three out of four considered multiple instance\nlearning problems as well as on immune repertoire classification with several\nhundreds of thousands of instances. On the UCI benchmark collections of small\nclassification tasks, where deep learning methods typically struggle, Hopfield\nlayers yielded a new state-of-the-art when compared to different machine\nlearning methods. Finally, Hopfield layers achieved state-of-the-art on two\ndrug design datasets. The implementation is available at:\nhttps://github.com/ml-jku/hopfield-layers\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:52:37 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 14:16:15 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 07:24:49 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ramsauer", "Hubert", ""], ["Sch\u00e4fl", "Bernhard", ""], ["Lehner", "Johannes", ""], ["Seidl", "Philipp", ""], ["Widrich", "Michael", ""], ["Adler", "Thomas", ""], ["Gruber", "Lukas", ""], ["Holzleitner", "Markus", ""], ["Pavlovi\u0107", "Milena", ""], ["Sandve", "Geir Kjetil", ""], ["Greiff", "Victor", ""], ["Kreil", "David", ""], ["Kopp", "Michael", ""], ["Klambauer", "G\u00fcnter", ""], ["Brandstetter", "Johannes", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "2008.02387", "submitter": "Rupesh Kumar Srivastava", "authors": "Nihat Engin Toklu, Pawe{\\l} Liskowski, Rupesh Kumar Srivastava", "title": "ClipUp: A Simple and Powerful Optimizer for Distribution-based Policy\n  Evolution", "comments": "20 pages, 7 figures. Extended version of work appearing in PPSN 2020.\n  Code available at https://github.com/nnaisense/pgpelib", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution-based search algorithms are an effective approach for\nevolutionary reinforcement learning of neural network controllers. In these\nalgorithms, gradients of the total reward with respect to the policy parameters\nare estimated using a population of solutions drawn from a search distribution,\nand then used for policy optimization with stochastic gradient ascent. A common\nchoice in the community is to use the Adam optimization algorithm for obtaining\nan adaptive behavior during gradient ascent, due to its success in a variety of\nsupervised learning settings. As an alternative to Adam, we propose to enhance\nclassical momentum-based gradient ascent with two simple techniques: gradient\nnormalization and update clipping. We argue that the resulting optimizer called\nClipUp (short for \"clipped updates\") is a better choice for distribution-based\npolicy evolution because its working principles are simple and easy to\nunderstand and its hyperparameters can be tuned more intuitively in practice.\nMoreover, it removes the need to re-tune hyperparameters if the reward scale\nchanges. Experiments show that ClipUp is competitive with Adam despite its\nsimplicity and is effective on challenging continuous control benchmarks,\nincluding the Humanoid control task based on the Bullet physics simulator.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 22:46:23 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 05:45:38 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 05:32:08 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Toklu", "Nihat Engin", ""], ["Liskowski", "Pawe\u0142", ""], ["Srivastava", "Rupesh Kumar", ""]]}, {"id": "2008.02454", "submitter": "Yash Bhalgat", "authors": "Yash Bhalgat, Yizhe Zhang, Jamie Lin, Fatih Porikli", "title": "Structured Convolutions for Efficient Neural Network Design", "comments": "Camera-ready for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we tackle model efficiency by exploiting redundancy in the\n\\textit{implicit structure} of the building blocks of convolutional neural\nnetworks. We start our analysis by introducing a general definition of\nComposite Kernel structures that enable the execution of convolution operations\nin the form of efficient, scaled, sum-pooling components. As its special case,\nwe propose \\textit{Structured Convolutions} and show that these allow\ndecomposition of the convolution operation into a sum-pooling operation\nfollowed by a convolution with significantly lower complexity and fewer\nweights. We show how this decomposition can be applied to 2D and 3D kernels as\nwell as the fully-connected layers. Furthermore, we present a Structural\nRegularization loss that promotes neural network layers to leverage on this\ndesired structure in a way that, after training, they can be decomposed with\nnegligible performance loss. By applying our method to a wide range of CNN\narchitectures, we demonstrate \"structured\" versions of the ResNets that are up\nto 2$\\times$ smaller and a new Structured-MobileNetV2 that is more efficient\nwhile staying within an accuracy loss of 1% on ImageNet and CIFAR-10 datasets.\nWe also show similar structured versions of EfficientNet on ImageNet and HRNet\narchitecture for semantic segmentation on the Cityscapes dataset. Our method\nperforms equally well or superior in terms of the complexity reduction in\ncomparison to the existing tensor decomposition and channel pruning methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 04:38:38 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 04:41:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bhalgat", "Yash", ""], ["Zhang", "Yizhe", ""], ["Lin", "Jamie", ""], ["Porikli", "Fatih", ""]]}, {"id": "2008.02580", "submitter": "Theo Ladune", "authors": "Th\\'eo Ladune (IETR), Pierrick Philippe, Wassim Hamidouche (IETR), Lu\n  Zhang (IETR), Olivier D\\'eforges (IETR)", "title": "Optical Flow and Mode Selection for Learning-based Video Coding", "comments": "MMSP 2020, IEEE 22nd International Workshop on Multimedia Signal\n  Processing, Sep 2020, Tampere, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for inter-frame coding based on two\ncomplementary autoencoders: MOFNet and CodecNet. MOFNet aims at computing and\nconveying the Optical Flow and a pixel-wise coding Mode selection. The optical\nflow is used to perform a prediction of the frame to code. The coding mode\nselection enables competition between direct copy of the prediction or\ntransmission through CodecNet. The proposed coding scheme is assessed under the\nChallenge on Learned Image Compression 2020 (CLIC20) P-frame coding conditions,\nwhere it is shown to perform on par with the state-of-the-art video codec\nITU/MPEG HEVC. Moreover, the possibility of copying the prediction enables to\nlearn the optical flow in an end-to-end fashion i.e. without relying on\npre-training and/or a dedicated loss term.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:21:22 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Ladune", "Th\u00e9o", "", "IETR"], ["Philippe", "Pierrick", "", "IETR"], ["Hamidouche", "Wassim", "", "IETR"], ["Zhang", "Lu", "", "IETR"], ["D\u00e9forges", "Olivier", "", "IETR"]]}, {"id": "2008.02837", "submitter": "Preslav Nakov", "authors": "Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov", "title": "aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF,\n  and Transfer Learning", "comments": "propaganda, persuasion, disinformation, fake news", "journal-ref": "SemEval-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our system for SemEval-2020 Task 11 on Detection of Propaganda\nTechniques in News Articles. We developed ensemble models using RoBERTa-based\nneural architectures, additional CRF layers, transfer learning between the two\nsubtasks, and advanced post-processing to handle the multi-label nature of the\ntask, the consistency between nested spans, repetitions, and labels from\nsimilar spans in training. We achieved sizable improvements over baseline\nfine-tuned RoBERTa models, and the official evaluation ranked our system 3rd\n(almost tied with the 2nd) out of 36 teams on the span identification subtask\nwith an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams\non the technique classification subtask with an F1 score of 0.62.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 18:45:25 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chernyavskiy", "Anton", ""], ["Ilvovsky", "Dmitry", ""], ["Nakov", "Preslav", ""]]}, {"id": "2008.03330", "submitter": "Karolos-Alexandros Tsakalos", "authors": "Karolos-Alexandros Tsakalos, Georgios Ch. Sirakoulis, Andrew\n  Adamatzky, Jim Smith", "title": "Protein Structured Reservoir computing for Spike-based Pattern\n  Recognition", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TPDS.2021.3068826", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays we witness a miniaturisation trend in the semiconductor industry\nbacked up by groundbreaking discoveries and designs in nanoscale\ncharacterisation and fabrication. To facilitate the trend and produce ever\nsmaller, faster and cheaper computing devices, the size of nanoelectronic\ndevices is now reaching the scale of atoms or molecules - a technical goal\nundoubtedly demanding for novel devices. Following the trend, we explore an\nunconventional route of implementing a reservoir computing on a single protein\nmolecule and introduce neuromorphic connectivity with a small-world networking\nproperty. We have chosen Izhikevich spiking neurons as elementary processors,\ncorresponding to the atoms of verotoxin protein, and its molecule as a\n'hardware' architecture of the communication networks connecting the\nprocessors. We apply on a single readout layer various training methods in a\nsupervised fashion to investigate whether the molecular structured Reservoir\nComputing (RC) system is capable to deal with machine learning benchmarks. We\nstart with the Remote Supervised Method, based on\nSpike-Timing-Dependent-Plasticity, and carry on with linear regression and\nscaled conjugate gradient back-propagation training methods. The RC network is\nevaluated as a proof-of-concept on the handwritten digit images from the MNIST\ndataset and demonstrates acceptable classification accuracy in comparison with\nother similar approaches.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 18:30:12 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:09:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tsakalos", "Karolos-Alexandros", ""], ["Sirakoulis", "Georgios Ch.", ""], ["Adamatzky", "Andrew", ""], ["Smith", "Jim", ""]]}, {"id": "2008.03470", "submitter": "Yulia Sandamirskaya", "authors": "Sandro Baumgartner, Alpha Renner, Raphaela Kreiser, Dongchen Liang,\n  Giacomo Indiveri, Yulia Sandamirskaya", "title": "Visual Pattern Recognition with on On-chip Learning: towards a Fully\n  Neuromorphic Approach", "comments": "5 pages. Accepted to ISCAS 2020 conference", "journal-ref": null, "doi": "10.1109/ISCAS45731.2020.9180628", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a spiking neural network (SNN) for visual pattern recognition with\non-chip learning on neuromorphichardware. We show how this network can learn\nsimple visual patterns composed of horizontal and vertical bars sensed by a\nDynamic Vision Sensor, using a local spike-based plasticity rule. During\nrecognition, the network classifies the pattern's identity while at the same\ntime estimating its location and scale. We build on previous work that used\nlearning with neuromorphic hardware in the loop and demonstrate that the\nproposed network can properly operate with on-chip learning, demonstrating a\ncomplete neuromorphic pattern learning and recognition setup. Our results show\nthat the network is robust against noise on the input (no accuracy drop when\nadding 130% noise) and against up to 20% noise in the neuron parameters.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 08:07:36 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Baumgartner", "Sandro", ""], ["Renner", "Alpha", ""], ["Kreiser", "Raphaela", ""], ["Liang", "Dongchen", ""], ["Indiveri", "Giacomo", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "2008.03501", "submitter": "Ilona Kulikovskikh", "authors": "Ilona Kulikovskikh and Tarzan Legovi\\'c", "title": "Why to \"grow\" and \"harvest\" deep learning models?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current expectations from training deep learning models with gradient-based\nmethods include: 1) transparency; 2) high convergence rates; 3) high inductive\nbiases. While the state-of-art methods with adaptive learning rate schedules\nare fast, they still fail to meet the other two requirements. We suggest\nreconsidering neural network models in terms of single-species population\ndynamics where adaptation comes naturally from open-ended processes of \"growth\"\nand \"harvesting\". We show that the stochastic gradient descent (SGD) with two\nbalanced pre-defined values of per capita growth and harvesting rates\noutperform the most common adaptive gradient methods in all of the three\nrequirements.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 11:55:24 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kulikovskikh", "Ilona", ""], ["Legovi\u0107", "Tarzan", ""]]}, {"id": "2008.03530", "submitter": "Ali ALsaeedi Ali H.Alsaeedi", "authors": "Ali Hakem Alsaeedi, Adil L. Albukhnefis, Dhiah Al-Shammary, Muntasir\n  Al-Asfoor", "title": "Extended Particle Swarm Optimization (EPSO) for Feature Selection of\n  High Dimensional Biomedical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes a novel Extended Particle Swarm Optimization model (EPSO)\nthat potentially enhances the search process of PSO for optimization problem.\nEvidently, gene expression profiles are significantly important measurement\nfactor in molecular biology that is used in medical diagnosis of cancer types.\nThe challenge to certain classification methodologies for gene expression\nprofiles lies in the thousands of features recorded for each sample. A modified\nWrapper feature selection model is applied with the aim of addressing the gene\nclassification challenge by replacing its randomness approach with EPSO and PSO\nrespectively. EPSO is initializing the random size of the population and\ndividing them into two groups in order to promote the exploration and reduce\nthe probability of falling in stagnation. Experimentally, EPSO has required\nless processing time to select the optimal features (average of 62.14 sec) than\nPSO (average of 95.72 sec). Furthermore, EPSO accuracy has provided better\nclassification results (start from 54% to 100%) than PSO (start from 52% to\n96%).\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 14:09:44 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Alsaeedi", "Ali Hakem", ""], ["Albukhnefis", "Adil L.", ""], ["Al-Shammary", "Dhiah", ""], ["Al-Asfoor", "Muntasir", ""]]}, {"id": "2008.03543", "submitter": "Kamal Berahmand", "authors": "Mehrdad Rostami, Kamal Berahmand, Saman Forouzandeh", "title": "A Novel Community Detection Based Genetic Algorithm for Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The selection of features is an essential data preprocessing stage in data\nmining. The core principle of feature selection seems to be to pick a subset of\npossible features by excluding features with almost no predictive information\nas well as highly associated redundant features. In the past several years, a\nvariety of meta-heuristic methods were introduced to eliminate redundant and\nirrelevant features as much as possible from high-dimensional datasets. Among\nthe main disadvantages of present meta-heuristic based approaches is that they\nare often neglecting the correlation between a set of selected features. In\nthis article, for the purpose of feature selection, the authors propose a\ngenetic algorithm based on community detection, which functions in three steps.\nThe feature similarities are calculated in the first step. The features are\nclassified by community detection algorithms into clusters throughout the\nsecond step. In the third step, features are picked by a genetic algorithm with\na new community-based repair operation. Nine benchmark classification problems\nwere analyzed in terms of the performance of the presented approach. Also, the\nauthors have compared the efficiency of the proposed approach with the findings\nfrom four available algorithms for feature selection. The findings indicate\nthat the new approach continuously yields improved classification accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:39:30 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rostami", "Mehrdad", ""], ["Berahmand", "Kamal", ""], ["Forouzandeh", "Saman", ""]]}, {"id": "2008.03602", "submitter": "Aditya Dhakal", "authors": "Aditya Dhakal, Junguk Cho, Sameer G. Kulkarni, K. K. Ramakrishnan,\n  Puneet Sharma", "title": "Spatial Sharing of GPU for Autotuning DNN models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are used for training, inference, and tuning the machine learning\nmodels. However, Deep Neural Network (DNN) vary widely in their ability to\nexploit the full power of high-performance GPUs. Spatial sharing of GPU enables\nmultiplexing several DNNs on the GPU and can improve GPU utilization, thus\nimproving throughput and lowering latency. DNN models given just the right\namount of GPU resources can still provide low inference latency, just as much\nas dedicating all of the GPU for their inference task. An approach to improve\nDNN inference is tuning of the DNN model. Autotuning frameworks find the\noptimal low-level implementation for a certain target device based on the\ntrained machine learning model, thus reducing the DNN's inference latency and\nincreasing inference throughput. We observe an interdependency between the\ntuned model and its inference latency. A DNN model tuned with specific GPU\nresources provides the best inference latency when inferred with close to the\nsame amount of GPU resources. While a model tuned with the maximum amount of\nthe GPU's resources has poorer inference latency once the GPU resources are\nlimited for inference. On the other hand, a model tuned with an appropriate\namount of GPU resources still achieves good inference latency across a wide\nrange of GPU resource availability. We explore the causes that impact the\ntuning of a model at different amounts of GPU resources. We present many\ntechniques to maximize resource utilization and improve tuning performance. We\nenable controlled spatial sharing of GPU to multiplex several tuning\napplications on the GPU. We scale the tuning server instances and shard the\ntuning model across multiple client instances for concurrent tuning of\ndifferent operators of a model, achieving better GPU multiplexing. With our\nimprovements, we decrease DNN autotuning time by up to 75 percent and increase\nthroughput by a factor of 5.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:27:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Dhakal", "Aditya", ""], ["Cho", "Junguk", ""], ["Kulkarni", "Sameer G.", ""], ["Ramakrishnan", "K. K.", ""], ["Sharma", "Puneet", ""]]}, {"id": "2008.03620", "submitter": "Javier Del Ser Dr.", "authors": "Aritz D. Martinez, Javier Del Ser, Esther Villar-Rodriguez, Eneko\n  Osaba, Javier Poyatos, Siham Tabik, Daniel Molina, Francisco Herrera", "title": "Lights and Shadows in Evolutionary Deep Learning: Taxonomy, Critical\n  Methodological Analysis, Cases of Study, Learned Lessons, Recommendations and\n  Challenges", "comments": "64 pages, 18 figures, under review for its consideration in\n  Information Fusion journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much has been said about the fusion of bio-inspired optimization algorithms\nand Deep Learning models for several purposes: from the discovery of network\ntopologies and hyper-parametric configurations with improved performance for a\ngiven task, to the optimization of the model's parameters as a replacement for\ngradient-based solvers. Indeed, the literature is rich in proposals showcasing\nthe application of assorted nature-inspired approaches for these tasks. In this\nwork we comprehensively review and critically examine contributions made so far\nbased on three axes, each addressing a fundamental question in this research\navenue: a) optimization and taxonomy (Why?), including a historical\nperspective, definitions of optimization problems in Deep Learning, and a\ntaxonomy associated with an in-depth analysis of the literature, b) critical\nmethodological analysis (How?), which together with two case studies, allows us\nto address learned lessons and recommendations for good practices following the\nanalysis of the literature, and c) challenges and new directions of research\n(What can be done, and what for?). In summary, three axes - optimization and\ntaxonomy, critical analysis, and challenges - which outline a complete vision\nof a merger of two technologies drawing up an exciting future for this area of\nfusion research.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 00:25:06 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Martinez", "Aritz D.", ""], ["Del Ser", "Javier", ""], ["Villar-Rodriguez", "Esther", ""], ["Osaba", "Eneko", ""], ["Poyatos", "Javier", ""], ["Tabik", "Siham", ""], ["Molina", "Daniel", ""], ["Herrera", "Francisco", ""]]}, {"id": "2008.03649", "submitter": "Edward Pantridge", "authors": "Edward Pantridge, Lee Spector", "title": "Code Building Genetic Programming", "comments": "Proceedings of the 2020 Genetic and Evolutionary Computation\n  Conference, Genetic Programming Track", "journal-ref": null, "doi": "10.1145/3377930.3390239", "report-no": null, "categories": "cs.PL cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the field of genetic programming has made significant\nadvances towards automatic programming. Research and development of\ncontemporary program synthesis methods, such as PushGP and Grammar Guided\nGenetic Programming, can produce programs that solve problems typically\nassigned in introductory academic settings. These problems focus on a narrow,\npredetermined set of simple data structures, basic control flow patterns, and\nprimitive, non-overlapping data types (without, for example, inheritance or\ncomposite types). Few, if any, genetic programming methods for program\nsynthesis have convincingly demonstrated the capability of synthesizing\nprograms that use arbitrary data types, data structures, and specifications\nthat are drawn from existing codebases. In this paper, we introduce Code\nBuilding Genetic Programming (CBGP) as a framework within which this can be\ndone, by leveraging programming language features such as reflection and\nfirst-class specifications. CBGP produces a computational graph that can be\nexecuted or translated into source code of a host language. To demonstrate the\nnovel capabilities of CBGP, we present results on new benchmarks that use\nnon-primitive, polymorphic data types as well as some standard program\nsynthesis benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 04:33:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Pantridge", "Edward", ""], ["Spector", "Lee", ""]]}, {"id": "2008.03658", "submitter": "Nitin Rathi", "authors": "Nitin Rathi, Kaushik Roy", "title": "DIET-SNN: Direct Input Encoding With Leakage and Threshold Optimization\n  in Deep Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-inspired spiking neural networks (SNNs), operating with asynchronous\nbinary signals (or spikes) distributed over time, can potentially lead to\ngreater computational efficiency on event-driven hardware. The state-of-the-art\nSNNs suffer from high inference latency, resulting from inefficient input\nencoding, and sub-optimal settings of the neuron parameters (firing threshold,\nand membrane leak). We propose DIET-SNN, a low-latency deep spiking network\nthat is trained with gradient descent to optimize the membrane leak and the\nfiring threshold along with other network parameters (weights). The membrane\nleak and threshold for each layer of the SNN are optimized with end-to-end\nbackpropagation to achieve competitive accuracy at reduced latency. The analog\npixel values of an image are directly applied to the input layer of DIET-SNN\nwithout the need to convert to spike-train. The first convolutional layer is\ntrained to convert inputs into spikes where leaky-integrate-and-fire (LIF)\nneurons integrate the weighted inputs and generate an output spike when the\nmembrane potential crosses the trained firing threshold. The trained membrane\nleak controls the flow of input information and attenuates irrelevant inputs to\nincrease the activation sparsity in the convolutional and dense layers of the\nnetwork. The reduced latency combined with high activation sparsity provides\nlarge improvements in computational efficiency. We evaluate DIET-SNN on image\nclassification tasks from CIFAR and ImageNet datasets on VGG and ResNet\narchitectures. We achieve top-1 accuracy of 69% with 5 timesteps (inference\nlatency) on the ImageNet dataset with 12x less compute energy than an\nequivalent standard ANN. Additionally, DIET-SNN performs 20-500x faster\ninference compared to other state-of-the-art SNN models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 05:07:17 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 23:14:39 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 02:55:31 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Rathi", "Nitin", ""], ["Roy", "Kaushik", ""]]}, {"id": "2008.03681", "submitter": "Zoubir Hamici", "authors": "Zoubir Hamici", "title": "Randomness Evaluation of a Genetic Algorithm for Image Encryption: A\n  Signal Processing Approach", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a randomness evaluation of a block cipher for secure image\ncommunication is presented. The GFHT cipher is a genetic algorithm, that\ncombines gene fusion (GF) and horizontal gene transfer (HGT) both inspired from\nantibiotic resistance in bacteria. The symmetric encryption key is generated by\nfour pairs of chromosomes with multi-layer random sequences. The encryption\nstarts by a GF of the principal key-agent in a single block, then HGT performs\nobfuscation where the genes are pixels and the chromosomes are the rows and\ncolumns. A Salt extracted from the image hash-value is used to implement\none-time pad (OTP) scheme, hence a modification of one pixel generates a\ndifferent encryption key without changing the main passphrase or key.\nTherefore, an extreme avalanche effect of 99% is achieved. Randomness\nevaluation based on random matrix theory, power spectral density, avalanche\neffect, 2D auto-correlation, pixels randomness tests and chi-square hypotheses\ntesting show that encrypted images adopt the statistical behavior of uniform\nwhite noise; hence validating the theoretical model by experimental results.\nMoreover, performance comparison with chaos-genetic ciphers shows the merit of\nthe GFHT algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 07:50:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hamici", "Zoubir", ""]]}, {"id": "2008.03936", "submitter": "Moritz Firsching", "authors": "Thomas Fischbacher and Iulia M. Comsa and Krzysztof Potempa and Moritz\n  Firsching and Luca Versari and Jyrki Alakuijala", "title": "Intelligent Matrix Exponentiation", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.RT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel machine learning architecture that uses the exponential of\na single input-dependent matrix as its only nonlinearity. The mathematical\nsimplicity of this architecture allows a detailed analysis of its behaviour,\nproviding robustness guarantees via Lipschitz bounds. Despite its simplicity, a\nsingle matrix exponential layer already provides universal approximation\nproperties and can learn fundamental functions of the input, such as periodic\nfunctions or multivariate polynomials. This architecture outperforms other\ngeneral-purpose architectures on benchmark problems, including CIFAR-10, using\nsubstantially fewer parameters.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 07:49:01 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Fischbacher", "Thomas", ""], ["Comsa", "Iulia M.", ""], ["Potempa", "Krzysztof", ""], ["Firsching", "Moritz", ""], ["Versari", "Luca", ""], ["Alakuijala", "Jyrki", ""]]}, {"id": "2008.04002", "submitter": "Maryam Hasani-Shoreh", "authors": "Maryam Hasani Shoreh, Renato Hermoza Aragon\\'es, Frank Neumann", "title": "Using Neural Networks and Diversifying Differential Evolution for\n  Dynamic Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic optimisation occurs in a variety of real-world problems. To tackle\nthese problems, evolutionary algorithms have been extensively used due to their\neffectiveness and minimum design effort. However, for dynamic problems, extra\nmechanisms are required on top of standard evolutionary algorithms. Among them,\ndiversity mechanisms have proven to be competitive in handling dynamism, and\nrecently, the use of neural networks have become popular for this purpose.\nConsidering the complexity of using neural networks in the process compared to\nsimple diversity mechanisms, we investigate whether they are competitive and\nthe possibility of integrating them to improve the results. However, for a fair\ncomparison, we need to consider the same time budget for each algorithm. Thus,\ninstead of the usual number of fitness evaluations as the measure for the\navailable time between changes, we use wall clock timing. The results show the\nsignificance of the improvement when integrating the neural network and\ndiversity mechanisms depends on the type and the frequency of changes.\nMoreover, we observe that for differential evolution, having a proper diversity\nin population when using neural networks plays a key role in the neural\nnetwork's ability to improve the results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 10:07:43 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Shoreh", "Maryam Hasani", ""], ["Aragon\u00e9s", "Renato Hermoza", ""], ["Neumann", "Frank", ""]]}, {"id": "2008.04103", "submitter": "Kamal Berahmand", "authors": "Mehrdad Rostami, Kamal Berahmand, Saman Forouzandeh", "title": "Review of Swarm Intelligence-based Feature Selection Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the past decades, the rapid growth of computer and database technologies\nhas led to the rapid growth of large-scale datasets. On the other hand, data\nmining applications with high dimensional datasets that require high speed and\naccuracy are rapidly increasing. An important issue with these applications is\nthe curse of dimensionality, where the number of features is much higher than\nthe number of patterns. One of the dimensionality reduction approaches is\nfeature selection that can increase the accuracy of the data mining task and\nreduce its computational complexity. The feature selection method aims at\nselecting a subset of features with the lowest inner similarity and highest\nrelevancy to the target class. It reduces the dimensionality of the data by\neliminating irrelevant, redundant, or noisy data. In this paper, a comparative\nanalysis of different feature selection methods is presented, and a general\ncategorization of these methods is performed. Moreover, in this paper,\nstate-of-the-art swarm intelligence are studied, and the recent feature\nselection methods based on these algorithms are reviewed. Furthermore, the\nstrengths and weaknesses of the different studied swarm intelligence-based\nfeature selection methods are evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 05:18:58 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rostami", "Mehrdad", ""], ["Berahmand", "Kamal", ""], ["Forouzandeh", "Saman", ""]]}, {"id": "2008.04130", "submitter": "Longkang Li", "authors": "Longkang Li, Hui-Ling Zhen, Mingxuan Yuan, Jiawen Lu, XialiangTong,\n  Jia Zeng, Jun Wang, Dirk Schnieders", "title": "Bilevel Learning Model Towards Industrial Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic industrial scheduling, aiming at optimizing the sequence of jobs\nover limited resources, is widely needed in manufacturing industries. However,\nexisting scheduling systems heavily rely on heuristic algorithms, which either\ngenerate ineffective solutions or compute inefficiently when job scale\nincreases. Thus, it is of great importance to develop new large-scale\nalgorithms that are not only efficient and effective, but also capable of\nsatisfying complex constraints in practice. In this paper, we propose a Bilevel\nDeep reinforcement learning Scheduler, \\textit{BDS}, in which the higher level\nis responsible for exploring an initial global sequence, whereas the lower\nlevel is aiming at exploitation for partial sequence refinements, and the two\nlevels are connected by a sliding-window sampling mechanism. In the\nimplementation, a Double Deep Q Network (DDQN) is used in the upper level and\nGraph Pointer Network (GPN) lies within the lower level. After the theoretical\nguarantee for the convergence of BDS, we evaluate it in an industrial automatic\nwarehouse scenario, with job number up to $5000$ in each production line. It is\nshown that our proposed BDS significantly outperforms two most used heuristics,\nthree strong deep networks, and another bilevel baseline approach. In\nparticular, compared with the most used greedy-based heuristic algorithm in\nreal world which takes nearly an hour, our BDS can decrease the makespan by\n27.5\\%, 28.6\\% and 22.1\\% for 3 largest datasets respectively, with\ncomputational time less than 200 seconds.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 13:46:28 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Longkang", ""], ["Zhen", "Hui-Ling", ""], ["Yuan", "Mingxuan", ""], ["Lu", "Jiawen", ""], ["XialiangTong", "", ""], ["Zeng", "Jia", ""], ["Wang", "Jun", ""], ["Schnieders", "Dirk", ""]]}, {"id": "2008.04208", "submitter": "Seyed Mohammad Mahdi Heidarpoor Yazdi", "authors": "Seyed Mohammad Mahdi Heidarpoor Yazdi, Abdolhossein Abbassian", "title": "Working Memory for Online Memory Binding Tasks: A Hybrid Model", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Working Memory is the brain module that holds and manipulates information\nonline. In this work, we design a hybrid model in which a simple feed-forward\nnetwork is coupled to a balanced random network via a read-write vector called\nthe interface vector. Three cases and their results are discussed similar to\nthe n-back task called, first-order memory binding task, generalized\nfirst-order memory task, and second-order memory binding task. The important\nresult is that our dual-component model of working memory shows good\nperformance with learning restricted to the feed-forward component only. Here\nwe take advantage of the random network property without learning. Finally, a\nmore complex memory binding task called, a cue-based memory binding task, is\nintroduced in which a cue is given as input representing a binding relation\nthat prompts the network to choose the useful chunk of memory. To our\nknowledge, this is the first time that random networks as a flexible memory is\nshown to play an important role in online binding tasks. We may interpret our\nresults as a candidate model of working memory in which the feed-forward\nnetwork learns to interact with the temporary storage random network as an\nattentional-controlling executive system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 14:06:07 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 08:33:38 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yazdi", "Seyed Mohammad Mahdi Heidarpoor", ""], ["Abbassian", "Abdolhossein", ""]]}, {"id": "2008.04210", "submitter": "Oluwasegun Ayokunle Somefun", "authors": "Oluwasegun A. Somefun, Kayode Akingbade and Folasade Dahunsi", "title": "From the logistic-sigmoid to nlogistic-sigmoid: modelling the COVID-19\n  pandemic growth", "comments": "applied to a real-world phenomenon. 11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-world growth processes, such as epidemic growth, are inherently noisy,\nuncertain and often involve multiple growth phases. The logistic-sigmoid\nfunction has been suggested and applied in the domain of modelling such growth\nprocesses. However, existing definitions are limiting, as they do not consider\ngrowth as restricted in two-dimension. Additionally, as the number of growth\nphases increase, the modelling and estimation of logistic parameters becomes\nmore cumbersome, requiring more complex tools and analysis. To remedy this, we\nintroduce the nlogistic-sigmoid function as a compact, unified modern\ndefinition of logistic growth for modelling such real-world growth phenomena.\nAlso, we introduce two characteristic metrics of the logistic-sigmoid curve\nthat can give more robust projections on the state of the growth process in\neach dimension. Specifically, we apply this function to modelling the daily\nWorld Health Organization published COVID-19 time-series data of infection and\ndeath cases of the world and countries of the world to date. Our results\ndemonstrate statistically significant goodness of fit greater than or equal to\n99% for affected countries of the world exhibiting patterns of either single or\nmultiple stages of the ongoing COVID-19 outbreak, such as the USA.\nConsequently, this modern logistic definition and its metrics, as a machine\nlearning tool, can help to provide clearer and more robust monitoring and\nquantification of the ongoing pandemic growth process.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 15:33:48 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 16:02:44 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 02:25:53 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Somefun", "Oluwasegun A.", ""], ["Akingbade", "Kayode", ""], ["Dahunsi", "Folasade", ""]]}, {"id": "2008.04212", "submitter": "Risto Miikkulainen", "authors": "Risto Miikkulainen", "title": "Creative AI Through Evolutionary Computation: Principles and Examples", "comments": "This is an extended version of arXiv:1901.03775", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main power of artificial intelligence is not in modeling what we already\nknow, but in creating solutions that are new. Such solutions exist in extremely\nlarge, high-dimensional, and complex search spaces. Population-based search\ntechniques, i.e. variants of evolutionary computation, are well suited to\nfinding them. These techniques make it possible to find creative solutions to\npractical problems in the real world, making creative AI through evolutionary\ncomputation the likely \"next deep learning.\"\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:53:39 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 02:16:35 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 02:10:27 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Miikkulainen", "Risto", ""]]}, {"id": "2008.04213", "submitter": "Yuan Sun", "authors": "Yuan Sun, Sheng Wang, Yunzhuang Shen, Xiaodong Li, Andreas T. Ernst,\n  and Michael Kirley", "title": "Boosting Ant Colony Optimization via Solution Prediction and Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an enhanced meta-heuristic (ML-ACO) that combines\nmachine learning (ML) and ant colony optimization (ACO) to solve combinatorial\noptimization problems. To illustrate the underlying mechanism of our enhanced\nalgorithm, we start by describing a test problem -- the orienteering problem --\nused to demonstrate the efficacy of ML-ACO. In this problem, the objective is\nto find a route that visits a subset of vertices in a graph within a time\nbudget to maximize the collected score. In the first phase of our ML-ACO\nalgorithm, an ML model is trained using a set of small problem instances where\nthe optimal solution is known. Specifically, classification models are used to\nclassify an edge as being part of the optimal route, or not, using\nproblem-specific features and statistical measures. We have tested several\nclassification models including graph neural networks, logistic regression and\nsupport vector machines. The trained model is then used to predict the\nprobability that an edge in the graph of a test problem instance belongs to the\ncorresponding optimal route. In the second phase, we incorporate the predicted\nprobabilities into the ACO component of our algorithm. Here, the probability\nvalues bias sampling towards favoring those predicted high-quality edges when\nconstructing feasible routes. We empirically show that ML-ACO generates results\nthat are significantly better than the standard ACO algorithm, especially when\nthe computational budget is limited. Furthermore, we show our algorithm is\nrobust in the sense that (a) its overall performance is not sensitive to any\nparticular classification model, and (b) it generalizes well to large and\nreal-world problem instances. Our approach integrating ML with a meta-heuristic\nis generic and can be applied to a wide range of combinatorial optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 13:03:37 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sun", "Yuan", ""], ["Wang", "Sheng", ""], ["Shen", "Yunzhuang", ""], ["Li", "Xiaodong", ""], ["Ernst", "Andreas T.", ""], ["Kirley", "Michael", ""]]}, {"id": "2008.04214", "submitter": "John Lindner", "authors": "Scott T. Miller, John F. Lindner, Anshul Choudhary, Sudeshna Sinha,\n  William L. Ditto", "title": "Mastering high-dimensional dynamics with Hamiltonian neural networks", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We detail how incorporating physics into neural network design can\nsignificantly improve the learning and forecasting of dynamical systems, even\nnonlinear systems of many dimensions. A map building perspective elucidates the\nsuperiority of Hamiltonian neural networks over conventional neural networks.\nThe results clarify the critical relation between data, dimension, and neural\nnetwork learning performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:14:42 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Miller", "Scott T.", ""], ["Lindner", "John F.", ""], ["Choudhary", "Anshul", ""], ["Sinha", "Sudeshna", ""], ["Ditto", "William L.", ""]]}, {"id": "2008.04222", "submitter": "Bertrand Georgeot", "authors": "S. Bompas, B. Georgeot and D. Gu\\'ery-Odelin", "title": "Accuracy of neural networks for the simulation of chaotic dynamics:\n  precision of training data vs precision of the algorithm", "comments": "10 pages, 15 figures, additonal data provided, published version", "journal-ref": "Chaos 30, 113118 (2020)", "doi": "10.1063/5.0021264", "report-no": null, "categories": "cs.NE cs.LG nlin.CD physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the influence of precision of the data and the algorithm for the\nsimulation of chaotic dynamics by neural networks techniques. For this purpose,\nwe simulate the Lorenz system with different precisions using three different\nneural network techniques adapted to time series, namely reservoir computing\n(using ESN), LSTM and TCN, for both short and long time predictions, and assess\ntheir efficiency and accuracy. Our results show that the ESN network is better\nat predicting accurately the dynamics of the system, and that in all cases the\nprecision of the algorithm is more important than the precision of the training\ndata for the accuracy of the predictions. This result gives support to the idea\nthat neural networks can perform time-series predictions in many practical\napplications for which data are necessarily of limited precision, in line with\nrecent results. It also suggests that for a given set of data the reliability\nof the predictions can be significantly improved by using a network with higher\nprecision than the one of the data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:25:37 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 16:03:25 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bompas", "S.", ""], ["Georgeot", "B.", ""], ["Gu\u00e9ry-Odelin", "D.", ""]]}, {"id": "2008.04223", "submitter": "Guohua Wu", "authors": "Aijuan Song, Guohua Wu, Witold Pedrycz", "title": "Integrating Variable Reduction Strategy with Evolutionary Algorithm for\n  Solving Nonlinear Equations Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear equations systems (NESs) are widely used in real-world problems\nwhile they are also difficult to solve due to their characteristics of\nnonlinearity and multiple roots. Evolutionary algorithm (EA) is one of the\nmethods for solving NESs, given their global search capability and an ability\nto locate multiple roots of a NES simultaneously within one run. Currently, the\nmajority of research on using EAs to solve NESs focuses on transformation\ntechniques and improving the performance of the used EAs. By contrast, the\nproblem domain knowledge of NESs is particularly investigated in this study,\nusing which we propose to incorporate the variable reduction strategy (VRS)\ninto EAs to solve NESs. VRS makes full use of the systems of expressing a NES\nand uses some variables (i.e., core variable) to represent other variables\n(i.e., reduced variables) through the variable relationships existing in the\nequation systems. It enables to reduce partial variables and equations and\nshrink the decision space, thereby reducing the complexity of the problem and\nimproving the search efficiency of the EAs. To test the effectiveness of VRS in\ndealing with NESs, this paper integrates VRS into two existing state-of-the-art\nEA methods (i.e., MONES and DRJADE), respectively. Experimental results show\nthat, with the assistance of VRS, the EA methods can significantly produce\nbetter results than the original methods and other compared methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:58:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Song", "Aijuan", ""], ["Wu", "Guohua", ""], ["Pedrycz", "Witold", ""]]}, {"id": "2008.04224", "submitter": "Lucas Batista PhD", "authors": "Lucas S. Batista, Felipe Campelo, Frederico G. Guimar\\~aes and Jaime\n  A. Ram\\'irez", "title": "The Cone epsilon-Dominance: An Approach for Evolutionary Multiobjective\n  Optimization", "comments": "42 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": "Report-no: 2013/01", "categories": "cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose the cone epsilon-dominance approach to improve convergence and\ndiversity in multiobjective evolutionary algorithms (MOEAs). A cone-eps-MOEA is\npresented and compared with MOEAs based on the standard Pareto relation\n(NSGA-II, NSGA-II*, SPEA2, and a clustered NSGA-II) and on the\nepsilon-dominance (eps-MOEA). The comparison is performed both in terms of\ncomputational complexity and on four performance indicators selected to\nquantify the quality of the final results obtained by each algorithm: the\nconvergence, diversity, hypervolume, and coverage of many sets metrics. Sixteen\nwell-known benchmark problems are considered in the experimental section,\nincluding the ZDT and the DTLZ families. To evaluate the possible differences\namongst the algorithms, a carefully designed experiment is performed for the\nfour performance metrics. The results obtained suggest that the cone-eps-MOEA\nis capable of presenting an efficient and balanced performance over all the\nperformance metrics considered. These results strongly support the conclusion\nthat the cone-eps-MOEA is a competitive approach for obtaining an efficient\nbalance between convergence and diversity to the Pareto front, and as such\nrepresents a useful tool for the solution of multiobjective optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:13:13 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Batista", "Lucas S.", ""], ["Campelo", "Felipe", ""], ["Guimar\u00e3es", "Frederico G.", ""], ["Ram\u00edrez", "Jaime A.", ""]]}, {"id": "2008.04245", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mahmoud Famouri, Maya Pavlova, and Siddharth Surana", "title": "TinySpeech: Attention Condensers for Deep Speech Recognition Neural\n  Networks on Edge Devices", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning have led to state-of-the-art performance across a\nmultitude of speech recognition tasks. Nevertheless, the widespread deployment\nof deep neural networks for on-device speech recognition remains a challenge,\nparticularly in edge scenarios where the memory and computing resources are\nhighly constrained (e.g., low-power embedded devices) or where the memory and\ncomputing budget dedicated to speech recognition is low (e.g., mobile devices\nperforming numerous tasks besides speech recognition). In this study, we\nintroduce the concept of attention condensers for building low-footprint,\nhighly-efficient deep neural networks for on-device speech recognition on the\nedge. An attention condenser is a self-attention mechanism that learns and\nproduces a condensed embedding characterizing joint local and cross-channel\nactivation relationships, and performs selective attention accordingly. To\nillustrate its efficacy, we introduce TinySpeech, low-precision deep neural\nnetworks comprising largely of attention condensers tailored for on-device\nspeech recognition using a machine-driven design exploration strategy, with one\ntailored specifically with microcontroller operation constraints. Experimental\nresults on the Google Speech Commands benchmark dataset for limited-vocabulary\nspeech recognition showed that TinySpeech networks achieved significantly lower\narchitectural complexity (as much as $507\\times$ fewer parameters), lower\ncomputational complexity (as much as $48\\times$ fewer multiply-add operations),\nand lower storage requirements (as much as $2028\\times$ lower weight memory\nrequirements) when compared to previous work. These results not only\ndemonstrate the efficacy of attention condensers for building highly efficient\nnetworks for on-device speech recognition, but also illuminate its potential\nfor accelerating deep learning on the edge and empowering TinyML applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:34:52 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 01:37:04 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 14:46:10 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 00:28:33 GMT"}, {"version": "v5", "created": "Tue, 22 Sep 2020 04:07:50 GMT"}, {"version": "v6", "created": "Mon, 12 Oct 2020 19:07:39 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wong", "Alexander", ""], ["Famouri", "Mahmoud", ""], ["Pavlova", "Maya", ""], ["Surana", "Siddharth", ""]]}, {"id": "2008.04415", "submitter": "David Daniel Albarrac\\'in Molina", "authors": "David Daniel Albarracin Molina", "title": "Adaptive music: Automated music composition and distribution", "comments": "211 pages, 50 figures, 17 tables, Technical Report Universidad de\n  Malaga (https://riuma.uma.es/xmlui/handle/10630/19581)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CY cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creativity, or the ability to produce new useful ideas, is commonly\nassociated to the human being, but there are many other examples in nature\nwhere this phenomenon can be observed. Inspired by this fact, in engineering\nand particularly in computational sciences, many different models have been\ndeveloped to tackle a number of problems. Music, a form of art broadly present\nalong the human history, is the main field addressed in this thesis, taking\nadvantage of the kind of ideas that bring diversity and creativity to nature\nand computation. We present Melomics, an algorithmic composition method based\non evolutionary search, with a genetic encoding of the solutions that are\ninterpreted in a complex developmental process that leads to music in standard\nformats. This bioinspired compositional system has exhibited a highly creative\npower and versatility to produce music of different type, which in many\noccasions has proven to be indistinguishable from the music made by human\ncomposers. The system also has enabled the emergence of a set of completely\nnovel applications: from effective tools that help anyone to easily obtain the\nprecise music they need, to radically new uses like adaptive music for therapy,\namusement or many other purposes. It is clear to us that there is yet much\nresearch to be done in this field and that countless and new unimaginable uses\nwill derive from it.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 09:38:06 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Molina", "David Daniel Albarracin", ""]]}, {"id": "2008.04470", "submitter": "M. Umut Isik", "authors": "Umut Isik, Ritwik Giri, Neerad Phansalkar, Jean-Marc Valin, Karim\n  Helwani, Arvindh Krishnaswamy", "title": "PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,\n  Semi-Supervised Conversational Data, and Biased Loss", "comments": "5 pages, 3 figures, INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network applications generally benefit from larger-sized models, but\nfor current speech enhancement models, larger scale networks often suffer from\ndecreased robustness to the variety of real-world use cases beyond what is\nencountered in training data. We introduce several innovations that lead to\nbetter large neural networks for speech enhancement. The novel PoCoNet\narchitecture is a convolutional neural network that, with the use of\nfrequency-positional embeddings, is able to more efficiently build\nfrequency-dependent features in the early layers. A semi-supervised method\nhelps increase the amount of conversational training data by pre-enhancing\nnoisy datasets, improving performance on real recordings. A new loss function\nbiased towards preserving speech quality helps the optimization better match\nhuman perceptual opinions on speech quality. Ablation experiments and objective\nand human opinion metrics show the benefits of the proposed improvements.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 01:24:45 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Isik", "Umut", ""], ["Giri", "Ritwik", ""], ["Phansalkar", "Neerad", ""], ["Valin", "Jean-Marc", ""], ["Helwani", "Karim", ""], ["Krishnaswamy", "Arvindh", ""]]}, {"id": "2008.04589", "submitter": "Daniel Tanneberg", "authors": "Leon Keller, Daniel Tanneberg, Svenja Stark, Jan Peters", "title": "Model-Based Quality-Diversity Search for Efficient Robot Learning", "comments": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in robot learning, it still remains a challenge to\nprogram a robot to deal with open-ended object manipulation tasks. One approach\nthat was recently used to autonomously generate a repertoire of diverse skills\nis a novelty based Quality-Diversity~(QD) algorithm. However, as most\nevolutionary algorithms, QD suffers from sample-inefficiency and, thus, it is\nchallenging to apply it in real-world scenarios. This paper tackles this\nproblem by integrating a neural network that predicts the behavior of the\nperturbed parameters into a novelty based QD algorithm. In the proposed\nModel-based Quality-Diversity search (M-QD), the network is trained\nconcurrently to the repertoire and is used to avoid executing unpromising\nactions in the novelty search process. Furthermore, it is used to adapt the\nskills of the final repertoire in order to generalize the skills to different\nscenarios. Our experiments show that enhancing a QD algorithm with such a\nforward model improves the sample-efficiency and performance of the\nevolutionary process and the skill adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 09:02:18 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Keller", "Leon", ""], ["Tanneberg", "Daniel", ""], ["Stark", "Svenja", ""], ["Peters", "Jan", ""]]}, {"id": "2008.04703", "submitter": "Amir Mosavi Prof", "authors": "Ali Sahragard, Hamid Falaghi, Mahdi Farhadi, Amir Mosavi, Abouzar\n  Estebsari", "title": "Generation expansion planning in the presence of wind power plants using\n  a genetic algorithm model", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the essential aspects of power system planning is generation expansion\nplanning (GEP). The purpose of GEP is to enhance construction planning and\nreduce the costs of installing different types of power plants. This paper\nproposes a method based on Genetic Algorithm (GA) for GEP in the presence of\nwind power plants. Since it is desired to integrate the maximum possible wind\npower production in GEP, the constraints for incorporating different levels of\nwind energy in power generation are investigated comprehensively. This will\nallow obtaining the maximum reasonable amount of wind penetration in the\nnetwork. Besides, due to the existence of different wind regimes, the\npenetration of strong and weak wind on GEP is assessed. The results show that\nthe maximum utilization of wind power generation capacity could increase the\nexploitation of more robust wind regimes. Considering the growth of the wind\nfarm industry and the cost reduction for building wind power plants, the\nsensitivity of GEP to the variations of this cost is investigated. The results\nfurther indicate that for a 10% reduction in the initial investment cost of\nwind power plants, the proposed model estimates that the overall cost will be\nminimized.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:20:15 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Sahragard", "Ali", ""], ["Falaghi", "Hamid", ""], ["Farhadi", "Mahdi", ""], ["Mosavi", "Amir", ""], ["Estebsari", "Abouzar", ""]]}, {"id": "2008.04978", "submitter": "Victoria Webster-Wood", "authors": "Victoria A. Webster-Wood, Jeffrey P. Gill, Peter J. Thomas, Hillel J.\n  Chiel", "title": "Control for Multifunctionality: Bioinspired Control Based on Feeding in\n  Aplysia californica", "comments": "Revisions have been made to improve manuscript clarity and expand the\n  introduction and discussion. The results are unchanged", "journal-ref": "Biol Cybern (2020)", "doi": "10.1007/s00422-020-00851-9", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animals exhibit remarkable feats of behavioral flexibility and\nmultifunctional control that remain challenging for robotic systems. The neural\nand morphological basis of multifunctionality in animals can provide a source\nof bio-inspiration for robotic controllers. However, many existing approaches\nto modeling biological neural networks rely on computationally expensive models\nand tend to focus solely on the nervous system, often neglecting the\nbiomechanics of the periphery. As a consequence, while these models are\nexcellent tools for neuroscience, they fail to predict functional behavior in\nreal time, which is a critical capability for robotic control. To meet the need\nfor real-time multifunctional control, we have developed a hybrid Boolean model\nframework capable of modeling neural bursting activity and simple biomechanics\nat speeds faster than real time. Using this approach, we present a\nmultifunctional model of Aplysia californica feeding that qualitatively\nreproduces three key feeding behaviors (biting, swallowing, and rejection),\ndemonstrates behavioral switching in response to external sensory cues, and\nincorporates both known neural connectivity and a simple bioinspired mechanical\nmodel of the feeding apparatus. We demonstrate that the model can be used for\nformulating testable hypotheses and discuss the implications of this approach\nfor robotic control and neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 19:26:50 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 14:25:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Webster-Wood", "Victoria A.", ""], ["Gill", "Jeffrey P.", ""], ["Thomas", "Peter J.", ""], ["Chiel", "Hillel J.", ""]]}, {"id": "2008.05575", "submitter": "Swayamjit Saha", "authors": "Swayamjit Saha, Niladri Majumder and Devansh Sangani", "title": "Comprehensive forecasting based analysis using stacked stateless and\n  stateful Gated Recurrent Unit models", "comments": "12 pages, 2 figures, 6 tables; typos corrected, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Photovoltaic power is a renewable source of energy which is highly used in\nindustries. In economically struggling countries it can be a potential source\nof electric energy as other non-renewable resources are already exhausting. Now\nif installation of a photovoltaic cell in a region is done prior to research,\nit may not provide the desired energy output required for running that region.\nHence forecasting is required which can elicit the output from a particular\nregion considering its geometrical coordinates, solar parameter like GHI and\nweather parameters like temperature and wind speed etc. Our paper explores\nforecasting of solar irradiance on four such regions, out of which three is in\nWest Bengal and one outside to depict with using stacked Gated Recurrent Unit\n(GRU) models. We have checked that stateful stacked gated recurrent unit model\nimproves the prediction accuracy significantly.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 21:13:16 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 15:19:53 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Saha", "Swayamjit", ""], ["Majumder", "Niladri", ""], ["Sangani", "Devansh", ""]]}, {"id": "2008.05695", "submitter": "Jianzong Wang", "authors": "Xiaoyang Qu, Jianzong Wang, Jing Xiao", "title": "Evolutionary Algorithm Enhanced Neural Architecture Search for\n  Text-Independent Speaker Verification", "comments": "will be presented in INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art speaker verification models are based on deep learning\ntechniques, which heavily depend on the handdesigned neural architectures from\nexperts or engineers. We borrow the idea of neural architecture search(NAS) for\nthe textindependent speaker verification task. As NAS can learn deep network\nstructures automatically, we introduce the NAS conception into the well-known\nx-vector network. Furthermore, this paper proposes an evolutionary algorithm\nenhanced neural architecture search method called Auto-Vector to automatically\ndiscover promising networks for the speaker verification task. The experimental\nresults demonstrate our NAS-based model outperforms state-of-the-art speaker\nverification models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 05:34:52 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Qu", "Xiaoyang", ""], ["Wang", "Jianzong", ""], ["Xiao", "Jing", ""]]}, {"id": "2008.06135", "submitter": "Hongmei He Ph.D", "authors": "Hongmei He, Mengyuan Chen, Gang Xu, Zhilong Zhu, Zhenhuan Zhu", "title": "Learnability and Robustness of Shallow Neural Networks Learned With a\n  Performance-Driven BP and a Variant PSO For Edge Decision-Making", "comments": "36 pages, 21 figues for corresponding eps files. Neural Comput &\n  Applic (2021)", "journal-ref": null, "doi": "10.1007/s00521-021-06019-1", "report-no": null, "categories": "cs.NE cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, the computing resources are limited without the benefit from\nGPU, especially in the edge devices of IoT enabled systems. It may not be easy\nto implement complex AI models in edge devices. The Universal Approximation\nTheorem states that a shallow neural network (SNN) can represent any nonlinear\nfunction. However, how fat is an SNN enough to solve a nonlinear\ndecision-making problem in edge devices? In this paper, we focus on the\nlearnability and robustness of SNNs, obtained by a greedy tight force heuristic\nalgorithm (performance driven BP) and a loose force meta-heuristic algorithm (a\nvariant of PSO). Two groups of experiments are conducted to examine the\nlearnability and the robustness of SNNs with Sigmoid activation,\nlearned/optimised by KPI-PDBPs and KPI-VPSOs, where, KPIs (key performance\nindicators: error (ERR), accuracy (ACC) and $F_1$ score) are the objectives,\ndriving the searching process. An incremental approach is applied to examine\nthe impact of hidden neuron numbers on the performance of SNNs,\nlearned/optimised by KPI-PDBPs and KPI-VPSOs. From the engineering prospective,\nall sensors are well justified for a specific task. Hence, all sensor readings\nshould be strongly correlated to the target. Therefore, the structure of an SNN\nshould depend on the dimensions of a problem space. The experimental results\nshow that the number of hidden neurons up to the dimension number of a problem\nspace is enough; the learnability of SNNs, produced by KPI-PDBP, is better than\nthat of SNNs, optimized by KPI-VPSO, regarding the performance and learning\ntime on the training data sets; the robustness of SNNs learned by KPI-PDBPs and\nKPI-VPSOs depends on the data sets; and comparing with other classic machine\nlearning models, ACC-PDBPs win for almost all tested data sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:33:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["He", "Hongmei", ""], ["Chen", "Mengyuan", ""], ["Xu", "Gang", ""], ["Zhu", "Zhilong", ""], ["Zhu", "Zhenhuan", ""]]}, {"id": "2008.06249", "submitter": "Martin Zaefferer", "authors": "Martin Zaefferer and Frederik Rehbach", "title": "Continuous Optimization Benchmarks by Simulation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58112-1_19", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmark experiments are required to test, compare, tune, and understand\noptimization algorithms. Ideally, benchmark problems closely reflect real-world\nproblem behavior. Yet, real-world problems are not always readily available for\nbenchmarking. For example, evaluation costs may be too high, or resources are\nunavailable (e.g., software or equipment). As a solution, data from previous\nevaluations can be used to train surrogate models which are then used for\nbenchmarking. The goal is to generate test functions on which the performance\nof an algorithm is similar to that on the real-world objective function.\nHowever, predictions from data-driven models tend to be smoother than the\nground-truth from which the training data is derived. This is especially\nproblematic when the training data becomes sparse. The resulting benchmarks may\nnot reflect the landscape features of the ground-truth, are too easy, and may\nlead to biased conclusions. To resolve this, we use simulation of Gaussian\nprocesses instead of estimation (or prediction). This retains the covariance\nproperties estimated during model training. While previous research suggested a\ndecomposition-based approach for a small-scale, discrete problem, we show that\nthe spectral simulation method enables simulation for continuous optimization\nproblems. In a set of experiments with an artificial ground-truth, we\ndemonstrate that this yields more accurate benchmarks than simply predicting\nwith the Gaussian process model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 08:50:57 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zaefferer", "Martin", ""], ["Rehbach", "Frederik", ""]]}, {"id": "2008.06348", "submitter": "Andrew Flynn Mr", "authors": "Andrew Flynn, Vassilios A. Tsachouridis, and Andreas Amann", "title": "Multifunctionality in a Reservoir Computer", "comments": null, "journal-ref": null, "doi": "10.1063/5.0019974", "report-no": null, "categories": "cs.NE cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifunctionality is a well observed phenomenological feature of biological\nneural networks and considered to be of fundamental importance to the survival\nof certain species over time. These multifunctional neural networks are capable\nof performing more than one task without changing any network connections. In\nthis paper we investigate how this neurological idiosyncrasy can be achieved in\nan artificial setting with a modern machine learning paradigm known as\n`Reservoir Computing'. A training technique is designed to enable a Reservoir\nComputer to perform tasks of a multifunctional nature. We explore the critical\neffects that changes in certain parameters can have on the Reservoir Computers'\nability to express multifunctionality. We also expose the existence of several\n`untrained attractors'; attractors which dwell within the prediction state\nspace of the Reservoir Computer that were not part of the training. We conduct\na bifurcation analysis of these untrained attractors and discuss the\nimplications of our results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 21:05:53 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 12:31:33 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Flynn", "Andrew", ""], ["Tsachouridis", "Vassilios A.", ""], ["Amann", "Andreas", ""]]}, {"id": "2008.06395", "submitter": "Francesco Mannella", "authors": "Francesco Mannella", "title": "Supervised Topological Maps", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling the internal representation space of a neural network is a\ndesirable feature because it allows to generate new data in a supervised\nmanner. In this paper we will show how this can be achieved while building a\nlow-dimensional mapping of the input stream, by deriving a generalized\nalgorithm starting from Self Organizing Maps (SOMs). SOMs are a kind of neural\nnetwork which can be trained with unsupervised learning to produce a\nlow-dimensional discretized mapping of the input space. They can be used for\nthe generation of new data through backward propagation of interpolations made\nfrom the mapping grid. Unfortunately the final topology of the mapping space of\na SOM is not known before learning, so interpolating new data in a supervised\nway is not an easy task. Here we will show a variation from the SOM algorithm\nconsisting in constraining the update of prototypes so that it is also a\nfunction of the distance of its prototypes from extrinsically given targets in\nthe mapping space. We will demonstrate how such variants, that we will call\nSupervised Topological Maps (STMs), allow for a supervised mapping where the\nposition of internal representations in the mapping space is determined by the\nexperimenter. Controlling the internal representation space in STMs reveals to\nbe an easier task than what is currently done using other algorithms such as\nvariational or adversarial autoencoders.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 14:30:16 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 21:50:10 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 09:30:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mannella", "Francesco", ""]]}, {"id": "2008.06634", "submitter": "Yuqiao Liu", "authors": "Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang", "title": "Evolving Deep Convolutional Neural Networks for Hyperspectral Image\n  Denoising", "comments": "8 pages, 4 figures, to be published in IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSIs) are susceptible to various noise factors leading\nto the loss of information, and the noise restricts the subsequent HSIs object\ndetection and classification tasks. In recent years, learning-based methods\nhave demonstrated their superior strengths in denoising the HSIs.\nUnfortunately, most of the methods are manually designed based on the extensive\nexpertise that is not necessarily available to the users interested. In this\npaper, we propose a novel algorithm to automatically build an optimal\nConvolutional Neural Network (CNN) to effectively denoise HSIs. Particularly,\nthe proposed algorithm focuses on the architectures and the initialization of\nthe connection weights of the CNN. The experiments of the proposed algorithm\nhave been well-designed and compared against the state-of-the-art peer\ncompetitors, and the experimental results demonstrate the competitive\nperformance of the proposed algorithm in terms of the different evaluation\nmetrics, visual assessments, and the computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 03:04:11 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Yuqiao", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "2008.06643", "submitter": "Stephen Whitelam", "authors": "Stephen Whitelam, Viktor Selin, Sang-Won Park, Isaac Tamblyn", "title": "Correspondence between neuroevolution and gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show analytically that training a neural network by stochastic mutation or\n\"neuroevolution\" of its weights is equivalent, in the limit of small mutations,\nto gradient descent on the loss function in the presence of Gaussian white\nnoise. Averaged over independent realizations of the learning process,\nneuroevolution is equivalent to gradient descent on the loss function. We use\nnumerical simulation to show that this correspondence can be observed for\nfinite mutations, for shallow and deep neural networks. Our results provide a\nconnection between two distinct types of neural-network training, and provide\njustification for the empirical success of neuroevolution.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 03:53:53 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 16:43:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Whitelam", "Stephen", ""], ["Selin", "Viktor", ""], ["Park", "Sang-Won", ""], ["Tamblyn", "Isaac", ""]]}, {"id": "2008.06727", "submitter": "Md Mostafizur Rahman Komol", "authors": "Md Mostafizur Rahman Komol, Mohammed Elhenawy, Shamsunnahar Yasmin,\n  Mahmoud Masoud and Andry Rakotonirainy", "title": "A Review on Drivers Red Light Running and Turning Behaviour Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drivers behaviour prediction has been an unceasing concern for transportation\nsafety divisions all over the world. A massive amount of lives and properties\nlosses due to the adversities at intersections and pedestrian crossings.\nEspecially for countries with poor road safety technologies, this toll knows no\nbounds. A myriad of research and studies have been mastered for technological\nevaluation and model representation over this issue. Instead, little\ncomprehensive review has been made on the drivers behaviour prediction at\nsignalised intersections on red-light running and turning. This Paper aims at\nincorporating previous researches on drivers behaviour prediction and the\nprediction parameters leading to traffic violation like red-light running and\nturning at intersection and pedestrian crossing. The review also covers the\nprobable crash scenarios by red-light running and turning and analyses the\ninnovation of counter-crash technologies with future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 14:57:43 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Komol", "Md Mostafizur Rahman", ""], ["Elhenawy", "Mohammed", ""], ["Yasmin", "Shamsunnahar", ""], ["Masoud", "Mahmoud", ""], ["Rakotonirainy", "Andry", ""]]}, {"id": "2008.06885", "submitter": "Tsuyoshi Kato", "authors": "Takahiko Henmi, Esmeraldo Ronnie Rey Zara, Yoshihiro Hirohashi,\n  Tsuyoshi Kato", "title": "Adaptive Signal Variances: CNN Initialization Through Modern\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have achieved the unwavering\nconfidence in its performance on image processing tasks. The CNN architecture\nconstitutes a variety of different types of layers including the convolution\nlayer and the max-pooling layer. CNN practitioners widely understand the fact\nthat the stability of learning depends on how to initialize the model\nparameters in each layer. Nowadays, no one doubts that the de facto standard\nscheme for initialization is the so-called Kaiming initialization that has been\ndeveloped by He et al. The Kaiming scheme was derived from a much simpler model\nthan the currently used CNN structure having evolved since the emergence of the\nKaiming scheme. The Kaiming model consists only of the convolution and fully\nconnected layers, ignoring the max-pooling layer and the global average pooling\nlayer. In this study, we derived the initialization scheme again not from the\nsimplified Kaiming model, but precisely from the modern CNN architectures, and\nempirically investigated how the new initialization method performs compared to\nthe de facto standard ones that are widely used today.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:26:29 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 06:11:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Henmi", "Takahiko", ""], ["Zara", "Esmeraldo Ronnie Rey", ""], ["Hirohashi", "Yoshihiro", ""], ["Kato", "Tsuyoshi", ""]]}, {"id": "2008.06937", "submitter": "Brian Gardner BG", "authors": "Brian Gardner, Andr\\'e Gr\\\"uning", "title": "Supervised Learning with First-to-Spike Decoding in Multilayer Spiking\n  Neural Networks", "comments": "41 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental studies support the notion of spike-based neuronal information\nprocessing in the brain, with neural circuits exhibiting a wide range of\ntemporally-based coding strategies to rapidly and efficiently represent sensory\nstimuli. Accordingly, it would be desirable to apply spike-based computation to\ntackling real-world challenges, and in particular transferring such theory to\nneuromorphic systems for low-power embedded applications. Motivated by this, we\npropose a new supervised learning method that can train multilayer spiking\nneural networks to solve classification problems based on a rapid,\nfirst-to-spike decoding strategy. The proposed learning rule supports multiple\nspikes fired by stochastic hidden neurons, and yet is stable by relying on\nfirst-spike responses generated by a deterministic output layer. In addition to\nthis, we also explore several distinct, spike-based encoding strategies in\norder to form compact representations of presented input data. We demonstrate\nthe classification performance of the learning rule as applied to several\nbenchmark datasets, including MNIST. The learning rule is capable of\ngeneralising from the data, and is successful even when used with constrained\nnetwork architectures containing few input and hidden layer neurons.\nFurthermore, we highlight a novel encoding strategy, termed `scanline\nencoding', that can transform image data into compact spatiotemporal patterns\nfor subsequent network processing. Designing constrained, but optimised,\nnetwork structures and performing input dimensionality reduction has strong\nimplications for neuromorphic applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 15:34:48 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gardner", "Brian", ""], ["Gr\u00fcning", "Andr\u00e9", ""]]}, {"id": "2008.06986", "submitter": "Subrata Goswami", "authors": "Subrata Goswami", "title": "False Detection (Positives and Negatives) in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a very important function of visual perception systems.\nSince the early days of classical object detection based on HOG to modern deep\nlearning based detectors, object detection has improved in accuracy. Two stage\ndetectors usually have higher accuracy than single stage ones. Both types of\ndetectors use some form of quantization of the search space of rectangular\nregions of image. There are far more of the quantized elements than true\nobjects. The way these bounding boxes are filtered out possibly results in the\nfalse positive and false negatives. This empirical experimental study explores\nways of reducing false positives and negatives with labelled data.. In the\nprocess also discovered insufficient labelling in Openimage 2019 Object\nDetection dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:09:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Goswami", "Subrata", ""]]}, {"id": "2008.07094", "submitter": "Lie Meng Pang", "authors": "Lie Meng Pang, Hisao Ishibuchi and Ke Shang", "title": "Decomposition-Based Multi-Objective Evolutionary Algorithm Design under\n  Two Algorithm Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of efficient and effective evolutionary multi-objective\noptimization (EMO) algorithms has been an active research topic in the\nevolutionary computation community. Over the years, many EMO algorithms have\nbeen proposed. The existing EMO algorithms are mainly developed based on the\nfinal population framework. In the final population framework, the final\npopulation of an EMO algorithm is presented to the decision maker. Thus, it is\nrequired that the final population produced by an EMO algorithm is a good\nsolution set. Recently, the use of solution selection framework was suggested\nfor the design of EMO algorithms. This framework has an unbounded external\narchive to store all the examined solutions. A pre-specified number of\nsolutions are selected from the archive as the final solutions presented to the\ndecision maker. When the solution selection framework is used, EMO algorithms\ncan be designed in a more flexible manner since the final population is not\nnecessarily to be a good solution set. In this paper, we examine the design of\nMOEA/D under these two frameworks. We use an offline genetic algorithm-based\nhyper-heuristic method to find the optimal configuration of MOEA/D in each\nframework. The DTLZ and WFG test suites and their minus versions are used in\nour experiments. The experimental results suggest the possibility that a more\nflexible, robust and high-performance MOEA/D algorithm can be obtained when the\nsolution selection framework is used.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 05:28:20 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Pang", "Lie Meng", ""], ["Ishibuchi", "Hisao", ""], ["Shang", "Ke", ""]]}, {"id": "2008.07127", "submitter": "Francesco Conti", "authors": "Alessio Burrello, Angelo Garofalo, Nazareno Bruschi, Giuseppe\n  Tagliavini, Davide Rossi, Francesco Conti", "title": "DORY: Automatic End-to-End Deployment of Real-World DNNs on Low-Cost IoT\n  MCUs", "comments": "14 pages, 12 figures, 4 tables, 2 listings. Accepted for publication\n  in IEEE Transactions on Computers\n  (https://ieeexplore.ieee.org/document/9381618)", "journal-ref": null, "doi": "10.1109/TC.2021.3066883", "report-no": null, "categories": "cs.DC cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme\nedge of the Internet-of-Things is a critical enabler to support pervasive Deep\nLearning-enhanced applications. Low-Cost MCU-based end-nodes have limited\non-chip memory and often replace caches with scratchpads, to reduce area\noverheads and increase energy efficiency -- requiring explicit DMA-based memory\ntransfers between different levels of the memory hierarchy. Mapping modern DNNs\non these systems requires aggressive topology-dependent tiling and\ndouble-buffering. In this work, we propose DORY (Deployment Oriented to memoRY)\n- an automatic tool to deploy DNNs on low cost MCUs with typically less than\n1MB of on-chip SRAM memory. DORY abstracts tiling as a Constraint Programming\n(CP) problem: it maximizes L1 memory utilization under the topological\nconstraints imposed by each DNN layer. Then, it generates ANSI C code to\norchestrate off- and on-chip transfers and computation phases. Furthermore, to\nmaximize speed, DORY augments the CP formulation with heuristics promoting\nperformance-effective tile sizes. As a case study for DORY, we target\nGreenWaves Technologies GAP8, one of the most advanced parallel ultra-low power\nMCU-class devices on the market. On this device, DORY achieves up to 2.5x\nbetter MAC/cycle than the GreenWaves proprietary software solution and 18.1x\nbetter than the state-of-the-art result on an STM32-F746 MCU on single layers.\nUsing our tool, GAP-8 can perform end-to-end inference of a 1.0-MobileNet-128\nnetwork consuming just 63 pJ/MAC on average @ 4.3 fps - 15.4x better than an\nSTM32-F746. We release all our developments - the DORY framework, the optimized\nbackend kernels, and the related heuristics - as open-source software.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 07:30:54 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 15:11:36 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 15:08:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Burrello", "Alessio", ""], ["Garofalo", "Angelo", ""], ["Bruschi", "Nazareno", ""], ["Tagliavini", "Giuseppe", ""], ["Rossi", "Davide", ""], ["Conti", "Francesco", ""]]}, {"id": "2008.07277", "submitter": "Renlong Jie", "authors": "Renlong Jie, Junbin Gao, Andrey Vasnev and Minh-Ngoc Tran", "title": "Adaptive Hierarchical Hyper-gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate learning rate adaption at different levels\nbased on the hyper-gradient descent framework and propose a method that\nadaptively learns the optimizer parameters by combining multiple levels of\nlearning rates with hierarchical structures. Meanwhile, we show the\nrelationship between regularizing over-parameterized learning rates and\nbuilding combinations of adaptive learning rates at different levels. The\nexperiments on several network architectures, including feed-forward networks,\nLeNet-5 and ResNet-18/34, show that the proposed multi-level adaptive approach\ncan outperform baseline adaptive methods in a variety of circumstances.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:01:36 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 13:19:18 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 06:48:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Jie", "Renlong", ""], ["Gao", "Junbin", ""], ["Vasnev", "Andrey", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "2008.07397", "submitter": "Daniel Engelsman", "authors": "Daniel Engelsman", "title": "A Study of a Genetic Algorithm for Polydisperse Spray Flames", "comments": "Advisor : Prof. Barry J. Greenberg, 66 Pages, 65 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern technological advancements constantly push forward the human-machine\ninteraction. Evolutionary Algorithms (EA) are an machine learning (ML) subclass\ninspired by the process of natural selection - Survival of the Fittest, as\nstated by the Darwinian Theory of Evolution. The most notable algorithm in that\nclass is the Genetic Algorithm (GA) - a powerful heuristic tool which enables\nthe generation of a high-quality solutions to optimization problems. In recent\ndecades the algorithm underwent remarkable improvement, which adapted it into a\nwide range of engineering problems, by heuristically searching for the optimal\nsolution. Despite being well-defined, many engineering problems may suffer from\nheavy analytical entanglement when approaching the derivation process, as\nrequired in classic optimization methods. Therefore, the main motivation here,\nis to work around that obstacle. In this piece of work, I would like to harness\nthe GA capabilities to examine optimality with respect to a unique combustion\nproblem, in a way that was never performed before. To be more precise, I would\nlike to utilize it to answer the question : What form of an initial droplet\nsize distribution (iDSD) will guarantee an optimal flame ? To answer this\nquestion, I will first provide a general introduction to the GA method, then\ndevelop the combustion model, and eventually merge both into an optimization\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 10:17:42 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Engelsman", "Daniel", ""]]}, {"id": "2008.07491", "submitter": "Andrea Mazza", "authors": "Gianfranco Chicco and Andrea Mazza", "title": "Metaheuristic optimization of power and energy systems: underlying\n  principles and main issues of the 'rush to heuristics'", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the power and energy systems area, a progressive increase of literature\ncontributions containing applications of metaheuristic algorithms is occurring.\nIn many cases, these applications are merely aimed at proposing the testing of\nan existing metaheuristic algorithm on a specific problem, claiming that the\nproposed method is better than other methods based on weak comparisons. This\n'rush to heuristics' does not happen in the evolutionary computation domain,\nwhere the rules for setting up rigorous comparisons are stricter, but are\ntypical of the domains of application of the metaheuristics. This paper\nconsiders the applications to power and energy systems, and aims at providing a\ncomprehensive view of the main issues concerning the use of metaheuristics for\nglobal optimization problems. A set of underlying principles that characterize\nthe metaheuristic algorithms is presented. The customization of metaheuristic\nalgorithms to fit the constraints of specific problems is discussed. Some\nweaknesses and pitfalls found in literature contributions are identified, and\nspecific guidelines are provided on how to prepare sound contributions on the\napplication of metaheuristic algorithms to specific problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:33:51 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Chicco", "Gianfranco", ""], ["Mazza", "Andrea", ""]]}, {"id": "2008.08035", "submitter": "Hesham Rakha", "authors": "Seifeldeen Eteifa, Hesham A. Rakha, Hoda Eldardiry", "title": "Predicting Coordinated Actuated Traffic Signal Change Times using LSTM\n  Neural Networks", "comments": "Paper submitted to Transportation Research Board Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle acceleration and deceleration maneuvers at traffic signals results in\nsignificant fuel and energy consumption levels. Green light optimal speed\nadvisory systems require reliable estimates of signal switching times to\nimprove vehicle fuel efficiency. Obtaining these estimates is difficult for\nactuated signals where the length of each green indication changes to\naccommodate varying traffic conditions. This study details a four-step Long\nShort-Term Memory deep learning-based methodology that can be used to provide\nreasonable switching time estimates from green to red and vice versa while\nbeing robust to missing data. The four steps are data gathering, data\npreparation, machine learning model tuning, and model testing and evaluation.\nThe input to the models included controller logic, signal timing parameters,\ntime of day, traffic state from detectors, vehicle actuation data, and\npedestrian actuation data. The methodology is applied and evaluated on data\nfrom an intersection in Northern Virginia. A comparative analysis is conducted\nbetween different loss functions including the mean squared error, mean\nabsolute error, and mean relative error used in LSTM and a new loss function is\nproposed. The results show that while the proposed loss function outperforms\nconventional loss functions in terms of overall absolute error values, the\nchoice of the loss function is dependent on the prediction horizon. In\nparticular, the proposed loss function is outperformed by the mean relative\nerror for very short prediction horizons and mean squared error for very long\nprediction horizons.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 15:11:21 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Eteifa", "Seifeldeen", ""], ["Rakha", "Hesham A.", ""], ["Eldardiry", "Hoda", ""]]}, {"id": "2008.08072", "submitter": "Michael S. Ryoo", "authors": "Michael S. Ryoo, AJ Piergiovanni, Juhana Kangaspunta, Anelia Angelova", "title": "AssembleNet++: Assembling Modality Representations via Attention\n  Connections", "comments": "ECCV 2020 camera-ready version", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a family of powerful video models which are able to: (i) learn\ninteractions between semantic object information and raw appearance and motion\nfeatures, and (ii) deploy attention in order to better learn the importance of\nfeatures at each convolutional block of the network. A new network component\nnamed peer-attention is introduced, which dynamically learns the attention\nweights using another block or input modality. Even without pre-training, our\nmodels outperform the previous work on standard public activity recognition\ndatasets with continuous videos, establishing new state-of-the-art. We also\nconfirm that our findings of having neural connections from the object modality\nand the use of peer-attention is generally applicable for different existing\narchitectures, improving their performances. We name our model explicitly as\nAssembleNet++. The code will be available at:\nhttps://sites.google.com/corp/view/assemblenet/\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:54:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ryoo", "Michael S.", ""], ["Piergiovanni", "AJ", ""], ["Kangaspunta", "Juhana", ""], ["Angelova", "Anelia", ""]]}, {"id": "2008.08073", "submitter": "Jerome Feldman", "authors": "Jerome A. Feldman (ICSI and UC Berkeley)", "title": "On the Evolution of Subjective Experience", "comments": "49 pages 5 figures. This 7/22/2021 version preserves all the content\n  of the previous version and adds additional discussion (in italics). It also\n  includes several new references to connect with current literature. A\n  companion arXiv article has also been updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective Experience (SE) is part of the ancient mind-body problem, which\ncontinues to be one of deepest mysteries of science. Despite major advances in\nmany fields, there is still no plausible causal link between SE and its\nrealization in the body. The core issue is the incompatibility of objective\n(3rd person) public science with subjective (1st person) private experience.\nAny scientific approach to SE assumes that it arose from extended evolutionary\nprocesses and that examining evolutionary history should help us understand it.\nWhile the core mystery remains, converging evidence from theoretical,\nexperimental, and computational studies yields strong constraints on SE and\nsome suggestions for further research. All animals confront many of the same\nfitness challenges. They all need some kind of internal model to relate their\nlife goals and actionable sensed information to action. We understand the\nevolution of the bodily aspects of human perception and emotion, but not the\nSE. The first evolutionary evidence for SE appears in vertebrates and much of\nits neural substrate and simulation mechanism is preserved in mammals and\nhumans. People exhibit the same phenomena, but there are remaining mysteries of\neveryday experience that are demonstrably incompatible with current\nneuroscience. In spite of this limitation, there is considerable progress on\nunderstanding the role of SE in the success of prostheses.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:54:39 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 17:41:53 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Feldman", "Jerome A.", "", "ICSI and UC Berkeley"]]}, {"id": "2008.08750", "submitter": "Sayed Kamaledin Ghiasi-Shirazi", "authors": "Ramin Zarei Sabzevar, Kamaledin Ghiasi-Shirazi, Ahad Harati", "title": "Prototype-based interpretation of the functionality of neurons in\n  winner-take-all neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototype-based learning (PbL) using a winner-take-all (WTA) network based on\nminimum Euclidean distance (ED-WTA) is an intuitive approach to multiclass\nclassification. By constructing meaningful class centers, PbL provides higher\ninterpretability and generalization than hyperplane-based learning (HbL)\nmethods based on maximum Inner Product (IP-WTA) and can efficiently detect and\nreject samples that do not belong to any classes. In this paper, we first prove\nthe equivalence of IP-WTA and ED-WTA from a representational point of view.\nThen, we show that naively using this equivalence leads to unintuitive ED-WTA\nnetworks in which the centers have high distances to data that they represent.\nWe propose $\\pm$ED-WTA which models each neuron with two prototypes: one\npositive prototype representing samples that are modeled by this neuron and a\nnegative prototype representing the samples that are erroneously won by that\nneuron during training. We propose a novel training algorithm for the\n$\\pm$ED-WTA network, which cleverly switches between updating the positive and\nnegative prototypes and is essential to the emergence of interpretable\nprototypes. Unexpectedly, we observed that the negative prototype of each\nneuron is indistinguishably similar to the positive one. The rationale behind\nthis observation is that the training data that are mistaken with a prototype\nare indeed similar to it. The main finding of this paper is this interpretation\nof the functionality of neurons as computing the difference between the\ndistances to a positive and a negative prototype, which is in agreement with\nthe BCM theory. In our experiments, we show that the proposed $\\pm$ED-WTA\nmethod constructs highly interpretable prototypes that can be successfully used\nfor detecting outlier and adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 03:15:37 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Sabzevar", "Ramin Zarei", ""], ["Ghiasi-Shirazi", "Kamaledin", ""], ["Harati", "Ahad", ""]]}, {"id": "2008.09017", "submitter": "Vijay Mago", "authors": "Mekaal Swerhun, Jasmine Foley, Brandon Massop and Vijay Mago", "title": "A summary of the prevalence of Genetic Algorithms in Bioinformatics from\n  2015 onwards", "comments": "20 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning has seen an increasing presencein a large\nvariety of fields, especially in health care and bioinformatics.More\nspecifically, the field where machine learning algorithms have found most\napplications is Genetic Algorithms.The objective of this paper is to conduct a\nsurvey of articles published from 2015 onwards that deal with Genetic\nAlgorithms(GA) and how they are used in bioinformatics.To achieve the\nobjective, a scoping review was conducted that utilized Google Scholar\nalongside Publish or Perish and the Scimago Journal & CountryRank to search for\nrespectable sources. Upon analyzing 31 articles from the field of\nbioinformatics, it became apparent that genetic algorithms rarely form a full\napplication, instead they rely on other vital algorithms such as support vector\nmachines.Indeed, support vector machines were the most prevalent algorithms\nused alongside genetic algorithms; however, while the usage of such algorithms\ncontributes to the heavy focus on accuracy by GA programs, it often sidelines\ncomputation times in the process. In fact, most applications employing GAs for\nclassification and feature selectionare nearing or at 100% success rate, and\nthe focus of future GA development should be directed elsewhere.\nPopulation-based searches, like GA, are often combined with other machine\nlearning algorithms. In this scoping review, genetic algorithms combined with\nSupport Vector Machines were found to perform best. The performance metric that\nwas evaluated most often was accuracy. Measuring the accuracy avoids measuring\nthe main weakness of GAs, which is computational time. The future of genetic\nalgorithms could be open-ended evolutionary algorithms, which attempt to\nincrease complexity and find diverse solutions, rather than optimize a fitness\nfunction and converge to a single best solution from the initial population of\nsolutions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 15:15:43 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Swerhun", "Mekaal", ""], ["Foley", "Jasmine", ""], ["Massop", "Brandon", ""], ["Mago", "Vijay", ""]]}, {"id": "2008.09206", "submitter": "Zheyuan Zhu", "authors": "Joseph Ulseth, Zheyuan Zhu, Guifang Li, Shuo Pang", "title": "Training of mixed-signal optical convolutional neural network with\n  reduced quantization level", "comments": "Manuscript prepared for submission to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3072193", "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-signal artificial neural networks (ANNs) that employ analog\nmatrix-multiplication accelerators can achieve higher speed and improved power\nefficiency. Though analog computing is known to be susceptible to noise and\ndevice imperfections, various analog computing paradigms have been considered\nas promising solutions to address the growing computing demand in machine\nlearning applications, thanks to the robustness of ANNs. This robustness has\nbeen explored in low-precision, fixed-point ANN models, which have proven\nsuccessful on compressing ANN model size on digital computers. However, these\npromising results and network training algorithms cannot be easily migrated to\nanalog accelerators. The reason is that digital computers typically carry\nintermediate results with higher bit width, though the inputs and weights of\neach ANN layers are of low bit width; while the analog intermediate results\nhave low precision, analogous to digital signals with a reduced quantization\nlevel. Here we report a training method for mixed-signal ANN with two types of\nerrors in its analog signals, random noise, and deterministic errors\n(distortions). The results showed that mixed-signal ANNs trained with our\nproposed method can achieve an equivalent classification accuracy with noise\nlevel up to 50% of the ideal quantization step size. We have demonstrated this\ntraining method on a mixed-signal optical convolutional neural network based on\ndiffractive optics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 20:46:22 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ulseth", "Joseph", ""], ["Zhu", "Zheyuan", ""], ["Li", "Guifang", ""], ["Pang", "Shuo", ""]]}, {"id": "2008.09377", "submitter": "Binyamin Manela", "authors": "Binyamin Manela, Armin Biess", "title": "Curriculum Learning with Hindsight Experience Replay for Sequential\n  Object Manipulation Tasks", "comments": "arXiv admin note: text overlap with arXiv:2001.03877", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning complex tasks from scratch is challenging and often impossible for\nhumans as well as for artificial agents. A curriculum can be used instead,\nwhich decomposes a complex task (target task) into a sequence of source tasks\n(the curriculum). Each source task is a simplified version of the next source\ntask with increasing complexity. Learning then occurs gradually by training on\neach source task while using knowledge from the curriculum's prior source\ntasks. In this study, we present a new algorithm that combines curriculum\nlearning with Hindsight Experience Replay (HER), to learn sequential object\nmanipulation tasks for multiple goals and sparse feedback. The algorithm\nexploits the recurrent structure inherent in many object manipulation tasks and\nimplements the entire learning process in the original simulation without\nadjusting it to each source task. We have tested our algorithm on three\nchallenging throwing tasks and show vast improvements compared to vanilla-HER.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:59:28 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Manela", "Binyamin", ""], ["Biess", "Armin", ""]]}, {"id": "2008.09409", "submitter": "Ruo Ando", "authors": "Ruo Ando, Yoshiyasu Takefuji", "title": "A constrained recursion algorithm for batch normalization of\n  tree-sturctured LSTM", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured LSTM is promising way to consider long-distance interaction\nover hierarchies. However, there have been few research efforts on the\nhyperparameter tuning of the construction and traversal of tree-structured\nLSTM. To name a few, hyperparamters such as the interval of state\ninitialization, the number of batches for normalization have been left\nunexplored specifically in applying batch normalization for reducing training\ncost and parallelization. In this paper, we propose a novel recursive algorithm\nfor traversing batch normalized tree-structured LSTM. In proposal method, we\nimpose the constraint on the recursion algorithm for the depth-first search of\nbinary tree representation of LSTM for which batch normalization is applied.\nWith our constrained recursion, we can control the hyperparameter in the\ntraversal of several tree-structured LSTMs which is generated in the process of\nbatch normalization. The tree traversal is divided into two steps. At first\nstage, the width-first search over models is applied for discover the start\npoint of the latest tree-structured LSTM block. Then, the depth-first search is\nrun to traverse tree-structured LSTM. Proposed method enables us to explore the\noptimized selection of hyperparameters of recursive neural network\nimplementation by changing the constraints of our recursion algorithm. In\nexperiment, we measure and plot the validation loss and computing time with\nchanging the length of internal of state initialization of tree-structured\nLSTM. It has been turned out that proposal method is effective for\nhyperparameter tuning such as the number of batches and length of interval of\nstate initialization of tree-structured LSTM.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:31:45 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ando", "Ruo", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "2008.09575", "submitter": "Vipul Mann", "authors": "Vipul Mann, Abhishek Sivaram, Laya Das, Venkat Venkatasubramanian", "title": "Robust and Efficient Swarm Communication Topologies for Hostile\n  Environments", "comments": null, "journal-ref": null, "doi": "10.1016/j.swevo.2021.100848", "report-no": null, "categories": "cs.NE cs.MA cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm Intelligence-based optimization techniques combine systematic\nexploration of the search space with information available from neighbors and\nrely strongly on communication among agents. These algorithms are typically\nemployed to solve problems where the function landscape is not adequately known\nand there are multiple local optima that could result in premature convergence\nfor other algorithms. Applications of such algorithms can be found in\ncommunication systems involving design of networks for efficient information\ndissemination to a target group, targeted drug-delivery where drug molecules\nsearch for the affected site before diffusing, and high-value target\nlocalization with a network of drones. In several of such applications, the\nagents face a hostile environment that can result in loss of agents during the\nsearch. Such a loss changes the communication topology of the agents and hence\nthe information available to agents, ultimately influencing the performance of\nthe algorithm. In this paper, we present a study of the impact of loss of\nagents on the performance of such algorithms as a function of the initial\nnetwork configuration. We use particle swarm optimization to optimize an\nobjective function with multiple sub-optimal regions in a hostile environment\nand study its performance for a range of network topologies with loss of\nagents. The results reveal interesting trade-offs between efficiency,\nrobustness, and performance for different topologies that are subsequently\nleveraged to discover general properties of networks that maximize performance.\nMoreover, networks with small-world properties are seen to maximize performance\nunder hostile conditions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 16:38:35 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Mann", "Vipul", ""], ["Sivaram", "Abhishek", ""], ["Das", "Laya", ""], ["Venkatasubramanian", "Venkat", ""]]}, {"id": "2008.09685", "submitter": "Rafael Pinto", "authors": "Rafael Pinto", "title": "Model-Free Episodic Control with State Aggregation", "comments": "8 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic control provides a highly sample-efficient method for reinforcement\nlearning while enforcing high memory and computational requirements. This work\nproposes a simple heuristic for reducing these requirements, and an application\nto Model-Free Episodic Control (MFEC) is presented. Experiments on Atari games\nshow that this heuristic successfully reduces MFEC computational demands while\nproducing no significant loss of performance when conservative choices of\nhyperparameters are used. Consequently, episodic control becomes a more\nfeasible option when dealing with reinforcement learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:20:49 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Pinto", "Rafael", ""]]}, {"id": "2008.09903", "submitter": "Leonardo Enzo Brito Da Silva", "authors": "Leonardo Enzo Brito da Silva and Nagasharath Rayapati and Donald C.\n  Wunsch II", "title": "iCVI-ARTMAP: Accelerating and improving clustering using adaptive\n  resonance theory predictive mapping and incremental cluster validity indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an adaptive resonance theory predictive mapping (ARTMAP)\nmodel which uses incremental cluster validity indices (iCVIs) to perform\nunsupervised learning, namely iCVI-ARTMAP. Incorporating iCVIs to the\ndecision-making and many-to-one mapping capabilities of ARTMAP can improve the\nchoices of clusters to which samples are incrementally assigned. These\nimprovements are accomplished by intelligently performing the operations of\nswapping sample assignments between clusters, splitting and merging clusters,\nand caching the values of variables when iCVI values need to be recomputed.\nUsing recursive formulations enables iCVI-ARTMAP to considerably reduce the\ncomputational burden associated with cluster validity index (CVI)-based offline\nclustering. Depending on the iCVI and the data set, it can achieve running\ntimes up to two orders of magnitude shorter than when using batch CVI\ncomputations. In this work, the incremental versions of Calinski-Harabasz,\nWB-index, Xie-Beni, Davies-Bouldin, Pakhira-Bandyopadhyay-Maulik, and\nnegentropy increment were integrated into fuzzy ARTMAP. Experimental results\nshow that, with proper choice of iCVI, iCVI-ARTMAP outperformed fuzzy adaptive\nresonance theory (ART), dual vigilance fuzzy ART, kmeans, spectral clustering,\nGaussian mixture models and hierarchical agglomerative clustering algorithms in\nmost of the synthetic benchmark data sets. It also performed competitively on\nreal world image benchmark data sets when clustering on projections and on\nlatent spaces generated by a deep clustering model. Naturally, the performance\nof iCVI-ARTMAP is subject to the selected iCVI and its suitability to the data\nat hand; fortunately, it is a general model wherein other iCVIs can be easily\nembedded.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 19:37:01 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["da Silva", "Leonardo Enzo Brito", ""], ["Rayapati", "Nagasharath", ""], ["Wunsch", "Donald C.", "II"]]}, {"id": "2008.09926", "submitter": "Anuraganand Sharma Dr", "authors": "Anuraganand Sharma", "title": "Optimistic variants of single-objective bilevel optimization for\n  evolutionary algorithms", "comments": "preprint: 15 pages, published: 20 pages", "journal-ref": "IJCIA, 2020, pp. 2050020-1 to 2050020-20", "doi": "10.1142/S1469026820500200", "report-no": "Vol. 19", "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single-objective bilevel optimization is a specialized form of constraint\noptimization problems where one of the constraints is an optimization problem\nitself. These problems are typically non-convex and strongly NP-Hard. Recently,\nthere has been an increased interest from the evolutionary computation\ncommunity to model bilevel problems due to its applicability in the real-world\napplications for decision-making problems. In this work, a partial nested\nevolutionary approach with a local heuristic search has been proposed to solve\nthe benchmark problems and have outstanding results. This approach relies on\nthe concept of intermarriage-crossover in search of feasible regions by\nexploiting information from the constraints. A new variant has also been\nproposed to the commonly used convergence approaches, i.e., optimistic and\npessimistic. It is called extreme optimistic approach. The experimental results\ndemonstrate the algorithm converges differently to known optimum solutions with\nthe optimistic variants. Optimistic approach also outperforms pessimistic\napproach. Comparative statistical analysis of our approach with other recently\npublished partial to complete evolutionary approaches demonstrates very\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 23:12:07 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Sharma", "Anuraganand", ""]]}, {"id": "2008.10425", "submitter": "Ajay Patrikar", "authors": "Ajay M. Patrikar", "title": "Efficient Design of Neural Networks with Random Weights", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single layer feedforward networks with random weights are known for their\nnon-iterative and fast training algorithms and are successful in a variety of\nclassification and regression problems. A major drawback of these networks is\nthat they require a large number of hidden units. In this paper, we propose a\ntechnique to reduce the number of hidden units substantially without affecting\nthe accuracy of the networks significantly. We introduce the concept of primary\nand secondary hidden units. The weights for the primary hidden units are chosen\nrandomly while the secondary hidden units are derived using pairwise\ncombinations of the primary hidden units. Using this technique, we show that\nthe number of hidden units can be reduced by at least one order of magnitude.\nWe experimentally show that this technique leads to significant drop in\ncomputations at inference time and has only a minor impact on network accuracy.\nA huge reduction in computations is possible if slightly lower accuracy is\nacceptable.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:24:25 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Patrikar", "Ajay M.", ""]]}, {"id": "2008.10599", "submitter": "William Peebles", "authors": "William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, Antonio\n  Torralba", "title": "The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement", "comments": "ECCV 2020 (Spotlight). Code available at\n  https://github.com/wpeebles/hessian_penalty . Project page and videos\n  available at https://www.wpeebles.com/hessian-penalty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing disentanglement methods for deep generative models rely on\nhand-picked priors and complex encoder-based architectures. In this paper, we\npropose the Hessian Penalty, a simple regularization term that encourages the\nHessian of a generative model with respect to its input to be diagonal. We\nintroduce a model-agnostic, unbiased stochastic approximation of this term\nbased on Hutchinson's estimator to compute it efficiently during training. Our\nmethod can be applied to a wide range of deep generators with just a few lines\nof code. We show that training with the Hessian Penalty often causes\naxis-aligned disentanglement to emerge in latent space when applied to ProGAN\non several datasets. Additionally, we use our regularization term to identify\ninterpretable directions in BigGAN's latent space in an unsupervised fashion.\nFinally, we provide empirical evidence that the Hessian Penalty encourages\nsubstantial shrinkage when applied to over-parameterized latent spaces.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:59:56 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Peebles", "William", ""], ["Peebles", "John", ""], ["Zhu", "Jun-Yan", ""], ["Efros", "Alexei", ""], ["Torralba", "Antonio", ""]]}, {"id": "2008.10633", "submitter": "Thomas Carroll", "authors": "Thomas L. Carroll", "title": "Adding Filters to Improve Reservoir Computer Performance", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2020.132798", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computers are a type of neuromorphic computer that may be built a\nan analog system, potentially creating powerful computers that are small, light\nand consume little power. Typically a reservoir computer is build by connecting\ntogether a set of nonlinear nodes into a network; connecting the nonlinear\nnodes may be difficult or expensive, however. This work shows how a reservoir\ncomputer may be expanded by adding functions to its output. The particular\nfunctions described here are linear filters, but other functions are possible.\nThe design and construction of linear filters is well known, and such filters\nmay be easily implemented in hardware such as field programmable gate arrays\n(FPGA's). The effect of adding filters on the reservoir computer performance is\nsimulated for a signal fitting problem, a prediction problem and a signal\nclassification problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 18:10:27 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 12:03:17 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Carroll", "Thomas L.", ""]]}, {"id": "2008.10937", "submitter": "Yuqiao Liu", "authors": "Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen, Kay Chen\n  Tan", "title": "A Survey on Evolutionary Neural Architecture Search", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved great success in many applications.\nThe architectures of DNNs play a crucial role in their performance, which is\nusually manually designed with rich expertise. However, such a design process\nis labour intensive because of the trial-and-error process, and also not easy\nto realize due to the rare expertise in practice. Neural Architecture Search\n(NAS) is a type of technology that can design the architectures automatically.\nAmong different methods to realize NAS, Evolutionary Computation (EC) methods\nhave recently gained much attention and success. Unfortunately, there has not\nyet been a comprehensive summary of the EC-based NAS algorithms. This paper\nreviews over 200 papers of most recent EC-based NAS methods in light of the\ncore components, to systematically discuss their design principles as well as\njustifications on the design. Furthermore, current challenges and issues are\nalso discussed to identify future research in this emerging field.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 11:00:46 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 14:07:46 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 15:35:25 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Liu", "Yuqiao", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""], ["Yen", "Gary G.", ""], ["Tan", "Kay Chen", ""]]}, {"id": "2008.11247", "submitter": "Lorenz Butschek", "authors": "Lorenz Butschek, Akram Akrout, Evangelia Dimitriadou, Marc Haelterman,\n  Serge Massar", "title": "Parallel photonic reservoir computing based on frequency multiplexing of\n  neurons", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photonic implementations of reservoir computing can achieve state-of-the-art\nperformance on a number of benchmark tasks, but are predominantly based on\nsequential data processing. Here we report a parallel implementation that uses\nfrequency domain multiplexing of neuron states, with the potential of\nsignificantly reducing the computation time compared to sequential\narchitectures. We illustrate its performance on two standard benchmark tasks.\nThe present work represents an important advance towards high speed, low\nfootprint, all optical photonic information processing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 19:30:42 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Butschek", "Lorenz", ""], ["Akrout", "Akram", ""], ["Dimitriadou", "Evangelia", ""], ["Haelterman", "Marc", ""], ["Massar", "Serge", ""]]}, {"id": "2008.11298", "submitter": "Abhiroop Bhattacharjee", "authors": "Abhiroop Bhattacharjee and Priyadarshini Panda", "title": "Rethinking Non-idealities in Memristive Crossbars for Adversarial\n  Robustness in Neural Networks", "comments": "7 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been shown to be prone to adversarial\nattacks. Memristive crossbars, being able to perform\nMatrix-Vector-Multiplications (MVMs) efficiently, are used to realize DNNs on\nhardware. However, crossbar non-idealities have always been devalued since they\ncause errors in performing MVMs, leading to computational accuracy losses in\nDNNs. Several software-based defenses have been proposed to make DNNs\nadversarially robust. However, no previous work has demonstrated the advantage\nconferred by the crossbar non-idealities in unleashing adversarial robustness.\nWe show that the intrinsic hardware non-idealities yield adversarial robustness\nto the mapped DNNs without any additional optimization. We evaluate the\nadversarial resilience of state-of-the-art DNNs (VGG8 & VGG16 networks) using\nbenchmark datasets (CIFAR-10, CIFAR-100 & Tiny Imagenet) across various\ncrossbar sizes. We find that crossbar non-idealities unleash significantly\ngreater adversarial robustness (>10-20%) in crossbar-mapped DNNs than baseline\nsoftware DNNs. We further assess the performance of our approach with other\nstate-of-the-art efficiency-driven adversarial defenses and find that our\napproach performs significantly well in terms of reducing adversarial loss.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 22:45:34 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 00:45:20 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Bhattacharjee", "Abhiroop", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2008.11303", "submitter": "Kennedy Araujo", "authors": "Kennedy Araujo and Tiberius Bonates and Bruno Prata", "title": "Integrated Cutting and Packing Heterogeneous Precast Beams Multiperiod\n  Production Planning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel variant of cutting production planning problems named\nIntegrated Cutting and Packing Heterogeneous Precast Beams Multiperiod\nProduction Planning (ICP-HPBMPP). We propose an integer linear programming\nmodel for the ICP-HPBMPP, as well as a lower bound for its optimal objective\nfunction value, which is empirically shown to be closer to the optimal solution\nvalue than the bound obtained from the linear relaxation of the model. We also\npropose a genetic algorithm approach for the ICP-HPBMPP as an alternative\nsolution method. We discuss computational experiments and propose a\nparameterization for the genetic algorithm using D-optimal experimental design.\nWe observe good performance of the exact approach when solving small-sized\ninstances, although there are difficulties in finding optimal solutions for\nmedium and large-sized problems, or even in finding feasible solutions for\nlarge instances. On the other hand, the genetic algorithm could find\ngood-quality solutions for large-sized instances within short computing times.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 23:10:37 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Araujo", "Kennedy", ""], ["Bonates", "Tiberius", ""], ["Prata", "Bruno", ""]]}, {"id": "2008.11491", "submitter": "Sam Blakeman", "authors": "Sam Blakeman, Denis Mareschal", "title": "Selective Particle Attention: Visual Feature-Based Attention in Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain uses selective attention to filter perceptual input so that\nonly the components that are useful for behaviour are processed using its\nlimited computational resources. We focus on one particular form of visual\nattention known as feature-based attention, which is concerned with identifying\nfeatures of the visual input that are important for the current task regardless\nof their spatial location. Visual feature-based attention has been proposed to\nimprove the efficiency of Reinforcement Learning (RL) by reducing the\ndimensionality of state representations and guiding learning towards relevant\nfeatures. Despite achieving human level performance in complex perceptual-motor\ntasks, Deep RL algorithms have been consistently criticised for their poor\nefficiency and lack of flexibility. Visual feature-based attention therefore\nrepresents one option for addressing these criticisms. Nevertheless, it is\nstill an open question how the brain is able to learn which features to attend\nto during RL. To help answer this question we propose a novel algorithm, termed\nSelective Particle Attention (SPA), which imbues a Deep RL agent with the\nability to perform selective feature-based attention. SPA learns which\ncombinations of features to attend to based on their bottom-up saliency and how\naccurately they predict future reward. We evaluate SPA on a multiple choice\ntask and a 2D video game that both involve raw pixel input and dynamic changes\nto the task structure. We show various benefits of SPA over approaches that\nnaively attend to either all or random subsets of features. Our results\ndemonstrate (1) how visual feature-based attention in Deep RL models can\nimprove their learning efficiency and ability to deal with sudden changes in\ntask structure and (2) that particle filters may represent a viable\ncomputational account of how visual feature-based attention occurs in the\nbrain.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 11:07:50 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Blakeman", "Sam", ""], ["Mareschal", "Denis", ""]]}, {"id": "2008.11545", "submitter": "Azlan Iqbal", "authors": "Azlan Iqbal", "title": "The Effects of Quantum Randomness on a System Exhibiting Computational\n  Creativity", "comments": "4 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present experimental results on the effects of using quantum or 'truly'\nrandom numbers, as opposed to pseudorandom numbers, in a system that exhibits\ncomputational creativity (given its ability to compose original chess\nproblems). The results indicate that using quantum random numbers too often or\ntoo seldom in the composing process does not have any positive effect on the\noutput generated. Interestingly, there is a 'sweet spot' of using quantum\nrandom numbers 15% of the time that results in fewer statistical outliers.\nOverall, it would appear that there may indeed be a slight advantage to using\nquantum random numbers in such a system and this may also be true in other\nsystems that exhibit computational creativity. The benefits of doing so should,\nhowever, be weighed against the overhead of obtaining quantum random numbers in\ncontrast to a pseudorandom number generator that is likely more convenient to\nincorporate.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 04:34:17 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 03:35:11 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Iqbal", "Azlan", ""]]}, {"id": "2008.11634", "submitter": "Alvaro Cabrejas Egea", "authors": "Alvaro Cabrejas-Egea, Shaun Howell, Maksis Knutins and Colm\n  Connaughton", "title": "Assessment of Reward Functions for Reinforcement Learning Traffic Signal\n  Control under Real-World Limitations", "comments": "Conference paper, 13 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.1109/SMC42975.2020.9283498", "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive traffic signal control is one key avenue for mitigating the growing\nconsequences of traffic congestion. Incumbent solutions such as SCOOT and SCATS\nrequire regular and time-consuming calibration, can't optimise well for\nmultiple road use modalities, and require the manual curation of many\nimplementation plans. A recent alternative to these approaches are deep\nreinforcement learning algorithms, in which an agent learns how to take the\nmost appropriate action for a given state of the system. This is guided by\nneural networks approximating a reward function that provides feedback to the\nagent regarding the performance of the actions taken, making it sensitive to\nthe specific reward function chosen. Several authors have surveyed the reward\nfunctions used in the literature, but attributing outcome differences to reward\nfunction choice across works is problematic as there are many uncontrolled\ndifferences, as well as different outcome metrics. This paper compares the\nperformance of agents using different reward functions in a simulation of a\njunction in Greater Manchester, UK, across various demand profiles, subject to\nreal world constraints: realistic sensor inputs, controllers, calibrated\ndemand, intergreen times and stage sequencing. The reward metrics considered\nare based on the time spent stopped, lost time, change in lost time, average\nspeed, queue length, junction throughput and variations of these magnitudes.\nThe performance of these reward functions is compared in terms of total waiting\ntime. We find that speed maximisation resulted in the lowest average waiting\ntimes across all demand levels, displaying significantly better performance\nthan other rewards previously introduced in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 15:47:15 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 16:00:15 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Cabrejas-Egea", "Alvaro", ""], ["Howell", "Shaun", ""], ["Knutins", "Maksis", ""], ["Connaughton", "Colm", ""]]}, {"id": "2008.11642", "submitter": "Carlo Michaelis", "authors": "Carlo Michaelis, Andrew B. Lehr and Christian Tetzlaff", "title": "Robust trajectory generation for robotic control on the neuromorphic\n  research chip Loihi", "comments": null, "journal-ref": null, "doi": "10.3389/fnbot.2020.589532", "report-no": null, "categories": "cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic hardware has several promising advantages compared to von\nNeumann architectures and is highly interesting for robot control. However,\ndespite the high speed and energy efficiency of neuromorphic computing,\nalgorithms utilizing this hardware in control scenarios are still rare. One\nproblem is the transition from fast spiking activity on the hardware, which\nacts on a timescale of a few milliseconds, to a control-relevant timescale on\nthe order of hundreds of milliseconds. Another problem is the execution of\ncomplex trajectories, which requires spiking activity to contain sufficient\nvariability, while at the same time, for reliable performance, network dynamics\nmust be adequately robust against noise. In this study we exploit a recently\ndeveloped biologically-inspired spiking neural network model, the so-called\nanisotropic network. We identified and transferred the core principles of the\nanisotropic network to neuromorphic hardware using Intel's neuromorphic\nresearch chip Loihi and validated the system on trajectories from a\nmotor-control task performed by a robot arm. We developed a network\narchitecture including the anisotropic network and a pooling layer which allows\nfast spike read-out from the chip and performs an inherent regularization. With\nthis, we show that the anisotropic network on Loihi reliably encodes sequential\npatterns of neural activity, each representing a robotic action, and that the\npatterns allow the generation of multidimensional trajectories on\ncontrol-relevant timescales. Taken together, our study presents a new algorithm\nthat allows the generation of complex robotic movements as a building block for\nrobotic control using state of the art neuromorphic hardware.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:02:39 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 12:15:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Michaelis", "Carlo", ""], ["Lehr", "Andrew B.", ""], ["Tetzlaff", "Christian", ""]]}, {"id": "2008.11659", "submitter": "Xing Lin", "authors": "Tiankuang Zhou, Xing Lin, Jiamin Wu, Yitong Chen, Hao Xie, Yipeng Li,\n  Jintao Fan, Huaqiang Wu, Lu Fang and Qionghai Dai", "title": "Large-scale neuromorphic optoelectronic computing with a reconfigurable\n  diffractive processing unit", "comments": null, "journal-ref": null, "doi": "10.1038/s41566-021-00796-w", "report-no": null, "categories": "eess.IV cs.LG cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application-specific optical processors have been considered disruptive\ntechnologies for modern computing that can fundamentally accelerate the\ndevelopment of artificial intelligence (AI) by offering substantially improved\ncomputing performance. Recent advancements in optical neural network\narchitectures for neural information processing have been applied to perform\nvarious machine learning tasks. However, the existing architectures have\nlimited complexity and performance; and each of them requires its own dedicated\ndesign that cannot be reconfigured to switch between different neural network\nmodels for different applications after deployment. Here, we propose an\noptoelectronic reconfigurable computing paradigm by constructing a diffractive\nprocessing unit (DPU) that can efficiently support different neural networks\nand achieve a high model complexity with millions of neurons. It allocates\nalmost all of its computational operations optically and achieves extremely\nhigh speed of data modulation and large-scale network parameter updating by\ndynamically programming optical modulators and photodetectors. We demonstrated\nthe reconfiguration of the DPU to implement various diffractive feedforward and\nrecurrent neural networks and developed a novel adaptive training approach to\ncircumvent the system imperfections. We applied the trained networks for\nhigh-speed classifying of handwritten digit images and human action videos over\nbenchmark datasets, and the experimental results revealed a comparable\nclassification accuracy to the electronic computing approaches. Furthermore,\nour prototype system built with off-the-shelf optoelectronic components\nsurpasses the performance of state-of-the-art graphics processing units (GPUs)\nby several times on computing speed and more than an order of magnitude on\nsystem energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:34:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhou", "Tiankuang", ""], ["Lin", "Xing", ""], ["Wu", "Jiamin", ""], ["Chen", "Yitong", ""], ["Xie", "Hao", ""], ["Li", "Yipeng", ""], ["Fan", "Jintao", ""], ["Wu", "Huaqiang", ""], ["Fang", "Lu", ""], ["Dai", "Qionghai", ""]]}, {"id": "2008.11881", "submitter": "Parth Mannan", "authors": "Parth Mannan, Ananda Samajdar and Tushar Krishna", "title": "CLAN: Continuous Learning using Asynchronous Neuroevolution on Commodity\n  Edge Devices", "comments": "Accepted and appears in ISPASS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in machine learning algorithms, especially the\ndevelopment of Deep Neural Networks (DNNs) have transformed the landscape of\nArtificial Intelligence (AI). With every passing day, deep learning based\nmethods are applied to solve new problems with exceptional results. The portal\nto the real world is the edge. The true impact of AI can only be fully realized\nif we can have AI agents continuously interacting with the real world and\nsolving everyday problems. Unfortunately, high compute and memory requirements\nof DNNs acts a huge barrier towards this vision. Today we circumvent this\nproblem by deploying special purpose inference hardware on the edge while\nprocuring trained models from the cloud. This approach, however, relies on\nconstant interaction with the cloud for transmitting all the data, training on\nmassive GPU clusters, and downloading updated models. This is challenging for\nbandwidth, privacy, and constant connectivity concerns that autonomous agents\nmay exhibit. In this paper we evaluate techniques for enabling adaptive\nintelligence on edge devices with zero interaction with any high-end\ncloud/server. We build a prototype distributed system of Raspberry Pis\ncommunicating via WiFi running NeuroEvolutionary (NE) learning and inference.\nWe evaluate the performance of such a collaborative system and detail the\ncompute/communication characteristics of different arrangements of the system\nthat trade-off parallelism versus communication. Using insights from our\nanalysis, we also propose algorithmic modifications to reduce communication by\nup to 3.6x during the learning phase to enhance scalability even further and\nmatch performance of higher end computing devices at scale. We believe that\nthese insights will enable algorithm-hardware co-design efforts for enabling\ncontinuous learning on the edge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 01:49:21 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Mannan", "Parth", ""], ["Samajdar", "Ananda", ""], ["Krishna", "Tushar", ""]]}, {"id": "2008.12020", "submitter": "Sayantari Ghosh", "authors": "Sayantari Ghosh and Saumik Bhattacharya", "title": "A Data-driven Understanding of COVID-19 Dynamics Using Sequential\n  Genetic Algorithm Based Probabilistic Cellular Automata", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG cs.NE nlin.CG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 pandemic is severely impacting the lives of billions across the\nglobe. Even after taking massive protective measures like nation-wide\nlockdowns, discontinuation of international flight services, rigorous testing\netc., the infection spreading is still growing steadily, causing thousands of\ndeaths and serious socio-economic crisis. Thus, the identification of the major\nfactors of this infection spreading dynamics is becoming crucial to minimize\nimpact and lifetime of COVID-19 and any future pandemic. In this work, a\nprobabilistic cellular automata based method has been employed to model the\ninfection dynamics for a significant number of different countries. This study\nproposes that for an accurate data-driven modeling of this infection spread,\ncellular automata provides an excellent platform, with a sequential genetic\nalgorithm for efficiently estimating the parameters of the dynamics. To the\nbest of our knowledge, this is the first attempt to understand and interpret\nCOVID-19 data using optimized cellular automata, through genetic algorithm. It\nhas been demonstrated that the proposed methodology can be flexible and robust\nat the same time, and can be used to model the daily active cases, total number\nof infected people and total death cases through systematic parameter\nestimation. Elaborate analyses for COVID-19 statistics of forty countries from\ndifferent continents have been performed, with markedly divergent time\nevolution of the infection spreading because of demographic and socioeconomic\nfactors. The substantial predictive power of this model has been established\nwith conclusions on the key players in this pandemic dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:53:21 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ghosh", "Sayantari", ""], ["Bhattacharya", "Saumik", ""]]}, {"id": "2008.12082", "submitter": "Philip Feldman", "authors": "Philip Feldman", "title": "Training robust anomaly detection using ML-Enhanced simulations", "comments": "12 pages, 13 figures. Presented at GVSETS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the use of neural networks to enhance simulations for\nsubsequent training of anomaly-detection systems. Simulations can provide edge\nconditions for anomaly detection which may be sparse or non-existent in\nreal-world data. Simulations suffer, however, by producing data that is \"too\nclean\" resulting in anomaly detection systems that cannot transition from\nsimulated data to actual conditions. Our approach enhances simulations using\nneural networks trained on real-world data to create outputs that are more\nrealistic and variable than traditional simulations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 12:28:07 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 10:56:59 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Feldman", "Philip", ""]]}, {"id": "2008.12833", "submitter": "Gabriel Spadon", "authors": "Gabriel Spadon, Shenda Hong, Bruno Brandoli, Stan Matwin, Jose F.\n  Rodrigues-Jr, and Jimeng Sun", "title": "Pay Attention to Evolution: Time Series Forecasting with Deep\n  Graph-Evolution Learning", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3076155", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series forecasting is one of the most active research topics in\nartificial intelligence. Applications in real-world time series should consider\ntwo factors for achieving reliable predictions: modeling dynamic dependencies\namong multiple variables and adjusting the model's intrinsic hyperparameters. A\nstill open gap in that literature is that statistical and ensemble learning\napproaches systematically present lower predictive performance than deep\nlearning methods. They generally disregard the data sequence aspect entangled\nwith multivariate data represented in more than one time series. Conversely,\nthis work presents a novel neural network architecture for time-series\nforecasting that combines the power of graph evolution with deep recurrent\nlearning on distinct data distributions; we named our method Recurrent Graph\nEvolution Neural Network (ReGENN). The idea is to infer multiple multivariate\nrelationships between co-occurring time-series by assuming that the temporal\ndata depends not only on inner variables and intra-temporal relationships\n(i.e., observations from itself) but also on outer variables and inter-temporal\nrelationships (i.e., observations from other-selves). An extensive set of\nexperiments was conducted comparing ReGENN with dozens of ensemble methods and\nclassical statistical ones, showing sound improvement of up to 64.87% over the\ncompeting algorithms. Furthermore, we present an analysis of the intermediate\nweights arising from ReGENN, showing that by looking at inter and\nintra-temporal relationships simultaneously, time-series forecasting is majorly\nimproved if paying attention to how multiple multivariate data synchronously\nevolve.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 20:10:07 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 11:10:35 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 15:46:00 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 19:58:48 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Spadon", "Gabriel", ""], ["Hong", "Shenda", ""], ["Brandoli", "Bruno", ""], ["Matwin", "Stan", ""], ["Rodrigues-Jr", "Jose F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "2008.12884", "submitter": "Josimar Chire Saire", "authors": "Josimar E. Chire-Saire", "title": "New feature for Complex Network based on Ant Colony Optimization for\n  High Level Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low level classification extracts features from the elements, i.e. physical\nto use them to train a model for a later classification. High level\nclassification uses high level features, the existent patterns, relationship\nbetween the data and combines low and high level features for classification.\nHigh Level features can be got from Complex Network created over the data.\nLocal and global features are used to describe the structure of a Complex\nNetwork, i.e. Average Neighbor Degree, Average Clustering. The present work\nproposed a novel feature to describe the architecture of the Network following\na Ant Colony System approach. The experiments shows the advantage of using this\nfeature because the sensibility with data of different classes.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 00:22:43 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chire-Saire", "Josimar E.", ""]]}, {"id": "2008.13187", "submitter": "Yanan Sun", "authors": "Yanan Sun and Xian Sun and Yuhan Fang and Gary Yen", "title": "A Novel Training Protocol for Performance Predictors of Evolutionary\n  Neural Architecture Search Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Neural Architecture Search (ENAS) can automatically design the\narchitectures of Deep Neural Networks (DNNs) using evolutionary computation\nalgorithms. However, most ENAS algorithms require intensive computational\nresource, which is not necessarily available to the users interested.\nPerformance predictors are a type of regression models which can assist to\naccomplish the search, while without exerting much computational resource.\nDespite various performance predictors have been designed, they employ the same\ntraining protocol to build the regression models: 1) sampling a set of DNNs\nwith performance as the training dataset, 2) training the model with the mean\nsquare error criterion, and 3) predicting the performance of DNNs newly\ngenerated during the ENAS. In this paper, we point out that the three steps\nconstituting the training protocol are not well though-out through intuitive\nand illustrative examples. Furthermore, we propose a new training protocol to\naddress these issues, consisting of designing a pairwise ranking indicator to\nconstruct the training target, proposing to use the logistic regression to fit\nthe training samples, and developing a differential method to building the\ntraining instances. To verify the effectiveness of the proposed training\nprotocol, four widely used regression models in the field of machine learning\nhave been chosen to perform the comparisons on two benchmark datasets. The\nexperimental results of all the comparisons demonstrate that the proposed\ntraining protocol can significantly improve the performance prediction accuracy\nagainst the traditional training protocols.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 14:39:28 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 06:22:28 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sun", "Yanan", ""], ["Sun", "Xian", ""], ["Fang", "Yuhan", ""], ["Yen", "Gary", ""]]}, {"id": "2008.13229", "submitter": "Ernest Greene", "authors": "Ernest Greene", "title": "An evolutionary perspective on the design of neuromorphic shape filters", "comments": null, "journal-ref": "IEEE Access, 2020, 8, 114228-114238", "doi": "10.1109/ACCESS.2020_3004412", "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A substantial amount of time and energy has been invested to develop machine\nvision using connectionist (neural network) principles. Most of that work has\nbeen inspired by theories advanced by neuroscientists and behaviorists for how\ncortical systems store stimulus information. Those theories call for\ninformation flow through connections among several neuron populations, with the\ninitial connections being random (or at least non-functional). Then the\nstrength or location of connections are modified through training trials to\nachieve an effective output, such as the ability to identify an object. Those\ntheories ignored the fact that animals that have no cortex, e.g., fish, can\ndemonstrate visual skills that outpace the best neural network models. Neural\ncircuits that allow for immediate effective vision and quick learning have been\npreprogrammed by hundreds of millions of years of evolution and the visual\nskills are available shortly after hatching. Cortical systems may be providing\nadvanced image processing, but most likely are using design principles that had\nbeen proven effective in simpler systems. The present article provides a brief\noverview of retinal and cortical mechanisms for registering shape information,\nwith the hope that it might contribute to the design of shape-encoding circuits\nthat more closely match the mechanisms of biological vision.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:53:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Greene", "Ernest", ""]]}, {"id": "2008.13245", "submitter": "Shrisha Rao", "authors": "Karn Dubey and Urja Kothari and Shrisha Rao", "title": "Floating-Point Multiplication Using Neuromorphic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing describes the use of VLSI systems to mimic\nneuro-biological architectures and is also looked at as a promising alternative\nto the traditional von Neumann architecture. Any new computing architecture\nwould need a system that can perform floating-point arithmetic. In this paper,\nwe describe a neuromorphic system that performs IEEE 754-compliant\nfloating-point multiplication. The complex process of multiplication is divided\ninto smaller sub-tasks performed by components Exponent Adder, Bias Subtractor,\nMantissa Multiplier and Sign OF/UF. We study the effect of the number of\nneurons per bit on accuracy and bit error rate, and estimate the optimal number\nof neurons needed for each component.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 19:07:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Dubey", "Karn", ""], ["Kothari", "Urja", ""], ["Rao", "Shrisha", ""]]}, {"id": "2008.13253", "submitter": "Edgar Galvan", "authors": "Edgar Galv\\'an, Oxana Gorshkova, Peter Mooney, Fred Valdez Ameneyro\n  and Erik Cuevas", "title": "Statistical Tree-based Population Seeding for Rolling Horizon EAs in\n  General Video Game Playing", "comments": "14 Pages, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Artificial Intelligence (AI) methods have been proposed over recent\nyears to create controllers to play multiple video games of different nature\nand complexity without revealing the specific mechanics of each of these games\nto the AI methods. In recent years, Evolutionary Algorithms (EAs) employing\nrolling horizon mechanisms have achieved extraordinary results in these type of\nproblems. However, some limitations are present in Rolling Horizon EAs making\nit a grand challenge of AI. These limitations include the wasteful mechanism of\ncreating a population and evolving it over a fraction of a second to propose an\naction to be executed by the game agent. Another limitation is to use a scalar\nvalue (fitness value) to direct evolutionary search instead of accounting for a\nmechanism that informs us how a particular agent behaves during the rolling\nhorizon simulation. In this work, we address both of these issues. We introduce\nthe use of a statistical tree that tackles the latter limitation. Furthermore,\nwe tackle the former limitation by employing a mechanism that allows us to seed\npart of the population using Monte Carlo Tree Search, a method that has\ndominated multiple General Video Game AI competitions. We show how the proposed\nnovel mechanism, called Statistical Tree-based Population Seeding, achieves\nbetter results compared to vanilla Rolling Horizon EAs in a set of 20 games,\nincluding 10 stochastic and 10 deterministic games.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 19:33:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Galv\u00e1n", "Edgar", ""], ["Gorshkova", "Oxana", ""], ["Mooney", "Peter", ""], ["Ameneyro", "Fred Valdez", ""], ["Cuevas", "Erik", ""]]}, {"id": "2008.13335", "submitter": "Thanh Tran", "authors": "Thanh Tran, Di You, Kyumin Lee", "title": "Quaternion-Based Self-Attentive Long Short-Term User Preference Encoding\n  for Recommendation", "comments": null, "journal-ref": "CIKM 2020", "doi": "10.1145/3340531.3411926", "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quaternion space has brought several benefits over the traditional Euclidean\nspace: Quaternions (i) consist of a real and three imaginary components,\nencouraging richer representations; (ii) utilize Hamilton product which better\nencodes the inter-latent interactions across multiple Quaternion components;\nand (iii) result in a model with smaller degrees of freedom and less prone to\noverfitting. Unfortunately, most of the current recommender systems rely on\nreal-valued representations in Euclidean space to model either user's long-term\nor short-term interests. In this paper, we fully utilize Quaternion space to\nmodel both user's long-term and short-term preferences. We first propose a\nQUaternion-based self-Attentive Long term user Encoding (QUALE) to study the\nuser's long-term intents. Then, we propose a QUaternion-based self-Attentive\nShort term user Encoding (QUASE) to learn the user's short-term interests. To\nenhance our models' capability, we propose to fuse QUALE and QUASE into one\nmodel, namely QUALSE, by using a Quaternion-based gating mechanism. We further\ndevelop Quaternion-based Adversarial learning along with the Bayesian\nPersonalized Ranking (QABPR) to improve our model's robustness. Extensive\nexperiments on six real-world datasets show that our fused QUALSE model\noutperformed 11 state-of-the-art baselines, improving 8.43% at HIT@1 and 10.27%\nat NDCG@1 on average compared with the best baseline.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 03:22:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tran", "Thanh", ""], ["You", "Di", ""], ["Lee", "Kyumin", ""]]}]