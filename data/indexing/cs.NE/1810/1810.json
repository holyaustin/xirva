[{"id": "1810.00424", "submitter": "Alexander Tong", "authors": "Alexander Tong, David van Dijk, Jay S. Stanley III, Matthew Amodio,\n  Kristina Yim, Rebecca Muhle, James Noonan, Guy Wolf, and Smita Krishnaswamy", "title": "Interpretable Neuron Structuring with Graph Spectral Regularization", "comments": "12 pages, 6 figures, presented at IDA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks are powerful approximators used to classify or embed\ndata into lower dimensional spaces, they are often regarded as black boxes with\nuninterpretable features. Here we propose Graph Spectral Regularization for\nmaking hidden layers more interpretable without significantly impacting\nperformance on the primary task. Taking inspiration from spatial organization\nand localization of neuron activations in biological networks, we use a graph\nLaplacian penalty to structure the activations within a layer. This penalty\nencourages activations to be smooth either on a predetermined graph or on a\nfeature-space graph learned from the data via co-activations of a hidden layer\nof the neural network. We show numerous uses for this additional structure\nincluding cluster indication and visualization in biological and image data\nsets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 17:18:35 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 02:00:39 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 00:13:46 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 12:18:58 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2020 19:55:11 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Tong", "Alexander", ""], ["van Dijk", "David", ""], ["Stanley", "Jay S.", "III"], ["Amodio", "Matthew", ""], ["Yim", "Kristina", ""], ["Muhle", "Rebecca", ""], ["Noonan", "James", ""], ["Wolf", "Guy", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1810.01064", "submitter": "Shuai Tang", "authors": "Shuai Tang, Virginia R. de Sa", "title": "Improving Sentence Representations with Consensus Maximisation", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.07443", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus maximisation learning can provide self-supervision when different\nviews are available of the same data. The distributional hypothesis provides\nanother form of useful self-supervision from adjacent sentences which are\nplentiful in large unlabelled corpora. Motivated by the observation that\ndifferent learning architectures tend to emphasise different aspects of\nsentence meaning, we present a new self-supervised learning framework for\nlearning sentence representations which minimises the disagreement between two\nviews of the same sentence where one view encodes the sentence with a recurrent\nneural network (RNN), and the other view encodes the same sentence with a\nsimple linear model. After learning, the individual views (networks) result in\nhigher quality sentence representations than their single-view learnt\ncounterparts (learnt using only the distributional hypothesis) as judged by\nperformance on standard downstream tasks. An ensemble of both views provides\neven better generalisation on both supervised and unsupervised downstream\ntasks. Also, importantly the ensemble of views trained with consensus\nmaximisation between the two different architectures performs better on\ndownstream tasks than an analogous ensemble made from the single-view trained\ncounterparts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 04:51:33 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 01:12:24 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 18:02:53 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 01:02:40 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Tang", "Shuai", ""], ["de Sa", "Virginia R.", ""]]}, {"id": "1810.01125", "submitter": "Paolo Pagliuca", "authors": "Paolo Pagliuca and Stefano Nolfi", "title": "Robust Optimization through Neuroevolution", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0213193", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for evolving solutions that are robust with respect to\nvariations of the environmental conditions (i.e. that can operate effectively\nin new conditions immediately, without the need to adapt to variations). The\nobtained results show how the method proposed is effective and computational\ntractable. It permits to improve performance on an extended version of the\ndouble-pole balancing problem, to outperform the best available human-designed\ncontrollers on a car racing problem, and to generate rather effective solutions\nfor a swarm robotic problem. The comparison of different algorithms indicates\nthat the CMA-ES and xNES methods, that operate by optimizing a distribution of\nparameters, represent the best options for the evolution of robust neural\nnetwork controllers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 09:10:42 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Pagliuca", "Paolo", ""], ["Nolfi", "Stefano", ""]]}, {"id": "1810.01185", "submitter": "Alexandru Constantin Serban", "authors": "Alexandru Constantin Serban, Erik Poll, Joost Visser", "title": "Adversarial Examples - A Complete Characterisation of the Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete characterisation of the phenomenon of adversarial\nexamples - inputs intentionally crafted to fool machine learning models. We aim\nto cover all the important concerns in this field of study: (1) the conjectures\non the existence of adversarial examples, (2) the security, safety and\nrobustness implications, (3) the methods used to generate and (4) protect\nagainst adversarial examples and (5) the ability of adversarial examples to\ntransfer between different machine learning models. We provide ample background\ninformation in an effort to make this document self-contained. Therefore, this\ndocument can be used as survey, tutorial or as a catalog of attacks and\ndefences using adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 11:54:51 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 21:48:42 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Serban", "Alexandru Constantin", ""], ["Poll", "Erik", ""], ["Visser", "Joost", ""]]}, {"id": "1810.01222", "submitter": "Olivier Sigaud", "authors": "Alo\\\"is Pourchot and Olivier Sigaud", "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy\n  search", "comments": "accepted at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are\ntwo popular approaches to policy search. The former is widely applicable and\nrather stable, but suffers from low sample efficiency. By contrast, the latter\nis more sample efficient, but the most sample efficient variants are also\nrather unstable and highly sensitive to hyper-parameter setting. So far, these\nfamilies of methods have mostly been compared as competing tools. However, an\nemerging approach consists in combining them so as to get the best of both\nworlds. Two previously existing combinations use either an ad hoc evolutionary\nalgorithm or a goal exploration process together with the Deep Deterministic\nPolicy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL\nalgorithm. In this paper, we propose a different combination scheme using the\nsimple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy\ngradient (td3), another off-policy deep RL algorithm which improves over ddpg.\nWe evaluate the resulting method, cem-rl, on a set of benchmarks classically\nused in deep RL. We show that cem-rl benefits from several advantages over its\ncompetitors and offers a satisfactory trade-off between performance and sample\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 13:12:13 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 13:32:11 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 14:11:24 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Pourchot", "Alo\u00efs", ""], ["Sigaud", "Olivier", ""]]}, {"id": "1810.01322", "submitter": "L\\'eonard Blier", "authors": "L\\'eonard Blier, Pierre Wolinski, Yann Ollivier", "title": "Learning with Random Learning Rates", "comments": "20 pages, 8 figures, code available on GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter tuning is a bothersome step in the training of deep learning\nmodels. One of the most sensitive hyperparameters is the learning rate of the\ngradient descent. We present the 'All Learning Rates At Once' (Alrao)\noptimization method for neural networks: each unit or feature in the network\ngets its own learning rate sampled from a random distribution spanning several\norders of magnitude. This comes at practically no computational cost. Perhaps\nsurprisingly, stochastic gradient descent (SGD) with Alrao performs close to\nSGD with an optimally tuned learning rate, for various architectures and\nproblems. Alrao could save time when testing deep learning models: a range of\nmodels could be quickly assessed with Alrao, and the most promising models\ncould then be trained more extensively. This text comes with a PyTorch\nimplementation of the method, which can be plugged on an existing PyTorch\nmodel: https://github.com/leonardblier/alrao .\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 15:21:07 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 21:52:49 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 14:29:19 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Blier", "L\u00e9onard", ""], ["Wolinski", "Pierre", ""], ["Ollivier", "Yann", ""]]}, {"id": "1810.01406", "submitter": "Ke Li", "authors": "Ke Li, Shichong Peng, Jitendra Malik", "title": "Super-Resolution via Conditional Implicit Maximum Likelihood Estimation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution (SISR) is a canonical problem with diverse\napplications. Leading methods like SRGAN produce images that contain various\nartifacts, such as high-frequency noise, hallucinated colours and shape\ndistortions, which adversely affect the realism of the result. In this paper,\nwe propose an alternative approach based on an extension of the method of\nImplicit Maximum Likelihood Estimation (IMLE). We demonstrate greater\neffectiveness at noise reduction and preservation of the original colours and\nshapes, yielding more realistic super-resolved images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:58:02 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Malik", "Jitendra", ""]]}, {"id": "1810.01742", "submitter": "Pietro Verzelli", "authors": "Pietro Verzelli, Lorenzo Livi and Cesare Alippi", "title": "A characterization of the Edge of Criticality in Binary Echo State\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/MLSP.2018.8516959", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo State Networks (ESNs) are simplified recurrent neural network models\ncomposed of a reservoir and a linear, trainable readout layer. The reservoir is\ntunable by some hyper-parameters that control the network behaviour. ESNs are\nknown to be effective in solving tasks when configured on a region in\n(hyper-)parameter space called \\emph{Edge of Criticality} (EoC), where the\nsystem is maximally sensitive to perturbations hence affecting its behaviour.\nIn this paper, we propose binary ESNs, which are architecturally equivalent to\nstandard ESNs but consider binary activation functions and binary recurrent\nweights. For these networks, we derive a closed-form expression for the EoC in\nthe autonomous case and perform simulations in order to assess their behavior\nin the case of noisy neurons and in the presence of a signal. We propose a\ntheoretical explanation for the fact that the variance of the input plays a\nmajor role in characterizing the EoC.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 14:08:34 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Verzelli", "Pietro", ""], ["Livi", "Lorenzo", ""], ["Alippi", "Cesare", ""]]}, {"id": "1810.01869", "submitter": "David Noever", "authors": "David Noever", "title": "Machine Learning Suites for Online Toxicity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify and classify toxic online commentary, the modern tools of data\nscience transform raw text into key features from which either thresholding or\nlearning algorithms can make predictions for monitoring offensive\nconversations. We systematically evaluate 62 classifiers representing 19 major\nalgorithmic families against features extracted from the Jigsaw dataset of\nWikipedia comments. We compare the classifiers based on statistically\nsignificant differences in accuracy and relative execution time. Among these\nclassifiers for identifying toxic comments, tree-based algorithms provide the\nmost transparently explainable rules and rank-order the predictive contribution\nof each feature. Among 28 features of syntax, sentiment, emotion and outlier\nword dictionaries, a simple bad word list proves most predictive of offensive\ncommentary.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:22:44 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Noever", "David", ""]]}, {"id": "1810.01916", "submitter": "Aydogan Ozcan", "authors": "Deniz Mengu, Yi Luo, Yair Rivenson, Aydogan Ozcan", "title": "Analysis of Diffractive Optical Neural Networks and Their Integration\n  with Electronic Neural Networks", "comments": "22 pages, 5 Figures, 4 Tables, 1 Supplementary Figure, 2\n  Supplementary Tables", "journal-ref": "IEEE Journal of Selected Topics in Quantum Electronics (2019)", "doi": "10.1109/JSTQE.2019.2921376", "report-no": null, "categories": "cs.NE cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical machine learning offers advantages in terms of power efficiency,\nscalability and computation speed. Recently, an optical machine learning method\nbased on Diffractive Deep Neural Networks (D2NNs) has been introduced to\nexecute a function as the input light diffracts through passive layers,\ndesigned by deep learning using a computer. Here we introduce improvements to\nD2NNs by changing the training loss function and reducing the impact of\nvanishing gradients in the error back-propagation step. Using five phase-only\ndiffractive layers, we numerically achieved a classification accuracy of 97.18%\nand 89.13% for optical recognition of handwritten digits and fashion products,\nrespectively; using both phase and amplitude modulation (complex-valued) at\neach layer, our inference performance improved to 97.81% and 89.32%,\nrespectively. Furthermore, we report the integration of D2NNs with electronic\nneural networks to create hybrid-classifiers that significantly reduce the\nnumber of input pixels into an electronic network using an ultra-compact\nfront-end D2NN with a layer-to-layer distance of a few wavelengths, also\nreducing the complexity of the successive electronic network. Using a 5-layer\nphase-only D2NN jointly-optimized with a single fully-connected electronic\nlayer, we achieved a classification accuracy of 98.71% and 90.04% for the\nrecognition of handwritten digits and fashion products, respectively. Moreover,\nthe input to the electronic network was compressed by >7.8 times down to 10x10\npixels. Beyond creating low-power and high-frame rate machine learning\nplatforms, D2NN-based hybrid neural networks will find applications in smart\noptical imager and sensor design.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 18:59:42 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 00:14:13 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 23:24:16 GMT"}, {"version": "v4", "created": "Sat, 8 Jun 2019 01:43:00 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Mengu", "Deniz", ""], ["Luo", "Yi", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1810.02225", "submitter": "Fan Zhang", "authors": "Fan Zhang, Miao Hu", "title": "Memristor-based Deep Convolution Neural Network: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we firstly introduce a method to efficiently implement\nlarge-scale high-dimensional convolution with realistic memristor-based circuit\ncomponents. An experiment verified simulator is adapted for accurate prediction\nof analog crossbar behavior. An improved conversion algorithm is developed to\nconvert convolution kernels to memristor-based circuits, which minimizes the\nerror with consideration of the data and kernel patterns in CNNs. With circuit\nsimulation for all convolution layers in ResNet-20, we found that 8-bit ADC/DAC\nis necessary to preserve software level classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 18:47:34 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Zhang", "Fan", ""], ["Hu", "Miao", ""]]}, {"id": "1810.02244", "submitter": "Christopher Morris", "authors": "Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton,\n  Jan Eric Lenssen, Gaurav Rattan, Martin Grohe", "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks", "comments": "Extended version with proofs, accepted at AAAI 2019, added units of\n  measurement of QM9 dataset into appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, graph neural networks (GNNs) have emerged as a powerful\nneural architecture to learn vector representations of nodes and graphs in a\nsupervised, end-to-end fashion. Up to now, GNNs have only been evaluated\nempirically---showing promising results. The following work investigates GNNs\nfrom a theoretical point of view and relates them to the $1$-dimensional\nWeisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have\nthe same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic\n(sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on\nthis, we propose a generalization of GNNs, so-called $k$-dimensional GNNs\n($k$-GNNs), which can take higher-order graph structures at multiple scales\ninto account. These higher-order structures play an essential role in the\ncharacterization of social networks and molecule graphs. Our experimental\nevaluation confirms our theoretical findings as well as confirms that\nhigher-order information is useful in the task of graph classification and\nregression.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 14:31:57 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 12:52:37 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 15:55:24 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Morris", "Christopher", ""], ["Ritzert", "Martin", ""], ["Fey", "Matthias", ""], ["Hamilton", "William L.", ""], ["Lenssen", "Jan Eric", ""], ["Rattan", "Gaurav", ""], ["Grohe", "Martin", ""]]}, {"id": "1810.02272", "submitter": "Dave Brown", "authors": "David W. Brown", "title": "MyCaffe: A Complete C# Re-Write of Caffe with Reinforcement Learning", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years Caffe, from Berkeley AI Research, has gained a strong\nfollowing in the deep learning community with over 15K forks on the\ngithub.com/BLVC/Caffe site. With its well organized, very modular C++ design it\nis easy to work with and very fast. However, in the world of Windows\ndevelopment, C# has helped accelerate development with many of the enhancements\nthat it offers over C++, such as garbage collection, a very rich .NET\nprogramming framework and easy database access via Entity Frameworks. So how\ncan a C# developer use the advances of C# to take full advantage of the\nbenefits offered by the Berkeley Caffe deep learning system? The answer is the\nfully open source, 'MyCaffe' for Windows .NET programmers. MyCaffe is an open\nsource, complete C# language re-write of Berkeley's Caffe. This article\ndescribes the general architecture of MyCaffe including the newly added\nMyCaffeTrainerRL for Reinforcement Learning. In addition, this article\ndiscusses how MyCaffe closely follows the C++ Caffe, while talking efficiently\nto the low level NVIDIA CUDA hardware to offer a high performance, highly\nprogrammable deep learning system for Windows .NET programmers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:15:36 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Brown", "David W.", ""]]}, {"id": "1810.02281", "submitter": "Nadav Cohen", "authors": "Sanjeev Arora, Nadav Cohen, Noah Golowich, Wei Hu", "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural\n  Networks", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze speed of convergence to global optimum for gradient descent\ntraining a deep linear neural network (parameterized as $x \\mapsto W_N W_{N-1}\n\\cdots W_1 x$) by minimizing the $\\ell_2$ loss over whitened data. Convergence\nat a linear rate is guaranteed when the following hold: (i) dimensions of\nhidden layers are at least the minimum of the input and output dimensions; (ii)\nweight matrices at initialization are approximately balanced; and (iii) the\ninitial loss is smaller than the loss of any rank-deficient solution. The\nassumptions on initialization (conditions (ii) and (iii)) are necessary, in the\nsense that violating any one of them may lead to convergence failure. Moreover,\nin the important case of output dimension 1, i.e. scalar regression, they are\nmet, and thus convergence to global optimum holds, with constant probability\nunder a random initialization scheme. Our results significantly extend previous\nanalyses, e.g., of deep linear residual networks (Bartlett et al., 2018).\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:53:32 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:40:08 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 06:58:22 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Arora", "Sanjeev", ""], ["Cohen", "Nadav", ""], ["Golowich", "Noah", ""], ["Hu", "Wei", ""]]}, {"id": "1810.02328", "submitter": "Gerald Friedland", "authors": "Gerald Friedland, Alfredo Metere, Mario Krell", "title": "A Practical Approach to Sizing Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "LLNL Technical Report 758456", "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memorization is worst-case generalization. Based on MacKay's information\ntheoretic model of supervised machine learning, this article discusses how to\npractically estimate the maximum size of a neural network given a training data\nset. First, we present four easily applicable rules to analytically determine\nthe capacity of neural network architectures. This allows the comparison of the\nefficiency of different network architectures independently of a task. Second,\nwe introduce and experimentally validate a heuristic method to estimate the\nneural network capacity requirement for a given dataset and labeling. This\nallows an estimate of the required size of a neural network for a given\nproblem. We conclude the article with a discussion on the consequences of\nsizing the network wrongly, which includes both increased computation effort\nfor training as well as reduced generalization capability.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:20:39 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Friedland", "Gerald", ""], ["Metere", "Alfredo", ""], ["Krell", "Mario", ""]]}, {"id": "1810.02679", "submitter": "Giovanni Iacca Dr.", "authors": "Giovanni Iacca", "title": "Distributed optimization in wireless sensor networks: an island-model\n  framework", "comments": null, "journal-ref": "Soft Computing, Volume 17, pp 2257-2277, 2013", "doi": "10.1007/s00500-013-1091-x", "report-no": null, "categories": "cs.NE cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Sensor Networks (WSNs) is an emerging technology in several\napplication domains, ranging from urban surveillance to environmental and\nstructural monitoring. Computational Intelligence (CI) techniques are\nparticularly suitable for enhancing these systems. However, when embedding CI\ninto wireless sensors, severe hardware limitations must be taken into account.\nIn this paper we investigate the possibility to perform an online, distributed\noptimization process within a WSN. Such a system might be used, for example, to\nimplement advanced network features like distributed modelling, self-optimizing\nprotocols, and anomaly detection, to name a few. The proposed approach, called\nDOWSN (Distributed Optimization for WSN) is an island-model infrastructure in\nwhich each node executes a simple, computationally cheap (both in terms of CPU\nand memory) optimization algorithm, and shares promising solutions with its\nneighbors. We perform extensive tests of different DOWSN configurations on a\nbenchmark made up of continuous optimization problems; we analyze the influence\nof the network parameters (number of nodes, inter-node communication period and\nprobability of accepting incoming solutions) on the optimization performance.\nFinally, we profile energy and memory consumption of DOWSN to show the\nefficient usage of the limited hardware resources available on the sensor\nnodes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:44:59 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Iacca", "Giovanni", ""]]}, {"id": "1810.02702", "submitter": "Giovanni Iacca Dr.", "authors": "A. Maesani, G. Iacca, D. Floreano", "title": "Memetic Viability Evolution for Constrained Optimization", "comments": null, "journal-ref": "IEEE Transactions on Evolutionary Computation, Volume 20, pp\n  125-144, 2016", "doi": "10.1109/TEVC.2015.2428292", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of evolutionary algorithms can be heavily undermined when\nconstraints limit the feasible areas of the search space. For instance, while\nCovariance Matrix Adaptation Evolution Strategy is one of the most efficient\nalgorithms for unconstrained optimization problems, it cannot be readily\napplied to constrained ones. Here, we used concepts from Memetic Computing,\ni.e. the harmonious combination of multiple units of algorithmic information,\nand Viability Evolution, an alternative abstraction of artificial evolution, to\ndevise a novel approach for solving optimization problems with inequality\nconstraints. Viability Evolution emphasizes elimination of solutions not\nsatisfying viability criteria, defined as boundaries on objectives and\nconstraints. These boundaries are adapted during the search to drive a\npopulation of local search units, based on Covariance Matrix Adaptation\nEvolution Strategy, towards feasible regions. These units can be recombined by\nmeans of Differential Evolution operators. Of crucial importance for the\nperformance of our method, an adaptive scheduler toggles between exploitation\nand exploration by selecting to advance one of the local search units and/or\nrecombine them. The proposed algorithm can outperform several state-of-the-art\nmethods on a diverse set of benchmark and engineering problems, both for\nquality of solutions and computational resources needed.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:15:11 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Maesani", "A.", ""], ["Iacca", "G.", ""], ["Floreano", "D.", ""]]}, {"id": "1810.02713", "submitter": "Giovanni Iacca Dr.", "authors": "D. Bucur, G. Iacca, M. Gaudesi, G. Squillero, A. Tonda", "title": "Optimizing groups of colluding strong attackers in mobile urban\n  communication networks with evolutionary algorithms", "comments": null, "journal-ref": "Applied Soft Computing, Volume 40, pp 416-426, 2016", "doi": "10.1016/j.asoc.2015.11.024", "report-no": null, "categories": "cs.NE cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In novel forms of the Social Internet of Things, any mobile user within\ncommunication range may help routing messages for another user in the network.\nThe resulting message delivery rate depends both on the users' mobility\npatterns and the message load in the network. This new type of configuration,\nhowever, poses new challenges to security, amongst them, assessing the effect\nthat a group of colluding malicious participants can have on the global message\ndelivery rate in such a network is far from trivial. In this work, after\nmodeling such a question as an optimization problem, we are able to find quite\ninteresting results by coupling a network simulator with an evolutionary\nalgorithm. The chosen algorithm is specifically designed to solve problems\nwhose solutions can be decomposed into parts sharing the same structure. We\ndemonstrate the effectiveness of the proposed approach on two medium-sized\nDelay-Tolerant Networks, realistically simulated in the urban contexts of two\ncities with very different route topology: Venice and San Francisco. In all\nexperiments, our methodology produces attack patterns that greatly lower\nnetwork performance with respect to previous studies on the subject, as the\nevolutionary core is able to exploit the specific weaknesses of each target\nconfiguration.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:25:50 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Bucur", "D.", ""], ["Iacca", "G.", ""], ["Gaudesi", "M.", ""], ["Squillero", "G.", ""], ["Tonda", "A.", ""]]}, {"id": "1810.03198", "submitter": "Jitin Kapila", "authors": "Kumarjit Pathak, Jitin Kapila", "title": "Reinforcement Evolutionary Learning Method for self-learning", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In statistical modelling the biggest threat is concept drift which makes the\nmodel gradually showing deteriorating performance over time. There are state of\nthe art methodologies to detect the impact of concept drift, however general\nstrategy considered to overcome the issue in performance is to rebuild or\nre-calibrate the model periodically as the variable patterns for the model\nchanges significantly due to market change or consumer behavior change etc.\nQuantitative research is the most widely spread application of data science in\nMarketing or financial domain where applicability of state of the art\nreinforcement learning for auto-learning is less explored paradigm.\nReinforcement learning is heavily dependent on having a simulated environment\nwhich is majorly available for gaming or online systems, to learn from the live\nfeedback. However, there are some research happened on the area of online\nadvertisement, pricing etc where due to the nature of the online learning\nenvironment scope of reinforcement learning is explored. Our proposed solution\nis a reinforcement learning based, true self-learning algorithm which can adapt\nto the data change or concept drift and auto learn and self-calibrate for the\nnew patterns of the data solving the problem of concept drift.\n  Keywords - Reinforcement learning, Genetic Algorithm, Q-learning,\nClassification modelling, CMA-ES, NES, Multi objective optimization, Concept\ndrift, Population stability index, Incremental learning, F1-measure, Predictive\nModelling, Self-learning, MCTS, AlphaGo, AlphaZero\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 19:25:48 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Pathak", "Kumarjit", ""], ["Kapila", "Jitin", ""]]}, {"id": "1810.03199", "submitter": "Blake Bordelon", "authors": "Bryce Bagley, Blake Bordelon, Benjamin Moseley, Ralf Wessel", "title": "Pre-Synaptic Pool Modification (PSPM): A Supervised Learning Procedure\n  for Spiking Neural Networks", "comments": "24 pages, 8 figures, Code and data can be found at\n  https://github.com/blakebordelon/Spiking-Neural-Network-Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning synaptic weights of spiking neural network (SNN) models that can\nreproduce target spike trains from provided neural firing data is a central\nproblem in computational neuroscience and spike-based computing. The discovery\nof the optimal weight values can be posed as a supervised learning task wherein\nthe weights of the model network are chosen to maximize the similarity between\nthe target spike trains and the model outputs. It is still largely unknown\nwhether optimizing spike train similarity of highly recurrent SNNs produces\nweight matrices similar to those of the ground truth model. To this end, we\npropose flexible heuristic supervised learning rules, termed Pre-Synaptic Pool\nModification (PSPM), that rely on stochastic weight updates in order to produce\nspikes within a short window of the desired times and eliminate spikes outside\nof this window. PSPM improves spike train similarity for all-to-all SNNs and\nmakes no assumption about the post-synaptic potential of the neurons or the\nstructure of the network since no gradients are required. We test whether\noptimizing for spike train similarity entails the discovery of accurate weights\nand explore the relative contributions of local and homeostatic weight updates.\nAlthough PSPM improves similarity between spike trains, the learned weights\noften differ from the weights of the ground truth model, implying that\nconnectome inference from spike data may require additional constraints on\nconnectivity statistics. We also find that spike train similarity is sensitive\nto local updates, but other measures of network activity such as avalanche\ndistributions, can be learned through synaptic homeostasis.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 19:43:09 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 13:18:06 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 23:38:08 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Bagley", "Bryce", ""], ["Bordelon", "Blake", ""], ["Moseley", "Benjamin", ""], ["Wessel", "Ralf", ""]]}, {"id": "1810.03377", "submitter": "Matthias Freiberger", "authors": "Matthias Freiberger, Andrew Katumba, Peter Bienstman and Joni Dambre", "title": "Training Passive Photonic Reservoirs with Integrated Optical Readout", "comments": "Accepted for publication in IEEE Transactions on Neural Networks and\n  Learning Systems (TNNLS-2017-P-8539.R1), copyright 2018 IEEE. This research\n  was funded by the EU Horizon 2020 PHRESCO Grant (Grant No. 688579) and the\n  BELSPO IAP P7-35 program Photonics@be. 11 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TNNLS.2018.2874571", "report-no": null, "categories": "cs.NE cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Moore's law comes to an end, neuromorphic approaches to computing are on\nthe rise. One of these, passive photonic reservoir computing, is a strong\ncandidate for computing at high bitrates (> 10 Gbps) and with low energy\nconsumption. Currently though, both benefits are limited by the necessity to\nperform training and readout operations in the electrical domain. Thus, efforts\nare currently underway in the photonic community to design an integrated\noptical readout, which allows to perform all operations in the optical domain.\nIn addition to the technological challenge of designing such a readout, new\nalgorithms have to be designed in order to train it. Foremost, suitable\nalgorithms need to be able to deal with the fact that the actual on-chip\nreservoir states are not directly observable. In this work, we investigate\nseveral options for such a training algorithm and propose a solution in which\nthe complex states of the reservoir can be observed by appropriately setting\nthe readout weights, while iterating over a predefined input sequence. We\nperform numerical simulations in order to compare our method with an ideal\nbaseline requiring full observability as well as with an established black-box\noptimization approach (CMA-ES).\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 11:26:08 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Freiberger", "Matthias", ""], ["Katumba", "Andrew", ""], ["Bienstman", "Peter", ""], ["Dambre", "Joni", ""]]}, {"id": "1810.03522", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb,\n  Erik Goodman and Wolfgang Banzhaf", "title": "NSGA-Net: Neural Architecture Search using Multi-Objective Genetic\n  Algorithm", "comments": "GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces NSGA-Net -- an evolutionary approach for neural\narchitecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a\nprocedure considering multiple and conflicting objectives, (2) an efficient\nprocedure balancing exploration and exploitation of the space of potential\nneural network architectures, and (3) a procedure finding a diverse set of\ntrade-off network architectures achieved in a single run. NSGA-Net is a\npopulation-based search algorithm that explores a space of potential neural\nnetwork architectures in three steps, namely, a population initialization step\nthat is based on prior-knowledge from hand-crafted architectures, an\nexploration step comprising crossover and mutation of architectures, and\nfinally an exploitation step that utilizes the hidden useful knowledge stored\nin the entire history of evaluated neural architectures in the form of a\nBayesian Network. Experimental results suggest that combining the dual\nobjectives of minimizing an error metric and computational complexity, as\nmeasured by FLOPs, allows NSGA-Net to find competitive neural architectures.\nMoreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with\nother state-of-the-art NAS methods while using orders of magnitude less\ncomputational resources. These results are encouraging and shows the promise to\nfurther use of EC methods in various deep-learning paradigms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 15:14:33 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 23:07:16 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lu", "Zhichao", ""], ["Whalen", "Ian", ""], ["Boddeti", "Vishnu", ""], ["Dhebar", "Yashesh", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""], ["Banzhaf", "Wolfgang", ""]]}, {"id": "1810.03652", "submitter": "Camila P.S. Tautenhain", "authors": "Camila P.S. Tautenhain and Mari\\'a C.V. Nascimento", "title": "An ensemble based on a bi-objective evolutionary spectral algorithm for\n  graph clustering", "comments": "Preprint accepted for publication in Expert Systems with Applications", "journal-ref": null, "doi": "10.1016/j.eswa.2019.112911", "report-no": null, "categories": "cs.SI cs.LG cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is a challenging pattern recognition problem whose goal is\nto identify vertex partitions with high intra-group connectivity. This paper\ninvestigates a bi-objective problem that maximizes the number of intra-cluster\nedges of a graph and minimizes the expected number of inter-cluster edges in a\nrandom graph with the same degree sequence as the original one. The difference\nbetween the two investigated objectives is the definition of the well-known\nmeasure of graph clustering quality: the modularity. We introduce a spectral\ndecomposition hybridized with an evolutionary heuristic, called MOSpecG, to\napproach this bi-objective problem and an ensemble strategy to consolidate the\nsolutions found by MOSpecG into a final robust partition. The results of\ncomputational experiments with real and artificial LFR networks demonstrated a\nsignificant improvement in the results and performance of the introduced method\nin regard to another bi-objective algorithm found in the literature. The\ncrossover operator based on the geometric interpretation of the modularity\nmaximization problem to match the communities of a pair of individuals was of\nutmost importance for the good performance of MOSpecG. Hybridizing spectral\ngraph theory and intelligent systems allowed us to define significantly\nhigh-quality community structures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 18:36:19 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 14:50:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tautenhain", "Camila P. S.", ""], ["Nascimento", "Mari\u00e1 C. V.", ""]]}, {"id": "1810.03946", "submitter": "Berton Huang", "authors": "Xiaobo Huang", "title": "Convolutional Neural Networks In Convolution", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Currently, increasingly deeper neural networks have been applied to improve\ntheir accuracy. In contrast, We propose a novel wider Convolutional Neural\nNetworks (CNN) architecture, motivated by the Multi-column Deep Neural Networks\nand the Network In Network(NIN), aiming for higher accuracy without input data\ntransmutation. In our architecture, namely \"CNN In Convolution\"(CNNIC), a small\nCNN, instead of the original generalized liner model(GLM) based filters, is\nconvoluted as kernel on the original image, serving as feature extracting layer\nof this networks. And further classifications are then carried out by a global\naverage pooling layer and a softmax layer. Dropout and orthonormal\ninitialization are applied to overcome training difficulties including slow\nconvergence and over-fitting. Persuasive classification performance is\ndemonstrated on MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 12:59:12 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Huang", "Xiaobo", ""]]}, {"id": "1810.03974", "submitter": "Dmitry Yarotsky", "authors": "Dmitry Yarotsky", "title": "Collective evolution of weights in wide neural networks", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a nonlinear integro-differential transport equation describing\ncollective evolution of weights under gradient descent in large-width\nneural-network-like models. We characterize stationary points of the evolution\nand analyze several scenarios where the transport equation can be solved\napproximately. We test our general method in the special case of linear\nfree-knot splines, and find good agreement between theory and experiment in\nobservations of global optima, stability of stationary points, and convergence\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 13:42:11 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Yarotsky", "Dmitry", ""]]}, {"id": "1810.04119", "submitter": "Dennis George Wilson", "authors": "DG Wilson, Julian F. Miller, Sylvain Cussat-Blanc, Herv\\'e Luga", "title": "Positional Cartesian Genetic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cartesian Genetic Programming (CGP) has many modifications across a variety\nof implementations, such as recursive connections and node weights. Alternative\ngenetic operators have also been proposed for CGP, but have not been fully\nstudied. In this work, we present a new form of genetic programming based on a\nfloating point representation. In this new form of CGP, called Positional CGP,\nnode positions are evolved. This allows for the evaluation of many different\ngenetic operators while allowing for previous CGP improvements like recurrency.\nUsing nine benchmark problems from three different classes, we evaluate the\noptimal parameters for CGP and PCGP, including novel genetic operators.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:49:04 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Wilson", "DG", ""], ["Miller", "Julian F.", ""], ["Cussat-Blanc", "Sylvain", ""], ["Luga", "Herv\u00e9", ""]]}, {"id": "1810.04122", "submitter": "Jennifer John", "authors": "Jennifer N. John, Conner Galloway, Alexander Valys", "title": "Deep Convolutional Neural Networks for Noise Detection in ECGs", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile electrocardiogram (ECG) recording technologies represent a promising\ntool to fight the ongoing epidemic of cardiovascular diseases, which are\nresponsible for more deaths globally than any other cause. While the ability to\nmonitor one's heart activity at any time in any place is a crucial advantage of\nsuch technologies, it is also the cause of a drawback: signal noise due to\nenvironmental factors can render the ECGs illegible. In this work, we develop\nconvolutional neural networks (CNNs) to automatically label ECGs for noise,\ntraining them on a novel noise-annotated dataset. By reducing distraction from\nnoisy intervals of signals, such networks have the potential to increase the\naccuracy of models for the detection of atrial fibrillation, long QT syndrome,\nand other cardiovascular conditions. Comparing several architectures, we find\nthat a 16-layer CNN adapted from the VGG16 network which generates one\nprediction per second on a 10-second input performs exceptionally well on this\ntask, with an AUC of 0.977.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 02:59:04 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["John", "Jennifer N.", ""], ["Galloway", "Conner", ""], ["Valys", "Alexander", ""]]}, {"id": "1810.04384", "submitter": "Aydogan Ozcan", "authors": "Deniz Mengu, Yi Luo, Yair Rivenson, Xing Lin, Muhammed Veli, Aydogan\n  Ozcan", "title": "Response to Comment on \"All-optical machine learning using diffractive\n  deep neural networks\"", "comments": "Response to arXiv:1809.08360v1 [cs.LG]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their Comment, Wei et al. (arXiv:1809.08360v1 [cs.LG]) claim that our\noriginal interpretation of Diffractive Deep Neural Networks (D2NN) represent a\nmischaracterization of the system due to linearity and passivity. In this\nResponse, we detail how this mischaracterization claim is unwarranted and\noblivious to several sections detailed in our original manuscript (Science,\nDOI: 10.1126/science.aat8084) that specifically introduced and discussed\noptical nonlinearities and reconfigurability of D2NNs, as part of our proposed\nframework to enhance its performance. To further refute the mischaracterization\nclaim of Wei et al., we, once again, demonstrate the depth feature of optical\nD2NNs by showing that multiple diffractive layers operating collectively within\na D2NN present additional degrees-of-freedom compared to a single diffractive\nlayer to achieve better classification accuracy, as well as improved output\nsignal contrast and diffraction efficiency as the number of diffractive layers\nincrease, showing the deepness of a D2NN, and its inherent depth advantage for\nimproved performance. In summary, the Comment by Wei et al. does not provide an\namendment to the original teachings of our original manuscript, and all of our\nresults, core conclusions and methodology of research reported in Science (DOI:\n10.1126/science.aat8084) remain entirely valid.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 06:32:49 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Mengu", "Deniz", ""], ["Luo", "Yi", ""], ["Rivenson", "Yair", ""], ["Lin", "Xing", ""], ["Veli", "Muhammed", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1810.04735", "submitter": "Jack Collins", "authors": "Jack Collins, Wade Geles, David Howard, Frederic Maire", "title": "Towards the Targeted Environment-Specific Evolution of Robot Components", "comments": "8 pages appearing in the 2018 Genetic and Evolutionary Computation\n  Conference Proceedings", "journal-ref": null, "doi": "10.1145/3205455.3205541", "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research considers the task of evolving the physical structure of a\nrobot to enhance its performance in various environments, which is a\nsignificant problem in the field of Evolutionary Robotics. Inspired by the\nfields of evolutionary art and sculpture, we evolve only targeted parts of a\nrobot, which simplifies the optimisation problem compared to traditional\napproaches that must simultaneously evolve both (actuated) body and brain.\nExploration fidelity is emphasised in areas of the robot most likely to benefit\nfrom shape optimisation, whilst exploiting existing robot structure and\ncontrol. Our approach uses a Genetic Algorithm to optimise collections of\nBezier splines that together define the shape of a legged robot's tibia, and\nleg performance is evaluated in parallel in a high-fidelity simulator. The leg\nis represented in the simulator as 3D-printable file, and as such can be\nreadily instantiated in reality. Provisional experiments in three distinct\nenvironments show the evolution of environment-specific leg structures that are\nboth high-performing and notably different to those evolved in the other\nenvironments. This proof-of-concept represents an important step towards the\nenvironment-dependent optimisation of performance-critical components for a\nrange of ubiquitous, standard, and already-capable robots that can carry out a\nwide variety of tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 03:02:07 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Collins", "Jack", ""], ["Geles", "Wade", ""], ["Howard", "David", ""], ["Maire", "Frederic", ""]]}, {"id": "1810.05018", "submitter": "Giovanni Iacca Dr.", "authors": "Giovanni Iacca, Fabio Caraffini, Ferrante Neri", "title": "Multi-Strategy Coevolving Aging Particle Optimization", "comments": null, "journal-ref": "International Journal of Neural Systems, Volume 24, Issue 1,\n  December 2013", "doi": "10.1142/S0129065714500087", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Multi-Strategy Coevolving Aging Particles (MS-CAP), a novel\npopulation-based algorithm for black-box optimization. In a memetic fashion,\nMS-CAP combines two components with complementary algorithm logics. In the\nfirst stage, each particle is perturbed independently along each dimension with\na progressively shrinking (decaying) radius, and attracted towards the current\nbest solution with an increasing force. In the second phase, the particles are\nmutated and recombined according to a multi-strategy approach in the fashion of\nthe ensemble of mutation strategies in Differential Evolution. The proposed\nalgorithm is tested, at different dimensionalities, on two complete black-box\noptimization benchmarks proposed at the Congress on Evolutionary Computation\n2010 and 2013. To demonstrate the applicability of the approach, we also test\nMS-CAP to train a Feedforward Neural Network modelling the kinematics of an\n8-link robot manipulator. The numerical results show that MS-CAP, for the\nsetting considered in this study, tends to outperform the state-of-the-art\noptimization algorithms on a large set of problems, thus resulting in a robust\nand versatile optimizer.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 13:46:57 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Iacca", "Giovanni", ""], ["Caraffini", "Fabio", ""], ["Neri", "Ferrante", ""]]}, {"id": "1810.05045", "submitter": "Chao Qian", "authors": "Chao Qian, Chao Bian, Yang Yu, Ke Tang, Xin Yao", "title": "Analysis of Noisy Evolutionary Optimization When Sampling Fails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In noisy evolutionary optimization, sampling is a common strategy to deal\nwith noise. By the sampling strategy, the fitness of a solution is evaluated\nmultiple times (called \\emph{sample size}) independently, and its true fitness\nis then approximated by the average of these evaluations. Previous studies on\nsampling are mainly empirical. In this paper, we first investigate the effect\nof sample size from a theoretical perspective. By analyzing the (1+1)-EA on the\nnoisy LeadingOnes problem, we show that as the sample size increases, the\nrunning time can reduce from exponential to polynomial, but then return to\nexponential. This suggests that a proper sample size is crucial in practice.\nThen, we investigate what strategies can work when sampling with any fixed\nsample size fails. By two illustrative examples, we prove that using parent or\noffspring populations can be better. Finally, we construct an artificial noisy\nexample to show that when using neither sampling nor populations is effective,\nadaptive sampling (i.e., sampling with an adaptive sample size) can work. This,\nfor the first time, provides a theoretical support for the use of adaptive\nsampling.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 14:35:31 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Qian", "Chao", ""], ["Bian", "Chao", ""], ["Yu", "Yang", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""]]}, {"id": "1810.05148", "submitter": "Roman Novak", "authors": "Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri\n  Hron, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein", "title": "Bayesian Deep Convolutional Networks with Many Channels are Gaussian\n  Processes", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a previously identified equivalence between wide fully connected\nneural networks (FCNs) and Gaussian processes (GPs). This equivalence enables,\nfor instance, test set predictions that would have resulted from a fully\nBayesian, infinitely wide trained FCN to be computed without ever instantiating\nthe FCN, but by instead evaluating the corresponding GP. In this work, we\nderive an analogous equivalence for multi-layer convolutional neural networks\n(CNNs) both with and without pooling layers, and achieve state of the art\nresults on CIFAR10 for GPs without trainable kernels. We also introduce a Monte\nCarlo method to estimate the GP corresponding to a given neural network\narchitecture, even in cases where the analytic form has too many terms to be\ncomputationally feasible.\n  Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs\nwith and without weight sharing are identical. As a consequence, translation\nequivariance, beneficial in finite channel CNNs trained with stochastic\ngradient descent (SGD), is guaranteed to play no role in the Bayesian treatment\nof the infinite channel limit - a qualitative difference between the two\nregimes that is not present in the FCN case. We confirm experimentally, that\nwhile in some scenarios the performance of SGD-trained finite CNNs approaches\nthat of the corresponding GPs as the channel count increases, with careful\ntuning SGD-trained CNNs can significantly outperform their corresponding GPs,\nsuggesting advantages from SGD training compared to fully Bayesian parameter\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:49:41 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 00:38:34 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 04:42:51 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 15:28:27 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Novak", "Roman", ""], ["Xiao", "Lechao", ""], ["Lee", "Jaehoon", ""], ["Bahri", "Yasaman", ""], ["Yang", "Greg", ""], ["Hron", "Jiri", ""], ["Abolafia", "Daniel A.", ""], ["Pennington", "Jeffrey", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1810.05281", "submitter": "Carola Doerr", "authors": "Carola Doerr, Hao Wang, Furong Ye, Sander van Rijn, Thomas B\\\"ack", "title": "IOHprofiler: A Benchmarking and Profiling Tool for Iterative\n  Optimization Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IOHprofiler is a new tool for analyzing and comparing iterative optimization\nheuristics. Given as input algorithms and problems written in C or Python, it\nprovides as output a statistical evaluation of the algorithms' performance by\nmeans of the distribution on the fixed-target running time and the fixed-budget\nfunction values. In addition, IOHprofiler also allows to track the evolution of\nalgorithm parameters, making our tool particularly useful for the analysis,\ncomparison, and design of (self-)adaptive algorithms.\n  IOHprofiler is a ready-to-use software. It consists of two parts: an\nexperimental part, which generates the running time data, and a post-processing\npart, which produces the summarizing comparisons and statistical evaluations.\nThe experimental part is build on the COCO software, which has been adjusted to\ncope with optimization problems that are formulated as functions\n$f:\\mathcal{S}^n \\to \\R$ with $\\mathcal{S}$ being a discrete alphabet of\nintegers. The post-processing part is our own work. It can be used as a\nstand-alone tool for the evaluation of running time data of arbitrary benchmark\nproblems. It accepts as input files not only the output files of IOHprofiler,\nbut also original COCO data files. The post-processing tool is designed for an\ninteractive evaluation, allowing the user to chose the ranges and the precision\nof the displayed data according to his/her needs.\n  IOHprofiler is available on GitHub at \\url{https://github.com/IOHprofiler}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 22:53:59 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Doerr", "Carola", ""], ["Wang", "Hao", ""], ["Ye", "Furong", ""], ["van Rijn", "Sander", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1810.05474", "submitter": "Marc Tanti", "authors": "Marc Tanti and Albert Gatt and Adrian Muscat", "title": "Pre-gen metrics: Predicting caption quality metrics without generating\n  captions", "comments": "13 pages, 6 figures This publication will appear in the Proceedings\n  of the First Workshop on Shortcomings in Vision and Language (2018). DOI to\n  be inserted later", "journal-ref": null, "doi": "10.1007/978-3-030-11018-5_11", "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image caption generation systems are typically evaluated against reference\noutputs. We show that it is possible to predict output quality without\ngenerating the captions, based on the probability assigned by the neural model\nto the reference captions. Such pre-gen metrics are strongly correlated to\nstandard evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 12:19:56 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Muscat", "Adrian", ""]]}, {"id": "1810.05475", "submitter": "Marc Tanti", "authors": "Marc Tanti and Albert Gatt and Kenneth P. Camilleri", "title": "Quantifying the amount of visual information used by neural caption\n  generators", "comments": "10 pages, 4 figures This publication will appear in the Proceedings\n  of the First Workshop on Shortcomings in Vision and Language (2018). DOI to\n  be inserted later", "journal-ref": null, "doi": "10.1007/978-3-030-11018-5_11", "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the sensitivity of neural image caption generators to\ntheir visual input. A sensitivity analysis and omission analysis based on image\nfoils is reported, showing that the extent to which image captioning\narchitectures retain and are sensitive to visual information varies depending\non the type of word being generated and the position in the caption as a whole.\nWe motivate this work in the context of broader goals in the field to achieve\nmore explainability in AI.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 12:23:08 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Camilleri", "Kenneth P.", ""]]}, {"id": "1810.05486", "submitter": "Jun Haeng Lee", "authors": "Hyunsun Park, Jun Haeng Lee, Youngmin Oh, Sangwon Ha, Seungwon Lee", "title": "Training Deep Neural Network in Limited Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy and resource efficient training of DNNs will greatly extend the\napplications of deep learning. However, there are three major obstacles which\nmandate accurate calculation in high precision. In this paper, we tackle two of\nthem related to the loss of gradients during parameter update and\nbackpropagation through a softmax nonlinearity layer in low precision training.\nWe implemented SGD with Kahan summation by employing an additional parameter to\nvirtually extend the bit-width of the parameters for a reliable parameter\nupdate. We also proposed a simple guideline to help select the appropriate\nbit-width for the last FC layer followed by a softmax nonlinearity layer. It\ndetermines the lower bound of the required bit-width based on the class size of\nthe dataset. Extensive experiments on various network architectures and\nbenchmarks verifies the effectiveness of the proposed technique for low\nprecision training.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 12:58:18 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Park", "Hyunsun", ""], ["Lee", "Jun Haeng", ""], ["Oh", "Youngmin", ""], ["Ha", "Sangwon", ""], ["Lee", "Seungwon", ""]]}, {"id": "1810.05488", "submitter": "Jun Haeng Lee", "authors": "Jun Haeng Lee, Sangwon Ha, Saerom Choi, Won-Jo Lee, Seungwon Lee", "title": "Quantization for Rapid Deployment of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at rapid deployment of the state-of-the-art deep neural\nnetworks (DNNs) to energy efficient accelerators without time-consuming fine\ntuning or the availability of the full datasets. Converting DNNs in full\nprecision to limited precision is essential in taking advantage of the\naccelerators with reduced memory footprint and computation power. However, such\na task is not trivial since it often requires the full training and validation\ndatasets for profiling the network statistics and fine tuning the networks to\nrecover the accuracy lost after quantization. To address these issues, we\npropose a simple method recognizing channel-level distribution to reduce the\nquantization-induced accuracy loss and minimize the required image samples for\nprofiling. We evaluated our method on eleven networks trained on the ImageNet\nclassification benchmark and a network trained on the Pascal VOC object\ndetection benchmark. The results prove that the networks can be quantized into\n8-bit integer precision without fine tuning.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 13:06:49 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Lee", "Jun Haeng", ""], ["Ha", "Sangwon", ""], ["Choi", "Saerom", ""], ["Lee", "Won-Jo", ""], ["Lee", "Seungwon", ""]]}, {"id": "1810.05526", "submitter": "Bas van Stein", "authors": "Bas van Stein, Hao Wang, Thomas B\\\"ack", "title": "Automatic Configuration of Deep Neural Networks with EGO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing the architecture for an artificial neural network is a cumbersome\ntask because of the numerous parameters to configure, including activation\nfunctions, layer types, and hyper-parameters. With the large number of\nparameters for most networks nowadays, it is intractable to find a good\nconfiguration for a given task by hand. In this paper an Efficient Global\nOptimization (EGO) algorithm is adapted to automatically optimize and configure\nconvolutional neural network architectures. A configurable neural network\narchitecture based solely on convolutional layers is proposed for the\noptimization. Without using any knowledge on the target problem and not using\nany data augmentation techniques, it is shown that on several image\nclassification tasks this approach is able to find competitive network\narchitectures in terms of prediction accuracy, compared to the best\nhand-crafted ones in literature. In addition, a very small training budget (200\nevaluations and 10 epochs in training) is spent on each optimized architectures\nin contrast to the usual long training time of hand-crafted networks. Moreover,\ninstead of the standard sequential evaluation in EGO, several candidate\narchitectures are proposed and evaluated in parallel, which saves the execution\noverheads significantly and leads to an efficient automation for deep neural\nnetwork design.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 09:06:15 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["van Stein", "Bas", ""], ["Wang", "Hao", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1810.05597", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Jianwen Xie, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled\n  with Matrix Representation of Self-Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a representational model for grid cells. In this model,\nthe 2D self-position of the agent is represented by a high-dimensional vector,\nand the 2D self-motion or displacement of the agent is represented by a matrix\nthat transforms the vector. Each component of the vector is a unit or a cell.\nThe model consists of the following three sub-models. (1) Vector-matrix\nmultiplication. The movement from the current position to the next position is\nmodeled by matrix-vector multiplication, i.e., the vector of the next position\nis obtained by multiplying the matrix of the motion to the vector of the\ncurrent position. (2) Magnified local isometry. The angle between two nearby\nvectors equals the Euclidean distance between the two corresponding positions\nmultiplied by a magnifying factor. (3) Global adjacency kernel. The inner\nproduct between two vectors measures the adjacency between the two\ncorresponding positions, which is defined by a kernel function of the Euclidean\ndistance between the two positions. Our representational model has explicit\nalgebra and geometry. It can learn hexagon patterns of grid cells, and it is\ncapable of error correction, path integral and path planning.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 16:34:07 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 06:17:11 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 00:22:05 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Gao", "Ruiqi", ""], ["Xie", "Jianwen", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1810.06773", "submitter": "Xiaodong Cui", "authors": "Xiaodong Cui, Wei Zhang, Zolt\\'an T\\\"uske and Michael Picheny", "title": "Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD)\nframework for optimizing deep neural networks. ESGD combines SGD and\ngradient-free evolutionary algorithms as complementary algorithms in one\nframework in which the optimization alternates between the SGD step and\nevolution step to improve the average fitness of the population. With a\nback-off strategy in the SGD step and an elitist strategy in the evolution\nstep, it guarantees that the best fitness in the population will never degrade.\nIn addition, individuals in the population optimized with various SGD-based\noptimizers using distinct hyper-parameters in the SGD step are considered as\ncompeting species in a coevolution setting such that the complementarity of the\noptimizers is also taken into account. The effectiveness of ESGD is\ndemonstrated across multiple applications including speech recognition, image\nrecognition and language modeling, using networks with a variety of deep\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 01:12:06 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Cui", "Xiaodong", ""], ["Zhang", "Wei", ""], ["T\u00fcske", "Zolt\u00e1n", ""], ["Picheny", "Michael", ""]]}, {"id": "1810.06807", "submitter": "Kartik Hegde", "authors": "Kartik Hegde, Rohit Agrawal, Yulun Yao, Christopher W. Fletcher", "title": "Morph: Flexible Acceleration for 3D CNN-based Video Understanding", "comments": "Appears in the proceedings of the 51st Annual IEEE/ACM International\n  Symposium on Microarchitecture (MICRO), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past several years have seen both an explosion in the use of\nConvolutional Neural Networks (CNNs) and the design of accelerators to make CNN\ninference practical. In the architecture community, the lion share of effort\nhas targeted CNN inference for image recognition. The closely related problem\nof video recognition has received far less attention as an accelerator target.\nThis is surprising, as video recognition is more computationally intensive than\nimage recognition, and video traffic is predicted to be the majority of\ninternet traffic in the coming years.\n  This paper fills the gap between algorithmic and hardware advances for video\nrecognition by providing a design space exploration and flexible architecture\nfor accelerating 3D Convolutional Neural Networks (3D CNNs) - the core kernel\nin modern video understanding. When compared to (2D) CNNs used for image\nrecognition, efficiently accelerating 3D CNNs poses a significant engineering\nchallenge due to their large (and variable over time) memory footprint and\nhigher dimensionality.\n  To address these challenges, we design a novel accelerator, called Morph,\nthat can adaptively support different spatial and temporal tiling strategies\ndepending on the needs of each layer of each target 3D CNN. We codesign a\nsoftware infrastructure alongside the Morph hardware to find good-fit\nparameters to control the hardware. Evaluated on state-of-the-art 3D CNNs,\nMorph achieves up to 3.4x (2.5x average) reduction in energy consumption and\nimproves performance/watt by up to 5.1x (4x average) compared to a baseline 3D\nCNN accelerator, with an area overhead of 5%. Morph further achieves a 15.9x\naverage energy reduction on 3D CNNs when compared to Eyeriss.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 04:49:15 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hegde", "Kartik", ""], ["Agrawal", "Rohit", ""], ["Yao", "Yulun", ""], ["Fletcher", "Christopher W.", ""]]}, {"id": "1810.06835", "submitter": "Andrew Rowley", "authors": "Andrew G. D. Rowley, Christian Brenninkmeijer, Simon Davidson, Donal\n  Fellows, Andrew Gait, David R. Lester, Luis A. Plana, Oliver Rhodes, Alan B.\n  Stokes, Steve B. Furber", "title": "SpiNNTools: The Execution Engine for the SpiNNaker Platform", "comments": null, "journal-ref": null, "doi": "10.3389/fnins.2019.00231", "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems are becoming more common place, as computers typically\ncontain multiple computation processors. The SpiNNaker architecture is such a\ndistributed architecture, containing millions of cores connected with a unique\ncommunication network, making it one of the largest neuromorphic computing\nplatforms in the world. Utilising these processors efficiently usually requires\nexpert knowledge of the architecture to generate executable code. This work\nintroduces a set of tools (SpiNNTools) that can map computational work\ndescribed as a graph in to executable code that runs on this novel machine. The\nSpiNNaker architecture is highly scalable which in turn produces unique\nchallenges in loading data, executing the mapped problem and the retrieval of\ndata. In this paper we describe these challenges in detail and the solutions\nimplemented.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 06:35:09 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Rowley", "Andrew G. D.", ""], ["Brenninkmeijer", "Christian", ""], ["Davidson", "Simon", ""], ["Fellows", "Donal", ""], ["Gait", "Andrew", ""], ["Lester", "David R.", ""], ["Plana", "Luis A.", ""], ["Rhodes", "Oliver", ""], ["Stokes", "Alan B.", ""], ["Furber", "Steve B.", ""]]}, {"id": "1810.07074", "submitter": "Roman Yampolskiy", "authors": "Roman V. Yampolskiy", "title": "Why We Do Not Evolve Software? Analysis of Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review the state-of-the-art results in evolutionary\ncomputation and observe that we do not evolve non trivial software from scratch\nand with no human intervention. A number of possible explanations are\nconsidered, but we conclude that computational complexity of the problem\nprevents it from being solved as currently attempted. A detailed analysis of\nnecessary and available computational resources is provided to support our\nfindings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 19:59:48 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Yampolskiy", "Roman V.", ""]]}, {"id": "1810.07085", "submitter": "Stef Maree", "authors": "S.C. Maree, T. Alderliesten, D. Thierens, P.A.N. Bosman", "title": "Real-Valued Evolutionary Multi-Modal Optimization driven by Hill-Valley\n  Clustering", "comments": null, "journal-ref": "In Proceedings of the Genetic and Evolutionary Computation\n  Conference 2018, GECCO-2018, July 15-19, 2018, Kyoto, Japan. ACM, New York,\n  NY, USA", "doi": "10.1145/3205455.3205477", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based evolutionary algorithms (EAs) adapt an underlying search model to\nfeatures of the problem at hand, such as the linkage between problem variables.\nThe performance of EAs often deteriorates as multiple modes in the fitness\nlandscape are modelled with a unimodal search model. The number of modes is\nhowever often unknown a priori, especially in a black-box setting, which\ncomplicates adaptation of the search model. In this work, we focus on models\nthat can adapt to the multi-modality of the fitness landscape. Specifically, we\nintroduce Hill-Valley Clustering, a remarkably simple approach to adaptively\ncluster the search space in niches, such that a single mode resides in each\nniche. In each of the located niches, a core search algorithm is initialized to\noptimize that niche. Combined with an EA and a restart scheme, the resulting\nHill-Valley EA (HillVallEA) is compared to current state-of-the-art niching\nmethods on a standard benchmark suite for multi-modal optimization. Numerical\nresults in terms of the detected number of global optima show that, in spite of\nits simplicity, HillVallEA is competitive within the limited budget of the\nbenchmark suite, and shows superior performance in the long run.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 15:38:22 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Maree", "S. C.", ""], ["Alderliesten", "T.", ""], ["Thierens", "D.", ""], ["Bosman", "P. A. N.", ""]]}, {"id": "1810.07291", "submitter": "Mehran Pesteie", "authors": "Mehran Pesteie, Purang Abolmaesumi, Robert Rohling", "title": "Deep Neural Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new unsupervised representation learning and visualization\nusing deep convolutional networks and self organizing maps called Deep Neural\nMaps (DNM). DNM jointly learns an embedding of the input data and a mapping\nfrom the embedding space to a two-dimensional lattice. We compare\nvisualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 data\nsets. Our experiments show that the DNM can learn efficient representations of\nthe input data, which reflects characteristics of each class. This is shown via\nback-projecting the neurons of the map on the data space.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 21:59:47 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Pesteie", "Mehran", ""], ["Abolmaesumi", "Purang", ""], ["Rohling", "Robert", ""]]}, {"id": "1810.07378", "submitter": "Tianyun Zhang", "authors": "Shaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Kaidi Xu, Yunfei\n  Yang, Fuxun Yu, Jian Tang, Makan Fardad, Sijia Liu, Xiang Chen, Xue Lin,\n  Yanzhi Wang", "title": "Progressive Weight Pruning of Deep Neural Networks using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) although achieving human-level performance in\nmany domains, have very large model size that hinders their broader\napplications on edge computing devices. Extensive research work have been\nconducted on DNN model compression or pruning. However, most of the previous\nwork took heuristic approaches. This work proposes a progressive weight pruning\napproach based on ADMM (Alternating Direction Method of Multipliers), a\npowerful technique to deal with non-convex optimization problems with\npotentially combinatorial constraints. Motivated by dynamic programming, the\nproposed method reaches extremely high pruning rate by using partial prunings\nwith moderate pruning rates. Therefore, it resolves the accuracy degradation\nand long convergence time problems when pursuing extremely high pruning ratios.\nIt achieves up to 34 times pruning rate for ImageNet dataset and 167 times\npruning rate for MNIST dataset, significantly higher than those reached by the\nliterature work. Under the same number of epochs, the proposed method also\nachieves faster convergence and higher compression rates. The codes and pruned\nDNN models are released in the link bit.ly/2zxdlss\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 03:51:38 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 16:41:06 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ye", "Shaokai", ""], ["Zhang", "Tianyun", ""], ["Zhang", "Kaiqi", ""], ["Li", "Jiayu", ""], ["Xu", "Kaidi", ""], ["Yang", "Yunfei", ""], ["Yu", "Fuxun", ""], ["Tang", "Jian", ""], ["Fardad", "Makan", ""], ["Liu", "Sijia", ""], ["Chen", "Xiang", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1810.07382", "submitter": "Kamran Kowsari", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Laura E. Barnes and Donald E.\n  Brown", "title": "Analysis of Railway Accidents' Narratives Using Deep Learning", "comments": "accepted in IEEE International Conference on Machine Learning and\n  Applications (IEEE ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00235", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic understanding of domain specific texts in order to extract useful\nrelationships for later use is a non-trivial task. One such relationship would\nbe between railroad accidents' causes and their correspondent descriptions in\nreports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B.\nRailroads involved in accidents are required to submit an accident report to\nthe Federal Railroad Administration (FRA). These reports contain a variety of\nfixed field entries including primary cause of the accidents (a coded variable\nwith 389 values) as well as a narrative field which is a short text description\nof the accident. Although these narratives provide more information than a\nfixed field entry, the terminologies used in these reports are not easy to\nunderstand by a non-expert reader. Therefore, providing an assisting method to\nfill in the primary cause from such domain specific texts(narratives) would\nhelp to label the accidents with more accuracy. Another important question for\ntransportation safety is whether the reported accident cause is consistent with\nnarrative description. To address these questions, we applied deep learning\nmethods together with powerful word embeddings such as Word2Vec and GloVe to\nclassify accident cause values for the primary cause field using the text in\nthe narratives. The results show that such approaches can both accurately\nclassify accident causes based on report narratives and find important\ninconsistencies in accident reporting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 04:30:02 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 22:08:21 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 16:16:48 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1810.07411", "submitter": "Alexander Ororbia", "authors": "Alexander Ororbia, Ankur Mali, C. Lee Giles, and Daniel Kifer", "title": "Continual Learning of Recurrent Neural Networks by Locally Aligning\n  Distributed Representations", "comments": "Important revisions made throughout (additional items/results added,\n  including a complexity analysis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal models based on recurrent neural networks have proven to be quite\npowerful in a wide variety of applications. However, training these models\noften relies on back-propagation through time, which entails unfolding the\nnetwork over many time steps, making the process of conducting credit\nassignment considerably more challenging. Furthermore, the nature of\nback-propagation itself does not permit the use of non-differentiable\nactivation functions and is inherently sequential, making parallelization of\nthe underlying training process difficult. Here, we propose the Parallel\nTemporal Neural Coding Network (P-TNCN), a biologically inspired model trained\nby the learning algorithm we call Local Representation Alignment. It aims to\nresolve the difficulties and problems that plague recurrent networks trained by\nback-propagation through time. The architecture requires neither unrolling in\ntime nor the derivatives of its internal activation functions. We compare our\nmodel and learning procedure to other back-propagation through time\nalternatives (which also tend to be computationally expensive), including\nreal-time recurrent learning, echo state networks, and unbiased online\nrecurrent optimization. We show that it outperforms these on sequence modeling\nbenchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing\nNotMNIST, and Penn Treebank. Notably, our approach can in some instances\noutperform full back-propagation through time as well as variants such as\nsparse attentive back-tracking. Significantly, the hidden unit correction phase\nof P-TNCN allows it to adapt to new datasets even if its synaptic weights are\nheld fixed (zero-shot adaptation) and facilitates retention of prior generative\nknowledge when faced with a task sequence. We present results that show the\nP-TNCN's ability to conduct zero-shot adaptation and online continual sequence\nmodeling.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 07:36:47 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 06:18:25 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 20:04:46 GMT"}, {"version": "v4", "created": "Sun, 11 Aug 2019 00:41:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ororbia", "Alexander", ""], ["Mali", "Ankur", ""], ["Giles", "C. Lee", ""], ["Kifer", "Daniel", ""]]}, {"id": "1810.07791", "submitter": "Dominika Woszczyk", "authors": "Dominika Woszczyk, Gerasimos Spanakis", "title": "MaaSim: A Liveability Simulation for Improving the Quality of Life in\n  Cities", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Urbanism is no longer planned on paper thanks to powerful models and 3D\nsimulation platforms. However, current work is not open to the public and lacks\nan optimisation agent that could help in decision making. This paper describes\nthe creation of an open-source simulation based on an existing Dutch\nliveability score with a built-in AI module. Features are selected using\nfeature engineering and Random Forests. Then, a modified scoring function is\nbuilt based on the former liveability classes. The score is predicted using\nRandom Forest for regression and achieved a recall of 0.83 with 10-fold\ncross-validation. Afterwards, Exploratory Factor Analysis is applied to select\nthe actions present in the model. The resulting indicators are divided into 5\ngroups, and 12 actions are generated. The performance of four optimisation\nalgorithms is compared, namely NSGA-II, PAES, SPEA2 and eps-MOEA, on three\nestablished criteria of quality: cardinality, the spread of the solutions,\nspacing, and the resulting score and number of turns. Although all four\nalgorithms show different strengths, eps-MOEA is selected to be the most\nsuitable for this problem. Ultimately, the simulation incorporates the model\nand the selected AI module in a GUI written in the Kivy framework for Python.\nTests performed on users show positive responses and encourage further\ninitiatives towards joining technology and public applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 15:19:41 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Woszczyk", "Dominika", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1810.08359", "submitter": "Zhongyi Hu", "authors": "Zhongyi Hu, Raymond Chiong, Ilung Pranata, Yukun Bao, Yuqing Lin", "title": "Malicious Web Domain Identification using Online Credibility and\n  Performance Data by Considering the Class Imbalance Issue", "comments": "20 pages", "journal-ref": "Industrial Management & Data Systems, 2018", "doi": "10.1108/IMDS-02-2018-0072", "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Malicious web domain identification is of significant importance to\nthe security protection of Internet users. With online credibility and\nperformance data, this paper aims to investigate the use of machine learning\ntech-niques for malicious web domain identification by considering the class\nimbalance issue (i.e., there are more benign web domains than malicious ones).\nDesign/methodology/approach: We propose an integrated resampling approach to\nhandle class imbalance by combining the Synthetic Minority Over-sampling\nTEchnique (SMOTE) and Particle Swarm Optimisation (PSO), a population-based\nmeta-heuristic algorithm. We use the SMOTE for over-sampling and PSO for\nunder-sampling. Findings: By applying eight well-known machine learning\nclassifiers, the proposed integrated resampling approach is comprehensively\nexamined using several imbalanced web domain datasets with different imbalance\nratios. Com-pared to five other well-known resampling approaches, experimental\nresults confirm that the proposed approach is highly effective. Practical\nimplications: This study not only inspires the practical use of online\ncredibility and performance data for identifying malicious web domains, but\nalso provides an effective resampling approach for handling the class\nimbal-ance issue in the area of malicious web domain identification.\nOriginality/value: Online credibility and performance data is applied to build\nmalicious web domain identification models using machine learning techniques.\nAn integrated resampling approach is proposed to address the class im-balance\nissue. The performance of the proposed approach is confirmed based on\nreal-world datasets with different imbalance ratios.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 05:54:40 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Hu", "Zhongyi", ""], ["Chiong", "Raymond", ""], ["Pranata", "Ilung", ""], ["Bao", "Yukun", ""], ["Lin", "Yuqing", ""]]}, {"id": "1810.08559", "submitter": "Alexander Wong", "authors": "Zhong Qiu Lin, Audrey G. Chung, and Alexander Wong", "title": "EdgeSpeechNets: Highly Efficient Deep Neural Networks for Speech\n  Recognition on the Edge", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.NE cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite showing state-of-the-art performance, deep learning for speech\nrecognition remains challenging to deploy in on-device edge scenarios such as\nmobile and other consumer devices. Recently, there have been greater efforts in\nthe design of small, low-footprint deep neural networks (DNNs) that are more\nappropriate for edge devices, with much of the focus on design principles for\nhand-crafting efficient network architectures. In this study, we explore a\nhuman-machine collaborative design strategy for building low-footprint DNN\narchitectures for speech recognition through a marriage of human-driven\nprincipled network design prototyping and machine-driven design exploration.\nThe efficacy of this design strategy is demonstrated through the design of a\nfamily of highly-efficient DNNs (nicknamed EdgeSpeechNets) for\nlimited-vocabulary speech recognition. Experimental results using the Google\nSpeech Commands dataset for limited-vocabulary speech recognition showed that\nEdgeSpeechNets have higher accuracies than state-of-the-art DNNs (with the best\nEdgeSpeechNet achieving ~97% accuracy), while achieving significantly smaller\nnetwork sizes (as much as 7.8x smaller) and lower computational cost (as much\nas 36x fewer multiply-add operations, 10x lower prediction latency, and 16x\nsmaller memory footprint on a Motorola Moto E phone), making them very\nwell-suited for on-device edge voice interface applications.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 00:47:20 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 19:25:08 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Lin", "Zhong Qiu", ""], ["Chung", "Audrey G.", ""], ["Wong", "Alexander", ""]]}, {"id": "1810.08578", "submitter": "Luke Godfrey", "authors": "Luke B. Godfrey and Michael S. Gashler", "title": "Leveraging Product as an Activation Function in Deep Networks", "comments": "6 pages, 3 figures, IEEE SMC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product unit neural networks (PUNNs) are powerful representational models\nwith a strong theoretical basis, but have proven to be difficult to train with\ngradient-based optimizers. We present windowed product unit neural networks\n(WPUNNs), a simple method of leveraging product as a nonlinearity in a neural\nnetwork. Windowing the product tames the complex gradient surface and enables\nWPUNNs to learn effectively, solving the problems faced by PUNNs. WPUNNs use\nproduct layers between traditional sum layers, capturing the representational\npower of product units and using the product itself as a nonlinearity. We find\nthe result that this method works as well as traditional nonlinearities like\nReLU on the MNIST dataset. We demonstrate that WPUNNs can also generalize gated\nunits in recurrent neural networks, yielding results comparable to LSTM\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 16:43:26 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Godfrey", "Luke B.", ""], ["Gashler", "Michael S.", ""]]}, {"id": "1810.08615", "submitter": "Ali Marjaninejad", "authors": "Ali Marjaninejad, Dar\\'io Urbina-Mel\\'endez, Brian A. Cohn, Francisco\n  J. Valero-Cuevas", "title": "Autonomous Functional Locomotion in a Tendon-Driven Limb via Limited\n  Experience", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots will become ubiquitously useful only when they can use few attempts to\nteach themselves to perform different tasks, even with complex bodies and in\ndynamical environments. Vertebrates, in fact, successfully use trial-and-error\nto learn multiple tasks in spite of their intricate tendon-driven anatomies.\nRoboticists find such tendon-driven systems particularly hard to control\nbecause they are simultaneously nonlinear, under-determined (many tendon\ntensions combine to produce few net joint torques), and over-determined (few\njoint rotations define how many tendons need to be reeled-in/payed-out). We\ndemonstrate---for the first time in simulation and in hardware---how a\nmodel-free approach allows few-shot autonomous learning to produce effective\nlocomotion in a 3-tendon/2-joint tendon-driven leg. Initially, an artificial\nneural network fed by sparsely sampled data collected using motor babbling\ncreates an inverse map from limb kinematics to motor activations, which is\nanalogous to juvenile vertebrates playing during development. Thereafter,\niterative reward-driven exploration of candidate motor activations\nsimultaneously refines the inverse map and finds a functional locomotor\nlimit-cycle autonomously. This biologically-inspired algorithm, which we call\nG2P (General to Particular), enables versatile adaptation of robots to changes\nin the target task, mechanics of their bodies, and environment. Moreover, this\nwork empowers future studies of few-shot autonomous learning in biological\nsystems, which is the foundation of their enviable functional versatility.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 16:53:01 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Marjaninejad", "Ali", ""], ["Urbina-Mel\u00e9ndez", "Dar\u00edo", ""], ["Cohn", "Brian A.", ""], ["Valero-Cuevas", "Francisco J.", ""]]}, {"id": "1810.08646", "submitter": "Sumit Bam Shrestha", "authors": "Sumit Bam Shrestha and Garrick Orchard", "title": "SLAYER: Spike Layer Error Reassignment in Time", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Configuring deep Spiking Neural Networks (SNNs) is an exciting research\navenue for low power spike event based computation. However, the spike\ngeneration function is non-differentiable and therefore not directly compatible\nwith the standard error backpropagation algorithm. In this paper, we introduce\na new general backpropagation mechanism for learning synaptic weights and\naxonal delays which overcomes the problem of non-differentiability of the spike\nfunction and uses a temporal credit assignment policy for backpropagating error\nto preceding layers. We describe and release a GPU accelerated software\nimplementation of our method which allows training both fully connected and\nconvolutional neural network (CNN) architectures. Using our software, we\ncompare our method against existing SNN based learning approaches and standard\nANN to SNN conversion techniques and show that our method achieves state of the\nart performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 10:10:03 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Shrestha", "Sumit Bam", ""], ["Orchard", "Garrick", ""]]}, {"id": "1810.08648", "submitter": "George Kyriakides", "authors": "George Kyriakides, Konstantinos Margaritis", "title": "Towards automated neural design: An open source, distributed neural\n  architecture research framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NORD (Neural Operations Research & Development) is an open source distributed\ndeep learning architectural research framework, based on PyTorch, MPI and\nHorovod. It aims to make research of deep architectures easier for experts of\ndifferent domains, in order to accelerate the process of finding better\narchitectures, as well as study the best architectures generated for different\ndatasets. Although currently under heavy development, the framework aims to\nallow the easy implementation of different design and optimization method\nfamilies (optimization algorithms, meta-heuristics, reinforcement learning\netc.) as well as the fair comparison between them. Furthermore, due to the\ncomputational resources required in order to optimize and evaluate network\narchitectures, it leverage the use of distributed computing, while aiming to\nminimize the researcher's overhead required to implement it. Moreover, it\nstrives to make the creation of architectures more intuitive, by implementing\nnetwork descriptors, allowing to separately define the architecture's nodes and\nconnections. In this paper, we present the framework's current state of\ndevelopment, while presenting its basic concepts, providing simple examples as\nwell as their experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 20:31:45 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Kyriakides", "George", ""], ["Margaritis", "Konstantinos", ""]]}, {"id": "1810.08649", "submitter": "Marta Bagi\\'nska", "authors": "Marta Bagi\\'nska and Piotr E. Srokosz", "title": "The Optimal ANN Model for Predicting Bearing Capacity of Shallow\n  Foundations Trained on Scarce Data", "comments": "KSCE Journal of Civil Engineering 2018", "journal-ref": null, "doi": "10.1007/s12205-018-2636-4", "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is focused on determining the potential of using deep neural\nnetworks (DNNs) to predict the ultimate bearing capacity of shallow foundation\nin situations when the experimental data which may be used to train networks is\nscarce. Two experiments involving testing over 17000 networks were conducted.\nThe first experiment was aimed at comparing the accuracy of shallow neural\nnetworks and DNNs predictions. It shows that when the experimental dataset used\nfor preparing models is small then DNNs have a significant advantage over\nshallow networks. The second experiment was conducted to compare the\nperformance of DNNs consisting of different number of neurons and layers.\nObtained results indicate that the optimal number of layers varies between 5 to\n7. Networks with less and - surprisingly - more layers obtain lower accuracy.\nMoreover, the number of neurons in DNN has a lower impact on the prediction\naccuracy than the number of DNN's layers. DNNs perform very well, even when\ntrained with only 6 samples. Basing on the results it seems that when\npredicting the ultimate bearing capacity with ANN models obtaining small but\nhigh-quality experimental training datasets instead of large training datasets\naffected by a higher error is an advisable approach.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 11:17:43 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Bagi\u0144ska", "Marta", ""], ["Srokosz", "Piotr E.", ""]]}, {"id": "1810.08650", "submitter": "Tao Yang", "authors": "Tao Yang, Yadong Wei, Zhijun Tu, Haolun Zeng, Michel A. Kinsy, Nanning\n  Zheng, and Pengju Ren", "title": "Design Space Exploration of Neural Network Activation Function Circuits", "comments": "5 pages, 5 figures, 16 conference", "journal-ref": null, "doi": "10.1109/TCAD.2018.2871198", "report-no": "08467987", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread application of artificial neural networks has prompted\nresearchers to experiment with FPGA and customized ASIC designs to speed up\ntheir computation. These implementation efforts have generally focused on\nweight multiplication and signal summation operations, and less on activation\nfunctions used in these applications. Yet, efficient hardware implementations\nof nonlinear activation functions like Exponential Linear Units (ELU), Scaled\nExponential Linear Units (SELU), and Hyperbolic Tangent (tanh), are central to\ndesigning effective neural network accelerators, since these functions require\nlots of resources. In this paper, we explore efficient hardware implementations\nof activation functions using purely combinational circuits, with a focus on\ntwo widely used nonlinear activation functions, i.e., SELU and tanh. Our\nexperiments demonstrate that neural networks are generally insensitive to the\nprecision of the activation function. The results also prove that the proposed\ncombinational circuit-based approach is very efficient in terms of speed and\narea, with negligible accuracy loss on the MNIST, CIFAR-10 and IMAGENET\nbenchmarks. Synopsys Design Compiler synthesis results show that circuit\ndesigns for tanh and SELU can save between 3.13-7.69 and 4.45-8:45 area\ncompared to the LUT/memory-based implementations, and can operate at 5.14GHz\nand 4.52GHz using the 28nm SVT library, respectively. The implementation is\navailable at: https://github.com/ThomasMrY/ActivationFunctionDemo.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 14:57:46 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Yang", "Tao", ""], ["Wei", "Yadong", ""], ["Tu", "Zhijun", ""], ["Zeng", "Haolun", ""], ["Kinsy", "Michel A.", ""], ["Zheng", "Nanning", ""], ["Ren", "Pengju", ""]]}, {"id": "1810.08651", "submitter": "Jessica Thompson", "authors": "Jessica A. F. Thompson, Yoshua Bengio, Elia Formisano, and Marc\n  Sch\\\"onwiesner", "title": "How can deep learning advance computational modeling of sensory\n  information processing?", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)", "journal-ref": null, "doi": null, "report-no": "MLINI/2016/04", "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, computational neuroscience, and cognitive science have\noverlapping goals related to understanding intelligence such that perception\nand behaviour can be simulated in computational systems. In neuroimaging,\nmachine learning methods have been used to test computational models of sensory\ninformation processing. Recently, these model comparison techniques have been\nused to evaluate deep neural networks (DNNs) as models of sensory information\nprocessing. However, the interpretation of such model evaluations is muddied by\nimprecise statistical conclusions. Here, we make explicit the types of\nconclusions that can be drawn from these existing model comparison techniques\nand how these conclusions change when the model in question is a DNN. We\ndiscuss how DNNs are amenable to new model comparison techniques that allow for\nstronger conclusions to be made about the computational mechanisms underlying\nsensory information processing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 23:39:34 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Thompson", "Jessica A. F.", ""], ["Bengio", "Yoshua", ""], ["Formisano", "Elia", ""], ["Sch\u00f6nwiesner", "Marc", ""]]}, {"id": "1810.08652", "submitter": "Yang Li", "authors": "Yanjun Zhang, Tie Li, Guangyu Na, Guoqing Li, Yang Li", "title": "Optimized Extreme Learning Machine for Power System Transient Stability\n  Prediction Using Synchrophasors", "comments": "Accepted by Mathematical Problems in Engineering", "journal-ref": null, "doi": "10.1155/2015/529724", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new optimized extreme learning machine- (ELM-) based method for power\nsystem transient stability prediction (TSP) using synchrophasors is presented\nin this paper. First, the input features symbolizing the transient stability of\npower systems are extracted from synchronized measurements. Then, an ELM\nclassifier is employed to build the TSP model. And finally, the optimal\nparameters of the model are optimized by using the improved particle swarm\noptimization (IPSO) algorithm. The novelty of the proposal is in the fact that\nit improves the prediction performance of the ELM-based TSP model by using IPSO\nto optimize the parameters of the model with synchrophasors. And finally, based\non the test results on both IEEE 39-bus system and a large-scale real power\nsystem, the correctness and validity of the presented approach are verified.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:07:16 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Yanjun", ""], ["Li", "Tie", ""], ["Na", "Guangyu", ""], ["Li", "Guoqing", ""], ["Li", "Yang", ""]]}, {"id": "1810.08653", "submitter": "Yonghua Yin", "authors": "Yonghua Yin", "title": "Deep Learning with the Random Neural Network and its Applications", "comments": "23 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random neural network (RNN) is a mathematical model for an \"integrate and\nfire\" spiking network that closely resembles the stochastic behaviour of\nneurons in mammalian brains. Since its proposal in 1989, there have been\nnumerous investigations into the RNN's applications and learning algorithms.\nDeep learning (DL) has achieved great success in machine learning. Recently,\nthe properties of the RNN for DL have been investigated, in order to combine\ntheir power. Recent results demonstrate that the gap between RNNs and DL can be\nbridged and the DL tools based on the RNN are faster and can potentially be\nused with less energy expenditure than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:47:51 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Yin", "Yonghua", ""]]}, {"id": "1810.08886", "submitter": "Youshan Zhang", "authors": "Youshan Zhang, Liangdong Guo, Qi Li, Junhui Li", "title": "Electricity consumption forecasting method based on MPSO-BP neural\n  network model", "comments": null, "journal-ref": "Proceedings of the 2016 4th International Conference On Electrical\n  Electronics Engineering And Computer Science (ICEEECS 2016), 50:674-678, 2016", "doi": "10.2991/iceeecs-16.2016.133", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of the electricity consumption forecasting\nmethod. An MPSO-BP (modified particle swarm optimization-back propagation)\nneural network model is constructed based on the history data of a mineral\ncompany of Anshan in China. The simulation showed that the convergence of the\nalgorithm and forecasting accuracy using the obtained model are better than\nthose of other traditional ones, such as BP, PSO, fuzzy neural network and so\non. Then we predict the electricity consumption of each month in 2017 based on\nthe MPSO-BP neural network model.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 03:16:22 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Youshan", ""], ["Guo", "Liangdong", ""], ["Li", "Qi", ""], ["Li", "Junhui", ""]]}, {"id": "1810.08923", "submitter": "Ehsan Hoseinzade", "authors": "Ehsan Hoseinzade, Saman Haratizadeh", "title": "CNNPred: CNN-based stock market prediction using several data sources", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction from financial data is one of the most important problems\nin market prediction domain for which many approaches have been suggested.\nAmong other modern tools, convolutional neural networks (CNN) have recently\nbeen applied for automatic feature selection and market prediction. However, in\nexperiments reported so far, less attention has been paid to the correlation\namong different markets as a possible source of information for extracting\nfeatures. In this paper, we suggest a CNN-based framework with specially\ndesigned CNNs, that can be applied on a collection of data from a variety of\nsources, including different markets, in order to extract features for\npredicting the future of those markets. The suggested framework has been\napplied for predicting the next day's direction of movement for the indices of\nS&P 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of\ninitial features. The evaluations show a significant improvement in\nprediction's performance compared to the state of the art baseline algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 10:34:56 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Hoseinzade", "Ehsan", ""], ["Haratizadeh", "Saman", ""]]}, {"id": "1810.08944", "submitter": "Saman Sadeghyan", "authors": "Saman Sadeghyan, Shahrokh Asadi", "title": "MS-BACO: A new Model Selection algorithm using Binary Ant Colony\n  Optimization for neural complexity and error reduction", "comments": "29 pages, 13 figures, 4 tables, 2 algorithms, preprint submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stabilizing the complexity of Feedforward Neural Networks (FNNs) for the\ngiven approximation task can be managed by defining an appropriate model\nmagnitude which is also greatly correlated with the generalization quality and\ncomputational efficiency. However, deciding on the right level of model\ncomplexity can be highly challenging in FNN applications. In this paper, a new\nModel Selection algorithm using Binary Ant Colony Optimization (MS-BACO) is\nproposed in order to achieve the optimal FNN model in terms of neural\ncomplexity and cross-entropy error. MS-BACO is a meta-heuristic algorithm that\ntreats the problem as a combinatorial optimization problem. By quantifying both\nthe amount of correlation exists among hidden neurons and the sensitivity of\nthe FNN output to the hidden neurons using a sample-based sensitivity analysis\nmethod called, extended Fourier amplitude sensitivity test, the algorithm\nmostly tends to select the FNN model containing hidden neurons with most\ndistinct hyperplanes and high contribution percentage. Performance of the\nproposed algorithm with three different designs of heuristic information is\ninvestigated. Comparison of the findings verifies that the newly introduced\nalgorithm is able to provide more compact and accurate FNN model.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 13:44:16 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Sadeghyan", "Saman", ""], ["Asadi", "Shahrokh", ""]]}, {"id": "1810.09084", "submitter": "Hin Wai Lui", "authors": "Hin Wai Lui", "title": "A general learning system based on neuron bursting and tonic firing", "comments": null, "journal-ref": "Medical Hypotheses, Volume 123, February 2019, Pages 35-46", "doi": "10.1016/j.mehy.2018.12.001", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a framework for the biological learning mechanism as a\ngeneral learning system. The proposal is as follows. The bursting and tonic\nmodes of firing patterns found in many neuron types in the brain correspond to\ntwo separate modes of information processing, with one mode resulting in\nawareness, and another mode being subliminal. In such a coding scheme, a neuron\nin bursting state codes for the highest level of perceptual abstraction\nrepresenting a pattern of sensory stimuli, or volitional abstraction\nrepresenting a pattern of muscle contraction sequences. Within the 50-250 ms\nminimum integration time of experience, the bursting neurons form synchrony\nensembles to allow for binding of related percepts. The degree which different\nbursting neurons can be merged into the same synchrony ensemble depends on the\nunderlying cortical connections that represent the degree of perceptual\nsimilarity. These synchrony ensembles compete for selective attention to remain\nactive. The dominant synchrony ensemble triggers episodic memory recall in the\nhippocampus, while forming new episodic memory with current sensory stimuli,\nresulting in a stream of thoughts. Neuromodulation modulates both top-down\nselection of synchrony ensembles, and memory formation. Episodic memory stored\nin the hippocampus is transferred to semantic and procedural memory in the\ncortex during rapid eye movement sleep, by updating cortical neuron synaptic\nweights with spike timing dependent plasticity. With the update of synaptic\nweights, new neurons become bursting while previous bursting neurons become\ntonic, allowing bursting neurons to move up to a higher level of perceptual\nabstraction. Finally, the proposed learning mechanism is compared with the\nback-propagation algorithm used in deep neural networks, and a proposal of how\nthe credit assignment problem can be addressed by the current proposal is\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 04:52:39 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Lui", "Hin Wai", ""]]}, {"id": "1810.09485", "submitter": "Nicola Milano", "authors": "Nicola Milano and Stefano Nolfi", "title": "Scaling Up Cartesian Genetic Programming through Preferential Selection\n  of Larger Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how efficiency of Cartesian Genetic Programming method can be\nscaled up through the preferential selection of phenotypically larger\nsolutions, i.e. through the preferential selection of larger solutions among\nequally good solutions. The advantage of the preferential selection of larger\nsolutions is validated on the six, seven and eight-bit parity problems, on a\ndynamically varying problem involving the classification of binary patterns,\nand on the Paige regression problem. In all cases, the preferential selection\nof larger solutions provides an advantage in term of the performance of the\nevolved solutions and in term of speed, the number of evaluations required to\nevolve optimal or high-quality solutions. The advantage provided by the\npreferential selection of larger solutions can be further extended by\nself-adapting the mutation rate through the one-fifth success rule. Finally,\nfor problems like the Paige regression in which neutrality plays a minor role,\nthe advantage of the preferential selection of larger solutions can be extended\nby preferring larger solutions also among quasi-neutral alternative candidate\nsolutions, i.e. solutions achieving slightly different performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:11:56 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Milano", "Nicola", ""], ["Nolfi", "Stefano", ""]]}, {"id": "1810.09619", "submitter": "Yiwen Guo", "authors": "Yiwen Guo, Chao Zhang, Changshui Zhang and Yurong Chen", "title": "Sparse DNNs with Improved Adversarial Robustness", "comments": "l1 regularization on weights --> l1 regularization on activations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are computationally/memory-intensive and\nvulnerable to adversarial attacks, making them prohibitive in some real-world\napplications. By converting dense models into sparse ones, pruning appears to\nbe a promising solution to reducing the computation/memory cost. This paper\nstudies classification models, especially DNN-based ones, to demonstrate that\nthere exists intrinsic relationships between their sparsity and adversarial\nrobustness. Our analyses reveal, both theoretically and empirically, that\nnonlinear DNN-based classifiers behave differently under $l_2$ attacks from\nsome linear ones. We further demonstrate that an appropriately higher model\nsparsity implies better robustness of nonlinear DNNs, whereas over-sparsified\nmodels can be more difficult to resist adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 01:05:41 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:32:50 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Guo", "Yiwen", ""], ["Zhang", "Chao", ""], ["Zhang", "Changshui", ""], ["Chen", "Yurong", ""]]}, {"id": "1810.09690", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers", "title": "Challenges of Convex Quadratic Bi-objective Benchmark Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex quadratic objective functions are an important base case in\nstate-of-the-art benchmark collections for single-objective optimization on\ncontinuous domains. Although often considered rather simple, they represent the\nhighly relevant challenges of non-separability and ill-conditioning. In the\nmulti-objective case, quadratic benchmark problems are under-represented. In\nthis paper we analyze the specific challenges that can be posed by quadratic\nfunctions in the bi-objective case. Our construction yields a full factorial\ndesign of 54 different problem classes. We perform experiments with\nwell-established algorithms to demonstrate the insights that can be supported\nby this function class. We find huge performance differences, which can be\nclearly attributed to two root causes: non-separability and alignment of the\nPareto set with the coordinate system.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 07:10:14 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 23:39:58 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 07:25:00 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 15:12:30 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Glasmachers", "Tobias", ""]]}, {"id": "1810.09945", "submitter": "Wojciech Samek", "authors": "Armin W. Thomas, Hauke R. Heekeren, Klaus-Robert M\\\"uller, Wojciech\n  Samek", "title": "Analyzing Neuroimaging Data Through Recurrent Deep Learning Models", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 16:23:27 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 07:31:32 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Thomas", "Armin W.", ""], ["Heekeren", "Hauke R.", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1810.10161", "submitter": "Yuxiu Hua", "authors": "Yuxiu Hua, Zhifeng Zhao, Rongpeng Li, Xianfu Chen, Zhiming Liu,\n  Honggang Zhang", "title": "Deep Learning with Long Short-Term Memory for Time Series Prediction", "comments": "9 pages, 5 figures, 14 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Time series prediction can be generalized as a process that extracts useful\ninformation from historical records and then determines future values. Learning\nlong-range dependencies that are embedded in time series is often an obstacle\nfor most algorithms, whereas Long Short-Term Memory (LSTM) solutions, as a\nspecific kind of scheme in deep learning, promise to effectively overcome the\nproblem. In this article, we first give a brief introduction to the structure\nand forward propagation mechanism of the LSTM model. Then, aiming at reducing\nthe considerable computing cost of LSTM, we put forward the Random Connectivity\nLSTM (RCLSTM) model and test it by predicting traffic and user mobility in\ntelecommunication networks. Compared to LSTM, RCLSTM is formed via stochastic\nconnectivity between neurons, which achieves a significant breakthrough in the\narchitecture formation of neural networks. In this way, the RCLSTM model\nexhibits a certain level of sparsity, which leads to an appealing decrease in\nthe computational complexity and makes the RCLSTM model become more applicable\nin latency-stringent application scenarios. In the field of telecommunication\nnetworks, the prediction of traffic series and mobility traces could directly\nbenefit from this improvement as we further demonstrate that the prediction\naccuracy of RCLSTM is comparable to that of the conventional LSTM no matter how\nwe change the number of training samples or the length of input sequences.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 03:06:38 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Hua", "Yuxiu", ""], ["Zhao", "Zhifeng", ""], ["Li", "Rongpeng", ""], ["Chen", "Xianfu", ""], ["Liu", "Zhiming", ""], ["Zhang", "Honggang", ""]]}, {"id": "1810.10180", "submitter": "Luke Metz", "authors": "Luke Metz, Niru Maheswaranathan, Jeremy Nixon, C. Daniel Freeman,\n  Jascha Sohl-Dickstein", "title": "Understanding and correcting pathologies in the training of learned\n  optimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown that learned functions can dramatically outperform\nhand-designed functions on perceptual tasks. Analogously, this suggests that\nlearned optimizers may similarly outperform current hand-designed optimizers,\nespecially for specific problems. However, learned optimizers are notoriously\ndifficult to train and have yet to demonstrate wall-clock speedups over\nhand-designed optimizers, and thus are rarely used in practice. Typically,\nlearned optimizers are trained by truncated backpropagation through an unrolled\noptimization process resulting in gradients that are either strongly biased\n(for short truncations) or have exploding norm (for long truncations). In this\nwork we propose a training scheme which overcomes both of these difficulties,\nby dynamically weighting two unbiased gradient estimators for a variational\nloss on optimizer performance, allowing us to train neural networks to perform\noptimization of a specific task faster than tuned first-order methods. We\ndemonstrate these results on problems where our learned optimizer trains\nconvolutional networks faster in wall-clock time compared to tuned first-order\nmethods and with an improvement in test loss.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 04:04:25 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 17:41:54 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 00:12:30 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 23:14:02 GMT"}, {"version": "v5", "created": "Sat, 8 Jun 2019 00:33:09 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Metz", "Luke", ""], ["Maheswaranathan", "Niru", ""], ["Nixon", "Jeremy", ""], ["Freeman", "C. Daniel", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1810.10338", "submitter": "Thomas Lampert", "authors": "Thomas Lampert, Odyss\\'ee Merveille, Jessica Schmitz, Germain\n  Forestier, Friedrich Feuerhake, C\\'edric Wemmert", "title": "Strategies for Training Stain Invariant CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of Digital Pathology is the analysis of multiple digitised\nwhole slide images from differently stained tissue sections. It is common\npractice to mount consecutive sections containing corresponding microscopic\nstructures on glass slides, and to stain them differently to highlight specific\ntissue components. These multiple staining modalities result in very different\nimages but include a significant amount of consistent image information. Deep\nlearning approaches have recently been proposed to analyse these images in\norder to automatically identify objects of interest for pathologists. These\nsupervised approaches require a vast amount of annotations, which are difficult\nand expensive to acquire---a problem that is multiplied with multiple\nstainings. This article presents several training strategies that make progress\ntowards stain invariant networks. By training the network on one commonly used\nstaining modality and applying it to images that include corresponding but\ndifferently stained tissue structures, the presented unsupervised strategies\ndemonstrate significant improvements over standard training strategies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 09:41:23 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 23:13:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lampert", "Thomas", ""], ["Merveille", "Odyss\u00e9e", ""], ["Schmitz", "Jessica", ""], ["Forestier", "Germain", ""], ["Feuerhake", "Friedrich", ""], ["Wemmert", "C\u00e9dric", ""]]}, {"id": "1810.10340", "submitter": "Sjoerd van Steenkiste", "authors": "Sjoerd van Steenkiste, Karol Kurach, J\\\"urgen Schmidhuber, Sylvain\n  Gelly", "title": "Investigating Object Compositionality in Generative Adversarial Networks", "comments": "A preliminary version of this work (arXiv v1) appeared under the\n  title \"A Case for Object Compositionality in Deep Generative Models of\n  Images\" as a workshop paper at the NeurIPS2018 workshop on \"Modeling the\n  Physical World: Perception, Learning, and Control\", and at the NeurIPS2018\n  workshop on \"Relational Representation Learning\"", "journal-ref": null, "doi": "10.1016/j.neunet.2020.07.007", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models seek to recover the process with which the observed\ndata was generated. They may be used to synthesize new samples or to\nsubsequently extract representations. Successful approaches in the domain of\nimages are driven by several core inductive biases. However, a bias to account\nfor the compositional way in which humans structure a visual scene in terms of\nobjects has frequently been overlooked. In this work, we investigate object\ncompositionality as an inductive bias for Generative Adversarial Networks\n(GANs). We present a minimal modification of a standard generator to\nincorporate this inductive bias and find that it reliably learns to generate\nimages as compositions of objects. Using this general design as a backbone, we\nthen propose two useful extensions to incorporate dependencies among objects\nand background. We extensively evaluate our approach on several multi-object\nimage datasets and highlight the merits of incorporating structure for\nrepresentation learning purposes. In particular, we find that our structured\nGANs are better at generating multi-object images that are more faithful to the\nreference distribution. More so, we demonstrate how, by leveraging the\nstructure of the learned generative process, one can `invert' the learned\ngenerative model to perform unsupervised instance segmentation. On the\nchallenging CLEVR dataset, it is shown how our approach is able to improve over\nother recent purely unsupervised object-centric approaches to image generation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 20:17:11 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 10:38:57 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:10:53 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["van Steenkiste", "Sjoerd", ""], ["Kurach", "Karol", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1810.10453", "submitter": "Timothy Atkinson", "authors": "Timothy Atkinson, Detlef Plump and Susan Stepney", "title": "Evolving Graphs with Semantic Neutral Drift", "comments": null, "journal-ref": null, "doi": "10.1007/s11047-019-09772-4", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of Semantic Neutral Drift (SND) for genetic\nprogramming (GP), where we exploit equivalence laws to design semantics\npreserving mutations guaranteed to preserve individuals' fitness scores. A\nnumber of digital circuit benchmark problems have been implemented with\nrule-based graph programs and empirically evaluated, demonstrating quantitative\nimprovements in evolutionary performance. Analysis reveals that the benefits of\nthe designed SND reside in more complex processes than simple growth of\nindividuals, and that there are circumstances where it is beneficial to choose\notherwise detrimental parameters for a GP system if that facilitates the\ninclusion of SND.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:32:05 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 17:01:08 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Atkinson", "Timothy", ""], ["Plump", "Detlef", ""], ["Stepney", "Susan", ""]]}, {"id": "1810.10485", "submitter": "Maitreya Patel", "authors": "Maitreya Patel, Anery Patel and Dr. Ranendu Ghosh", "title": "Precipitation Nowcasting: Leveraging bidirectional LSTM and 1D CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-term rainfall forecasting, also known as precipitation nowcasting has\nbecome a potentially fundamental technology impacting significant real-world\napplications ranging from flight safety, rainstorm alerts to farm irrigation\ntimings. Since weather forecasting involves identifying the underlying\nstructure in a huge amount of data, deep-learning based precipitation\nnowcasting has intuitively outperformed the traditional linear extrapolation\nmethods. Our research work intends to utilize the recent advances in deep\nlearning to nowcasting, a multi-variable time series forecasting problem.\nSpecifically, we leverage a bidirectional LSTM (Long Short-Term Memory) neural\nnetwork architecture which remarkably captures the temporal features and\nlong-term dependencies from historical data. To further our studies, we compare\nthe bidirectional LSTM network with 1D CNN model to prove the capabilities of\nsequence models over feed-forward neural architectures in forecasting related\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 16:42:42 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Patel", "Maitreya", ""], ["Patel", "Anery", ""], ["Ghosh", "Dr. Ranendu", ""]]}, {"id": "1810.10687", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong and Tao Li and Wenxue Liu and Yang Chen", "title": "Structure Learning of Deep Networks via DNA Computing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) has gained state-of-the-art results in\nmany pattern recognition and computer vision tasks. However, most of the CNN\nstructures are manually designed by experienced researchers. Therefore, auto-\nmatically building high performance networks becomes an important problem. In\nthis paper, we introduce the idea of using DNA computing algorithm to\nautomatically learn high-performance architectures. In DNA computing algorithm,\nwe use short DNA strands to represent layers and long DNA strands to represent\noverall networks. We found that most of the learned models perform similarly,\nand only those performing worse during the first runs of training will perform\nworse finally than others. The indicates that: 1) Using DNA computing algorithm\nto learn deep architectures is feasible; 2) Local minima should not be a\nproblem of deep networks; 3) We can use early stop to kill the models with the\nbad performance just after several runs of training. In our experiments, an\naccuracy 99.73% was obtained on the MNIST data set and an accuracy 95.10% was\nobtained on the CIFAR-10 data set.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 02:06:24 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Li", "Tao", ""], ["Liu", "Wenxue", ""], ["Chen", "Yang", ""]]}, {"id": "1810.10708", "submitter": "Bo-Jian Hou", "authors": "Bo-Jian Hou and Zhi-Hua Zhou", "title": "Learning with Interpretable Structure from Gated RNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretability of deep learning models has raised extended attention\nthese years. It will be beneficial if we can learn an interpretable structure\nfrom deep learning models. In this paper, we focus on Recurrent Neural\nNetworks~(RNNs) especially gated RNNs whose inner mechanism is still not\nclearly understood. We find that Finite State Automaton~(FSA) that processes\nsequential data has more interpretable inner mechanism according to the\ndefinition of interpretability and can be learned from RNNs as the\ninterpretable structure. We propose two methods to learn FSA from RNN based on\ntwo different clustering methods. With the learned FSA and via experiments on\nartificial and real datasets, we find that FSA is more trustable than the RNN\nfrom which it learned, which gives FSA a chance to substitute RNNs in\napplications involving humans' lives or dangerous facilities. Besides, we\nanalyze how the number of gates affects the performance of RNN. Our result\nsuggests that gate in RNN is important but the less the better, which could be\na guidance to design other RNNs. Finally, we observe that the FSA learned from\nRNN gives semantic aggregated states and its transition graph shows us a very\ninteresting vision of how RNNs intrinsically handle text classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 03:42:08 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 12:07:02 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Hou", "Bo-Jian", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1810.10801", "submitter": "Yulia Sandamirskaya", "authors": "Sebastian Glatz, Julien N.P. Martel, Raphaela Kreiser, Ning Qiao, and\n  Yulia Sandamirskaya", "title": "Adaptive motor control and learning in a spiking neural network realised\n  on a mixed-signal neuromorphic processor", "comments": "6+1 pages, 4 figures, will appear in one of the Robotics conferences", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA)\n  2019", "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing is a new paradigm for design of both the computing\nhardware and algorithms inspired by biological neural networks. The event-based\nnature and the inherent parallelism make neuromorphic computing a promising\nparadigm for building efficient neural network based architectures for control\nof fast and agile robots. In this paper, we present a spiking neural network\narchitecture that uses sensory feedback to control rotational velocity of a\nrobotic vehicle. When the velocity reaches the target value, the mapping from\nthe target velocity of the vehicle to the correct motor command, both\nrepresented in the spiking neural network on the neuromorphic device, is\nautonomously stored on the device using on-chip plastic synaptic weights. We\nvalidate the controller using a wheel motor of a miniature mobile vehicle and\ninertia measurement unit as the sensory feedback and demonstrate online\nlearning of a simple 'inverse model' in a two-layer spiking neural network on\nthe neuromorphic chip. The prototype neuromorphic device that features 256\nspiking neurons allows us to realise a simple proof of concept architecture for\nthe purely neuromorphic motor control and learning. The architecture can be\neasily scaled-up if a larger neuromorphic device is available.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:22:17 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Glatz", "Sebastian", ""], ["Martel", "Julien N. P.", ""], ["Kreiser", "Raphaela", ""], ["Qiao", "Ning", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1810.10802", "submitter": "Lei Yu", "authors": "Lei Yu", "title": "Tackling Sequence to Sequence Mapping Problems with Neural Networks", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language Processing (NLP), it is important to detect the\nrelationship between two sequences or to generate a sequence of tokens given\nanother observed sequence. We call the type of problems on modelling sequence\npairs as sequence to sequence (seq2seq) mapping problems. A lot of research has\nbeen devoted to finding ways of tackling these problems, with traditional\napproaches relying on a combination of hand-crafted features, alignment models,\nsegmentation heuristics, and external linguistic resources. Although great\nprogress has been made, these traditional approaches suffer from various\ndrawbacks, such as complicated pipeline, laborious feature engineering, and the\ndifficulty for domain adaptation. Recently, neural networks emerged as a\npromising solution to many problems in NLP, speech recognition, and computer\nvision. Neural models are powerful because they can be trained end to end,\ngeneralise well to unseen examples, and the same framework can be easily\nadapted to a new domain.\n  The aim of this thesis is to advance the state-of-the-art in seq2seq mapping\nproblems with neural networks. We explore solutions from three major aspects:\ninvestigating neural models for representing sequences, modelling interactions\nbetween sequences, and using unpaired data to boost the performance of neural\nmodels. For each aspect, we propose novel models and evaluate their efficacy on\nvarious tasks of seq2seq mapping.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:24:13 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Yu", "Lei", ""]]}, {"id": "1810.10944", "submitter": "JaeSung Choi", "authors": "Jaesung Choi and Pilwon Kim", "title": "Critical Neuromorphic Computing based on Explosive Synchronization", "comments": "14pages, 8figures", "journal-ref": null, "doi": "10.1063/1.5086902", "report-no": null, "categories": "cs.NE cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous oscillations in neuronal ensembles have been proposed to provide\na neural basis for the information processes in the brain. In this work, we\npresent a neuromorphic computing algorithm based on oscillator synchronization\nin a critical regime. The algorithm uses the high dimensional transient\ndynamics perturbed by an input and translates it into proper output stream. One\nof the benefits of adopting coupled phase oscillators as neuromorphic elements\nis that the synchrony among oscillators can be finely tuned at a critical\nstate. Especially near a critical state, the marginally synchronized\noscillators operate with high efficiency and maintain better computing\nperformances. We also show that explosive synchronization which is induced from\nspecific neuronal connectivity produces more improved and stable outputs. This\nwork provides a systematic way to encode computing in a large size coupled\noscillators, which may be useful in designing neuromorphic devices.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 12:57:06 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Choi", "Jaesung", ""], ["Kim", "Pilwon", ""]]}, {"id": "1810.11393", "submitter": "Jo\\~ao Sacramento", "authors": "Jo\\~ao Sacramento, Rui Ponte Costa, Yoshua Bengio, Walter Senn", "title": "Dendritic cortical microcircuits approximate the backpropagation\n  algorithm", "comments": "To appear in Advances in Neural Information Processing Systems 31\n  (NIPS 2018). 12 pages, 3 figures, 9 pages of supplementary material (2\n  supplementary figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has seen remarkable developments over the last years, many of\nthem inspired by neuroscience. However, the main learning mechanism behind\nthese advances - error backpropagation - appears to be at odds with\nneurobiology. Here, we introduce a multilayer neuronal network model with\nsimplified dendritic compartments in which error-driven synaptic plasticity\nadapts the network towards a global desired output. In contrast to previous\nwork our model does not require separate phases and synaptic learning is driven\nby local dendritic prediction errors continuously in time. Such errors\noriginate at apical dendrites and occur due to a mismatch between predictive\ninput from lateral interneurons and activity from actual top-down feedback.\nThrough the use of simple dendritic compartments and different cell-types our\nmodel can represent both error and normal activity within a pyramidal neuron.\nWe demonstrate the learning capabilities of the model in regression and\nclassification tasks, and show analytically that it approximates the error\nbackpropagation algorithm. Moreover, our framework is consistent with recent\nobservations of learning between brain areas and the architecture of cortical\nmicrocircuits. Overall, we introduce a novel view of learning on dendritic\ncortical circuits and on how the brain may solve the long-standing synaptic\ncredit assignment problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 15:40:58 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Sacramento", "Jo\u00e3o", ""], ["Costa", "Rui Ponte", ""], ["Bengio", "Yoshua", ""], ["Senn", "Walter", ""]]}, {"id": "1810.11491", "submitter": "Alexander Fabisch", "authors": "Alexander Fabisch", "title": "Empirical Evaluation of Contextual Policy Search with a Comparison-based\n  Surrogate Model and Active Covariance Matrix Adaptation", "comments": "Supplementary material for poster paper accepted at GECCO 2019;\n  https://doi.org/10.1145/3319619.3321935", "journal-ref": null, "doi": "10.1145/3319619.3321935", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual policy search (CPS) is a class of multi-task reinforcement\nlearning algorithms that is particularly useful for robotic applications. A\nrecent state-of-the-art method is Contextual Covariance Matrix Adaptation\nEvolution Strategies (C-CMA-ES). It is based on the standard black-box\noptimization algorithm CMA-ES. There are two useful extensions of CMA-ES that\nwe will transfer to C-CMA-ES and evaluate empirically: ACM-ES, which uses a\ncomparison-based surrogate model, and aCMA-ES, which uses an active update of\nthe covariance matrix. We will show that improvements with these methods can be\nimpressive in terms of sample-efficiency, although this is not relevant any\nmore for the robotic domain.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:35:27 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 21:20:16 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fabisch", "Alexander", ""]]}, {"id": "1810.11521", "submitter": "William Severa", "authors": "William Severa, Craig M. Vineyard, Ryan Dellana, Stephen J. Verzi,\n  James B. Aimone", "title": "Whetstone: A Method for Training Deep Artificial Neural Networks for\n  Binary Communication", "comments": null, "journal-ref": null, "doi": "10.1038/s42256-018-0015-y", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new technique for training networks for low-precision\ncommunication. Targeting minimal communication between nodes not only enables\nthe use of emerging spiking neuromorphic platforms, but may additionally\nstreamline processing conventionally. Low-power and embedded neuromorphic\nprocessors potentially offer dramatic performance-per-Watt improvements over\ntraditional von Neumann processors, however programming these brain-inspired\nplatforms generally requires platform-specific expertise which limits their\napplicability. To date, the majority of artificial neural networks have not\noperated using discrete spike-like communication.\n  We present a method for training deep spiking neural networks using an\niterative modification of the backpropagation optimization algorithm. This\nmethod, which we call Whetstone, effectively and reliably configures a network\nfor a spiking hardware target with little, if any, loss in performance.\nWhetstone networks use single time step binary communication and do not require\na rate code or other spike-based coding scheme, thus producing networks\ncomparable in timing and size to conventional ANNs, albeit with binarized\ncommunication. We demonstrate Whetstone on a number of image classification\nnetworks, describing how the sharpening process interacts with different\ntraining optimizers and changes the distribution of activity within the\nnetwork. We further note that Whetstone is compatible with several\nnon-classification neural network applications, such as autoencoders and\nsemantic segmentation. Whetstone is widely extendable and currently implemented\nusing custom activation functions within the Keras wrapper to the popular\nTensorFlow machine learning framework.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 20:24:02 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Severa", "William", ""], ["Vineyard", "Craig M.", ""], ["Dellana", "Ryan", ""], ["Verzi", "Stephen J.", ""], ["Aimone", "James B.", ""]]}, {"id": "1810.11532", "submitter": "Jun He", "authors": "Jun He, Yu Chen and Yuren Zhou", "title": "A Theoretical Framework of Approximation Error Analysis of Evolutionary\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the empirical study of evolutionary algorithms, the solution quality is\nevaluated by either the fitness value or approximation error. The latter\nmeasures the fitness difference between an approximation solution and the\noptimal solution. Since the approximation error analysis is more convenient\nthan the direct estimation of the fitness value, this paper focuses on\napproximation error analysis. However, it is straightforward to extend all\nrelated results from the approximation error to the fitness value. Although the\nevaluation of solution quality plays an essential role in practice, few\nrigorous analyses have been conducted on this topic. This paper aims at\nestablishing a novel theoretical framework of approximation error analysis of\nevolutionary algorithms for discrete optimization. This framework is divided\ninto two parts. The first part is about exact expressions of the approximation\nerror. Two methods, Jordan form and Schur's triangularization, are presented to\nobtain an exact expression. The second part is about upper bounds on\napproximation error. Two methods, convergence rate and auxiliary matrix\niteration, are proposed to estimate the upper bound. The applicability of this\nframework is demonstrated through several examples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 21:20:25 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["He", "Jun", ""], ["Chen", "Yu", ""], ["Zhou", "Yuren", ""]]}, {"id": "1810.11672", "submitter": "Jun He Dr", "authors": "Yu Chen, Jun He", "title": "Average Convergence Rate of Evolutionary Algorithms II: Continuous\n  Optimization", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2020.12.076", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average convergence rate (ACR) measures how fast the approximation error\nof an evolutionary algorithm converges to zero per generation. It is defined as\nthe geometric average of the reduction rate of the approximation error over\nconsecutive generations. This paper makes a theoretical analysis of the ACR in\ncontinuous optimization. The obtained results are summarized as follows.\nAccording to the limit property, the ACR is classified into two categories: (1)\nlinear ACR whose limit inferior value is larger than a positive and (2)\nsublinear ACR whose value converges to zero. Then, it is proven that the ACR is\nlinear for evolutionary programming using positive landscape-adaptive mutation,\nbut sublinear for that using landscape-invariant or zero landscape-adaptive\nmutation. The relationship between the ACR and the decision space dimension is\nalso classified into two categories: (1) polynomial ACR whose value is larger\nthan the reciprocal of a polynomial function of the dimension for any\ngeneration, and (2) exponential ACR whose value is less than the reciprocal of\nan exponential function of the dimension for an exponential long period. It is\nproven that for easy problems such as linear functions, the ACR of the (1+1)\nadaptive random univariate search is polynomial. But for hard functions such as\nthe deceptive function, the ACR of both the (1+1) adaptive random univariate\nsearch and evolutionary programming is exponential.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 16:41:28 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 10:22:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Yu", ""], ["He", "Jun", ""]]}, {"id": "1810.11701", "submitter": "Dongchi Yu", "authors": "Dongchi Yu, Lu Wang", "title": "Hull Form Optimization with Principal Component Analysis and Deep Neural\n  Network", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and modifying complex hull forms for optimal vessel performances\nhave been a major challenge for naval architects. In the present study,\nPrincipal Component Analysis (PCA) is introduced to compress the geometric\nrepresentation of a group of existing vessels, and the resulting principal\nscores are manipulated to generate a large number of derived hull forms, which\nare evaluated computationally for their calm-water performances. The results\nare subsequently used to train a Deep Neural Network (DNN) to accurately\nestablish the relation between different hull forms and their associated\nperformances. Then, based on the fast, parallel DNN-based hull-form evaluation,\nthe large-scale search for optimal hull forms is performed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 20:37:47 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Yu", "Dongchi", ""], ["Wang", "Lu", ""]]}, {"id": "1810.11714", "submitter": "Andrew Hundt", "authors": "Andrew Hundt, Varun Jain, Chia-Hung Lin, Chris Paxton, Gregory D.\n  Hager", "title": "The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints", "comments": "This is a major revision refocusing the topic towards the JHU CoSTAR\n  Block Stacking Dataset, workspace constraints, and a comparison of HyperTrees\n  with hand-designed algorithms. 12 pages, 10 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot can now grasp an object more effectively than ever before, but once\nit has the object what happens next? We show that a mild relaxation of the task\nand workspace constraints implicit in existing object grasping datasets can\ncause neural network based grasping algorithms to fail on even a simple block\nstacking task when executed under more realistic circumstances.\n  To address this, we introduce the JHU CoSTAR Block Stacking Dataset (BSD),\nwhere a robot interacts with 5.1 cm colored blocks to complete an\norder-fulfillment style block stacking task. It contains dynamic scenes and\nreal time-series data in a less constrained environment than comparable\ndatasets. There are nearly 12,000 stacking attempts and over 2 million frames\nof real data. We discuss the ways in which this dataset provides a valuable\nresource for a broad range of other topics of investigation.\n  We find that hand-designed neural networks that work on prior datasets do not\ngeneralize to this task. Thus, to establish a baseline for this dataset, we\ndemonstrate an automated search of neural network based models using a novel\nmultiple-input HyperTree MetaModel, and find a final model which makes\nreasonable 3D pose predictions for grasping and stacking on our dataset.\n  The CoSTAR BSD, code, and instructions are available at\nhttps://sites.google.com/site/costardataset.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 21:26:42 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 23:17:17 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Hundt", "Andrew", ""], ["Jain", "Varun", ""], ["Lin", "Chia-Hung", ""], ["Paxton", "Chris", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1810.11760", "submitter": "Luis Lamb", "authors": "Felipe Grando, Lisando Z. Granville and Luis C. Lamb", "title": "Machine Learning in Network Centrality Measures: Tutorial and Outlook", "comments": "7 tables, 9 figures, version accepted at ACM Computing Surveys.\n  https://doi.org/10.1145/3237192", "journal-ref": "ACM Comput. Surv. 51, 5, Article 102 (October 2018), 32 pages", "doi": "10.1145/3237192", "report-no": null, "categories": "cs.LG cs.NE cs.NI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks are ubiquitous to several Computer Science domains.\nCentrality measures are an important analysis mechanism to uncover vital\nelements of complex networks. However, these metrics have high computational\ncosts and requirements that hinder their applications in large real-world\nnetworks. In this tutorial, we explain how the use of neural network learning\nalgorithms can render the application of the metrics in complex networks of\narbitrary size. Moreover, the tutorial describes how to identify the best\nconfiguration for neural network training and learning such for tasks, besides\npresenting an easy way to generate and acquire training data. We do so by means\nof a general methodology, using complex network models adaptable to any\napplication. We show that a regression model generated by the neural network\nsuccessfully approximates the metric values and therefore are a robust,\neffective alternative in real-world applications. The methodology and proposed\nmachine learning model use only a fraction of time with respect to other\napproximation algorithms, which is crucial in complex network applications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 04:51:08 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Grando", "Felipe", ""], ["Granville", "Lisando Z.", ""], ["Lamb", "Luis C.", ""]]}, {"id": "1810.11875", "submitter": "Yanan Sun", "authors": "Yanan Sun, Bing Xue, Mengjie Zhang and Gary G. Yen", "title": "Automatically Evolving CNN Architectures Based on Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Convolutional Neural Networks (CNNs) highly relies on\ntheir architectures. In order to design a CNN with promising performance,\nextended expertise in both CNNs and the investigated problem is required, which\nis not necessarily held by every user interested in CNNs or the problem domain.\nIn this paper, we propose to automatically evolve CNN architectures by using a\ngenetic algorithm based on ResNet blocks and DenseNet blocks. The proposed\nalgorithm is \\textbf{completely} automatic in designing CNN architectures,\nparticularly, neither pre-processing before it starts nor post-processing on\nthe designed CNN is needed. Furthermore, the proposed algorithm does not\nrequire users with domain knowledge on CNNs, the investigated problem or even\ngenetic algorithms. The proposed algorithm is evaluated on CIFAR10 and CIFAR100\nagainst 18 state-of-the-art peer competitors. Experimental results show that it\noutperforms state-of-the-art CNNs hand-crafted and CNNs designed by automatic\npeer competitors in terms of the classification accuracy, and achieves the\ncompetitive classification accuracy against semi-automatic peer competitors. In\naddition, the proposed algorithm consumes much less time than most peer\ncompetitors in finding the best CNN architectures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 20:21:27 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 23:34:51 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""], ["Yen", "Gary G.", ""]]}, {"id": "1810.11896", "submitter": "Nima Anari", "authors": "Nima Anari, Constantinos Daskalakis, Wolfgang Maass, Christos H.\n  Papadimitriou, Amin Saberi, Santosh Vempala", "title": "Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of\n  Neurons", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze linear independence of rank one tensors produced by tensor powers\nof randomly perturbed vectors. This enables efficient decomposition of sums of\nhigh-order tensors. Our analysis builds upon [BCMV14] but allows for a wider\nrange of perturbation models, including discrete ones. We give an application\nto recovering assemblies of neurons.\n  Assemblies are large sets of neurons representing specific memories or\nconcepts. The size of the intersection of two assemblies has been shown in\nexperiments to represent the extent to which these memories co-occur or these\nconcepts are related; the phenomenon is called association of assemblies. This\nsuggests that an animal's memory is a complex web of associations, and poses\nthe problem of recovering this representation from cognitive data. Motivated by\nthis problem, we study the following more general question: Can we reconstruct\nthe Venn diagram of a family of sets, given the sizes of their $\\ell$-wise\nintersections? We show that as long as the family of sets is randomly\nperturbed, it is enough for the number of measurements to be polynomially\nlarger than the number of nonempty regions of the Venn diagram to fully\nreconstruct the diagram.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 22:15:48 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Anari", "Nima", ""], ["Daskalakis", "Constantinos", ""], ["Maass", "Wolfgang", ""], ["Papadimitriou", "Christos H.", ""], ["Saberi", "Amin", ""], ["Vempala", "Santosh", ""]]}, {"id": "1810.11914", "submitter": "Dong Yin", "authors": "Dong Yin and Kannan Ramchandran and Peter Bartlett", "title": "Rademacher Complexity for Adversarially Robust Generalization", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning models are vulnerable to adversarial attacks; for\nexample, adding adversarial perturbations that are imperceptible to humans can\noften make machine learning models produce wrong predictions with high\nconfidence. Moreover, although we may obtain robust models on the training\ndataset via adversarial training, in some problems the learned models cannot\ngeneralize well to the test data. In this paper, we focus on $\\ell_\\infty$\nattacks, and study the adversarially robust generalization problem through the\nlens of Rademacher complexity. For binary linear classifiers, we prove tight\nbounds for the adversarial Rademacher complexity, and show that the adversarial\nRademacher complexity is never smaller than its natural counterpart, and it has\nan unavoidable dimension dependence, unless the weight vector has bounded\n$\\ell_1$ norm. The results also extend to multi-class linear classifiers. For\n(nonlinear) neural networks, we show that the dimension dependence in the\nadversarial Rademacher complexity also exists. We further consider a surrogate\nadversarial loss for one-hidden layer ReLU network and prove margin bounds for\nthis setting. Our results indicate that having $\\ell_1$ norm constraints on the\nweight matrices might be a potential way to improve generalization in the\nadversarial setting. We demonstrate experimental results that validate our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 00:51:08 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 06:40:59 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 07:03:12 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 04:23:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yin", "Dong", ""], ["Ramchandran", "Kannan", ""], ["Bartlett", "Peter", ""]]}, {"id": "1810.12065", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song", "title": "On the Convergence Rate of Training Recurrent Neural Networks", "comments": "V2/V3/V4 polish writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can local-search methods such as stochastic gradient descent (SGD) avoid\nbad local minima in training multi-layer neural networks? Why can they fit\nrandom labels even given non-convex and non-smooth architectures? Most existing\ntheory only covers networks with one hidden layer, so can we go deeper?\n  In this paper, we focus on recurrent neural networks (RNNs) which are\nmulti-layer networks widely used in natural language processing. They are\nharder to analyze than feedforward neural networks, because the $\\textit{same}$\nrecurrent unit is repeatedly applied across the entire time horizon of length\n$L$, which is analogous to feedforward networks of depth $L$. We show when the\nnumber of neurons is sufficiently large, meaning polynomial in the training\ndata size and in $L$, then SGD is capable of minimizing the regression loss in\nthe linear convergence rate. This gives theoretical evidence of how RNNs can\nmemorize data.\n  More importantly, in this paper we build general toolkits to analyze\nmulti-layer networks with ReLU activations. For instance, we prove why ReLU\nactivations can prevent exponential gradient explosion or vanishing, and build\na perturbation theory to analyze first-order approximation of multi-layer\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 11:45:02 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 15:25:15 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 11:47:47 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 10:08:59 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Song", "Zhao", ""]]}, {"id": "1810.12140", "submitter": "Thiago Santos TFS", "authors": "Thiago Santos and Sebastiao Xavier", "title": "A Convergence indicator for Multi-Objective Optimisation Algorithms", "comments": "Submitted to TEMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithms of multi-objective optimisation had a relative growth in the\nlast years. Thereby, it's requires some way of comparing the results of these.\nIn this sense, performance measures play a key role. In general, it's\nconsidered some properties of these algorithms such as capacity, convergence,\ndiversity or convergence-diversity. There are some known measures such as\ngenerational distance (GD), inverted generational distance (IGD), hypervolume\n(HV), Spread($\\Delta$), Averaged Hausdorff distance ($\\Delta_p$), R2-indicator,\namong others. In this paper, we focuses on proposing a new indicator to measure\nconvergence based on the traditional formula for Shannon entropy. The main\nfeatures about this measure are: 1) It does not require tho know the true\nPareto set and 2) Medium computational cost when compared with Hypervolume.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:21:34 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Santos", "Thiago", ""], ["Xavier", "Sebastiao", ""]]}, {"id": "1810.12162", "submitter": "Pranav Shyam", "authors": "Pranav Shyam, Wojciech Ja\\'skowski, Faustino Gomez", "title": "Model-Based Active Exploration", "comments": "ICML 2019. Code: https://github.com/nnaisense/max", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration is an unsolved problem in Reinforcement Learning which\nis usually addressed by reactively rewarding the agent for fortuitously\nencountering novel situations. This paper introduces an efficient active\nexploration algorithm, Model-Based Active eXploration (MAX), which uses an\nensemble of forward models to plan to observe novel events. This is carried out\nby optimizing agent behaviour with respect to a measure of novelty derived from\nthe Bayesian perspective of exploration, which is estimated using the\ndisagreement between the futures predicted by the ensemble members. We show\nempirically that in semi-random discrete environments where directed\nexploration is critical to make progress, MAX is at least an order of magnitude\nmore efficient than strong baselines. MAX scales to high-dimensional continuous\nenvironments where it builds task-agnostic models that can be used for any\ndownstream task.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:43:48 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 11:22:22 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 18:00:02 GMT"}, {"version": "v4", "created": "Mon, 13 May 2019 20:58:53 GMT"}, {"version": "v5", "created": "Thu, 13 Jun 2019 19:33:27 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Shyam", "Pranav", ""], ["Ja\u015bkowski", "Wojciech", ""], ["Gomez", "Faustino", ""]]}, {"id": "1810.12217", "submitter": "Elena Agliari", "authors": "Alberto Fachechi, Elena Agliari, Adriano Barra", "title": "Dreaming neural networks: forgetting spurious memories and reinforcing\n  pure ones", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": "Roma01.Math", "categories": "cs.NE cond-mat.dis-nn math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Hopfield model for associative neural networks accounts for\nbiological Hebbian learning and acts as the harmonic oscillator for pattern\nrecognition, however its maximal storage capacity is $\\alpha \\sim 0.14$, far\nfrom the theoretical bound for symmetric networks, i.e. $\\alpha =1$. Inspired\nby sleeping and dreaming mechanisms in mammal brains, we propose an extension\nof this model displaying the standard on-line (awake) learning mechanism (that\nallows the storage of external information in terms of patterns) and an\noff-line (sleep) unlearning$\\&$consolidating mechanism (that allows\nspurious-pattern removal and pure-pattern reinforcement): this obtained daily\nprescription is able to saturate the theoretical bound $\\alpha=1$, remaining\nalso extremely robust against thermal noise. Both neural and synaptic features\nare analyzed both analytically and numerically. In particular, beyond obtaining\na phase diagram for neural dynamics, we focus on synaptic plasticity and we\ngive explicit prescriptions on the temporal evolution of the synaptic matrix.\nWe analytically prove that our algorithm makes the Hebbian kernel converge with\nhigh probability to the projection matrix built over the pure stored patterns.\nFurthermore, we obtain a sharp and explicit estimate for the \"sleep rate\" in\norder to ensure such a convergence. Finally, we run extensive numerical\nsimulations (mainly Monte Carlo sampling) to check the approximations\nunderlying the analytical investigations (e.g., we developed the whole theory\nat the so called replica-symmetric level, as standard in the\nAmit-Gutfreund-Sompolinsky reference framework) and possible finite-size\neffects, finding overall full agreement with the theory.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 16:09:02 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Fachechi", "Alberto", ""], ["Agliari", "Elena", ""], ["Barra", "Adriano", ""]]}, {"id": "1810.12411", "submitter": "Sergiy Bokhnyak", "authors": "Heng xin Fun, Sergiy V Bokhnyak, Francesco Saverio Zuppichini", "title": "Counting in Language with RNNs", "comments": "Withdrawing due to lack of key acknowledgements and follow up work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine a possible reason for the LSTM outperforming the GRU\non language modeling and more specifically machine translation. We hypothesize\nthat this has to do with counting. This is a consistent theme across the\nliterature of long term dependence, counting, and language modeling for RNNs.\nUsing the simplified forms of language -- Context-Free and Context-Sensitive\nLanguages -- we show how exactly the LSTM performs its counting based on their\ncell states during inference and why the GRU cannot perform as well.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 21:11:07 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 14:07:15 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Fun", "Heng xin", ""], ["Bokhnyak", "Sergiy V", ""], ["Zuppichini", "Francesco Saverio", ""]]}, {"id": "1810.12436", "submitter": "Wei Wang", "authors": "Shibo Zhou, Wei Wang", "title": "Object Detection based on LIDAR Temporal Pulses using Spiking Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks has been successfully used in the processing of Lidar data,\nespecially in the scenario of autonomous driving. However, existing methods\nheavily rely on pre-processing of the pulse signals derived from Lidar sensors\nand therefore result in high computational overhead and considerable latency.\nIn this paper, we proposed an approach utilizing Spiking Neural Network (SNN)\nto address the object recognition problem directly with raw temporal pulses. To\nhelp with the evaluation and benchmarking, a comprehensive temporal pulses\ndata-set was created to simulate Lidar reflection in different road scenarios.\nBeing tested with regard to recognition accuracy and time efficiency under\ndifferent noise conditions, our proposed method shows remarkable performance\nwith the inference accuracy up to 99.83% (with 10% noise) and the average\nrecognition delay as low as 265 ns. It highlights the potential of SNN in\nautonomous driving and some related applications. In particular, to our best\nknowledge, this is the first attempt to use SNN to directly perform object\nrecognition on raw Lidar temporal pulses.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:21:33 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhou", "Shibo", ""], ["Wang", "Wei", ""]]}, {"id": "1810.12456", "submitter": "Shuai Tang", "authors": "Shuai Tang, Paul Smolensky, Virginia R. de Sa", "title": "A Simple Recurrent Unit with Reduced Tensor Product Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  idely used recurrent units, including Long-short Term Memory (LSTM) and the\nGated Recurrent Unit (GRU), perform well on natural language tasks, but their\nability to learn structured representations is still questionable. Exploiting\nreduced Tensor Product Representations (TPRs) --- distributed representations\nof symbolic structure in which vector-embedded symbols are bound to\nvector-embedded structural positions --- we propose the TPRU, a simple\nrecurrent unit that, at each time step, explicitly executes structural-role\nbinding and unbinding operations to incorporate structural information into\nlearning. A gradient analysis of our proposed TPRU is conducted to support our\nmodel design, and its performance on multiple datasets shows the effectiveness\nof our design choices. Furthermore, observations on a linguistically grounded\nstudy demonstrate the interpretability of our TPRU.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 23:31:39 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 23:18:56 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 06:05:25 GMT"}, {"version": "v4", "created": "Thu, 31 Jan 2019 17:02:40 GMT"}, {"version": "v5", "created": "Mon, 27 May 2019 04:50:59 GMT"}, {"version": "v6", "created": "Tue, 5 Nov 2019 10:38:38 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Tang", "Shuai", ""], ["Smolensky", "Paul", ""], ["de Sa", "Virginia R.", ""]]}, {"id": "1810.12470", "submitter": "Thomas Gabor", "authors": "Thomas Gabor, Lenz Belzner, Claudia Linnhoff-Popien", "title": "Inheritance-Based Diversity Measures for Explicit Convergence Control in\n  Evolutionary Algorithms", "comments": "GECCO '18: Genetic and Evolutionary Computation Conference, 2018,\n  Kyoto, Japan", "journal-ref": null, "doi": "10.1145/3205455.3205630", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity is an important factor in evolutionary algorithms to prevent\npremature convergence towards a single local optimum. In order to maintain\ndiversity throughout the process of evolution, various means exist in\nliterature. We analyze approaches to diversity that (a) have an explicit and\nquantifiable influence on fitness at the individual level and (b) require no\n(or very little) additional domain knowledge such as domain-specific distance\nfunctions. We also introduce the concept of genealogical diversity in a broader\nstudy. We show that employing these approaches can help evolutionary algorithms\nfor global optimization in many cases.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 00:51:02 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Gabor", "Thomas", ""], ["Belzner", "Lenz", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "1810.12483", "submitter": "Thomas Gabor", "authors": "Thomas Gabor, Lenz Belzner, Thomy Phan, Kyrill Schmid", "title": "Preparing for the Unexpected: Diversity Improves Planning Resilience in\n  Evolutionary Algorithms", "comments": "ICAC, 2018, Trento", "journal-ref": null, "doi": "10.1109/ICAC.2018.00023", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automatic optimization techniques find their way into industrial\napplications, the behavior of many complex systems is determined by some form\nof planner picking the right actions to optimize a given objective function. In\nmany cases, the mapping of plans to objective reward may change due to\nunforeseen events or circumstances in the real world. In those cases, the\nplanner usually needs some additional effort to adjust to the changed situation\nand reach its previous level of performance. Whenever we still need to continue\npolling the planner even during re-planning, it oftentimes exhibits severely\nlacking performance. In order to improve the planner's resilience to unforeseen\nchange, we argue that maintaining a certain level of diversity amongst the\nconsidered plans at all times should be added to the planner's objective.\nEffectively, we encourage the planner to keep alternative plans to its\ncurrently best solution. As an example case, we implement a diversity-aware\ngenetic algorithm using two different metrics for diversity (differing in their\ngenerality) and show that the blow in performance due to unexpected change can\nbe severely lessened in the average case. We also analyze the parameter\nsettings necessary for these techniques in order to gain an intuition how they\ncan be incorporated into larger frameworks or process models for software and\nsystems engineering.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 01:49:43 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Gabor", "Thomas", ""], ["Belzner", "Lenz", ""], ["Phan", "Thomy", ""], ["Schmid", "Kyrill", ""]]}, {"id": "1810.12640", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Bernard Girau, Nicolas Rougier, Andres Upegui, Benoit\n  Miramond", "title": "Neuromorphic hardware as a self-organizing computing system", "comments": "Published in IEEE World Congress on Computational Intelligence\n  (WCCI), International Workshop: Neuromorphic Hardware in Practice and Use\n  (NHPU), Jul. 2018, Rio de Janeiro, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the self-organized neuromorphic architecture named SOMA.\nThe objective is to study neural-based self-organization in computing systems\nand to prove the feasibility of a self-organizing hardware structure.\nConsidering that these properties emerge from large scale and fully connected\nneural maps, we will focus on the definition of a self-organizing hardware\narchitecture based on digital spiking neurons that offer hardware efficiency.\nFrom a biological point of view, this corresponds to a combination of the\nso-called synaptic and structural plasticities. We intend to define\ncomputational models able to simultaneously self-organize at both computation\nand communication levels, and we want these models to be hardware-compliant,\nfault tolerant and scalable by means of a neuro-cellular structure.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:35:07 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Khacef", "Lyes", ""], ["Girau", "Bernard", ""], ["Rougier", "Nicolas", ""], ["Upegui", "Andres", ""], ["Miramond", "Benoit", ""]]}, {"id": "1810.12742", "submitter": "Felix Huber", "authors": "Felix Huber", "title": "Efficient Tree Solver for Hines Matrices on the GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain consists of a large number of interconnected neurons\ncommunicating via exchange of electrical spikes. Simulations play an important\nrole in better understanding electrical activity in the brain and offers a way\nto to compare measured data to simulated data such that experimental data can\nbe interpreted better. A key component in such simulations is an efficient\nsolver for the Hines matrices used in computing inter-neuron signal\npropagation. In order to achieve high performance simulations, it is crucial to\nhave an efficient solver algorithm. In this report we explain a new parallel\nGPU solver for these matrices which offers fine grained parallelization and\nallows for work balancing during the simulation setup.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 14:00:02 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 09:32:29 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Huber", "Felix", ""]]}, {"id": "1810.12752", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong, Xin Lin, Kang Chen, Qingyang Li, and Kaizhu Huang", "title": "Long Short-Term Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is an important cognition process of humans, which helps humans\nconcentrate on critical information during their perception and learning.\nHowever, although many machine learning models can remember information of\ndata, they have no the attention mechanism. For example, the long short-term\nmemory (LSTM) network is able to remember sequential information, but it cannot\npay special attention to part of the sequences. In this paper, we present a\nnovel model called long short-term attention (LSTA), which seamlessly\nintegrates the attention mechanism into the inner cell of LSTM. More than\nprocessing long short term dependencies, LSTA can focus on important\ninformation of the sequences with the attention mechanism. Extensive\nexperiments demonstrate that LSTA outperforms LSTM and related models on the\nsequence learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 14:08:30 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 02:44:15 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Lin", "Xin", ""], ["Chen", "Kang", ""], ["Li", "Qingyang", ""], ["Huang", "Kaizhu", ""]]}, {"id": "1810.12754", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong, Guohua Yue and Xiao Ling", "title": "Recurrent Attention Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) has been successfully applied in many sequence\nlearning problems. Such as handwriting recognition, image description, natural\nlanguage processing and video motion analysis. After years of development,\nresearchers have improved the internal structure of the RNN and introduced many\nvariants. Among others, Gated Recurrent Unit (GRU) is one of the most widely\nused RNN model. However, GRU lacks the capability of adaptively paying\nattention to certain regions or locations, so that it may cause information\nredundancy or loss during leaning. In this paper, we propose a RNN model,\ncalled Recurrent Attention Unit (RAU), which seamlessly integrates the\nattention mechanism into the interior of GRU by adding an attention gate. The\nattention gate can enhance GRU's ability to remember long-term memory and help\nmemory cells quickly discard unimportant content. RAU is capable of extracting\ninformation from the sequential data by adaptively selecting a sequence of\nregions or locations and pay more attention to the selected regions during\nlearning. Extensive experiments on image classification, sentiment\nclassification and language modeling show that RAU consistently outperforms GRU\nand other baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 14:09:19 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Yue", "Guohua", ""], ["Ling", "Xiao", ""]]}, {"id": "1810.12805", "submitter": "Tristan Milne", "authors": "Tristan Milne", "title": "Piecewise Strong Convexity of Neural Networks", "comments": "16 pages, 2 figures. NeurIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the loss surface of a feed-forward neural network with ReLU\nnon-linearities, regularized with weight decay. We show that the regularized\nloss function is piecewise strongly convex on an important open set which\ncontains, under some conditions, all of its global minimizers. This is used to\nprove that local minima of the regularized loss function in this set are\nisolated, and that every differentiable critical point in this set is a local\nminimum, partially addressing an open problem given at the Conference on\nLearning Theory (COLT) 2015; our result is also applied to linear neural\nnetworks to show that with weight decay regularization, there are no non-zero\ncritical points in a norm ball obtaining training error below a given\nthreshold. We also include an experimental section where we validate our\ntheoretical work and show that the regularized loss function is almost always\npiecewise strongly convex when restricted to stochastic gradient descent\ntrajectories for three standard image classification problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:17:56 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 01:27:52 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 18:22:39 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Milne", "Tristan", ""]]}, {"id": "1810.13166", "submitter": "Natalia D\\'iaz-Rodr\\'iguez", "authors": "Natalia D\\'iaz-Rodr\\'iguez and Vincenzo Lomonaco and David Filliat and\n  Davide Maltoni", "title": "Don't forget, there is more than forgetting: new metrics for Continual\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning consists of algorithms that learn from a stream of\ndata/tasks continuously and adaptively thought time, enabling the incremental\ndevelopment of ever more complex knowledge and skills. The lack of consensus in\nevaluating continual learning algorithms and the almost exclusive focus on\nforgetting motivate us to propose a more comprehensive set of implementation\nindependent metrics accounting for several factors we believe have practical\nimplications worth considering in the deployment of real AI systems that learn\ncontinually: accuracy or performance over time, backward and forward knowledge\ntransfer, memory overhead as well as computational efficiency. Drawing\ninspiration from the standard Multi-Attribute Value Theory (MAVT) we further\npropose to fuse these metrics into a single score for ranking purposes and we\nevaluate our proposal with five continual learning strategies on the iCIFAR-100\ncontinual learning benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:15:02 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Lomonaco", "Vincenzo", ""], ["Filliat", "David", ""], ["Maltoni", "Davide", ""]]}, {"id": "1810.13197", "submitter": "Valentin Vielzeuf", "authors": "Valentin Vielzeuf, Corentin Kervadec, St\\'ephane Pateux, Fr\\'ed\\'eric\n  Jurie", "title": "The Many Moods of Emotion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to the facial expression generation\nproblem. Building upon the assumption of the psychological community that\nemotion is intrinsically continuous, we first design our own continuous emotion\nrepresentation with a 3-dimensional latent space issued from a neural network\ntrained on discrete emotion classification. The so-obtained representation can\nbe used to annotate large in the wild datasets and later used to trained a\nGenerative Adversarial Network. We first show that our model is able to map\nback to discrete emotion classes with a objectively and subjectively better\nquality of the images than usual discrete approaches. But also that we are able\nto pave the larger space of possible facial expressions, generating the many\nmoods of emotion. Moreover, two axis in this space may be found to generate\nsimilar expression changes as in traditional continuous representations such as\narousal-valence. Finally we show from visual interpretation, that the third\nremaining dimension is highly related to the well-known dominance dimension\nfrom psychology.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:24:08 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Kervadec", "Corentin", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1810.13292", "submitter": "Duc Tam Nguyen", "authors": "Duc Tam Nguyen, Zhongyu Lou, Michael Klar, Thomas Brox", "title": "Anomaly Detection With Multiple-Hypotheses Predictions", "comments": "In proceedings of the 36th International Conference on Machine\n  Learning (ICML), Long Beach, California, PMLR 97, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one-class-learning tasks, only the normal case (foreground) can be modeled\nwith data, whereas the variation of all possible anomalies is too erratic to be\ndescribed by samples. Thus, due to the lack of representative data, the\nwide-spread discriminative approaches cannot cover such learning tasks, and\nrather generative models, which attempt to learn the input density of the\nforeground, are used. However, generative models suffer from a large input\ndimensionality (as in images) and are typically inefficient learners. We\npropose to learn the data distribution of the foreground more efficiently with\na multi-hypotheses autoencoder. Moreover, the model is criticized by a\ndiscriminator, which prevents artificial data modes not supported by data, and\nenforces diversity across hypotheses. Our multiple-hypothesesbased anomaly\ndetection framework allows the reliable identification of out-of-distribution\nsamples. For anomaly detection on CIFAR-10, it yields up to 3.9% points\nimprovement over previously reported results. On a real anomaly detection task,\nthe approach reduces the error of the baseline models from 6.8% to 1.5%.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:05:44 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 12:36:22 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 06:13:45 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 15:05:09 GMT"}, {"version": "v5", "created": "Fri, 31 May 2019 12:25:42 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Nguyen", "Duc Tam", ""], ["Lou", "Zhongyu", ""], ["Klar", "Michael", ""], ["Brox", "Thomas", ""]]}, {"id": "1810.13329", "submitter": "Doyun Kim", "authors": "Doyun Kim, Han Young Yim, Sanghyuck Ha, Changgwun Lee, and Inyup Kang", "title": "Convolutional Neural Network Quantization using Generalized Gamma\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As edge applications using convolutional neural networks (CNN) models grow,\nit is becoming necessary to introduce dedicated hardware accelerators in which\nnetwork parameters and feature-map data are represented with limited precision.\nIn this paper we propose a novel quantization algorithm for energy-efficient\ndeployment of the hardware accelerators. For weights and biases, the optimal\nbit length of the fractional part is determined so that the quantization error\nis minimized over their distribution. For feature-map data, meanwhile, their\nsample distribution is well approximated with the generalized gamma\ndistribution (GGD), and accordingly the optimal quantization step size can be\nobtained through the asymptotical closed form solution of GGD. The proposed\nquantization algorithm has a higher signal-to-quantization-noise ratio (SQNR)\nthan other quantization schemes previously proposed for CNNs, and even can be\nmore improved by tuning the quantization parameters, resulting in efficient\nimplementation of the hardware accelerators for CNNs in terms of power\nconsumption and memory bandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:17:05 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Kim", "Doyun", ""], ["Yim", "Han Young", ""], ["Ha", "Sanghyuck", ""], ["Lee", "Changgwun", ""], ["Kang", "Inyup", ""]]}]