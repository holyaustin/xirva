[{"id": "1607.00036", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio", "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend neural Turing machine (NTM) model into a dynamic neural Turing\nmachine (D-NTM) by introducing a trainable memory addressing scheme. This\naddressing scheme maintains for each memory cell two separate vectors, content\nand address vectors. This allows the D-NTM to learn a wide variety of\nlocation-based addressing strategies including both linear and nonlinear ones.\nWe implement the D-NTM with both continuous, differentiable and discrete,\nnon-differentiable read/write mechanisms. We investigate the mechanisms and\neffects of learning to read and write into a memory through experiments on\nFacebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is\nevaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM\nbaselines. We have done extensive analysis of our model and different\nvariations of NTM on bAbI task. We also provide further experimental results on\nsequential pMNIST, Stanford Natural Language Inference, associative recall and\ncopy tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:45:12 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 05:56:48 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Chandar", "Sarath", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1607.00318", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "The Evolution of Sex through the Baldwin Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests that the fundamental haploid-diploid cycle of eukaryotic\nsex exploits a rudimentary form of the Baldwin effect. With this explanation\nfor the basic cycle, the other associated phenomena can be explained as\nevolution tuning the amount and frequency of learning experienced by an\norganism. Using the well-known NK model of fitness landscapes it is shown that\nvarying landscape ruggedness varies the benefit of the haploid-diploid cycle,\nwhether based upon endomitosis or syngamy. The utility of pre-meiotic doubling\nand recombination during the cycle are also shown to vary with landscape\nruggedness. This view is suggested as underpinning, rather than contradicting,\nmany existing explanations for sex.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 17:02:16 GMT"}, {"version": "v10", "created": "Fri, 25 Nov 2016 12:31:28 GMT"}, {"version": "v11", "created": "Tue, 7 Feb 2017 18:22:53 GMT"}, {"version": "v12", "created": "Thu, 9 Feb 2017 18:16:48 GMT"}, {"version": "v13", "created": "Fri, 24 Feb 2017 17:11:59 GMT"}, {"version": "v14", "created": "Wed, 22 Mar 2017 10:47:01 GMT"}, {"version": "v15", "created": "Wed, 31 May 2017 12:21:54 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 10:12:41 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 10:29:17 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 13:04:28 GMT"}, {"version": "v5", "created": "Tue, 2 Aug 2016 11:25:48 GMT"}, {"version": "v6", "created": "Wed, 3 Aug 2016 10:22:13 GMT"}, {"version": "v7", "created": "Fri, 5 Aug 2016 09:29:32 GMT"}, {"version": "v8", "created": "Mon, 8 Aug 2016 08:00:49 GMT"}, {"version": "v9", "created": "Mon, 22 Aug 2016 10:11:24 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1607.00765", "submitter": "Kaveh Hassani", "authors": "Kaveh Hassani and Won-Sook Lee", "title": "Multi-Objective Design of State Feedback Controllers Using Reinforced\n  Quantum-Behaved Particle Swarm Optimization", "comments": null, "journal-ref": "Applied Soft Computing, 41, pp. 66-76, 2016", "doi": "10.1016/j.asoc.2015.12.024", "report-no": null, "categories": "cs.NE cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel and generic multi-objective design paradigm is\nproposed which utilizes quantum-behaved PSO(QPSO) for deciding the optimal\nconfiguration of the LQR controller for a given problem considering a set of\ncompeting objectives. There are three main contributions introduced in this\npaper as follows. (1) The standard QPSO algorithm is reinforced with an\ninformed initialization scheme based on the simulated annealing algorithm and\nGaussian neighborhood selection mechanism. (2) It is also augmented with a\nlocal search strategy which integrates the advantages of memetic algorithm into\nconventional QPSO. (3) An aggregated dynamic weighting criterion is introduced\nthat dynamically combines the soft and hard constraints with control objectives\nto provide the designer with a set of Pareto optimal solutions and lets her to\ndecide the target solution based on practical preferences. The proposed method\nis compared against a gradient-based method, seven meta-heuristics, and the\ntrial-and-error method on two control benchmarks using sensitivity analysis and\nfull factorial parameter selection and the results are validated using\none-tailed T-test. The experimental results suggest that the proposed method\noutperforms opponent methods in terms of controller effort, measures associated\nwith transient response and criteria related to steady-state.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 08:24:42 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hassani", "Kaveh", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1607.01628", "submitter": "Evgeny Matusov", "authors": "Wenhu Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter", "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective way for biasing the attention\nmechanism of a sequence-to-sequence neural machine translation (NMT) model\ntowards the well-studied statistical word alignment models. We show that our\nnovel guided alignment training approach improves translation quality on\nreal-life e-commerce texts consisting of product titles and descriptions,\novercoming the problems posed by many unknown words and a large type/token\nratio. We also show that meta-data associated with input texts such as topic or\ncategory information can significantly improve translation quality when used as\nan additional signal to the decoder part of the network. With both novel\nfeatures, the BLEU score of the NMT system on a product title set improves from\n18.6 to 21.3%. Even larger MT quality gains are obtained through domain\nadaptation of a general domain NMT system to e-commerce data. The developed NMT\nsystem also performs well on the IWSLT speech translation task, where an\nensemble of four variant systems outperforms the phrase-based baseline by 2.1%\nBLEU absolute.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:13:12 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Chen", "Wenhu", ""], ["Matusov", "Evgeny", ""], ["Khadivi", "Shahram", ""], ["Peter", "Jan-Thorsten", ""]]}, {"id": "1607.01691", "submitter": "Emmanuel Osegi", "authors": "Vincent Ike Anireh and Emmanuel Ndidi Osegi", "title": "A Modified Activation Function with Improved Run-Times For Neural\n  Networks", "comments": "22pages, 12 figures, 3 tables; Submitted for Publication", "journal-ref": "Advances in Multidisciplinary & Scientific Research Journal. Vol.\n  3. No.2, Pp 33-44", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a modified version of the Hyperbolic Tangent\nActivation Function as a learning unit generator for neural networks. The\nfunction uses an integer calibration constant as an approximation to the Euler\nnumber, e, based on a quadratic Real Number Formula (RNF) algorithm and an\nadaptive normalization constraint on the input activations to avoid the\nvanishing gradient. We demonstrate the effectiveness of the proposed\nmodification using a hypothetical and real world dataset and show that lower\nrun-times can be achieved by learning algorithms using this function leading to\nimproved speed-ups and learning accuracies during training.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 16:05:52 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Anireh", "Vincent Ike", ""], ["Osegi", "Emmanuel Ndidi", ""]]}, {"id": "1607.01719", "submitter": "Baochen Sun", "authors": "Baochen Sun, Kate Saenko", "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are able to learn powerful representations from large\nquantities of labeled input data, however they cannot always generalize well\nacross changes in input distributions. Domain adaptation algorithms have been\nproposed to compensate for the degradation in performance due to domain shift.\nIn this paper, we address the case when the target domain is unlabeled,\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\ndomain adaptation method that aligns the second-order statistics of the source\nand target distributions with a linear transformation. Here, we extend CORAL to\nlearn a nonlinear transformation that aligns correlations of layer activations\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\ndatasets show state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Sun", "Baochen", ""], ["Saenko", "Kate", ""]]}, {"id": "1607.01730", "submitter": "Jialin Liu Ph.D", "authors": "Jialin Liu, Diego P\\'erez-Li\\'ebana and Simon M. Lucas", "title": "Rolling Horizon Coevolutionary Planning for Two-Player Video Games", "comments": "2 figures, 1 table, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new algorithm for decision making in two-player\nreal-time video games. As with Monte Carlo Tree Search, the algorithm can be\nused without heuristics and has been developed for use in general video game\nAI. The approach is to extend recent work on rolling horizon evolutionary\nplanning, which has been shown to work well for single-player games, to two (or\nin principle many) player games. To select an action the algorithm co-evolves\ntwo (or in the general case N) populations, one for each player, where each\nindividual is a sequence of actions for the respective player. The fitness of\neach individual is evaluated by playing it against a selection of\naction-sequences from the opposing population. When choosing an action to take\nin the game, the first action is chosen from the fittest member of the\npopulation for that player. The new algorithm is compared with a number of\ngeneral video game AI algorithms on three variations of a two-player space\nbattle game, with promising results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 18:03:18 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Liu", "Jialin", ""], ["P\u00e9rez-Li\u00e9bana", "Diego", ""], ["Lucas", "Simon M.", ""]]}, {"id": "1607.01750", "submitter": "Alyssa Adams", "authors": "Alyssa M Adams, Hector Zenil, Paul CW Davies, Sara I Walker", "title": "Formal Definitions of Unbounded Evolution and Innovation Reveal\n  Universal Mechanisms for Open-Ended Evolution in Dynamical Systems", "comments": "Main document: 17 pages, Supplement: 21 pages Presented at OEE2: The\n  Second Workshop on Open-Ended Evolution, 15th International Conference on the\n  Synthesis and Simulation of Living Systems (ALIFE XV), Canc\\'un, Mexico, 4-8\n  July 2016 (http://www.tim-taylor.com/oee2/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended evolution (OEE) is relevant to a variety of biological, artificial\nand technological systems, but has been challenging to reproduce in silico.\nMost theoretical efforts focus on key aspects of open-ended evolution as it\nappears in biology. We recast the problem as a more general one in dynamical\nsystems theory, providing simple criteria for open-ended evolution based on two\nhallmark features: unbounded evolution and innovation. We define unbounded\nevolution as patterns that are non-repeating within the expected Poincare\nrecurrence time of an equivalent isolated system, and innovation as\ntrajectories not observed in isolated systems. As a case study, we implement\nnovel variants of cellular automata (CA) in which the update rules are allowed\nto vary with time in three alternative ways. Each is capable of generating\nconditions for open-ended evolution, but vary in their ability to do so. We\nfind that state-dependent dynamics, widely regarded as a hallmark of life,\nstatistically out-performs other candidate mechanisms, and is the only\nmechanism to produce open-ended evolution in a scalable manner, essential to\nthe notion of ongoing evolution. This analysis suggests a new framework for\nunifying mechanisms for generating OEE with features distinctive to life and\nits artifacts, with broad applicability to biological and artificial systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 19:20:59 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 07:15:34 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Adams", "Alyssa M", ""], ["Zenil", "Hector", ""], ["Davies", "Paul CW", ""], ["Walker", "Sara I", ""]]}, {"id": "1607.01963", "submitter": "Liang Lu", "authors": "Liang Lu", "title": "Sequence Training and Adaptation of Highway Deep Neural Networks", "comments": "6 pages, 3 figures, published at IEEE SLT 2016. arXiv admin note:\n  text overlap with arXiv:1610.05812", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highway deep neural network (HDNN) is a type of depth-gated feedforward\nneural network, which has shown to be easier to train with more hidden layers\nand also generalise better compared to conventional plain deep neural networks\n(DNNs). Previously, we investigated a structured HDNN architecture for speech\nrecognition, in which the two gate functions were tied across all the hidden\nlayers, and we were able to train a much smaller model without sacrificing the\nrecognition accuracy. In this paper, we carry on the study of this architecture\nwith sequence-discriminative training criterion and speaker adaptation\ntechniques on the AMI meeting speech recognition corpus. We show that these two\ntechniques improve speech recognition accuracy on top of the model trained with\nthe cross entropy criterion. Furthermore, we demonstrate that the two gate\nfunctions that are tied across all the hidden layers are able to control the\ninformation flow over the whole network, and we can achieve considerable\nimprovements by only updating these gate functions in both sequence training\nand adaptation experiments.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 11:24:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 15:19:55 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 10:10:30 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 10:23:26 GMT"}, {"version": "v5", "created": "Wed, 22 Mar 2017 15:59:30 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Lu", "Liang", ""]]}, {"id": "1607.02028", "submitter": "Nadir Murru", "authors": "Giuseppe Air\\`o Farulla, Tiziana Armano, Anna Capietto, Nadir Murru,\n  Rosaria Rossini", "title": "Artificial neural networks and fuzzy logic for recognizing alphabet\n  characters and mathematical symbols", "comments": null, "journal-ref": "Lecture Notes in Computer Science, Volume 9759 2016, Computers\n  Helping People with Special Needs, p. 7-14", "doi": "10.1007/978-3-319-41264-1_1", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition software (OCR) are important tools for\nobtaining accessible texts. We propose the use of artificial neural networks\n(ANN) in order to develop pattern recognition algorithms capable of recognizing\nboth normal texts and formulae. We present an original improvement of the\nbackpropagation algorithm. Moreover, we describe a novel image segmentation\nalgorithm that exploits fuzzy logic for separating touching characters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 12:23:47 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Farulla", "Giuseppe Air\u00f2", ""], ["Armano", "Tiziana", ""], ["Capietto", "Anna", ""], ["Murru", "Nadir", ""], ["Rossini", "Rosaria", ""]]}, {"id": "1607.02250", "submitter": "Yiming Cui", "authors": "Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang and Guoping Hu", "title": "Consensus Attention-based Neural Networks for Chinese Reading\n  Comprehension", "comments": "9+1 pages, published at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading comprehension has embraced a booming in recent NLP research. Several\ninstitutes have released the Cloze-style reading comprehension data, and these\nhave greatly accelerated the research of machine comprehension. In this work,\nwe firstly present Chinese reading comprehension datasets, which consist of\nPeople Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we\npropose a consensus attention-based neural network architecture to tackle the\nCloze-style reading comprehension problem, which aims to induce a consensus\nattention over every words in the query. Experimental results show that the\nproposed neural network significantly outperforms the state-of-the-art\nbaselines in several public datasets. Furthermore, we setup a baseline for\nChinese reading comprehension task, and hopefully this would speed up the\nprocess for future research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:46:48 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 05:49:42 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 09:21:09 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Chen", "Zhipeng", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1607.02303", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label\n  Tree Embeddings for Audio Scene Recognition", "comments": "Task1 technical report for the DCASE2016 challenge. arXiv admin note:\n  text overlap with arXiv:1606.07908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this report our audio scene recognition system submitted to\nthe DCASE 2016 challenge. Firstly, given the label set of the scenes, a label\ntree is automatically constructed. This category taxonomy is then used in the\nfeature extraction step in which an audio scene instance is represented by a\nlabel tree embedding image. Different convolutional neural networks, which are\ntailored for the task at hand, are finally learned on top of the image features\nfor scene recognition. Our system reaches an overall recognition accuracy of\n81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute\nimprovements of 8.7% and 6.1% on the development and test data, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:39:05 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:05:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02467", "submitter": "Marc Dymetman", "authors": "Marc Dymetman, Chunyang Xiao", "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge", "comments": "Updated version of arXiv:1607.02467. Presented at the NIPS-2016 RNN\n  Symposium, Barcelona, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 17:35:51 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 10:56:22 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Dymetman", "Marc", ""], ["Xiao", "Chunyang", ""]]}, {"id": "1607.02488", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Kevin Gimpel", "title": "Adjusting for Dropout Variance in Batch Normalization and Weight\n  Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to adjust for the variance introduced by dropout with corrections\nto weight initialization and Batch Normalization, yielding higher accuracy.\nThough dropout can preserve the expected input to a neuron between train and\ntest, the variance of the input differs. We thus propose a new weight\ninitialization by correcting for the influence of dropout rates and an\narbitrary nonlinearity's influence on variance through simple corrective\nscalars. Since Batch Normalization trained with dropout estimates the variance\nof a layer's incoming distribution with some inputs dropped, the variance also\ndiffers between train and test. After training a network with Batch\nNormalization and dropout, we simply update Batch Normalization's variance\nmoving averages with dropout off and obtain state of the art on CIFAR-10 and\nCIFAR-100 without data augmentation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 18:39:47 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:36:50 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1607.02717", "submitter": "Manuel Mazzara", "authors": "Roman Bauer, Lukas Breitwieser, Alberto Di Meglio, Leonard Johard,\n  Marcus Kaiser, Marco Manca, Manuel Mazzara, Max Talanov", "title": "The BioDynaMo Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulations have become a very powerful tool for scientific\nresearch. Given the vast complexity that comes with many open scientific\nquestions, a purely analytical or experimental approach is often not viable.\nFor example, biological systems (such as the human brain) comprise an extremely\ncomplex organization and heterogeneous interactions across different spatial\nand temporal scales. In order to facilitate research on such problems, the\nBioDynaMo project (\\url{https://biodynamo.web.cern.ch/}) aims at a general\nplatform for computer simulations for biological research. Since the scientific\ninvestigations require extensive computer resources, this platform should be\nexecutable on hybrid cloud computing systems, allowing for the efficient use of\nstate-of-the-art computing technology. This paper describes challenges during\nthe early stages of the software development process. In particular, we\ndescribe issues regarding the implementation and the highly interdisciplinary\nas well as international nature of the collaboration. Moreover, we explain the\nmethodologies, the approach, and the lessons learnt by the team during these\nfirst stages.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 08:54:30 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Bauer", "Roman", ""], ["Breitwieser", "Lukas", ""], ["Di Meglio", "Alberto", ""], ["Johard", "Leonard", ""], ["Kaiser", "Marcus", ""], ["Manca", "Marco", ""], ["Mazzara", "Manuel", ""], ["Talanov", "Max", ""]]}, {"id": "1607.02857", "submitter": "Lars Hertel", "authors": "Lars Hertel, Huy Phan, Alfred Mertins", "title": "Classifying Variable-Length Audio Files with All-Convolutional Networks\n  and Masked Global Pooling", "comments": "Technical report for the DCASE-2016 challenge (task 1 and task 4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a deep all-convolutional neural network with masked global pooling\nto perform single-label classification for acoustic scene classification and\nmulti-label classification for domestic audio tagging in the DCASE-2016\ncontest. Our network achieved an average accuracy of 84.5% on the four-fold\ncross-validation for acoustic scene recognition, compared to the provided\nbaseline of 72.5%, and an average equal error rate of 0.17 for domestic audio\ntagging, compared to the baseline of 0.21. The network therefore improves the\nbaselines by a relative amount of 17% and 19%, respectively. The network only\nconsists of convolutional layers to extract features from the short-time\nFourier transform and one global pooling layer to combine those features. It\nparticularly possesses neither fully-connected layers, besides the\nfully-connected output layer, nor dropout layers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 08:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Hertel", "Lars", ""], ["Phan", "Huy", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.03070", "submitter": "Bruno Umbria Pedroni", "authors": "Bruno U. Pedroni, Sadique Sheik, Siddharth Joshi, Georgios Detorakis,\n  Somnath Paul, Charles Augustine, Emre Neftci, Gert Cauwenberghs", "title": "Forward Table-Based Presynaptic Event-Triggered Spike-Timing-Dependent\n  Plasticity", "comments": "Submitted to BioCAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-timing-dependent plasticity (STDP) incurs both causal and acausal\nsynaptic weight updates, for negative and positive time differences between\npre-synaptic and post-synaptic spike events. For realizing such updates in\nneuromorphic hardware, current implementations either require forward and\nreverse lookup access to the synaptic connectivity table, or rely on\nmemory-intensive architectures such as crossbar arrays. We present a novel\nmethod for realizing both causal and acausal weight updates using only forward\nlookup access of the synaptic connectivity table, permitting memory-efficient\nimplementation. A simplified implementation in FPGA, using a single timer\nvariable for each neuron, closely approximates exact STDP cumulative weight\nupdates for neuron refractory periods greater than 10 ms, and reduces to exact\nSTDP for refractory periods greater than the STDP time window. Compared to\nconventional crossbar implementation, the forward table-based implementation\nleads to substantial memory savings for sparsely connected networks supporting\nscalable neuromorphic systems with fully reconfigurable synaptic connectivity\nand plasticity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 18:37:12 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 11:12:40 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Pedroni", "Bruno U.", ""], ["Sheik", "Sadique", ""], ["Joshi", "Siddharth", ""], ["Detorakis", "Georgios", ""], ["Paul", "Somnath", ""], ["Augustine", "Charles", ""], ["Neftci", "Emre", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1607.03085", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Recurrent Memory Array Structures", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following report introduces ideas augmenting standard Long Short Term\nMemory (LSTM) architecture with multiple memory cells per hidden unit in order\nto improve its generalization capabilities. It considers both deterministic and\nstochastic variants of memory operation. It is shown that the nondeterministic\nArray-LSTM approach improves state-of-the-art performance on character level\ntext prediction achieving 1.402 BPC on enwik8 dataset. Furthermore, this report\nestabilishes baseline neural-based results of 1.12 BPC and 1.19 BPC for enwik9\nand enwik10 datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:29:44 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 16:46:33 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2016 02:01:55 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1607.03250", "submitter": "Hengyuan Hu", "authors": "Hengyuan Hu, Rui Peng, Yu-Wing Tai, Chi-Keung Tang", "title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards\n  Efficient Deep Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural networks are getting deeper and wider. While their\nperformance increases with the increasing number of layers and neurons, it is\ncrucial to design an efficient deep architecture in order to reduce\ncomputational and memory costs. Designing an efficient neural network, however,\nis labor intensive requiring many experiments, and fine-tunings. In this paper,\nwe introduce network trimming which iteratively optimizes the network by\npruning unimportant neurons based on analysis of their outputs on a large\ndataset. Our algorithm is inspired by an observation that the outputs of a\nsignificant portion of neurons in a large network are mostly zero, regardless\nof what inputs the network received. These zero activation neurons are\nredundant, and can be removed without affecting the overall accuracy of the\nnetwork. After pruning the zero activation neurons, we retrain the network\nusing the weights before pruning as initialization. We alternate the pruning\nand retraining to further reduce zero activations in a network. Our experiments\non the LeNet and VGG-16 show that we can achieve high compression ratio of\nparameters without losing or even achieving higher accuracy than the original\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 07:43:01 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Hu", "Hengyuan", ""], ["Peng", "Rui", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1607.03316", "submitter": "Dirk Weissenborn", "authors": "Dirk Weissenborn", "title": "Separating Answers from Queries for Neural Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural architecture for answering queries, designed to\noptimally leverage explicit support in the form of query-answer memories. Our\nmodel is able to refine and update a given query while separately accumulating\nevidence for predicting the answer. Its architecture reflects this separation\nwith dedicated embedding matrices and loosely connected information pathways\n(modules) for updating the query and accumulating evidence. This separation of\nresponsibilities effectively decouples the search for query related support and\nthe prediction of the answer. On recent benchmark datasets for reading\ncomprehension, our model achieves state-of-the-art results. A qualitative\nanalysis reveals that the model effectively accumulates weighted evidence from\nthe query and over multiple support retrieval cycles which results in a robust\nanswer prediction.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 11:43:15 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 11:54:46 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 13:37:41 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Weissenborn", "Dirk", ""]]}, {"id": "1607.03317", "submitter": "Per Kristian Lehre", "authors": "Duc-Cuong Dang and Thomas Jansen and Per Kristian Lehre", "title": "Populations can be essential in tracking dynamic optima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world optimisation problems are often dynamic. Previously good solutions\nmust be updated or replaced due to changes in objectives and constraints. It is\noften claimed that evolutionary algorithms are particularly suitable for\ndynamic optimisation because a large population can contain different solutions\nthat may be useful in the future. However, rigorous theoretical demonstrations\nfor how populations in dynamic optimisation can be essential are sparse and\nrestricted to special cases.\n  This paper provides theoretical explanations of how populations can be\nessential in evolutionary dynamic optimisation in a general and natural\nsetting. We describe a natural class of dynamic optimisation problems where a\nsufficiently large population is necessary to keep track of moving optima\nreliably. We establish a relationship between the population-size and the\nprobability that the algorithm loses track of the optimum.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 11:52:48 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Dang", "Duc-Cuong", ""], ["Jansen", "Thomas", ""], ["Lehre", "Per Kristian", ""]]}, {"id": "1607.03474", "submitter": "Julian Georg Zilly", "authors": "Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn\\'ik and\n  J\\\"urgen Schmidhuber", "title": "Recurrent Highway Networks", "comments": "12 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n'deep' transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:36:50 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 17:07:42 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 19:39:22 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 21:10:42 GMT"}, {"version": "v5", "created": "Tue, 4 Jul 2017 19:29:23 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Zilly", "Julian Georg", ""], ["Srivastava", "Rupesh Kumar", ""], ["Koutn\u00edk", "Jan", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1607.04063", "submitter": "Carsten Witt", "authors": "Dirk Sudholt and Carsten Witt", "title": "Update Strength in EDAs and ACO: How to Avoid Genetic Drift", "comments": "32 pages. An extended abstract of this work will appear in the\n  proceedings of the Genetic and Evolutionary Computation Conference (GECCO\n  2016). This revision fixes the abstract in the metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a rigorous runtime analysis concerning the update strength, a\nvital parameter in probabilistic model-building GAs such as the step size $1/K$\nin the compact Genetic Algorithm (cGA) and the evaporation factor $\\rho$ in\nACO. While a large update strength is desirable for exploitation, there is a\ngeneral trade-off: too strong updates can lead to genetic drift and poor\nperformance. We demonstrate this trade-off for the cGA and a simple MMAS ACO\nalgorithm on the OneMax function. More precisely, we obtain lower bounds on the\nexpected runtime of $\\Omega(K\\sqrt{n} + n \\log n)$ and $\\Omega(\\sqrt{n}/\\rho +\nn \\log n)$, respectively, showing that the update strength should be limited to\n$1/K, \\rho = O(1/(\\sqrt{n} \\log n))$. In fact, choosing $1/K, \\rho \\sim\n1/(\\sqrt{n}\\log n)$ both algorithms efficiently optimize OneMax in expected\ntime $O(n \\log n)$. Our analyses provide new insights into the stochastic\nbehavior of probabilistic model-building GAs and propose new guidelines for\nsetting the update strength in global optimization.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 10:11:59 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 07:51:28 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Sudholt", "Dirk", ""], ["Witt", "Carsten", ""]]}, {"id": "1607.04267", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Enhanced Boolean Correlation Matrix Memory", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an Enhanced Boolean version of the Correlation Matrix\nMemory (CMM), which is useful to work with binary memories. A novel Boolean\nOrthonormalization Process (BOP) is presented to convert a non-orthonormal\nBoolean basis, i.e., a set of non-orthonormal binary vectors (in a Boolean\nsense) to an orthonormal Boolean basis, i.e., a set of orthonormal binary\nvectors (in a Boolean sense). This work shows that it is possible to improve\nthe performance of Boolean CMM thanks BOP algorithm. Besides, the BOP algorithm\nhas a lot of additional fields of applications, e.g.: Steganography, Hopfield\nNetworks, Bi-level image processing, etc. Finally, it is important to mention\nthat the BOP is an extremely stable and fast algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 16:22:47 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1607.04324", "submitter": "Xin-She Yang", "authors": "Aziz Ouaarab, B. Ahiod, Xin-She Yang", "title": "Random-Key Cuckoo Search for the Travelling Salesman Problem", "comments": "13 pages, 6 figures", "journal-ref": "Soft Computing, 19(4), 1099-1106(2015)", "doi": "10.1007/s00500-014-1322-9", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization problems are typically NP-hard, and thus very\nchallenging to solve. In this paper, we present the random key cuckoo search\n(RKCS) algorithm for solving the famous Travelling Salesman Problem (TSP). We\nused a simplified random-key encoding scheme to pass from a continuous space\n(real numbers) to a combinatorial space. We also consider the displacement of a\nsolution in both spaces using Levy flights. The performance of the proposed\nRKCS is tested against a set of benchmarks of symmetric TSP from the well-known\nTSPLIB library. The results of the tests show that RKCS is superior to some\nother metaheuristic algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 12:40:50 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Ouaarab", "Aziz", ""], ["Ahiod", "B.", ""], ["Yang", "Xin-She", ""]]}, {"id": "1607.04423", "submitter": "Yiming Cui", "authors": "Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu and Guoping Hu", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "comments": "8+2 pages. accepted as a conference paper at ACL2017 (long paper)", "journal-ref": "ACL 2017 Vol.1 Long Papers 593-602", "doi": "10.18653/v1/P17-1055", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloze-style queries are representative problems in reading comprehension.\nOver the past few months, we have seen much progress that utilizing neural\nnetwork approach to solve Cloze-style questions. In this paper, we present a\nnovel model called attention-over-attention reader for the Cloze-style reading\ncomprehension task. Our model aims to place another attention mechanism over\nthe document-level attention, and induces \"attended attention\" for final\npredictions. Unlike the previous works, our neural network model requires less\npre-defined hyper-parameters and uses an elegant architecture for modeling.\nExperimental results show that the proposed attention-over-attention model\nsignificantly outperforms various state-of-the-art systems by a large margin in\npublic datasets, such as CNN and Children's Book Test datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:10:11 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 09:46:02 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 06:17:42 GMT"}, {"version": "v4", "created": "Tue, 6 Jun 2017 02:51:54 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Cui", "Yiming", ""], ["Chen", "Zhipeng", ""], ["Wei", "Si", ""], ["Wang", "Shijin", ""], ["Liu", "Ting", ""], ["Hu", "Guoping", ""]]}, {"id": "1607.04576", "submitter": "John M. Pierre", "authors": "John M. Pierre, Mark Butler, Jacob Portnoff, and Luis Aguilar", "title": "Neural Discourse Modeling of Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown recent promise in many language-related tasks\nsuch as the modeling of conversations. We extend RNN-based sequence to sequence\nmodels to capture the long range discourse across many turns of conversation.\nWe perform a sensitivity analysis on how much additional context affects\nperformance, and provide quantitative and qualitative evidence that these\nmodels are able to capture discourse relationships across multiple utterances.\nOur results quantifies how adding an additional RNN layer for modeling\ndiscourse improves the quality of output utterances and providing more of the\nprevious conversation as input also improves performance. By searching the\ngenerated outputs for specific discourse markers we show how neural discourse\nmodels can exhibit increased coherence and cohesion in conversations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:43:40 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Pierre", "John M.", ""], ["Butler", "Mark", ""], ["Portnoff", "Jacob", ""], ["Aguilar", "Luis", ""]]}, {"id": "1607.04589", "submitter": "Siddharth Sigtia", "authors": "Siddharth Sigtia, Adam M. Stark, Sacha Krstulovic and Mark D. Plumbley", "title": "Automatic Environmental Sound Recognition: Performance versus\n  Computational Cost", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing\n  24(11): 2096-2107, Nov 2016", "doi": "10.1109/TASLP.2016.2592698", "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the Internet of Things (IoT), sound sensing applications\nare required to run on embedded platforms where notions of product pricing and\nform factor impose hard constraints on the available computing power. Whereas\nAutomatic Environmental Sound Recognition (AESR) algorithms are most often\ndeveloped with limited consideration for computational cost, this article seeks\nwhich AESR algorithm can make the most of a limited amount of computing power\nby comparing the sound classification performance em as a function of its\ncomputational cost. Results suggest that Deep Neural Networks yield the best\nratio of sound classification accuracy across a range of computational costs,\nwhile Gaussian Mixture Models offer a reasonable accuracy at a consistently\nsmall cost, and Support Vector Machines stand between both in terms of\ncompromise between accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 17:29:26 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Sigtia", "Siddharth", ""], ["Stark", "Adam M.", ""], ["Krstulovic", "Sacha", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1607.04793", "submitter": "Eliya Nachmani", "authors": "Eliya Nachmani, Yair Beery and David Burshtein", "title": "Learning to Decode Linear Codes Using Deep Learning", "comments": "Presented at the Allerton Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel deep learning method for improving the belief propagation algorithm\nis proposed. The method generalizes the standard belief propagation algorithm\nby assigning weights to the edges of the Tanner graph. These edges are then\ntrained using deep learning techniques. A well-known property of the belief\npropagation algorithm is the independence of the performance on the transmitted\ncodeword. A crucial property of our new method is that our decoder preserved\nthis property. Furthermore, this property allows us to learn only a single\ncodeword instead of exponential number of code-words. Improvements over the\nbelief propagation algorithm are demonstrated for various high density parity\ncheck codes.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 19:09:26 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 14:43:52 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Nachmani", "Eliya", ""], ["Beery", "Yair", ""], ["Burshtein", "David", ""]]}, {"id": "1607.05108", "submitter": "Zichao Yang", "authors": "Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, Alex Smola", "title": "Neural Machine Translation with Recurrent Attention Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing which words have been attended to in previous time steps while\ngenerating a translation is a rich source of information for predicting what\nwords will be attended to in the future. We improve upon the attention model of\nBahdanau et al. (2014) by explicitly modeling the relationship between previous\nand subsequent attention levels for each word using one recurrent network per\ninput word. This architecture easily captures informative features, such as\nfertility and regularities in relative distortion. In experiments, we show our\nparameterization of attention improves translation quality.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 14:44:26 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Yang", "Zichao", ""], ["Hu", "Zhiting", ""], ["Deng", "Yuntian", ""], ["Dyer", "Chris", ""], ["Smola", "Alex", ""]]}, {"id": "1607.05213", "submitter": "Mark Wineberg", "authors": "Sebastian Lenartowicz, Mark Wineberg", "title": "mpEAd: Multi-Population EA Diagrams", "comments": "6 pages. An extended abstract of this work will appear in the\n  proceedings of the eighteenth Genetic and Evolutionary Computation Conference\n  (GECCO 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-population evolutionary algorithms are, by nature, highly complex and\ndifficult to describe. Even two populations working in concert (or opposition)\npresent a myriad of potential configurations that are often difficult to relate\nusing text alone. Little effort has been made, however, to depict these kinds\nof systems, relying solely on the simple structural connections (related using\nad hoc diagrams) between populations and often leaving out crucial details. In\nthis paper, we propose a notation and accompanying formalism for consistently\nand powerfully depicting these structures and the relationships within them in\nan intuitive and consistent way. Using our notation, we examine simple\nco-evolutionary systems and discover new configurations by the simple process\nof \"drawing on a whiteboard\". Finally, we demonstrate that even complex,\nhighly-interconnected systems with large numbers of populations can be\nunderstood with ease using the advanced features of our formalism\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 17:33:34 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Lenartowicz", "Sebastian", ""], ["Wineberg", "Mark", ""]]}, {"id": "1607.05390", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta and Yew-Soon Ong", "title": "Genetic Transfer or Population Diversification? Deciphering the Secret\n  Ingredients of Evolutionary Multitask Optimization", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary multitasking has recently emerged as a novel paradigm that\nenables the similarities and/or latent complementarities (if present) between\ndistinct optimization tasks to be exploited in an autonomous manner simply by\nsolving them together with a unified solution representation scheme. An\nimportant matter underpinning future algorithmic advancements is to develop a\nbetter understanding of the driving force behind successful multitask\nproblem-solving. In this regard, two (seemingly disparate) ideas have been put\nforward, namely, (a) implicit genetic transfer as the key ingredient\nfacilitating the exchange of high-quality genetic material across tasks, and\n(b) population diversification resulting in effective global search of the\nunified search space encompassing all tasks. In this paper, we present some\nempirical results that provide a clearer picture of the relationship between\nthe two aforementioned propositions. For the numerical experiments we make use\nof Sudoku puzzles as case studies, mainly because of their feature that\noutwardly unlike puzzle statements can often have nearly identical final\nsolutions. The experiments reveal that while on many occasions genetic transfer\nand population diversity may be viewed as two sides of the same coin, the wider\nimplication of genetic transfer, as shall be shown herein, captures the true\nessence of evolutionary multitasking to the fullest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 03:33:06 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Gupta", "Abhishek", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1607.05418", "submitter": "Soheil Hashemi", "authors": "Hokchhay Tann, Soheil Hashemi, R. Iris Bahar, Sherief Reda", "title": "Runtime Configurable Deep Neural Networks for Energy-Accuracy Trade-off", "comments": null, "journal-ref": null, "doi": "10.1145/2968456.2968458", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dynamic configuration technique for deep neural networks\nthat permits step-wise energy-accuracy trade-offs during runtime. Our\nconfiguration technique adjusts the number of channels in the network\ndynamically depending on response time, power, and accuracy targets. To enable\nthis dynamic configuration technique, we co-design a new training algorithm,\nwhere the network is incrementally trained such that the weights in channels\ntrained in earlier steps are fixed. Our technique provides the flexibility of\nmultiple networks while storing and utilizing one set of weights. We evaluate\nour techniques using both an ASIC-based hardware accelerator as well as a\nlow-power embedded GPGPU and show that our approach leads to only a small or\nnegligible loss in the final network accuracy. We analyze the performance of\nour proposed methodology using three well-known networks for MNIST, CIFAR-10,\nand SVHN datasets, and we show that we are able to achieve up to 95% energy\nreduction with less than 1% accuracy loss across the three benchmarks. In\naddition, compared to prior work on dynamic network reconfiguration, we show\nthat our approach leads to approximately 50% savings in storage requirements,\nwhile achieving similar accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:27:05 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 20:42:51 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Tann", "Hokchhay", ""], ["Hashemi", "Soheil", ""], ["Bahar", "R. Iris", ""], ["Reda", "Sherief", ""]]}, {"id": "1607.05666", "submitter": "Yuxuan Wang", "authors": "Yuxuan Wang, Pascal Getreuer, Thad Hughes, Richard F. Lyon, Rif A.\n  Saurous", "title": "Trainable Frontend For Robust and Far-Field Keyword Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and far-field speech recognition is critical to enable true hands-free\ncommunication. In far-field conditions, signals are attenuated due to distance.\nTo improve robustness to loudness variation, we introduce a novel frontend\ncalled per-channel energy normalization (PCEN). The key ingredient of PCEN is\nthe use of an automatic gain control based dynamic compression to replace the\nwidely used static (such as log or root) compression. We evaluate PCEN on the\nkeyword spotting task. On our large rerecorded noisy and far-field eval sets,\nwe show that PCEN significantly improves recognition performance. Furthermore,\nwe model PCEN as neural network layers and optimize high-dimensional PCEN\nparameters jointly with the keyword spotting acoustic model. The trained PCEN\nfrontend demonstrates significant further improvements without increasing model\ncomplexity or inference-time cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 17:17:58 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Wang", "Yuxuan", ""], ["Getreuer", "Pascal", ""], ["Hughes", "Thad", ""], ["Lyon", "Richard F.", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1607.05690", "submitter": "Alex Graves", "authors": "Alex Graves", "title": "Stochastic Backpropagation through Mixture Density Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to backpropagate stochastic gradients through continuous latent\ndistributions has been crucial to the emergence of variational autoencoders and\nstochastic gradient variational Bayes. The key ingredient is an unbiased and\nlow-variance way of estimating gradients with respect to distribution\nparameters from gradients evaluated at distribution samples. The\n\"reparameterization trick\" provides a class of transforms yielding such\nestimators for many continuous distributions, including the Gaussian and other\nmembers of the location-scale family. However the trick does not readily extend\nto mixture density models, due to the difficulty of reparameterizing the\ndiscrete distribution over mixture weights. This report describes an\nalternative transform, applicable to any continuous multivariate distribution\nwith a differentiable density function from which samples can be drawn, and\nuses it to derive an unbiased estimator for mixture density weight derivatives.\nCombined with the reparameterization trick applied to the individual mixture\ncomponents, this estimator makes it straightforward to train variational\nautoencoders with mixture-distributed latent variables, or to perform\nstochastic variational inference with a mixture density variational posterior.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 18:37:00 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Graves", "Alex", ""]]}, {"id": "1607.05944", "submitter": "Matej Hoffmann", "authors": "Matej Hoffmann and Nada Bednarova", "title": "The encoding of proprioceptive inputs in the brain: knowns and unknowns\n  from a robotic perspective", "comments": "in Proceedings of Kognice a um\\v{e}l\\'y \\v{z}ivot XVI [Cognition and\n  Artificial Life XVI] 2016, ISBN 978-80-01-05915-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Somatosensory inputs can be grossly divided into tactile (or cutaneous) and\nproprioceptive -- the former conveying information about skin stimulation, the\nlatter about limb position and movement. The principal proprioceptors are\nconstituted by muscle spindles, which deliver information about muscle length\nand speed. In primates, this information is relayed to the primary\nsomatosensory cortex and eventually the posterior parietal cortex, where\nintegrated information about body posture (postural schema) is presumably\navailable. However, coming from robotics and seeking a biologically motivated\nmodel that could be used in a humanoid robot, we faced a number of\ndifficulties. First, it is not clear what neurons in the ascending pathway and\nprimary somatosensory cortex code. To an engineer, joint angles would seem the\nmost useful variables. However, the lengths of individual muscles have\nnonlinear relationships with the angles at joints. Kim et al. (Neuron, 2015)\nfound different types of proprioceptive neurons in the primary somatosensory\ncortex -- sensitive to movement of single or multiple joints or to static\npostures. Second, there are indications that the somatotopic arrangement (\"the\nhomunculus\") of these brain areas is to a significant extent learned. However,\nthe mechanisms behind this developmental process are unclear. We will report\nfirst results from modeling of this process using data obtained from body\nbabbling in the iCub humanoid robot and feeding them into a Self-Organizing Map\n(SOM). Our results reveal that the SOM algorithm is only suited to develop\nreceptive fields of the posture-selective type. Furthermore, the SOM algorithm\nhas intrinsic difficulties when combined with population code on its input and\nin particular with nonlinear tuning curves (sigmoids or Gaussians).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 13:23:04 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Hoffmann", "Matej", ""], ["Bednarova", "Nada", ""]]}, {"id": "1607.05954", "submitter": "Carlos Dafonte", "authors": "C. Dafonte, D. Fustes, M. Manteiga, D. Garabato, M. A. Alvarez, A.\n  Ulla, C. Allende Prieto", "title": "On the estimation of stellar parameters with uncertainty prediction from\n  Generative Artificial Neural Networks: application to Gaia RVS simulated\n  spectra", "comments": null, "journal-ref": "A&A 594, A68 (2016)", "doi": "10.1051/0004-6361/201527045", "report-no": null, "categories": "astro-ph.IM astro-ph.SR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aims. We present an innovative artificial neural network (ANN) architecture,\ncalled Generative ANN (GANN), that computes the forward model, that is it\nlearns the function that relates the unknown outputs (stellar atmospheric\nparameters, in this case) to the given inputs (spectra). Such a model can be\nintegrated in a Bayesian framework to estimate the posterior distribution of\nthe outputs. Methods. The architecture of the GANN follows the same scheme as a\nnormal ANN, but with the inputs and outputs inverted. We train the network with\nthe set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]),\nobtaining the stellar spectra for such inputs. The residuals between the\nspectra in the grid and the estimated spectra are minimized using a validation\ndataset to keep solutions as general as possible. Results. The performance of\nboth conventional ANNs and GANNs to estimate the stellar parameters as a\nfunction of the star brightness is presented and compared for different\nGalactic populations. GANNs provide significantly improved parameterizations\nfor early and intermediate spectral types with rich and intermediate\nmetallicities. The behaviour of both algorithms is very similar for our sample\nof late-type stars, obtaining residuals in the derivation of [Fe/H] and\n[alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs<12, which accounts\nfor a number in the order of four million stars to be observed by the Radial\nVelocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty\nestimation of computed astrophysical parameters is crucial for the validation\nof the parameterization itself and for the subsequent exploitation by the\nastronomical community. GANNs produce not only the parameters for a given\nspectrum, but a goodness-of-fit between the observed spectrum and the predicted\none for a given set of parameters. Moreover, they allow us to obtain the full\nposterior distribution...\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:16:56 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Dafonte", "C.", ""], ["Fustes", "D.", ""], ["Manteiga", "M.", ""], ["Garabato", "D.", ""], ["Alvarez", "M. A.", ""], ["Ulla", "A.", ""], ["Prieto", "C. Allende", ""]]}, {"id": "1607.06025", "submitter": "Janez Starc", "authors": "Janez Starc and Dunja Mladeni\\'c", "title": "Constructing a Natural Language Inference Dataset using Generative\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference is an important task for Natural Language\nUnderstanding. It is concerned with classifying the logical relation between\ntwo sentences. In this paper, we propose several text generative neural\nnetworks for generating text hypothesis, which allows construction of new\nNatural Language Inference datasets. To evaluate the models, we propose a new\nmetric -- the accuracy of the classifier trained on the generated dataset. The\naccuracy obtained by our best generative model is only 2.7% lower than the\naccuracy of the classifier trained on the original, human crafted dataset.\nFurthermore, the best generated dataset combined with the original dataset\nachieves the highest accuracy. The best model learns a mapping embedding for\neach training example. By comparing various metrics we show that datasets that\nobtain higher ROUGE or METEOR scores do not necessarily yield higher\nclassification accuracies. We also provide analysis of what are the\ncharacteristics of a good dataset including the distinguishability of the\ngenerated datasets from the original one.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:59:21 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 08:33:27 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Starc", "Janez", ""], ["Mladeni\u0107", "Dunja", ""]]}, {"id": "1607.06113", "submitter": "Mahmood Rashid Ph.D.", "authors": "Mahmood A. Rashid, Sumaiya Iqbal, Firas Khatib, Md Tamjidul Hoque,\n  Abdul Sattar", "title": "Guided macro-mutation in a graded energy based genetic algorithm for\n  protein structure prediction", "comments": "29 pages. arXiv admin note: text overlap with arXiv:1311.3840", "journal-ref": "Computational Biology and Chemistry - Elsevier, 2016", "doi": "10.1016/j.compbiolchem.2016.01.008", "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein structure prediction is considered as one of the most challenging and\ncomputationally intractable combinatorial problem. Thus, the efficient modeling\nof convoluted search space, the clever use of energy functions, and more\nimportantly, the use of effective sampling algorithms become crucial to address\nthis problem. For protein structure modeling, an off-lattice model provides\nlimited scopes to exercise and evaluate the algorithmic developments due to its\nastronomically large set of data-points. In contrast, an on-lattice model\nwidens the scopes and permits studying the relatively larger proteins because\nof its finite set of data-points. In this work, we took the full advantage of\nan on-lattice model by using a face-centered-cube lattice that has the highest\npacking density with the maximum degree of freedom. We proposed a graded\nenergy-strategically mixes the Miyazawa-Jernigan (MJ) energy with the\nhydrophobic-polar (HP) energy-based genetic algorithm (GA) for conformational\nsearch. In our application, we introduced a 2x2 HP energy guided macro-mutation\noperator within the GA to explore the best possible local changes exhaustively.\nConversely, the 20x20 MJ energy model-the ultimate objective function of our GA\nthat needs to be minimized-considers the impacts amongst the 20 different amino\nacids and allow searching the globally acceptable conformations. On a set of\nbenchmark proteins, our proposed approach outperformed state-of-the-art\napproaches in terms of the free energy levels and the root-mean-square\ndeviations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:47:07 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Rashid", "Mahmood A.", ""], ["Iqbal", "Sumaiya", ""], ["Khatib", "Firas", ""], ["Hoque", "Md Tamjidul", ""], ["Sattar", "Abdul", ""]]}, {"id": "1607.06125", "submitter": "Ahmed Hassanien", "authors": "Ahmed Mamdouh A. Hassanien", "title": "Sequence to sequence learning for unconstrained scene text recognition", "comments": "It is my master thesis. The thesis was done at Sony Technology Center\n  Stuttgart and presented to Nile University. The thesis supervisors are Mark\n  Blaxall, Fabien Cardinaux, and Motaz Abdelwahab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a state-of-the-art approach for unconstrained natural\nscene text recognition. We propose a cascade approach that incorporates a\nconvolutional neural network (CNN) architecture followed by a long short term\nmemory model (LSTM). The CNN learns visual features for the characters and uses\nthem with a softmax layer to detect sequence of characters. While the CNN gives\nvery good recognition results, it does not model relation between characters,\nhence gives rise to false positive and false negative cases (confusing\ncharacters due to visual similarities like \"g\" and \"9\", or confusing background\npatches with characters; either removing existing characters or adding\nnon-existing ones) To alleviate these problems we leverage recent developments\nin LSTM architectures to encode contextual information. We show that the LSTM\ncan dramatically reduce such errors and achieve state-of-the-art accuracy in\nthe task of unconstrained natural scene text recognition. Moreover we manually\nremove all occurrences of the words that exist in the test set from our\ntraining set to test whether our approach will generalize to unseen data. We\nuse the ICDAR 13 test set for evaluation and compare the results with the state\nof the art approaches [11, 18]. We finally present an application of the work\nin the domain of for traffic monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 21:02:16 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Hassanien", "Ahmed Mamdouh A.", ""]]}, {"id": "1607.06153", "submitter": "Marek Rei", "authors": "Marek Rei, Helen Yannakoudakis", "title": "Compositional Sequence Labeling Models for Error Detection in Learner\n  Writing", "comments": "Proceedings of ACL 2016", "journal-ref": null, "doi": "10.18653/v1/P16-1112", "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the first experiments using neural network models\nfor the task of error detection in learner writing. We perform a systematic\ncomparison of alternative compositional architectures and propose a framework\nfor error detection based on bidirectional LSTMs. Experiments on the CoNLL-14\nshared task dataset show the model is able to outperform other participants on\ndetecting errors in learner writing. Finally, the model is integrated with a\npublicly deployed self-assessment system, leading to performance comparable to\nhuman annotators.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 23:26:33 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Yannakoudakis", "Helen", ""]]}, {"id": "1607.06275", "submitter": "Peng Li", "authors": "Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu", "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain\n  Factoid Question Answering", "comments": "10 pages, 3 figures, withdraw experimental results on CNN/Daily Mail\n  datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While question answering (QA) with neural network, i.e. neural QA, has\nachieved promising results in recent years, lacking of large scale real-word QA\ndataset is still a challenge for developing and evaluating neural QA system. To\nalleviate this problem, we propose a large scale human annotated real-world QA\ndataset WebQA with more than 42k questions and 556k evidences. As existing\nneural QA methods resolve QA either as sequence generation or\nclassification/ranking problem, they face challenges of expensive softmax\ncomputation, unseen answers handling or separate candidate answer generation\ncomponent. In this work, we cast neural QA as a sequence labeling problem and\npropose an end-to-end sequence labeling model, which overcomes all the above\nchallenges. Experimental results on WebQA show that our model outperforms the\nbaselines significantly with an F1 score of 74.69% with word-based input, and\nthe performance drops only 3.72 F1 points with more challenging character-based\ninput.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:40:50 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 10:56:45 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Li", "Peng", ""], ["Li", "Wei", ""], ["He", "Zhengyan", ""], ["Wang", "Xuguang", ""], ["Cao", "Ying", ""], ["Zhou", "Jie", ""], ["Xu", "Wei", ""]]}, {"id": "1607.06641", "submitter": "Jialin Liu Ph.D", "authors": "Jialin Liu, Michael Fairbank, Diego P\\'erez-Li\\'ebana, Simon M. Lucas", "title": "Optimal resampling for the noisy OneMax problem", "comments": "8 pages, 1 table, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OneMax problem is a standard benchmark optimisation problem for a binary\nsearch space. Recent work on applying a Bandit-Based Random Mutation\nHill-Climbing algorithm to the noisy OneMax Problem showed that it is important\nto choose a good value for the resampling number to make a careful trade off\nbetween taking more samples in order to reduce noise, and taking fewer samples\nto reduce the total computational cost. This paper extends that observation, by\nderiving an analytical expression for the running time of the RMHC algorithm\nwith resampling applied to the noisy OneMax problem, and showing both\ntheoretically and empirically that the optimal resampling number increases with\nthe number of dimensions in the search space.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 11:51:49 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 08:41:23 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 15:40:50 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Liu", "Jialin", ""], ["Fairbank", "Michael", ""], ["P\u00e9rez-Li\u00e9bana", "Diego", ""], ["Lucas", "Simon M.", ""]]}, {"id": "1607.07043", "submitter": "Amir Shahroudy", "authors": "Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang", "title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition - analysis of human actions based on 3D skeleton data -\nbecomes popular recently due to its succinctness, robustness, and\nview-invariant representation. Recent attempts on this problem suggested to\ndevelop RNN-based learning methods to model the contextual dependency in the\ntemporal domain. In this paper, we extend this idea to spatio-temporal domains\nto analyze the hidden sources of action-related information within the input\ndata over both domains concurrently. Inspired by the graphical structure of the\nhuman skeleton, we further propose a more powerful tree-structure based\ntraversal method. To handle the noise and occlusion in 3D skeleton data, we\nintroduce new gating mechanism within LSTM to learn the reliability of the\nsequential input data and accordingly adjust its effect on updating the\nlong-term context information stored in the memory cell. Our method achieves\nstate-of-the-art performance on 4 challenging benchmark datasets for 3D human\naction analysis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 13:39:11 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Xu", "Dong", ""], ["Wang", "Gang", ""]]}, {"id": "1607.07078", "submitter": "Saba Emrani", "authors": "Saba Emrani and Hamid Krim", "title": "Effective Connectivity-Based Neural Decoding: A Causal\n  Interaction-Driven Approach", "comments": "16 pages, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric model-free causality measurebased on multivariate\ndelay embedding that can efficiently detect linear and nonlinear causal\ninteractions between time series with no prior information. We then exploit the\nproposed causal interaction measure in real MEG data analysis. The results are\nused to construct effective connectivity maps of brain activity to decode\ndifferent categories of visual stimuli. Moreover, we discovered that the\nMEG-based effective connectivity maps as a response to structured images\nexhibit more geometric patterns, as disclosed by analyzing the evolution of\ntoplogical structures of the underlying networks using persistent homology.\nExtensive simulation and experimental result have been carried out to\nsubstantiate the capabilities of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 18:44:54 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Emrani", "Saba", ""], ["Krim", "Hamid", ""]]}, {"id": "1607.07249", "submitter": "Joern Hees", "authors": "J\\\"orn Hees, Rouven Bauer, Joachim Folz, Damian Borth and Andreas\n  Dengel", "title": "An Evolutionary Algorithm to Learn SPARQL Queries for\n  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia", "comments": "15 pages, 2 figures, as of 2016-09-13\n  6a19d5d7020770dc0711081ce2c1e52f71bf4b86", "journal-ref": null, "doi": "10.1007/978-3-319-49004-5_22", "report-no": null, "categories": "cs.AI cs.DB cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient usage of the knowledge provided by the Linked Data community is\noften hindered by the need for domain experts to formulate the right SPARQL\nqueries to answer questions. For new questions they have to decide which\ndatasets are suitable and in which terminology and modelling style to phrase\nthe SPARQL query.\n  In this work we present an evolutionary algorithm to help with this\nchallenging task. Given a training list of source-target node-pair examples our\nalgorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The\nlearned patterns can be visualised to form the basis for further investigation,\nor they can be used to predict target nodes for new source nodes.\n  Amongst others, we apply our algorithm to a dataset of several hundred human\nassociations (such as \"circle - square\") to find patterns for them in DBpedia.\nWe show the scalability of the algorithm by running it against a SPARQL\nendpoint loaded with > 7.9 billion triples. Further, we use the resulting\nSPARQL queries to mimic human associations with a Mean Average Precision (MAP)\nof 39.9 % and a Recall@10 of 63.9 %.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 12:47:38 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 12:13:14 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 10:27:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hees", "J\u00f6rn", ""], ["Bauer", "Rouven", ""], ["Folz", "Joachim", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1607.07514", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Prashanth Vijayaraghavan and Deb Roy", "title": "Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM\n  Encoder-Decoder", "comments": "SIGIR 2016, July 17-21, 2016, Pisa. Proceedings of SIGIR 2016. Pisa,\n  Italy (2016)", "journal-ref": null, "doi": "10.1145/2911451.2914762", "report-no": null, "categories": "cs.CL cs.AI cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Tweet2Vec, a novel method for generating general-purpose vector\nrepresentation of tweets. The model learns tweet embeddings using\ncharacter-level CNN-LSTM encoder-decoder. We trained our model on 3 million,\nrandomly selected English-language tweets. The model was evaluated using two\nmethods: tweet semantic similarity and tweet sentiment categorization,\noutperforming the previous state-of-the-art in both tasks. The evaluations\ndemonstrate the power of the tweet embeddings generated by our model for\nvarious tweet categorization tasks. The vector representations generated by our\nmodel are generic, and hence can be applied to a variety of tasks. Though the\nmodel presented in this paper is trained on English-language tweets, the method\npresented can be used to learn tweet embeddings for different languages.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 00:58:14 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Vijayaraghavan", "Prashanth", ""], ["Roy", "Deb", ""]]}, {"id": "1607.07695", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal Ertugrul, Mete Ozay, Fatos Tunay Yarman Vural", "title": "Hierarchical Multi-resolution Mesh Networks for Brain Decoding", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework, called Hierarchical Multi-resolution Mesh\nNetworks (HMMNs), which establishes a set of brain networks at multiple time\nresolutions of fMRI signal to represent the underlying cognitive process. The\nsuggested framework, first, decomposes the fMRI signal into various frequency\nsubbands using wavelet transforms. Then, a brain network, called mesh network,\nis formed at each subband by ensembling a set of local meshes. The locality\naround each anatomic region is defined with respect to a neighborhood system\nbased on functional connectivity. The arc weights of a mesh are estimated by\nridge regression formed among the average region time series. In the final\nstep, the adjacency matrices of mesh networks obtained at different subbands\nare ensembled for brain decoding under a hierarchical learning architecture,\ncalled, fuzzy stacked generalization (FSG). Our results on Human Connectome\nProject task-fMRI dataset reflect that the suggested HMMN model can\nsuccessfully discriminate tasks by extracting complementary information\nobtained from mesh arc weights of multiple subbands. We study the topological\nproperties of the mesh networks at different resolutions using the network\nmeasures, namely, node degree, node strength, betweenness centrality and global\nefficiency; and investigate the connectivity of anatomic regions, during a\ncognitive task. We observe significant variations among the network topologies\nobtained for different subbands. We, also, analyze the diversity properties of\nclassifier ensemble, trained by the mesh networks in multiple subbands and\nobserve that the classifiers in the ensemble collaborate with each other to\nfuse the complementary information freed at each subband. We conclude that the\nfMRI data, recorded during a cognitive task, embed diverse information across\nthe anatomic regions at each resolution.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 17:26:31 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 20:42:47 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ertugrul", "Itir Onal", ""], ["Ozay", "Mete", ""], ["Vural", "Fatos Tunay Yarman", ""]]}, {"id": "1607.08064", "submitter": "Christian Bailer", "authors": "Christian Bailer and Kiran Varanasi and Didier Stricker", "title": "CNN-based Patch Matching for Optical Flow with Thresholded Hinge\n  Embedding Loss", "comments": "Fixed bracket error in equation 3 (it has no major influence in the\n  approach, but on the optimal t value)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based approaches have not yet achieved their full potential in\noptical flow estimation, where their performance still trails heuristic\napproaches. In this paper, we present a CNN based patch matching approach for\noptical flow estimation. An important contribution of our approach is a novel\nthresholded loss for Siamese networks. We demonstrate that our loss performs\nclearly better than existing losses. It also allows to speed up training by a\nfactor of 2 in our tests. Furthermore, we present a novel way for calculating\nCNN based features for different image scales, which performs better than\nexisting methods. We also discuss new ways of evaluating the robustness of\ntrained features for the application of patch matching for optical flow. An\ninteresting discovery in our paper is that low-pass filtering of feature maps\ncan increase the robustness of features created by CNNs. We proved the\ncompetitive performance of our approach by submitting it to the KITTI 2012,\nKITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art\nresults on all three datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 12:41:00 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:29:19 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 18:57:55 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 06:28:24 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bailer", "Christian", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1607.08878", "submitter": "Randal Olson", "authors": "Randal S. Olson and Jason H. Moore", "title": "Identifying and Harnessing the Building Blocks of Machine Learning\n  Pipelines for Sensible Initialization of a Data Science Automation Tool", "comments": "13 pages, 5 figures, preprint of chapter to appear in GPTP 2016 book", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data science continues to grow in popularity, there will be an increasing\nneed to make data science tools more scalable, flexible, and accessible. In\nparticular, automated machine learning (AutoML) systems seek to automate the\nprocess of designing and optimizing machine learning pipelines. In this\nchapter, we present a genetic programming-based AutoML system called TPOT that\noptimizes a series of feature preprocessors and machine learning models with\nthe goal of maximizing classification accuracy on a supervised classification\nproblem. Further, we analyze a large database of pipelines that were previously\nused to solve various supervised classification problems and identify 100 short\nseries of machine learning operations that appear the most frequently, which we\ncall the building blocks of machine learning pipelines. We harness these\nbuilding blocks to initialize TPOT with promising solutions, and find that this\nsensible initialization method significantly improves TPOT's performance on one\nbenchmark at no cost of significantly degrading performance on the others.\nThus, sensible initialization with machine learning pipeline building blocks\nshows promise for GP-based AutoML systems, and should be further refined in\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:06:39 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}]