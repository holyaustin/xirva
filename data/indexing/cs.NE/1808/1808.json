[{"id": "1808.00076", "submitter": "Gabriel de Souza Pereira Moreira", "authors": "Gabriel de Souza P. Moreira, Felipe Ferreira, Adilson Marques da Cunha", "title": "News Session-Based Recommendations using Deep Neural Networks", "comments": "Accepted for the Third Workshop on Deep Learning for Recommender\n  Systems - DLRS 2018, October 02-07, 2018, Vancouver, Canada.\n  https://recsys.acm.org/recsys18/dlrs/", "journal-ref": null, "doi": "10.1145/3270323.3270328", "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommender systems are aimed to personalize users experiences and help\nthem to discover relevant articles from a large and dynamic search space.\nTherefore, news domain is a challenging scenario for recommendations, due to\nits sparse user profiling, fast growing number of items, accelerated item's\nvalue decay, and users preferences dynamic shift. Some promising results have\nbeen recently achieved by the usage of Deep Learning techniques on Recommender\nSystems, specially for item's feature extraction and for session-based\nrecommendations with Recurrent Neural Networks. In this paper, it is proposed\nan instantiation of the CHAMELEON -- a Deep Learning Meta-Architecture for News\nRecommender Systems. This architecture is composed of two modules, the first\nresponsible to learn news articles representations, based on their text and\nmetadata, and the second module aimed to provide session-based recommendations\nusing Recurrent Neural Networks. The recommendation task addressed in this work\nis next-item prediction for users sessions: \"what is the next most likely\narticle a user might read in a session?\" Users sessions context is leveraged by\nthe architecture to provide additional information in such extreme cold-start\nscenario of news recommendation. Users' behavior and item features are both\nmerged in an hybrid recommendation approach. A temporal offline evaluation\nmethod is also proposed as a complementary contribution, for a more realistic\nevaluation of such task, considering dynamic factors that affect global\nreadership interests like popularity, recency, and seasonality. Experiments\nwith an extensive number of session-based recommendation methods were performed\nand the proposed instantiation of CHAMELEON meta-architecture obtained a\nsignificant relative improvement in top-n accuracy and ranking metrics (10% on\nHit Rate and 13% on MRR) over the best benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 21:15:54 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 01:02:00 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 03:09:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Moreira", "Gabriel de Souza P.", ""], ["Ferreira", "Felipe", ""], ["da Cunha", "Adilson Marques", ""]]}, {"id": "1808.00193", "submitter": "Chen Yukang", "authors": "Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, Chang Huang,\n  Lisen Mu, Xinggang Wang", "title": "Reinforced Evolutionary Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an important yet challenging task in\nnetwork design due to its high computational consumption. To address this\nissue, we propose the Reinforced Evolutionary Neural Architecture Search (RE-\nNAS), which is an evolutionary method with the reinforced mutation for NAS. Our\nmethod integrates reinforced mutation into an evolution algorithm for neural\narchitecture exploration, in which a mutation controller is introduced to learn\nthe effects of slight modifications and make mutation actions. The reinforced\nmutation controller guides the model population to evolve efficiently.\nFurthermore, as child models can inherit parameters from their parents during\nevolution, our method requires very limited computational resources. In\nexperiments, we conduct the proposed search method on CIFAR-10 and obtain a\npowerful network architecture, RENASNet. This architecture achieves a\ncompetitive result on CIFAR-10. The explored network architecture is\ntransferable to ImageNet and achieves a new state-of-the-art accuracy, i.e.,\n75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test\nits performance on semantic segmentation with DeepLabv3 on the PASCAL VOC.\nRENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83%\nmIOU without being pre-trained on COCO.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:53:53 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 02:09:29 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 11:12:02 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chen", "Yukang", ""], ["Meng", "Gaofeng", ""], ["Zhang", "Qian", ""], ["Xiang", "Shiming", ""], ["Huang", "Chang", ""], ["Mu", "Lisen", ""], ["Wang", "Xinggang", ""]]}, {"id": "1808.00206", "submitter": "Tiantian Wang", "authors": "Tiantian Wang, Long Yang", "title": "Beetle Swarm Optimization Algorithm:Theory and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new meta-heuristic algorithm, called beetle swarm\noptimization algorithm, is proposed by enhancing the performance of swarm\noptimization through beetle foraging principles. The performance of 23\nbenchmark functions is tested and compared with widely used algorithms,\nincluding particle swarm optimization algorithm, genetic algorithm (GA) and\ngrasshopper optimization algorithm . Numerical experiments show that the beetle\nswarm optimization algorithm outperforms its counterparts. Besides, to\ndemonstrate the practical impact of the proposed algorithm, two classic\nengineering design problems, namely, pressure vessel design problem and\nhimmelblaus optimization problem, are also considered and the proposed beetle\nswarm optimization algorithm is shown to be competitive in those applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:34:46 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 11:38:27 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wang", "Tiantian", ""], ["Yang", "Long", ""]]}, {"id": "1808.00252", "submitter": "Pablo Barros", "authors": "Pablo Barros, Emilia Barakova, Stefan Wermter", "title": "A Deep Neural Model Of Emotion Appraisal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emotional concepts play a huge role in our daily life since they take part\ninto many cognitive processes: from the perception of the environment around us\nto different learning processes and natural communication. Social robots need\nto communicate with humans, which increased also the popularity of affective\nembodied models that adopt different emotional concepts in many everyday tasks.\nHowever, there is still a gap between the development of these solutions and\nthe integration and development of a complex emotion appraisal system, which is\nmuch necessary for true social robots. In this paper, we propose a deep neural\nmodel which is designed in the light of different aspects of developmental\nlearning of emotional concepts to provide an integrated solution for internal\nand external emotion appraisal. We evaluate the performance of the proposed\nmodel with different challenging corpora and compare it with state-of-the-art\nmodels for external emotion appraisal. To extend the evaluation of the proposed\nmodel, we designed and collected a novel dataset based on a Human-Robot\nInteraction (HRI) scenario. We deployed the model in an iCub robot and\nevaluated the capability of the robot to learn and describe the affective\nbehavior of different persons based on observation. The performed experiments\ndemonstrate that the proposed model is competitive with the state of the art in\ndescribing emotion behavior in general. In addition, it is able to generate\ninternal emotional concepts that evolve through time: it continuously forms and\nupdates the formed emotional concepts, which is a step towards creating an\nemotional appraisal model grounded in the robot experiences.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 10:28:58 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Barros", "Pablo", ""], ["Barakova", "Emilia", ""], ["Wermter", "Stefan", ""]]}, {"id": "1808.00260", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris", "title": "A Review on the Application of Natural Computing in Environmental\n  Informatics", "comments": "Proc. of EnviroInfo 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural computing offers new opportunities to understand, model and analyze\nthe complexity of the physical and human-created environment. This paper\nexamines the application of natural computing in environmental informatics, by\ninvestigating related work in this research field. Various nature-inspired\ntechniques are presented, which have been employed to solve different relevant\nproblems. Advantages and disadvantages of these techniques are discussed,\ntogether with analysis of how natural computing is generally used in\nenvironmental research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 10:53:01 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Kamilaris", "Andreas", ""]]}, {"id": "1808.00300", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Carl Doersch and Adam Santoro and Peter\n  Battaglia", "title": "Learning Visual Question Answering by Bootstrapping Hard Attention", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms in biological perception are thought to select subsets\nof perceptual information for more sophisticated processing which would be\nprohibitive to perform on all sensory inputs. In computer vision, however,\nthere has been relatively little exploration of hard attention, where some\ninformation is selectively ignored, in spite of the success of soft attention,\nwhere information is re-weighted and aggregated, but never filtered out. Here,\nwe introduce a new approach for hard attention and find it achieves very\ncompetitive performance on a recently-released visual question answering\ndatasets, equalling and in some cases surpassing similar soft attention\narchitectures while entirely ignoring some features. Even though the hard\nattention mechanism is thought to be non-differentiable, we found that the\nfeature magnitudes correlate with semantic relevance, and provide a useful\nsignal for our mechanism's attentional selection criterion. Because hard\nattention selects important features of the input information, it can also be\nmore efficient than analogous soft attention mechanisms. This is especially\nimportant for recent approaches that use non-local pairwise operations, whereby\ncomputational and memory costs are quadratic in the size of the set of\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 12:39:43 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Doersch", "Carl", ""], ["Santoro", "Adam", ""], ["Battaglia", "Peter", ""]]}, {"id": "1808.00458", "submitter": "Sunil Pai", "authors": "Sunil Pai, Ben Bartlett, Olav Solgaard, David A. B. Miller", "title": "Matrix optimization on universal unitary photonic devices", "comments": "18 pages, 2 tables, 14 figures, 6 videos (videos provided in Appendix\n  via external URL link)", "journal-ref": "Phys. Rev. Applied 11, 064044 (2019)", "doi": "10.1103/PhysRevApplied.11.064044", "report-no": null, "categories": "eess.SP cs.ET cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal unitary photonic devices can apply arbitrary unitary\ntransformations to a vector of input modes and provide a promising hardware\nplatform for fast and energy-efficient machine learning using light. We\nsimulate the gradient-based optimization of random unitary matrices on\nuniversal photonic devices composed of imperfect tunable interferometers. If\ndevice components are initialized uniform-randomly, the locally-interacting\nnature of the mesh components biases the optimization search space towards\nbanded unitary matrices, limiting convergence to random unitary matrices. We\ndetail a procedure for initializing the device by sampling from the\ndistribution of random unitary matrices and show that this greatly improves\nconvergence speed. We also explore mesh architecture improvements such as\nadding extra tunable beamsplitters or permuting waveguide layers to further\nimprove the training speed and scalability of these devices.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:27:13 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 19:14:24 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 01:44:06 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Pai", "Sunil", ""], ["Bartlett", "Ben", ""], ["Solgaard", "Olav", ""], ["Miller", "David A. B.", ""]]}, {"id": "1808.00508", "submitter": "Andrew Trask", "authors": "Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, Phil\n  Blunsom", "title": "Neural Arithmetic Logic Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks can learn to represent and manipulate numerical information,\nbut they seldom generalize well outside of the range of numerical values\nencountered during training. To encourage more systematic numerical\nextrapolation, we propose an architecture that represents numerical quantities\nas linear activations which are manipulated using primitive arithmetic\noperators, controlled by learned gates. We call this module a neural arithmetic\nlogic unit (NALU), by analogy to the arithmetic logic unit in traditional\nprocessors. Experiments show that NALU-enhanced neural networks can learn to\ntrack time, perform arithmetic over images of numbers, translate numerical\nlanguage into real-valued scalars, execute computer code, and count objects in\nimages. In contrast to conventional architectures, we obtain substantially\nbetter generalization both inside and outside of the range of numerical values\nencountered during training, often extrapolating orders of magnitude beyond\ntrained numerical ranges.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 18:58:53 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Trask", "Andrew", ""], ["Hill", "Felix", ""], ["Reed", "Scott", ""], ["Rae", "Jack", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1808.00524", "submitter": "Xiao-Feng Xie", "authors": "Xiao-Feng Xie, Zun-Jing Wang", "title": "Cooperative Group Optimization with Ants (CGO-AS): Leverage Optimization\n  with Mixed Individual and Social Learning", "comments": null, "journal-ref": "Applied Soft Computing, 50: 223-234, 2017", "doi": "10.1016/j.asoc.2016.11.018", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present CGO-AS, a generalized Ant System (AS) implemented in the framework\nof Cooperative Group Optimization (CGO), to show the leveraged optimization\nwith a mixed individual and social learning. Ant colony is a simple yet\nefficient natural system for understanding the effects of primary intelligence\non optimization. However, existing AS algorithms are mostly focusing on their\ncapability of using social heuristic cues while ignoring their individual\nlearning. CGO can integrate the advantages of a cooperative group and a\nlow-level algorithm portfolio design, and the agents of CGO can explore both\nindividual and social search. In CGO-AS, each ant (agent) is added with an\nindividual memory, and is implemented with a novel search strategy to use\nindividual and social cues in a controlled proportion. The presented CGO-AS is\ntherefore especially useful in exposing the power of the mixed individual and\nsocial learning for improving optimization. The optimization performance is\ntested with instances of the Traveling Salesman Problem (TSP). The results\nprove that a cooperative ant group using both individual and social learning\nobtains a better performance than the systems solely using either individual or\nsocial learning. The best performance is achieved under the condition when\nagents use individual memory as their primary information source, and\nsimultaneously use social memory as their searching guidance. In comparison\nwith existing AS systems, CGO-AS retains a faster learning speed toward those\nhigher-quality solutions, especially in the later learning cycles. The leverage\nin optimization by CGO-AS is highly possible due to its inherent feature of\nadaptively maintaining the population diversity in the individual memory of\nagents, and of accelerating the learning process with accumulated knowledge in\nthe social memory.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 19:28:45 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Xie", "Xiao-Feng", ""], ["Wang", "Zun-Jing", ""]]}, {"id": "1808.00597", "submitter": "Michael Hazoglou", "authors": "Michael Hazoglou, Todd Hylton", "title": "Saccadic Predictive Vision Model with a Fovea", "comments": "10 pages, 6 figure, Accepted in International Conference of\n  Neuromorphic Computing (2018)", "journal-ref": null, "doi": "10.1145/3229884.3229886", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model that emulates saccades, the rapid movements of the eye,\ncalled the Error Saccade Model, based on the prediction error of the Predictive\nVision Model (PVM). The Error Saccade Model carries out movements of the\nmodel's field of view to regions with the highest prediction error. Comparisons\nof the Error Saccade Model on Predictive Vision Models with and without a fovea\nshow that a fovea-like structure in the input level of the PVM improves the\nError Saccade Model's ability to pursue detailed objects in its view. We\nhypothesize that the improvement is due to poorer resolution in the periphery\ncausing higher prediction error when an object passes, triggering a saccade to\nthe next location.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 23:47:41 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Hazoglou", "Michael", ""], ["Hylton", "Todd", ""]]}, {"id": "1808.00675", "submitter": "Zhaofei Yu", "authors": "Zhaofei Yu, Yonghong Tian, Tiejun Huang, Jian K. Liu", "title": "Winner-Take-All as Basic Probabilistic Inference Unit of Neuronal\n  Circuits", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental observations of neuroscience suggest that the brain is working a\nprobabilistic way when computing information with uncertainty. This processing\ncould be modeled as Bayesian inference. However, it remains unclear how\nBayesian inference could be implemented at the level of neuronal circuits of\nthe brain. In this study, we propose a novel general-purpose neural\nimplementation of probabilistic inference based on a ubiquitous network of\ncortical microcircuits, termed winner-take-all (WTA) circuit. We show that each\nWTA circuit could encode the distribution of states defined on a variable. By\nconnecting multiple WTA circuits together, the joint distribution can be\nrepresented for arbitrary probabilistic graphical models. Moreover, we prove\nthat the neural dynamics of WTA circuit is able to implement one of the most\npowerful inference methods in probabilistic graphical models, mean-field\ninference. We show that the synaptic drive of each spiking neuron in the WTA\ncircuit encodes the marginal probability of the variable in each state, and the\nfiring probability (or firing rate) of each neuron is proportional to the\nmarginal probability. Theoretical analysis and experimental results demonstrate\nthat the WTA circuits can get comparable inference result as mean-field\napproximation. Taken together, our results suggest that the WTA circuit could\nbe seen as the minimal inference unit of neuronal circuits.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 05:56:30 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Yu", "Zhaofei", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""], ["Liu", "Jian K.", ""]]}, {"id": "1808.00733", "submitter": "Alex James Dr", "authors": "Olga Krestinskaya, Alex Pappachen James", "title": "Approximate Probabilistic Neural Networks with Gated Threshold Logic", "comments": null, "journal-ref": "IEEE NANO 2018", "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Neural Network (PNN) is a feed-forward artificial neural\nnetwork developed for solving classification problems. This paper proposes a\nhardware implementation of an approximated PNN (APNN) algorithm in which the\nconventional exponential function of the PNN is replaced with gated threshold\nlogic. The weights of the PNN are approximated using a memristive crossbar\narchitecture. In particular, the proposed algorithm performs normalization of\nthe training weights, and quantization into 16 levels which significantly\nreduces the complexity of the circuit.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:48:11 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Krestinskaya", "Olga", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1808.00737", "submitter": "Alex James Dr", "authors": "Olga Krestinskaya, Alex Pappachen James", "title": "Binary Weighted Memristive Analog Deep Neural Network for Near-Sensor\n  Edge Processing", "comments": null, "journal-ref": "IEEE NANO 2018", "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memristive crossbar aims to implement analog weighted neural network,\nhowever, the realistic implementation of such crossbar arrays is not possible\ndue to limited switching states of memristive devices. In this work, we propose\nthe design of an analog deep neural network with binary weight update through\nbackpropagation algorithm using binary state memristive devices. We show that\nsuch networks can be successfully used for image processing task and has the\nadvantage of lower power consumption and small on-chip area in comparison with\ndigital counterparts. The proposed network was benchmarked for MNIST\nhandwritten digits recognition achieving an accuracy of approximately 90%.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:51:07 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Krestinskaya", "Olga", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1808.00783", "submitter": "Peter M. Roth", "authors": "Mina Basirat and Peter M. Roth", "title": "The Quest for the Golden Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 12:44:09 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Basirat", "Mina", ""], ["Roth", "Peter M.", ""]]}, {"id": "1808.01137", "submitter": "Anders Martinsson", "authors": "Johannes Lengler, Anders Martinsson, Angelika Steger", "title": "When Does Hillclimbing Fail on Monotone Functions: An entropy\n  compression argument", "comments": "14 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hillclimbing is an essential part of any optimization algorithm. An important\nbenchmark for hillclimbing algorithms on pseudo-Boolean functions $f: \\{0,1\\}^n\n\\to \\mathbb{R}$ are (strictly) montone functions, on which a surprising number\nof hillclimbers fail to be efficient. For example, the $(1+1)$-Evolutionary\nAlgorithm is a standard hillclimber which flips each bit independently with\nprobability $c/n$ in each round. Perhaps surprisingly, this algorithm shows a\nphase transition: it optimizes any monotone pseudo-boolean function in\nquasilinear time if $c<1$, but there are monotone functions for which the\nalgorithm needs exponential time if $c>2.2$. But so far it was unclear whether\nthe threshold is at $c=1$.\n  In this paper we show how Moser's entropy compression argument can be adapted\nto this situation, that is, we show that a long runtime would allow us to\nencode the random steps of the algorithm with less bits than their entropy.\nThus there exists a $c_0 > 1$ such that for all $0<c\\le c_0$ the\n$(1+1)$-Evolutionary Algorithm with rate $c/n$ finds the optimum in $O(n \\log^2\nn)$ steps in expectation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:48:51 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Lengler", "Johannes", ""], ["Martinsson", "Anders", ""], ["Steger", "Angelika", ""]]}, {"id": "1808.01150", "submitter": "Faizal Hafiz", "authors": "Faizal Hafiz, Akshya Swain, Nitish Patel, Chirag Naik", "title": "A Two-Dimensional (2-D) Learning Framework for Particle Swarm based\n  Feature Selection", "comments": null, "journal-ref": "Elsevier - Pattern Recognition, Volume 76, 2018, Pages 416-433", "doi": "10.1016/j.patcog.2017.11.027", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new generalized two dimensional learning approach for\nparticle swarm based feature selection. The core idea of the proposed approach\nis to include the information about the subset cardinality into the learning\nframework by extending the dimension of the velocity. The 2D-learning framework\nretains all the key features of the original PSO, despite the extra learning\ndimension. Most of the popular variants of PSO can easily be adapted into this\n2D learning framework for feature selection problems. The efficacy of the\nproposed learning approach has been evaluated considering several benchmark\ndata and two induction algorithms: Naive-Bayes and k-Nearest Neighbor. The\nresults of the comparative investigation including the time-complexity analysis\nwith GA, ACO and five other PSO variants illustrate that the proposed 2D\nlearning approach gives feature subset with relatively smaller cardinality and\nbetter classification performance with shorter run times.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 10:50:43 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Hafiz", "Faizal", ""], ["Swain", "Akshya", ""], ["Patel", "Nitish", ""], ["Naik", "Chirag", ""]]}, {"id": "1808.01280", "submitter": "ShihChung Lo Ph.D.", "authors": "ShihChung B. Lo, Ph.D., Matthew T. Freedman, M.D., Seong K. Mun,\n  Ph.D., and Heang-Ping Chan, Ph.D", "title": "Geared Rotationally Identical and Invariant Convolutional Neural Network\n  Systems", "comments": "14 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theorems and techniques to form different types of transformationally\ninvariant processing and to produce the same output quantitatively based on\neither transformationally invariant operators or symmetric operations have\nrecently been introduced by the authors. In this study, we further propose to\ncompose a geared rotationally identical CNN system (GRI-CNN) with a small step\nangle by connecting networks of participated processes at the first flatten\nlayer. Using an ordinary CNN structure as a base, requirements for constructing\na GRI-CNN include the use of either symmetric input vector or kernels with an\nangle increment that can form a complete cycle as a \"gearwheel\". Four basic\nGRI-CNN structures were studied. Each of them can produce quantitatively\nidentical output results when a rotation angle of the input vector is evenly\ndivisible by the step angle of the gear. Our study showed when an input vector\nrotated with an angle does not match to a step angle, the GRI-CNN can also\nproduce a highly consistent result. With a design of using an ultra-fine\ngear-tooth step angle (e.g., 1 degree or 0.1 degree), all four GRI-CNN systems\ncan be constructed virtually isotropically.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 02:27:40 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 15:08:37 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 11:26:09 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Lo", "ShihChung B.", ""], ["D.", "Ph.", ""], ["Freedman", "Matthew T.", ""], ["D.", "M.", ""], ["Mun", "Seong K.", ""], ["D.", "Ph.", ""], ["Chan", "Heang-Ping", ""], ["D", "Ph.", ""]]}, {"id": "1808.01342", "submitter": "Xiao-Feng Xie", "authors": "Xiao-Feng Xie, Jiming Liu, Zun-Jing Wang", "title": "A Cooperative Group Optimization System", "comments": null, "journal-ref": "Soft Computing 18(3): 469-495, 2014", "doi": "10.1007/s00500-013-1069-8", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A cooperative group optimization (CGO) system is presented to implement CGO\ncases by integrating the advantages of the cooperative group and low-level\nalgorithm portfolio design. Following the nature-inspired paradigm of a\ncooperative group, the agents not only explore in a parallel way with their\nindividual memory, but also cooperate with their peers through the group\nmemory. Each agent holds a portfolio of (heterogeneous) embedded search\nheuristics (ESHs), in which each ESH can drive the group into a stand-alone CGO\ncase, and hybrid CGO cases in an algorithmic space can be defined by low-level\ncooperative search among a portfolio of ESHs through customized memory sharing.\nThe optimization process might also be facilitated by a passive group leader\nthrough encoding knowledge in the search landscape. Based on a concrete\nframework, CGO cases are defined by a script assembling over instances of\nalgorithmic components in a toolbox. A multilayer design of the script, with\nthe support of the inherent updatable graph in the memory protocol, enables a\nsimple way to address the challenge of accumulating heterogeneous ESHs and\ndefining customized portfolios without any additional code. The CGO system is\nimplemented for solving the constrained optimization problem with some generic\ncomponents and only a few domain-specific components. Guided by the insights\nfrom algorithm portfolio design, customized CGO cases based on basic search\noperators can achieve competitive performance over existing algorithms as\ncompared on a set of commonly-used benchmark instances. This work might provide\na basic step toward a user-oriented development framework, since the\nalgorithmic space might be easily evolved by accumulating competent ESHs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:19:53 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Xie", "Xiao-Feng", ""], ["Liu", "Jiming", ""], ["Wang", "Zun-Jing", ""]]}, {"id": "1808.01363", "submitter": "Ananda Samajdar", "authors": "Ananda Samajdar, Parth Mannan, Kartikay Garg and Tushar Krishna", "title": "GeneSys: Enabling Continuous Learning through Neural Network Evolution\n  in Hardware", "comments": "This work is accepted and will appear in MICRO-51", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning systems rely on (a) a hand-tuned neural network\ntopology, (b) massive amounts of labeled training data, and (c) extensive\ntraining over large-scale compute resources to build a system that can perform\nefficient image classification or speech recognition. Unfortunately, we are\nstill far away from implementing adaptive general purpose intelligent systems\nwhich would need to learn autonomously in unknown environments and may not have\naccess to some or any of these three components. Reinforcement learning and\nevolutionary algorithm (EA) based methods circumvent this problem by\ncontinuously interacting with the environment and updating the models based on\nobtained rewards. However, deploying these algorithms on ubiquitous autonomous\nagents at the edge (robots/drones) demands extremely high energy-efficiency due\nto (i) tight power and energy budgets, (ii) continuous/lifelong interaction\nwith the environment, (iii) intermittent or no connectivity to the cloud to run\nheavy-weight processing. To address this need, we present GENESYS, an HW-SW\nprototype of an EA-based learning system, that comprises a closed loop learning\nengine called EvE and an inference engine called ADAM. EvE can evolve the\ntopology and weights of neural networks completely in hardware for the task at\nhand, without requiring hand-optimization or backpropagation training. ADAM\ncontinuously interacts with the environment and is optimized for efficiently\nrunning the irregular neural networks generated by EvE. GENESYS identifies and\nleverages multiple unique avenues of parallelism unique to EAs that we term\n'gene'- level parallelism, and 'population'-level parallelism. We ran GENESYS\nwith a suite of environments from OpenAI gym and observed 2-5 orders of\nmagnitude higher energy-efficiency over state-of-the-art embedded and desktop\nCPU and GPU systems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 21:13:12 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 19:25:58 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Samajdar", "Ananda", ""], ["Mannan", "Parth", ""], ["Garg", "Kartikay", ""], ["Krishna", "Tushar", ""]]}, {"id": "1808.01766", "submitter": "Dedimuni De Silva", "authors": "M. U. B. Dias, D. D. N. De Silva, S. Fernando", "title": "On Optimizing Deep Convolutional Neural Networks by Evolutionary\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization for deep networks is currently a very active area of research.\nAs neural networks become deeper, the ability in manually optimizing the\nnetwork becomes harder. Mini-batch normalization, identification of effective\nrespective fields, momentum updates, introduction of residual blocks, learning\nrate adoption, etc. have been proposed to speed up the rate of convergent in\nmanual training process while keeping the higher accuracy level. However, the\nproblem of finding optimal topological structure for a given problem is\nbecoming a challenging task need to be addressed immediately. Few researchers\nhave attempted to optimize the network structure using evolutionary computing\napproaches. Among them, few have successfully evolved networks with\nreinforcement learning and long-short-term memory. A very few has applied\nevolutionary programming into deep convolution neural networks. These attempts\nare mainly evolved the network structure and then subsequently optimized the\nhyper-parameters of the network. However, a mechanism to evolve the deep\nnetwork structure under the techniques currently being practiced in manual\nprocess is still absent. Incorporation of such techniques into chromosomes\nlevel of evolutionary computing, certainly can take us to better topological\ndeep structures. The paper concludes by identifying the gap between\nevolutionary based deep neural networks and deep neural networks. Further, it\nproposes some insights for optimizing deep neural networks using evolutionary\ncomputing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 08:29:26 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Dias", "M. U. B.", ""], ["De Silva", "D. D. N.", ""], ["Fernando", "S.", ""]]}, {"id": "1808.02016", "submitter": "Abduallah Mohamed", "authors": "Abduallah A. Mohamed and Christian Claudel", "title": "MCRM: Mother Compact Recurrent Memory", "comments": "Submitted to AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTMs and GRUs are the most common recurrent neural network architectures\nused to solve temporal sequence problems. The two architectures have differing\ndata flows dealing with a common component called the cell state (also referred\nto as the memory). We attempt to enhance the memory by presenting a\nmodification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are\na type of a nested LSTM-GRU architecture where the cell state is the GRU hidden\nstate. The concatenation of the forget gate and input gate interactions from\nthe LSTM are considered an input to the GRU cell. Because MCRMs has this type\nof nesting, MCRMs have a compact memory pattern consisting of neurons that acts\nexplicitly in both long-term and short-term fashions. For some specific tasks,\nempirical results show that MCRMs outperform previously used architectures.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 15:48:39 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 06:00:41 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 21:37:25 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Mohamed", "Abduallah A.", ""], ["Claudel", "Christian", ""]]}, {"id": "1808.02185", "submitter": "Xiao-Feng Xie", "authors": "Xiao-Feng Xie", "title": "Round-Table Group Optimization for Sequencing Problems", "comments": null, "journal-ref": "International Journal of Applied Metaheuristic Computing 2012,\n  3(4), 1-24", "doi": "10.4018/jamc.2012100101", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a round-table group optimization (RTGO) algorithm is\npresented. RTGO is a simple metaheuristic framework using the insights of\nresearch on group creativity. In a cooperative group, the agents work in\niterative sessions to search innovative ideas in a common problem landscape.\nEach agent has one base idea stored in its individual memory, and one social\nidea fed by a round-table group support mechanism in each session. The idea\ncombination and improvement processes are respectively realized by using a\nrecombination search (XS) strategy and a local search (LS) strategy, to build\non the base and social ideas. RTGO is then implemented for solving two\ndifficult sequencing problems, i.e., the flowshop scheduling problem and the\nquadratic assignment problem. The domain-specific LS strategies are adopted\nfrom existing algorithms, whereas a general XS class, called socially biased\ncombination (SBX), is realized in a modular form. The performance of RTGO is\nthen evaluated on commonly-used benchmark datasets. Good performance on\ndifferent problems can be achieved by RTGO using appropriate SBX operators.\nFurthermore, RTGO is able to outperform some existing methods, including\nmethods using the same LS strategies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 02:22:26 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Xie", "Xiao-Feng", ""]]}, {"id": "1808.02234", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Dianhui Wang", "title": "Deep Stacked Stochastic Configuration Networks for Lifelong Learning of\n  Non-Stationary Data Streams", "comments": "This paper has been published in Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of SCN offers a fast framework with universal approximation\nguarantee for lifelong learning of non-stationary data streams. Its adaptive\nscope selection property enables for proper random generation of hidden unit\nparameters advancing conventional randomized approaches constrained with a\nfixed scope of random parameters. This paper proposes deep stacked stochastic\nconfiguration network (DSSCN) for continual learning of non-stationary data\nstreams which contributes two major aspects: 1) DSSCN features a\nself-constructing methodology of deep stacked network structure where hidden\nunit and hidden layer are extracted automatically from continuously generated\ndata streams; 2) the concept of SCN is developed to randomly assign inverse\ncovariance matrix of multivariate Gaussian function in the hidden node addition\nstep bypassing its computationally prohibitive tuning phase. Numerical\nevaluation and comparison with prominent data stream algorithms under two\nprocedures: periodic hold-out and prequential test-then-train processes\ndemonstrate the advantage of proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:15:54 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 15:44:25 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 13:24:25 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Wang", "Dianhui", ""]]}, {"id": "1808.02668", "submitter": "Corentin Kervadec", "authors": "Valentin Vielzeuf, Corentin Kervadec, St\\'ephane Pateux, Alexis\n  Lechervy, Fr\\'ed\\'eric Jurie", "title": "An Occam's Razor View on Learning Audiovisual Emotion Recognition with\n  Small Training Sets", "comments": null, "journal-ref": "ICMI (EmotiW) 2018, Oct 2018, Boulder, Colorado, United States", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a light-weight and accurate deep neural model for\naudiovisual emotion recognition. To design this model, the authors followed a\nphilosophy of simplicity, drastically limiting the number of parameters to\nlearn from the target datasets, always choosing the simplest earning methods:\ni) transfer learning and low-dimensional space embedding allows to reduce the\ndimensionality of the representations. ii) The isual temporal information is\nhandled by a simple score-per-frame selection process, averaged across time.\niii) A simple frame selection echanism is also proposed to weight the images of\na sequence. iv) The fusion of the different modalities is performed at\nprediction level (late usion). We also highlight the inherent challenges of the\nAFEW dataset and the difficulty of model selection with as few as 383\nvalidation equences. The proposed real-time emotion classifier achieved a\nstate-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at\nhe Emotion in the Wild 2018 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 08:43:43 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Kervadec", "Corentin", ""], ["Pateux", "St\u00e9phane", ""], ["Lechervy", "Alexis", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1808.02822", "submitter": "Maximilian Alber", "authors": "Maximilian Alber, Irwan Bello, Barret Zoph, Pieter-Jan Kindermans,\n  Prajit Ramachandran, Quoc Le", "title": "Backprop Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The back-propagation algorithm is the cornerstone of deep learning. Despite\nits importance, few variations of the algorithm have been attempted. This work\npresents an approach to discover new variations of the back-propagation\nequation. We use a domain specific lan- guage to describe update equations as a\nlist of primitive functions. An evolution-based method is used to discover new\npropagation rules that maximize the generalization per- formance after a few\nepochs of training. We find several update equations that can train faster with\nshort training times than standard back-propagation, and perform similar as\nstandard back-propagation at convergence.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:23:14 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Alber", "Maximilian", ""], ["Bello", "Irwan", ""], ["Zoph", "Barret", ""], ["Kindermans", "Pieter-Jan", ""], ["Ramachandran", "Prajit", ""], ["Le", "Quoc", ""]]}, {"id": "1808.02997", "submitter": "Felipe Campelo", "authors": "Felipe Campelo and Fernanda Takahashi", "title": "Sample size estimation for power and accuracy in the experimental\n  comparison of algorithms", "comments": "Main text: 31 pages, 5 figures; Supplemental materials: 20 pages, 3\n  figures; Submitted to the Journal of Heuristics on October 2017", "journal-ref": null, "doi": "10.1007/s10732-018-9396-7", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental comparisons of performance represent an important aspect of\nresearch on optimization algorithms. In this work we present a methodology for\ndefining the required sample sizes for designing experiments with desired\nstatistical properties for the comparison of two methods on a given problem\nclass. The proposed approach allows the experimenter to define desired levels\nof accuracy for estimates of mean performance differences on individual problem\ninstances, as well as the desired statistical power for comparing mean\nperformances over a problem class of interest. The method calculates the\nrequired number of problem instances, and runs the algorithms on each test\ninstance so that the accuracy of the estimated differences in performance is\ncontrolled at the predefined level. Two examples illustrate the application of\nthe proposed method, and its ability to achieve the desired statistical\nproperties with a methodologically sound definition of the relevant sample\nsizes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 02:17:52 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 14:52:32 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Campelo", "Felipe", ""], ["Takahashi", "Fernanda", ""]]}, {"id": "1808.03166", "submitter": "Jos\\'e Halloy", "authors": "Leo Cazenille, Nicolas Bredeche and Jos\\'e Halloy", "title": "Evolutionary optimisation of neural network models for fish collective\n  behaviours in mixed groups of robots and zebrafish", "comments": "10 pages, 4 figures", "journal-ref": "Lecture Notes in Computer Science, vol 10928. Springer, Cham, 2018", "doi": "10.1007/978-3-319-95972-6_10", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal and robot social interactions are interesting both for ethological\nstudies and robotics. On the one hand, the robots can be tools and models to\nanalyse animal collective behaviours, on the other hand, the robots and their\nartificial intelligence are directly confronted and compared to the natural\nanimal collective intelligence. The first step is to design robots and their\nbehavioural controllers that are capable of socially interact with animals.\nDesigning such behavioural bio-mimetic controllers remains an important\nchallenge as they have to reproduce the animal behaviours and have to be\ncalibrated on experimental data. Most animal collective behavioural models are\ndesigned by modellers based on experimental data. This process is long and\ncostly because it is difficult to identify the relevant behavioural features\nthat are then used as a priori knowledge in model building. Here, we want to\nmodel the fish individual and collective behaviours in order to develop robot\ncontrollers. We explore the use of optimised black-box models based on\nartificial neural networks (ANN) to model fish behaviours. While the ANN may\nnot be biomimetic but rather bio-inspired, they can be used to link perception\nto motor responses. These models are designed to be implementable as robot\ncontrollers to form mixed-groups of fish and robots, using few a priori\nknowledge of the fish behaviours. We present a methodology with multilayer\nperceptron or echo state networks that are optimised through evolutionary\nalgorithms to model accurately the fish individual and collective behaviours in\na bounded rectangular arena. We assess the biomimetism of the generated models\nand compare them to the fish experimental behaviours.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 13:54:23 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Cazenille", "Leo", ""], ["Bredeche", "Nicolas", ""], ["Halloy", "Jos\u00e9", ""]]}, {"id": "1808.03327", "submitter": "Shounak Datta", "authors": "Avisek Gupta, Shounak Datta, Swagatam Das", "title": "Fuzzy Clustering to Identify Clusters at Different Levels of Fuzziness:\n  An Evolutionary Multi-Objective Optimization Approach", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy clustering methods identify naturally occurring clusters in a dataset,\nwhere the extent to which different clusters are overlapped can differ. Most\nmethods have a parameter to fix the level of fuzziness. However, the\nappropriate level of fuzziness depends on the application at hand. This paper\npresents Entropy $c$-Means (ECM), a method of fuzzy clustering that\nsimultaneously optimizes two contradictory objective functions, resulting in\nthe creation of fuzzy clusters with different levels of fuzziness. This allows\nECM to identify clusters with different degrees of overlap. ECM optimizes the\ntwo objective functions using two multi-objective optimization methods,\nNon-dominated Sorting Genetic Algorithm II (NSGA-II), and Multiobjective\nEvolutionary Algorithm based on Decomposition (MOEA/D). We also propose a\nmethod to select a suitable trade-off clustering from the Pareto front.\nExperiments on challenging synthetic datasets as well as real-world datasets\nshow that ECM leads to better cluster detection compared to the conventional\nfuzzy clustering methods as well as previously used multi-objective methods for\nfuzzy clustering.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 19:48:41 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Gupta", "Avisek", ""], ["Datta", "Shounak", ""], ["Das", "Swagatam", ""]]}, {"id": "1808.03357", "submitter": "Adam Kohan", "authors": "Adam A. Kohan, Edward A. Rietman, Hava T. Siegelmann", "title": "Error Forward-Propagation: Reusing Feedforward Connections to Propagate\n  Errors in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Error Forward-Propagation, a biologically plausible mechanism to\npropagate error feedback forward through the network. Architectural constraints\non connectivity are virtually eliminated for error feedback in the brain;\nsystematic backward connectivity is not used or needed to deliver error\nfeedback. Feedback as a means of assigning credit to neurons earlier in the\nforward pathway for their contribution to the final output is thought to be\nused in learning in the brain. How the brain solves the credit assignment\nproblem is unclear. In machine learning, error backpropagation is a highly\nsuccessful mechanism for credit assignment in deep multilayered networks.\nBackpropagation requires symmetric reciprocal connectivity for every neuron.\nFrom a biological perspective, there is no evidence of such an architectural\nconstraint, which makes backpropagation implausible for learning in the brain.\nThis architectural constraint is reduced with the use of random feedback\nweights. Models using random feedback weights require backward connectivity\npatterns for every neuron, but avoid symmetric weights and reciprocal\nconnections. In this paper, we practically remove this architectural\nconstraint, requiring only a backward loop connection for effective error\nfeedback. We propose reusing the forward connections to deliver the error\nfeedback by feeding the outputs into the input receiving layer. This mechanism,\nError Forward-Propagation, is a plausible basis for how error feedback occurs\ndeep in the brain independent of and yet in support of the functionality\nunderlying intricate network architectures. We show experimentally that\nrecurrent neural networks with two and three hidden layers can be trained using\nError Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving\n$1.90\\%$ and $11\\%$ generalization errors respectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 21:52:10 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Kohan", "Adam A.", ""], ["Rietman", "Edward A.", ""], ["Siegelmann", "Hava T.", ""]]}, {"id": "1808.03387", "submitter": "Janardan Misra", "authors": "Janardan Misra", "title": "Computational Complexity of Observing Evolution in Artificial-Life Forms", "comments": "arXiv admin note: substantial text overlap with arXiv:0901.1610", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations are an essential component of the simulation based studies on\nartificial-evolutionary systems (AES) by which entities are identified and\ntheir behavior is observed to uncover higher-level \"emergent\" phenomena.\nBecause of the heterogeneity of AES models and implicit nature of observations,\nprecise characterization of the observation process, independent of the\nunderlying micro-level reaction semantics of the model, is a difficult problem.\nBuilding upon the multiset based algebraic framework to characterize\nstate-space trajectory of AES model simulations, we estimate bounds on\ncomputational resource requirements of the process of automatically discovering\nlife-like evolutionary behavior in AES models during simulations. For\nillustration, we consider the case of Langton's Cellular Automata model and\ncharacterize the worst case computational complexity bounds for identifying\nentity and population level reproduction.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 04:18:55 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Misra", "Janardan", ""]]}, {"id": "1808.03471", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "The Evolution of Sex Chromosomes through the Baldwin Effect", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been suggested that the fundamental haploid-diploid cycle of\neukaryotic sex exploits a rudimentary form of the Baldwin effect. Thereafter\nthe other associated phenomena can be explained as evolution tuning the amount\nand frequency of learning experienced by an organism. Using the well-known NK\nmodel of fitness landscapes it is here shown that the emergence of sex\ndetermination systems can also be explained under this view of eukaryotic\nevolution.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 09:54:30 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 14:21:57 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 13:40:45 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2020 12:15:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1808.03703", "submitter": "Daniel Kondratyuk", "authors": "Daniel Kondratyuk, Tom\\'a\\v{s} Gaven\\v{c}iak, Milan Straka, Jan\n  Haji\\v{c}", "title": "LemmaTag: Jointly Tagging and Lemmatizing for Morphologically-Rich\n  Languages with BRNNs", "comments": "8 pages, 3 figures. Submitted to EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LemmaTag, a featureless neural network architecture that jointly\ngenerates part-of-speech tags and lemmas for sentences by using bidirectional\nRNNs with character-level and word-level embeddings. We demonstrate that both\ntasks benefit from sharing the encoding part of the network, predicting tag\nsubcategories, and using the tagger output as an input to the lemmatizer. We\nevaluate our model across several languages with complex morphology, which\nsurpasses state-of-the-art accuracy in both part-of-speech tagging and\nlemmatization in Czech, German, and Arabic.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 20:46:32 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 16:36:40 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kondratyuk", "Daniel", ""], ["Gaven\u010diak", "Tom\u00e1\u0161", ""], ["Straka", "Milan", ""], ["Haji\u010d", "Jan", ""]]}, {"id": "1808.03818", "submitter": "Yanan Sun", "authors": "Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen", "title": "Automatically designing CNN architectures using genetic algorithm for\n  image classification", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics, 2020", "doi": "10.1109/TCYB.2020.2983860", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have gained a remarkable success on many\nimage classification tasks in recent years. However, the performance of CNNs\nhighly relies upon their architectures. For most state-of-the-art CNNs, their\narchitectures are often manually-designed with expertise in both CNNs and the\ninvestigated problems. Therefore, it is difficult for users, who have no\nextended expertise in CNNs, to design optimal CNN architectures for their own\nimage classification problems of interest. In this paper, we propose an\nautomatic CNN architecture design method by using genetic algorithms, to\neffectively address the image classification tasks. The most merit of the\nproposed algorithm remains in its \"automatic\" characteristic that users do not\nneed domain knowledge of CNNs when using the proposed algorithm, while they can\nstill obtain a promising CNN architecture for the given images. The proposed\nalgorithm is validated on widely used benchmark image classification datasets,\nby comparing to the state-of-the-art peer competitors covering eight\nmanually-designed CNNs, seven automatic+manually tuning and five automatic CNN\narchitecture design algorithms. The experimental results indicate the proposed\nalgorithm outperforms the existing automatic CNN architecture design algorithms\nin terms of classification accuracy, parameter numbers and consumed\ncomputational resources. The proposed algorithm also shows the very comparable\nclassification accuracy to the best one from manually-designed and\nautomatic+manually tuning CNNs, while consumes much less of computational\nresource.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 15:32:06 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 13:19:56 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 02:31:49 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""], ["Yen", "Gary G.", ""]]}, {"id": "1808.03884", "submitter": "Cameron Musco", "authors": "Nancy Lynch and Cameron Musco", "title": "A Basic Compositional Model for Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a formal, mathematical foundation for modeling and\nreasoning about the behavior of $synchronous$, $stochastic$ $Spiking$ $Neural$\n$Networks$ $(SNNs)$. We define a basic SNN model, in which a neuron's only\nstate is a Boolean value indicating whether the neuron is currently firing. We\nalso define the $external\\ behavior$ of an SNN. We define two operators on\nSNNs: a $composition\\ operator$, which supports modeling of SNNs as\ncombinations of smaller SNNs, and a $hiding\\ operator$, which reclassifies some\noutput behavior of an SNN as internal. We prove results describing how the\nexternal behavior of a network built using these operators is related to the\nexternal behavior of the component networks. Finally, we give a formal\ndefinition of a $problem$ to be solved by an SNN, and give basic results\nshowing how the composition and hiding operators affect the problems that are\nsolved by the networks. We illustrate our definitions with three examples: a\nBoolean circuit constructed from gates, an $Attention$ network constructed from\na $Winner$-$Take$-$All$ network and a $Filter$ network, and a toy example\ninvolving combining two networks in a cyclic fashion.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 02:38:36 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:04:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lynch", "Nancy", ""], ["Musco", "Cameron", ""]]}, {"id": "1808.03920", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Ziyin Liu, Amir Zadeh, Louis-Philippe Morency", "title": "Multimodal Language Analysis with Recurrent Multistage Fusion", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational modeling of human multimodal language is an emerging research\narea in natural language processing spanning the language, visual and acoustic\nmodalities. Comprehending multimodal language requires modeling not only the\ninteractions within each modality (intra-modal interactions) but more\nimportantly the interactions between modalities (cross-modal interactions). In\nthis paper, we propose the Recurrent Multistage Fusion Network (RMFN) which\ndecomposes the fusion problem into multiple stages, each of them focused on a\nsubset of multimodal signals for specialized, effective fusion. Cross-modal\ninteractions are modeled using this multistage fusion approach which builds\nupon intermediate representations of previous stages. Temporal and intra-modal\ninteractions are modeled by integrating our proposed fusion approach with a\nsystem of recurrent neural networks. The RMFN displays state-of-the-art\nperformance in modeling human multimodal language across three public datasets\nrelating to multimodal sentiment analysis, emotion recognition, and speaker\ntraits recognition. We provide visualizations to show that each stage of fusion\nfocuses on a different subset of multimodal signals, learning increasingly\ndiscriminative multimodal representations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 10:04:45 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Liang", "Paul Pu", ""], ["Liu", "Ziyin", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1808.03958", "submitter": "Zhaofei Yu", "authors": "Shanshan Jia and Zhaofei Yu and Arno Onken and Yonghong Tian and\n  Tiejun Huang and Jian K. Liu", "title": "Neural System Identification with Spike-triggered Non-negative Matrix\n  Factorization", "comments": "updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuronal circuits formed in the brain are complex with intricate connection\npatterns. Such complexity is also observed in the retina as a relatively simple\nneuronal circuit. A retinal ganglion cell receives excitatory inputs from\nneurons in previous layers as driving forces to fire spikes. Analytical methods\nare required that can decipher these components in a systematic manner.\nRecently a method termed spike-triggered non-negative matrix factorization\n(STNMF) has been proposed for this purpose. In this study, we extend the scope\nof the STNMF method. By using the retinal ganglion cell as a model system, we\nshow that STNMF can detect various computational properties of upstream bipolar\ncells, including spatial receptive field, temporal filter, and transfer\nnonlinearity. In addition, we recover synaptic connection strengths from the\nweight matrix of STNMF. Furthermore, we show that STNMF can separate spikes of\na ganglion cell into a few subsets of spikes where each subset is contributed\nby one presynaptic bipolar cell. Taken together, these results corroborate that\nSTNMF is a useful method for deciphering the structure of neuronal circuits.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 15:37:40 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 05:09:10 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 03:02:36 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2020 12:14:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Jia", "Shanshan", ""], ["Yu", "Zhaofei", ""], ["Onken", "Arno", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""], ["Liu", "Jian K.", ""]]}, {"id": "1808.04752", "submitter": "Yunhui Guo", "authors": "Yunhui Guo", "title": "A Survey on Methods and Theories of Quantized Neural Networks", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are the state-of-the-art methods for many real-world\ntasks, such as computer vision, natural language processing and speech\nrecognition. For all its popularity, deep neural networks are also criticized\nfor consuming a lot of memory and draining battery life of devices during\ntraining and inference. This makes it hard to deploy these models on mobile or\nembedded devices which have tight resource constraints. Quantization is\nrecognized as one of the most effective approaches to satisfy the extreme\nmemory requirements that deep neural network models demand. Instead of adopting\n32-bit floating point format to represent weights, quantized representations\nstore weights using more compact formats such as integers or even binary\nnumbers. Despite a possible degradation in predictive performance, quantization\nprovides a potential solution to greatly reduce the model size and the energy\nconsumption. In this survey, we give a thorough review of different aspects of\nquantized neural networks. Current challenges and trends of quantized neural\nnetworks are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:11:43 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 08:26:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Guo", "Yunhui", ""]]}, {"id": "1808.05238", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Yufang Huang, Liang Zeng, Xuming Chen, Yong Liu, Zhen\n  Qian, Nan Du, Wei Fan, Xiaohui Xie", "title": "AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume\n  Segmentation of Head and Neck Anatomy", "comments": "6 figures, 4 videos in GitHub and YouTube. Accepted by Medical\n  Physics. Code and videos are available on GitHub. Video:\n  https://www.youtube.com/watch?v=_PpIUIm4XLU", "journal-ref": null, "doi": "10.1002/mp.13300", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Methods: Our deep learning model, called AnatomyNet, segments OARs from head\nand neck CT images in an end-to-end fashion, receiving whole-volume HaN CT\nimages as input and generating masks of all OARs of interest in one shot.\nAnatomyNet is built upon the popular 3D U-net architecture, but extends it in\nthree important ways: 1) a new encoding scheme to allow auto-segmentation on\nwhole-volume CT images instead of local patches or subsets of slices, 2)\nincorporating 3D squeeze-and-excitation residual blocks in encoding layers for\nbetter feature representation, and 3) a new loss function combining Dice scores\nand focal loss to facilitate the training of the neural model. These features\nare designed to address two main challenges in deep-learning-based HaN\nsegmentation: a) segmenting small anatomies (i.e., optic chiasm and optic\nnerves) occupying only a few slices, and b) training with inconsistent data\nannotations with missing ground truth for some anatomical structures.\n  Results: We collected 261 HaN CT images to train AnatomyNet, and used MICCAI\nHead and Neck Auto Segmentation Challenge 2015 as a benchmark dataset to\nevaluate the performance of AnatomyNet. The objective is to segment nine\nanatomies: brain stem, chiasm, mandible, optic nerve left, optic nerve right,\nparotid gland left, parotid gland right, submandibular gland left, and\nsubmandibular gland right. Compared to previous state-of-the-art results from\nthe MICCAI 2015 competition, AnatomyNet increases Dice similarity coefficient\nby 3.3% on average. AnatomyNet takes about 0.12 seconds to fully segment a head\nand neck CT image of dimension 178 x 302 x 225, significantly faster than\nprevious methods. In addition, the model is able to process whole-volume CT\nimages and delineate all OARs in one pass, requiring little pre- or\npost-processing.\nhttps://github.com/wentaozhu/AnatomyNet-for-anatomical-segmentation.git.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 18:03:12 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 00:23:48 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Zhu", "Wentao", ""], ["Huang", "Yufang", ""], ["Zeng", "Liang", ""], ["Chen", "Xuming", ""], ["Liu", "Yong", ""], ["Qian", "Zhen", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1808.05377", "submitter": "Thomas Elsken", "authors": "Thomas Elsken, Jan Hendrik Metzen, Frank Hutter", "title": "Neural Architecture Search: A Survey", "comments": null, "journal-ref": "Journal of Machine Learning Research 20 (2019) 1-21", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has enabled remarkable progress over the last years on a\nvariety of tasks, such as image recognition, speech recognition, and machine\ntranslation. One crucial aspect for this progress are novel neural\narchitectures. Currently employed architectures have mostly been developed\nmanually by human experts, which is a time-consuming and error-prone process.\nBecause of this, there is growing interest in automated neural architecture\nsearch methods. We provide an overview of existing work in this field of\nresearch and categorize them according to three dimensions: search space,\nsearch strategy, and performance estimation strategy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 08:45:01 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 13:49:26 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 09:50:47 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Elsken", "Thomas", ""], ["Metzen", "Jan Hendrik", ""], ["Hutter", "Frank", ""]]}, {"id": "1808.05385", "submitter": "Yu Li", "authors": "Yu Li, Lizhong Ding, Xin Gao", "title": "On the Decision Boundary of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning models and techniques have achieved great empirical\nsuccess, our understanding of the source of success in many aspects remains\nvery limited. In an attempt to bridge the gap, we investigate the decision\nboundary of a production deep learning architecture with weak assumptions on\nboth the training data and the model. We demonstrate, both theoretically and\nempirically, that the last weight layer of a neural network converges to a\nlinear SVM trained on the output of the last hidden layer, for both the binary\ncase and the multi-class case with the commonly used cross-entropy loss.\nFurthermore, we show empirically that training a neural network as a whole,\ninstead of only fine-tuning the last weight layer, may result in better bias\nconstant for the last weight layer, which is important for generalization. In\naddition to facilitating the understanding of deep learning, our result can be\nhelpful for solving a broad range of practical problems of deep learning, such\nas catastrophic forgetting and adversarial attacking. The experiment codes are\navailable at https://github.com/lykaust15/NN_decision_boundary\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:25:50 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 06:23:32 GMT"}, {"version": "v3", "created": "Tue, 1 Jan 2019 08:20:22 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Li", "Yu", ""], ["Ding", "Lizhong", ""], ["Gao", "Xin", ""]]}, {"id": "1808.05488", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional\n  Network Inference on Video Streams", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.04313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few years have brought advances in computer vision at an amazing\npace, grounded on new findings in deep neural network construction and training\nas well as the availability of large labeled datasets. Applying these networks\nto images demands a high computational effort and pushes the use of\nstate-of-the-art networks on real-time video data out of reach of embedded\nplatforms. Many recent works focus on reducing network complexity for real-time\ninference on embedded computing platforms. We adopt an orthogonal viewpoint and\npropose a novel algorithm exploiting the spatio-temporal sparsity of pixel\nchanges. This optimized inference procedure resulted in an average speed-up of\n9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of <0.1%\nand no retraining of the network for a semantic segmentation application.\nSimilarly, an average speed-up of 7.0x has been achieved for a pose detection\nDNN and a reduction of 5x of the number of arithmetic operations to be\nperformed for object detection on static camera video surveillance data. These\nthroughput gains combined with a lower power consumption result in an energy\nefficiency of 511 GOp/s/W compared to 70 GOp/s/W for the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:27:29 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 17:07:31 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1808.05525", "submitter": "Ahmed Aly", "authors": "Ahmed Aly, Joanne B. Dugan", "title": "Experiential Robot Learning with Accelerated Neuroevolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivative-based optimization techniques such as Stochastic Gradient Descent\nhas been wildly successful in training deep neural networks. However, it has\nconstraints such as end-to-end network differentiability. As an alternative, we\npresent the Accelerated Neuroevolution algorithm. The new algorithm is aimed\ntowards physical robotic learning tasks following the Experiential Robot\nLearning method. We test our algorithm first on a simulated task of playing the\ngame Flappy Bird, then on a physical NAO robot in a static Object Centering\ntask. The agents successfully navigate the given tasks, in a relatively low\nnumber of generations. Based on our results, we propose to use the algorithm in\nmore complex tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:51:49 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Aly", "Ahmed", ""], ["Dugan", "Joanne B.", ""]]}, {"id": "1808.05566", "submitter": "Florian Meier", "authors": "Hafsteinn Einarsson, Marcelo Matheus Gauy, Johannes Lengler, Florian\n  Meier, Asier Mujika, Angelika Steger, Felix Weissenberger", "title": "The linear hidden subset problem for the (1+1) EA with scheduled and\n  adaptive mutation rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unbiased $(1+1)$ evolutionary algorithms on linear functions with an\nunknown number $n$ of bits with non-zero weight. Static algorithms achieve an\noptimal runtime of $O(n (\\ln n)^{2+\\epsilon})$, however, it remained unclear\nwhether more dynamic parameter policies could yield better runtime guarantees.\nWe consider two setups: one where the mutation rate follows a fixed schedule,\nand one where it may be adapted depending on the history of the run. For the\nfirst setup, we give a schedule that achieves a runtime of $(1\\pm o(1))\\beta n\n\\ln n$, where $\\beta \\approx 3.552$, which is an asymptotic improvement over\nthe runtime of the static setup. Moreover, we show that no schedule admits a\nbetter runtime guarantee and that the optimal schedule is essentially unique.\nFor the second setup, we show that the runtime can be further improved to\n$(1\\pm o(1)) e n \\ln n$, which matches the performance of algorithms that know\n$n$ in advance.\n  Finally, we study the related model of initial segment uncertainty with\nstatic position-dependent mutation rates, and derive asymptotically optimal\nlower bounds. This answers a question by Doerr, Doerr, and K\\\"otzing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:15:13 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Einarsson", "Hafsteinn", ""], ["Gauy", "Marcelo Matheus", ""], ["Lengler", "Johannes", ""], ["Meier", "Florian", ""], ["Mujika", "Asier", ""], ["Steger", "Angelika", ""], ["Weissenberger", "Felix", ""]]}, {"id": "1808.05850", "submitter": "Sander van Rijn", "authors": "Carola Doerr, Furong Ye, Sander van Rijn, Hao Wang, Thomas B\\\"ack", "title": "Towards a Theory-Guided Benchmarking Suite for Discrete Black-Box\n  Optimization Heuristics: Profiling $(1+\\lambda)$ EA Variants on OneMax and\n  LeadingOnes", "comments": null, "journal-ref": "GECCO '18 Proceedings of the Genetic and Evolutionary Computation\n  Conference Pages 951-958 Kyoto, Japan - July 15-19, 2018", "doi": "10.1145/3205455.3205621", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical and empirical research on evolutionary computation methods\ncomplement each other by providing two fundamentally different approaches\ntowards a better understanding of black-box optimization heuristics. In\ndiscrete optimization, both streams developed rather independently of each\nother, but we observe today an increasing interest in reconciling these two\nsub-branches. In continuous optimization, the COCO (COmparing Continuous\nOptimisers) benchmarking suite has established itself as an important platform\nthat theoreticians and practitioners use to exchange research ideas and\nquestions. No widely accepted equivalent exists in the research domain of\ndiscrete black-box optimization.\n  Marking an important step towards filling this gap, we adjust the COCO\nsoftware to pseudo-Boolean optimization problems, and obtain from this a\nbenchmarking environment that allows a fine-grained empirical analysis of\ndiscrete black-box heuristics. In this documentation we demonstrate how this\ntest bed can be used to profile the performance of evolutionary algorithms.\nMore concretely, we study the optimization behavior of several $(1+\\lambda)$ EA\nvariants on the two benchmark problems OneMax and LeadingOnes. This comparison\nmotivates a refined analysis for the optimization time of the $(1+\\lambda)$ EA\non LeadingOnes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 13:23:06 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Doerr", "Carola", ""], ["Ye", "Furong", ""], ["van Rijn", "Sander", ""], ["Wang", "Hao", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1808.05979", "submitter": "Khalid Raza", "authors": "Tarun Kumar Gupta and Khalid Raza", "title": "Optimizing Deep Neural Network Architecture: A Tabu Search Based\n  Approach", "comments": "15 pages, 2 figures, 2 algorithms, 2 tables", "journal-ref": "Neural Processing Letters (2020), Springer", "doi": "10.1007/s11063-020-10234-7", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Feedforward neural network (FNN) fully de-pends upon the\nselection of architecture and training algorithm. FNN architecture can be\ntweaked using several parameters, such as the number of hidden layers, number\nof hidden neurons at each hidden layer and number of connections between\nlayers. There may be exponential combinations for these architectural\nattributes which may be unmanageable manually, so it requires an algorithm\nwhich can automatically design an optimal architecture with high generalization\nability. Numerous optimization algorithms have been utilized for FNN\narchitecture determination. This paper proposes a new methodology which can\nwork on the estimation of hidden layers and their respective neurons for FNN.\nThis work combines the advantages of Tabu search (TS) and Gradient descent with\nmomentum backpropagation (GDM) training algorithm to demonstrate how Tabu\nsearch can automatically select the best architecture from the populated\narchitectures based on minimum testing error criteria. The proposed approach\nhas been tested on four classification benchmark dataset of different size.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 20:12:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Gupta", "Tarun Kumar", ""], ["Raza", "Khalid", ""]]}, {"id": "1808.06377", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Serkan Kiranyaz, Moncef Gabbouj, Alexandros Iosifidis", "title": "Progressive Operational Perceptron with Memory", "comments": "11 pages, 4 figures, 5 tables, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Operational Perceptron (GOP) was proposed to generalize the\nlinear neuron model in the traditional Multilayer Perceptron (MLP) and this\nmodel can mimic the synaptic connections of the biological neurons that have\nnonlinear neurochemical behaviours. Progressive Operational Perceptron (POP) is\na multilayer network composing of GOPs which is formed layer-wise\nprogressively. In this work, we propose major modifications that can accelerate\nas well as augment the progressive learning procedure of POP by incorporating\nan information-preserving, linear projection path from the input to the output\nlayer at each progressive step. The proposed extensions can be interpreted as a\nmechanism that provides direct information extracted from the previously\nlearned layers to the network, hence the term \"memory\". This allows the network\nto learn deeper architectures with better data representations. An extensive\nset of experiments show that the proposed modifications can surpass the\nlearning capability of the original POPs and other related algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 10:21:45 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 14:26:36 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 07:02:22 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Kiranyaz", "Serkan", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1808.06604", "submitter": "Megan McCracken", "authors": "Megan McCracken", "title": "Artificial Neural Networks in Fluid Dynamics: A Novel Approach to the\n  Navier-Stokes Equations", "comments": "4 pages, 8 figures, PEARC '18: Practice and Experience in Advanced\n  Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": null, "doi": "10.1145/3219104.3229262", "report-no": null, "categories": "cs.NA cs.NE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been used to solve different types of large data related\nproblems in many different fields.This project takes a novel approach to\nsolving the Navier-Stokes Equations for turbulence by training a neural network\nusing Bayesian Cluster and SOM neighbor weighting to map ionospheric velocity\nfields based on 3-dimensional inputs. Parameters used in this problem included\nthe velocity, Reynolds number, Prandtl number, and temperature. In this project\ndata was obtained from Johns-Hopkins University to train the neural network\nusing MATLAB. The neural network was able to map the velocity fields within a\nsixty-seven percent accuracy of the validation data used. Further studies will\nfocus on higher accuracy and solving further non-linear differential equations\nusing convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 14:46:57 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["McCracken", "Megan", ""]]}, {"id": "1808.06661", "submitter": "Bin Wang", "authors": "Bin Wang and Yanan Sun and Bing Xue and Mengjie Zhang", "title": "A Hybrid Differential Evolution Approach to Designing Deep Convolutional\n  Neural Networks for Image Classification", "comments": "Accepted by The Australasian Joint Conference on Artificial\n  Intelligence 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have demonstrated their superiority in\nimage classification, and evolutionary computation (EC) methods have recently\nbeen surging to automatically design the architectures of CNNs to save the\ntedious work of manually designing CNNs. In this paper, a new hybrid\ndifferential evolution (DE) algorithm with a newly added crossover operator is\nproposed to evolve the architectures of CNNs of any lengths, which is named\nDECNN. There are three new ideas in the proposed DECNN method. Firstly, an\nexisting effective encoding scheme is refined to cater for variable-length CNN\narchitectures; Secondly, the new mutation and crossover operators are developed\nfor variable-length DE to optimise the hyperparameters of CNNs; Finally, the\nnew second crossover is introduced to evolve the depth of the CNN\narchitectures. The proposed algorithm is tested on six widely-used benchmark\ndatasets and the results are compared to 12 state-of-the-art methods, which\nshows the proposed method is vigorously competitive to the state-of-the-art\nalgorithms. Furthermore, the proposed method is also compared with a method\nusing particle swarm optimisation with a similar encoding strategy named IPPSO,\nand the proposed DECNN outperforms IPPSO in terms of the accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:24:45 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 01:32:59 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1808.06928", "submitter": "W B Langdon", "authors": "W. B. Langdon", "title": "The Distribution of Reversible Functions is Normal", "comments": "Published as Chapter 11, pp173-187, in Genetic Programming Theory and\n  Practice, 2003, Rick L. Riolo and Bill Worzel (editors)", "journal-ref": null, "doi": "10.1007/978-1-4419-8983-3_11", "report-no": null, "categories": "cs.ET cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of reversible programs tends to a limit as their size\nincreases. For problems with a Hamming distance fitness function the limiting\ndistribution is binomial with an exponentially small chance (but non~zero)\nchance of perfect solution. Sufficiently good reversible circuits are more\ncommon. Expected RMS error is also calculated. Random unitary matrices may\nsuggest possible extension to quantum computing. Using the genetic programming\n(GP) benchmark, the six multiplexor, circuits of Toffoli gates are shown to\ngive a fitness landscape amenable to evolutionary search. Minimal CCNOT\nsolutions to the six multiplexer are found but larger circuits are more\nevolvable.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 13:06:25 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Langdon", "W. B.", ""]]}, {"id": "1808.07390", "submitter": "Kailiang Wu", "authors": "Kailiang Wu, Dongbin Xiu", "title": "An Explicit Neural Network Construction for Piecewise Constant Function\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an explicit construction for feedforward neural network (FNN),\nwhich provides a piecewise constant approximation for multivariate functions.\nThe proposed FNN has two hidden layers, where the weights and thresholds are\nexplicitly defined and do not require numerical optimization for training.\nUnlike most of the existing work on explicit FNN construction, the proposed FNN\ndoes not rely on tensor structure in multiple dimensions. Instead, it\nautomatically creates Voronoi tessellation of the domain, based on the given\ndata of the target function, and piecewise constant approximation of the\nfunction. This makes the construction more practical for applications. We\npresent both theoretical analysis and numerical examples to demonstrate its\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 14:45:46 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wu", "Kailiang", ""], ["Xiu", "Dongbin", ""]]}, {"id": "1808.07530", "submitter": "Ali Athar", "authors": "Ali Athar", "title": "An Overview of Datatype Quantization Techniques for Convolutional Neural\n  Networks", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are becoming increasingly popular due to\ntheir superior performance in the domain of computer vision, in applications\nsuch as objection detection and recognition. However, they demand complex,\npower-consuming hardware which makes them unsuitable for implementation on\nlow-power mobile and embedded devices. In this paper, a description and\ncomparison of various techniques is presented which aim to mitigate this\nproblem. This is primarily achieved by quantizing the floating-point weights\nand activations to reduce the hardware requirements, and adapting the training\nand inference algorithms to maintain the network's performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 19:20:45 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Athar", "Ali", ""]]}, {"id": "1808.07692", "submitter": "Qinbing Fu", "authors": "Qinbing Fu and Nicola Bellotto and Shigang Yue", "title": "A Directionally Selective Neural Network with Separated ON and OFF\n  Pathways for Translational Motion Perception in a Visually Cluttered\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With respect to biological findings underlying fly's physiology in the past\ndecade, we present a directionally selective neural network, with a\nfeed-forward structure and entirely low-level visual processing, so as to\nimplement direction selective neurons in the fly's visual system, which are\nmainly sensitive to wide-field translational movements in four cardinal\ndirections. In this research, we highlight the functionality of ON and OFF\npathways, separating motion information for parallel computation corresponding\nto light-on and light-off selectivity. Through this modeling study, we\ndemonstrate several achievements compared with former bio-plausible\ntranslational motion detectors, like the elementary motion detectors. First, we\nthoroughly mimic the fly's preliminary motion-detecting pathways with newly\nrevealed fly's physiology. Second, we improve the speed response to moving\ndark/light features via the design of ensembles of same polarity cells in the\ndual-pathways. Moreover, we alleviate the impact of irrelevant motion in a\nvisually cluttered environment like the shifting of background and windblown\nvegetation, via the modeling of spatiotemporal dynamics. We systematically\ntested the DSNN against stimuli ranging from synthetic and real-world scenes,\nto notably a visual modality of a ground micro robot. The results demonstrated\nthat the DSNN outperforms former bio-plausible translational motion detectors.\nImportantly, we verified its computational simplicity and effectiveness\nbenefiting the building of neuromorphic vision sensor for robots.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 10:44:35 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Fu", "Qinbing", ""], ["Bellotto", "Nicola", ""], ["Yue", "Shigang", ""]]}, {"id": "1808.08121", "submitter": "Mojtaba Heidarysafa", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Donald E. Brown, Kiana Jafari\n  Meimandi, Laura E. Barnes", "title": "An Improvement of Data Classification Using Random Multimodel Deep\n  Learning (RMDL)", "comments": "published in International Journal of Machine Learning and Computing\n  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890", "journal-ref": null, "doi": "10.18178/ijmlc.2018.8.4.703", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth in the number of complex datasets every year requires\nmore enhancement in machine learning methods to provide robust and accurate\ndata classification. Lately, deep learning approaches have achieved surpassing\nresults in comparison to previous machine learning algorithms. However, finding\nthe suitable structure for these models has been a challenge for researchers.\nThis paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,\ndeep learning approach for classification. RMDL solves the problem of finding\nthe best deep learning structure and architecture while simultaneously\nimproving robustness and accuracy through ensembles of deep learning\narchitectures. In short, RMDL trains multiple randomly generated models of Deep\nNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural\nNetwork (RNN) in parallel and combines their results to produce better result\nof any of those models individually. In this paper, we describe RMDL model and\ncompare the results for image and text classification as well as face\nrecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for\nimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text\nclassification. Lastly, we used ORL dataset to compare the model performance on\nface recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 00:38:14 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Meimandi", "Kiana Jafari", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1808.08173", "submitter": "Daniel Saunders", "authors": "Daniel J. Saunders, Hava T. Siegelmann, Robert Kozma, Mikl\\'os\n  Ruszink\\'o", "title": "STDP Learning of Image Patches with Convolutional Spiking Neural\n  Networks", "comments": "7 pages, 9 figures, and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks are motivated from principles of neural systems and\nmay possess unexplored advantages in the context of machine learning. A class\nof \\textit{convolutional spiking neural networks} is introduced, trained to\ndetect image features with an unsupervised, competitive learning mechanism.\nImage features can be shared within subpopulations of neurons, or each may\nevolve independently to capture different features in different regions of\ninput space. We analyze the time and memory requirements of learning with and\noperating such networks. The MNIST dataset is used as an experimental testbed,\nand comparisons are made between the performance and convergence speed of a\nbaseline spiking neural network.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 15:27:39 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Saunders", "Daniel J.", ""], ["Siegelmann", "Hava T.", ""], ["Kozma", "Robert", ""], ["Ruszink\u00f3", "Mikl\u00f3s", ""]]}, {"id": "1808.08186", "submitter": "Kumar Sankar Ray", "authors": "Rajesh Misra, Kumar S. Ray", "title": "Dual approach for object tracking based on optical flow and swarm\n  intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision,object tracking is a very old and complex problem.Though\nthere are several existing algorithms for object tracking, still there are\nseveral challenges remain to be solved. For instance, variation of illumination\nof light, noise, occlusion, sudden start and stop of moving object, shading\netc,make the object tracking a complex problem not only for dynamic background\nbut also for static background. In this paper we propose a dual approach for\nobject tracking based on optical flow and swarm Intelligence.The optical flow\nbased KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the\ntarget object from first frame to last frame of a video sequence;whereas swarm\nIntelligence based PSO (Particle Swarm Optimization) tracker simultaneously\ntracks the boundary information of the target object from second frame to last\nframe of the same video sequence.This dual function of tracking makes the\ntrackers very much robust with respect to the above stated problems. The\nflexibility of our approach is that it can be successfully applicable in\nvariable background as well as static background.We compare the performance of\nthe proposed dual tracking algorithm with several benchmark datasets and obtain\nvery competitive results in general and in most of the cases we obtained\nsuperior results using dual tracking algorithm. We also compare the performance\nof the proposed dual tracker with some existing PSO based algorithms for\ntracking and achieved better results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 14:17:45 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Misra", "Rajesh", ""], ["Ray", "Kumar S.", ""]]}, {"id": "1808.08347", "submitter": "Risto Miikkulainen", "authors": "Jingbo Jiang, Diego Legrand, Robert Severn, Risto Miikkulainen", "title": "A Comparison of the Taguchi Method and Evolutionary Optimization in\n  Multivariate Testing", "comments": "5 pages, 4 figures, IAAI-19", "journal-ref": "Proceedings of the 2020 IEEE Congress on Evolutionary Computation", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate testing has recently emerged as a promising technique in web\ninterface design. In contrast to the standard A/B testing, multivariate\napproach aims at evaluating a large number of values in a few key variables\nsystematically. The Taguchi method is a practical implementation of this idea,\nfocusing on orthogonal combinations of values. This paper evaluates an\nalternative method: population-based search, i.e. evolutionary optimization.\nIts performance is compared to that of the Taguchi method in several simulated\nconditions, including an orthogonal one designed to favor the Taguchi method,\nand two realistic conditions with dependences between variables. Evolutionary\noptimization is found to perform significantly better especially in the\nrealistic conditions, suggesting that it forms a good approach for web\ninterface design in the future.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 02:45:02 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 22:43:48 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 03:14:35 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jiang", "Jingbo", ""], ["Legrand", "Diego", ""], ["Severn", "Robert", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1808.08444", "submitter": "Shin Yoo Dr", "authors": "Jinhan Kim and Robert Feldt and Shin Yoo", "title": "Guiding Deep Learning System Testing using Surprise Adequacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) systems are rapidly being adopted in safety and security\ncritical domains, urgently calling for ways to test their correctness and\nrobustness. Testing of DL systems has traditionally relied on manual collection\nand labelling of data. Recently, a number of coverage criteria based on neuron\nactivation values have been proposed. These criteria essentially count the\nnumber of neurons whose activation during the execution of a DL system\nsatisfied certain properties, such as being above predefined thresholds.\nHowever, existing coverage criteria are not sufficiently fine grained to\ncapture subtle behaviours exhibited by DL systems. Moreover, evaluations have\nfocused on showing correlation between adversarial examples and proposed\ncriteria rather than evaluating and guiding their use for actual testing of DL\nsystems. We propose a novel test adequacy criterion for testing of DL systems,\ncalled Surprise Adequacy for Deep Learning Systems (SADL), which is based on\nthe behaviour of DL systems with respect to their training data. We measure the\nsurprise of an input as the difference in DL system's behaviour between the\ninput and the training data (i.e., what was learnt during training), and\nsubsequently develop this as an adequacy criterion: a good test input should be\nsufficiently but not overtly surprising compared to training data. Empirical\nevaluation using a range of DL systems from simple image classifiers to\nautonomous driving car platforms shows that systematic sampling of inputs based\non their surprise can improve classification accuracy of DL systems against\nadversarial examples by up to 77.5% via retraining.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 15:55:59 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Kim", "Jinhan", ""], ["Feldt", "Robert", ""], ["Yoo", "Shin", ""]]}, {"id": "1808.08517", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Witold Pedrycz, Geoffrey I. Webb", "title": "An Incremental Construction of Deep Neuro Fuzzy System for Continual\n  Learning of Non-stationary Data Streams", "comments": "This paper has been published in IEEE Transactions on Fuzzy Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing FNNs are mostly developed under a shallow network configuration\nhaving lower generalization power than those of deep structures. This paper\nproposes a novel self-organizing deep FNN, namely DEVFNN. Fuzzy rules can be\nautomatically extracted from data streams or removed if they play limited role\nduring their lifespan. The structure of the network can be deepened on demand\nby stacking additional layers using a drift detection method which not only\ndetects the covariate drift, variations of input space, but also accurately\nidentifies the real drift, dynamic changes of both feature space and target\nspace. DEVFNN is developed under the stacked generalization principle via the\nfeature augmentation concept where a recently developed algorithm, namely\ngClass, drives the hidden layer. It is equipped by an automatic feature\nselection method which controls activation and deactivation of input attributes\nto induce varying subsets of input features. A deep network simplification\nprocedure is put forward using the concept of hidden layer merging to prevent\nuncontrollable growth of dimensionality of input space due to the nature of\nfeature augmentation approach in building a deep network structure. DEVFNN\nworks in the sample-wise fashion and is compatible for data stream\napplications. The efficacy of DEVFNN has been thoroughly evaluated using seven\ndatasets with non-stationary properties under the prequential test-then-train\nprotocol. It has been compared with four popular continual learning algorithms\nand its shallow counterpart where DEVFNN demonstrates improvement of\nclassification accuracy. Moreover, it is also shown that the concept drift\ndetection method is an effective tool to control the depth of network structure\nwhile the hidden layer merging scenario is capable of simplifying the network\ncomplexity of a deep network with negligible compromise of generalization\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 08:10:13 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 03:14:33 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Pedrycz", "Witold", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1808.08777", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura, Toshihide Harada", "title": "Adaptive Structural Learning of Deep Belief Network for Medical\n  Examination Data and Its Knowledge Extraction by using C4.5", "comments": "8 pages, 7 figures, The First IEEE International Conference on\n  Artificial Intelligence and Knowledge Engineering (AIKE2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has a hierarchical network architecture to represent the\ncomplicated feature of input patterns. The adaptive structural learning method\nof Deep Belief Network (DBN) has been developed. The method can discover an\noptimal number of hidden neurons for given input data in a Restricted Boltzmann\nMachine (RBM) by neuron generation-annihilation algorithm, and generate a new\nhidden layer in DBN by the extension of the algorithm. In this paper, the\nproposed adaptive structural learning of DBN was applied to the comprehensive\nmedical examination data for the cancer prediction. The prediction system shows\nhigher classification accuracy (99.8% for training and 95.5% for test) than the\ntraditional DBN. Moreover, the explicit knowledge with respect to the relation\nbetween input and output patterns was extracted from the trained DBN network by\nC4.5. Some characteristics extracted in the form of IF-THEN rules to find an\ninitial cancer at the early stage were reported in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:47:31 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""], ["Harada", "Toshihide", ""]]}, {"id": "1808.08798", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues and Francisco C. Pereira", "title": "Beyond expectation: Deep joint mean and quantile regression for\n  spatio-temporal problems", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal problems are ubiquitous and of vital importance in many\nresearch fields. Despite the potential already demonstrated by deep learning\nmethods in modeling spatio-temporal data, typical approaches tend to focus\nsolely on conditional expectations of the output variables being modeled. In\nthis paper, we propose a multi-output multi-quantile deep learning approach for\njointly modeling several conditional quantiles together with the conditional\nexpectation as a way to provide a more complete \"picture\" of the predictive\ndensity in spatio-temporal problems. Using two large-scale datasets from the\ntransportation domain, we empirically demonstrate that, by approaching the\nquantile regression problem from a multi-task learning perspective, it is\npossible to solve the embarrassing quantile crossings problem, while\nsimultaneously significantly outperforming state-of-the-art quantile regression\nmethods. Moreover, we show that jointly modeling the mean and several\nconditional quantiles not only provides a rich description about the predictive\ndensity that can capture heteroscedastic properties at a neglectable\ncomputational overhead, but also leads to improved predictions of the\nconditional expectation due to the extra information and a regularization\neffect induced by the added quantiles.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 11:49:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Pereira", "Francisco C.", ""]]}, {"id": "1808.08818", "submitter": "J\\\"org Stork", "authors": "J\\\"org Stork and A.E. Eiben and Thomas Bartz-Beielstein", "title": "A new Taxonomy of Continuous Global Optimization Algorithms", "comments": "35 pages total, 28 written pages, 4 figures, 2019 Reworked Version", "journal-ref": "Natural Computing, 2020, 1-24", "doi": "10.1007/s11047-020-09820-4", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate-based optimization, nature-inspired metaheuristics, and hybrid\ncombinations have become state of the art in algorithm design for solving\nreal-world optimization problems. Still, it is difficult for practitioners to\nget an overview that explains their advantages in comparison to a large number\nof available methods in the scope of optimization. Available taxonomies lack\nthe embedding of current approaches in the larger context of this broad field.\nThis article presents a taxonomy of the field, which explores and matches\nalgorithm strategies by extracting similarities and differences in their search\nstrategies. A particular focus lies on algorithms using surrogates,\nnature-inspired designs, and those created by design optimization. The\nextracted features of components or operators allow us to create a set of\nclassification indicators to distinguish between a small number of classes. The\nfeatures allow a deeper understanding of components of the search strategies\nand further indicate the close connections between the different algorithm\ndesigns. We present intuitive analogies to explain the basic principles of the\nsearch algorithms, particularly useful for novices in this research field.\nFurthermore, this taxonomy allows recommendations for the applicability of the\ncorresponding algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 12:35:44 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 12:03:58 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Stork", "J\u00f6rg", ""], ["Eiben", "A. E.", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "1808.09058", "submitter": "Adenilton Jos\\'e da Silva", "authors": "Priscila G. M. dos Santos, Rodrigo S. Sousa, Ismael C. S. Araujo and\n  Adenilton J. da Silva", "title": "Quantum enhanced cross-validation for near-optimal neural networks\n  architecture selection", "comments": null, "journal-ref": "International Journal of Quantum Information, Volume 16, No. 06,\n  1840005 (2018)", "doi": "10.1142/S0219749918400051", "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a quantum-classical algorithm to evaluate and select\nclassical artificial neural networks architectures. The proposed algorithm is\nbased on a probabilistic quantum memory and the possibility to train artificial\nneural networks in superposition. We obtain an exponential quantum speedup in\nthe evaluation of neural networks. We also verify experimentally through a\nreduced experimental analysis that the proposed algorithm can be used to select\nnear-optimal neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 23:04:22 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Santos", "Priscila G. M. dos", ""], ["Sousa", "Rodrigo S.", ""], ["Araujo", "Ismael C. S.", ""], ["da Silva", "Adenilton J.", ""]]}, {"id": "1808.10394", "submitter": "Dejan Brkic", "authors": "Pavel Praks and Dejan Brkic", "title": "Symbolic regression based genetic approximations of the Colebrook\n  equation for flow friction", "comments": "12 pages, 7 figures, 65 references", "journal-ref": "Praks, P.; Brki\\'c, D. Symbolic Regression-Based Genetic\n  Approximations of the Colebrook Equation for Flow Friction. Water 2018,\n  10(9), 1175", "doi": "10.3390/w10091175", "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used in hydraulics, the Colebrook equation for flow friction relates\nimplicitly to the input parameters; the Reynolds number, and the relative\nroughness of inner pipe surface, with the output unknown parameter; the flow\nfriction factor. In this paper, a few explicit approximations to the Colebrook\nequation are generated using the ability of artificial intelligence to make\ninner patterns to connect input and output parameters in explicit way not\nknowing their nature or the physical law that connects them, but only knowing\nraw numbers. The fact that the used genetic programming tool does not know the\nstructure of the Colebrook equation which is based on computationally expensive\nlogarithmic law, is used to obtain better structure of the approximations which\nis less demanding for calculation but also enough accurate. All generated\napproximations are with low computational cost because they contain a limited\nnumber of logarithmic forms used although for normalization of input parameters\nor for acceleration, but they are also sufficiently accurate. The relative\nerror regarding the friction factor in best case is up to 0.13% with only two\nlogarithmic forms used. As the second logarithm can be accurately approximated\nby the Pade approximation, practically the same error is obtained also using\nonly one logarithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:37:56 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Praks", "Pavel", ""], ["Brkic", "Dejan", ""]]}, {"id": "1808.10733", "submitter": "Stefano Nichele", "authors": "Bartosz Gembala, Anis Yazidi, H{\\aa}rek Haugerud, Stefano Nichele", "title": "Autonomous Configuration of Network Parameters in Operating Systems\n  using Evolutionary Algorithms", "comments": "ACM RACS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By default, the Linux network stack is not configured for highspeed large\nfile transfer. The reason behind this is to save memory resources. It is\npossible to tune the Linux network stack by increasing the network buffers size\nfor high-speed networks that connect server systems in order to handle more\nnetwork packets. However, there are also several other TCP/IP parameters that\ncan be tuned in an Operating System (OS). In this paper, we leverage Genetic\nAlgorithms (GAs) to devise a system which learns from the history of the\nnetwork traffic and uses this knowledge to optimize the current performance by\nadjusting the parameters. This can be done for a standard Linux kernel using\nsysctl or /proc. For a Virtual Machine (VM), virtually any type of OS can be\ninstalled and an image can swiftly be compiled and deployed. By being a\nsandboxed environment, risky configurations can be tested without the danger of\nharming the system. Different scenarios for network parameter configurations\nare thoroughly tested, and an increase of up to 65% throughput speed is\nachieved compared to the default Linux configuration.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 13:25:40 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Gembala", "Bartosz", ""], ["Yazidi", "Anis", ""], ["Haugerud", "H\u00e5rek", ""], ["Nichele", "Stefano", ""]]}, {"id": "1808.10866", "submitter": "Luis Meira", "authors": "Felipe F. M\\\"uller and Luis A. A. Meira", "title": "Algoritmos Gen\\'eticos Aplicado ao Problema de Roteamento de Ve\\'iculos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routing problems are often faced by companies who serve costumers through\nvehicles. Such problems have a challenging structure to optimize, despite the\nrecent advances in combinatorial optimization. The goal of this project is to\nstudy and propose optimization algorithms to the vehicle routing problems\n(VRP). Focus will be on the problem variant in which the length of the route is\nrestricted by a constant. A real problem will be tackled: optimization of\npostmen routes. Such problem was modeled as {multi-objective} in a roadmap with\n25 vehicles and {30,000 deliveries} per day.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:48:53 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["M\u00fcller", "Felipe F.", ""], ["Meira", "Luis A. A.", ""]]}]