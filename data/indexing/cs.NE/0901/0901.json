[{"id": "0901.0317", "submitter": "Janardan Misra", "authors": "Janardan Misra", "title": "Design of a P System based Artificial Graph Chemistry", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Chemistries (ACs) are symbolic chemical metaphors for the\nexploration of Artificial Life, with specific focus on the origin of life. In\nthis work we define a P system based artificial graph chemistry to understand\nthe principles leading to the evolution of life-like structures in an AC set up\nand to develop a unified framework to characterize and classify symbolic\nartificial chemistries by devising appropriate formalism to capture semantic\nand organizational information. An extension of P system is considered by\nassociating probabilities with the rules providing the topological framework\nfor the evolution of a labeled undirected graph based molecular reaction\nsemantics.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2009 17:35:49 GMT"}], "update_date": "2009-01-06", "authors_parsed": [["Misra", "Janardan", ""]]}, {"id": "0901.0597", "submitter": "Reza Rastegar", "authors": "Reza Rastegar", "title": "On the Optimal Convergence Probability of Univariate Estimation of\n  Distribution Algorithms", "comments": "evolutionary computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain bounds on the probability of convergence to the\noptimal solution for the compact Genetic Algorithm (cGA) and the Population\nBased Incremental Learning (PBIL). We also give a sufficient condition for\nconvergence of these algorithms to the optimal solution and compute a range of\npossible values of the parameters of these algorithms for which they converge\nto the optimal solution with a confidence level.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2009 06:36:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2009 18:27:38 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2009 13:34:45 GMT"}, {"version": "v4", "created": "Mon, 13 Sep 2010 12:40:44 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Rastegar", "Reza", ""]]}, {"id": "0901.0598", "submitter": "Reza Rastegar", "authors": "Reza Rastegar, Arash Hariri", "title": "A Step Forward in Studying the Compact Genetic Algorithm", "comments": "13 Pages", "journal-ref": "Evolutionary Computation (2006),Vol 14, No 3, 277-290", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compact Genetic Algorithm (cGA) is an Estimation of Distribution\nAlgorithm that generates offspring population according to the estimated\nprobabilistic model of the parent population instead of using traditional\nrecombination and mutation operators. The cGA only needs a small amount of\nmemory; therefore, it may be quite useful in memory-constrained applications.\nThis paper introduces a theoretical framework for studying the cGA from the\nconvergence point of view in which, we model the cGA by a Markov process and\napproximate its behavior using an Ordinary Differential Equation (ODE). Then,\nwe prove that the corresponding ODE converges to local optima and stays there.\nConsequently, we conclude that the cGA will converge to the local optima of the\nfunction to be optimized.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2009 07:07:00 GMT"}], "update_date": "2009-01-07", "authors_parsed": [["Rastegar", "Reza", ""], ["Hariri", "Arash", ""]]}, {"id": "0901.1144", "submitter": "Arturo Berrones", "authors": "Arturo Berrones", "title": "Bayesian Inference Based on Stationary Fokker-Planck Sampling", "comments": "Accepted in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel formalism for Bayesian learning in the context of complex inference\nmodels is proposed. The method is based on the use of the Stationary\nFokker--Planck (SFP) approach to sample from the posterior density. Stationary\nFokker--Planck sampling generalizes the Gibbs sampler algorithm for arbitrary\nand unknown conditional densities. By the SFP procedure approximate analytical\nexpressions for the conditionals and marginals of the posterior can be\nconstructed. At each stage of SFP, the approximate conditionals are used to\ndefine a Gibbs sampling process, which is convergent to the full joint\nposterior. By the analytical marginals efficient learning methods in the\ncontext of Artificial Neural Networks are outlined. Off--line and incremental\nBayesian inference and Maximum Likelihood Estimation from the posterior is\nperformed in classification and regression examples. A comparison of SFP with\nother Monte Carlo strategies in the general problem of sampling from arbitrary\ndensities is also presented. It is shown that SFP is able to jump large\nlow--probabilty regions without the need of a careful tuning of any step size\nparameter. In fact, the SFP method requires only a small set of meaningful\nparameters which can be selected following clear, problem--independent\nguidelines. The computation cost of SFP, measured in terms of loss function\nevaluations, grows linearly with the given model's dimension.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2009 22:09:02 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2009 15:40:00 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2009 01:02:27 GMT"}], "update_date": "2009-10-26", "authors_parsed": [["Berrones", "Arturo", ""]]}, {"id": "0901.1152", "submitter": "Victor Eliashberg", "authors": "Victor Eliashberg", "title": "A nonclassical symbolic theory of working memory, mental computations,\n  and mental set", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper tackles four basic questions associated with human brain as a\nlearning system. How can the brain learn to (1) mentally simulate different\nexternal memory aids, (2) perform, in principle, any mental computations using\nimaginary memory aids, (3) recall the real sensory and motor events and\nsynthesize a combinatorial number of imaginary events, (4) dynamically change\nits mental set to match a combinatorial number of contexts? We propose a\nuniform answer to (1)-(4) based on the general postulate that the human\nneocortex processes symbolic information in a \"nonclassical\" way. Instead of\nmanipulating symbols in a read/write memory, as the classical symbolic systems\ndo, it manipulates the states of dynamical memory representing different\ntemporary attributes of immovable symbolic structures stored in a long-term\nmemory. The approach is formalized as the concept of E-machine. Intuitively, an\nE-machine is a system that deals mainly with characteristic functions\nrepresenting subsets of memory pointers rather than the pointers themselves.\nThis nonclassical symbolic paradigm is Turing universal, and, unlike the\nclassical one, is efficiently implementable in homogeneous neural networks with\ntemporal modulation topologically resembling that of the neocortex.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2009 23:42:45 GMT"}], "update_date": "2009-01-12", "authors_parsed": [["Eliashberg", "Victor", ""]]}, {"id": "0901.1610", "submitter": "Janardan Misra", "authors": "Janardan Misra", "title": "Towards a Framework for Observing Artificial Evolutionary Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing the emergence of evolutionary behavior as a defining\ncharacteristic of 'life' is a major step in the Artificial life (ALife)\nstudies. We present here an abstract formal framework for this aim based upon\nthe notion of high-level observations made on the ALife model at hand during\nits simulations. An observation process is defined as a computable\ntransformation from the underlying dynamic structure of the model universe to a\ntuple consisting of abstract components needed to establish the evolutionary\nprocesses in the model. Starting with defining entities and their evolutionary\nrelationships observed during the simulations of the model, the framework\nprescribes a series of definitions, followed by the axioms (conditions) that\nmust be met in order to establish the level of evolutionary behavior in the\nmodel. The examples of Cellular Automata based Langton Loops and Lambda\ncalculus based Algorithmic Chemistry are used to illustrate the framework.\nGeneric design suggestions for the ALife research are also drawn based upon the\nframework design and case study analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2009 16:35:58 GMT"}], "update_date": "2009-01-13", "authors_parsed": [["Misra", "Janardan", ""]]}]