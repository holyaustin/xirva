[{"id": "1503.00036", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "Norm-Based Capacity Control in Neural Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capacity, convexity and characterization of a general\nfamily of norm-constrained feed-forward networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 23:50:22 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 22:55:08 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1503.00107", "submitter": "Shujian Huang", "authors": "Shujian Huang and Huadong Chen and Xinyu Dai and Jiajun Chen", "title": "Non-linear Learning for Statistical Machine Translation", "comments": "submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical machine translation (SMT) systems usually use a linear\ncombination of features to model the quality of each translation hypothesis.\nThe linear combination assumes that all the features are in a linear\nrelationship and constrains that each feature interacts with the rest features\nin an linear manner, which might limit the expressive power of the model and\nlead to a under-fit model on the current data. In this paper, we propose a\nnon-linear modeling for the quality of translation hypotheses based on neural\nnetworks, which allows more complex interaction between features. A learning\nframework is presented for training the non-linear models. We also discuss\npossible heuristics in designing the network structure which may improve the\nnon-linear learning performance. Experimental results show that with the basic\nfeatures of a hierarchical phrase-based machine translation system, our method\nproduce translations that are better than a linear model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 09:53:32 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Huang", "Shujian", ""], ["Chen", "Huadong", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1503.00504", "submitter": "Chetan Singh Thakur", "authors": "Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Richard F.\n  Lyon, Andr\\'e van Schaik", "title": "FPGA Implementation of the CAR Model of the Cochlea", "comments": "ISCAS-2014", "journal-ref": null, "doi": "10.1109/ISCAS.2014.6865519", "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The front end of the human auditory system, the cochlea, converts sound\nsignals from the outside world into neural impulses transmitted along the\nauditory pathway for further processing. The cochlea senses and separates sound\nin a nonlinear active fashion, exhibiting remarkable sensitivity and frequency\ndiscrimination. Although several electronic models of the cochlea have been\nproposed and implemented, none of these are able to reproduce all the\ncharacteristics of the cochlea, including large dynamic range, large gain and\nsharp tuning at low sound levels, and low gain and broad tuning at intense\nsound levels. Here, we implement the Cascade of Asymmetric Resonators (CAR)\nmodel of the cochlea on an FPGA. CAR represents the basilar membrane filter in\nthe Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC)\ncochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zero\nfilter cascade model of auditory filtering. It uses simple nonlinear extensions\nof conventional digital filter stages that are well suited to FPGA\nimplementations, so that we are able to implement up to 1224 cochlear sections\non Virtex-6 FPGA to process sound data in real time. The FPGA implementation of\nthe electronic cochlea described here may be used as a front-end sound analyser\nfor various machine-hearing applications.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 12:55:12 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Thakur", "Chetan Singh", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["Lyon", "Richard F.", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1503.00505", "submitter": "Chetan Singh Thakur", "authors": "Chetan Singh Thakur, Tara Julia Hamilton, Runchun Wang, Jonathan\n  Tapson and Andr\\'e van Schaik", "title": "A neuromorphic hardware framework based on population coding", "comments": "In submission to IJCNN2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the biological nervous system, large neuronal populations work\ncollaboratively to encode sensory stimuli. These neuronal populations are\ncharacterised by a diverse distribution of tuning curves, ensuring that the\nentire range of input stimuli is encoded. Based on these principles, we have\ndesigned a neuromorphic system called a Trainable Analogue Block (TAB), which\nencodes given input stimuli using a large population of neurons with a\nheterogeneous tuning curve profile. Heterogeneity of tuning curves is achieved\nusing random device mismatches in VLSI (Very Large Scale Integration) process\nand by adding a systematic offset to each hidden neuron. Here, we present\nmeasurement results of a single test cell fabricated in a 65nm technology to\nverify the TAB framework. We have mimicked a large population of neurons by\nre-using measurement results from the test cell by varying offset. We thus\ndemonstrate the learning capability of the system for various regression tasks.\nThe TAB system may pave the way to improve the design of analogue circuits for\ncommercial applications, by rendering circuits insensitive to random mismatch\nthat arises due to the manufacturing process.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 12:55:54 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Thakur", "Chetan Singh", ""], ["Hamilton", "Tara Julia", ""], ["Wang", "Runchun", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1503.00669", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Tao Hu, Dmitri B. Chklovskii", "title": "A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A\n  Derivation from Multidimensional Scaling of Streaming Data", "comments": "Accepted for publication in Neural Computation", "journal-ref": null, "doi": "10.1162/NECO_a_00745", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models of early sensory processing typically reduce the\ndimensionality of streaming input data. Such networks learn the principal\nsubspace, in the sense of principal component analysis (PCA), by adjusting\nsynaptic weights according to activity-dependent learning rules. When derived\nfrom a principled cost function these rules are nonlocal and hence biologically\nimplausible. At the same time, biologically plausible local rules have been\npostulated rather than derived from a principled cost function. Here, to bridge\nthis gap, we derive a biologically plausible network for subspace learning on\nstreaming data by minimizing a principled cost function. In a departure from\nprevious work, where cost was quantified by the representation, or\nreconstruction, error, we adopt a multidimensional scaling (MDS) cost function\nfor streaming data. The resulting algorithm relies only on biologically\nplausible Hebbian and anti-Hebbian local learning rules. In a stochastic\nsetting, synaptic weights converge to a stationary state which projects the\ninput data onto the principal subspace. If the data are generated by a\nnonstationary distribution, the network can track the principal subspace. Thus,\nour result makes a step towards an algorithmic theory of neural computation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 19:39:33 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Hu", "Tao", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1503.00680", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix\n  Factorization Can Cluster and Discover Sparse Features", "comments": "2014 Asilomar Conference on Signals, Systems and Computers", "journal-ref": null, "doi": "10.1109/ACSSC.2014.7094553", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite our extensive knowledge of biophysical properties of neurons, there\nis no commonly accepted algorithmic theory of neuronal function. Here we\nexplore the hypothesis that single-layer neuronal networks perform online\nsymmetric nonnegative matrix factorization (SNMF) of the similarity matrix of\nthe streamed data. By starting with the SNMF cost function we derive an online\nalgorithm, which can be implemented by a biologically plausible network with\nlocal learning rules. We demonstrate that such network performs soft clustering\nof the data as well as sparse feature discovery. The derived algorithm\nreplicates many known aspects of sensory anatomy and biophysical properties of\nneurons including unipolar nature of neuronal activity and synaptic weights,\nlocal synaptic plasticity rules and the dependence of learning rate on\ncumulative neuronal activity. Thus, we make a step towards an algorithmic\ntheory of neuronal function, which should facilitate large-scale neural circuit\nsimulations and biologically inspired artificial intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 19:57:28 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1503.00690", "submitter": "Cengiz Pehlevan", "authors": "Tao Hu, Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "A Hebbian/Anti-Hebbian Network for Online Sparse Dictionary Learning\n  Derived from Symmetric Matrix Factorization", "comments": "2014 Asilomar Conference on Signals, Systems and Computers. v2: fixed\n  a typo in equation 23", "journal-ref": null, "doi": "10.1109/ACSSC.2014.7094519", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Olshausen and Field (OF) proposed that neural computations in the primary\nvisual cortex (V1) can be partially modeled by sparse dictionary learning. By\nminimizing the regularized representation error they derived an online\nalgorithm, which learns Gabor-filter receptive fields from a natural image\nensemble in agreement with physiological experiments. Whereas the OF algorithm\ncan be mapped onto the dynamics and synaptic plasticity in a single-layer\nneural network, the derived learning rule is nonlocal - the synaptic weight\nupdate depends on the activity of neurons other than just pre- and postsynaptic\nones - and hence biologically implausible. Here, to overcome this problem, we\nderive sparse dictionary learning from a novel cost-function - a regularized\nerror of the symmetric factorization of the input's similarity matrix. Our\nalgorithm maps onto a neural network of the same architecture as OF but using\nonly biologically plausible local learning rules. When trained on natural\nimages our network learns Gabor-filter receptive fields and reproduces the\ncorrelation among synaptic weights hard-wired in the OF network. Therefore,\nonline symmetric matrix factorization may serve as an algorithmic theory of\nneural computation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:16:19 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 17:09:03 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Hu", "Tao", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1503.00778", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra", "title": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a basic task in many fields including signal processing,\nneuroscience and machine learning where the goal is to learn a basis that\nenables a sparse representation of a given set of data, if one exists. Its\nstandard formulation is as a non-convex optimization problem which is solved in\npractice by heuristics based on alternating minimization. Re- cent work has\nresulted in several algorithms for sparse coding with provable guarantees, but\nsomewhat surprisingly these are outperformed by the simple alternating\nminimization heuristics. Here we give a general framework for understanding\nalternating minimization which we leverage to analyze existing heuristics and\nto design new ones also with provable guarantees. Some of these algorithms seem\nimplementable on simple neural architectures, which was the original motivation\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\nfirst efficient algorithm for sparse coding that works almost up to the\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\nprevious algorithms that approached or surpassed this limit run in time\nexponential in some natural parameter. Finally, our algorithms improve upon the\nsample complexity of existing approaches. We believe that our analysis\nframework will have applications in other settings where simple iterative\nalgorithms are used.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 23:02:56 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1503.01007", "submitter": "Armand Joulin", "authors": "Armand Joulin, Tomas Mikolov", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent achievements in machine learning, we are still very far\nfrom achieving real artificial intelligence. In this paper, we discuss the\nlimitations of standard deep learning approaches and show that some of these\nlimitations can be overcome by learning how to grow the complexity of a model\nin a structured way. Specifically, we study the simplest sequence prediction\nproblems that are beyond the scope of what is learnable with standard recurrent\nnetworks, algorithmically generated sequences which can only be learned by\nmodels which have the capacity to count and to memorize sequences. We show that\nsome basic algorithms can be learned from sequential data using a recurrent\nnetwork associated with a trainable memory.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:50:28 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 22:41:39 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 19:23:44 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 20:37:55 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1503.01445", "submitter": "Thomas Unterthiner", "authors": "Thomas Unterthiner, Andreas Mayr, G\\\"unter Klambauer, Sepp Hochreiter", "title": "Toxicity Prediction using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyday we are exposed to various chemicals via food additives, cleaning and\ncosmetic products and medicines -- and some of them might be toxic. However\ntesting the toxicity of all existing compounds by biological experiments is\nneither financially nor logistically feasible. Therefore the government\nagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the\n\"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge\nwas to assess the performance of computational methods in predicting the\ntoxicity of chemical compounds. State of the art toxicity prediction methods\nbuild upon specifically-designed chemical descriptors developed over decades.\nThough Deep Learning is new to the field and was never applied to toxicity\nprediction before, it clearly outperformed all other participating methods. In\nthis application paper we show that deep nets automatically learn features\nresembling well-established toxicophores. In total, our Deep Learning approach\nwon both of the panel-challenges (nuclear receptors and stress response) as\nwell as the overall Grand Challenge, and thereby sets a new standard in tox\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:18:55 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Unterthiner", "Thomas", ""], ["Mayr", "Andreas", ""], ["Klambauer", "G\u00fcnter", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1503.01524", "submitter": "Casey Handmer", "authors": "Casey J. Handmer", "title": "Genetic optimization of the Hyperloop route through the Grapevine", "comments": "16 pages, 4 figures, 1 Mathematica notebook attachment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a genetic algorithm that employs a versatile fitness function\nto optimize route selection for the Hyperloop, a proposed high speed passenger\ntransportation system.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 03:29:16 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Handmer", "Casey J.", ""]]}, {"id": "1503.01824", "submitter": "MinYoung Kim", "authors": "Minyoung Kim, Luca Rigazio", "title": "Deep Clustered Convolutional Kernels", "comments": "draft", "journal-ref": "JMLR: Workshop and Conference Proceedings 44 (2015) 160-172", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved state of the art performance\nthanks to new training algorithms for rapid parameter estimation and new\nregularization methods to reduce overfitting. However, in practice the network\narchitecture has to be manually set by domain experts, generally by a costly\ntrial and error procedure, which often accounts for a large portion of the\nfinal system performance. We view this as a limitation and propose a novel\ntraining algorithm that automatically optimizes network architecture, by\nprogressively increasing model complexity and then eliminating model redundancy\nby selectively removing parameters at training time. For convolutional neural\nnetworks, our method relies on iterative split/merge clustering of\nconvolutional kernels interleaved by stochastic gradient descent. We present a\ntraining algorithm and experimental results on three different vision tasks,\nshowing improved performance compared to similarly sized hand-crafted\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 00:53:40 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Kim", "Minyoung", ""], ["Rigazio", "Luca", ""]]}, {"id": "1503.01838", "submitter": "Fandong Meng", "authors": "Fandong Meng and Zhengdong Lu and Mingxuan Wang and Hang Li and Wenbin\n  Jiang and Qun Liu", "title": "Encoding Source Language with Convolutional Neural Network for Machine\n  Translation", "comments": "Accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed neural network joint model (NNJM) (Devlin et al., 2014)\naugments the n-gram target language model with a heuristically chosen source\ncontext window, achieving state-of-the-art performance in SMT. In this paper,\nwe give a more systematic treatment by summarizing the relevant source\ninformation through a convolutional architecture guided by the target\ninformation. With different guiding signals during decoding, our specifically\ndesigned convolution+gating architectures can pinpoint the parts of a source\nsentence that are relevant to predicting a target word, and fuse them with the\ncontext of entire source sentence to form a unified representation. This\nrepresentation, together with target language words, are fed to a deep neural\nnetwork (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English\ntranslation tasks show that the proposed model can achieve significant\nimprovements over the previous NNJM by up to +1.08 BLEU points on average\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 03:04:54 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 08:28:32 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 01:34:58 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 10:07:40 GMT"}, {"version": "v5", "created": "Mon, 8 Jun 2015 09:04:14 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Meng", "Fandong", ""], ["Lu", "Zhengdong", ""], ["Wang", "Mingxuan", ""], ["Li", "Hang", ""], ["Jiang", "Wenbin", ""], ["Liu", "Qun", ""]]}, {"id": "1503.01847", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "V. Sree Hari Rao, M. Naresh Kumar", "title": "Estimation of the parameters of an infectious disease model using neural\n  networks", "comments": "17 pages, 11 figures", "journal-ref": "Nonlinear Analysis: Real World Applications 11(2010) 1810-1818", "doi": "10.1016/j.nonrwa.2009.04.006", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a realistic mathematical model taking into account\nthe mutual interference among the interacting populations. This model attempts\nto describe the control (vaccination) function as a function of the number of\ninfective individuals, which is an improvement over the existing susceptible\ninfective epidemic models. Regarding the growth of the epidemic as a nonlinear\nphenomenon we have developed a neural network architecture to estimate the\nvital parameters associated with this model. This architecture is based on a\nrecently developed new class of neural networks known as co-operative and\nsupportive neural networks. The application of this architecture to the present\nstudy involves preprocessing of the input data, and this renders an efficient\nestimation of the rate of spread of the epidemic. It is observed that the\nproposed new neural network outperforms a simple feed-forward neural network\nand polynomial regression.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 04:48:00 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Rao", "V. Sree Hari", ""], ["Kumar", "M. Naresh", ""]]}, {"id": "1503.01919", "submitter": "S{\\o}ren S{\\o}nderby", "authors": "S{\\o}ren Kaae S{\\o}nderby, Casper Kaae S{\\o}nderby, Henrik Nielsen,\n  Ole Winther", "title": "Convolutional LSTM Networks for Subcellular Localization of Proteins", "comments": null, "journal-ref": "Algorithms for Computational Biology 9199 (2015) 68", "doi": "10.1007/978-3-319-21233-3_6", "report-no": null, "categories": "q-bio.QM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is widely used to analyze biological sequence data.\nNon-sequential models such as SVMs or feed-forward neural networks are often\nused although they have no natural way of handling sequences of varying length.\nRecurrent neural networks such as the long short term memory (LSTM) model on\nthe other hand are designed to handle sequences. In this study we demonstrate\nthat LSTM networks predict the subcellular location of proteins given only the\nprotein sequence with high accuracy (0.902) outperforming current state of the\nart algorithms. We further improve the performance by introducing convolutional\nfilters and experiment with an attention mechanism which lets the LSTM focus on\nspecific parts of the protein. Lastly we introduce new visualizations of both\nthe convolutional filters and the attention mechanisms and show how they can be\nused to extract biological relevant knowledge from the LSTM networks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 11:21:26 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["S\u00f8nderby", "Casper Kaae", ""], ["Nielsen", "Henrik", ""], ["Winther", "Ole", ""]]}, {"id": "1503.01954", "submitter": "Malte Probst", "authors": "Malte Probst", "title": "Denoising Autoencoders for fast Combinatorial Black Box Optimization", "comments": "corrected typos and small inconsistencies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of Distribution Algorithms (EDAs) require flexible probability\nmodels that can be efficiently learned and sampled. Autoencoders (AE) are\ngenerative stochastic networks with these desired properties. We integrate a\nspecial type of AE, the Denoising Autoencoder (DAE), into an EDA and evaluate\nthe performance of DAE-EDA on several combinatorial optimization problems with\na single objective. We asses the number of fitness evaluations as well as the\nrequired CPU times. We compare the results to the performance to the Bayesian\nOptimization Algorithm (BOA) and RBM-EDA, another EDA which is based on a\ngenerative neural network which has proven competitive with BOA. For the\nconsidered problem instances, DAE-EDA is considerably faster than BOA and\nRBM-EDA, sometimes by orders of magnitude. The number of fitness evaluations is\nhigher than for BOA, but competitive with RBM-EDA. These results show that DAEs\ncan be useful tools for problems with low but non-negligible fitness evaluation\ncosts.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 13:47:58 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 13:55:34 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Probst", "Malte", ""]]}, {"id": "1503.02031", "submitter": "Vivek Kulkarni", "authors": "Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, Oliver Williams", "title": "To Drop or Not to Drop: Robustness, Consistency and Differential Privacy\n  Properties of Dropout", "comments": "Currently under review for ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep belief networks (DBNs) requires optimizing a non-convex\nfunction with an extremely large number of parameters. Naturally, existing\ngradient descent (GD) based methods are prone to arbitrarily poor local minima.\nIn this paper, we rigorously show that such local minima can be avoided (upto\nan approximation error) by using the dropout technique, a widely used heuristic\nin this domain. In particular, we show that by randomly dropping a few nodes of\na one-hidden layer neural network, the training objective function, up to a\ncertain approximation error, decreases by a multiplicative factor.\n  On the flip side, we show that for training convex empirical risk minimizers\n(ERM), dropout in fact acts as a \"stabilizer\" or regularizer. That is, a simple\ndropout based GD method for convex ERMs is stable in the face of arbitrary\nchanges to any one of the training points. Using the above assertion, we show\nthat dropout provides fast rates for generalization error in learning (convex)\ngeneralized linear models (GLM). Moreover, using the above mentioned stability\nproperties of dropout, we design dropout based differentially private\nalgorithms for solving ERMs. The learned GLM thus, preserves privacy of each of\nthe individual training points while providing accurate predictions for new\ntest points. Finally, we empirically validate our stability assertions for\ndropout in the context of convex ERMs and show that surprisingly, dropout\nsignificantly outperforms (in terms of prediction accuracy) the L2\nregularization based methods for several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 18:39:53 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Jain", "Prateek", ""], ["Kulkarni", "Vivek", ""], ["Thakurta", "Abhradeep", ""], ["Williams", "Oliver", ""]]}, {"id": "1503.02108", "submitter": "Zhen Huang", "authors": "Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jiadong Wu, and\n  Chin-Hui Lee", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian approach to adapting parameters of a well-trained\ncontext-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to\nimprove automatic speech recognition performance. Given an abundance of DNN\nparameters but with only a limited amount of data, the effectiveness of the\nadapted DNN model can often be compromised. We formulate maximum a posteriori\n(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an\naugmented linear hidden networks connected to the output tied states, or\nsenones, and compare it to feature space MAP linear regression previously\nproposed. Experimental evidences on the 20,000-word open vocabulary Wall Street\nJournal task demonstrate the feasibility of the proposed framework. In\nsupervised adaptation, the proposed MAP adaptation approach provides more than\n10% relative error reduction and consistently outperforms the conventional\ntransformation based methods. Furthermore, we present an initial attempt to\ngenerate hierarchical priors to improve adaptation efficiency and effectiveness\nwith limited adaptation data by exploiting similarities among senones.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 22:48:29 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 04:53:53 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Huang", "Zhen", ""], ["Siniscalchi", "Sabato Marco", ""], ["Chen", "I-Fan", ""], ["Wu", "Jiadong", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "1503.02357", "submitter": "Zhaopeng Tu", "authors": "Zhaopeng Tu, Baotian Hu, Zhengdong Lu, and Hang Li", "title": "Context-Dependent Translation Selection Using Convolutional Neural\n  Network", "comments": "Short version is accepted by ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for translation selection in statistical machine\ntranslation, in which a convolutional neural network is employed to judge the\nsimilarity between a phrase pair in two languages. The specifically designed\nconvolutional architecture encodes not only the semantic similarity of the\ntranslation pair, but also the context containing the phrase in the source\nlanguage. Therefore, our approach is able to capture context-dependent semantic\nsimilarities of translation pairs. We adopt a curriculum learning strategy to\ntrain the model: we classify the training examples into easy, medium, and\ndifficult categories, and gradually build the ability of representing phrase\nand sentence level context by using training examples from easy to difficult.\nExperimental results show that our approach significantly outperforms the\nbaseline system by up to 1.4 BLEU points.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 02:16:19 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 01:07:40 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Tu", "Zhaopeng", ""], ["Hu", "Baotian", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1503.02364", "submitter": "Lifeng Shang", "authors": "Lifeng Shang, Zhengdong Lu, Hang Li", "title": "Neural Responding Machine for Short-Text Conversation", "comments": "accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Responding Machine (NRM), a neural network-based response\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\nframework: it formalizes the generation of response as a decoding process based\non the latent representation of the input text, while both encoding and\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\nwith a large amount of one-round conversation data collected from a\nmicroblogging service. Empirical study shows that NRM can generate\ngrammatically correct and content-wise appropriate responses to over 75% of the\ninput text, outperforming state-of-the-arts in the same setting, including\nretrieval-based and SMT-based models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 02:54:29 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 02:28:58 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Shang", "Lifeng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1503.02427", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang and Zhengdong Lu and Hang Li and Qun Liu", "title": "Syntax-based Deep Matching of Short Texts", "comments": "Accepted by IJCAI-2015 as full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in natural language processing, ranging from machine translation\nto question answering, can be reduced to the problem of matching two sentences\nor more generally two short texts. We propose a new approach to the problem,\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\napproach consists of two components, 1) a mining algorithm to discover patterns\nfor matching two short-texts, defined in the product space of dependency trees,\nand 2) a deep neural network for matching short texts using the mined patterns,\nas well as a learning algorithm to build the network having a sparse structure.\nWe test our algorithm on the problem of matching a tweet and a response in\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\none without using dependency trees and one based on word-embedding, all with\nlarge margins\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 11:11:15 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 03:24:58 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 08:31:01 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 04:48:25 GMT"}, {"version": "v5", "created": "Mon, 18 May 2015 13:26:28 GMT"}, {"version": "v6", "created": "Fri, 12 Jun 2015 08:26:01 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Liu", "Qun", ""]]}, {"id": "1503.02531", "submitter": "Oriol Vinyals", "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean", "title": "Distilling the Knowledge in a Neural Network", "comments": "NIPS 2014 Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:44:49 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Hinton", "Geoffrey", ""], ["Vinyals", "Oriol", ""], ["Dean", "Jeff", ""]]}, {"id": "1503.02852", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang and Wonyong Sung", "title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "comments": "Accepted by the 40th IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP) 2015", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178129", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown outstanding performance on\nprocessing sequence data. However, they suffer from long training time, which\ndemands parallel implementations of the training procedure. Parallelization of\nthe training algorithms for RNNs are very challenging because internal\nrecurrent paths form dependencies between two different time frames. In this\npaper, we first propose a generalized graph-based RNN structure that covers the\nmost popular long short-term memory (LSTM) network. Then, we present a\nparallelization approach that automatically explores parallelisms of arbitrary\nRNNs by analyzing the graph structure. The experimental results show that the\nproposed approach shows great speed-up even with a single training stream, and\nfurther accelerates the training when combined with multiple parallel training\nstreams.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 10:27:55 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1503.03011", "submitter": "S Gopal Krishna Patro", "authors": "S. Gopal Krishna Patro, Pragyan Parimita Sahoo, Ipsita Panda, Kishore\n  Kumar Sahu", "title": "Technical Analysis on Financial Forecasting", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial forecasting is an estimation of future financial outcomes for a\ncompany, industry, country using historical internal accounting and sales data.\nWe may predict the future outcome of BSE_SENSEX practically by some soft\ncomputing techniques and can also optimized using PSO (Particle Swarm\nOptimization), EA (Evolutionary Algorithm) or DEA (Differential Evolutionary\nAlgorithm) etc. PSO is a biologically inspired computational search &\noptimization method developed in 1995 by Dr. Eberhart and Dr. Kennedy based on\nthe social behaviors of fish schooling or birds flocking. PSO is a promising\nmethod to train Artificial Neural Network (ANN). It is easy to implement then\nGenetic Algorithm except few parameters are adjusted. PSO is a random & pattern\nsearch technique based on populating of particle. In PSO, the particles are\nhaving some position and velocity in the search space. Two terms are used in\nPSO one is Local Best and another one is Global Best. To optimize problems that\nare like Irregular, Noisy, Change over time, Static etc. PSO uses a classic\noptimization method such as Gradient Decent & Quasi-Newton Methods. The\nobservation and review of few related studies in the last few years, focusing\non function of PSO, modification of PSO and operation that have implemented\nusing PSO like function optimization, ANN Training & Fuzzy Control etc.\nDifferential Evolution is an efficient EA technique for optimization of\nnumerical problems, financial problems etc. PSO technique is introduced due to\nthe swarming behavior of animals which is the collective behavior of similar\nsize that aggregates together.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 17:48:17 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Patro", "S. Gopal Krishna", ""], ["Sahoo", "Pragyan Parimita", ""], ["Panda", "Ipsita", ""], ["Sahu", "Kishore Kumar", ""]]}, {"id": "1503.03167", "submitter": "Tejas Kulkarni", "authors": "Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, Joshua B. Tenenbaum", "title": "Deep Convolutional Inverse Graphics Network", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a\nmodel that learns an interpretable representation of images. This\nrepresentation is disentangled with respect to transformations such as\nout-of-plane rotations and lighting variations. The DC-IGN model is composed of\nmultiple layers of convolution and de-convolution operators and is trained\nusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a\ntraining procedure to encourage neurons in the graphics code layer to represent\na specific transformation (e.g. pose or light). Given a single input image, our\nmodel can generate new images of the same object with variations in pose and\nlighting. We present qualitative and quantitative results of the model's\nefficacy at learning a 3D rendering engine.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 04:08:42 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 04:57:24 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 02:22:07 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 02:10:00 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Whitney", "Will", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1503.03175", "submitter": "Jayadeva", "authors": "Udit Kumar, Sumit Soman and Jayadeva", "title": "Benchmarking NLopt and state-of-art algorithms for Continuous Global\n  Optimization via Hybrid IACO$_\\mathbb{R}$", "comments": "24 pages, 10 figures", "journal-ref": "Swarm and Evolutionary Computation 27 (2016): 116-131", "doi": "10.1016/j.swevo.2015.10.005", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comparative analysis of the performance of the\nIncremental Ant Colony algorithm for continuous optimization\n($IACO_\\mathbb{R}$), with different algorithms provided in the NLopt library.\nThe key objective is to understand how the various algorithms in the NLopt\nlibrary perform in combination with the Multi Trajectory Local Search (Mtsls1)\ntechnique. A hybrid approach has been introduced in the local search strategy\nby the use of a parameter which allows for probabilistic selection between\nMtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch is\nmade based on the algorithm being used in the previous iteration. The paper\npresents an exhaustive comparison on the performance of these approaches on\nSoft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014\nbenchmarks. For both benchmarks, we conclude that the best performing algorithm\nis a hybrid variant of Mtsls1 with BFGS for local search.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 04:59:12 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Kumar", "Udit", ""], ["Soman", "Sumit", ""], ["Jayadeva", "", ""]]}, {"id": "1503.03211", "submitter": "Emmanuel Osegi", "authors": "J.O. Orove, N.E. Osegi, and B.O. Eke", "title": "A Multi-Gene Genetic Programming Application for Predicting Students\n  Failure at School", "comments": "14 pages, 9 figures, Journal paper. arXiv admin note: text overlap\n  with arXiv:1403.0623 by other authors", "journal-ref": "African Journal of Computing & ICT, 7(3), 21-34 (2015)", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Several efforts to predict student failure rate (SFR) at school accurately\nstill remains a core problem area faced by many in the educational sector. The\nprocedure for forecasting SFR are rigid and most often times require data\nscaling or conversion into binary form such as is the case of the logistic\nmodel which may lead to lose of information and effect size attenuation. Also,\nthe high number of factors, incomplete and unbalanced dataset, and black boxing\nissues as in Artificial Neural Networks and Fuzzy logic systems exposes the\nneed for more efficient tools. Currently the application of Genetic Programming\n(GP) holds great promises and has produced tremendous positive results in\ndifferent sectors. In this regard, this study developed GPSFARPS, a software\napplication to provide a robust solution to the prediction of SFR using an\nevolutionary algorithm known as multi-gene genetic programming. The approach is\nvalidated by feeding a testing data set to the evolved GP models. Result\nobtained from GPSFARPS simulations show its unique ability to evolve a suitable\nfailure rate expression with a fast convergence at 30 generations from a\nmaximum specified generation of 500. The multi-gene system was also able to\nminimize the evolved model expression and accurately predict student failure\nrate using a subset of the original expression\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 08:17:11 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Orove", "J. O.", ""], ["Osegi", "N. E.", ""], ["Eke", "B. O.", ""]]}, {"id": "1503.03244", "submitter": "Baotian Hu", "authors": "Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen", "title": "Convolutional Neural Network Architectures for Matching Natural Language\n  Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic matching is of central importance to many natural language tasks\n\\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to\nadequately model the internal structures of language objects and the\ninteraction between them. As a step toward this goal, we propose convolutional\nneural network models for matching two sentences, by adapting the convolutional\nstrategy in vision and speech. The proposed models not only nicely represent\nthe hierarchical structures of sentences with their layer-by-layer composition\nand pooling, but also capture the rich matching patterns at different levels.\nOur models are rather generic, requiring no prior knowledge on language, and\ncan hence be applied to matching tasks of different nature and in different\nlanguages. The empirical study on a variety of matching tasks demonstrates the\nefficacy of the proposed model on a variety of matching tasks and its\nsuperiority to competitor models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 09:46:36 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Hu", "Baotian", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Chen", "Qingcai", ""]]}, {"id": "1503.03438", "submitter": "Mark Tygert", "authors": "Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur\n  Szlam, and Mark Tygert", "title": "A mathematical motivation for complex-valued convolutional networks", "comments": "11 pages, 3 figures; this is the retitled version submitted to the\n  journal, \"Neural Computation\"", "journal-ref": "Neural Computation, 28 (5): 815-825, May 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex-valued convolutional network (convnet) implements the repeated\napplication of the following composition of three operations, recursively\napplying the composition to an input vector of nonnegative real numbers: (1)\nconvolution with complex-valued vectors followed by (2) taking the absolute\nvalue of every entry of the resulting vectors followed by (3) local averaging.\nFor processing real-valued random vectors, complex-valued convnets can be\nviewed as \"data-driven multiscale windowed power spectra,\" \"data-driven\nmultiscale windowed absolute spectra,\" \"data-driven multiwavelet absolute\nvalues,\" or (in their most general configuration) \"data-driven nonlinear\nmultiwavelet packets.\" Indeed, complex-valued convnets can calculate multiscale\nwindowed spectra when the convnet filters are windowed complex-valued\nexponentials. Standard real-valued convnets, using rectified linear units\n(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.\npooling, etc., do not obviously exhibit the same exact correspondence with\ndata-driven wavelets (whereas for complex-valued convnets, the correspondence\nis much more than just a vague analogy). Courtesy of the exact correspondence,\nthe remarkably rich and rigorous body of mathematical analysis for wavelets\napplies directly to (complex-valued) convnets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 18:24:13 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 00:42:09 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2015 19:04:02 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Bruna", "Joan", ""], ["Chintala", "Soumith", ""], ["LeCun", "Yann", ""], ["Piantino", "Serkan", ""], ["Szlam", "Arthur", ""], ["Tygert", "Mark", ""]]}, {"id": "1503.03562", "submitter": "Zhiyong Cheng", "authors": "Zhiyong Cheng, Daniel Soudry, Zexi Mao, Zhenzhong Lan", "title": "Training Binary Multilayer Neural Networks for Image Classification\n  using Expectation Backpropagation", "comments": "8 pages with 1 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to Multilayer Neural Networks with real weights, Binary Multilayer\nNeural Networks (BMNNs) can be implemented more efficiently on dedicated\nhardware. BMNNs have been demonstrated to be effective on binary classification\ntasks with Expectation BackPropagation (EBP) algorithm on high dimensional text\ndatasets. In this paper, we investigate the capability of BMNNs using the EBP\nalgorithm on multiclass image classification tasks. The performances of binary\nneural networks with multiple hidden layers and different numbers of hidden\nunits are examined on MNIST. We also explore the effectiveness of image spatial\nfilters and the dropout technique in BMNNs. Experimental results on MNIST\ndataset show that EBP can obtain 2.12% test error with binary weights and 1.66%\ntest error with real weights, which is comparable to the results of standard\nBackPropagation algorithm on fully connected MNNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 02:24:31 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 01:32:15 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2015 21:47:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Cheng", "Zhiyong", ""], ["Soudry", "Daniel", ""], ["Mao", "Zexi", ""], ["Lan", "Zhenzhong", ""]]}, {"id": "1503.04069", "submitter": "Klaus Greff", "authors": "Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\\'ik, Bas R.\n  Steunebrink, J\\\"urgen Schmidhuber", "title": "LSTM: A Search Space Odyssey", "comments": "12 pages, 6 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (\n  Volume: 28, Issue: 10, Oct. 2017 ) Pages: 2222 - 2232", "doi": "10.1109/TNNLS.2016.2582924", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several variants of the Long Short-Term Memory (LSTM) architecture for\nrecurrent neural networks have been proposed since its inception in 1995. In\nrecent years, these networks have become the state-of-the-art models for a\nvariety of machine learning problems. This has led to a renewed interest in\nunderstanding the role and utility of various computational components of\ntypical LSTM variants. In this paper, we present the first large-scale analysis\nof eight LSTM variants on three representative tasks: speech recognition,\nhandwriting recognition, and polyphonic music modeling. The hyperparameters of\nall LSTM variants for each task were optimized separately using random search,\nand their importance was assessed using the powerful fANOVA framework. In\ntotal, we summarize the results of 5400 experimental runs ($\\approx 15$ years\nof CPU time), which makes our study the largest of its kind on LSTM networks.\nOur results show that none of the variants can improve upon the standard LSTM\narchitecture significantly, and demonstrate the forget gate and the output\nactivation function to be its most critical components. We further observe that\nthe studied hyperparameters are virtually independent and derive guidelines for\ntheir efficient adjustment.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 14:01:38 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 11:40:31 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Greff", "Klaus", ""], ["Srivastava", "Rupesh Kumar", ""], ["Koutn\u00edk", "Jan", ""], ["Steunebrink", "Bas R.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1503.04475", "submitter": "Eric Lienert", "authors": "Eric Lienert", "title": "Simulation of Genetic Algorithm: Traffic Light Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic is a problem in many urban areas worldwide. Traffic flow is dictated\nby certain devices such as traffic lights. The traffic lights signal when each\nlane is able to pass through the intersection. Often, static schedules\ninterfere with ideal traffic flow. The purpose of this project was to find a\nway to make intersections controlled with traffic lights more efficient. This\ngoal was accomplished through the creation of a genetic algorithm, which\nenhances an input algorithm through genetic principles to produce the fittest\nalgorithm. The program was comprised of two major elements: coding in Java and\ncoding in Simulation of Urban Mobility (SUMO), which is an environment that\nsimulates real traffic. The Java code called upon the SUMO simulation via a\ncommand prompt which ran the simulation, received the output, altered the\nalgorithm, and looped. The SUMO component initialized a simulation in which a 1\nx 1 street layout was created, each intersection with its own traffic light.\nEach loop enhanced the input algorithm by altering the scheduling string\n(dictates the light changes). After the looped simulations were executed, the\ndata was then analyzed. This was accomplished by creating an algorithm based\nupon regular practice, timed traffic lights, and comparing the output which was\ncomprised of the total time it took for all vehicles to exit the system and the\naverage time it took each individual vehicle to exit the system. These\ndifferent variables: the time it took the average vehicle to exit the system\nand total time for all vehicles to exit the system, where then graphed together\nto provide a visual aid. The genetic algorithm did improve traffic light and\ntraffic flow efficiency in comparison to traditional scheduling methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 20:56:04 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Lienert", "Eric", ""]]}, {"id": "1503.04596", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell and Tony Vladusich", "title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional\n  Neural Network", "comments": "7 pages, 2 figures, Paper at IJCNN 2015 (International Joint\n  Conference on Neural Networks, 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network architecture and training method designed to\nenable very rapid training and low implementation complexity. Due to its\ntraining speed and very few tunable parameters, the method has strong potential\nfor applications requiring frequent retraining or online training. The approach\nis characterized by (a) convolutional filters based on biologically inspired\nvisual processing filters, (b) randomly-valued classifier-stage input weights,\n(c) use of least squares regression to train the classifier output weights in a\nsingle batch, and (d) linear classifier-stage output units. We demonstrate the\nefficacy of the method by applying it to image classification. Our results\nmatch existing state-of-the-art results on the MNIST (0.37% error) and\nNORB-small (2.2% error) image classification databases, but with very fast\ntraining times compared to standard deep network approaches. The network's\nperformance on the Google Street View House Number (SVHN) (4% error) database\nis also competitive with state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 10:41:30 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 06:26:40 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2015 13:02:08 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Vladusich", "Tony", ""]]}, {"id": "1503.04673", "submitter": "Yuriy Pershin", "authors": "Y. V. Pershin, L. K. Castelano, F. Hartmann, V. Lopez-Richard and M.\n  Di Ventra", "title": "A Memcomputing Pascaline", "comments": null, "journal-ref": "IEEE Trans. Circ. Syst. II 63, 558 (2016)", "doi": "10.1109/TCSII.2016.2530378", "report-no": null, "categories": "cs.ET cond-mat.mes-hall cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original Pascaline was a mechanical calculator able to sum and subtract\nintegers. It encodes information in the angles of mechanical wheels and through\na set of gears, and aided by gravity, could perform the calculations. Here, we\nshow that such a concept can be realized in electronics using memory elements\nsuch as memristive systems. By using memristive emulators we have demonstrated\nexperimentally the memcomputing version of the mechanical Pascaline, capable of\nprocessing and storing the numerical results in the multiple levels of each\nmemristive element. Our result is the first experimental demonstration of\nmultidigit arithmetics with multi-level memory devices that further emphasizes\nthe versatility and potential of memristive systems for future\nmassively-parallel high-density computing architectures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 14:47:05 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Pershin", "Y. V.", ""], ["Castelano", "L. K.", ""], ["Hartmann", "F.", ""], ["Lopez-Richard", "V.", ""], ["Di Ventra", "M.", ""]]}, {"id": "1503.04881", "submitter": "Xiaodan Zhu", "authors": "Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo", "title": "Long Short-Term Memory Over Tree Structures", "comments": "On February 6th, 2015, this work was submitted to the International\n  Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chain-structured long short-term memory (LSTM) has showed to be effective\nin a wide range of problems such as speech recognition and machine translation.\nIn this paper, we propose to extend it to tree structures, in which a memory\ncell can reflect the history memories of multiple child cells or multiple\ndescendant cells in a recursive process. We call the model S-LSTM, which\nprovides a principled way of considering long-distance interaction over\nhierarchies, e.g., language or image parse structures. We leverage the models\nfor semantic composition to understand the meaning of text, a fundamental\nproblem in natural language understanding, and show that it outperforms a\nstate-of-the-art recursive model by replacing its composition layers with the\nS-LSTM memory blocks. We also show that utilizing the given structures is\nhelpful in achieving a performance better than that without considering the\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 23:59:02 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Zhu", "Xiaodan", ""], ["Sobhani", "Parinaz", ""], ["Guo", "Hongyu", ""]]}, {"id": "1503.04941", "submitter": "J.H. van Hateren", "authors": "J.H. van Hateren", "title": "How the symbol grounding of living organisms can be realized in\n  artificial agents", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system with artificial intelligence usually relies on symbol manipulation,\nat least partly and implicitly. However, the interpretation of the symbols -\nwhat they represent and what they are about - is ultimately left to humans, as\ndesigners and users of the system. How symbols can acquire meaning for the\nsystem itself, independent of external interpretation, is an unsolved problem.\nSome grounding of symbols can be obtained by embodiment, that is, by causally\nconnecting symbols (or sub-symbolic variables) to the physical environment,\nsuch as in a robot with sensors and effectors. However, a causal connection as\nsuch does not produce representation and aboutness of the kind that symbols\nhave for humans. Here I present a theory that explains how humans and other\nliving organisms have acquired the capability to have symbols and sub-symbolic\nvariables that represent, refer to, and are about something else. The theory\nshows how reference can be to physical objects, but also to abstract objects,\nand even how it can be misguided (errors in reference) or be about non-existing\nobjects. I subsequently abstract the primary components of the theory from\ntheir biological context, and discuss how and under what conditions the theory\ncould be implemented in artificial agents. A major component of the theory is\nthe strong nonlinearity associated with (potentially unlimited)\nself-reproduction. The latter is likely not acceptable in artificial systems.\nIt remains unclear if goals other than those inherently serving\nself-reproduction can have aboutness and if such goals could be stabilized.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 08:00:49 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["van Hateren", "J. H.", ""]]}, {"id": "1503.05272", "submitter": "Abhisek Ukil", "authors": "A. Ukil, J. Bernasconi, H. Braendle, H. Buijs, S. Bonenfant", "title": "Improved Calibration of Near-Infrared Spectra by Using Ensembles of\n  Neural Network Models", "comments": "7 pages", "journal-ref": "IEEE Sensors Journal, vol. 10, no. 3, pp. 578-584, 2010", "doi": "10.1109/JSEN.2009.2038124", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IR or near-infrared (NIR) spectroscopy is a method used to identify a\ncompound or to analyze the composition of a material. Calibration of NIR\nspectra refers to the use of the spectra as multivariate descriptors to predict\nconcentrations of the constituents. To build a calibration model,\nstate-of-the-art software predominantly uses linear regression techniques. For\nnonlinear calibration problems, neural network-based models have proved to be\nan interesting alternative. In this paper, we propose a novel extension of the\nconventional neural network-based approach, the use of an ensemble of neural\nnetwork models. The individual neural networks are obtained by resampling the\navailable training data with bootstrapping or cross-validation techniques. The\nresults obtained for a realistic calibration example show that the\nensemble-based approach produces a significantly more accurate and robust\ncalibration model than conventional regression methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 02:54:04 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ukil", "A.", ""], ["Bernasconi", "J.", ""], ["Braendle", "H.", ""], ["Buijs", "H.", ""], ["Bonenfant", "S.", ""]]}, {"id": "1503.05471", "submitter": "Danila Doroshin", "authors": "Danila Doroshin, Alexander Yamshinin, Nikolay Lubimov, Marina\n  Nastasenko, Mikhail Kotov, Maxim Tkachenko", "title": "Shared latent subspace modelling within Gaussian-Binary Restricted\n  Boltzmann Machines for NIST i-Vector Challenge 2014", "comments": "5 pages, 3 figures, submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to speaker subspace modelling based on\nGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is\nbased on the idea of shared factors as in the Probabilistic Linear Discriminant\nAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,\nherein the speaker factor is shared over all vectors of the speaker. Then\nMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.\nVarious new scoring techniques for speaker verification using GRBM are\nproposed. The results for NIST i-vector Challenge 2014 dataset are presented.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:28:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Doroshin", "Danila", ""], ["Yamshinin", "Alexander", ""], ["Lubimov", "Nikolay", ""], ["Nastasenko", "Marina", ""], ["Kotov", "Mikhail", ""], ["Tkachenko", "Maxim", ""]]}, {"id": "1503.05671", "submitter": "James Martens", "authors": "James Martens, Roger Grosse", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "comments": "Reduction ratio formula corrected. Removed incorrect claim about\n  geodesics in footnote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method for approximating natural gradient descent in\nneural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).\nK-FAC is based on an efficiently invertible approximation of a neural network's\nFisher information matrix which is neither diagonal nor low-rank, and in some\ncases is completely non-sparse. It is derived by approximating various large\nblocks of the Fisher (corresponding to entire layers) as being the Kronecker\nproduct of two much smaller matrices. While only several times more expensive\nto compute than the plain stochastic gradient, the updates produced by K-FAC\nmake much more progress optimizing the objective, which results in an algorithm\nthat can be much faster than stochastic gradient descent with momentum in\npractice. And unlike some previously proposed approximate\nnatural-gradient/Newton methods which use high-quality non-diagonal curvature\nmatrices (such as Hessian-free optimization), K-FAC works very well in highly\nstochastic optimization regimes. This is because the cost of storing and\ninverting K-FAC's approximation to the curvature matrix does not depend on the\namount of data used to estimate it, which is a feature typically associated\nonly with diagonal or low-rank approximations to the curvature matrix.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 08:30:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 20:19:14 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 05:48:59 GMT"}, {"version": "v4", "created": "Thu, 21 May 2015 00:25:06 GMT"}, {"version": "v5", "created": "Fri, 24 Jul 2015 02:30:35 GMT"}, {"version": "v6", "created": "Wed, 4 May 2016 00:29:33 GMT"}, {"version": "v7", "created": "Mon, 8 Jun 2020 01:28:58 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Martens", "James", ""], ["Grosse", "Roger", ""]]}, {"id": "1503.05724", "submitter": "Sebastian Urban", "authors": "Sebastian Urban, Patrick van der Smagt", "title": "A Neural Transfer Function for a Smooth and Differentiable Transition\n  Between Additive and Multiplicative Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. This leads either to an\ninefficient distribution of computational resources or an extensive increase in\nthe computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 11:48:14 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 18:01:52 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 14:45:03 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1503.05743", "submitter": "Ken Miura", "authors": "Ken Miura and Tatsuya Harada", "title": "Implementation of a Practical Distributed Calculation System with\n  Browsers and JavaScript, and Application to Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning can achieve outstanding results in various fields. However, it\nrequires so significant computational power that graphics processing units\n(GPUs) and/or numerous computers are often required for the practical\napplication. We have developed a new distributed calculation framework called\n\"Sashimi\" that allows any computer to be used as a distribution node only by\naccessing a website. We have also developed a new JavaScript neural network\nframework called \"Sukiyaki\" that uses general purpose GPUs with web browsers.\nSukiyaki performs 30 times faster than a conventional JavaScript library for\ndeep convolutional neural networks (deep CNNs) learning. The combination of\nSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the\ndistributed deep learning of deep CNNs only with web browsers on various\ndevices. The libraries that comprise the proposed methods are available under\nMIT license at http://mil-tokyo.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 12:41:29 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Miura", "Ken", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1503.05831", "submitter": "Abhisek Ukil", "authors": "A. Ukil, J. Bernasconi", "title": "Neural Network-Based Active Learning in Multivariate Calibration", "comments": "9 pages in final printed version", "journal-ref": "IEEE Transactions on Systems, Man, Cybernetics-Part C, vol. 42,\n  issue 6, pp. 1763-1771, 2012", "doi": "10.1109/TSMCC.2012.2220963", "report-no": null, "categories": "cs.NE cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In chemometrics, data from infrared or near-infrared (NIR) spectroscopy are\noften used to identify a compound or to analyze the composition of amaterial.\nThis involves the calibration of models that predict the concentration\nofmaterial constituents from the measured NIR spectrum. An interesting aspect\nof multivariate calibration is to achieve a particular accuracy level with a\nminimum number of training samples, as this reduces the number of laboratory\ntests and thus the cost of model building. In these chemometric models, the\ninput refers to a proper representation of the spectra and the output to the\nconcentrations of the sample constituents. The search for a most informative\nnew calibration sample thus has to be performed in the output space of the\nmodel, rather than in the input space as in conventionalmodeling problems. In\nthis paper, we propose to solve the corresponding inversion problem by\nutilizing the disagreements of an ensemble of neural networks to represent the\nprediction error in the unexplored component space. The next calibration sample\nis then chosen at a composition where the individual models of the ensemble\ndisagree most. The results obtained for a realistic chemometric calibration\nexample show that the proposed active learning can achieve a given calibration\naccuracy with less training samples than random sampling.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 16:30:21 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ukil", "A.", ""], ["Bernasconi", "J.", ""]]}, {"id": "1503.05849", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Time-Domain Audio Error Correction via Probabilistic\n  Re-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of recording, storage and transmission of time-domain audio\nsignals, errors may be introduced that are difficult to correct in an\nunsupervised way. Here, we train a convolutional deep neural network to\nre-synthesize input time-domain speech signals at its output layer. We then use\nthis abstract transformation, which we call a deep transform (DT), to perform\nprobabilistic re-synthesis on further speech (of the same speaker) which has\nbeen degraded. Using the convolutive DT, we demonstrate the recovery of speech\naudio that has been subject to extreme degradation. This approach may be useful\nfor correction of errors in communications devices.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 17:24:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1503.06004", "submitter": "Abhisek Ukil", "authors": "A. Ukil, W. Siti, J. Jordaan", "title": "Feeder Load Balancing using Neural Network", "comments": "6 pages in final published version", "journal-ref": "Lecture Notes in Computer Science, Springer, vol. 3972, pp.\n  1311-1316, 2006", "doi": "10.1007/11760023_190", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution system problems, such as planning, loss minimization, and\nenergy restoration, usually involve the phase balancing or network\nreconfiguration procedures. The determination of an optimal phase balance is,\nin general, a combinatorial optimization problem. This paper proposes optimal\nreconfiguration of the phase balancing using the neural network, to switch on\nand off the different switches, allowing the three phases supply by the\ntransformer to the end-users to be balanced. This paper presents the\napplication examples of the proposed method using the real and simulated test\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 06:48:14 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Ukil", "A.", ""], ["Siti", "W.", ""], ["Jordaan", "J.", ""]]}, {"id": "1503.06046", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Cocktail Party Source Separation via Probabilistic\n  Re-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cocktail party listening scenarios, the human brain is able to separate\ncompeting speech signals. However, the signal processing implemented by the\nbrain to perform cocktail party listening is not well understood. Here, we\ntrained two separate convolutive autoencoder deep neural networks (DNN) to\nseparate monaural and binaural mixtures of two concurrent speech streams. We\nthen used these DNNs as convolutive deep transform (CDT) devices to perform\nprobabilistic re-synthesis. The CDTs operated directly in the time-domain. Our\nsimulations demonstrate that very simple neural networks are capable of\nexploiting monaural and binaural information available in a cocktail party\nlistening scenario.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 12:00:44 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1503.06342", "submitter": "Luc Berthouze", "authors": "Peter Overbury and Luc Berthouze", "title": "Using novelty-biased GA to sample diversity in graphs satisfying\n  constraints", "comments": "Extended version of a short paper accepted for publication in\n  Proceedings of Genetic and Evolutionary Computation Conference (GECCO'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.NE cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of the network underlying many complex systems, whether\nartificial or natural, plays a significant role in how these systems operate.\nAs a result, much emphasis has been placed on accurately describing networks\nusing network theoretic metrics. When it comes to generating networks with\nsimilar properties, however, the set of available techniques and properties\nthat can be controlled for remains limited. Further, whilst it is becoming\nclear that some of the metrics currently used to control the generation of such\nnetworks are not very prescriptive so that networks could potentially exhibit\nvery different higher-order structure within those constraints, network\ngenerating algorithms typically produce fairly contrived networks and lack\nmechanisms by which to systematically explore the space of network solutions.\nIn this paper, we explore the potential of a multi-objective novelty-biased GA\nto provide a viable alternative to these algorithms. We believe our results\nprovide the first proof of principle that (i) it is possible to use GAs to\ngenerate graphs satisfying set levels of key classical graph theoretic\nproperties and (ii) it is possible to generate diverse solutions within these\nconstraints. The paper is only a preliminary step, however, and we identify key\navenues for further development.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 19:32:07 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Overbury", "Peter", ""], ["Berthouze", "Luc", ""]]}, {"id": "1503.06383", "submitter": "Angshul Majumdar Dr.", "authors": "Angshul Majumdar", "title": "Real-time Dynamic MRI Reconstruction using Stacked Denoising Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of real-time dynamic MRI reconstruction.\nThere are a handful of studies on this topic; these techniques are either based\non compressed sensing or employ Kalman Filtering. These techniques cannot\nachieve the reconstruction speed necessary for real-time reconstruction. In\nthis work, we propose a new approach to MRI reconstruction. We learn a\nnon-linear mapping from the unstructured aliased images to the corresponding\nclean images using a stacked denoising autoencoder (SDAE). The training for\nSDAE is slow, but the reconstruction is very fast - only requiring a few matrix\nvector multiplications. In this work, we have shown that using SDAE one can\nreconstruct the MRI frame faster than the data acquisition rate, thereby\nachieving real-time reconstruction. The quality of reconstruction is of the\nsame order as a previous compressed sensing based online reconstruction\ntechnique.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 04:09:31 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Majumdar", "Angshul", ""]]}, {"id": "1503.06410", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "KIT-14-001", "categories": "cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure or F-score is one of the most commonly used single number\nmeasures in Information Retrieval, Natural Language Processing and Machine\nLearning, but it is based on a mistake, and the flawed assumptions render it\nunsuitable for use in most contexts! Fortunately, there are better\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:42:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1503.06424", "submitter": "Juan Juli\\'an Merelo-Guerv\\'os Pr.", "authors": "Juan Juli\\'an Merelo-Guerv\\'os, Pablo Garc\\'ia-S\\'anchez", "title": "Modeling browser-based distributed evolutionary computation systems", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  From the era of big science we are back to the \"do it yourself\", where you do\nnot have any money to buy clusters or subscribe to grids but still have\nalgorithms that crave many computing nodes and need them to measure\nscalability. Fortunately, this coincides with the era of big data, cloud\ncomputing, and browsers that include JavaScript virtual machines. Those are the\nreasons why this paper will focus on two different aspects of volunteer or\nfreeriding computing: first, the pragmatic: where to find those resources,\nwhich ones can be used, what kind of support you have to give them; and then,\nthe theoretical: how evolutionary algorithms can be adapted to an environment\nin which nodes come and go, have different computing capabilities and operate\nin complete asynchrony of each other. We will examine the setup needed to\ncreate a very simple distributed evolutionary algorithm using JavaScript and\nthen find a model of how users react to it by collecting data from several\nexperiments featuring different classical benchmark functions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 13:20:57 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Merelo-Guerv\u00f3s", "Juan Juli\u00e1n", ""], ["Garc\u00eda-S\u00e1nchez", "Pablo", ""]]}, {"id": "1503.06452", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Unsupervised model compression for multilayer bootstrap networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multilayer bootstrap network (MBN) has demonstrated promising\nperformance in unsupervised dimensionality reduction. It can learn compact\nrepresentations in standard data sets, i.e. MNIST and RCV1. However, as a\nbootstrap method, the prediction complexity of MBN is high. In this paper, we\npropose an unsupervised model compression framework for this general problem of\nunsupervised bootstrap methods. The framework compresses a large unsupervised\nbootstrap model into a small model by taking the bootstrap model and its\napplication together as a black box and learning a mapping function from the\ninput of the bootstrap model to the output of the application by a supervised\nlearner. To specialize the framework, we propose a new technique, named\ncompressive MBN. It takes MBN as the unsupervised bootstrap model and deep\nneural network (DNN) as the supervised learner. Our initial result on MNIST\nshowed that compressive MBN not only maintains the high prediction accuracy of\nMBN but also is over thousands of times faster than MBN at the prediction\nstage. Our result suggests that the new technique integrates the effectiveness\nof MBN on unsupervised learning and the effectiveness and efficiency of DNN on\nsupervised learning together for the effectiveness and efficiency of\ncompressive MBN on unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 18:22:28 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1503.06866", "submitter": "Ndoundam Rene", "authors": "Serge Alain Eb\\'el\\'e, Ren\\`e Ndoundam", "title": "Study of all the periods of a Neuronal Recurrence Equation", "comments": "22 pages", "journal-ref": "Complex Systems, 24, 2015", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We characterize the structure of the periods of a neuronal recurrence\nequation. Firstly, we give a characterization of k-chains in 0-1 periodic\nsequences. Secondly, we characterize the periods of all cycles of some neuronal\nrecurrence equation. Thirdly, we explain how these results can be used to\ndeduce the existence of the generalized period-halving bifurcation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 22:30:39 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 18:56:03 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2015 16:48:49 GMT"}, {"version": "v4", "created": "Sat, 20 Feb 2016 09:57:40 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Eb\u00e9l\u00e9", "Serge Alain", ""], ["Ndoundam", "Ren\u00e8", ""]]}, {"id": "1503.06962", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Probabilistic Binary-Mask Cocktail-Party Source Separation in a\n  Convolutional Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separation of competing speech is a key challenge in signal processing and a\nfeat routinely performed by the human auditory brain. A long standing benchmark\nof the spectrogram approach to source separation is known as the ideal binary\nmask. Here, we train a convolutional deep neural network, on a two-speaker\ncocktail party problem, to make probabilistic predictions about binary masks.\nOur results approach ideal binary mask performance, illustrating that\nrelatively simple deep neural networks are capable of robust binary mask\nprediction. We also illustrate the trade-off between prediction statistics and\nseparation quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 09:34:51 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1503.07077", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Kyle W. Willett, Joni Dambre", "title": "Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction", "comments": "Accepted for publication in MNRAS. 20 pages, 14 figures", "journal-ref": null, "doi": "10.1093/mnras/stv632", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 15:34:06 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Dieleman", "Sander", ""], ["Willett", "Kyle W.", ""], ["Dambre", "Joni", ""]]}, {"id": "1503.07490", "submitter": "Subhajit Sengupta", "authors": "Subhajit Sengupta and Karthik S. Gurumoorthy and Arunava Banerjee", "title": "Sensitivity Analysis for additive STDP rule", "comments": "On Computational Neuroscience/ 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning\nrule. The basis of STDP has strong experimental evidences and it depends on\nprecise input and output spike timings. In this paper we show that under\nbiologically plausible spiking regime, slight variability in the spike timing\nleads to drastically different evolution of synaptic weights when its dynamics\nare governed by the additive STDP rule.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 16:07:20 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 13:59:32 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Sengupta", "Subhajit", ""], ["Gurumoorthy", "Karthik S.", ""], ["Banerjee", "Arunava", ""]]}, {"id": "1503.07609", "submitter": "Yanping Liu", "authors": "Yanping Liu, Erik D. Reichle", "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although different learning systems are coordinated to afford complex\nbehavior, little is known about how this occurs. This article describes a\ntheoretical framework that specifies how complex behaviors that might be\nthought to require error-driven learning might instead be acquired through\nsimple reinforcement. This framework includes specific assumptions about the\nmechanisms that contribute to the evolution of (artificial) neural networks to\ngenerate topologies that allow the networks to learn large-scale complex\nproblems using only information about the quality of their performance. The\npractical and theoretical implications of the framework are discussed, as are\npossible biological analogs of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 03:33:47 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Liu", "Yanping", ""], ["Reichle", "Erik D.", ""]]}, {"id": "1503.07793", "submitter": "Srinjoy Das", "authors": "Srinjoy Das, Bruno Umbria Pedroni, Paul Merolla, John Arthur, Andrew\n  S. Cassidy, Bryan L. Jackson, Dharmendra Modha, Gert Cauwenberghs, Ken\n  Kreutz-Delgado", "title": "Gibbs Sampling with Low-Power Spiking Digital Neurons", "comments": "Accepted at ISCAS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines and Deep Belief Networks have been successfully\nused in a wide variety of applications including image classification and\nspeech recognition. Inference and learning in these algorithms uses a Markov\nChain Monte Carlo procedure called Gibbs sampling. A sigmoidal function forms\nthe kernel of this sampler which can be realized from the firing statistics of\nnoisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paper\ndemonstrates such an implementation on an array of digital spiking neurons with\nstochastic leak and threshold properties for inference tasks and presents some\nkey performance metrics for such a hardware-based sampler in both the\ngenerative and discriminative contexts.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 17:14:00 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 14:21:17 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Das", "Srinjoy", ""], ["Pedroni", "Bruno Umbria", ""], ["Merolla", "Paul", ""], ["Arthur", "John", ""], ["Cassidy", "Andrew S.", ""], ["Jackson", "Bryan L.", ""], ["Modha", "Dharmendra", ""], ["Cauwenberghs", "Gert", ""], ["Kreutz-Delgado", "Ken", ""]]}, {"id": "1503.08294", "submitter": "Giacomo Parigi", "authors": "Giacomo Parigi, Angelo Stramieri, Danilo Pau, Marco Piastra", "title": "A Multi-signal Variant for the GPU-based Parallelization of Growing\n  Self-Organizing Networks", "comments": "17 pages", "journal-ref": "Informatics in Control, Automation and Robotics - 9th\n  International Conference, ICINCO 2012 Rome, Italy, July 28-31, 2012 Revised\n  Selected Papers. Part I, pp. 83-100", "doi": "10.1007/978-3-319-03500-0_6", "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the many possible approaches for the parallelization of self-organizing\nnetworks, and in particular of growing self-organizing networks, perhaps the\nmost common one is producing an optimized, parallel implementation of the\nstandard sequential algorithms reported in the literature. In this paper we\nexplore an alternative approach, based on a new algorithm variant specifically\ndesigned to match the features of the large-scale, fine-grained parallelism of\nGPUs, in which multiple input signals are processed at once. Comparative tests\nhave been performed, using both parallel and sequential implementations of the\nnew algorithm variant, in particular for a growing self-organizing network that\nreconstructs surfaces from point clouds. The experimental results show that\nthis approach allows harnessing in a more effective way the intrinsic\nparallelism that the self-organizing networks algorithms seem intuitively to\nsuggest, obtaining better performances even with networks of smaller size.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 10:51:55 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Parigi", "Giacomo", ""], ["Stramieri", "Angelo", ""], ["Pau", "Danilo", ""], ["Piastra", "Marco", ""]]}, {"id": "1503.08322", "submitter": "Marco Piastra", "authors": "Giacomo Parigi, Andrea Pedrini, Marco Piastra", "title": "Some Further Evidence about Magnification and Shape in Neural Gas", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2015.7280550", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Neural gas (NG) is a robust vector quantization algorithm with a well-known\nmathematical model. According to this, the neural gas samples the underlying\ndata distribution following a power law with a magnification exponent that\ndepends on data dimensionality only. The effects of shape in the input data\ndistribution, however, are not entirely covered by the NG model above, due to\nthe technical difficulties involved. The experimental work described here shows\nthat shape is indeed relevant in determining the overall NG behavior; in\nparticular, some experiments reveal richer and complex behaviors induced by\nshape that cannot be explained by the power law alone. Although a more\ncomprehensive analytical model remains to be defined, the evidence collected in\nthese experiments suggests that the NG algorithm has an interesting potential\nfor detecting complex shapes in noisy datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 16:33:20 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Parigi", "Giacomo", ""], ["Pedrini", "Andrea", ""], ["Piastra", "Marco", ""]]}, {"id": "1503.08895", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston and Rob Fergus", "title": "End-To-End Memory Networks", "comments": "Accepted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural network with a recurrent attention model over a\npossibly large external memory. The architecture is a form of Memory Network\n(Weston et al., 2015) but unlike the model in that work, it is trained\nend-to-end, and hence requires significantly less supervision during training,\nmaking it more generally applicable in realistic settings. It can also be seen\nas an extension of RNNsearch to the case where multiple computational steps\n(hops) are performed per output symbol. The flexibility of the model allows us\nto apply it to tasks as diverse as (synthetic) question answering and to\nlanguage modeling. For the former our approach is competitive with Memory\nNetworks, but with less supervision. For the latter, on the Penn TreeBank and\nText8 datasets our approach demonstrates comparable performance to RNNs and\nLSTMs. In both cases we show that the key concept of multiple computational\nhops yields improved results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 03:05:37 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 02:23:20 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 04:19:33 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2015 21:42:20 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2015 19:41:57 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Weston", "Jason", ""], ["Fergus", "Rob", ""]]}, {"id": "1503.09129", "submitter": "Brian Gardner BG", "authors": "Brian Gardner, Ioana Sporea, Andr\\'e Gr\\\"uning", "title": "Encoding Spike Patterns in Multilayer Spiking Neural Networks", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": "10.1162/NECO_a_00790", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information encoding in the nervous system is supported through the precise\nspike-timings of neurons; however, an understanding of the underlying processes\nby which such representations are formed in the first place remains unclear.\nHere we examine how networks of spiking neurons can learn to encode for input\npatterns using a fully temporal coding scheme. To this end, we introduce a\nlearning rule for spiking networks containing hidden neurons which optimizes\nthe likelihood of generating desired output spiking patterns. We show the\nproposed learning rule allows for a large number of accurate input-output spike\npattern mappings to be learnt, which outperforms other existing learning rules\nfor spiking neural networks: both in the number of mappings that can be learnt\nas well as the complexity of spike train encodings that can be utilised. The\nlearning rule is successful even in the presence of input noise, is\ndemonstrated to solve the linearly non-separable XOR computation and\ngeneralizes well on an example dataset. We further present a biologically\nplausible implementation of backpropagated learning in multilayer spiking\nnetworks, and discuss the neural mechanisms that might underlie its function.\nOur approach contributes both to a systematic understanding of how pattern\nencodings might take place in the nervous system, and a learning rule that\ndisplays strong technical capability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 17:12:07 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Gardner", "Brian", ""], ["Sporea", "Ioana", ""], ["Gr\u00fcning", "Andr\u00e9", ""]]}]