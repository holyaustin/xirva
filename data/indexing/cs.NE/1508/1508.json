[{"id": "1508.00021", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, \\'Etienne Simon, Alex Auvolat, Pascal\n  Vincent, Yoshua Bengio", "title": "Artificial Neural Networks Applied to Taxi Destination Prediction", "comments": "ECML/PKDD discovery challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our first-place solution to the ECML/PKDD discovery challenge on\ntaxi destination prediction. The task consisted in predicting the destination\nof a taxi based on the beginning of its trajectory, represented as a\nvariable-length sequence of GPS points, and diverse associated\nmeta-information, such as the departure time, the driver id and client\ninformation. Contrary to most published competitor approaches, we used an\nalmost fully automated approach based on neural networks and we ranked first\nout of 381 teams. The architectures we tried use multi-layer perceptrons,\nbidirectional recurrent neural networks and models inspired from recently\nintroduced memory networks. Our approach could easily be adapted to other\napplications in which the goal is to predict a fixed-length output from a\nvariable-length sequence.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 20:24:20 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 15:09:35 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Simon", "\u00c9tienne", ""], ["Auvolat", "Alex", ""], ["Vincent", "Pascal", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1508.00097", "submitter": "Jaderick Pabico", "authors": "Jaderick P. Pabico and Elizer A. Albacea", "title": "The Interactive Effects of Operators and Parameters to GA Performance\n  Under Different Problem Sizes", "comments": "19 pages", "journal-ref": "Philippine Computing Journal 3(2):26-37, 2008", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The complex effect of genetic algorithm's (GA) operators and parameters to\nits performance has been studied extensively by researchers in the past but\nnone studied their interactive effects while the GA is under different problem\nsizes. In this paper, We present the use of experimental model (1)~to\ninvestigate whether the genetic operators and their parameters interact to\naffect the offline performance of GA, (2)~to find what combination of genetic\noperators and parameter settings will provide the optimum performance for GA,\nand (3)~to investigate whether these operator-parameter combination is\ndependent on the problem size. We designed a GA to optimize a family of\ntraveling salesman problems (TSP), with their optimal solutions known for\nconvenient benchmarking. Our GA was set to use different algorithms in\nsimulating selection ($\\Omega_s$), different algorithms ($\\Omega_c$) and\nparameters ($p_c$) in simulating crossover, and different parameters ($p_m$) in\nsimulating mutation. We used several $n$-city TSPs ($n=\\{5, 7, 10, 100,\n1000\\}$) to represent the different problem sizes (i.e., size of the resulting\nsearch space as represented by GA schemata). Using analysis of variance of\n3-factor factorial experiments, we found out that GA performance is affected by\n$\\Omega_s$ at small problem size (5-city TSP) where the algorithm Partially\nMatched Crossover significantly outperforms Cycle Crossover at $95\\%$\nconfidence level.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 08:01:50 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Pabico", "Jaderick P.", ""], ["Albacea", "Elizer A.", ""]]}, {"id": "1508.00144", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva, Julie Henriques, Juan-Pablo Ortega", "title": "Quantitative evaluation of the performance of discrete-time reservoir\n  computers in the forecasting, filtering, and reconstruction of stochastic\n  stationary signals", "comments": "21 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the notion of information processing capacity for\nnon-independent input signals in the context of reservoir computing (RC). The\npresence of input autocorrelation makes worthwhile the treatment of forecasting\nand filtering problems for which we explicitly compute this generalized\ncapacity as a function of the reservoir parameter values using a streamlined\nmodel. The reservoir model leading to these developments is used to show that,\nwhenever that approximation is valid, this computational paradigm satisfies the\nso called separation and fading memory properties that are usually associated\nwith good information processing performances. We show that several standard\nmemory, forecasting, and filtering problems that appear in the parametric\nstochastic time series context can be readily formulated and tackled via RC\nwhich, as we show, significantly outperforms standard techniques in some\ninstances.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 16:42:38 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 10:21:19 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 14:44:12 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Henriques", "Julie", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1508.00200", "submitter": "Jian Tang", "authors": "Jian Tang, Meng Qu, Qiaozhu Mei", "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\n  Networks", "comments": "KDD 2015", "journal-ref": null, "doi": "10.1145/2783258.2783307", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\nhave been attracting increasing attention due to their simplicity, scalability,\nand effectiveness. However, comparing to sophisticated deep learning\narchitectures such as convolutional neural networks, these methods usually\nyield inferior results when applied to particular machine learning tasks. One\npossible reason is that these text embedding methods learn the representation\nof text in a fully unsupervised way, without leveraging the labeled information\navailable for the task. Although the low dimensional representations learned\nare applicable to many different tasks, they are not particularly tuned for any\ntask. In this paper, we fill this gap by proposing a semi-supervised\nrepresentation learning method for text data, which we call the\n\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\nboth labeled and unlabeled data to learn the embedding of text. The labeled\ninformation and different levels of word co-occurrence information are first\nrepresented as a large-scale heterogeneous text network, which is then embedded\ninto a low dimensional space through a principled and efficient algorithm. This\nlow dimensional embedding not only preserves the semantic closeness of words\nand documents, but also has a strong predictive power for the particular task.\nCompared to recent supervised approaches based on convolutional neural\nnetworks, predictive text embedding is comparable or more effective, much more\nefficient, and has fewer parameters to tune.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 06:18:10 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Tang", "Jian", ""], ["Qu", "Meng", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1508.00230", "submitter": "Mohammad Abu Alsheikh", "authors": "Mohammad Abu Alsheikh, Shaowei Lin, Hwee-Pink Tan, and Dusit Niyato", "title": "Toward a Robust Sparse Data Representation for Wireless Sensor Networks", "comments": "8 pages", "journal-ref": "IEEE 40th Conference on Local Computer Networks (LCN), Clearwater\n  Beach, FL, 2015, pp. 117-124", "doi": "10.1109/LCN.2015.7366290", "report-no": null, "categories": "cs.NI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing has been successfully used for optimized operations in\nwireless sensor networks. However, raw data collected by sensors may be neither\noriginally sparse nor easily transformed into a sparse data representation.\nThis paper addresses the problem of transforming source data collected by\nsensor nodes into a sparse representation with a few nonzero elements. Our\ncontributions that address three major issues include: 1) an effective method\nthat extracts population sparsity of the data, 2) a sparsity ratio guarantee\nscheme, and 3) a customized learning algorithm of the sparsifying dictionary.\nWe introduce an unsupervised neural network to extract an intrinsic sparse\ncoding of the data. The sparse codes are generated at the activation of the\nhidden layer using a sparsity nomination constraint and a shrinking mechanism.\nOur analysis using real data samples shows that the proposed method outperforms\nconventional sparsity-inducing methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 13:12:50 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Alsheikh", "Mohammad Abu", ""], ["Lin", "Shaowei", ""], ["Tan", "Hwee-Pink", ""], ["Niyato", "Dusit", ""]]}, {"id": "1508.00330", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Gustavo Carneiro", "title": "On the Importance of Normalisation Layers in Deep Learning with\n  Piecewise Linear Activation Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feedforward neural networks with piecewise linear activations are\ncurrently producing the state-of-the-art results in several public datasets.\nThe combination of deep learning models and piecewise linear activation\nfunctions allows for the estimation of exponentially complex functions with the\nuse of a large number of subnetworks specialized in the classification of\nsimilar input examples. During the training process, these subnetworks avoid\noverfitting with an implicit regularization scheme based on the fact that they\nmust share their parameters with other subnetworks. Using this framework, we\nhave made an empirical observation that can improve even more the performance\nof such models. We notice that these models assume a balanced initial\ndistribution of data points with respect to the domain of the piecewise linear\nactivation function. If that assumption is violated, then the piecewise linear\nactivation units can degenerate into purely linear activation units, which can\nresult in a significant reduction of their capacity to learn complex functions.\nFurthermore, as the number of model layers increases, this unbalanced initial\ndistribution makes the model ill-conditioned. Therefore, we propose the\nintroduction of batch normalisation units into deep feedforward neural networks\nwith piecewise linear activations, which drives a more balanced use of these\nactivation units, where each region of the activation function is trained with\na relatively large proportion of training samples. Also, this batch\nnormalisation promotes the pre-conditioning of very deep learning models. We\nshow that by introducing maxout and batch normalisation units to the network in\nnetwork model results in a model that produces classification results that are\nbetter than or comparable to the current state of the art in CIFAR-10,\nCIFAR-100, MNIST, and SVHN datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 07:24:07 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 06:44:10 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Liao", "Zhibin", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1508.00451", "submitter": "Rein Houthooft", "authors": "Rein Houthooft, Filip De Turck", "title": "Integrated Inference and Learning of Neural Factors in Structural\n  Support Vector Machines", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2016.03.014", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling pattern recognition problems in areas such as computer vision,\nbioinformatics, speech or text recognition is often done best by taking into\naccount task-specific statistical relations between output variables. In\nstructured prediction, this internal structure is used to predict multiple\noutputs simultaneously, leading to more accurate and coherent predictions.\nStructural support vector machines (SSVMs) are nonprobabilistic models that\noptimize a joint input-output function through margin-based learning. Because\nSSVMs generally disregard the interplay between unary and interaction factors\nduring the training phase, final parameters are suboptimal. Moreover, its\nfactors are often restricted to linear combinations of input features, limiting\nits generalization power. To improve prediction accuracy, this paper proposes:\n(i) Joint inference and learning by integration of back-propagation and\nloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM\nfactors to neural networks that form highly nonlinear functions of input\nfeatures. Image segmentation benchmark results demonstrate improvements over\nconventional SSVM training methods in terms of accuracy, highlighting the\nfeasibility of end-to-end SSVM training with neural factors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:29:57 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 12:41:52 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 16:02:00 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 22:46:17 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Houthooft", "Rein", ""], ["De Turck", "Filip", ""]]}, {"id": "1508.00457", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "Evolutionary Multimodal Optimization: A Short Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world problems always have different multiple solutions. For instance,\noptical engineers need to tune the recording parameters to get as many optimal\nsolutions as possible for multiple trials in the varied-line-spacing\nholographic grating design problem. Unfortunately, most traditional\noptimization techniques focus on solving for a single optimal solution. They\nneed to be applied several times; yet all solutions are not guaranteed to be\nfound. Thus the multimodal optimization problem was proposed. In that problem,\nwe are interested in not only a single optimal point, but also the others. With\nstrong parallel search capability, evolutionary algorithms are shown to be\nparticularly effective in solving this type of problem. In particular, the\nevolutionary algorithms for multimodal optimization usually not only locate\nmultiple optima in a single run, but also preserve their population diversity\nthroughout a run, resulting in their global optimization ability on multimodal\nfunctions. In addition, the techniques for multimodal optimization are borrowed\nas diversity maintenance techniques to other problems. In this chapter, we\ndescribe and review the state-of-the-arts evolutionary algorithms for\nmultimodal optimization in terms of methodology, benchmarking, and application.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:45:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1508.00468", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "Evolutionary Algorithms: Concepts, Designs, and Applications in\n  Bioinformatics: Evolutionary Algorithms for Bioinformatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.GN q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) in\nthe early 1970s, the study of evolutionary algorithm has emerged as a popular\nresearch field (Civicioglu & Besdok, 2013). Researchers from various scientific\nand engineering disciplines have been digging into this field, exploring the\nunique power of evolutionary algorithms (Hadka & Reed, 2013). Many applications\nhave been successfully proposed in the past twenty years. For example,\nmechanical design (Lampinen & Zelinka, 1999), electromagnetic optimization\n(Rahmat-Samii & Michielssen, 1999), environmental protection (Bertini, Felice,\nMoretti, & Pizzuti, 2010), finance (Larkin & Ryan, 2010), musical orchestration\n(Esling, Carpentier, & Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, &\nTorresen, 2010), and nuclear reactor core design (Sacco, Henderson,\nRios-Coelho, Ali, & Pereira, 2009). In particular, its function optimization\ncapability was highlighted (Goldberg & Richardson, 1987) because of its high\nadaptability to different function landscapes, to which we cannot apply\ntraditional optimization techniques (Wong, Leung, & Wong, 2009). Here we review\nthe applications of evolutionary algorithms in bioinformatics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 16:05:34 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1508.00984", "submitter": "Pitoyo Hartono", "authors": "Pitoyo Hartono", "title": "Dimension Reduction with Non-degrading Generalization", "comments": null, "journal-ref": "Neural Computing and Applications 30 (2018) 905-915", "doi": "10.1007/s00521-016-2726-5", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing high dimensional data by projecting them into two or three\ndimensional space is one of the most effective ways to intuitively understand\nthe data's underlying characteristics, for example their class neighborhood\nstructure. While data visualization in low dimensional space can be efficient\nfor revealing the data's underlying characteristics, classifying a new sample\nin the reduced-dimensional space is not always beneficial because of the loss\nof information in expressing the data. It is possible to classify the data in\nthe high dimensional space, while visualizing them in the low dimensional\nspace, but in this case, the visualization is often meaningless because it\nfails to illustrate the underlying characteristics that are crucial for the\nclassification process.\n  In this paper, the performance-preserving property of the previously proposed\nRestricted Radial Basis Function Network in reducing the dimension of labeled\ndata is explained. Here, it is argued through empirical experiments that the\ninternal representation of the Restricted Radial Basis Function Network, which\nduring the supervised learning process organizes a visualizable two dimensional\nmap, does not only preserve the topographical structure of high dimensional\ndata but also captures their class neighborhood structures that are important\nfor classifying them. Hence, unlike many of the existing dimension reduction\nmethods, the Restricted Radial Basis Function Network offers two dimensional\nvisualization that is strongly correlated with the classification process.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 06:32:01 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hartono", "Pitoyo", ""]]}, {"id": "1508.01006", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang and Dong Wang", "title": "Relation Classification via Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has gained much success in sentence-level relation\nclassification. For example, convolutional neural networks (CNN) have delivered\ncompetitive performance without much effort on feature engineering as the\nconventional pattern-based methods. Thus a lot of works have been produced\nbased on CNN structures. However, a key issue that has not been well addressed\nby the CNN-based method is the lack of capability to learn temporal features,\nespecially long-distance dependency between nominal pairs. In this paper, we\npropose a simple framework based on recurrent neural networks (RNN) and compare\nit with CNN-based model. To show the limitation of popular used SemEval-2010\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\n2014). Experiments on two different datasets strongly indicates that the\nRNN-based model can deliver better performance on relation classification, and\nit is particularly capable of learning long-distance relation patterns. This\nmakes it suitable for real-world applications where complicated expressions are\noften involved.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:03:46 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 03:51:00 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Zhang", "Dongxu", ""], ["Wang", "Dong", ""]]}, {"id": "1508.01008", "submitter": "Jaeyong Chung", "authors": "Jaeyong Chung, Taehwan Shin, Yongshin Kang", "title": "INsight: A Neuromorphic Computing System for Evaluation of Large Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been demonstrated impressive results in various\ncognitive tasks such as object detection and image classification. In order to\nexecute large networks, Von Neumann computers store the large number of weight\nparameters in external memories, and processing elements are timed-shared,\nwhich leads to power-hungry I/O operations and processing bottlenecks. This\npaper describes a neuromorphic computing system that is designed from the\nground up for the energy-efficient evaluation of large-scale neural networks.\nThe computing system consists of a non-conventional compiler, a neuromorphic\narchitecture, and a space-efficient microarchitecture that leverages existing\nintegrated circuit design methodologies. The compiler factorizes a trained,\nfeedforward network into a sparsely connected network, compresses the weights\nlinearly, and generates a time delay neural network reducing the number of\nconnections. The connections and units in the simplified network are mapped to\nsilicon synapses and neurons. We demonstrate an implementation of the\nneuromorphic computing system based on a field-programmable gate array that\nperforms the MNIST hand-written digit classification with 97.64% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:11:06 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Chung", "Jaeyong", ""], ["Shin", "Taehwan", ""], ["Kang", "Yongshin", ""]]}, {"id": "1508.01011", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang, Tianyi Luo, Dong Wang and Rong Liu", "title": "Learning from LDA using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:22:25 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Zhang", "Dongxu", ""], ["Luo", "Tianyi", ""], ["Wang", "Dong", ""], ["Liu", "Rong", ""]]}, {"id": "1508.01084", "submitter": "Fabio Anselmi", "authors": "Fabio Anselmi, Lorenzo Rosasco, Cheston Tan, Tomaso Poggio", "title": "Deep Convolutional Networks are Hierarchical Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In i-theory a typical layer of a hierarchical architecture consists of HW\nmodules pooling the dot products of the inputs to the layer with the\ntransformations of a few templates under a group. Such layers include as\nspecial cases the convolutional layers of Deep Convolutional Networks (DCNs) as\nwell as the non-convolutional layers (when the group contains only the\nidentity). Rectifying nonlinearities -- which are used by present-day DCNs --\nare one of the several nonlinearities admitted by i-theory for the HW module.\nWe discuss here the equivalence between group averages of linear combinations\nof rectifying nonlinearities and an associated kernel. This property implies\nthat present-day DCNs can be exactly equivalent to a hierarchy of kernel\nmachines with pooling and non-pooling layers. Finally, we describe a conjecture\nfor theoretically understanding hierarchies of such modules. A main consequence\nof the conjecture is that hierarchies of trained HW modules minimize memory\nrequirements while computing a selective and invariant representation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:18:17 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Anselmi", "Fabio", ""], ["Rosasco", "Lorenzo", ""], ["Tan", "Cheston", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1508.01168", "submitter": "Tung T. Vu", "authors": "Tung T. Vu, Ha Hoang Kha, Trung Q. Duong, Nguyen-Son Vo", "title": "Particle Swarm Optimization for Weighted Sum Rate Maximization in MIMO\n  Broadcast Channels", "comments": "submitted to Wireless Personal Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NE math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the downlink multiple-input-multipleoutput\n(MIMO) broadcast channels in which a base transceiver station (BTS) broadcasts\nmultiple data streams to K MIMO mobile stations (MSs) simultaneously. In order\nto maximize the weighted sum-rate (WSR) of the system subject to the\ntransmitted power constraint, the design problem is to find the pre-coding\nmatrices at BTS and the decoding matrices at MSs. However, such a design\nproblem is typically a nonlinear and nonconvex optimization and, thus, it is\nquite hard to obtain the analytical solutions. To tackle with the mathematical\ndifficulties, we propose an efficient stochastic optimization algorithm to\noptimize the transceiver matrices. Specifically, we utilize the linear minimum\nmean square error (MMSE) Wiener filters at MSs. Then, we introduce the\nconstrained particle swarm optimization (PSO) algorithm to jointly optimize the\nprecoding and decoding matrices. Numerical experiments are exhibited to\nvalidate the effectiveness of the proposed algorithm in terms of convergence,\ncomputational complexity and total WSR.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 02:16:11 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Vu", "Tung T.", ""], ["Kha", "Ha Hoang", ""], ["Duong", "Trung Q.", ""], ["Vo", "Nguyen-Son", ""]]}, {"id": "1508.01211", "submitter": "Navdeep Jaitly", "authors": "William Chan and Navdeep Jaitly and Quoc V. Le and Oriol Vinyals", "title": "Listen, Attend and Spell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:17:58 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 00:38:43 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Chan", "William", ""], ["Jaitly", "Navdeep", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1508.01310", "submitter": "Elham Shadkam", "authors": "Afsane Akbarzadeh, Elham Shadkam", "title": "The study of cuckoo optimization algorithm for production planning\n  problem", "comments": null, "journal-ref": null, "doi": "10.5121/ijcax.2015.2301", "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Nonlinear programming problems are hard problems, and one of the\nmost widely used and common problems for production planning problem to\noptimize. In this study, one of the mathematical models of production planning\nis survey and the problem solved by cuckoo algorithm. Cuckoo Algorithm is\nefficient method to solve continues non linear problem. Moreover, mentioned\nmodels of production planning solved with Genetic algorithm and Lingo software\nand the results will compared. The Cuckoo Algorithm is suitable choice for\noptimization in convergence of solution\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 08:08:45 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Akbarzadeh", "Afsane", ""], ["Shadkam", "Elham", ""]]}, {"id": "1508.01887", "submitter": "Zhanglin Peng", "authors": "Zhanglin Peng, Ya Li, Zhaoquan Cai and Liang Lin", "title": "Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning\n  in Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how the traditional image classification pipelines can\nbe extended into a deep architecture, inspired by recent successes of deep\nneural networks. We propose a deep boosting framework based on layer-by-layer\njoint feature boosting and dictionary learning. In each layer, we construct a\ndictionary of filters by combining the filters from the lower layer, and\niteratively optimize the image representation with a joint\ndiscriminative-generative formulation, i.e. minimization of empirical\nclassification error plus regularization of analysis image generation over\ntraining images. For optimization, we perform two iterating steps: i) to\nminimize the classification error, select the most discriminative features\nusing the gentle adaboost algorithm; ii) according to the feature selection,\nupdate the filters to minimize the regularization on analysis image\nrepresentation using the gradient descent method. Once the optimization is\nconverged, we learn the higher layer representation in the same way. Our model\ndelivers several distinct advantages. First, our layer-wise optimization\nprovides the potential to build very deep architectures. Second, the generated\nimage representation is compact and meaningful. In several visual recognition\ntasks, our framework outperforms existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 11:42:21 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 09:45:07 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Peng", "Zhanglin", ""], ["Li", "Ya", ""], ["Cai", "Zhaoquan", ""], ["Lin", "Liang", ""]]}, {"id": "1508.02354", "submitter": "Dimitri Kartsaklis", "authors": "Jianpeng Cheng and Dimitri Kartsaklis", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models\n  of Meaning", "comments": "Accepted for presentation at EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep compositional models of meaning acting on distributional representations\nof words in order to produce vectors of larger text constituents are evolving\nto a popular area of NLP research. We detail a compositional distributional\nframework based on a rich form of word embeddings that aims at facilitating the\ninteractions between words in the context of a sentence. Embeddings and\ncomposition layers are jointly learned against a generic objective that\nenhances the vectors with syntactic information from the surrounding context.\nFurthermore, each word is associated with a number of senses, the most\nplausible of which is selected dynamically during the composition process. We\nevaluate the produced vectors qualitatively and quantitatively with positive\nresults. At the sentence level, the effectiveness of the framework is\ndemonstrated on the MSRPar task, for which we report results within the\nstate-of-the-art range.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 19:04:18 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:50:43 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Kartsaklis", "Dimitri", ""]]}, {"id": "1508.02505", "submitter": "Maryam Keyvanara", "authors": "Maryam Keyvanara, Seyed Amirhassan Monadjemi", "title": "Simulating Brain Reaction to Methamphetamine Regarding Consumer\n  Personality", "comments": "10 Pages, 4 Figures, Journal Paper", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA) Vol. 6, No. 4, July 2015, pp. 63-72", "doi": "10.5121/ijaia.2015.6406", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addiction, as a nervous disease, can be analysed using mathematical modelling\nand computer simulations. In this paper, we use an existing mathematical model\nto predict and simulate human brain response to the consumption of a single\ndose of methamphetamine. The model is implemented and coded in Matlab. Three\ntypes of personalities including introverts, ambiverts and extroverts are\nstudied. The parameters of the mathematical model are calibrated and optimized,\naccording to psychological theories, using a real coded genetic algorithm. The\nsimulations show significant correlation between people response to\nmethamphetamine abuse and their personality. They also show that one of the\ncauses of tendency to stimulants roots in consumers personality traits. The\nresults can be used as a tool for reducing attitude towards addiction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 07:44:18 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Keyvanara", "Maryam", ""], ["Monadjemi", "Seyed Amirhassan", ""]]}, {"id": "1508.02521", "submitter": "Sajid Ullah", "authors": "Sajid Ullah, Mussarat Wahid", "title": "Topology Control of wireless sensor network using Quantum Inspired\n  Genetic algorithm", "comments": "4 Figures/6 pages", "journal-ref": "International Journal of Swarm Intelligence and Evolutionary\n  Computation :2015", "doi": "10.4172/2090-4908.1000121", "report-no": null, "categories": "cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, an evolving Linked Quantum register has been introduced, which\nare group vector of binary pair of genes, which in its local proximity\nrepresent those nodes that will have high connectivity and keep the energy\nconsumption at low, and which are taken into account for topology control. The\nregister works in higher dimension. Here order-2 Quantum inspired genetic\nalgorithm has been used and also higher order can be used to achieve greater\nversatility in topology control of nodes. Numerical result has been obtained,\nanalysis is done as how the result has previously been obtained with Quantum\ngenetic algorithm and results are compared too. For future work, factor is\nhinted which would exploit the algorithm to work in more computational\nintensive problem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 08:53:06 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 14:01:52 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 19:11:15 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ullah", "Sajid", ""], ["Wahid", "Mussarat", ""]]}, {"id": "1508.02774", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "Benchmarking of LSTM Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTM (Long Short-Term Memory) recurrent neural networks have been highly\nsuccessful in a number of application areas. This technical report describes\nthe use of the MNIST and UW3 databases for benchmarking LSTM networks and\nexplores the effect of different architectural and hyperparameter choices on\nperformance. Significant findings include: (1) LSTM performance depends\nsmoothly on learning rates, (2) batching and momentum has no significant effect\non performance, (3) softmax training outperforms least square training, (4)\npeephole units are not useful, (5) the standard non-linearities (tanh and\nsigmoid) perform best, (6) bidirectional training combined with CTC performs\nbetter than other methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 23:31:49 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.02788", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "The Effects of Hyperparameters on SGD Training of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of neural network classifiers is determined by a number of\nhyperparameters, including learning rate, batch size, and depth. A number of\nattempts have been made to explore these parameters in the literature, and at\ntimes, to develop methods for optimizing them. However, exploration of\nparameter spaces has often been limited. In this note, I report the results of\nlarge scale experiments exploring these different parameters and their\ninteractions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 01:01:11 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.02790", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "On the Convergence of SGD Training of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are usually trained by some form of stochastic gradient\ndescent (SGD)). A number of strategies are in common use intended to improve\nSGD optimization, such as learning rate schedules, momentum, and batching.\nThese are motivated by ideas about the occurrence of local minima at different\nscales, valleys, and other phenomena in the objective function. Empirical\nresults presented here suggest that these phenomena are not significant factors\nin SGD optimization of MLP-related objective functions, and that the behavior\nof stochastic gradient descent in these problems is better described as the\nsimultaneous convergence at different rates of many, largely non-interacting\nsubproblems\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 01:11:47 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.02792", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "Possible Mechanisms for Neural Reconfigurability and their Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a biologically and evolutionarily plausible neural\narchitecture that allows a single group of neurons, or an entire cortical\npathway, to be dynamically reconfigured to perform multiple, potentially very\ndifferent computations. The paper shows that reconfigurability can account for\nthe observed stochastic and distributed coding behavior of neurons and provides\na parsimonious explanation for timing phenomena in psychophysical experiments.\nIt also shows that reconfigurable pathways correspond to classes of statistical\nclassifiers that include decision lists, decision trees, and hierarchical\nBayesian methods. Implications for the interpretation of neurophysiological and\npsychophysical results are discussed, and future experiments for testing the\nreconfigurability hypothesis are explored.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 01:23:35 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.03174", "submitter": "Victor Hernandez-Urbina", "authors": "Victor Hernandez-Urbina", "title": "Logical N-AND Gate on a Molecular Turing Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Boolean algebra, it is known that the logical function that corresponds to\nthe negation of the conjunction --NAND-- is universal in the sense that any\nother logical function can be built based on it. This property makes it\nessential to modern digital electronics and computer processor design. Here, we\ndesign a molecular Turing machine that computes the NAND function over binary\nstrings of arbitrary length. For this purpose, we will perform a mathematical\nabstraction of the kind of operations that can be done over a double-stranded\nDNA molecule, as well as presenting a molecular encoding of the input symbols\nfor such a machine.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 11:08:28 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Hernandez-Urbina", "Victor", ""]]}, {"id": "1508.03606", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar and Johannes Rauh", "title": "Hierarchical Models as Marginals of Hierarchical Models", "comments": "18 pages, 4 figures, 2 tables, WUPES'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the representation of hierarchical models in terms of\nmarginals of other hierarchical models with smaller interactions. We focus on\nbinary variables and marginals of pairwise interaction models whose hidden\nvariables are conditionally independent given the visible variables. In this\ncase the problem is equivalent to the representation of linear subspaces of\npolynomials by feedforward neural networks with soft-plus computational units.\nWe show that every hidden variable can freely model multiple interactions among\nthe visible variables, which allows us to generalize and improve previous\nresults. In particular, we show that a restricted Boltzmann machine with less\nthan $[ 2(\\log(v)+1) / (v+1) ] 2^v-1$ hidden binary variables can approximate\nevery distribution of $v$ visible binary variables arbitrarily well, compared\nto $2^{v-1}-1$ from the best previously known result.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:56:00 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 19:48:07 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Montufar", "Guido", ""], ["Rauh", "Johannes", ""]]}, {"id": "1508.03790", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao, Trevor Cohn, Katerina Vylomova, Kevin Duh, and Chris\n  Dyer", "title": "Depth-Gated LSTM", "comments": "Content presented in 2015 Jelinek Summer Workshop on Speech and\n  Language Technology on August 14th 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we present an extension of long short-term memory (LSTM)\nneural networks to using a depth gate to connect memory cells of adjacent\nlayers. Doing so introduces a linear dependence between lower and upper layer\nrecurrent units. Importantly, the linear dependence is gated through a gating\nfunction, which we call depth gate. This gate is a function of the lower layer\nmemory cell, the input to and the past memory cell of this layer. We conducted\nexperiments and verified that this new architecture of LSTMs was able to\nimprove machine translation and language modeling performances.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 04:31:37 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 19:38:58 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2015 07:13:04 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2015 04:24:20 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Yao", "Kaisheng", ""], ["Cohn", "Trevor", ""], ["Vylomova", "Katerina", ""], ["Duh", "Kevin", ""], ["Dyer", "Chris", ""]]}, {"id": "1508.03854", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Online Representation Learning in Recurrent Neural Language Models", "comments": "In Proceedings of EMNLP 2015", "journal-ref": null, "doi": "10.18653/v1/D15-1026", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an extension of continuous online learning in recurrent neural\nnetwork language models. The model keeps a separate vector representation of\nthe current unit of text being processed and adaptively adjusts it after each\nprediction. The initial experiments give promising results, indicating that the\nmethod is able to increase language modelling accuracy, while also decreasing\nthe parameters needed to store the model along with the computation required at\neach step.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 18:27:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1508.04186", "submitter": "Hao Yi Ong", "authors": "Hao Yi Ong, Kevin Chavez, Augustus Hong", "title": "Distributed Deep Q-Learning", "comments": "Updated figure of distributed deep learning architecture, updated\n  content throughout paper including dealing with minor grammatical issues and\n  highlighting differences of our paper with respect to prior work. arXiv admin\n  note: text overlap with arXiv:1312.5602 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is based on the deep Q-network, a convolutional neural\nnetwork trained with a variant of Q-learning. Its input is raw pixels and its\noutput is a value function estimating future rewards from taking an action\ngiven a system state. To distribute the deep Q-network training, we adapt the\nDistBelief software framework to the context of efficiently training\nreinforcement learning agents. As a result, the method is completely\nasynchronous and scales well with the number of machines. We demonstrate that\nthe deep Q-network agent, receiving only the pixels and the game score as\ninputs, was able to achieve reasonable success on a simple game with minimal\nparameter tuning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 01:00:32 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 09:06:38 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Ong", "Hao Yi", ""], ["Chavez", "Kevin", ""], ["Hong", "Augustus", ""]]}, {"id": "1508.04306", "submitter": "John Hershey", "authors": "John R. Hershey, Zhuo Chen, Jonathan Le Roux, Shinji Watanabe", "title": "Deep clustering: Discriminative embeddings for segmentation and\n  separation", "comments": "Originally submitted on June 5, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of acoustic source separation in a deep learning\nframework we call \"deep clustering.\" Rather than directly estimating signals or\nmasking functions, we train a deep network to produce spectrogram embeddings\nthat are discriminative for partition labels given in training data. Previous\ndeep network approaches provide great advantages in terms of learning power and\nspeed, but previously it has been unclear how to use them to separate signals\nin a class-independent way. In contrast, spectral clustering approaches are\nflexible with respect to the classes and number of items to be segmented, but\nit has been unclear how to leverage the learning power and speed of deep\nnetworks. To obtain the best of both worlds, we use an objective function that\nto train embeddings that yield a low-rank approximation to an ideal pairwise\naffinity matrix, in a class-independent way. This avoids the high cost of\nspectral factorization and instead produces compact clusters that are amenable\nto simple clustering methods. The segmentations are therefore implicitly\nencoded in the embeddings, and can be \"decoded\" by clustering. Preliminary\nexperiments show that the proposed method can separate speech: when trained on\nspectrogram features containing mixtures of two speakers, and tested on\nmixtures of a held-out set of speakers, it can infer masking functions that\nimprove signal quality by around 6dB. We show that the model can generalize to\nthree-speaker mixtures despite training only on two-speaker mixtures. The\nframework can be used without class labels, and therefore has the potential to\nbe trained on a diverse set of sound types, and to generalize to novel sources.\nWe hope that future work will lead to segmentation of arbitrary sounds, with\nextensions to microphone array methods as well as image segmentation and other\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 13:12:34 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hershey", "John R.", ""], ["Chen", "Zhuo", ""], ["Roux", "Jonathan Le", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1508.04395", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel,\n  Yoshua Bengio", "title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 17:40:00 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 23:07:20 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Chorowski", "Jan", ""], ["Serdyuk", "Dmitriy", ""], ["Brakel", "Philemon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1508.04422", "submitter": "Vince Lyzinski", "authors": "Aren Jansen, Gregory Sell, Vince Lyzinski", "title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural\n  Networks", "comments": "10 pages, 2 figures, 1 table, this paper is under consideration for\n  publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several popular graph embedding techniques for representation learning and\ndimensionality reduction rely on performing computationally expensive\neigendecompositions to derive a nonlinear transformation of the input data\nspace. The resulting eigenvectors encode the embedding coordinates for the\ntraining samples only, and so the embedding of novel data samples requires\nfurther costly computation. In this paper, we present a method for the\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\nparametrically approximate these nonlinear maps. Compared with traditional\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\ngeneralize with equal or better fidelity and require orders of magnitude less\ncomputation at test time. Moreover, we find that unsupervised pretraining of\nthe DNNs improves optimization for larger network sizes, thus removing\nsensitivity to model selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 19:47:31 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 16:07:53 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 15:50:41 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Jansen", "Aren", ""], ["Sell", "Gregory", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1508.04561", "submitter": "Wlodzislaw Duch", "authors": "W{\\l}odzis{\\l}aw Duch", "title": "Memetics and Neural Models of Conspiracy Theories", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conspiracy theories, or in general seriously distorted beliefs, are\nwidespread. How and why are they formed in the brain is still more a matter of\nspeculation rather than science. In this paper one plausible mechanisms is\ninvestigated: rapid freezing of high neuroplasticity (RFHN). Emotional arousal\nincreases neuroplasticity and leads to creation of new pathways spreading\nneural activation. Using the language of neurodynamics a meme is defined as\nquasi-stable associative memory attractor state. Depending on the temporal\ncharacteristics of the incoming information and the plasticity of the network,\nmemory may self-organize creating memes with large attractor basins, linking\nmany unrelated input patterns. Memes with fake rich associations distort\nrelations between memory states. Simulations of various neural network models\ntrained with competitive Hebbian learning (CHL) on stationary and\nnon-stationary data lead to the same conclusion: short learning with high\nplasticity followed by rapid decrease of plasticity leads to memes with large\nattraction basins, distorting input pattern representations in associative\nmemory. Such system-level models may be used to understand creation of\ndistorted beliefs and formation of conspiracy memes, understood as strong\nattractor states of the neurodynamics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:20:17 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 17:38:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Duch", "W\u0142odzis\u0142aw", ""]]}, {"id": "1508.05128", "submitter": "Ondrej Kuzelka", "authors": "Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Ondrej Kuzelka", "title": "Lifted Relational Neural Networks", "comments": "Expanded section on weight learning, added explanation of\n  relationship to convolutional neural networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method combining relational-logic representations with neural\nnetwork learning. A general lifted architecture, possibly reflecting some\nbackground domain knowledge, is described through relational rules which may be\nhandcrafted or learned. The relational rule-set serves as a template for\nunfolding possibly deep neural networks whose structures also reflect the\nstructures of given training or testing relational examples. Different networks\ncorresponding to different examples share their weights, which co-evolve during\ntraining by stochastic gradient descent algorithm. The framework allows for\nhierarchical relational modeling constructs and learning of latent relational\nconcepts through shared hidden layers weights corresponding to the rules.\nDiscovery of notable relational concepts and experiments on 78 relational\nlearning benchmarks demonstrate favorable performance of the method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 21:18:25 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 12:55:45 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Sourek", "Gustav", ""], ["Aschenbrenner", "Vojtech", ""], ["Zelezny", "Filip", ""], ["Kuzelka", "Ondrej", ""]]}, {"id": "1508.05133", "submitter": "Tamir Hazan", "authors": "Tamir Hazan and Tommi Jaakkola", "title": "Steps Toward Deep Kernel Methods from Infinite Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary deep neural networks exhibit impressive results on practical\nproblems. These networks generalize well although their inherent capacity may\nextend significantly beyond the number of training examples. We analyze this\nbehavior in the context of deep, infinite neural networks. We show that deep\ninfinite layers are naturally aligned with Gaussian processes and kernel\nmethods, and devise stochastic kernels that encode the information of these\nnetworks. We show that stability results apply despite the size, offering an\nexplanation for their empirical success.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 21:35:52 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 18:27:36 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Hazan", "Tamir", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1508.05342", "submitter": "No\\'e Casas", "authors": "Noe Casas", "title": "Genetic Algorithms for multimodal optimization: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we provide a comprehensive review of the different\nevolutionary algorithm techniques used to address multimodal optimization\nproblems, classifying them according to the nature of their approach. On the\none hand there are algorithms that address the issue of the early convergence\nto a local optimum by differentiating the individuals of the population into\ngroups and limiting their interaction, hence having each group evolve with a\nhigh degree of independence. On the other hand other approaches are based on\ndirectly addressing the lack of genetic diversity of the population by\nintroducing elements into the evolutionary dynamics that promote new niches of\nthe genotypical space to be explored. Finally, we study multi-objective\noptimization genetic algorithms, that handle the situations where multiple\ncriteria have to be satisfied with no penalty for any of them. Very rich\nliterature has arised over the years on these topics, and we aim at offering an\noverview of the most important techniques of each branch of the field.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 15:29:25 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Casas", "Noe", ""]]}, {"id": "1508.05463", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, and Alexander Wong", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks is a branch in machine learning that has seen a meteoric\nrise in popularity due to its powerful abilities to represent and model\nhigh-level abstractions in highly complex data. One area in deep neural\nnetworks that is ripe for exploration is neural connectivity formation. A\npivotal study on the brain tissue of rats found that synaptic formation for\nspecific functional connectivity in neocortical neural microcircuits can be\nsurprisingly well modeled and predicted as a random formation. Motivated by\nthis intriguing finding, we introduce the concept of StochasticNet, where deep\nneural networks are formed via stochastic connectivity between neurons. As a\nresult, any type of deep neural networks can be formed as a StochasticNet by\nallowing the neuron connectivity to be stochastic. Stochastic synaptic\nformations, in a deep neural network architecture, can allow for efficient\nutilization of neurons for performing specific tasks. To evaluate the\nfeasibility of such a deep neural network architecture, we train a\nStochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and\nSTL-10). Experimental results show that a StochasticNet, using less than half\nthe number of neural connections as a conventional deep neural network,\nachieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and\nSVHN dataset. Interestingly, StochasticNet with less than half the number of\nneural connections, achieved a higher accuracy (relative improvement in test\nerror rate of ~6% compared to ConvNet) on the STL-10 dataset than a\nconventional deep neural network. Finally, StochasticNets have faster\noperational speeds while achieving better or similar accuracy performances.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 03:36:43 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 19:05:03 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 01:34:17 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 20:30:05 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Wong", "Alexander", ""]]}, {"id": "1508.05508", "submitter": "Baolin Peng", "authors": "Baolin Peng, Zhengdong Lu, Hang Li and Kam-Fai Wong", "title": "Towards Neural Network-based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Reasoner, a framework for neural network-based reasoning\nover natural language sentences. Given a question, Neural Reasoner can infer\nover multiple supporting facts and find an answer to the question in specific\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\nto model the complicated logical relations in reasoning tasks. Assuming no\nparticular structure exists in the question and facts, Neural Reasoner is able\nto accommodate different types of reasoning and different forms of language\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\neffectively in an end-to-end manner. Our empirical studies show that Neural\nReasoner can outperform existing neural reasoning systems with remarkable\nmargins on two difficult artificial tasks (Positional Reasoning and Path\nFinding) proposed in [8]. For example, it improves the accuracy on Path\nFinding(10K) from 33.4% [6] to over 98%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:15:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Peng", "Baolin", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1508.05752", "submitter": "Witold Bo{\\l}t", "authors": "Witold Bo{\\l}t, Jan M. Baetens and Bernard De Baets", "title": "An evolutionary approach to the identification of Cellular Automata\n  based on partial observations", "comments": "IEEE CEC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the identification problem of Cellular Automata\n(CAs). The problem is defined and solved in the context of partial observations\nwith time gaps of unknown length, i.e. pre-recorded, partial configurations of\nthe system at certain, unknown time steps. A solution method based on a\nmodified variant of a Genetic Algorithm (GA) is proposed and illustrated with\nbrief experimental results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 10:47:47 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Bo\u0142t", "Witold", ""], ["Baetens", "Jan M.", ""], ["De Baets", "Bernard", ""]]}, {"id": "1508.06092", "submitter": "Rossella Cancelliere", "authors": "R. Cancelliere and R. Deluca and M. Gai and P. Gallinari and L. Rubini", "title": "An analysis of numerical issues in neural training by pseudoinversion", "comments": "11 pages, submitted to: Comp. Appl. Math", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some novel strategies have recently been proposed for single hidden layer\nneural network training that set randomly the weights from input to hidden\nlayer, while weights from hidden to output layer are analytically determined by\npseudoinversion. These techniques are gaining popularity in spite of their\nknown numerical issues when singular and/or almost singular matrices are\ninvolved. In this paper we discuss a critical use of Singular Value Analysis\nfor identification of these drawbacks and we propose an original use of\nregularisation to determine the output weights, based on the concept of\ncritical hidden layer size. This approach also allows to limit the training\ncomputational effort. Besides, we introduce a novel technique which relies an\neffective determination of input weights to the hidden layer dimension. This\napproach is tested for both regression and classification tasks, resulting in a\nsignificant performance improvement with respect to alternative methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 09:51:35 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cancelliere", "R.", ""], ["Deluca", "R.", ""], ["Gai", "M.", ""], ["Gallinari", "P.", ""], ["Rubini", "L.", ""]]}, {"id": "1508.06095", "submitter": "Rossella Cancelliere", "authors": "Rossella Cancelliere, Mario Gai, Patrick Gallinari, Luca Rubini", "title": "OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based\n  Neural Training", "comments": "Published on Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2015.07.015", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the training of single hidden layer neural networks\nby pseudoinversion, which, in spite of its popularity, is sometimes affected by\nnumerical instability issues. Regularization is known to be effective in such\ncases, so that we introduce, in the framework of Tikhonov regularization, a\nmatricial reformulation of the problem which allows us to use the condition\nnumber as a diagnostic tool for identification of instability. By imposing\nwell-conditioning requirements on the relevant matrices, our theoretical\nanalysis allows the identification of an optimal value for the regularization\nparameter from the standpoint of stability. We compare with the value derived\nby cross-validation for overfitting control and optimisation of the\ngeneralization performance. We test our method for both regression and\nclassification tasks. The proposed method is quite effective in terms of\npredictivity, often with some improvement on performance with respect to the\nreference cases considered. This approach, due to analytical determination of\nthe regularization parameter, dramatically reduces the computational load\nrequired by many other techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 10:09:31 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cancelliere", "Rossella", ""], ["Gai", "Mario", ""], ["Gallinari", "Patrick", ""], ["Rubini", "Luca", ""]]}, {"id": "1508.06483", "submitter": "Naoki Hamada", "authors": "Naoki Hamada, Katsumi Homma, Hiroyuki Higuchi and Hideyuki Kikuchi", "title": "Population Synthesis via k-Nearest Neighbor Crossover Kernel", "comments": "10 pages, 4 figures, IEEE International Conference on Data Mining\n  (ICDM) 2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.65", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of multi-agent simulations brings about a need for\npopulation synthesis. It is a task of reconstructing the entire population from\na sampling survey of limited size (1% or so), supplying the initial conditions\nfrom which simulations begin. This paper presents a new kernel density\nestimator for this task. Our method is an analogue of the classical\nBreiman-Meisel-Purcell estimator, but employs novel techniques that harness the\nhuge degree of freedom which is required to model high-dimensional nonlinearly\ncorrelated datasets: the crossover kernel, the k-nearest neighbor restriction\nof the kernel construction set and the bagging of kernels. The performance as a\nstatistical estimator is examined through real and synthetic datasets. We\nprovide an \"optimization-free\" parameter selection rule for our method, a\ntheory of how our method works and a computational cost analysis. To\ndemonstrate the usefulness as a population synthesizer, our method is applied\nto a household synthesis task for an urban micro-simulator.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 13:22:37 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hamada", "Naoki", ""], ["Homma", "Katsumi", ""], ["Higuchi", "Hiroyuki", ""], ["Kikuchi", "Hideyuki", ""]]}, {"id": "1508.06535", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Deep Convolutional Neural Networks for Smile Recognition", "comments": "MSc thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis describes the design and implementation of a smile detector based\non deep convolutional neural networks. It starts with a summary of neural\nnetworks, the difficulties of training them and new training methods, such as\nRestricted Boltzmann Machines or autoencoders. It then provides a literature\nreview of convolutional neural networks and recurrent neural networks. In order\nto select databases for smile recognition, comprehensive statistics of\ndatabases popular in the field of facial expression recognition were generated\nand are summarized in this thesis. It then proposes a model for smile\ndetection, of which the main part is implemented. The experimental results are\ndiscussed in this thesis and justified based on a comprehensive model selection\nperformed. All experiments were run on a Tesla K40c GPU benefiting from a\nspeedup of up to factor 10 over the computations on a CPU. A smile detection\ntest accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous\nFacial Action (DISFA) database, significantly outperforming existing approaches\nwith accuracies ranging from 65.55% to 79.67%. This experiment is re-run under\nvarious variations, such as retaining less neutral images or only the low or\nhigh intensities, of which the results are extensively compared.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 15:39:09 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1508.06538", "submitter": "Hector Zenil", "authors": "Hector Zenil, Angelika Schmidt and Jesper Tegn\\'er", "title": "Causality, Information and Biological Computation: An algorithmic\n  software approach to life, disease and the immune system", "comments": "30 pages, 8 figures. Invited chapter contribution to Information and\n  Causality: From Matter to Life. Sara I. Walker, Paul C.W. Davies and George\n  Ellis (eds.), Cambridge University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biology has taken strong steps towards becoming a computer science aiming at\nreprogramming nature after the realisation that nature herself has reprogrammed\norganisms by harnessing the power of natural selection and the digital\nprescriptive nature of replicating DNA. Here we further unpack ideas related to\ncomputability, algorithmic information theory and software engineering, in the\ncontext of the extent to which biology can be (re)programmed, and with how we\nmay go about doing so in a more systematic way with all the tools and concepts\noffered by theoretical computer science in a translation exercise from\ncomputing to molecular biology and back. These concepts provide a means to a\nhierarchical organization thereby blurring previously clear-cut lines between\nconcepts like matter and life, or between tumour types that are otherwise taken\nas different and may not have however a different cause. This does not diminish\nthe properties of life or make its components and functions less interesting.\nOn the contrary, this approach makes for a more encompassing and integrated\nview of nature, one that subsumes observer and observed within the same system,\nand can generate new perspectives and tools with which to view complex diseases\nlike cancer, approaching them afresh from a software-engineering viewpoint that\ncasts evolution in the role of programmer, cells as computing machines, DNA and\ngenes as instructions and computer programs, viruses as hacking devices, the\nimmune system as a software debugging tool, and diseases as an\ninformation-theoretic battlefield where all these forces deploy. We show how\ninformation theory and algorithmic programming may explain fundamental\nmechanisms of life and death.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 19:38:03 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 10:06:12 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 01:03:18 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2016 00:55:07 GMT"}, {"version": "v5", "created": "Wed, 20 Jan 2016 01:54:15 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Zenil", "Hector", ""], ["Schmidt", "Angelika", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1508.06576", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker and Matthias Bethge", "title": "A Neural Algorithm of Artistic Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:14:42 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 08:24:59 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1508.06585", "submitter": "Galin Georgiev", "authors": "Galin Georgiev", "title": "Towards universal neural nets: Gibbs machines and ACE", "comments": "v5: added thermodynamic identities and variational error estimation;\n  expanded references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study from a physics viewpoint a class of generative neural nets, Gibbs\nmachines, designed for gradual learning. While including variational\nauto-encoders, they offer a broader universal platform for incrementally adding\nnewly learned features, including physical symmetries. Their direct connection\nto statistical physics and information geometry is established. A variational\nPythagorean theorem justifies invoking the exponential/Gibbs class of\nprobabilities for creating brand new objects. Combining these nets with\nclassifiers, gives rise to a brand of universal generative neural nets -\nstochastic auto-classifier-encoders (ACE). ACE have state-of-the-art\nperformance in their class, both for classification and density estimation for\nthe MNIST data set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:43:08 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 21:49:06 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2015 03:35:59 GMT"}, {"version": "v4", "created": "Fri, 8 Apr 2016 22:11:23 GMT"}, {"version": "v5", "created": "Thu, 30 Jun 2016 06:26:34 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Georgiev", "Galin", ""]]}, {"id": "1508.06586", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Financial Market Modeling with Quantum Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.NE physics.soc-ph q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Econophysics has developed as a research field that applies the formalism of\nStatistical Mechanics and Quantum Mechanics to address Economics and Finance\nproblems. The branch of Econophysics that applies of Quantum Theory to\nEconomics and Finance is called Quantum Econophysics. In Finance, Quantum\nEconophysics' contributions have ranged from option pricing to market dynamics\nmodeling, behavioral finance and applications of Game Theory, integrating the\nempirical finding, from human decision analysis, that shows that nonlinear\nupdate rules in probabilities, leading to non-additive decision weights, can be\ncomputationally approached from quantum computation, with resulting quantum\ninterference terms explaining the non-additive probabilities. The current work\ndraws on these results to introduce new tools from Quantum Artificial\nIntelligence, namely Quantum Artificial Neural Networks as a way to build and\nsimulate financial market models with adaptive selection of trading rules,\nleading to turbulence and excess kurtosis in the returns distributions for a\nwide range of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:49:14 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1508.06615", "submitter": "Yoon Kim", "authors": "Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush", "title": "Character-Aware Neural Language Models", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 19:25:34 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 23:18:00 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 03:18:13 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2015 22:59:24 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Kim", "Yoon", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1508.06705", "submitter": "Sekou Remy", "authors": "Jeff Kinnison and Sekou L. Remy", "title": "Using Genetic Algorithms to Benchmark the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": "Sp2015M03", "categories": "cs.DC cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel application of Genetic Algorithms(GAs) to\nquantify the performance of Platform as a Service (PaaS), a cloud service model\nthat plays a critical role in both industry and academia. While Cloud\nbenchmarks are not new, in this novel concept, the authors use a GA to take\nadvantage of the elasticity in Cloud services in a graceful manner that was not\npreviously possible. Using Google App Engine, Heroku, and Python Anywhere with\nthree distinct classes of client computers running our GA codebase, we\nquantified the completion time for application of the GA to search for the\nparameters of controllers for dynamical systems. Our results show statistically\nsignificant differences in PaaS performance by vendor, and also that the\nperformance of the PaaS performance is dependent upon the client that uses it.\nResults also show the effectiveness of our GA in determining the level of\nservice of PaaS providers, and for determining if the level of service of one\nPaaS vendor is repeatable with another. Such a concept could then increase the\nappeal of PaaS Cloud services by making them more financially appealing.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 03:09:08 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Kinnison", "Jeff", ""], ["Remy", "Sekou L.", ""]]}, {"id": "1508.06802", "submitter": "Johannes Lengler", "authors": "Carola Doerr, Johannes Lengler", "title": "Introducing Elitist Black-Box Models: When Does Elitist Selection Weaken\n  the Performance of Evolutionary Algorithms?", "comments": "A short version of this work has been presented at the GECCO\n  conference 2015 in Madrid, Spain. Available at\n  http://dl.acm.org/citation.cfm?doid=2739480.2754654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box complexity theory provides lower bounds for the runtime of\nblack-box optimizers like evolutionary algorithms and serves as an inspiration\nfor the design of new genetic algorithms. Several black-box models covering\ndifferent classes of algorithms exist, each highlighting a different aspect of\nthe algorithms under considerations. In this work we add to the existing\nblack-box notions a new \\emph{elitist black-box model}, in which algorithms are\nrequired to base all decisions solely on (a fixed number of) the best search\npoints sampled so far. Our model combines features of the ranking-based and the\nmemory-restricted black-box models with elitist selection.\n  We provide several examples for which the elitist black-box complexity is\nexponentially larger than that the respective complexities in all previous\nblack-box models, thus showing that the elitist black-box complexity can be\nmuch closer to the runtime of typical evolutionary algorithms.\n  We also introduce the concept of $p$-Monte Carlo black-box complexity, which\nmeasures the time it takes to optimize a problem with failure probability at\nmost $p$. Even for small~$p$, the $p$-Monte Carlo black-box complexity of a\nfunction class $\\mathcal F$ can be smaller by an exponential factor than its\ntypically regarded Las Vegas complexity (which measures the \\emph{expected}\ntime it takes to optimize $\\mathcal F$).\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 11:17:34 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Doerr", "Carola", ""], ["Lengler", "Johannes", ""]]}, {"id": "1508.06904", "submitter": "Markus Thom", "authors": "Markus Thom and Franz Gritschneder", "title": "Rapid Exact Signal Scanning with Deep Convolutional Neural Networks", "comments": "Pages 1-16 only: Copyright (c) 2016 IEEE. Personal use is permitted,\n  but republication/redistribution requires IEEE permission", "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 5, pp.\n  1235-1250 (2017)", "doi": "10.1109/TSP.2016.2631454", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous formulation of the dynamics of a signal processing scheme aimed at\ndense signal scanning without any loss in accuracy is introduced and analyzed.\nRelated methods proposed in the recent past lack a satisfactory analysis of\nwhether they actually fulfill any exactness constraints. This is improved\nthrough an exact characterization of the requirements for a sound sliding\nwindow approach. The tools developed in this paper are especially beneficial if\nConvolutional Neural Networks are employed, but can also be used as a more\ngeneral framework to validate related approaches to signal scanning. The\nproposed theory helps to eliminate redundant computations and renders special\ncase treatment unnecessary, resulting in a dramatic boost in efficiency\nparticularly on massively parallel processors. This is demonstrated both\ntheoretically in a computational complexity analysis and empirically on modern\nparallel processors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 15:50:26 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 18:49:52 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 12:18:13 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 17:38:00 GMT"}, {"version": "v5", "created": "Wed, 2 Aug 2017 13:23:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Thom", "Markus", ""], ["Gritschneder", "Franz", ""]]}, {"id": "1508.06944", "submitter": "Yoram Burak", "authors": "Nimrod Shaham, Yoram Burak", "title": "Continuous parameter working memory in a balanced chaotic neural network", "comments": "Expanded and revised version of the manuscript. Accepted to PLoS\n  Computational Biology (2017). 29 pages, 8 figures and 4 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proposed that neural noise in the cortex arises from chaotic\ndynamics in the balanced state: in this model of cortical dynamics, the\nexcitatory and inhibitory inputs to each neuron approximately cancel, and\nactivity is driven by fluctuations of the synaptic inputs around their mean. It\nremains unclear whether neural networks in the balanced state can perform tasks\nthat are highly sensitive to noise, such as storage of continuous parameters in\nworking memory, while also accounting for the irregular behavior of single\nneurons. Here we show that continuous parameter working memory can be\nmaintained in the balanced state, in a neural circuit with a simple network\narchitecture. We show analytically that in the limit of an infinite network,\nthe dynamics generated by this architecture are characterized by a continuous\nset of steady balanced states, allowing for the indefinite storage of a\ncontinuous parameter. In finite networks, we show that the chaotic noise drives\ndiffusive motion along the approximate attractor, which gradually degrades the\nstored memory. We analyze the dynamics and show that the slow diffusive motion\ninduces slowly decaying temporal cross correlations in the activity, which\ndiffer substantially from those previously described in the balanced state. We\ncalculate the diffusivity, and show that it is inversely proportional to the\nsystem size. For large enough (but realistic) neural population sizes, and with\nsuitable tuning of the network connections, the proposed balanced network can\nsustain continuous parameter values in memory over time scales larger by\nseveral orders of magnitude than the single neuron time scale.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 17:24:13 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 14:25:08 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 09:15:14 GMT"}, {"version": "v4", "created": "Thu, 27 Apr 2017 15:39:27 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Shaham", "Nimrod", ""], ["Burak", "Yoram", ""]]}, {"id": "1508.07096", "submitter": "Yanping Huang", "authors": "Yanping Huang, Sai Zhang", "title": "Partitioning Large Scale Deep Belief Networks Using Dropout", "comments": "arXiv admin note: text overlap with arXiv:1207.0580 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown great promise in many practical\napplications, ranging from speech recognition, visual object recognition, to\ntext processing. However, most of the current deep learning methods suffer from\nscalability problems for large-scale applications, forcing researchers or users\nto focus on small-scale problems with fewer parameters.\n  In this paper, we consider a well-known machine learning model, deep belief\nnetworks (DBNs) that have yielded impressive classification performance on a\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\nan approach that can use the computing clusters in a distributed environment to\ntrain large models, while the dense matrix computations within a single machine\nare sped up using graphics processors (GPU). When training a DBN, each machine\nrandomly drops out a portion of neurons in each hidden layer, for each training\ncase, making the remaining neurons only learn to detect features that are\ngenerally helpful for producing the correct answer. Within our approach, we\nhave developed four methods to combine outcomes from each machine to form a\nunified model. Our preliminary experiment on the mnst handwritten digit\ndatabase demonstrates that our approach outperforms the state of the art test\nerror rate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 05:24:06 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Huang", "Yanping", ""], ["Zhang", "Sai", ""]]}, {"id": "1508.07130", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Parallel Dither and Dropout for Regularising Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective regularisation during training can mean the difference between\nsuccess and failure for deep neural networks. Recently, dither has been\nsuggested as alternative to dropout for regularisation during batch-averaged\nstochastic gradient descent (SGD). In this article, we show that these methods\nfail without batch averaging and we introduce a new, parallel regularisation\nmethod that may be used without batch averaging. Our results for\nparallel-regularised non-batch-SGD are substantially better than what is\npossible with batch-SGD. Furthermore, our results demonstrate that dither and\ndropout are complimentary.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 08:50:18 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1508.07551", "submitter": "Karim Awudu", "authors": "Awudu Karim and Shangbo Zhou", "title": "X-TREPAN: a multi class regression and adapted extraction of\n  comprehensible decision tree in artificial neural networks", "comments": "17 Pages, 8 Tables, 8 Figures, 6 Equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the TREPAN algorithm is enhanced and extended for extracting\ndecision trees from neural networks. We empirically evaluated the performance\nof the algorithm on a set of databases from real world events. This benchmark\nenhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree\ninduction algorithms to analyze the datasets. The models are then compared with\nX-TREPAN for comprehensibility and classification accuracy. Furthermore, we\nvalidate the experimentations by applying statistical methods. Finally, the\nmodified algorithm is extended to work with multi-class regression problems and\nthe ability to comprehend generalized feed forward networks is achieved.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 10:14:48 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Karim", "Awudu", ""], ["Zhou", "Shangbo", ""]]}, {"id": "1508.07700", "submitter": "Gerard Howard", "authors": "David Howard, Larry Bull and Pier-Luca Lanzi", "title": "A Cognitive Architecture Based on a Learning Classifier System with\n  Spiking Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Classifier Systems (LCS) are population-based reinforcement learners\nthat were originally designed to model various cognitive phenomena. This paper\npresents an explicitly cognitive LCS by using spiking neural networks as\nclassifiers, providing each classifier with a measure of temporal dynamism. We\nemploy a constructivist model of growth of both neurons and synaptic\nconnections, which permits a Genetic Algorithm (GA) to automatically evolve\nsufficiently-complex neural structures. The spiking classifiers are coupled\nwith a temporally-sensitive reinforcement learning algorithm, which allows the\nsystem to perform temporal state decomposition by appropriately rewarding\n\"macro-actions,\" created by chaining together multiple atomic actions. The\ncombination of temporal reinforcement learning and neural information\nprocessing is shown to outperform benchmark neural classifier systems, and\nsuccessfully solve a robotic navigation task.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 06:35:01 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Howard", "David", ""], ["Bull", "Larry", ""], ["Lanzi", "Pier-Luca", ""]]}, {"id": "1508.07741", "submitter": "Luk\\'a\\v{s} Bajer", "authors": "Lukas Bajer and Martin Holena", "title": "Model Guided Sampling Optimization for Low-dimensional Problems", "comments": null, "journal-ref": "Bajer, L. & Holena, M. Model Guided Sampling Optimization for\n  Low-dimensional Problems. in ICAART 2015 Proceedings of the International\n  Conference on Agents and Artificial Intelligence, Volume 2 451-456\n  (SCITEPRESS, Lisbon, 2015)", "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of very expensive black-box functions requires utilization of\nmaximum information gathered by the process of optimization. Model Guided\nSampling Optimization (MGSO) forms a more robust alternative to Jones'\nGaussian-process-based EGO algorithm. Instead of EGO's maximizing expected\nimprovement, the MGSO uses sampling the probability of improvement which is\nshown to be helpful against trapping in local minima. Further, the MGSO can\nreach close-to-optimum solutions faster than standard optimization algorithms\non low dimensional or smooth problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 09:42:33 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Bajer", "Lukas", ""], ["Holena", "Martin", ""]]}]