[{"id": "1410.0162", "submitter": "Ozgur Yilmaz", "authors": "Ozgur Yilmaz", "title": "Reservoir Computing using Cellular Automata", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework of reservoir computing. Cellular automaton is\nused as the reservoir of dynamical systems. Input is randomly projected onto\nthe initial conditions of automaton cells and nonlinear computation is\nperformed on the input via application of a rule in the automaton for a period\nof time. The evolution of the automaton creates a space-time volume of the\nautomaton state space, and it is used as the reservoir. The proposed framework\nis capable of long short-term memory and it requires orders of magnitude less\ncomputation compared to Echo State Networks. Also, for additive cellular\nautomaton rules, reservoir features can be combined using Boolean operations,\nwhich provides a direct way for concept building and symbolic processing, and\nit is much more efficient compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 09:36:19 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Yilmaz", "Ozgur", ""]]}, {"id": "1410.0446", "submitter": "Arash Mahyari", "authors": "Arash Golibagh Mahyari, Selin Aviyente", "title": "Identification of Dynamic functional brain network states Through Tensor\n  Decomposition", "comments": "2014 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "journal-ref": null, "doi": "10.1109/ICASSP.2014.6853969", "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in high resolution neuroimaging, there has been a growing\ninterest in the detection of functional brain connectivity. Complex network\ntheory has been proposed as an attractive mathematical representation of\nfunctional brain networks. However, most of the current studies of functional\nbrain networks have focused on the computation of graph theoretic indices for\nstatic networks, i.e. long-time averages of connectivity networks. It is\nwell-known that functional connectivity is a dynamic process and the\nconstruction and reorganization of the networks is key to understanding human\ncognition. Therefore, there is a growing need to track dynamic functional brain\nnetworks and identify time intervals over which the network is\nquasi-stationary. In this paper, we present a tensor decomposition based method\nto identify temporally invariant 'network states' and find a common topographic\nrepresentation for each state. The proposed methods are applied to\nelectroencephalogram (EEG) data during the study of error-related negativity\n(ERN).\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 03:41:53 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Mahyari", "Arash Golibagh", ""], ["Aviyente", "Selin", ""]]}, {"id": "1410.0507", "submitter": "Claudius Gros", "authors": "Rodrigo Echeveste and Claudius Gros", "title": "Generating functionals for computational intelligence: the Fisher\n  information as an objective function for self-limiting Hebbian learning rules", "comments": null, "journal-ref": "Frontiers in Robotics and AI 1, 1 (2014)", "doi": "10.3389/frobt.2014.00001", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating functionals may guide the evolution of a dynamical system and\nconstitute a possible route for handling the complexity of neural networks as\nrelevant for computational intelligence. We propose and explore a new objective\nfunction, which allows to obtain plasticity rules for the afferent synaptic\nweights. The adaption rules are Hebbian, self-limiting, and result from the\nminimization of the Fisher information with respect to the synaptic flux. We\nperform a series of simulations examining the behavior of the new learning\nrules in various circumstances. The vector of synaptic weights aligns with the\nprincipal direction of input activities, whenever one is present. A linear\ndiscrimination is performed when there are two or more principal directions;\ndirections having bimodal firing-rate distributions, being characterized by a\nnegative excess kurtosis, are preferred. We find robust performance and full\nhomeostatic adaption of the synaptic weights results as a by-product of the\nsynaptic flux minimization. This self-limiting behavior allows for stable\nonline learning for arbitrary durations. The neuron acquires new information\nwhen the statistics of input activities is changed at a certain point of the\nsimulation, showing however, a distinct resilience to unlearn previously\nacquired knowledge. Learning is fast when starting with randomly drawn synaptic\nweights and substantially slower when the synaptic weights are already fully\nadapted.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 10:52:49 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Echeveste", "Rodrigo", ""], ["Gros", "Claudius", ""]]}, {"id": "1410.0510", "submitter": "Ludovic Denoyer", "authors": "Ludovic Denoyer and Patrick Gallinari", "title": "Deep Sequential Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks sequentially build high-level features through their\nsuccessive layers. We propose here a new neural network model where each layer\nis associated with a set of candidate mappings. When an input is processed, at\neach layer, one mapping among these candidates is selected according to a\nsequential decision process. The resulting model is structured according to a\nDAG like architecture, so that a path from the root to a leaf node defines a\nsequence of transformations. Instead of considering global transformations,\nlike in classical multilayer networks, this model allows us for learning a set\nof local transformations. It is thus able to process data with different\ncharacteristics through specific sequences of such local transformations,\nincreasing the expression power of this model w.r.t a classical multilayered\nnetwork. The learning algorithm is inspired from policy gradient techniques\ncoming from the reinforcement learning domain and is used here instead of the\nclassical back-propagation based gradient descent techniques. Experiments on\ndifferent datasets show the relevance of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 10:58:17 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1410.0547", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Design Mining Interacting Wind Turbines", "comments": null, "journal-ref": "Evolutionary Computation (2016), 24(1):89-111", "doi": "10.1162/EVCO_a_00144", "report-no": null, "categories": "cs.NE cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An initial study of surrogate-assisted evolutionary algorithms used to design\nvertical-axis wind turbines wherein candidate prototypes are evaluated under\nfan generated wind conditions after being physically instantiated by a 3D\nprinter has recently been presented. Unlike other approaches, such as\ncomputational fluid dynamics simulations, no mathematical formulations were\nused and no model assumptions were made. This paper extends that work by\nexploring alternative surrogate modelling and evolutionary techniques. The\naccuracy of various modelling algorithms used to estimate the fitness of\nevaluated individuals from the initial experiments is compared. The effect of\ntemporally windowing surrogate model training samples is explored. A\nsurrogate-assisted approach based on an enhanced local search is introduced;\nand alternative coevolution collaboration schemes are examined.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 13:32:59 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 14:20:23 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1410.0602", "submitter": "Helmut Katzgraber", "authors": "Roberto Santana, Ross B. McDonald and Helmut G. Katzgraber", "title": "A probabilistic evolutionary optimization approach to compute\n  quasiparticle braids", "comments": "9 pages,7 figures. Accepted at SEAL 2014", "journal-ref": "Simulated Evolution and Learning, Lecture Notes in Computer\n  Science 8886, 13 (2014)", "doi": "10.1007/978-3-319-13563-2_2", "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological quantum computing is an alternative framework for avoiding the\nquantum decoherence problem in quantum computation. The problem of executing a\ngate in this framework can be posed as the problem of braiding quasiparticles.\nBecause these are not Abelian, the problem can be reduced to finding an optimal\nproduct of braid generators where the optimality is defined in terms of the\ngate approximation and the braid's length. In this paper we propose the use of\ndifferent variants of estimation of distribution algorithms to deal with the\nproblem. Furthermore, we investigate how the regularities of the braid\noptimization problem can be translated into statistical regularities by means\nof the Boltzmann distribution. We show that our best algorithm is able to\nproduce many solutions that approximates the target gate with an accuracy in\nthe order of $10^{-6}$, and have lengths up to 9 times shorter than those\nexpected from braids of the same accuracy obtained with other methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 16:26:03 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Santana", "Roberto", ""], ["McDonald", "Ross B.", ""], ["Katzgraber", "Helmut G.", ""]]}, {"id": "1410.0630", "submitter": "Sherjil Ozair", "authors": "Sherjil Ozair and Yoshua Bengio", "title": "Deep Directed Generative Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discrete data, the likelihood $P(x)$ can be rewritten exactly and\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\nhas enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq\nf(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the\nfirst factor gives rise to the log-likelihood reconstruction error of an\nautoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\ndecoder. The log of the second term can be seen as a regularizer on the encoded\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\ncan be represented by a deep neural network and trained to maximize the average\nof the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder\n$f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\nprobability mass in a smaller number of (relevant) dimensions over which the\ndistribution factorizes. Generating samples from the model is straightforward\nusing ancestral sampling. One challenge is that regular back-propagation cannot\nbe used to obtain the gradient on the parameters of the encoder, but we find\nthat using the straight-through estimator works well here. We also find that\nalthough optimizing a single level of such architecture may be difficult, much\nbetter results can be obtained by pre-training and stacking them, gradually\ntransforming the data distribution into one that is more easily captured by a\nsimple parametric model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:09:42 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Ozair", "Sherjil", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.0640", "submitter": "Hugo Jair  Escalante", "authors": "Hugo Jair Escalante, Mauricio A. Garc\\'ia-Lim\\'on, Alicia\n  Morales-Reyes, Mario Graff, Manuel Montes-y-G\\'omez, Eduardo F. Morales", "title": "Term-Weighting Learning via Genetic Programming for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to learning term-weighting schemes\n(TWSs) in the context of text classification. In text mining a TWS determines\nthe way in which documents will be represented in a vector space model, before\napplying a classifier. Whereas acceptable performance has been obtained with\nstandard TWSs (e.g., Boolean and term-frequency schemes), the definition of\nTWSs has been traditionally an art. Further, it is still a difficult task to\ndetermine what is the best TWS for a particular problem and it is not clear\nyet, whether better schemes, than those currently available, can be generated\nby combining known TWS. We propose in this article a genetic program that aims\nat learning effective TWSs that can improve the performance of current schemes\nin text classification. The genetic program learns how to combine a set of\nbasic units to give rise to discriminative TWSs. We report an extensive\nexperimental study comprising data sets from thematic and non-thematic text\nclassification as well as from image classification. Our study shows the\nvalidity of the proposed method; in fact, we show that TWSs learned with the\ngenetic program outperform traditional schemes and other TWSs proposed in\nrecent works. Further, we show that TWSs learned from a specific domain can be\neffectively used for other tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:38:11 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 19:47:03 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 20:48:29 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Escalante", "Hugo Jair", ""], ["Garc\u00eda-Lim\u00f3n", "Mauricio A.", ""], ["Morales-Reyes", "Alicia", ""], ["Graff", "Mario", ""], ["Montes-y-G\u00f3mez", "Manuel", ""], ["Morales", "Eduardo F.", ""]]}, {"id": "1410.0736", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis\n  DeCoste, Wei Di, Yizhou Yu", "title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\n  Visual Recognition", "comments": "Add new results on ImageNet using VGG-16-layer building block net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification, visual separability between different object\ncategories is highly uneven, and some categories are more difficult to\ndistinguish than others. Such difficult categories demand more dedicated\nclassifiers. However, existing deep convolutional neural networks (CNN) are\ntrained as flat N-way classifiers, and few efforts have been made to leverage\nthe hierarchical structure of categories. In this paper, we introduce\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\nwhile distinguishing difficult classes using fine category classifiers. During\nHD-CNN training, component-wise pretraining is followed by global finetuning\nwith a multinomial logistic loss regularized by a coarse category consistency\nterm. In addition, conditional executions of fine category classifiers and\nlayer parameter compression make HD-CNNs scalable for large-scale visual\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\nCNNs by 2.65%, 3.1% and 1.1%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 01:17:20 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:51:51 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 03:11:49 GMT"}, {"version": "v4", "created": "Sat, 16 May 2015 03:36:32 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Piramuthu", "Robinson", ""], ["Jagadeesh", "Vignesh", ""], ["DeCoste", "Dennis", ""], ["Di", "Wei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1410.0759", "submitter": "Bryan Catanzaro", "authors": "Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,\n  John Tran, Bryan Catanzaro, Evan Shelhamer", "title": "cuDNN: Efficient Primitives for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a library of efficient implementations of deep learning\nprimitives. Deep learning workloads are computationally intensive, and\noptimizing their kernels is difficult and time-consuming. As parallel\narchitectures evolve, kernels must be reoptimized, which makes maintaining\ncodebases difficult over time. Similar issues have long been addressed in the\nHPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).\nHowever, there is no analogous library for deep learning. Without such a\nlibrary, researchers implementing deep learning workloads on parallel\nprocessors must create and optimize their own implementations of the main\ncomputational kernels, and this work must be repeated as new parallel\nprocessors emerge. To address this problem, we have created a library similar\nin intent to BLAS, with optimized routines for deep learning workloads. Our\nimplementation contains routines for GPUs, although similarly to the BLAS\nlibrary, these routines could be implemented for other platforms. The library\nis easy to integrate into existing frameworks, and provides optimized\nperformance and memory usage. For example, integrating cuDNN into Caffe, a\npopular framework for convolutional networks, improves performance by 36% on a\nstandard model while also reducing memory consumption.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 06:16:43 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 06:00:21 GMT"}, {"version": "v3", "created": "Thu, 18 Dec 2014 01:13:16 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Chetlur", "Sharan", ""], ["Woolley", "Cliff", ""], ["Vandermersch", "Philippe", ""], ["Cohen", "Jonathan", ""], ["Tran", "John", ""], ["Catanzaro", "Bryan", ""], ["Shelhamer", "Evan", ""]]}, {"id": "1410.0781", "submitter": "Nadav Cohen", "authors": "Nadav Cohen and Amnon Shashua", "title": "SimNets: A Generalization of Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep layered architecture that generalizes classical\nconvolutional neural networks (ConvNets). The architecture, called SimNets, is\ndriven by two operators, one being a similarity function whose family contains\nthe convolution operator used in ConvNets, and the other is a new soft\nmax-min-mean operator called MEX that realizes classical operators like ReLU\nand max pooling, but has additional capabilities that make SimNets a powerful\ngeneralization of ConvNets. Three interesting properties emerge from the\narchitecture: (i) the basic input to hidden layer to output machinery contains\nas special cases kernel machines with the Exponential and Generalized Gaussian\nkernels, the output units being \"neurons in feature space\" (ii) in its general\nform, the basic machinery has a higher abstraction level than kernel machines,\nand (iii) initializing networks using unsupervised learning is natural.\nExperiments demonstrate the capability of achieving state of the art accuracy\nwith networks that are an order of magnitude smaller than comparable ConvNets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 08:47:03 GMT"}, {"version": "v2", "created": "Sat, 25 Oct 2014 09:47:07 GMT"}, {"version": "v3", "created": "Sun, 7 Dec 2014 15:51:28 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1410.0948", "submitter": "Eug\\'enio Rodrigues", "authors": "Eug\\'enio Rodrigues, Ad\\'elio R. Gaspar, \\'Alvaro Gomes and Manuel\n  Gameiro da Silva", "title": "Contributions of natural ventilation on thermal performance of\n  alternative floor plan designs", "comments": "8 pages, 2 figures, Proceeding of RoomVent 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the earliest phase of architectural design process, practitioners\nafter analyzing the client's design program, legal requirements, topographic\nconstraints, and preferences synthesize these requirements into architectural\nfloor plan drawings. Design decisions taken in this phase may significantly\ncontribute to the building performance. On account of this reason, it is\nimportant to estimate and compare alternative solutions, when it is still\nmanageable to change the building design.\n  The authors have been developing a prototype tool to assist architects during\nthis initial design phase. It is made up of two algorithms. The first algorithm\ngenerates alternative floor plans according to the architect's preferences and\nrequirements, and the client's design program. It consists in one evolutionary\nstrategy approach enhanced with local search technique to allocate rooms on\nseveral levels in the two-dimensional space. The second algorithm evaluates,\nranks, and optimizes those floor plans according to thermal performance\ncriteria. The prototype tool is coupled with dynamic simulation program, which\nestimates the thermal behavior of each solution. A sequential variable\noptimization is used to change several geometric values of different\narchitectural elements in the floor plans to explore the improvement potential.\n  In the present communication, the two algorithms are used in an iterative\nprocess to generate and optimize the thermal performance of alternative floor\nplans. In the building simulation specifications of EnergyPlus program, the\nairflow network model has been used in order to adequately model the air\ninfiltration and the airflows through indoor spaces. A case study of a\nsingle-family house with three rooms in a single level is presented.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 19:36:37 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Rodrigues", "Eug\u00e9nio", ""], ["Gaspar", "Ad\u00e9lio R.", ""], ["Gomes", "\u00c1lvaro", ""], ["da Silva", "Manuel Gameiro", ""]]}, {"id": "1410.1151", "submitter": "Vladimir Bochkarev", "authors": "Yulia S. Maslennikova, Vladimir V. Bochkarev", "title": "Training Algorithm for Neuro-Fuzzy Network Based on Singular Spectrum\n  Analysis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a combination of an noise-reduction algorithm\nbased on Singular Spectrum Analysis (SSA) and a standard feedforward neural\nprediction model. Basically, the proposed algorithm consists of two different\nsteps: data preprocessing based on the SSA filtering method and step-by-step\ntraining procedure in which we use a simple feedforward multilayer neural\nnetwork with backpropagation learning. The proposed noise-reduction procedure\nsuccessfully removes most of the noise. That increases long-term predictability\nof the processed dataset comparison with the raw dataset. The method was\napplied to predict the International sunspot number RZ time series. The results\nshow that our combined technique has better performances than those offered by\nthe same network directly applied to raw dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 12:25:15 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Maslennikova", "Yulia S.", ""], ["Bochkarev", "Vladimir V.", ""]]}, {"id": "1410.1165", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez, J\\\"urgen\n  Schmidhuber", "title": "Understanding Locally Competitive Networks", "comments": "9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed neural network activation functions such as rectified\nlinear, maxout, and local winner-take-all have allowed for faster and more\neffective training of deep neural architectures on large and complex datasets.\nThe common trait among these functions is that they implement local competition\nbetween small groups of computational units within a layer, so that only part\nof the network is activated for any given input pattern. In this paper, we\nattempt to visualize and understand this self-modularization, and suggest a\nunified explanation for the beneficial properties of such networks. We also\nshow how our insights can be directly useful for efficiently performing\nretrieval over large datasets using neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 14:46:47 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 20:07:17 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 01:22:49 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Masci", "Jonathan", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1410.2381", "submitter": "Mohamed Sayedelahl Dr", "authors": "R. M. Farouk, S. Badr and M. Sayed Elahl", "title": "Recognition of cDNA microarray image Using Feedforward artificial neural\n  network", "comments": "17 pages, 7 figures and 23 References", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 5, No. 5, September 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complementary DNA (cDNA) sequence is considered to be the magic biometric\ntechnique for personal identification. In this paper, we present a new method\nfor cDNA recognition based on the artificial neural network (ANN). Microarray\nimaging is used for the concurrent identification of thousands of genes. We\nhave segmented the location of the spots in a cDNA microarray. Thus, a precise\nlocalization and segmenting of a spot are essential to obtain a more accurate\nintensity measurement, leading to a more precise expression measurement of a\ngene. The segmented cDNA microarray image is resized and it is used as an input\nfor the proposed artificial neural network. For matching and recognition, we\nhave trained the artificial neural network. Recognition results are given for\nthe galleries of cDNA sequences . The numerical results show that, the proposed\nmatching technique is an effective in the cDNA sequences process. We also\ncompare our results with previous results and find out that, the proposed\ntechnique is an effective matching performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 08:37:21 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Farouk", "R. M.", ""], ["Badr", "S.", ""], ["Elahl", "M. Sayed", ""]]}, {"id": "1410.2479", "submitter": "Andreas Schwarz", "authors": "Andreas Schwarz, Christian Huemmer, Roland Maas, Walter Kellermann", "title": "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy\n  and Reverberant Environments", "comments": "accepted for ICASSP2015", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178798", "report-no": null, "categories": "cs.CL cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a spatial diffuseness feature for deep neural network (DNN)-based\nautomatic speech recognition to improve recognition accuracy in reverberant and\nnoisy environments. The feature is computed in real-time from multiple\nmicrophone signals without requiring knowledge or estimation of the direction\nof arrival, and represents the relative amount of diffuse noise in each time\nand frequency bin. It is shown that using the diffuseness feature as an\nadditional input to a DNN-based acoustic model leads to a reduced word error\nrate for the REVERB challenge corpus, both compared to logmelspec features\nextracted from noisy signals, and features enhanced by spectral subtraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:15:42 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 13:54:06 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Schwarz", "Andreas", ""], ["Huemmer", "Christian", ""], ["Maas", "Roland", ""], ["Kellermann", "Walter", ""]]}, {"id": "1410.3541", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Yuriy V. Pershin, Fabio L. Traversa, Massimiliano Di Ventra", "title": "Memcomputing with membrane memcapacitive systems", "comments": null, "journal-ref": "Nanotechnology, vol. 26, page 225201 (9 pp), year 2015", "doi": "10.1088/0957-4484/26/22/225201", "report-no": null, "categories": "cs.ET cond-mat.mes-hall cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show theoretically that networks of membrane memcapacitive systems --\ncapacitors with memory made out of membrane materials -- can be used to perform\na complete set of logic gates in a massively parallel way by simply changing\nthe external input amplitudes, but not the topology of the network. This\npolymorphism is an important characteristic of memcomputing (computing with\nmemories) that closely reproduces one of the main features of the brain. A\npractical realization of these membrane memcapacitive systems, using, e.g.,\ngraphene or other 2D materials, would be a step forward towards a solid-state\nrealization of memcomputing with passive devices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 00:18:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 21:31:14 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Pershin", "Yuriy V.", ""], ["Traversa", "Fabio L.", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1410.3744", "submitter": "Chee Seng Chan", "authors": "Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso and Paolo Remagnino", "title": "Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking", "comments": "Accepted in Information Sciences, new abrupt motion (MAMo) dataset is\n  introduced", "journal-ref": "Information Sciences, vol. 283, pp. 267-287, 2014", "doi": "10.1016/j.ins.2014.01.003", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional tracking solutions are not feasible in handling abrupt motion as\nthey are based on smooth motion assumption or an accurate motion model. Abrupt\nmotion is not subject to motion continuity and smoothness. To assuage this, we\ndeem tracking as an optimisation problem and propose a novel abrupt motion\ntracker that based on swarm intelligence - the SwaTrack. Unlike existing\nswarm-based filtering methods, we first of all introduce an optimised\nswarm-based sampling strategy to tradeoff between the exploration and\nexploitation of the search space in search for the optimal proposal\ndistribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allow\non the fly tuning of the best mean and variance of the distribution for\nsampling. Such innovating idea of combining these strategies in an ingenious\nway in the PSO framework to handle the abrupt motion, which so far no existing\nworks are found. Experimental results in both quantitative and qualitative had\nshown the effectiveness of the proposed method in tracking abrupt motions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:06:13 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Lim", "Mei Kuan", ""], ["Chan", "Chee Seng", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1410.3831", "submitter": "Pankaj Mehta", "authors": "Pankaj Mehta and David J. Schwab", "title": "An exact mapping between the Variational Renormalization Group and Deep\n  Learning", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a broad set of techniques that uses multiple layers of\nrepresentation to automatically learn relevant features directly from\nstructured data. Recently, such techniques have yielded record-breaking results\non a diverse set of difficult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of\ndeep learning, relatively little is understood theoretically about why these\ntechniques are so successful at feature learning and compression. Here, we show\nthat deep learning is intimately related to one of the most important and\nsuccessful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of\nrelevant features (i.e. operators) as a physical system is examined at\ndifferent length scales. We construct an exact mapping from the variational\nrenormalization group, first introduced by Kadanoff, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\nOur results suggests that deep learning algorithms may be employing a\ngeneralized RG-like scheme to learn relevant features from data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 20:00:09 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Mehta", "Pankaj", ""], ["Schwab", "David J.", ""]]}, {"id": "1410.3864", "submitter": "Chiranjib Saha", "authors": "Debdipta Goswami, Chiranjib Saha, Kunal Pal, Swagatam Das", "title": "Multi-Agent Shape Formation and Tracking Inspired from a Social Foraging\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principle of Swarm Intelligence has recently found widespread application in\nformation control and automated tracking by the automated multi-agent system.\nThis article proposes an elegant and effective method inspired by foraging\ndynamics to produce geometric-patterns by the search agents. Starting from a\nrandom initial orientation, it is investigated how the foraging dynamics can be\nmodified to achieve convergence of the agents on the desired pattern with\nalmost uniform density. Guided through the proposed dynamics, the agents can\nalso track a moving point by continuously circulating around the point. An\nanalytical treatment supported with computer simulation results is provided to\nbetter understand the convergence behaviour of the system.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 20:51:07 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 09:10:44 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Goswami", "Debdipta", ""], ["Saha", "Chiranjib", ""], ["Pal", "Kunal", ""], ["Das", "Swagatam", ""]]}, {"id": "1410.4281", "submitter": "Xiangang Li", "authors": "Xiangang Li, Xihong Wu", "title": "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks\n  for Large Vocabulary Speech Recognition", "comments": "submitted to ICASSP 2015 which does not perform blind reviews", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory (LSTM) based acoustic modeling methods have recently\nbeen shown to give state-of-the-art performance on some speech recognition\ntasks. To achieve a further performance improvement, in this research, deep\nextensions on LSTM are investigated considering that deep hierarchical model\nhas turned out to be more efficient than a shallow one. Motivated by previous\nresearch on constructing deep recurrent neural networks (RNNs), alternative\ndeep LSTM architectures are proposed and empirically evaluated on a large\nvocabulary conversational telephone speech recognition task. Meanwhile,\nregarding to multi-GPU devices, the training process for LSTM networks is\nintroduced and discussed. Experimental results demonstrate that the deep LSTM\nnetworks benefit from the depth and yield the state-of-the-art performance on\nthis task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 02:44:41 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 02:23:06 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Li", "Xiangang", ""], ["Wu", "Xihong", ""]]}, {"id": "1410.4343", "submitter": "Chiranjib Saha", "authors": "Anupam Trivedi, Kunal Pal, Chiranjib Saha, Dipti Srinivasan", "title": "Enhanced Multiobjective Evolutionary Algorithm based on Decomposition\n  for Solving the Unit Commitment Problem", "comments": "This paper has been withdrawn by the author due to 1. inconsistency\n  of vector representation in equation (12) and 2. algorithm 3 has not been\n  included in this text which makes the description incomplete", "journal-ref": null, "doi": null, "report-no": "IEEE Transactions on Industrial Informatics - Manuscript ID\n  TII-14-0921", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unit commitment (UC) problem is a nonlinear, high-dimensional, highly\nconstrained, mixed-integer power system optimization problem and is generally\nsolved in the literature considering minimizing the system operation cost as\nthe only objective. However, due to increasing environmental concerns, the\nrecent attention has shifted to incorporating emission in the problem\nformulation. In this paper, a multi-objective evolutionary algorithm based on\ndecomposition (MOEA/D) is proposed to solve the UC problem as a multi-objective\noptimization problem considering minimizing cost and emission as the multiple\nobjec- tives. Since, UC problem is a mixed-integer optimization problem\nconsisting of binary UC variables and continuous power dispatch variables, a\nnovel hybridization strategy is proposed within the framework of MOEA/D such\nthat genetic algorithm (GA) evolves the binary variables while differential\nevolution (DE) evolves the continuous variables. Further, a novel non-uniform\nweight vector distribution strategy is proposed and a parallel island model\nbased on combination of MOEA/D with uniform and non-uniform weight vector\ndistribution strategy is implemented to enhance the performance of the\npresented algorithm. Extensive case studies are presented on different test\nsystems and the effectiveness of the proposed hybridization strategy, the\nnon-uniform weight vector distribution strategy and parallel island model is\nverified through stringent simulated results. Further, exhaustive benchmarking\nagainst the algorithms proposed in the literature is presented to demonstrate\nthe superiority of the proposed algorithm in obtaining significantly better\nconverged and uniformly distributed trade-off solutions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 09:17:51 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 18:10:19 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Trivedi", "Anupam", ""], ["Pal", "Kunal", ""], ["Saha", "Chiranjib", ""], ["Srinivasan", "Dipti", ""]]}, {"id": "1410.4599", "submitter": "Erte Pan", "authors": "Erte Pan and Zhu Han", "title": "Non-parametric Bayesian Learning with Deep Learning Structure and Its\n  Applications in Wireless Networks", "comments": "5 pages, 2 figures and 1 algorithm list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an infinite hierarchical non-parametric Bayesian\nmodel to extract the hidden factors over observed data, where the number of\nhidden factors for each layer is unknown and can be potentially infinite.\nMoreover, the number of layers can also be infinite. We construct the model\nstructure that allows continuous values for the hidden factors and weights,\nwhich makes the model suitable for various applications. We use the\nMetropolis-Hastings method to infer the model structure. Then the performance\nof the algorithm is evaluated by the experiments. Simulation results show that\nthe model fits the underlying structure of simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 22:29:12 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 21:55:30 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Pan", "Erte", ""], ["Han", "Zhu", ""]]}, {"id": "1410.4615", "submitter": "Wojciech Zaremba", "authors": "Wojciech Zaremba, Ilya Sutskever", "title": "Learning to Execute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are\nwidely used because they are expressive and are easy to train. Our interest\nlies in empirically evaluating the expressiveness and the learnability of LSTMs\nin the sequence-to-sequence regime by training them to evaluate short computer\nprograms, a domain that has traditionally been seen as too complex for neural\nnetworks. We consider a simple class of programs that can be evaluated with a\nsingle left-to-right pass using constant memory. Our main result is that LSTMs\ncan learn to map the character-level representations of such programs to their\ncorrect outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks' performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit\nnumbers with 99% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 01:35:12 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 03:46:49 GMT"}, {"version": "v3", "created": "Thu, 19 Feb 2015 15:33:35 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1410.4985", "submitter": "Danesh Tarapore", "authors": "Danesh Tarapore and Jean-Baptiste Mouret", "title": "Evolvability signatures of generative encodings: beyond standard\n  performance benchmarks", "comments": "24 pages with 12 figures in the main text, and 4 supplementary\n  figures. Accepted at Information Sciences journal (in press). Supplemental\n  videos are available online at, see http://goo.gl/uyY1RX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary robotics is a promising approach to autonomously synthesize\nmachines with abilities that resemble those of animals, but the field suffers\nfrom a lack of strong foundations. In particular, evolutionary systems are\ncurrently assessed solely by the fitness score their evolved artifacts can\nachieve for a specific task, whereas such fitness-based comparisons provide\nlimited insights about how the same system would evaluate on different tasks,\nand its adaptive capabilities to respond to changes in fitness (e.g., from\ndamages to the machine, or in new situations). To counter these limitations, we\nintroduce the concept of \"evolvability signatures\", which picture the\npost-mutation statistical distribution of both behavior diversity (how\ndifferent are the robot behaviors after a mutation?) and fitness values (how\ndifferent is the fitness after a mutation?). We tested the relevance of this\nconcept by evolving controllers for hexapod robot locomotion using five\ndifferent genotype-to-phenotype mappings (direct encoding, generative encoding\nof open-loop and closed-loop central pattern generators, generative encoding of\nneural networks, and single-unit pattern generators (SUPG)). We observed a\npredictive relationship between the evolvability signature of each encoding and\nthe number of generations required by hexapods to adapt from incurred damages.\nOur study also reveals that, across the five investigated encodings, the SUPG\nscheme achieved the best evolvability signature, and was always foremost in\nrecovering an effective gait following robot damages. Overall, our evolvability\nsignatures neatly complement existing task-performance benchmarks, and pave the\nway for stronger foundations for research in evolutionary robotics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 18:16:26 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2015 13:16:20 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Tarapore", "Danesh", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1410.5401", "submitter": "Alex Graves", "authors": "Alex Graves, Greg Wayne, Ivo Danihelka", "title": "Neural Turing Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 19:28:26 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 16:01:39 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Graves", "Alex", ""], ["Wayne", "Greg", ""], ["Danihelka", "Ivo", ""]]}, {"id": "1410.5610", "submitter": "Gabriele Scheler", "authors": "Gabriele Scheler", "title": "Logarithmic distributions prove that intrinsic learning is Hebbian", "comments": null, "journal-ref": "Scheler G. Logarithmic distributions prove that intrinsic learning\n  is Hebbian [version 2; referees: 2 approved]. F1000Research 2017, 6:1222", "doi": "10.12688/f1000research.12130.2", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present data for the lognormal distributions of spike\nrates, synaptic weights and intrinsic excitability (gain) for neurons in\nvarious brain areas, such as auditory or visual cortex, hippocampus,\ncerebellum, striatum, midbrain nuclei. We find a remarkable consistency of\nheavy-tailed, specifically lognormal, distributions for rates, weights and\ngains in all brain areas examined. The difference between strongly recurrent\nand feed-forward connectivity (cortex vs. striatum and cerebellum),\nneurotransmitter (GABA (striatum) or glutamate (cortex)) or the level of\nactivation (low in cortex, high in Purkinje cells and midbrain nuclei) turns\nout to be irrelevant for this feature. Logarithmic scale distribution of\nweights and gains appears to be a general, functional property in all cases\nanalyzed. We then created a generic neural model to investigate adaptive\nlearning rules that create and maintain lognormal distributions. We\nconclusively demonstrate that not only weights, but also intrinsic gains, need\nto have strong Hebbian learning in order to produce and maintain the\nexperimentally attested distributions. This provides a solution to the\nlong-standing question about the type of plasticity exhibited by intrinsic\nexcitability.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 10:32:31 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 04:41:14 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 15:00:09 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Scheler", "Gabriele", ""]]}, {"id": "1410.5652", "submitter": "Andr\\'as Kir\\'aly Ph.D.", "authors": "Tam\\'as Varga, Andr\\'as Kir\\'aly, J\\'anos Abonyi", "title": "Improvement of PSO algorithm by memory based gradient search -\n  application in inventory management", "comments": "book chapter, 20 pages, 7 figures, 2 tables", "journal-ref": "Swarm Intelligence and Bio-Inspired Computation: Theory and\n  Applications, 1st Edition, pages 403-422. Elsevier, Oxford, 2013", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced inventory management in complex supply chains requires effective and\nrobust nonlinear optimization due to the stochastic nature of supply and demand\nvariations. Application of estimated gradients can boost up the convergence of\nParticle Swarm Optimization (PSO) algorithm but classical gradient calculation\ncannot be applied to stochastic and uncertain systems. In these situations\nMonte-Carlo (MC) simulation can be applied to determine the gradient. We\ndeveloped a memory based algorithm where instead of generating and evaluating\nnew simulated samples the stored and shared former function evaluations of the\nparticles are sampled to estimate the gradients by local weighted least squares\nregression. The performance of the resulted regional gradient-based PSO is\nverified by several benchmark problems and in a complex application example\nwhere optimal reorder points of a supply chain are determined.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 13:40:23 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Varga", "Tam\u00e1s", ""], ["Kir\u00e1ly", "Andr\u00e1s", ""], ["Abonyi", "J\u00e1nos", ""]]}, {"id": "1410.5850", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj", "title": "A Fast Hybrid Primal Heuristic for Multiband Robust Capacitated Network\n  Design with Multiple Time Periods", "comments": "This is the authors' final version of the paper published in Applied\n  Soft Computing 26, 497-507, 2015, DOI: 10.1016/j.asoc.2014.10.016. The final\n  publication is available at Elsevier ScienceDirect via\n  http://dx.doi.org/10.1016/j.asoc.2014.10.016", "journal-ref": "Applied Soft Computing 26 (2015) 497-507", "doi": "10.1016/j.asoc.2014.10.016", "report-no": null, "categories": "math.OC cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Robust Multiperiod Network Design Problem, a\ngeneralization of the Capacitated Network Design Problem (CNDP) that, besides\nestablishing flow routing and network capacity installation as in a canonical\nCNDP, also considers a planning horizon made up of multiple time periods and\nprotection against fluctuations in traffic volumes. As a remedy against traffic\nvolume uncertainty, we propose a Robust Optimization model based on Multiband\nRobustness (B\\\"using and D'Andreagiovanni, 2012), a refinement of classical\nGamma-Robustness by Bertsimas and Sim that uses a system of multiple deviation\nbands. Since the resulting optimization problem may prove very challenging even\nfor instances of moderate size solved by a state-of-the-art optimization\nsolver, we propose a hybrid primal heuristic that combines a randomized fixing\nstrategy inspired by ant colony optimization, which exploits information coming\nfrom linear relaxations of the problem, and an exact large neighbourhood\nsearch. Computational experiments on a set of realistic instances from the\nSNDlib show that our original heuristic can run fast and produce solutions of\nextremely high quality associated with low optimality gaps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 20:40:58 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 21:13:50 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Krolikowski", "Jonatan", ""], ["Pulaj", "Jonad", ""]]}, {"id": "1410.6413", "submitter": "Vladimir Bochkarev", "authors": "Vladimir V. Bochkarev and Yulia S. Maslennikova", "title": "Initialization of multilayer forecasting artifical neural networks", "comments": "9 pages, 3 figures", "journal-ref": "Uchenye Zapiski Kazanskogo Universiteta. Seriya\n  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)", "doi": null, "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method was developed for initialising artificial neural\nnetworks predicting dynamics of time series. Initial weighting coefficients\nwere determined for neurons analogously to the case of a linear prediction\nfilter. Moreover, to improve the accuracy of the initialization method for a\nmultilayer neural network, some variants of decomposition of the transformation\nmatrix corresponding to the linear prediction filter were suggested. The\nefficiency of the proposed neural network prediction method by forecasting\nsolutions of the Lorentz chaotic system is shown in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 16:54:39 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Bochkarev", "Vladimir V.", ""], ["Maslennikova", "Yulia S.", ""]]}, {"id": "1410.7326", "submitter": "Sebastian Risi", "authors": "Sebastian Risi, Julian Togelius", "title": "Neuroevolution in Games: State of the Art and Open Challenges", "comments": "- Added more references - Corrected typos - Added an overview table\n  (Table 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys research on applying neuroevolution (NE) to games. In\nneuroevolution, artificial neural networks are trained through evolutionary\nalgorithms, taking inspiration from the way biological brains evolved. We\nanalyse the application of NE in games along five different axes, which are the\nrole NE is chosen to play in a game, the different types of neural networks\nused, the way these networks are evolved, how the fitness is determined and\nwhat type of input the network receives. The article also highlights important\nopen research challenges in the field.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 17:46:28 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 12:40:30 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 21:21:41 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Risi", "Sebastian", ""], ["Togelius", "Julian", ""]]}, {"id": "1410.7455", "submitter": "Daniel Povey", "authors": "Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur", "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging", "comments": "Accepted as workshop contribution to ICLR 2015. 12 pages plus 16\n  pages of appendices, International Conference on Learning Representations\n  (ICLR): Workshop track, 2015. [2 sets of minor fixes post-publication.]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:45:41 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 21:53:05 GMT"}, {"version": "v3", "created": "Sat, 8 Nov 2014 03:35:52 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 02:16:31 GMT"}, {"version": "v5", "created": "Fri, 20 Feb 2015 05:12:34 GMT"}, {"version": "v6", "created": "Sun, 29 Mar 2015 18:25:06 GMT"}, {"version": "v7", "created": "Tue, 9 Jun 2015 22:34:22 GMT"}, {"version": "v8", "created": "Mon, 22 Jun 2015 22:07:56 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Povey", "Daniel", ""], ["Zhang", "Xiaohui", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1410.7550", "submitter": "Marc Deisenroth", "authors": "Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Marc Peter Deisenroth", "title": "Learning deep dynamical models from image pixels", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dynamical systems is important in many disciplines, e.g., control,\nrobotics, or neurotechnology. Commonly the state of these systems is not\ndirectly observed, but only available through noisy and potentially\nhigh-dimensional observations. In these cases, system identification, i.e.,\nfinding the measurement mapping and the transition mapping (system dynamics) in\nlatent space can be challenging. For linear system dynamics and measurement\nmappings efficient solutions for system identification are available. However,\nin practical applications, the linearity assumptions does not hold, requiring\nnon-linear system identification techniques. If additionally the observations\nare high-dimensional (e.g., images), non-linear system identification is\ninherently hard. To address the problem of non-linear system identification\nfrom high-dimensional observations, we combine recent advances in deep learning\nand system identification. In particular, we jointly learn a low-dimensional\nembedding of the observation by means of deep auto-encoders and a predictive\ntransition model in this low-dimensional space. We demonstrate that our model\nenables learning good predictive models of dynamical systems from pixel\ninformation only.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 08:37:01 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1410.7881", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar and Bipin Rajendran", "title": "A neural circuit for navigation inspired by C. elegans Chemotaxis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an artificial neural circuit for contour tracking and navigation\ninspired by the chemotaxis of the nematode Caenorhabditis elegans. In order to\nharness the computational advantages spiking neural networks promise over their\nnon-spiking counterparts, we develop a network comprising 7-spiking neurons\nwith non-plastic synapses which we show is extremely robust in tracking a range\nof concentrations. Our worm uses information regarding local temporal gradients\nin sodium chloride concentration to decide the instantaneous path for foraging,\nexploration and tracking. A key neuron pair in the C. elegans chemotaxis\nnetwork is the ASEL & ASER neuron pair, which capture the gradient of\nconcentration sensed by the worm in their graded membrane potentials. The\nprimary sensory neurons for our network are a pair of artificial spiking\nneurons that function as gradient detectors whose design is adapted from a\ncomputational model of the ASE neuron pair in C. elegans. Simulations show that\nour worm is able to detect the set-point with approximately four times higher\nprobability than the optimal memoryless Levy foraging model. We also show that\nour spiking neural network is much more efficient and noise-resilient while\nnavigating and tracking a contour, as compared to an equivalent non-spiking\nnetwork. We demonstrate that our model is extremely robust to noise and with\nslight modifications can be used for other practical applications such as\nobstacle avoidance. Our network model could also be extended for use in\nthree-dimensional contour tracking or obstacle avoidance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:54:22 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Santurkar", "Shibani", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1410.7883", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar and Bipin Rajendran", "title": "Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired\n  by C. elegans Chemotaxis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a spiking neural network for navigation motivated by the\nchemotaxis network of Caenorhabditis elegans. Our network uses information\nregarding temporal gradients in the tracking variable's concentration to make\nnavigational decisions. The gradient information is determined by mimicking the\nunderlying mechanisms of the ASE neurons of C. elegans. Simulations show that\nour model is able to forage and track a target set-point in extremely noisy\nenvironments. We develop a VLSI implementation for the main gradient detector\nneurons, which could be integrated with standard comparator circuitry to\ndevelop a robust circuit for navigation and contour tracking.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:56:00 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Santurkar", "Shibani", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1410.8206", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech\n  Zaremba", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "comments": "ACL 2015 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) is a new approach to machine translation\nthat has shown promising results that are comparable to traditional approaches.\nA significant weakness in conventional NMT systems is their inability to\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\nsmall vocabularies with a single unk symbol that represents every possible\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\neffective technique to address this problem. We train an NMT system on data\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\nsystem to emit, for each OOV word in the target sentence, the position of its\ncorresponding word in the source sentence. This information is later utilized\nin a post-processing step that translates every OOV word using a dictionary.\nOur experiments on the WMT14 English to French translation task show that this\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\nour NMT system is the first to surpass the best result achieved on a WMT14\ncontest task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 00:20:31 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 19:44:50 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 23:11:46 GMT"}, {"version": "v4", "created": "Sat, 30 May 2015 19:57:28 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Sutskever", "Ilya", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1410.8586", "submitter": "Tao Chen", "authors": "Tao Chen, Damian Borth, Trevor Darrell and Shih-Fu Chang", "title": "DeepSentiBank: Visual Sentiment Concept Classification with Deep\n  Convolutional Neural Networks", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a visual sentiment concept classification method based\non deep convolutional neural networks (CNNs). The visual sentiment concepts are\nadjective noun pairs (ANPs) automatically discovered from the tags of web\nphotos, and can be utilized as effective statistical cues for detecting\nemotions depicted in the images. Nearly one million Flickr images tagged with\nthese ANPs are downloaded to train the classifiers of the concepts. We adopt\nthe popular model of deep convolutional neural networks which recently shows\ngreat performance improvement on classifying large-scale web-based image\ndataset such as ImageNet. Our deep CNNs model is trained based on Caffe, a\nnewly developed deep learning framework. To deal with the biased training data\nwhich only contains images with strong sentiment and to prevent overfitting, we\ninitialize the model with the model weights trained from ImageNet. Performance\nevaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called\nDeepSentiBank) is significantly improved in both annotation accuracy and\nretrieval performance, compared to its predecessors which mainly use binary SVM\nclassification models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:57:12 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Chen", "Tao", ""], ["Borth", "Damian", ""], ["Darrell", "Trevor", ""], ["Chang", "Shih-Fu", ""]]}]