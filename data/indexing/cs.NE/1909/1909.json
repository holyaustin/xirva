[{"id": "1909.00021", "submitter": "Javier Turek", "authors": "Javier S. Turek, Shailee Jain, Vy Vo, Mihai Capota, Alexander G. Huth,\n  Theodore L. Willke", "title": "Approximating Stacked and Bidirectional Recurrent Architectures with the\n  Delayed Recurrent Neural Network", "comments": "to be published in Proceedings of International Conference on Machine\n  Learning 2020 (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that topological enhancements to recurrent neural\nnetworks (RNNs) can increase their expressiveness and representational\ncapacity. Two popular enhancements are stacked RNNs, which increases the\ncapacity for learning non-linear functions, and bidirectional processing, which\nexploits acausal information in a sequence. In this work, we explore the\ndelayed-RNN, which is a single-layer RNN that has a delay between the input and\noutput. We prove that a weight-constrained version of the delayed-RNN is\nequivalent to a stacked-RNN. We also show that the delay gives rise to partial\nacausality, much like bidirectional networks. Synthetic experiments confirm\nthat the delayed-RNN can mimic bidirectional networks, solving some acausal\ntasks similarly, and outperforming them in others. Moreover, we show similar\nperformance to bidirectional networks in a real-world natural language\nprocessing task. These results suggest that delayed-RNNs can approximate\ntopologies including stacked RNNs, bidirectional RNNs, and stacked\nbidirectional RNNs - but with equivalent or faster runtimes for the\ndelayed-RNNs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:18:04 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 05:45:08 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Turek", "Javier S.", ""], ["Jain", "Shailee", ""], ["Vo", "Vy", ""], ["Capota", "Mihai", ""], ["Huth", "Alexander G.", ""], ["Willke", "Theodore L.", ""]]}, {"id": "1909.00025", "submitter": "Sebastian Flennerhag", "authors": "Sebastian Flennerhag and Andrei A. Rusu and Razvan Pascanu and\n  Francesco Visin and Hujun Yin and Raia Hadsell", "title": "Meta-Learning with Warped Gradient Descent", "comments": "28 pages, 13 figures, 3 tables. Published as a conference paper at\n  ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an efficient update rule from data that promotes rapid learning of\nnew tasks from the same distribution remains an open problem in meta-learning.\nTypically, previous works have approached this issue either by attempting to\ntrain a neural network that directly produces updates or by attempting to learn\nbetter initialisations or scaling factors for a gradient-based update rule.\nBoth of these approaches pose challenges. On one hand, directly producing an\nupdate forgoes a useful inductive bias and can easily lead to non-converging\nbehaviour. On the other hand, approaches that try to control a gradient-based\nupdate rule typically resort to computing gradients through the learning\nprocess to obtain their meta-gradients, leading to methods that can not scale\nbeyond few-shot task adaptation. In this work, we propose Warped Gradient\nDescent (WarpGrad), a method that intersects these approaches to mitigate their\nlimitations. WarpGrad meta-learns an efficiently parameterised preconditioning\nmatrix that facilitates gradient descent across the task distribution.\nPreconditioning arises by interleaving non-linear layers, referred to as\nwarp-layers, between the layers of a task-learner. Warp-layers are meta-learned\nwithout backpropagating through the task training process in a manner similar\nto methods that learn to directly produce updates. WarpGrad is computationally\nefficient, easy to implement, and can scale to arbitrarily large meta-learning\nproblems. We provide a geometrical interpretation of the approach and evaluate\nits effectiveness in a variety of settings, including few-shot, standard\nsupervised, continual and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:27:35 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 08:57:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Flennerhag", "Sebastian", ""], ["Rusu", "Andrei A.", ""], ["Pascanu", "Razvan", ""], ["Visin", "Francesco", ""], ["Yin", "Hujun", ""], ["Hadsell", "Raia", ""]]}, {"id": "1909.00052", "submitter": "Amey Agrawal", "authors": "Amey Agrawal, Rohit Karlupia", "title": "Learning Digital Circuits: A Journey Through Weight Invariant\n  Self-Pruning Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, in the paper \"Weight Agnostic Neural Networks\" Gaier & Ha utilized\narchitecture search to find networks where the topology completely encodes the\nknowledge. However, architecture search in topology space is expensive. We use\nthe existing framework of binarized networks to find performant topologies by\nconstraining the weights to be either, zero or one. We show that such\ntopologies achieve performance similar to standard networks while pruning more\nthan 99% weights. We further demonstrate that these topologies can perform\ntasks using constant weights without any explicit tuning. Finally, we discover\nthat in our setup each neuron acts like a NOR gate, virtually learning a\ndigital circuit. We demonstrate the efficacy of our approach on computer vision\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:07:39 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 18:42:12 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 22:23:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Agrawal", "Amey", ""], ["Karlupia", "Rohit", ""]]}, {"id": "1909.00141", "submitter": "Deren Lei", "authors": "Siyao Li, Deren Lei, Pengda Qin, William Yang Wang", "title": "Deep Reinforcement Learning with Distributional Semantic Rewards for\n  Abstractive Summarization", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has been a commonly-used strategy for the\nabstractive summarization task to address both the exposure bias and\nnon-differentiable task issues. However, the conventional reward Rouge-L simply\nlooks for exact n-grams matches between candidates and annotated references,\nwhich inevitably makes the generated sentences repetitive and incoherent. In\nthis paper, instead of Rouge-L, we explore the practicability of utilizing the\ndistributional semantics to measure the matching degrees. With distributional\nsemantics, sentence-level evaluation can be obtained, and semantically-correct\nphrases can also be generated without being limited to the surface form of the\nreference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets\nshow that our proposed distributional semantics reward (DSR) has distinct\nsuperiority in capturing the lexical and compositional diversity of natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 06:13:33 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 23:30:26 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Li", "Siyao", ""], ["Lei", "Deren", ""], ["Qin", "Pengda", ""], ["Wang", "William Yang", ""]]}, {"id": "1909.00237", "submitter": "Shubhankar Mohapatra", "authors": "Shubhankar Mohapatra, Moumita Sarkar, Anjali Mohapatra and Bhawani\n  Sankar Biswal", "title": "Triclustering of Gene Expression Microarray Data Using Coarse-Grained\n  Parallel Genetic Algorithm", "comments": null, "journal-ref": "Springer Lecture Notes in Networks and Systems 2016 - 2020", "doi": null, "report-no": null, "categories": "cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray data analysis is one of the major area of research in the field\ncomputational biology. Numerous techniques like clustering, biclustering are\noften applied to microarray data to extract meaningful outcomes which play key\nroles in practical healthcare affairs like disease identification, drug\ndiscovery etc. But these techniques become obsolete when time as an another\nfactor is considered for evaluation in such data. This problem motivates to use\ntriclustering method on gene expression 3D microarray data. In this article, a\nnew methodology based on coarse-grained parallel genetic approach is proposed\nto locate meaningful triclusters in gene expression data. The outcomes are\nquite impressive as they are more effective as compared to traditional state of\nthe art genetic approaches previously applied for triclustering of 3D GCT\nmicroarray data.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 16:15:12 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mohapatra", "Shubhankar", ""], ["Sarkar", "Moumita", ""], ["Mohapatra", "Anjali", ""], ["Biswal", "Bhawani Sankar", ""]]}, {"id": "1909.00361", "submitter": "Yiming Cui", "authors": "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu", "title": "Cross-Lingual Machine Reading Comprehension", "comments": "10 pages, accepted as a conference paper at EMNLP-IJCNLP 2019 (long\n  paper)", "journal-ref": "EMNLP 2019 1586-1595", "doi": "10.18653/v1/D19-1169", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the community has made great progress on Machine Reading Comprehension\n(MRC) task, most of the previous works are solving English-based MRC problems,\nand there are few efforts on other languages mainly due to the lack of\nlarge-scale training data. In this paper, we propose Cross-Lingual Machine\nReading Comprehension (CLMRC) task for the languages other than English.\nFirstly, we present several back-translation approaches for CLMRC task, which\nis straightforward to adopt. However, to accurately align the answer into\nanother language is difficult and could introduce additional noise. In this\ncontext, we propose a novel model called Dual BERT, which takes advantage of\nthe large-scale training data provided by rich-resource language (such as\nEnglish) and learn the semantic relations between the passage and question in a\nbilingual context, and then utilize the learned knowledge to improve reading\ncomprehension performance of low-resource language. We conduct experiments on\ntwo Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The\nresults show consistent and significant improvements over various\nstate-of-the-art systems by a large margin, which demonstrate the potentials in\nCLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 09:14:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Cui", "Yiming", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Qin", "Bing", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1909.00562", "submitter": "Junya Ono", "authors": "Junya Ono, Masao Utiyama, Eiichiro Sumita", "title": "Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent\n  Neural Network Machine Translation", "comments": "9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduction of training time is an important issue in many tasks like patent\ntranslation involving neural networks. Data parallelism and model parallelism\nare two common approaches for reducing training time using multiple graphics\nprocessing units (GPUs) on one machine. In this paper, we propose a hybrid\ndata-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent\nneural network (RNN) machine translation. We apply a model parallel approach to\nthe RNN encoder-decoder part of the Seq2Seq model and a data parallel approach\nto the attention-softmax part of the model. We achieved a speed-up of 4.13 to\n4.20 times when using 4 GPUs compared with the training speed when using 1 GPU\nwithout affecting machine translation accuracy as measured in terms of BLEU\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 06:41:34 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 06:12:02 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Ono", "Junya", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "1909.00590", "submitter": "Hansika Hewamalage", "authors": "Hansika Hewamalage, Christoph Bergmeir, Kasun Bandara", "title": "Recurrent Neural Networks for Time Series Forecasting: Current Status\n  and Future Directions", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2020.06.008", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) have become competitive forecasting methods,\nas most notably shown in the winning method of the recent M4 competition.\nHowever, established statistical models such as ETS and ARIMA gain their\npopularity not only from their high accuracy, but they are also suitable for\nnon-expert users as they are robust, efficient, and automatic. In these areas,\nRNNs have still a long way to go. We present an extensive empirical study and\nan open-source software framework of existing RNN architectures for\nforecasting, that allow us to develop guidelines and best practices for their\nuse. For example, we conclude that RNNs are capable of modelling seasonality\ndirectly if the series in the dataset possess homogeneous seasonal patterns,\notherwise we recommend a deseasonalization step. Comparisons against ETS and\nARIMA demonstrate that the implemented (semi-)automatic RNN models are no\nsilver bullets, but they are competitive alternatives in many situations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 08:20:30 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 12:32:55 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 01:12:24 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2020 05:46:58 GMT"}, {"version": "v5", "created": "Wed, 23 Dec 2020 01:56:57 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Hewamalage", "Hansika", ""], ["Bergmeir", "Christoph", ""], ["Bandara", "Kasun", ""]]}, {"id": "1909.00669", "submitter": "Jawar Singh Dr.", "authors": "Alok Kumar Kamal and Jawar Singh", "title": "Ultra-Low Energy and High Speed LIF Neuron using Silicon Bipolar Impact\n  Ionization MOSFET for Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Silicon bipolar impact ionization MOSFET offers the potential for realization\nof leaky integrated fire (LIF) neuron due to the presence of parasitic BJT in\nthe floating body. In this work, we have proposed an L shaped gate bipolar\nimpact ionization MOS (L-BIMOS), with reduced breakdown voltage ($V_{B}$ = 1.68\nV) and demonstrated the functioning of LIF neuron based on positive feedback\nmechanism of parasitic BJT. Using 2-D TCAD simulations, we manifest that the\nproposed L-BIMOS exhibits a low threshold voltage (0.2 V) for firing a spike,\nand the minimum energy required to fire a single spike for L-BIMOS is\ncalculated to be 0.18 pJ, which makes proposed device $194\\times$ more energy\nefficient than PD-SOI MOSFET silicon neuron (MOSFET silicon neuron) and\n$5\\times10^{3}$ times more energy efficient than analog/digital circuit based\nconventional neuron. Furthermore, the proposed L-BIMOS silicon neuron exhibits\nspiking frequency in the GHz range, when the drain is biased at $V_{DG}$ = 2.0\nV.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 11:14:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kamal", "Alok Kumar", ""], ["Singh", "Jawar", ""]]}, {"id": "1909.00732", "submitter": "Sayantan Auddy", "authors": "Sayantan Auddy, Sven Magg, Stefan Wermter", "title": "Hierarchical Control for Bipedal Locomotion using Central Pattern\n  Generators and Neural Networks", "comments": "In: Proceedings of the Joint IEEE International Conference on\n  Development and Learning and on Epigenetic Robotics (ICDL-EpiRob), Oslo,\n  Norway, Aug. 19-22, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of bipedal locomotion may be attributed to the difficulty in\nsynchronizing joint movements while at the same time achieving high-level\nobjectives such as walking in a particular direction. Artificial central\npattern generators (CPGs) can produce synchronized joint movements and have\nbeen used in the past for bipedal locomotion. However, most existing CPG-based\napproaches do not address the problem of high-level control explicitly. We\npropose a novel hierarchical control mechanism for bipedal locomotion where an\noptimized CPG network is used for joint control and a neural network acts as a\nhigh-level controller for modulating the CPG network. By separating motion\ngeneration from motion modulation, the high-level controller does not need to\ncontrol individual joints directly but instead can develop to achieve a higher\ngoal using a low-dimensional control signal. The feasibility of the\nhierarchical controller is demonstrated through simulation experiments using\nthe Neuro-Inspired Companion (NICO) robot. Experimental results demonstrate the\ncontroller's ability to function even without the availability of an exact\nrobot model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:27:30 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Auddy", "Sayantan", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1909.00835", "submitter": "Stephen Whitelam", "authors": "Stephen Whitelam, Daniel Jacobson, Isaac Tamblyn", "title": "Evolutionary reinforcement learning of dynamical large deviations", "comments": null, "journal-ref": null, "doi": "10.1063/5.0015301", "report-no": null, "categories": "cond-mat.stat-mech cs.LG cs.NE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to calculate the likelihood of dynamical large deviations using\nevolutionary reinforcement learning. An agent, a stochastic model, propagates a\ncontinuous-time Monte Carlo trajectory and receives a reward conditioned upon\nthe values of certain path-extensive quantities. Evolution produces\nprogressively fitter agents, eventually allowing the calculation of a piece of\na large-deviation rate function for a particular model and path-extensive\nquantity. For models with small state spaces the evolutionary process acts\ndirectly on rates, and for models with large state spaces the process acts on\nthe weights of a neural network that parameterizes the model's rates. This\napproach shows how path-extensive physics problems can be considered within a\nframework widely used in machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:07:51 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 20:42:32 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 00:46:56 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 19:21:04 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Whitelam", "Stephen", ""], ["Jacobson", "Daniel", ""], ["Tamblyn", "Isaac", ""]]}, {"id": "1909.00894", "submitter": "Yu Chen", "authors": "Cong Wang and Yu Chen and Jun He and Chengwang Xie", "title": "Error Analysis of Elitist Randomized Search Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When globally optimal solutions of complicated optimization problems cannot\nbe located by evolutionary algorithms (EAs) in polynomial expected running\ntime, the hitting time/running time analysis is not flexible enough to\naccommodate the requirement of theoretical study, because sometimes we have no\nidea on what approximation ratio is available in polynomial expected running\ntime. Thus, it is necessary to propose an alternative routine for the\ntheoretical analysis of EAs. To bridge the gap between theoretical analysis and\nalgorithm implementation, in this paper we perform an error analysis where\nexpected approximation error is estimated to evaluate performances of\nrandomized search heuristics (RSHs). Based on the Markov chain model of RSHs,\nthe multi-step transition matrix can be computed by diagonalizing the one-step\ntransition matrix, and a general framework for estimation of expected\napproximation errors is proposed. Case studies indicate that the error analysis\nworks well for both uni- and multi-modal benchmark problems. It leads to\nprecise estimations of approximation error instead of asymptotic results on\nfitness values, which demonstrates its competitiveness to fixed budget\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:11:20 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 11:28:30 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 12:31:44 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 12:08:18 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Cong", ""], ["Chen", "Yu", ""], ["He", "Jun", ""], ["Xie", "Chengwang", ""]]}, {"id": "1909.01302", "submitter": "Zihan Pan", "authors": "Zihan Pan, Yansong Chua, Jibin Wu, Malu Zhang, Haizhou Li, and\n  Eliathamby Ambikairajah", "title": "An efficient and perceptually motivated auditory neural encoding and\n  decoding algorithm for spiking neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auditory front-end is an integral part of a spiking neural network (SNN) when\nperforming auditory cognitive tasks. It encodes the temporal dynamic stimulus,\nsuch as speech and audio, into an efficient, effective and reconstructable\nspike pattern to facilitate the subsequent processing. However, most of the\nauditory front-ends in current studies have not made use of recent findings in\npsychoacoustics and physiology concerning human listening. In this paper, we\npropose a neural encoding and decoding scheme that is optimized for speech\nprocessing. The neural encoding scheme, that we call Biologically plausible\nAuditory Encoding (BAE), emulates the functions of the perceptual components of\nthe human auditory system, that include the cochlear filter bank, the inner\nhair cells, auditory masking effects from psychoacoustic models, and the spike\nneural encoding by the auditory nerve. We evaluate the perceptual quality of\nthe BAE scheme using PESQ; the performance of the BAE based on speech\nrecognition experiments. Finally, we also built and published two spike-version\nof speech datasets: the Spike-TIDIGITS and the Spike-TIMIT, for researchers to\nuse and benchmarking of future SNN research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:48:47 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 06:02:57 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Pan", "Zihan", ""], ["Chua", "Yansong", ""], ["Wu", "Jibin", ""], ["Zhang", "Malu", ""], ["Li", "Haizhou", ""], ["Ambikairajah", "Eliathamby", ""]]}, {"id": "1909.01311", "submitter": "Charlotte Frenkel", "authors": "Charlotte Frenkel, Martin Lefebvre, David Bol", "title": "Learning without feedback: Fixed random learning signals allow for\n  feedforward training of deep neural networks", "comments": "This document is the paper as accepted for publication in the\n  Frontiers in Neuroscience journal, the fully-edited paper is available at\n  https://www.frontiersin.org/articles/10.3389/fnins.2021.629892", "journal-ref": null, "doi": "10.3389/fnins.2021.629892", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the backpropagation of error algorithm enables deep neural network\ntraining, it implies (i) bidirectional synaptic weight transport and (ii)\nupdate locking until the forward and backward passes are completed. Not only do\nthese constraints preclude biological plausibility, but they also hinder the\ndevelopment of low-cost adaptive smart sensors at the edge, as they severely\nconstrain memory accesses and entail buffering overhead. In this work, we show\nthat the one-hot-encoded labels provided in supervised classification problems,\ndenoted as targets, can be viewed as a proxy for the error sign. Therefore,\ntheir fixed random projections enable a layerwise feedforward training of the\nhidden layers, thus solving the weight transport and update locking problems\nwhile relaxing the computational and memory requirements. Based on these\nobservations, we propose the direct random target projection (DRTP) algorithm\nand demonstrate that it provides a tradeoff between accuracy and computational\ncost that is suitable for adaptive edge computing devices.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 17:04:00 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 22:09:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Frenkel", "Charlotte", ""], ["Lefebvre", "Martin", ""], ["Bol", "David", ""]]}, {"id": "1909.01571", "submitter": "JaeSung Choi", "authors": "Jaesung Choi and Pilwon Kim", "title": "Reservoir Computing based on Quenched Chaos", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2020.110131", "report-no": null, "categories": "nlin.CD cs.CC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing(RC) is a brain-inspired computing framework that employs\na transient dynamical system whose reaction to an input signal is transformed\nto a target output. One of the central problems in RC is to find a reliable\nreservoir with a large criticality, since computing performance of a reservoir\nis maximized near the phase transition. In this work, we propose a continuous\nreservoir that utilizes transient dynamics of coupled chaotic oscillators in a\ncritical regime where sudden amplitude death occurs. This \"explosive death\" not\nonly brings the system a large criticality which provides a variety of orbits\nfor computing, but also stabilizes them which otherwise diverge soon in chaotic\nunits. The proposed framework shows better results in tasks for signal\nreconstructions than RC based on explosive synchronization of regular phase\noscillators. We also show that the information capacity of the reservoirs can\nbe used as a predictive measure for computational capability of a reservoir at\na critical point.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 06:46:20 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Choi", "Jaesung", ""], ["Kim", "Pilwon", ""]]}, {"id": "1909.01575", "submitter": "Jacob Rafati", "authors": "Jacob Rafati and David C. Noelle", "title": "Learning sparse representations in reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms allow artificial agents to improve\ntheir selection of actions to increase rewarding experiences in their\nenvironments. Temporal Difference (TD) Learning -- a model-free RL method -- is\na leading account of the midbrain dopamine system and the basal ganglia in\nreinforcement learning. These algorithms typically learn a mapping from the\nagent's current sensed state to a selected action (known as a policy function)\nvia learning a value function (expected future rewards). TD Learning methods\nhave been very successful on a broad range of control tasks, but learning can\nbecome intractably slow as the state space of the environment grows. This has\nmotivated methods that learn internal representations of the agent's state,\neffectively reducing the size of the state space and restructuring state\nrepresentations in order to support generalization. However, TD Learning\ncoupled with an artificial neural network, as a function approximator, has been\nshown to fail to learn some fairly simple control tasks, challenging this\nexplanation of reward-based learning. We hypothesize that such failures do not\narise in the brain because of the ubiquitous presence of lateral inhibition in\nthe cortex, producing sparse distributed internal representations that support\nthe learning of expected future reward. The sparse conjunctive representations\ncan avoid catastrophic interference while still supporting generalization. We\nprovide support for this conjecture through computational simulations,\ndemonstrating the benefits of learned sparse representations for three\nproblematic classic control tasks: Puddle-world, Mountain-car, and Acrobot.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 06:58:32 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Rafati", "Jacob", ""], ["Noelle", "David C.", ""]]}, {"id": "1909.01671", "submitter": "Hal Ccsd", "authors": "Nicolas Audebert (OBELIX), Alexandre Boulch, Bertrand Le Saux,\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Distance transform regression for spatially-aware deep semantic\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding visual scenes relies more and more on dense pixel-wise\nclassification obtained via deep fully convolutional neural networks. However,\ndue to the nature of the networks, predictions often suffer from blurry\nboundaries and ill-segmented shapes, fueling the need for post-processing. This\nwork introduces a new semantic segmentation regularization based on the\nregression of a distance transform. After computing the distance transform on\nthe label masks, we train a FCN in a multi-task setting in both discrete and\ncontinuous spaces by learning jointly classification and distance regression.\nThis requires almost no modification of the network structure and adds a very\nlow overhead to the training process. Learning to approximate the distance\ntransform back-propagates spatial cues that implicitly regularizes the\nsegmentation. We validate this technique with several architectures on various\ndatasets, and we show significant improvements compared to competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:05:08 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX"], ["Boulch", "Alexandre", "", "OBELIX"], ["Saux", "Bertrand Le", "", "OBELIX"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1909.01709", "submitter": "Niklas Heim", "authors": "Niklas Heim, James E. Avery", "title": "Adaptive Anomaly Detection in Chaotic Time Series with a Spatially Aware\n  Echo State Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work builds an automated anomaly detection method for chaotic time\nseries, and more concretely for turbulent, high-dimensional, ocean simulations.\nWe solve this task by extending the Echo State Network by spatially aware input\nmaps, such as convolutions, gradients, cosine transforms, et cetera, as well as\na spatially aware loss function. The spatial ESN is used to create predictions\nwhich reduce the detection problem to thresholding of the prediction error. We\nbenchmark our detection framework on different tasks of increasing difficulty\nto show the generality of the framework before applying it to raw climate model\noutput in the region of the Japanese ocean current Kuroshio, which exhibits a\nbimodality that is not easily detected by the naked eye. The code is available\nas an open source Python package, Torsk, available at\nhttps://github.com/nmheim/torsk, where we also provide supplementary material\nand programs that reproduce the results shown in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:46:07 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Heim", "Niklas", ""], ["Avery", "James E.", ""]]}, {"id": "1909.01730", "submitter": "Carl Andersson", "authors": "Carl Andersson, Ant\\^onio H. Ribeiro, Koen Tiels, Niklas Wahlstr\\\"om\n  and Thomas B. Sch\\\"on", "title": "Deep Convolutional Networks in System Identification", "comments": "Accepted to Conference on Decision and Control, The first two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments within deep learning are relevant for nonlinear system\nidentification problems. In this paper, we establish connections between the\ndeep learning and the system identification communities. It has recently been\nshown that convolutional architectures are at least as capable as recurrent\narchitectures when it comes to sequence modeling tasks. Inspired by these\nresults we explore the explicit relationships between the recently proposed\ntemporal convolutional network (TCN) and two classic system identification\nmodel structures; Volterra series and block-oriented models. We end the paper\nwith an experimental study where we provide results on two real-world problems,\nthe well-known Silverbox dataset and a newer dataset originating from ground\nvibration experiments on an F-16 fighter aircraft.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:34:32 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 14:30:44 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Andersson", "Carl", ""], ["Ribeiro", "Ant\u00f4nio H.", ""], ["Tiels", "Koen", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1909.02115", "submitter": "Ali Sharifara", "authors": "Razieh Tavakoli, Mohammad Najafi and Ali Sharifara", "title": "Artificial Neural Networks and Adaptive Neuro-fuzzy Models for\n  Prediction of Remaining Useful Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U.S. water distribution system contains thousands of miles of pipes\nconstructed from different materials, and of various sizes, and age. These\npipes suffer from physical, environmental, structural and operational stresses,\ncausing deterioration which eventually leads to their failure. Pipe\ndeterioration results in increased break rates, reduced hydraulic capacity, and\ndetrimental impacts on water quality. Therefore, it is crucial to use accurate\nmodels to forecast deterioration rates along with estimating the remaining\nuseful life of the pipes to implement essential interference plans in order to\nprevent catastrophic failures. This paper discusses a computational model that\nforecasts the RUL of water pipes by applying Artificial Neural Networks (ANNs)\nas well as Adaptive Neural Fuzzy Inference System (ANFIS). These models are\ntrained and tested acquired field data to identify the significant parameters\nthat impact the prediction of RUL. It is concluded that, on average, with\napproximately 10\\% of wall thickness loss in existing cast iron, ductile iron,\nasbestos-cement, and steel water pipes, the reduction of the remaining useful\nlife is approximately 50%\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 01:29:52 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Tavakoli", "Razieh", ""], ["Najafi", "Mohammad", ""], ["Sharifara", "Ali", ""]]}, {"id": "1909.02425", "submitter": "Andr\\'es Camero", "authors": "Andr\\'es Camero, Jamal Toutouh, Enrique Alba", "title": "Random Error Sampling-based Recurrent Neural Network Architecture\n  Optimization", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence 96 (2020):\n  103946", "doi": "10.1016/j.engappai.2020.103946", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are good at solving prediction problems. However,\nfinding a network that suits a problem is quite hard because their performance\nis strongly affected by their architecture configuration. Automatic\narchitecture optimization methods help to find the most suitable design, but\nthey are not extensively adopted because of their high computational cost. In\nthis work, we introduce the Random Error Sampling-based Neuroevolution (RESN),\nan evolutionary algorithm that uses the mean absolute error random sampling, a\ntraining-free approach to predict the expected performance of an artificial\nneural network, to optimize the architecture of a network. We empirically\nvalidate our proposal on three prediction problems, and compare our technique\nto training-based architecture optimization techniques and to neuroevolutionary\napproaches. Our findings show that we can achieve state-of-the-art error\nperformance and that we reduce by half the time needed to perform the\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:38:41 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 21:53:23 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 10:55:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Camero", "Andr\u00e9s", ""], ["Toutouh", "Jamal", ""], ["Alba", "Enrique", ""]]}, {"id": "1909.02549", "submitter": "Daniel Saunders", "authors": "Daniel J. Saunders, Cooper Sigrist, Kenneth Chaney, Robert Kozma, Hava\n  T. Siegelmann", "title": "Minibatch Processing in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are a promising candidate for\nbiologically-inspired and energy efficient computation. However, their\nsimulation is notoriously time consuming, and may be seen as a bottleneck in\ndeveloping competitive training methods with potential deployment on\nneuromorphic hardware platforms. To address this issue, we provide an\nimplementation of mini-batch processing applied to clock-based SNN simulation,\nleading to drastically increased data throughput. To our knowledge, this is the\nfirst general-purpose implementation of mini-batch processing in a spiking\nneural networks simulator, which works with arbitrary neuron and synapse\nmodels. We demonstrate nearly constant-time scaling with batch size on a\nsimulation setup (up to GPU memory limits), and showcase the effectiveness of\nlarge batch sizes in two SNN application domains, resulting in $\\approx$880X\nand $\\approx$24X reductions in wall-clock time respectively. Different\nparameter reduction techniques are shown to produce different learning outcomes\nin a simulation of networks trained with spike-timing-dependent plasticity.\nMachine learning practitioners and biological modelers alike may benefit from\nthe drastically reduced simulation time and increased iteration speed this\nmethod enables. Code to reproduce the benchmarks and experimental findings in\nthis paper can be found at https://github.com/djsaunde/snn-minibatch.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:42:31 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Saunders", "Daniel J.", ""], ["Sigrist", "Cooper", ""], ["Chaney", "Kenneth", ""], ["Kozma", "Robert", ""], ["Siegelmann", "Hava T.", ""]]}, {"id": "1909.02562", "submitter": "Houssem Ben Braiek", "authors": "Houssem Ben Braiek and Foutse Khomh", "title": "TFCheck : A TensorFlow Library for Detecting Training Issues in Neural\n  Network Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing inclusion of Machine Learning (ML) models in safety critical\nsystems like autonomous cars have led to the development of multiple\nmodel-based ML testing techniques. One common denominator of these testing\ntechniques is their assumption that training programs are adequate and\nbug-free. These techniques only focus on assessing the performance of the\nconstructed model using manually labeled data or automatically generated data.\nHowever, their assumptions about the training program are not always true as\ntraining programs can contain inconsistencies and bugs. In this paper, we\nexamine training issues in ML programs and propose a catalog of verification\nroutines that can be used to detect the identified issues, automatically. We\nimplemented the routines in a Tensorflow-based library named TFCheck. Using\nTFCheck, practitioners can detect the aforementioned issues automatically. To\nassess the effectiveness of TFCheck, we conducted a case study with real-world,\nmutants, and synthetic training programs. Results show that TFCheck can\nsuccessfully detect training issues in ML code implementations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:21:22 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Braiek", "Houssem Ben", ""], ["Khomh", "Foutse", ""]]}, {"id": "1909.02603", "submitter": "Kameron Harris", "authors": "Kameron Decker Harris", "title": "Additive function approximation in the brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological learning systems such as the mushroom body, hippocampus, and\ncerebellum are built from sparsely connected networks of neurons. For a new\nunderstanding of such networks, we study the function spaces induced by sparse\nrandom features and characterize what functions may and may not be learned. A\nnetwork with $d$ inputs per neuron is found to be equivalent to an additive\nmodel of order $d$, whereas with a degree distribution the network combines\nadditive terms of different orders. We identify three specific advantages of\nsparsity: additive function approximation is a powerful inductive bias that\nlimits the curse of dimensionality, sparse networks are stable to outlier noise\nin the inputs, and sparse random features are scalable. Thus, even simple brain\narchitectures can be powerful function approximators. Finally, we hope that\nthis work helps popularize kernel theories of networks among computational\nneuroscientists.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 19:07:33 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 21:41:07 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Harris", "Kameron Decker", ""]]}, {"id": "1909.02702", "submitter": "Stefano Massaroli", "authors": "Stefano Massaroli, Michael Poli, Federico Califano, Angela Faragasso,\n  Jinkyoo Park, Atsushi Yamashita, Hajime Asama", "title": "Port-Hamiltonian Approach to Neural Network Training", "comments": "To appear in the Proceedings of the 58th IEEE Conference on Decision\n  and Control (CDC 2019). The first two authors contributed equally to the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are discrete entities: subdivided into discrete layers and\nparametrized by weights which are iteratively optimized via difference\nequations. Recent work proposes networks with layer outputs which are no longer\nquantized but are solutions of an ordinary differential equation (ODE);\nhowever, these networks are still optimized via discrete methods (e.g. gradient\ndescent). In this paper, we explore a different direction: namely, we propose a\nnovel framework for learning in which the parameters themselves are solutions\nof ODEs. By viewing the optimization process as the evolution of a\nport-Hamiltonian system, we can ensure convergence to a minimum of the\nobjective function. Numerical experiments have been performed to show the\nvalidity and effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 03:31:40 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Massaroli", "Stefano", ""], ["Poli", "Michael", ""], ["Califano", "Federico", ""], ["Faragasso", "Angela", ""], ["Park", "Jinkyoo", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""]]}, {"id": "1909.02982", "submitter": "Theo Jaunet", "authors": "Theo Jaunet, Romain Vuillemot and Christian Wolf", "title": "DRLViz: Understanding Decisions and Memory in Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DRLViz, a visual analytics interface to interpret the internal\nmemory of an agent (e.g. a robot) trained using deep reinforcement learning.\nThis memory is composed of large temporal vectors updated when the agent moves\nin an environment and is not trivial to understand due to the number of\ndimensions, dependencies to past vectors, spatial/temporal correlations, and\nco-correlation between dimensions. It is often referred to as a black box as\nonly inputs (images) and outputs (actions) are intelligible for humans. Using\nDRLViz, experts are assisted to interpret decisions using memory reduction\ninteractions, and to investigate the role of parts of the memory when errors\nhave been made (e.g. wrong direction). We report on DRLViz applied in the\ncontext of video games simulators (ViZDoom) for a navigation scenario with item\ngathering tasks. We also report on experts evaluation using DRLViz, and\napplicability of DRLViz to other scenarios and navigation problems beyond\nsimulation games, as well as its contribution to black box models\ninterpretability and explainability in the field of visual analytics.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:56:39 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 16:07:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Jaunet", "Theo", ""], ["Vuillemot", "Romain", ""], ["Wolf", "Christian", ""]]}, {"id": "1909.03287", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Luigi Di Sotto", "title": "A Non-Negative Factorization approach to node pooling in Graph\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses a pooling mechanism to induce subsampling in graph\nstructured data and introduces it as a component of a graph convolutional\nneural network. The pooling mechanism builds on the Non-Negative Matrix\nFactorization (NMF) of a matrix representing node adjacency and node similarity\nas adaptively obtained through the vertices embedding learned by the model.\nSuch mechanism is applied to obtain an incrementally coarser graph where nodes\nare adaptively pooled into communities based on the outcomes of the\nnon-negative factorization. The empirical analysis on graph classification\nbenchmarks shows how such coarsening process yields significant improvements in\nthe predictive performance of the model with respect to its non-pooled\ncounterpart.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 15:27:49 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bacciu", "Davide", ""], ["Di Sotto", "Luigi", ""]]}, {"id": "1909.03306", "submitter": "Massimiliano Lupo Pasini Dr.", "authors": "Massimiliano Lupo Pasini, Junqi Yin, Ying Wai Li, Markus Eisenbach", "title": "A scalable constructive algorithm for the optimization of neural network\n  architectures", "comments": "12 pages, 15 figures, 3 table", "journal-ref": "Parallel Computing, 2021", "doi": "10.1016/j.parco.2021.102788", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new scalable method to optimize the architecture of an\nartificial neural network. The proposed algorithm, called Greedy Search for\nNeural Network Architecture, aims to determine a neural network with minimal\nnumber of layers that is at least as performant as neural networks of the same\nstructure identified by other hyperparameter search algorithms in terms of\naccuracy and computational cost. Numerical results performed on benchmark\ndatasets show that, for these datasets, our method outperforms state-of-the-art\nhyperparameter optimization algorithms in terms of attainable predictive\nperformance by the selected neural network architecture, and time-to-solution\nfor the hyperparameter optimization to complete.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 17:22:28 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 17:26:10 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 14:13:57 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Pasini", "Massimiliano Lupo", ""], ["Yin", "Junqi", ""], ["Li", "Ying Wai", ""], ["Eisenbach", "Markus", ""]]}, {"id": "1909.03342", "submitter": "Jun He Dr", "authors": "Jun He, Thomas Jansen, Christine Zarges", "title": "Unlimited Budget Analysis of Randomised Search Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance analysis of all kinds of randomised search heuristics is a\nrapidly growing and developing field. Run time and solution quality are two\npopular measures of the performance of these algorithms. The focus of this\npaper is on the solution quality an optimisation heuristic achieves, not on the\ntime it takes to reach this goal, setting it far apart from runtime analysis.\nWe contribute to its further development by introducing a novel analytical\nframework, called unlimited budget analysis, to derive the expected fitness\nvalue after arbitrary computational steps. It has its roots in the very\nrecently introduced approximation error analysis and bears some similarity to\nfixed budget analysis. We present the framework, apply it to simple\nmutation-based algorithms, covering both, local and global search. We provide\nanalytical results for a number of pseudo-Boolean functions for unlimited\nbudget analysis and compare them to results derived within the fixed budget\nframework for the same algorithms and functions. There are also results of\nexperiments to compare bounds obtained in the two different frameworks with the\nactual observed performance. The study show that unlimited budget analysis may\nlead to the same or more general estimation beyond fixed budget.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 21:30:58 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 11:34:50 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["He", "Jun", ""], ["Jansen", "Thomas", ""], ["Zarges", "Christine", ""]]}, {"id": "1909.03385", "submitter": "Hokchhay Tann", "authors": "Hokchhay Tann, Heng Zhao, Sherief Reda", "title": "A Resource-Efficient Embedded Iris Recognition System Using Fully\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of Fully Convolutional Networks (FCN) in iris segmentation have\nshown promising advances. For mobile and embedded systems, a significant\nchallenge is that the proposed FCN architectures are extremely computationally\ndemanding. In this article, we propose a resource-efficient, end-to-end iris\nrecognition flow, which consists of FCN-based segmentation, contour fitting,\nfollowed by Daugman normalization and encoding. To attain accurate and\nefficient FCN models, we propose a three-step SW/HW co-design methodology\nconsisting of FCN architectural exploration, precision quantization, and\nhardware acceleration. In our exploration, we propose multiple FCN models, and\nin comparison to previous works, our best-performing model requires 50X less\nFLOPs per inference while achieving a new state-of-the-art segmentation\naccuracy. Next, we select the most efficient set of models and further reduce\ntheir computational complexity through weights and activations quantization\nusing 8-bit dynamic fixed-point (DFP) format. Each model is then incorporated\ninto an end-to-end flow for true recognition performance evaluation. A few of\nour end-to-end pipelines outperform the previous state-of-the-art on two\ndatasets evaluated. Finally, we propose a novel DFP accelerator and fully\ndemonstrate the SW/HW co-design realization of our flow on an embedded FPGA\nplatform. In comparison with the embedded CPU, our hardware acceleration\nachieves up to 8.3X speedup for the overall pipeline while using less than 15%\nof the available FPGA resources. We also provide comparisons between the FPGA\nsystem and an embedded GPU showing different benefits and drawbacks for the two\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 04:21:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tann", "Hokchhay", ""], ["Zhao", "Heng", ""], ["Reda", "Sherief", ""]]}, {"id": "1909.03560", "submitter": "Anthony Rhodes", "authors": "Anthony D. Rhodes", "title": "Evolving Order and Chaos: Comparing Particle Swarm Optimization and\n  Genetic Algorithms for Global Coordination of Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply two evolutionary search algorithms: Particle Swarm Optimization\n(PSO) and Genetic Algorithms (GAs) to the design of Cellular Automata (CA) that\ncan perform computational tasks requiring global coordination. In particular,\nwe compare search efficiency for PSO and GAs applied to both the density\nclassification problem and to the novel generation of 'chaotic' CA. Our work\nfurthermore introduces a new variant of PSO, the Binary Global-Local PSO\n(BGL-PSO).\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 22:57:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Rhodes", "Anthony D.", ""]]}, {"id": "1909.03569", "submitter": "Prince Zizhuang Wang", "authors": "Prince Zizhuang Wang and William Yang Wang", "title": "Neural Gaussian Copula for Variational Autoencoder", "comments": "11 pages", "journal-ref": "EMNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational language models seek to estimate the posterior of latent\nvariables with an approximated variational posterior. The model often assumes\nthe variational posterior to be factorized even when the true posterior is not.\nThe learned variational posterior under this assumption does not capture the\ndependency relationships over latent variables. We argue that this would cause\na typical training problem called posterior collapse observed in all other\nvariational language models. We propose Gaussian Copula Variational Autoencoder\n(VAE) to avert this problem. Copula is widely used to model correlation and\ndependencies of high-dimensional random variables, and therefore it is helpful\nto maintain the dependency relationships that are lost in VAE. The empirical\nresults show that by modeling the correlation of latent variables explicitly\nusing a neural parametric copula, we can avert this training difficulty while\ngetting competitive results among all other VAE approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 00:10:58 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Prince Zizhuang", ""], ["Wang", "William Yang", ""]]}, {"id": "1909.03742", "submitter": "Jary Pomponi", "authors": "Jary Pomponi, Simone Scardapane, Vincenzo Lomonaco, Aurelio Uncini", "title": "Efficient Continual Learning in Neural Networks with Embedding\n  Regularization", "comments": null, "journal-ref": "Neurocomputing, 397, pp. 139-148, 2020", "doi": "10.1016/j.neucom.2020.01.093", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning of deep neural networks is a key requirement for scaling\nthem up to more complex applicative scenarios and for achieving real lifelong\nlearning of these architectures. Previous approaches to the problem have\nconsidered either the progressive increase in the size of the networks, or have\ntried to regularize the network behavior to equalize it with respect to\npreviously observed tasks. In the latter case, it is essential to understand\nwhat type of information best represents this past behavior. Common techniques\ninclude regularizing the past outputs, gradients, or individual weights. In\nthis work, we propose a new, relatively simple and efficient method to perform\ncontinual learning by regularizing instead the network internal embeddings. To\nmake the approach scalable, we also propose a dynamic sampling strategy to\nreduce the memory footprint of the required external storage. We show that our\nmethod performs favorably with respect to state-of-the-art approaches in the\nliterature, while requiring significantly less space in memory and\ncomputational time. In addition, inspired inspired by to recent works, we\nevaluate the impact of selecting a more flexible model for the activation\nfunctions inside the network, evaluating the impact of catastrophic forgetting\non the activation functions themselves.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:16:47 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 14:24:29 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Pomponi", "Jary", ""], ["Scardapane", "Simone", ""], ["Lomonaco", "Vincenzo", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1909.03817", "submitter": "Xinyue Zheng", "authors": "Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao shi, Feiyu Xu", "title": "Efficient Automatic Meta Optimization Search for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on meta-learning either relied on elaborately hand-designed\nnetwork structures or adopted specialized learning rules to a particular\ndomain. We propose a universal framework to optimize the meta-learning process\nautomatically by adopting neural architecture search technique (NAS). NAS\nautomatically generates and evaluates meta-learner's architecture for few-shot\nlearning problems, while the meta-learner uses meta-learning algorithm to\noptimize its parameters based on the distribution of learning tasks. Parameter\nsharing and experience replay are adopted to accelerate the architectures\nsearching process, so it takes only 1-2 GPU days to find good architectures.\nExtensive experiments on Mini-ImageNet and Omniglot show that our algorithm\nexcels in few-shot learning tasks. The best architecture found on Mini-ImageNet\nachieves competitive results when transferred to Omniglot, which shows the high\ntransferability of architectures among different computer vision problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 02:48:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zheng", "Xinyue", ""], ["Wang", "Peng", ""], ["Wang", "Qigang", ""], ["shi", "Zhongchao", ""], ["Xu", "Feiyu", ""]]}, {"id": "1909.03830", "submitter": "Di Wang", "authors": "Di Wang, Feiqing Huang, Jingyu Zhao, Guodong Li, Guangjian Tian", "title": "Compact Autoregressive Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive networks can achieve promising performance in many sequence\nmodeling tasks with short-range dependence. However, when handling\nhigh-dimensional inputs and outputs, the huge amount of parameters in the\nnetwork lead to expensive computational cost and low learning efficiency. The\nproblem can be alleviated slightly by introducing one more narrow hidden layer\nto the network, but the sample size required to achieve a certain training\nerror is still large. To address this challenge, we rearrange the weight\nmatrices of a linear autoregressive network into a tensor form, and then make\nuse of Tucker decomposition to represent low-rank structures. This leads to a\nnovel compact autoregressive network, called Tucker AutoRegressive (TAR) net.\nInterestingly, the TAR net can be applied to sequences with long-range\ndependence since the dimension along the sequential order is reduced.\nTheoretical studies show that the TAR net improves the learning efficiency, and\nrequires much fewer samples for model training. Experiments on synthetic and\nreal-world datasets demonstrate the promising performance of the proposed\ncompact network.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:20:08 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Di", ""], ["Huang", "Feiqing", ""], ["Zhao", "Jingyu", ""], ["Li", "Guodong", ""], ["Tian", "Guangjian", ""]]}, {"id": "1909.04240", "submitter": "Stephan Hoyer", "authors": "Stephan Hoyer, Jascha Sohl-Dickstein, Sam Greydanus", "title": "Neural reparameterization improves structural optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural optimization is a popular method for designing objects such as\nbridge trusses, airplane wings, and optical devices. Unfortunately, the quality\nof solutions depends heavily on how the problem is parameterized. In this\npaper, we propose using the implicit bias over functions induced by neural\nnetworks to improve the parameterization of structural optimization. Rather\nthan directly optimizing densities on a grid, we instead optimize the\nparameters of a neural network which outputs those densities. This\nreparameterization leads to different and often better solutions. On a\nselection of 116 structural optimization tasks, our approach produces the best\ndesign 50% more often than the best baseline method.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 02:07:09 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 00:22:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Hoyer", "Stephan", ""], ["Sohl-Dickstein", "Jascha", ""], ["Greydanus", "Sam", ""]]}, {"id": "1909.04293", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Christoph Bergmeir and Hansika Hewamalage", "title": "LSTM-MSNet: Leveraging Forecasts on Sets of Related Time Series with\n  Multiple Seasonal Patterns", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2020.2985720", "report-no": null, "categories": "stat.AP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating forecasts for time series with multiple seasonal cycles is an\nimportant use-case for many industries nowadays. Accounting for the\nmulti-seasonal patterns becomes necessary to generate more accurate and\nmeaningful forecasts in these contexts. In this paper, we propose Long\nShort-Term Memory Multi-Seasonal Net (LSTM-MSNet), a decomposition based,\nunified prediction framework to forecast time series with multiple seasonal\npatterns. The current state of the art in this space are typically univariate\nmethods, in which the model parameters of each time series are estimated\nindependently. Consequently, these models are unable to include key patterns\nand structures that may be shared by a collection of time series. In contrast,\nLSTM-MSNet is a globally trained Long Short-Term Memory network (LSTM), where a\nsingle prediction model is built across all the available time series to\nexploit the cross series knowledge in a group of related time series.\nFurthermore, our methodology combines a series of state-of-the-art\nmultiseasonal decomposition techniques to supplement the LSTM learning\nprocedure. In our experiments, we are able to show that on datasets from\ndisparate data sources, like e.g. the popular M4 forecasting competition, a\ndecomposition step is beneficial, whereas in the common real-world situation of\nhomogeneous series from a single application, exogenous seasonal variables or\nno seasonal preprocessing at all are better choices. All options are readily\nincluded in the framework and allow us to achieve competitive results for both\ncases, outperforming many state-of-the-art multi-seasonal forecasting methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 05:15:24 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 12:52:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bandara", "Kasun", ""], ["Bergmeir", "Christoph", ""], ["Hewamalage", "Hansika", ""]]}, {"id": "1909.04320", "submitter": "Faizal Hafiz", "authors": "Faizal Hafiz and Akshya Swain and Eduardo M.A.M. Mendes and Luis\n  Aguirre", "title": "Multi-objective Evolutionary Approach to Grey-Box Identification of Buck\n  Converter", "comments": null, "journal-ref": null, "doi": "10.1109/TCSI.2020.2970759", "report-no": null, "categories": "eess.SY cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study proposes a simple grey-box identification approach to model\na real DC-DC buck converter operating in continuous conduction mode. The\nproblem associated with the information void in the observed dynamical data,\nwhich is often obtained over a relatively narrow input range, is alleviated by\nexploiting the known static behavior of buck converter as a priori knowledge. A\nsimple method is developed based on the concept of term clusters to determine\nthe static response of the candidate models. The error in the static behavior\nis then directly embedded into the multi-objective framework for structure\nselection. In essence, the proposed approach casts grey-box identification\nproblem into a multi-objective framework to balance bias-variance dilemma of\nmodel building while explicitly integrating a priori knowledge into the\nstructure selection process. The results of the investigation, considering the\ncase of practical buck converter, demonstrate that it is possible to identify\nparsimonious models which can capture both the dynamic and static behavior of\nthe system over a wide input range.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 06:46:20 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 14:18:36 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Hafiz", "Faizal", ""], ["Swain", "Akshya", ""], ["Mendes", "Eduardo M. A. M.", ""], ["Aguirre", "Luis", ""]]}, {"id": "1909.04430", "submitter": "Tim Taylor", "authors": "Norman Packard, Mark A. Bedau, Alastair Channon, Takashi Ikegami,\n  Steen Rasmussen, Kenneth O. Stanley, Tim Taylor", "title": "An Overview of Open-Ended Evolution: Editorial Introduction to the\n  Open-Ended Evolution II Special Issue", "comments": "This article is published in the Artificial Life journal\n  (https://www.mitpressjournals.org/loi/artl) and is copyright (c) 2019\n  Massachusetts Institute of Technology. It it posted on arXiv.org after the\n  publication embargo period in accordance with MIT Press Journals' author\n  posting guidelines\n  (https://www.mitpressjournals.org/for_authors#authorposting)", "journal-ref": "Artificial Life, 25(2), pp. 93-103, 2019", "doi": "10.1162/artl_a_00291", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nature's spectacular inventiveness, reflected in the enormous diversity of\nform and function displayed by the biosphere, is a feature of life that\ndistinguishes living most strongly from nonliving. It is, therefore, not\nsurprising that this aspect of life should become a central focus of artificial\nlife. We have known since Darwin that the diversity is produced dynamically,\nthrough the process of evolution; this has led life's creative productivity to\nbe called Open-Ended Evolution (OEE) in the field. This article introduces the\nsecond of two special issues on current research in OEE and provides an\noverview of the contents of both special issues. Most of the work was presented\nat a workshop on open-ended evolution that was held as a part of the 2018\nConference on Artificial Life in Tokyo, and much of it had antecedents in two\nprevious workshops on open-ended evolution at artificial life conferences in\nCancun and York. We present a simplified categorization of OEE and summarize\nprogress in the field as represented by the articles in this special issue.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:20:19 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Packard", "Norman", ""], ["Bedau", "Mark A.", ""], ["Channon", "Alastair", ""], ["Ikegami", "Takashi", ""], ["Rasmussen", "Steen", ""], ["Stanley", "Kenneth O.", ""], ["Taylor", "Tim", ""]]}, {"id": "1909.04509", "submitter": "Stephen Tridgell", "authors": "Stephen Tridgell, Martin Kumm, Martin Hardieck, David Boland, Duncan\n  Moss, Peter Zipf, Philip H.W. Leong", "title": "Unrolling Ternary Neural Networks", "comments": "Accepted in TRETS", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of neural networks for large scale or real-time\napplications necessitates hardware acceleration. Most approaches assume that\nthe network architecture and parameters are unknown at design time, permitting\nusage in a large number of applications. This paper demonstrates, for the case\nwhere the neural network architecture and ternary weight values are known a\npriori, that extremely high throughput implementations of neural network\ninference can be made by customising the datapath and routing to remove\nunnecessary computations and data movement. This approach is ideally suited to\nFPGA implementations as a specialized implementation of a trained network\nimproves efficiency while still retaining generality with the reconfigurability\nof an FPGA. A VGG style network with ternary weights and fixed point\nactivations is implemented for the CIFAR10 dataset on Amazon's AWS F1 instance.\nThis paper demonstrates how to remove 90% of the operations in convolutional\nlayers by exploiting sparsity and compile-time optimizations. The\nimplementation in hardware achieves 90.9 +/- 0.1% accuracy and 122 k frames per\nsecond, with a latency of only 29 us, which is the fastest CNN inference\nimplementation reported so far on an FPGA.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 05:03:06 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Tridgell", "Stephen", ""], ["Kumm", "Martin", ""], ["Hardieck", "Martin", ""], ["Boland", "David", ""], ["Moss", "Duncan", ""], ["Zipf", "Peter", ""], ["Leong", "Philip H. W.", ""]]}, {"id": "1909.04548", "submitter": "Minsoo Rhu", "authors": "Yujeong Choi, Minsoo Rhu", "title": "PREMA: A Predictive Multi-task Scheduling Algorithm For Preemptible\n  Neural Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To amortize cost, cloud vendors providing DNN acceleration as a service to\nend-users employ consolidation and virtualization to share the underlying\nresources among multiple DNN service requests. This paper makes a case for a\n\"preemptible\" neural processing unit (NPU) and a \"predictive\" multi-task\nscheduler to meet the latency demands of high-priority inference while\nmaintaining high throughput. We evaluate both the mechanisms that enable NPUs\nto be preemptible and the policies that utilize them to meet scheduling\nobjectives. We show that preemptive NPU multi-tasking can achieve an average\n7.8x, 1.4x, and 4.8x improvement in latency, throughput, and SLA satisfaction,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:34:31 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Choi", "Yujeong", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1909.04559", "submitter": "Frederik Mallmann-Trenn", "authors": "Nancy Lynch and Frederik Mallmann-Trenn", "title": "Learning Hierarchically Structured Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of how concepts that have structure get represented in\nthe brain. Specifically, we introduce a model for hierarchically structured\nconcepts and we show how a biologically plausible neural network can recognize\nthese concepts, and how it can learn them in the first place. Our main goal is\nto introduce a general framework for these tasks and prove formally how both\n(recognition and learning) can be achieved.\n  We show that both tasks can be accomplished even in presence of noise. For\nlearning, we analyze Oja's rule formally, a well-known biologically-plausible\nrule for adjusting the weights of synapses. We complement the learning results\nwith lower bounds asserting that, in order to recognize concepts of a certain\nhierarchical depth, neural networks must have a corresponding number of layers.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:11:38 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 20:07:22 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 11:10:16 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 08:23:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lynch", "Nancy", ""], ["Mallmann-Trenn", "Frederik", ""]]}, {"id": "1909.04605", "submitter": "Jose Rodrigues Jr", "authors": "Jose F Rodrigues-Jr, Gabriel Spadon, Bruno Brandoli, Sihem Amer-Yahia", "title": "Patient trajectory prediction in the Mimic-III dataset, challenges and\n  pitfalls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical prognosis has gained interest as artificial intelligence\nevolves and the potential for computer-aided medicine becomes evident.\nNevertheless, it is challenging to design an effective system that, given a\npatient's medical history, is able to predict probable future conditions.\nPrevious works, mostly carried out over private datasets, have tackled the\nproblem by using artificial neural network architectures that cannot deal with\nlow-cardinality datasets, or by means of non-generalizable inference\napproaches. We introduce a Deep Learning architecture whose design results from\nan intensive experimental process. The final architecture is based on two\nparallel Minimal Gated Recurrent Unit networks working in bi-directional\nmanner, which was extensively tested with the open-access Mimic-III dataset.\nOur results demonstrate significant improvements in automated medical\nprognosis, as measured with Recall@k. We summarize our experience as a set of\nrelevant insights for the design of Deep Learning architectures. Our work\nimproves the performance of computer-aided medicine and can serve as a guide in\ndesigning artificial neural networks used in prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:30:24 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 12:05:15 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 08:45:00 GMT"}, {"version": "v4", "created": "Thu, 28 Nov 2019 13:07:08 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Rodrigues-Jr", "Jose F", ""], ["Spadon", "Gabriel", ""], ["Brandoli", "Bruno", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "1909.04757", "submitter": "Wenrui Zhang", "authors": "Changqing Xu, Wenrui Zhang, Yu Liu, Peng Li", "title": "Boosting Throughput and Efficiency of Hardware Spiking Neural\n  Accelerators using Time Compression Supporting Multiple Spike Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": "Frontiers in Neuroscience, 14, p.104", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are the third generation of neural networks\nand can explore both rate and temporal coding for energy-efficient event-driven\ncomputation. However, the decision accuracy of existing SNN designs is\ncontingent upon processing a large number of spikes over a long period.\nNevertheless, the switching power of SNN hardware accelerators is proportional\nto the number of spikes processed while the length of spike trains limits\nthroughput and static power efficiency. This paper presents the first study on\ndeveloping temporal compression to significantly boost throughput and reduce\nenergy dissipation of digital hardware SNN accelerators while being applicable\nto multiple spike codes. The proposed compression architectures consist of\nlow-cost input spike compression units, novel input-and-output-weighted spiking\nneurons, and reconfigurable time constant scaling to support large and flexible\ntime compression ratios. Our compression architectures can be transparently\napplied to any given pre-designed SNNs employing either rate or temporal codes\nwhile incurring minimal modification of the neural models, learning algorithms,\nand hardware design. Using spiking speech and image recognition datasets, we\ndemonstrate the feasibility of supporting large time compression ratios of up\nto 16x, delivering up to 15.93x, 13.88x, and 86.21x improvements in throughput,\nenergy dissipation, the tradeoffs between hardware area, runtime, energy, and\nclassification accuracy, respectively based on different spike codes on a\nXilinx Zynq-7000 FPGA. These results are achieved while incurring little extra\nhardware overhead.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:16:04 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Xu", "Changqing", ""], ["Zhang", "Wenrui", ""], ["Liu", "Yu", ""], ["Li", "Peng", ""]]}, {"id": "1909.04846", "submitter": "Mehdi Neshat", "authors": "Mehdi Neshat, Bradley Alexander, Angus Simpson", "title": "Covariance Matrix Adaptation Greedy Search Applied to Water Distribution\n  System Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water distribution system design is a challenging optimisation problem with a\nhigh number of search dimensions and constraints. In this way, Evolutionary\nAlgorithms (EAs) have been widely applied to optimise WDS to minimise cost\nsubject whilst meeting pressure constraints. This paper proposes a new hybrid\nevolutionary framework that consists of three distinct phases. The first phase\napplied CMA-ES, a robust adaptive meta-heuristic for continuous optimisation.\nThis is followed by an upward-greedy search phase to remove pressure\nviolations. Finally, a downward greedy search phase is used to reduce oversized\npipes. To assess the effectiveness of the hybrid method, it was applied to five\nwell-known WDSs case studies. The results reveal that the new framework\noutperforms CMA-ES by itself and other previously applied heuristics on most\nbenchmarks in terms of both optimisation speed and network cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 04:36:14 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Neshat", "Mehdi", ""], ["Alexander", "Bradley", ""], ["Simpson", "Angus", ""]]}, {"id": "1909.05073", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma,\n  Bin Ren, Yanzhi Wang", "title": "PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for\n  Real-time Execution on Mobile Devices", "comments": "To appear in Proceedings of the 34th AAAI Conference on Artificial\n  Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression techniques on Deep Neural Network (DNN) have been widely\nacknowledged as an effective way to achieve acceleration on a variety of\nplatforms, and DNN weight pruning is a straightforward and effective method.\nThere are currently two mainstreams of pruning methods representing two\nextremes of pruning regularity: non-structured, fine-grained pruning can\nachieve high sparsity and accuracy, but is not hardware friendly; structured,\ncoarse-grained pruning exploits hardware-efficient structures in pruning, but\nsuffers from accuracy drop when the pruning rate is high. In this paper, we\nintroduce PCONV, comprising a new sparsity dimension, -- fine-grained pruning\npatterns inside the coarse-grained structures. PCONV comprises two types of\nsparsities, Sparse Convolution Patterns (SCP) which is generated from\nintra-convolution kernel pruning and connectivity sparsity generated from\ninter-convolution kernel pruning. Essentially, SCP enhances accuracy due to its\nspecial vision properties, and connectivity sparsity increases pruning rate\nwhile maintaining balanced workload on filter computation. To deploy PCONV, we\ndevelop a novel compiler-assisted DNN inference framework and execute PCONV\nmodels in real-time without accuracy compromise, which cannot be achieved in\nprior work. Our experimental results show that, PCONV outperforms three\nstate-of-art end-to-end DNN frameworks, TensorFlow-Lite, TVM, and Alibaba\nMobile Neural Network with speedup up to 39.2x, 11.4x, and 6.3x, respectively,\nwith no accuracy loss. Mobile devices can achieve real-time inference on\nlarge-scale DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 03:58:29 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 01:33:36 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 00:18:07 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 19:39:06 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ma", "Xiaolong", ""], ["Guo", "Fu-Ming", ""], ["Niu", "Wei", ""], ["Lin", "Xue", ""], ["Tang", "Jian", ""], ["Ma", "Kaisheng", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1909.05176", "submitter": "Ling Feng", "authors": "Ling Feng, Lin Zhang and Choy Heng Lai", "title": "Optimal Machine Intelligence at the Edge of Chaos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE nlin.AO nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been suggested that the biological brain operates at some\ncritical point between two different phases, possibly order and chaos. Despite\nmany indirect empirical evidence from the brain and analytical indication on\nsimple neural networks, the foundation of this hypothesis on generic non-linear\nsystems remains unclear. Here we develop a general theory that reveals the\nexact edge of chaos is the boundary between the chaotic phase and the\n(pseudo)periodic phase arising from Neimark-Sacker bifurcation. This edge is\nanalytically determined by the asymptotic Jacobian norm values of the\nnon-linear operator and influenced by the dimensionality of the system. The\noptimality at the edge of chaos is associated with the highest information\ntransfer between input and output at this point similar to that of the logistic\nmap. As empirical validations, our experiments on the various deep learning\nmodels in computer vision demonstrate the optimality of the models near the\nedge of chaos, and we observe that the state-of-art training algorithms push\nthe models towards such edge as they become more accurate. We further\nestablishes the theoretical understanding of deep learning model generalization\nthrough asymptotic stability.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:23:13 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 10:16:34 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Feng", "Ling", ""], ["Zhang", "Lin", ""], ["Lai", "Choy Heng", ""]]}, {"id": "1909.05193", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin, Zhezhi He and Deliang Fan", "title": "TBT: Targeted Neural Network Attack with Bit Trojan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security of modern Deep Neural Networks (DNNs) is under severe scrutiny as\nthe deployment of these models become widespread in many intelligence-based\napplications. Most recently, DNNs are attacked through Trojan which can\neffectively infect the model during the training phase and get activated only\nthrough specific input patterns (i.e, trigger) during inference. In this work,\nfor the first time, we propose a novel Targeted Bit Trojan(TBT) method, which\ncan insert a targeted neural Trojan into a DNN through the bit-flip attack. Our\nalgorithm efficiently generates a trigger specifically designed to locate\ncertain vulnerable bits of DNN weights stored in main memory (i.e., DRAM). The\nobjective is that once the attacker flips these vulnerable bits, the network\nstill operates with normal inference accuracy with benign input. However, when\nthe attacker activates the trigger by embedding it with any input, the network\nis forced to classify all inputs to a certain target class. We demonstrate that\nflipping only several vulnerable bits identified by our method, using available\nbit-flip techniques (i.e, row-hammer), can transform a fully functional DNN\nmodel into a Trojan-infected model. We perform extensive experiments of\nCIFAR-10, SVHN and ImageNet datasets on both VGG-16 and Resnet-18\narchitectures. Our proposed TBT could classify 92 % of test images to a target\nclass with as little as 84 bit-flips out of 88 million weight bits on Resnet-18\nfor CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 07:51:26 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 03:32:09 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 00:16:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""]]}, {"id": "1909.05233", "submitter": "Ankur Mali", "authors": "Ankur Mali, Alexander Ororbia, C. Lee Giles", "title": "The Neural State Pushdown Automata", "comments": "10 pages, 7 Table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to learn complex grammars, recurrent neural networks (RNNs) require\nsufficient computational resources to ensure correct grammar recognition. A\nwidely-used approach to expand model capacity would be to couple an RNN to an\nexternal memory stack. Here, we introduce a \"neural state\" pushdown automaton\n(NSPDA), which consists of a digital stack, instead of an analog one, that is\ncoupled to a neural network state machine. We empirically show its\neffectiveness in recognizing various context-free grammars (CFGs). First, we\ndevelop the underlying mechanics of the proposed higher order recurrent network\nand its manipulation of a stack as well as how to stably program its underlying\npushdown automaton (PDA) to achieve desired finite-state network dynamics.\nNext, we introduce a noise regularization scheme for higher-order (tensor)\nnetworks, to our knowledge the first of its kind, and design an algorithm for\nimproved incremental learning. Finally, we design a method for inserting\ngrammar rules into a NSPDA and empirically show that this prior knowledge\nimproves its training convergence time by an order of magnitude and, in some\ncases, leads to better generalization. The NSPDA is also compared to a\nclassical analog stack neural network pushdown automaton (NNPDA) as well as a\nwide array of first and second-order RNNs with and without external memory,\ntrained using different learning algorithms. Our results show that, for Dyck(2)\nlanguages, prior rule-based knowledge is critical for optimization convergence\nand for ensuring generalization to longer sequences at test time. We observe\nthat many RNNs with and without memory, but no prior knowledge, fail to\nconverge and generalize poorly on CFGs.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 00:32:11 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 20:12:33 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Mali", "Ankur", ""], ["Ororbia", "Alexander", ""], ["Giles", "C. Lee", ""]]}, {"id": "1909.05331", "submitter": "Roja Eini", "authors": "Roja Eini, Sherif Abdelwahed", "title": "Learning-based Model Predictive Control for Smart Building Thermal\n  Management", "comments": "5 pages, 9 figures, conference paper, accepted in the 16th\n  International Conference on Smart Cities: Improving Quality of Life Using ICT\n  & IoT (HONET-ICT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning-based model predictive control (MPC) approach\nfor the thermal control of a four-zone smart building. The objectives are to\nminimize energy consumption and maintain the residents' comfort. The proposed\ncontrol scheme incorporates learning with the model-based control. The\noccupancy profile in the building zones are estimated in a long-term horizon\nthrough the artificial neural network (ANN), and this data is fed into the\nmodel-based predictor to get the indoor temperature predictions. The Energy\nPlus software is utilized as the actual dataset provider (weather data, indoor\ntemperature, energy consumption). The optimization problem, including the\nactual and predicted data, is solved in each step of the simulation and the\ninput setpoint temperature for the heating/cooling system, is generated.\nComparing the results of the proposed approach with the conventional MPC\nresults proved the significantly better performance of the proposed method in\nenergy savings (40.56% less cooling power consumption and 16.73% less heating\npower consumption), and residents' comfort.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 19:47:02 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Eini", "Roja", ""], ["Abdelwahed", "Sherif", ""]]}, {"id": "1909.05401", "submitter": "Xueyuan She", "authors": "Xueyuan She, Yun Long, Saibal Mukhopadhyay", "title": "Improving Robustness of ReRAM-based Spiking Neural Network Accelerator\n  with Stochastic Spike-timing-dependent-plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-timing-dependent-plasticity (STDP) is an unsupervised learning\nalgorithm for spiking neural network (SNN), which promises to achieve deeper\nunderstanding of human brain and more powerful artificial intelligence. While\nconventional computing system fails to simulate SNN efficiently,\nprocess-in-memory (PIM) based on devices such as ReRAM can be used in designing\nfast and efficient STDP based SNN accelerators, as it operates in high\nresemblance with biological neural network. However, the real-life\nimplementation of such design still suffers from impact of input noise and\ndevice variation. In this work, we present a novel stochastic STDP algorithm\nthat uses spiking frequency information to dynamically adjust synaptic\nbehavior. The algorithm is tested in pattern recognition task with noisy input\nand shows accuracy improvement over deterministic STDP. In addition, we show\nthat the new algorithm can be used for designing a robust ReRAM based SNN\naccelerator that has strong resilience to device variation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 23:26:39 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["She", "Xueyuan", ""], ["Long", "Yun", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1909.05507", "submitter": "Mateusz Ostaszewski", "authors": "Wojciech Masarczyk, Przemys{\\l}aw G{\\l}omb, Bartosz Grabowski, Mateusz\n  Ostaszewski", "title": "Effective training of deep convolutional neural networks for\n  hyperspectral image classification through artificial labeling", "comments": null, "journal-ref": "Remote Sens. 2020, 12, 2653", "doi": "10.3390/rs12162653", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging is a rich source of data, allowing for multitude of\neffective applications. However, such imaging remains challenging because of\nlarge data dimension and, typically, small pool of available training examples.\nWhile deep learning approaches have been shown to be successful in providing\neffective classification solutions, especially for high dimensional problems,\nunfortunately they work best with a lot of labelled examples available. To\nalleviate the second requirement for a particular dataset the transfer learning\napproach can be used: first the network is pre-trained on some dataset with\nlarge amount of training labels available, then the actual dataset is used to\nfine-tune the network. This strategy is not straightforward to apply with\nhyperspectral images, as it is often the case that only one particular image of\nsome type or characteristic is available. In this paper, we propose and\ninvestigate a simple and effective strategy of transfer learning that uses\nunsupervised pre-training step without label information. This approach can be\napplied to many of the hyperspectral classification problems. Performed\nexperiments show that it is very effective at improving the classification\naccuracy without being restricted to a particular image type or neural network\narchitecture. The experiments were carried out on several deep neural network\narchitectures and various sizes of labeled training sets. The greatest\nimprovement in overall accuracy on the Indian Pines and Pavia University\ndatasets is over 21 and 13 percentage points, respectively. An additional\nadvantage of the proposed approach is the unsupervised nature of the\npre-training step, which can be done immediately after image acquisition,\nwithout the need of the potentially costly expert's time.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:47:21 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:48:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Masarczyk", "Wojciech", ""], ["G\u0142omb", "Przemys\u0142aw", ""], ["Grabowski", "Bartosz", ""], ["Ostaszewski", "Mateusz", ""]]}, {"id": "1909.05508", "submitter": "Giuseppe Paolo Mr", "authors": "Giuseppe Paolo and Alban Laflaqui\\`ere and Alexandre Coninx and\n  Stephane Doncieux", "title": "Unsupervised Learning and Exploration of Reachable Outcome Space", "comments": "Published at IEEE International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing Reinforcement Learning in sparse rewards settings, with very\nlittle prior knowledge, is a challenging problem since there is no signal to\nproperly guide the learning process. In such situations, a good search strategy\nis fundamental. At the same time, not having to adapt the algorithm to every\nsingle problem is very desirable. Here we introduce TAXONS, a Task Agnostic\neXploration of Outcome spaces through Novelty and Surprise algorithm. Based on\na population-based divergent-search approach, it learns a set of diverse\npolicies directly from high-dimensional observations, without any task-specific\ninformation. TAXONS builds a repertoire of policies while training an\nautoencoder on the high-dimensional observation of the final state of the\nsystem to build a low-dimensional outcome space. The learned outcome space,\ncombined with the reconstruction error, is used to drive the search for new\npolicies. Results show that TAXONS can find a diverse set of controllers,\ncovering a good part of the ground-truth outcome space, while having no\ninformation about such space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:47:44 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 12:34:35 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 18:03:22 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 09:20:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Paolo", "Giuseppe", ""], ["Laflaqui\u00e8re", "Alban", ""], ["Coninx", "Alexandre", ""], ["Doncieux", "Stephane", ""]]}, {"id": "1909.05587", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Mirko Nentwig, Yohannes Kassahun, Francis Li,\n  Stanislav Bochkarev, Akif Kamal, David Dolson, Secil Altintas, Arif Virani,\n  and Alexander Wong", "title": "Human-Machine Collaborative Design for Accelerated Design of Compact\n  Deep Neural Networks for Autonomous Driving", "comments": "7 pages; BMVC Workshop on Visual AI and Entrepreneurship (VAIE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective deep learning development process is critical for widespread\nindustrial adoption, particularly in the automotive sector. A typical\nindustrial deep learning development cycle involves customizing and\nre-designing an off-the-shelf network architecture to meet the operational\nrequirements of the target application, leading to considerable trial and error\nwork by a machine learning practitioner. This approach greatly impedes\ndevelopment with a long turnaround time and the unsatisfactory quality of the\ncreated models. As a result, a development platform that can aid engineers in\ngreatly accelerating the design and production of compact, optimized deep\nneural networks is highly desirable. In this joint industrial case study, we\nstudy the efficacy of the GenSynth AI-assisted AI design platform for\naccelerating the design of custom, optimized deep neural networks for\nautonomous driving through human-machine collaborative design. We perform a\nquantitative examination by evaluating 10 different compact deep neural\nnetworks produced by GenSynth for the purpose of object detection via a\nNASNet-based user network prototype design, targeted at a low-cost GPU-based\naccelerated embedded system. Furthermore, we quantitatively assess the talent\nhours and GPU processing hours used by the GenSynth process and three other\napproaches based on the typical industrial development process. In addition, we\nquantify the annual cloud cost savings for comprehensive testing using networks\nproduced by GenSynth. Finally, we assess the usability and merits of the\nGenSynth process through user feedback. The findings of this case study showed\nthat GenSynth is easy to use and can be effective at accelerating the design\nand production of compact, customized deep neural network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 12:00:50 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Nentwig", "Mirko", ""], ["Kassahun", "Yohannes", ""], ["Li", "Francis", ""], ["Bochkarev", "Stanislav", ""], ["Kamal", "Akif", ""], ["Dolson", "David", ""], ["Altintas", "Secil", ""], ["Virani", "Arif", ""], ["Wong", "Alexander", ""]]}, {"id": "1909.05631", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Simon Alford, Vijay Gadepally, Michael Jones, Lauren\n  Milechin, Ryan Robinett, Sid Samsi", "title": "Sparse Deep Neural Network Graph Challenge", "comments": "7 pages, 5 figures, 3 tables, 60 references, accepted to IEEE HPEC\n  2019. arXiv admin note: substantial text overlap with arXiv:1807.03165,\n  arXiv:1708.02937, arXiv:1708.06866", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916336", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to\ndeveloping new solutions for analyzing graphs and sparse data. Sparse AI\nanalytics present unique scalability difficulties. The proposed Sparse Deep\nNeural Network (DNN) Challenge draws upon prior challenges from machine\nlearning, high performance computing, and visual analytics to create a\nchallenge that is reflective of emerging sparse AI systems. The Sparse DNN\nChallenge is based on a mathematically well-defined DNN inference computation\nand can be implemented in any programming environment. Sparse DNN inference is\namenable to both vertex-centric implementations and array-based implementations\n(e.g., using the GraphBLAS.org standard). The computations are simple enough\nthat performance predictions can be made based on simple computing hardware\nmodels. The input data sets are derived from the MNIST handwritten letters. The\nsurrounding I/O and verification provide the context for each sparse DNN\ninference that allows rigorous definition of both the input and the output.\nFurthermore, since the proposed sparse DNN challenge is scalable in both\nproblem size and hardware, it can be used to measure and quantitatively compare\na wide range of present day and future systems. Reference implementations have\nbeen implemented and their serial and parallel performance have been measured.\nSpecifications, data, and software are publicly available at GraphChallenge.org\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 02:29:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Alford", "Simon", ""], ["Gadepally", "Vijay", ""], ["Jones", "Michael", ""], ["Milechin", "Lauren", ""], ["Robinett", "Ryan", ""], ["Samsi", "Sid", ""]]}, {"id": "1909.05729", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang and Lin Meng", "title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended\n  Animation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing graph neural networks (GNNs) based on the spectral graph\nconvolutional operator have been criticized for its performance degradation,\nwhich is especially common for the models with deep architectures. In this\npaper, we further identify the suspended animation problem with the existing\nGNNs. Such a problem happens when the model depth reaches the suspended\nanimation limit, and the model will not respond to the training data any more\nand become not learnable. Analysis about the causes of the suspended animation\nproblem with existing GNNs will be provided in this paper, whereas several\nother peripheral factors that will impact the problem will be reported as well.\nTo resolve the problem, we introduce the GResNet (Graph Residual Network)\nframework in this paper, which creates extensively connected highways to\ninvolve nodes' raw features or intermediate representations throughout the\ngraph for all the model layers. Different from the other learning settings, the\nextensive connections in the graph data will render the existing simple\nresidual learning methods fail to work. We prove the effectiveness of the\nintroduced new graph residual terms from the norm preservation perspective,\nwhich will help avoid dramatic changes to the node's representations between\nsequential layers. Detailed studies about the GResNet framework for many\nexisting GNNs, including GCN, GAT and LoopyNet, will be reported in the paper\nwith extensive empirical experiments on real-world benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:46:12 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 17:13:36 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Jiawei", ""], ["Meng", "Lin", ""]]}, {"id": "1909.05962", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Mehdi Moradi", "title": "SegNAS3D: Network Architecture Search with Derivative-Free Global\n  Optimization for 3D Image Segmentation", "comments": "This paper was accepted by the International Conference on Medical\n  Image Computing and Computer-Assisted Intervention - MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has largely reduced the need for manual feature selection in\nimage segmentation. Nevertheless, network architecture optimization and\nhyperparameter tuning are mostly manual and time consuming. Although there are\nincreasing research efforts on network architecture search in computer vision,\nmost works concentrate on image classification but not segmentation, and there\nare very limited efforts on medical image segmentation especially in 3D. To\nremedy this, here we propose a framework, SegNAS3D, for network architecture\nsearch of 3D image segmentation. In this framework, a network architecture\ncomprises interconnected building blocks that consist of operations such as\nconvolution and skip connection. By representing the block structure as a\nlearnable directed acyclic graph, hyperparameters such as the number of feature\nchannels and the option of using deep supervision can be learned together\nthrough derivative-free global optimization. Experiments on 43 3D brain\nmagnetic resonance images with 19 structures achieved an average Dice\ncoefficient of 82%. Each architecture search required less than three days on\nthree GPUs and produced architectures that were much smaller than the\nstate-of-the-art manually created architectures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 21:51:28 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Moradi", "Mehdi", ""]]}, {"id": "1909.06037", "submitter": "Jiangjun Tang", "authors": "Jiangjun Tang and George Leu and Yu-Bin Yang", "title": "Optimisation of Air-Ground Swarm Teaming for Target Search, using\n  Differential Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a swarm teaming perspective that enhances the scope of\nclassic investigations on survivable networks. A target searching generic\ncontext is considered as test-bed, in which a swarm of ground agents and a\nswarm of UAVs cooperate so that the ground agents reach as many targets as\npossible in the field while also remaining connected as much as possible at all\ntimes. To optimise the system against both these objectives in the same time,\nwe use an evolutionary computation approach in the form of a differential\nevolution algorithm. Results are encouraging, showing a good evolution of the\nfitness function used as part of the differential evolution, and a good\nperformance of the evolved dual-swarm system, which exhibits an optimal\ntrade-off between target reaching and connectivity.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:16:55 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Tang", "Jiangjun", ""], ["Leu", "George", ""], ["Yang", "Yu-Bin", ""]]}, {"id": "1909.06143", "submitter": "Yadong Li", "authors": "Yadong Li and Xin Cui", "title": "Shapley Interpretation and Activation in Neural Networks", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Shapley value approach to help address neural networks'\ninterpretability and \"vanishing gradient\" problems. Our method is based on an\naccurate analytical approximation to the Shapley value of a neuron with ReLU\nactivation. This analytical approximation admits a linear propagation of\nrelevance across neural network layers, resulting in a simple, fast and\nsensible interpretation of neural networks' decision making process.\n  We then derived a globally continuous and non-vanishing Shapley gradient,\nwhich can replace the conventional gradient in training neural network layers\nwith ReLU activation, and leading to better training performance. We further\nderived a Shapley Activation (SA) function, which is a close approximation to\nReLU but features the Shapley gradient. The SA is easy to implement in existing\nmachine learning frameworks. Numerical tests show that SA consistently\noutperforms ReLU in training convergence, accuracy and stability.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 11:13:27 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 00:26:36 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Yadong", ""], ["Cui", "Xin", ""]]}, {"id": "1909.06161", "submitter": "Jonas Kubilius", "authors": "Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Ha Hong, Najib J. Majaj,\n  Rishi Rajalingham, Elias B. Issa, Pouya Bashivan, Jonathan Prescott-Roy,\n  Kailyn Schmidt, Aran Nayebi, Daniel Bear, Daniel L. K. Yamins, and James J.\n  DiCarlo", "title": "Brain-Like Object Recognition with High-Performing Shallow Recurrent\n  ANNs", "comments": "NeurIPS 2019 (Oral). Code available at\n  https://github.com/dicarlolab/neurips2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional artificial neural networks (ANNs) are the leading class of\ncandidate models of the mechanisms of visual processing in the primate ventral\nstream. While initially inspired by brain anatomy, over the past years, these\nANNs have evolved from a simple eight-layer architecture in AlexNet to\nextremely deep and branching architectures, demonstrating increasingly better\nobject categorization performance, yet bringing into question how brain-like\nthey still are. In particular, typical deep models from the machine learning\ncommunity are often hard to map onto the brain's anatomy due to their vast\nnumber of layers and missing biologically-important connections, such as\nrecurrence. Here we demonstrate that better anatomical alignment to the brain\nand high performance on machine learning as well as neuroscience measures do\nnot have to be in contradiction. We developed CORnet-S, a shallow ANN with four\nanatomically mapped areas and recurrent connectivity, guided by Brain-Score, a\nnew large-scale composite of neural and behavioral benchmarks for quantifying\nthe functional fidelity of models of the primate ventral visual stream. Despite\nbeing significantly shallower than most models, CORnet-S is the top model on\nBrain-Score and outperforms similarly compact models on ImageNet. Moreover, our\nextensive analyses of CORnet-S circuitry variants reveal that recurrence is the\nmain predictive factor of both Brain-Score and ImageNet top-1 performance.\nFinally, we report that the temporal evolution of the CORnet-S \"IT\" neural\npopulation resembles the actual monkey IT population dynamics. Taken together,\nthese results establish CORnet-S, a compact, recurrent ANN, as the current best\nmodel of the primate ventral visual stream.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:09:34 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 07:30:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kubilius", "Jonas", ""], ["Schrimpf", "Martin", ""], ["Kar", "Kohitij", ""], ["Hong", "Ha", ""], ["Majaj", "Najib J.", ""], ["Rajalingham", "Rishi", ""], ["Issa", "Elias B.", ""], ["Bashivan", "Pouya", ""], ["Prescott-Roy", "Jonathan", ""], ["Schmidt", "Kailyn", ""], ["Nayebi", "Aran", ""], ["Bear", "Daniel", ""], ["Yamins", "Daniel L. K.", ""], ["DiCarlo", "James J.", ""]]}, {"id": "1909.06228", "submitter": "S VenkataKeerthy", "authors": "S. VenkataKeerthy, Rohit Aggarwal, Shalini Jain, Maunendra Sankar\n  Desarkar, Ramakrishna Upadrasta and Y. N. Srikant", "title": "IR2Vec: LLVM IR based Scalable Program Embeddings", "comments": "Accepted in ACM TACO", "journal-ref": null, "doi": "10.1145/3418463", "report-no": null, "categories": "cs.PL cs.LG cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose IR2Vec, a Concise and Scalable encoding infrastructure to\nrepresent programs as a distributed embedding in continuous space. This\ndistributed embedding is obtained by combining representation learning methods\nwith flow information to capture the syntax as well as the semantics of the\ninput programs. As our infrastructure is based on the Intermediate\nRepresentation (IR) of the source code, obtained embeddings are both language\nand machine independent. The entities of the IR are modeled as relationships,\nand their representations are learned to form a seed embedding vocabulary.\nUsing this infrastructure, we propose two incremental encodings:Symbolic and\nFlow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary,\nand Flow-Aware encodings are obtained by augmenting the Symbolic encodings with\nthe flow information.\n  We show the effectiveness of our methodology on two optimization tasks\n(Heterogeneous device mapping and Thread coarsening). Our way of representing\nthe programs enables us to use non-sequential models resulting in orders of\nmagnitude of faster training time. Both the encodings generated by IR2Vec\noutperform the existing methods in both the tasks, even while using simple\nmachine learning models. In particular, our results improve or match the\nstate-of-the-art speedup in 11/14 benchmark-suites in the device mapping task\nacross two platforms and 53/68 benchmarks in the Thread coarsening task across\nfour different platforms. When compared to the other methods, our embeddings\nare more scalable, is non-data-hungry, and has betterOut-Of-Vocabulary (OOV)\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:41:40 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:22:25 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 09:24:01 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["VenkataKeerthy", "S.", ""], ["Aggarwal", "Rohit", ""], ["Jain", "Shalini", ""], ["Desarkar", "Maunendra Sankar", ""], ["Upadrasta", "Ramakrishna", ""], ["Srikant", "Y. N.", ""]]}, {"id": "1909.06254", "submitter": "Saaduddin Mahmud", "authors": "Saaduddin Mahmud, Moumita Choudhury, Md. Mosaddek Khan, Long\n  Tran-Thanh and Nicholas R. Jennings", "title": "AED: An Anytime Evolutionary DCOP Algorithm", "comments": "9 pages, 6 figures, 2 tables. Appeared in the proceedings of the 19th\n  International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS\n  2020)", "journal-ref": null, "doi": "10.5555/3398761.3398859", "report-no": null, "categories": "cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary optimization is a generic population-based metaheuristic that\ncan be adapted to solve a wide variety of optimization problems and has proven\nvery effective for combinatorial optimization problems. However, the potential\nof this metaheuristic has not been utilized in Distributed Constraint\nOptimization Problems (DCOPs), a well-known class of combinatorial optimization\nproblems prevalent in Multi-Agent Systems. In this paper, we present a novel\npopulation-based algorithm, Anytime Evolutionary DCOP (AED), that uses\nevolutionary optimization to solve DCOPs. In AED, the agents cooperatively\nconstruct an initial set of random solutions and gradually improve them through\na new mechanism that considers an optimistic approximation of local benefits.\nMoreover, we present a new anytime update mechanism for AED that identifies the\nbest among a distributed set of candidate solutions and notifies all the agents\nwhen a new best is found. In our theoretical analysis, we prove that AED is\nanytime. Finally, we present empirical results indicating AED outperforms the\nstate-of-the-art DCOP algorithms in terms of solution quality.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:26:31 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 11:29:11 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 14:49:45 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 05:46:25 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mahmud", "Saaduddin", ""], ["Choudhury", "Moumita", ""], ["Khan", "Md. Mosaddek", ""], ["Tran-Thanh", "Long", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "1909.06447", "submitter": "Gwenaelle Cunha Sergio", "authors": "Premkumar Vincent, Gwenaelle Cunha Sergio, Jaewon Jang, In Man Kang,\n  Jaehoon Park, Hyeok Kim, Minho Lee, Jin-Hyuk Bae", "title": "Application of Genetic Algorithm for More Efficient Multi-Layer\n  Thickness Optimization in Solar Cells", "comments": "Published (https://www.mdpi.com/1996-1073/13/7/1726), Code\n  (https://github.com/gcunhase/GeneticAlgorithm-SolarCells)", "journal-ref": "Energies 13 (2020) 1-14", "doi": "10.3390/en13071726", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thin-film solar cells are predominately designed similar to a stacked\nstructure. Optimizing the layer thicknesses in this stack structure is crucial\nto extract the best efficiency of the solar cell. The commonplace method used\nin optimization simulations, such as for optimizing the optical spacer layers'\nthicknesses, is the parameter sweep. Our simulation study shows that the\nimplementation of a meta-heuristic method like the genetic algorithm results in\na significantly faster and accurate search method when compared to the\nbrute-force parameter sweep method in both single and multi-layer optimization.\nWhile other sweep methods can also outperform the brute-force method, they do\nnot consistently exhibit $100\\%$ accuracy in the optimized results like our\ngenetic algorithm. We have used a well-studied P3HT-based structure to test our\nalgorithm. Our best-case scenario was observed to use $60.84\\%$ fewer\nsimulations than the brute-force method.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 11:46:29 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:19:57 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 07:57:52 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Vincent", "Premkumar", ""], ["Sergio", "Gwenaelle Cunha", ""], ["Jang", "Jaewon", ""], ["Kang", "In Man", ""], ["Park", "Jaehoon", ""], ["Kim", "Hyeok", ""], ["Lee", "Minho", ""], ["Bae", "Jin-Hyuk", ""]]}, {"id": "1909.06553", "submitter": "Aydogan Ozcan", "authors": "Yi Luo, Deniz Mengu, Nezih T. Yardimci, Yair Rivenson, Muhammed Veli,\n  Mona Jarrahi, Aydogan Ozcan", "title": "Design of Task-Specific Optical Systems Using Broadband Diffractive\n  Neural Networks", "comments": "36 pages, 5 figures", "journal-ref": "Light: Science & Applications (2019)", "doi": "10.1038/s41377-019-0223-1", "report-no": null, "categories": "cs.NE physics.comp-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a broadband diffractive optical neural network design that\nsimultaneously processes a continuum of wavelengths generated by a\ntemporally-incoherent broadband source to all-optically perform a specific task\nlearned using deep learning. We experimentally validated the success of this\nbroadband diffractive neural network architecture by designing, fabricating and\ntesting seven different multi-layer, diffractive optical systems that transform\nthe optical wavefront generated by a broadband THz pulse to realize (1) a\nseries of tunable, single passband as well as dual passband spectral filters,\nand (2) spatially-controlled wavelength de-multiplexing. Merging the native or\nengineered dispersion of various material systems with a deep learning-based\ndesign strategy, broadband diffractive neural networks help us engineer\nlight-matter interaction in 3D, diverging from intuitive and analytical design\nmethods to create task-specific optical components that can all-optically\nperform deterministic tasks or statistical inference for optical machine\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 08:02:41 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Luo", "Yi", ""], ["Mengu", "Deniz", ""], ["Yardimci", "Nezih T.", ""], ["Rivenson", "Yair", ""], ["Veli", "Muhammed", ""], ["Jarrahi", "Mona", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1909.06711", "submitter": "Joseph Monaco", "authors": "Joseph D. Monaco, Grace M. Hwang, Kevin M. Schultz, Kechen Zhang", "title": "Cognitive swarming in complex environments with attractor dynamics and\n  oscillatory computing", "comments": "16 pages, 7 figures", "journal-ref": "Biol Cybern 114, 269-284 (2020)", "doi": "10.1007/s00422-020-00823-z", "report-no": null, "categories": "cs.MA cs.NE cs.RO nlin.AO q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neurobiological theories of spatial cognition developed with respect to\nrecording data from relatively small and/or simplistic environments compared to\nanimals' natural habitats. It has been unclear how to extend theoretical models\nto large or complex spaces. Complementarily, in autonomous systems technology,\napplications have been growing for distributed control methods that scale to\nlarge numbers of low-footprint mobile platforms. Animals and many-robot groups\nmust solve common problems of navigating complex and uncertain environments.\nHere, we introduce the 'NeuroSwarms' control framework to investigate whether\nadaptive, autonomous swarm control of minimal artificial agents can be achieved\nby direct analogy to neural circuits of rodent spatial cognition. NeuroSwarms\nanalogizes agents to neurons and swarming groups to recurrent networks. We\nimplemented neuron-like agent interactions in which mutually visible agents\noperate as if they were reciprocally-connected place cells in an attractor\nnetwork. We attributed a phase state to agents to enable patterns of\noscillatory synchronization similar to hippocampal models of theta-rhythmic\n(5-12 Hz) sequence generation. We demonstrate that multi-agent swarming and\nreward-approach dynamics can be expressed as a mobile form of Hebbian learning\nand that NeuroSwarms supports a single-entity paradigm that directly informs\ntheoretical models of animal cognition. We present emergent behaviors including\nphase-organized rings and trajectory sequences that interact with environmental\ncues and geometry in large, fragmented mazes. Thus, NeuroSwarms is a model\nartificial spatial system that integrates autonomous control and theoretical\nneuroscience to potentially uncover common principles to advance both domains.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 02:02:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Monaco", "Joseph D.", ""], ["Hwang", "Grace M.", ""], ["Schultz", "Kevin M.", ""], ["Zhang", "Kechen", ""]]}, {"id": "1909.06876", "submitter": "Henrik R. Larsson Dr.", "authors": "H. R. Larsson, A. C. T. van Duin, B. Hartke", "title": "Global optimization of parameters in the reactive force field ReaxFF for\n  SiOH", "comments": null, "journal-ref": "J. Comput. Chem. 2013, 34, 2178-2189", "doi": "10.1002/jcc.23382", "report-no": null, "categories": "physics.comp-ph cs.NE physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have used unbiased global optimization to fit a reactive force field to a\ngiven set of reference data. Specifically, we have employed genetic algorithms\n(GA) to fit ReaxFF to SiOH data, using an in-house GA code that is parallelized\nacross reference data items via the message-passing interface (MPI). Details of\nGA tuning turn out to be far less important for global optimization efficiency\nthan using suitable ranges within which the parameters are varied. To establish\nthese ranges, either prior knowledge can be used or successive stages of GA\noptimizations, each building upon the best parameter vectors and ranges found\nin the previous stage. We finally arrive at optimized force fields with smaller\nerror measures than those published previously. Hence, this optimization\napproach will contribute to converting force-field fitting from a specialist\ntask to an everyday commodity, even for the more difficult case of reactive\nforce fields.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 20:18:48 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Larsson", "H. R.", ""], ["van Duin", "A. C. T.", ""], ["Hartke", "B.", ""]]}, {"id": "1909.06964", "submitter": "Qing Yang", "authors": "Qing Yang, Jiachen Mao, Zuoguan Wang, Hai Li", "title": "DASNet: Dynamic Activation Sparsity for Neural Network Efficiency\n  Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the execution speed and efficiency of neural networks in embedded\nsystems, it is crucial to decrease the model size and computational complexity.\nIn addition to conventional compression techniques, e.g., weight pruning and\nquantization, removing unimportant activations can reduce the amount of data\ncommunication and the computation cost. Unlike weight parameters, the pattern\nof activations is directly related to input data and thereby changes\ndynamically. To regulate the dynamic activation sparsity (DAS), in this work,\nwe propose a generic low-cost approach based on winners-take-all (WTA) dropout\ntechnique. The network enhanced by the proposed WTA dropout, namely\n\\textit{DASNet}, features structured activation sparsity with an improved\nsparsity level. Compared to the static feature map pruning methods, DASNets\nprovide better computation cost reduction. The WTA technique can be easily\napplied in deep neural networks without incurring additional training\nvariables. More importantly, DASNet can be seamlessly integrated with other\ncompression techniques, such as weight pruning and quantization, without\ncompromising on accuracy. Our experiments on various networks and datasets\npresent significant run-time speedups with negligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 03:53:39 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Yang", "Qing", ""], ["Mao", "Jiachen", ""], ["Wang", "Zuoguan", ""], ["Li", "Hai", ""]]}, {"id": "1909.07122", "submitter": "Bin Liang", "authors": "Jingkai Weng, Yujiang Ding, Chengbo Hu, Xue-feng Zhu, Bin Liang, Jing\n  Yang, Jianchun Cheng", "title": "Meta-neural-network for Realtime and Passive Deep-learning-based Object\n  Recognition", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": "10.1038/s41467-020-19693-x", "report-no": null, "categories": "cs.NE cs.LG physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning recently show great success across disciplines yet\nconventionally require time-consuming computer processing or bulky-sized\ndiffractive elements. Here we theoretically propose and experimentally\ndemonstrate a purely-passive \"meta-neural-network\" with compactness and\nhigh-resolution for real-time recognizing complicated objects by analyzing\nacoustic scattering. We prove our meta-neural-network mimics standard neural\nnetwork despite its small footprint, thanks to unique capability of its\nmetamaterial unit cells, dubbed \"meta-neurons\", to produce\ndeep-subwavelength-distribution of discrete phase shift as learnable parameters\nduring training. The resulting device exhibits the \"intelligence\" to perform\ndesired tasks with potential to address the current trade-off between reducing\ndevice's size, cost and energy consumption and increasing recognition speed and\naccuracy, showcased by an example of handwritten digit recognition. Our\nmechanism opens the route to new metamaterial-based deep-learning paradigms and\nenable conceptual devices such as smart transducers automatically analyzing\nsignals, with far-reaching implications for acoustics, optics and related\nfields.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 11:07:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Weng", "Jingkai", ""], ["Ding", "Yujiang", ""], ["Hu", "Chengbo", ""], ["Zhu", "Xue-feng", ""], ["Liang", "Bin", ""], ["Yang", "Jing", ""], ["Cheng", "Jianchun", ""]]}, {"id": "1909.07172", "submitter": "Hang Zou", "authors": "Hang Zou, Chao Zhang, Samson Lasaulce, Lucas Saludjian and Patrick\n  Panciatici", "title": "Decision Set Optimization and Energy-Efficient MIMO Communications", "comments": "7 pages, 5 figures", "journal-ref": "30th IEEE International Symposium on Personal, Indoor and Mobile\n  Radio Communications (PIMRC'19),8-11 September 2019, Istanbul, Turkey", "doi": null, "report-no": null, "categories": "cs.IT cs.NE eess.SP math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assuming that the number of possible decisions for a transmitter (e.g., the\nnumber of possible beamforming vectors) has to be finite and is given, this\npaper investigates for the first time the problem of determining the best\ndecision set when energy-efficiency maximization is pursued. We propose a\nframework to find a good (finite) decision set which induces a minimal\nperformance loss w.r.t. to the continuous case. We exploit this framework for a\nscenario of energy-efficient MIMO communications in which transmit power and\nbeamforming vectors have to be adapted jointly to the channel given under\nfinite-rate feedback. To determine a good decision set we propose an algorithm\nwhich combines the approach of Invasive Weed Optimization (IWO) and an\nEvolutionary Algorithm (EA). We provide a numerical analysis which illustrates\nthe benefits of our point of view. In particular, given a performance loss\nlevel, the feedback rate can by reduced by 2 when the transmit decision set has\nbeen designed properly by using our algorithm. The impact on energy-efficiency\nis also seen to be significant.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:00:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zou", "Hang", ""], ["Zhang", "Chao", ""], ["Lasaulce", "Samson", ""], ["Saludjian", "Lucas", ""], ["Panciatici", "Patrick", ""]]}, {"id": "1909.07425", "submitter": "Abdul Fatir Ansari", "authors": "Abdul Fatir Ansari, Jonathan Scarlett, Harold Soh", "title": "A Characteristic Function Approach to Deep Implicit Generative Modeling", "comments": "CVPR 2020 (Oral), Code available at\n  https://github.com/clear-nus/OCFGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit Generative Models (IGMs) such as GANs have emerged as effective\ndata-driven models for generating samples, particularly images. In this paper,\nwe formulate the problem of learning an IGM as minimizing the expected distance\nbetween characteristic functions. Specifically, we minimize the distance\nbetween characteristic functions of the real and generated data distributions\nunder a suitably-chosen weighting distribution. This distance metric, which we\nterm as the characteristic function distance (CFD), can be (approximately)\ncomputed with linear time-complexity in the number of samples, in contrast with\nthe quadratic-time Maximum Mean Discrepancy (MMD). By replacing the discrepancy\nmeasure in the critic of a GAN with the CFD, we obtain a model that is simple\nto implement and stable to train. The proposed metric enjoys desirable\ntheoretical properties including continuity and differentiability with respect\nto generator parameters, and continuity in the weak topology. We further\npropose a variation of the CFD in which the weighting distribution parameters\nare also optimized during training; this obviates the need for manual tuning,\nand leads to an improvement in test power relative to CFD. We demonstrate\nexperimentally that our proposed method outperforms WGAN and MMD-GAN variants\non a variety of unsupervised image generation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 18:19:49 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 18:19:16 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Ansari", "Abdul Fatir", ""], ["Scarlett", "Jonathan", ""], ["Soh", "Harold", ""]]}, {"id": "1909.07486", "submitter": "Franz Scherr", "authors": "Anand Subramoney and Franz Scherr and Wolfgang Maass", "title": "Reservoirs learn to learn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reservoirs in the form of liquid state machines, i.e.,\nrecurrently connected networks of spiking neurons with randomly chosen weights.\nSo far only the weights of a linear readout were adapted for a specific task.\nWe wondered whether the performance of liquid state machines can be improved if\nthe recurrent weights are chosen with a purpose, rather than randomly. After\nall, weights of recurrent connections in the brain are also not assumed to be\nrandomly chosen. Rather, these weights were probably optimized during\nevolution, development, and prior learning experiences for specific task\ndomains. In order to examine the benefits of choosing recurrent weights within\na liquid with a purpose, we applied the Learning-to-Learn (L2L) paradigm to our\nmodel: We optimized the weights of the recurrent connections -- and hence the\ndynamics of the liquid state machine -- for a large family of potential\nlearning tasks, which the network might have to learn later through\nmodification of the weights of readout neurons. We found that this two-tiered\nprocess substantially improves the learning speed of liquid state machines for\nspecific tasks. In fact, this learning speed increases further if one does not\ntrain the weights of linear readouts at all, and relies instead on the internal\ndynamics and fading memory of the network for remembering salient information\nthat it could extract from preceding examples for the current learning task.\nThis second type of learning has recently been proposed to underlie fast\nlearning in the prefrontal cortex and motor cortex, and hence it is of interest\nto explore its performance also in models. Since liquid state machines share\nmany properties with other types of reservoirs, our results raise the question\nwhether L2L conveys similar benefits also to these other reservoirs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 21:12:24 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 11:03:45 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Subramoney", "Anand", ""], ["Scherr", "Franz", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1909.07490", "submitter": "Rayan Mosli", "authors": "Rayan Mosli, Matthew Wright, Bo Yuan and Yin Pan", "title": "They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with\n  Fewer Queries Using Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been found to be susceptible to adversarial\nexamples that are often indistinguishable from the original inputs. These\nadversarial examples are created by applying adversarial perturbations to input\nsamples, which would cause them to be misclassified by the target models.\nAttacks that search and apply the perturbations to create adversarial examples\nare performed in both white-box and black-box settings, depending on the\ninformation available to the attacker about the target. For black-box attacks,\nthe only capability available to the attacker is the ability to query the\ntarget with specially crafted inputs and observing the labels returned by the\nmodel. Current black-box attacks either have low success rates, requires a high\nnumber of queries, or produce adversarial examples that are easily\ndistinguishable from their sources. In this paper, we present AdversarialPSO, a\nblack-box attack that uses fewer queries to create adversarial examples with\nhigh success rates. AdversarialPSO is based on the evolutionary search\nalgorithm Particle Swarm Optimization, a populationbased gradient-free\noptimization algorithm. It is flexible in balancing the number of queries\nsubmitted to the target vs the quality of imperceptible adversarial examples.\nThe attack has been evaluated using the image classification benchmark datasets\nCIFAR-10, MNIST, and Imagenet, achieving success rates of 99.6%, 96.3%, and\n82.0%, respectively, while submitting substantially fewer queries than the\nstate-of-the-art. We also present a black-box method for isolating salient\nfeatures used by models when making classifications. This method, called Swarms\nwith Individual Search Spaces or SWISS, creates adversarial examples by finding\nand modifying the most important features in the input.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 21:24:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mosli", "Rayan", ""], ["Wright", "Matthew", ""], ["Yuan", "Bo", ""], ["Pan", "Yin", ""]]}, {"id": "1909.07514", "submitter": "Jae-sun Seo", "authors": "Shihui Yin, Xiaoyu Sun, Shimeng Yu, Jae-sun Seo", "title": "High-Throughput In-Memory Computing for Binary Deep Neural Networks with\n  Monolithically Integrated RRAM and 90nm CMOS", "comments": null, "journal-ref": null, "doi": "10.1109/TED.2020.3015178", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning hardware designs have been bottlenecked by conventional\nmemories such as SRAM due to density, leakage and parallel computing\nchallenges. Resistive devices can address the density and volatility issues,\nbut have been limited by peripheral circuit integration. In this work, we\ndemonstrate a scalable RRAM based in-memory computing design, termed XNOR-RRAM,\nwhich is fabricated in a 90nm CMOS technology with monolithic integration of\nRRAM devices between metal 1 and 2. We integrated a 128x64 RRAM array with CMOS\nperipheral circuits including row/column decoders and flash analog-to-digital\nconverters (ADCs), which collectively become a core component for scalable\nRRAM-based in-memory computing towards large deep neural networks (DNNs). To\nmaximize the parallelism of in-memory computing, we assert all 128 wordlines of\nthe RRAM array simultaneously, perform analog computing along the bitlines, and\ndigitize the bitline voltages using ADCs. The resistance distribution of low\nresistance states is tightened by write-verify scheme, and the ADC offset is\ncalibrated. Prototype chip measurements show that the proposed design achieves\nhigh binary DNN accuracy of 98.5% for MNIST and 83.5% for CIFAR-10 datasets,\nrespectively, with energy efficiency of 24 TOPS/W and 158 GOPS throughput. This\nrepresents 5.6X, 3.2X, 14.1X improvements in throughput, energy-delay product\n(EDP), and energy-delay-squared product (ED2P), respectively, compared to the\nstate-of-the-art literature. The proposed XNOR-RRAM can enable intelligent\nfunctionalities for area-/energy-constrained edge computing devices.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:55:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yin", "Shihui", ""], ["Sun", "Xiaoyu", ""], ["Yu", "Shimeng", ""], ["Seo", "Jae-sun", ""]]}, {"id": "1909.07729", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Alex Heinecke, Dhiraj Kalamkar, Sudarshan Srinivasan,\n  Eric C. Qin, Naveen K. Mellempudi, Dipankar Das, Kunal Banerjee, Bharat Kaul,\n  Pradeep Dubey", "title": "K-TanH: Efficient TanH For Deep Learning", "comments": "6 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose K-TanH, a novel, highly accurate, hardware efficient approximation\nof popular activation function TanH for Deep Learning. K-TanH consists of\nparameterized low-precision integer operations, such as, shift and add/subtract\n(no floating point operation needed) where parameters are stored in very small\nlook-up tables that can fit in CPU registers. K-TanH can work on various\nnumerical formats, such as, Float32 and BFloat16. High quality approximations\nto other activation functions, e.g., Sigmoid, Swish and GELU, can be derived\nfrom K-TanH. Our AVX512 implementation of K-TanH demonstrates $>5\\times$ speed\nup over Intel SVML, and it is consistently superior in efficiency over other\napproximations that use floating point arithmetic. Finally, we achieve\nstate-of-the-art Bleu score and convergence results for training language\ntranslation model GNMT on WMT16 data sets with approximate TanH obtained via\nK-TanH on BFloat16 inputs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:43:23 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 05:05:39 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 10:02:50 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kundu", "Abhisek", ""], ["Heinecke", "Alex", ""], ["Kalamkar", "Dhiraj", ""], ["Srinivasan", "Sudarshan", ""], ["Qin", "Eric C.", ""], ["Mellempudi", "Naveen K.", ""], ["Das", "Dipankar", ""], ["Banerjee", "Kunal", ""], ["Kaul", "Bharat", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1909.07908", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, Wilfried Haensch", "title": "Algorithm for Training Neural Networks on Resistive Device Arrays", "comments": "26 pages, 7 fiures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware architectures composed of resistive cross-point device arrays can\nprovide significant power and speed benefits for deep neural network training\nworkloads using stochastic gradient descent (SGD) and backpropagation (BP)\nalgorithm. The training accuracy on this imminent analog hardware however\nstrongly depends on the switching characteristics of the cross-point elements.\nOne of the key requirements is that these resistive devices must change\nconductance in a symmetrical fashion when subjected to positive or negative\npulse stimuli. Here, we present a new training algorithm, so-called the\n\"Tiki-Taka\" algorithm, that eliminates this stringent symmetry requirement. We\nshow that device asymmetry introduces an unintentional implicit cost term into\nthe SGD algorithm, whereas in the \"Tiki-Taka\" algorithm a coupled dynamical\nsystem simultaneously minimizes the original objective function of the neural\nnetwork and the unintentional cost term due to device asymmetry in a\nself-consistent fashion. We tested the validity of this new algorithm on a\nrange of network architectures such as fully connected, convolutional and LSTM\nnetworks. Simulation results on these various networks show that whatever\naccuracy is achieved using the conventional SGD algorithm with symmetric\n(ideal) device switching characteristics the same accuracy is also achieved\nusing the \"Tiki-Taka\" algorithm with non-symmetric (non-ideal) device switching\ncharacteristics. Moreover, all the operations performed on the arrays are still\nparallel and therefore the implementation cost of this new algorithm on array\narchitectures is minimal; and it maintains the aforementioned power and speed\nbenefits. These algorithmic improvements are crucial to relax the material\nspecification and to realize technologically viable resistive crossbar arrays\nthat outperform digital accelerators for similar training tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:52:28 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Haensch", "Wilfried", ""]]}, {"id": "1909.08018", "submitter": "Zihan Pan", "authors": "Zihan Pan, Jibin Wu, Yansong Chua, Malu Zhang, and Haizhou Li", "title": "Neural Population Coding for Effective Temporal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural encoding plays an important role in faithfully describing the\ntemporally rich patterns, whose instances include human speech and\nenvironmental sounds. For tasks that involve classifying such spatio-temporal\npatterns with the Spiking Neural Networks (SNNs), how these patterns are\nencoded directly influence the difficulty of the task. In this paper, we\ncompare several existing temporal and population coding schemes and evaluate\nthem on both speech (TIDIGITS) and sound (RWCP) datasets. We show that, with\npopulation neural codings, the encoded patterns are linearly separable using\nthe Support Vector Machine (SVM). We note that the population neural codings\neffectively project the temporal information onto the spatial domain, thus\nimproving linear separability in the spatial dimension, achieving an accuracy\nof 95\\% and 100\\% for TIDIGITS and RWCP datasets classified using the SVM,\nrespectively. This observation suggests that an effective neural coding scheme\ngreatly simplifies the classification problem such that a simple linear\nclassifier would suffice. The above datasets are then classified using the\nTempotron, an SNN-based classifier. SNN classification results agree with the\nSVM findings that population neural codings help to improve classification\naccuracy. Hence, other than the learning algorithm, effective neural encoding\nis just as important as an SNN designed to recognize spatio-temporal patterns.\nIt is an often neglected but powerful abstraction that deserves further study.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:46:50 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 03:26:20 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Pan", "Zihan", ""], ["Wu", "Jibin", ""], ["Chua", "Yansong", ""], ["Zhang", "Malu", ""], ["Li", "Haizhou", ""]]}, {"id": "1909.08158", "submitter": "Takahiro Homma", "authors": "Takahiro Homma", "title": "Generation mechanism of cell assembly to store information about hand\n  recognition", "comments": null, "journal-ref": "Heliyon Volume 6, Issue 11, November 2020, e05347", "doi": "10.1016/j.heliyon.2020.e05347", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A specific memory is stored in a cell assembly that is activated during fear\nlearning in mice; however, research regarding cell assemblies associated with\nprocedural and habit learning processes is lacking. In modeling studies,\nsimulations of the learning process for hand regard, which is a type of\nprocedural learning, resulted in the formation of cell assemblies. However, the\nmechanisms through which the cell assemblies form and the information stored in\nthese cell assemblies remain unknown. In this paper, the relationship between\nhand movements and weight changes during the simulated learning process for\nhand regard was used to elucidate the mechanism through which inhibitory\nweights are generated, which plays an important role in the formation of cell\nassemblies. During the early training phase, trial and error attempts to bring\nthe hand into the field of view caused the generation of inhibitory weights,\nand the cell assemblies self-organized from these inhibitory weights. The\ninformation stored in the cell assemblies was estimated by examining the\ncontributions of the cell assemblies outputs to hand movements. During\nsustained hand regard, the outputs from these cell assemblies moved the hand\ninto the field of view, using hand-related inputs almost exclusively.\nTherefore, infants are likely able to select the inputs associated with their\nhand (that is, distinguish between their hand and others), based on the\ninformation stored in the cell assembly, and move their hands into the field of\nview during sustained hand regard.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 01:19:55 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 08:07:12 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Homma", "Takahiro", ""]]}, {"id": "1909.08242", "submitter": "EPTCS", "authors": "Mateusz \\'Sla\\.zy\\'nski (AGH University of Science and Technology),\n  Salvador Abreu (University of \\'Evora and LISP), Grzegorz J. Nalepa (AGH\n  University of Science and Technology)", "title": "Generating Local Search Neighborhood with Synthesized Logic Programs", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 168-181", "doi": "10.4204/EPTCS.306.22", "report-no": null, "categories": "cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Search meta-heuristics have been proven a viable approach to solve\ndifficult optimization problems. Their performance depends strongly on the\nsearch space landscape, as defined by a cost function and the selected\nneighborhood operators. In this paper we present a logic programming based\nframework, named Noodle, designed to generate bespoke Local Search\nneighborhoods tailored to specific discrete optimization problems. The proposed\nsystem consists of a domain specific language, which is inspired by logic\nprogramming, as well as a genetic programming solver, based on the grammar\nevolution algorithm. We complement the description with a preliminary\nexperimental evaluation, where we synthesize efficient neighborhood operators\nfor the traveling salesman problem, some of which reproduce well-known results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:05:35 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["\u015ala\u017cy\u0144ski", "Mateusz", "", "AGH University of Science and Technology"], ["Abreu", "Salvador", "", "University of \u00c9vora and LISP"], ["Nalepa", "Grzegorz J.", "", "AGH\n  University of Science and Technology"]]}, {"id": "1909.08261", "submitter": "EPTCS", "authors": "Mateusz \\'Sla\\.zy\\'nski (AGH University of Science and Technology)", "title": "Research Report on Automatic Synthesis of Local Search Neighborhood\n  Operators", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 433-440", "doi": "10.4204/EPTCS.306.59", "report-no": null, "categories": "cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Programming (CP) and Local Search (LS) are different paradigms for\ndealing with combinatorial search and optimization problems. Their\ncomplementary features motivated researchers to create hybrid CP/LS solutions,\nmaintaining both the modeling capabilities of CP and the computational\nadvantages of the heuristic-based LS approach. Research presented in this\nreport is focused on developing a novel method to infer an efficient LS\nneighborhood operator based on the problem structure, as modeled in the CP\nparadigm. We consider a limited formal language that we call a Neighborhood\nDefinition Language, used to specify the neighborhood operators in a\nfine-grained and declarative manner. Together with Logic Programming runtime\ncalled Noodle, it allows to automatically synthesize complex operators using a\nGrammar Evolution algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:15:41 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["\u015ala\u017cy\u0144ski", "Mateusz", "", "AGH University of Science and Technology"]]}, {"id": "1909.08288", "submitter": "Pedro Machado", "authors": "Pedro Machado, Georgina Cosma, T.M McGinnity", "title": "NatCSNN: A Convolutional Spiking Neural Network for recognition of\n  objects extracted from natural images", "comments": "12 pages", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2019:\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\n  Science, vol 11727", "doi": "10.1007/978-3-030-30487-4_28", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological image processing is performed by complex neural networks composed\nof thousands of neurons interconnected via thousands of synapses, some of which\nare excitatory and others inhibitory. Spiking neural models are distinguished\nfrom classical neurons by being biological plausible and exhibiting the same\ndynamics as those observed in biological neurons. This paper proposes a Natural\nConvolutional Neural Network (NatCSNN) which is a 3-layer bio-inspired\nConvolutional Spiking Neural Network (CSNN), for classifying objects extracted\nfrom natural images. A two-stage training algorithm is proposed using\nunsupervised Spike Timing Dependent Plasticity (STDP) learning (phase 1) and\nReSuMe supervised learning (phase 2). The NatCSNN was trained and tested on the\nCIFAR-10 dataset and achieved an average testing accuracy of 84.7% which is an\nimprovement over the 2-layer neural networks previously applied to this\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 08:52:42 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Machado", "Pedro", ""], ["Cosma", "Georgina", ""], ["McGinnity", "T. M", ""]]}, {"id": "1909.08303", "submitter": "Stefano Nolfi", "authors": "Luca Simione and Stefano Nolfi", "title": "Long-Term Progress and Behavior Complexification in Competitive\n  Co-Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility to use competitive evolutionary algorithms to generate\nlong-term progress is normally prevented by the convergence on limit cycle\ndynamics in which the evolving agents keep progressing against their current\ncompetitors by periodically rediscovering solutions adopted previously over and\nover again. This leads to local but not to global progress, i.e. progress\nagainst all possible competitors. We propose a new competitive algorithm that\nproduces long-term global progress by identifying and by filtering out\nopportunistic variations, i.e. variations leading to progress against current\ncompetitors and retrogression against other competitors. The efficacy of the\nmethod is validated on the co-evolution of predator and prey robots, a classic\nproblem that has been used in other related researches. The accumulation of\nglobal progress over many generations leads to effective solutions that involve\nthe production of rather articulated behaviors. The complexity of the behavior\ndisplayed by the evolving robots increases across generations although\nprogresses in performance are not always accompanied by behavior\ncomplexification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:15:41 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 08:50:36 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Simione", "Luca", ""], ["Nolfi", "Stefano", ""]]}, {"id": "1909.08341", "submitter": "Zhi-Hua Zhou", "authors": "Shao-Qun Zhang and Zhao-Yu Zhang and Zhi-Hua Zhou", "title": "Bifurcation Spiking Neural Network", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) has attracted much attention due to its great\npotential of modeling time-dependent signals. The firing rate of spiking\nneurons is decided by control rate which is fixed manually in advance, and\nthus, whether the firing rate is adequate for modeling actual time series\nrelies on fortune. Though it is demanded to have an adaptive control rate, it\nis a non-trivial task because the control rate and the connection weights\nlearned during the training process are usually entangled. In this paper, we\nshow that the firing rate is related to the eigenvalue of the spike generation\nfunction. Inspired by this insight, by enabling the spike generation function\nto have adaptable eigenvalues rather than parametric control rates, we develop\nthe Bifurcation Spiking Neural Network (BSNN), which has an adaptive firing\nrate and is insensitive to the setting of control rates. Experiments validate\nthe effectiveness of BSNN on a broad range of tasks, showing that BSNN achieves\nsuperior performance to existing SNNs and is robust to the setting of control\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:34:59 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 09:06:18 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 14:13:12 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Shao-Qun", ""], ["Zhang", "Zhao-Yu", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1909.08490", "submitter": "Md. Abu Bakr Siddique", "authors": "Fathma Siddique, Shadman Sakib, Md. Abu Bakr Siddique", "title": "Recognition of Handwritten Digit using Convolutional Neural Network in\n  Python with Tensorflow and Comparison of Performance for Various Hidden\n  Layers", "comments": "To be published in 5th International Conference on Advances in\n  Electrical Engineering (ICAEE-2019)", "journal-ref": "2019 5th International Conference on Advances in Electrical\n  Engineering (ICAEE)", "doi": "10.1109/ICAEE48663.2019.8975496", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent times, with the increase of Artificial Neural Network (ANN), deep\nlearning has brought a dramatic twist in the field of machine learning by\nmaking it more artificially intelligent. Deep learning is remarkably used in\nvast ranges of fields because of its diverse range of applications such as\nsurveillance, health, medicine, sports, robotics, drones, etc. In deep\nlearning, Convolutional Neural Network (CNN) is at the center of spectacular\nadvances that mixes Artificial Neural Network (ANN) and up to date deep\nlearning strategies. It has been used broadly in pattern recognition, sentence\nclassification, speech recognition, face recognition, text categorization,\ndocument analysis, scene, and handwritten digit recognition. The goal of this\npaper is to observe the variation of accuracies of CNN to classify handwritten\ndigits using various numbers of hidden layers and epochs and to make the\ncomparison between the accuracies. For this performance evaluation of CNN, we\nperformed our experiment using Modified National Institute of Standards and\nTechnology (MNIST) dataset. Further, the network is trained using stochastic\ngradient descent and the backpropagation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 13:44:38 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Siddique", "Fathma", ""], ["Sakib", "Shadman", ""], ["Siddique", "Md. Abu Bakr", ""]]}, {"id": "1909.08691", "submitter": "Jin-Kao Hao", "authors": "Yangming Zhou, Jin-Kao Hao, Zhang-Hua Fu, Zhe Wang, and Xiangjing Lai", "title": "Variable Population Memetic Search: A Case Study on the Critical Node\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population-based memetic algorithms have been successfully applied to solve\nmany difficult combinatorial problems. Often, a population of fixed size was\nused in such algorithms to record some best solutions sampled during the\nsearch. However, given the particular features of the problem instance under\nconsideration, a population of variable size would be more suitable to ensure\nthe best search performance possible. In this work, we propose variable\npopulation memetic search (VPMS), where a strategic population sizing mechanism\nis used to dynamically adjust the population size during the memetic search\nprocess. Our VPMS approach starts its search from a small population of only\ntwo solutions to focus on exploitation, and then adapts the population size\naccording to the search status to continuously influence the balancing between\nexploitation and exploration. We illustrate an application of the VPMS approach\nto solve the challenging critical node problem (CNP). We show that the VPMS\nalgorithm integrating a variable population, an effective local optimization\nprocedure (called diversified late acceptance search) and a backbone-based\ncrossover operator performs very well compared to state-of-the-art CNP\nalgorithms. The algorithm is able to discover new upper bounds for 13 instances\nout of the 42 popular benchmark instances, while matching 23 previous\nbest-known upper bounds.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:21:02 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zhou", "Yangming", ""], ["Hao", "Jin-Kao", ""], ["Fu", "Zhang-Hua", ""], ["Wang", "Zhe", ""], ["Lai", "Xiangjing", ""]]}, {"id": "1909.08711", "submitter": "Egemen Sert", "authors": "Egemen Sert, Yaneer Bar-Yam, Alfredo J. Morales", "title": "Segregation Dynamics with Reinforcement Learning and Agent Based\n  Modeling", "comments": "14 pages, 4 figures + supplemental material, in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Societies are complex. Properties of social systems can be explained by the\ninterplay and weaving of individual actions. Incentives are key to understand\npeople's choices and decisions. For instance, individual preferences of where\nto live may lead to the emergence of social segregation. In this paper, we\ncombine Reinforcement Learning (RL) with Agent Based Models (ABM) in order to\naddress the self-organizing dynamics of social segregation and explore the\nspace of possibilities that emerge from considering different types of\nincentives. Our model promotes the creation of interdependencies and\ninteractions among multiple agents of two different kinds that want to\nsegregate from each other. For this purpose, agents use Deep Q-Networks to make\ndecisions based on the rules of the Schelling Segregation model and the\nPredator-Prey model. Despite the segregation incentive, our experiments show\nthat spatial integration can be achieved by establishing interdependencies\namong agents of different kinds. They also reveal that segregated areas are\nmore probable to host older people than diverse areas, which attract younger\nones. Through this work, we show that the combination of RL and ABMs can create\nan artificial environment for policy makers to observe potential and existing\nbehaviors associated to incentives.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 21:12:03 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Sert", "Egemen", ""], ["Bar-Yam", "Yaneer", ""], ["Morales", "Alfredo J.", ""]]}, {"id": "1909.08825", "submitter": "Benjamin Deneu", "authors": "Benjamin Deneu (LIRMM, ZENITH), Maximilien Servajean (LIRMM),\n  Christophe Botella (BIOSP, ZENITH), Alexis Joly (ZENITH)", "title": "Evaluation of Deep Species Distribution Models using Environment and\n  Co-occurrences", "comments": null, "journal-ref": "CLEF 2019, Sep 2019, Lugano, Switzerland", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an evaluation of several approaches of plants species\ndistribution modeling based on spatial, environmental and co-occurrences data\nusing machine learning methods. In particular, we re-evaluate the environmental\nconvolutional neural network model that obtained the best performance of the\nGeoLifeCLEF 2018 challenge but on a revised dataset that fixes some of the\nissues of the previous one. We also go deeper in the analysis of co-occurrences\ninformation by evaluating a new model that jointly takes environmental\nvariables and co-occurrences as inputs of an end-to-end network. Results show\nthat the environmental models are the best performing methods and that there is\na significant amount of complementary information between co-occurrences and\nenvironment. Indeed, the model learned on both inputs allows a significant\nperformance gain compared to the environmental model alone.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 06:51:56 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Deneu", "Benjamin", "", "LIRMM, ZENITH"], ["Servajean", "Maximilien", "", "LIRMM"], ["Botella", "Christophe", "", "BIOSP, ZENITH"], ["Joly", "Alexis", "", "ZENITH"]]}, {"id": "1909.08891", "submitter": "\\'Alvaro Parafita Mart\\'inez", "authors": "\\'Alvaro Parafita, Jordi Vitri\\`a", "title": "Explaining Visual Models by Causal Attribution", "comments": "2019 ICCV Workshop on Interpreting and Explaining Visual Artificial\n  Intelligence Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model explanations based on pure observational data cannot compute the\neffects of features reliably, due to their inability to estimate how each\nfactor alteration could affect the rest. We argue that explanations should be\nbased on the causal model of the data and the derived intervened causal models,\nthat represent the data distribution subject to interventions. With these\nmodels, we can compute counterfactuals, new samples that will inform us how the\nmodel reacts to feature changes on our input. We propose a novel explanation\nmethodology based on Causal Counterfactuals and identify the limitations of\ncurrent Image Generative Models in their application to counterfactual\ncreation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 09:52:31 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Parafita", "\u00c1lvaro", ""], ["Vitri\u00e0", "Jordi", ""]]}, {"id": "1909.09072", "submitter": "Xudong Liu", "authors": "Ahmed Moussa, Xudong Liu", "title": "Learning Optimal and Near-Optimal Lexicographic Preference Lists", "comments": "Published in the Proceedings of the 32nd International Florida\n  Artificial Intelligence Research Society Conference, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning problems of an intuitive and concise preference model,\ncalled lexicographic preference lists (LP-lists). Given a set of examples that\nare pairwise ordinal preferences over a universe of objects built of attributes\nof discrete values, we want to learn (1) an optimal LP-list that decides the\nmaximum number of these examples, or (2) a near-optimal LP-list that decides as\nmany examples as it can. To this end, we introduce a dynamic programming based\nalgorithm and a genetic algorithm for these two learning problems,\nrespectively. Furthermore, we empirically demonstrate that the sub-optimal\nmodels computed by the genetic algorithm very well approximate the de facto\noptimal models computed by our dynamic programming based algorithm, and that\nthe genetic algorithm outperforms the baseline greedy heuristic with higher\naccuracy predicting new preferences.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:10:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Moussa", "Ahmed", ""], ["Liu", "Xudong", ""]]}, {"id": "1909.09227", "submitter": "Marcos Eduardo Valle", "authors": "Marcos Eduardo Valle and Rodolfo Anibal Lobo", "title": "An Introduction to Quaternion-Valued Recurrent Projection Neural\n  Networks", "comments": "Accepted to be Published in: Proceedings of the 8th Brazilian\n  Conference on Intelligent Systems (BRACIS 2019), October 15-18, 2019,\n  Salvador, BA, Brazil", "journal-ref": "Proceedings of 8th Brazilian Conference on Intelligent Systems\n  (BRACIS 2019)", "doi": "10.1109/BRACIS.2019.00151", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypercomplex-valued neural networks, including quaternion-valued neural\nnetworks, can treat multi-dimensional data as a single entity. In this paper,\nwe introduce the quaternion-valued recurrent projection neural networks\n(QRPNNs). Briefly, QRPNNs are obtained by combining the non-local projection\nlearning with the quaternion-valued recurrent correlation neural network\n(QRCNNs). We show that QRPNNs overcome the cross-talk problem of QRCNNs. Thus,\nthey are appropriate to implement associative memories. Furthermore,\ncomputational experiments reveal that QRPNNs exhibit greater storage capacity\nand noise tolerance than their corresponding QRCNNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 20:36:33 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Valle", "Marcos Eduardo", ""], ["Lobo", "Rodolfo Anibal", ""]]}, {"id": "1909.09366", "submitter": "Alianna Maren", "authors": "Alianna J. Maren", "title": "2-D Cluster Variation Method Free Energy: Fundamentals and Pragmatics", "comments": "88 pages, 28 figures, 7 tables, 31 references, 4 appendices", "journal-ref": null, "doi": null, "report-no": "Themasis Technical Report TR-2019-02v1 (ajm)", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being invented in 1951 by R. Kikuchi, the 2-D Cluster Variation\nMethod (CVM), has not yet received attention. Nevertheless, this method can\nusefully characterize 2-D topographies using just two parameters; the\nactivation enthalpy and the interaction enthalpy. This Technical Report\npresents 2-D CVM details, including the dependence of the various configuration\nvariables on the enthalpy parameters, as well as illustrations of various\ntopographies (ranging from scale-free-like to rich club-like) that result from\ndifferent parameter selection. The complete derivation for the analytic\nsolution, originally presented simply as a result in Kikuchi and Brush (1967)\nis given here, along with careful comparison of the analytically-predicted\nconfiguration variables versus those obtained when performing computational\nfree energy minimization on a 2-D grid. The 2-D CVM can potentially function as\na secondary free energy minimization within the hidden layer of a neural\nnetwork, providing a basis for extending node activations over time and\nallowing temporal correlation of patterns.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 08:23:42 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Maren", "Alianna J.", ""]]}, {"id": "1909.09432", "submitter": "SeyedAbolghasem Mirroshandel", "authors": "Erfan Miahi, Seyed Abolghasem Mirroshandel, Alexis Nasr", "title": "Genetic Neural Architecture Search for automatic assessment of human\n  sperm images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Male infertility is a disease which affects approximately 7% of men. Sperm\nmorphology analysis (SMA) is one of the main diagnosis methods for this\nproblem. Manual SMA is an inexact, subjective, non-reproducible, and hard to\nteach process. As a result, in this paper, we introduce a novel automatic SMA\nbased on a neural architecture search algorithm termed Genetic Neural\nArchitecture Search (GeNAS). For this purpose, we used a collection of images\ncalled MHSMA dataset contains 1,540 sperm images which have been collected from\n235 patients with infertility problems. GeNAS is a genetic algorithm that acts\nas a meta-controller which explores the constrained search space of plain\nconvolutional neural network architectures. Every individual of the genetic\nalgorithm is a convolutional neural network trained to predict morphological\ndeformities in different segments of human sperm (head, vacuole, and acrosome),\nand its fitness is calculated by a novel proposed method named GeNAS-WF\nespecially designed for noisy, low resolution, and imbalanced datasets. Also, a\nhashing method is used to save each trained neural architecture fitness, so we\ncould reuse them during fitness evaluation and speed up the algorithm. Besides,\nin terms of running time and computation power, our proposed architecture\nsearch method is far more efficient than most of the other existing neural\narchitecture search algorithms. Additionally, other proposed methods have been\nevaluated on balanced datasets, whereas GeNAS is built specifically for noisy,\nlow quality, and imbalanced datasets which are common in the field of medical\nimaging. In our experiments, the best neural architecture found by GeNAS has\nreached an accuracy of 91.66%, 77.33%, and 77.66% in the vacuole, head, and\nacrosome abnormality detection, respectively. In comparison to other proposed\nalgorithms for MHSMA dataset, GeNAS achieved state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:25:05 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 11:12:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Miahi", "Erfan", ""], ["Mirroshandel", "Seyed Abolghasem", ""], ["Nasr", "Alexis", ""]]}, {"id": "1909.09444", "submitter": "Hojjat Rakhshani", "authors": "Hojjat Rakhshani, Lhassane Idoumghar, Julien Lepagnot, and Mathieu\n  Brevilliers", "title": "From feature selection to continuous optimization", "comments": "Accepted for EA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metaheuristic algorithms (MAs) have seen unprecedented growth thanks to their\nsuccessful applications in fields including engineering and health sciences. In\nthis work, we investigate the use of a deep learning (DL) model as an\nalternative tool to do so. The proposed method, called MaNet, is motivated by\nthe fact that most of the DL models often need to solve massive nasty\noptimization problems consisting of millions of parameters. Feature selection\nis the main adopted concepts in MaNet that helps the algorithm to skip\nirrelevant or partially relevant evolutionary information and uses those which\ncontribute most to the overall performance. The introduced model is applied on\nseveral unimodal and multimodal continuous problems. The experiments indicate\nthat MaNet is able to yield competitive results compared to one of the best\nhand-designed algorithms for the aforementioned problems, in terms of the\nsolution accuracy and scalability.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:13:27 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 09:49:30 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Rakhshani", "Hojjat", ""], ["Idoumghar", "Lhassane", ""], ["Lepagnot", "Julien", ""], ["Brevilliers", "Mathieu", ""]]}, {"id": "1909.09502", "submitter": "Alexander Ororbia", "authors": "Travis J. Desell, AbdElRahman A. ElSaid and Alexander G. Ororbia", "title": "An Empirical Exploration of Deep Recurrent Connections and Memory Cells\n  Using Neuro-Evolution", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuro-evolution and neural architecture search algorithms have gained\nincreasing interest due to the challenges involved in designing optimal\nartificial neural networks (ANNs). While these algorithms have been shown to\npossess the potential to outperform the best human crafted architectures, a\nless common use of them is as a tool for analysis of ANN structural components\nand connectivity structures. In this work, we focus on this particular use-case\nto develop a rigorous examination and comparison framework for analyzing\nrecurrent neural networks (RNNs) applied to time series prediction using the\nnovel neuro-evolutionary process known as Evolutionary eXploration of\nAugmenting Memory Models (EXAMM). Specifically, we use our EXAMM-based analysis\nto investigate the capabilities of recurrent memory cells and the\ngeneralization ability afforded by various complex recurrent connectivity\npatterns that span one or more steps in time, i.e., deep recurrent connections.\nEXAMM, in this study, was used to train over 10.56 million RNNs in 5,280\nrepeated experiments with varying components. While many modern, often\nhand-crafted RNNs rely on complex memory cells (which have internal recurrent\nconnections that only span a single time step) operating under the assumption\nthat these sufficiently latch information and handle long term dependencies,\nour results show that networks evolved with deep recurrent connections perform\nsignificantly better than those without. More importantly, in some cases, the\nbest performing RNNs consisted of only simple neurons and deep time skip\nconnections, without any memory cells. These results strongly suggest that\nutilizing deep time skip connections in RNNs for time series data prediction\nnot only deserves further, dedicated study, but also demonstrate the potential\nof neuro-evolution as a means to better study, understand, and train effective\nRNNs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 13:45:23 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 00:54:37 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 19:11:08 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Desell", "Travis J.", ""], ["ElSaid", "AbdElRahman A.", ""], ["Ororbia", "Alexander G.", ""]]}, {"id": "1909.09586", "submitter": "Ralf C. Staudemeyer", "authors": "Ralf C. Staudemeyer and Eric Rothstein Morris", "title": "Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent\n  Neural Networks", "comments": "42 pages, 11 figures, tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are one of the\nmost powerful dynamic classifiers publicly known. The network itself and the\nrelated learning algorithms are reasonably well documented to get an idea how\nit works. This paper will shed more light into understanding how LSTM-RNNs\nevolved and why they work impressively well, focusing on the early,\nground-breaking publications. We significantly improved documentation and fixed\na number of errors and inconsistencies that accumulated in previous\npublications. To support understanding we as well revised and unified the\nnotation used.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:44:51 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Staudemeyer", "Ralf C.", ""], ["Morris", "Eric Rothstein", ""]]}, {"id": "1909.09588", "submitter": "Rene Schaub", "authors": "Rene Schaub", "title": "What are Neural Networks made of?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Deep Learning methods is not well understood, though various\nattempts at explaining it have been made, typically centered on properties of\nstochastic gradient descent. Even less clear is why certain neural network\narchitectures perform better than others. We provide a potential opening with\nthe hypothesis that neural network training is a form of Genetic Programming.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 21:59:26 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Schaub", "Rene", ""]]}, {"id": "1909.09595", "submitter": "Cheonbok  Park", "authors": "Cheonbok Park, Inyoup Na, Yongjang Jo, Sungbok Shin, Jaehyo Yoo, Bum\n  Chul Kwon, Jian Zhao, Hyungjong Noh, Yeonsoo Lee, Jaegul Choo", "title": "SANVis: Visual Analytics for Understanding Self-Attention Networks", "comments": "VAST Short - IEEE VIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention networks, a deep neural network architecture inspired by humans'\nattention mechanism, have seen significant success in image captioning, machine\ntranslation, and many other applications. Recently, they have been further\nevolved into an advanced approach called multi-head self-attention networks,\nwhich can encode a set of input vectors, e.g., word vectors in a sentence, into\nanother set of vectors. Such encoding aims at simultaneously capturing diverse\nsyntactic and semantic features within a set, each of which corresponds to a\nparticular attention head, forming altogether multi-head attention. Meanwhile,\nthe increased model complexity prevents users from easily understanding and\nmanipulating the inner workings of models. To tackle the challenges, we present\na visual analytics system called SANVis, which helps users understand the\nbehaviors and the characteristics of multi-head self-attention networks. Using\na state-of-the-art self-attention model called Transformer, we demonstrate\nusage scenarios of SANVis in machine translation tasks. Our system is available\nat http://short.sanvis.org\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:59:40 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Park", "Cheonbok", ""], ["Na", "Inyoup", ""], ["Jo", "Yongjang", ""], ["Shin", "Sungbok", ""], ["Yoo", "Jaehyo", ""], ["Kwon", "Bum Chul", ""], ["Zhao", "Jian", ""], ["Noh", "Hyungjong", ""], ["Lee", "Yeonsoo", ""], ["Choo", "Jaegul", ""]]}, {"id": "1909.09788", "submitter": "Albert Gatt", "authors": "Somaye Jafaritazehjani and Albert Gatt and Marc Tanti", "title": "Visuallly Grounded Generation of Entailments from Premises", "comments": "Proceedings of the 12th International Conference on Natural Language\n  Generation (INLG 2019), 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Inference (NLI) is the task of determining the semantic\nrelationship between a premise and a hypothesis. In this paper, we focus on the\n{\\em generation} of hypotheses from premises in a multimodal setting, to\ngenerate a sentence (hypothesis) given an image and/or its description\n(premise) as the input. The main goals of this paper are (a) to investigate\nwhether it is reasonable to frame NLI as a generation task; and (b) to consider\nthe degree to which grounding textual premises in visual information is\nbeneficial to generation. We compare different neural architectures, showing\nthrough automatic and human evaluation that entailments can indeed be generated\nsuccessfully. We also show that multimodal models outperform unimodal models in\nthis task, albeit marginally.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 07:56:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Jafaritazehjani", "Somaye", ""], ["Gatt", "Albert", ""], ["Tanti", "Marc", ""]]}, {"id": "1909.10340", "submitter": "Gideon Kowadlo", "authors": "Gideon Kowadlo, Abdelrahman Ahmed, and David Rawlinson", "title": "AHA! an 'Artificial Hippocampal Algorithm' for Episodic Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of ML research concerns slow, statistical learning of i.i.d.\nsamples from large, labelled datasets. Animals do not learn this way. An\nenviable characteristic of animal learning is `episodic' learning - the ability\nto memorise a specific experience as a composition of existing concepts, after\njust one experience, without provided labels. The new knowledge can then be\nused to distinguish between similar experiences, to generalise between classes,\nand to selectively consolidate to long-term memory. The Hippocampus is known to\nbe vital to these abilities. AHA is a biologically-plausible computational\nmodel of the Hippocampus. Unlike most machine learning models, AHA is trained\nwithout external labels and uses only local credit assignment. We demonstrate\nAHA in a superset of the Omniglot one-shot classification benchmark. The\nextended benchmark covers a wider range of known hippocampal functions by\ntesting pattern separation, completion, and recall of original input. These\nfunctions are all performed within a single configuration of the computational\nmodel. Despite these constraints, image classification results are comparable\nto conventional deep convolutional ANNs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 12:49:47 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 15:47:27 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 06:00:38 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2019 01:56:18 GMT"}, {"version": "v5", "created": "Wed, 25 Mar 2020 04:08:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Kowadlo", "Gideon", ""], ["Ahmed", "Abdelrahman", ""], ["Rawlinson", "David", ""]]}, {"id": "1909.10387", "submitter": "Jacopo Panerati", "authors": "Hehui Zheng (1), Jacopo Panerati (2), Giovanni Beltrame (2), Amanda\n  Prorok (1) ((1) University of Cambridge, (2) Polytechnique Montreal)", "title": "An Adversarial Approach to Private Flocking in Mobile Robot Teams", "comments": "10 pages, 13 figures", "journal-ref": "IEEE Robotics and Automation Letters (2020)", "doi": "10.1109/LRA.2020.2967331", "report-no": null, "categories": "cs.RO cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy is an important facet of defence against adversaries. In this letter,\nwe introduce the problem of private flocking. We consider a team of mobile\nrobots flocking in the presence of an adversary, who is able to observe all\nrobots' trajectories, and who is interested in identifying the leader. We\npresent a method that generates private flocking controllers that hide the\nidentity of the leader robot. Our approach towards privacy leverages a\ndata-driven adversarial co-optimization scheme. We design a mechanism that\noptimizes flocking control parameters, such that leader inference is hindered.\nAs the flocking performance improves, we successively train an adversarial\ndiscriminator that tries to infer the identity of the leader robot. To evaluate\nthe performance of our co-optimization scheme, we investigate different classes\nof reference trajectories. Although it is reasonable to assume that there is an\ninherent trade-off between flocking performance and privacy, our results\ndemonstrate that we are able to achieve high flocking performance and\nsimultaneously reduce the risk of revealing the leader.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:28:56 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 12:58:07 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 17:23:47 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Zheng", "Hehui", "", "University of Cambridge"], ["Panerati", "Jacopo", "", "Polytechnique Montreal"], ["Beltrame", "Giovanni", "", "Polytechnique Montreal"], ["Prorok", "Amanda", "", "University of Cambridge"]]}, {"id": "1909.10447", "submitter": "Pranava Madhyastha", "authors": "Pranava Madhyastha, Rishabh Jain", "title": "On Model Stability as a Function of Random Seed", "comments": "v1; Accepted for publication at CoNLL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on quantifying model stability as a function of\nrandom seed by investigating the effects of the induced randomness on model\nperformance and the robustness of the model in general. We specifically perform\na controlled study on the effect of random seeds on the behaviour of attention,\ngradient-based and surrogate model based (LIME) interpretations. Our analysis\nsuggests that random seeds can adversely affect the consistency of models\nresulting in counterfactual interpretations. We propose a technique called\nAggressive Stochastic Weight Averaging (ASWA)and an extension called\nNorm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the\nstability of models over random seeds. With our ASWA and NASWA based\noptimization, we are able to improve the robustness of the original model, on\naverage reducing the standard deviation of the model's performance by 72%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:00:06 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Madhyastha", "Pranava", ""], ["Jain", "Rishabh", ""]]}, {"id": "1909.10616", "submitter": "Huaqing Zhang", "authors": "Huaqing Zhang, Xiaolin Cheng, Hui Zang and Dae Hoon Park", "title": "Compiler-Level Matrix Multiplication Optimization for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important linear algebra routine, GEneral Matrix Multiplication (GEMM), is\na fundamental operator in deep learning. Compilers need to translate these\nroutines into low-level code optimized for specific hardware. Compiler-level\noptimization of GEMM has significant performance impact on training and\nexecuting deep learning models. However, most deep learning frameworks rely on\nhardware-specific operator libraries in which GEMM optimization has been mostly\nachieved by manual tuning, which restricts the performance on different target\nhardware. In this paper, we propose two novel algorithms for GEMM optimization\nbased on the TVM framework, a lightweight Greedy Best First Search (G-BFS)\nmethod based on heuristic search, and a Neighborhood Actor Advantage Critic\n(N-A2C) method based on reinforcement learning. Experimental results show\nsignificant performance improvement of the proposed methods, in both the\noptimality of the solution and the cost of search in terms of time and fraction\nof the search space explored. Specifically, the proposed methods achieve 24%\nand 40% savings in GEMM computation time over state-of-the-art XGBoost and RNN\nmethods, respectively, while exploring only 0.1% of the search space. The\nproposed approaches have potential to be applied to other operator-level\noptimizations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 21:03:19 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Huaqing", ""], ["Cheng", "Xiaolin", ""], ["Zang", "Hui", ""], ["Park", "Dae Hoon", ""]]}, {"id": "1909.10815", "submitter": "Renqian Luo", "authors": "Renqian Luo, Tao Qin, Enhong Chen", "title": "Balanced One-shot Neural Architecture Optimization", "comments": "Code and model checkpoints are publicly available at\n  https://github.com/renqianluo/NAO_pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to rank candidate architectures is the key to the performance of\nneural architecture search~(NAS). One-shot NAS is proposed to reduce the\nexpense but shows inferior performance against conventional NAS and is not\nadequately stable. We investigate into this and find that the ranking\ncorrelation between architectures under one-shot training and the ones under\nstand-alone full training is poor, which misleads the algorithm to discover\nbetter architectures. Further, we show that the training of architectures of\ndifferent sizes under the current one-shot method is imbalanced, which causes\nthe evaluated performances of the architectures to be less predictable of their\nground-truth performances and affects the ranking correlation heavily.\nConsequently, we propose Balanced NAO where we introduce balanced training of\nthe supernet during the search procedure to encourage more updates for large\narchitectures than small architectures by sampling architectures in proportion\nto their model sizes. Comprehensive experiments verify that our proposed method\nis effective and robust which leads to a more stable search. The final\ndiscovered architecture shows significant improvements against baselines with a\ntest error rate of 2.60\\% on CIFAR-10 and top-1 accuracy of 74.4% on ImageNet\nunder the mobile setting. Code and model checkpoints will be publicly\navailable. The code is available at github.com/renqianluo/NAO_pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:18:52 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 04:20:18 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Luo", "Renqian", ""], ["Qin", "Tao", ""], ["Chen", "Enhong", ""]]}, {"id": "1909.10837", "submitter": "Ying Chen", "authors": "Shibo Zhou, Xiaohua LI, Ying Chen, Sanjeev T. Chandrasekaran, Arindam\n  Sanyal", "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust\n  Performance", "comments": null, "journal-ref": "Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21),\n  2021: 35(12),11143-11151", "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural network (SNN) is interesting both theoretically and\npractically because of its strong bio-inspiration nature and potentially\noutstanding energy efficiency. Unfortunately, its development has fallen far\nbehind the conventional deep neural network (DNN), mainly because of difficult\ntraining and lack of widely accepted hardware experiment platforms. In this\npaper, we show that a deep temporal-coded SNN can be trained easily and\ndirectly over the benchmark datasets CIFAR10 and ImageNet, with testing\naccuracy within 1% of the DNN of equivalent size and architecture. Training\nbecomes similar to DNN thanks to the closed-form solution to the spiking\nwaveform dynamics. Considering that SNNs should be implemented in practical\nneuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2\nbits and with weights perturbed by random noise to demonstrate its robustness\nin practical applications. In addition, we develop a phase-domain signal\nprocessing circuit schematic to implement our spiking neuron with 90% gain of\nenergy efficiency over existing work. This paper demonstrates that the\ntemporal-coded deep SNN is feasible for applications with high performance and\nhigh energy efficient.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:28:11 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 06:28:18 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 05:31:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Shibo", ""], ["LI", "Xiaohua", ""], ["Chen", "Ying", ""], ["Chandrasekaran", "Sanjeev T.", ""], ["Sanyal", "Arindam", ""]]}, {"id": "1909.11015", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey, Soumendu Chakraborty, Swalpa Kumar Roy, Snehasis\n  Mukherjee, Satish Kumar Singh, Bidyut Baran Chaudhuri", "title": "diffGrad: An Optimization Method for Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Decent (SGD) is one of the core techniques behind the\nsuccess of deep neural networks. The gradient provides information on the\ndirection in which a function has the steepest rate of change. The main problem\nwith basic SGD is to change by equal sized steps for all parameters,\nirrespective of gradient behavior. Hence, an efficient way of deep network\noptimization is to make adaptive step sizes for each parameter. Recently,\nseveral attempts have been made to improve gradient descent methods such as\nAdaGrad, AdaDelta, RMSProp and Adam. These methods rely on the square roots of\nexponential moving averages of squared past gradients. Thus, these methods do\nnot take advantage of local change in gradients. In this paper, a novel\noptimizer is proposed based on the difference between the present and the\nimmediate past gradient (i.e., diffGrad). In the proposed diffGrad optimization\ntechnique, the step size is adjusted for each parameter in such a way that it\nshould have a larger step size for faster gradient changing parameters and a\nlower step size for lower gradient changing parameters. The convergence\nanalysis is done using the regret bound approach of online learning framework.\nRigorous analysis is made in this paper over three synthetic complex non-convex\nfunctions. The image categorization experiments are also conducted over the\nCIFAR10 and CIFAR100 datasets to observe the performance of diffGrad with\nrespect to the state-of-the-art optimizers such as SGDM, AdaGrad, AdaDelta,\nRMSProp, AMSGrad, and Adam. The residual unit (ResNet) based Convolutional\nNeural Networks (CNN) architecture is used in the experiments. The experiments\nshow that diffGrad outperforms other optimizers. Also, we show that diffGrad\nperforms uniformly well for training CNN using different activation functions.\nThe source code is made publicly available at\nhttps://github.com/shivram1987/diffGrad.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:20:05 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 06:11:50 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 06:51:39 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Dubey", "Shiv Ram", ""], ["Chakraborty", "Soumendu", ""], ["Roy", "Swalpa Kumar", ""], ["Mukherjee", "Snehasis", ""], ["Singh", "Satish Kumar", ""], ["Chaudhuri", "Bidyut Baran", ""]]}, {"id": "1909.11022", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio, Alessio Micheli", "title": "Reservoir Topology in Deep Echo State Networks", "comments": "Preprint of the paper published in the proceedings of ICANN 2019", "journal-ref": null, "doi": "10.1007/978-3-030-30493-5_6", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Echo State Networks (DeepESNs) recently extended the applicability of\nReservoir Computing (RC) methods towards the field of deep learning. In this\npaper we study the impact of constrained reservoir topologies in the\narchitectural design of deep reservoirs, through numerical experiments on\nseveral RC benchmarks. The major outcome of our investigation is to show the\nremarkable effect, in terms of predictive performance gain, achieved by the\nsynergy between a deep reservoir construction and a structured organization of\nthe recurrent units in each layer. Our results also indicate that a\nparticularly advantageous architectural setting is obtained in correspondence\nof DeepESNs where reservoir units are structured according to a permutation\nrecurrent matrix.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 16:15:16 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""]]}, {"id": "1909.11145", "submitter": "Timo C. Wunderlich", "authors": "Timo C. Wunderlich, Akos F. Kungl, Eric M\\\"uller, Johannes Schemmel,\n  Mihai Petrovici", "title": "Brain-Inspired Hardware for Artificial Intelligence: Accelerated\n  Learning in a Physical-Model Spiking Neural Network", "comments": null, "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2019:\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\n  Science, vol 11727. Springer, Cham", "doi": "10.1007/978-3-030-30487-4_10", "report-no": null, "categories": "cs.NE cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future developments in artificial intelligence will profit from the existence\nof novel, non-traditional substrates for brain-inspired computing. Neuromorphic\ncomputers aim to provide such a substrate that reproduces the brain's\ncapabilities in terms of adaptive, low-power information processing. We present\nresults from a prototype chip of the BrainScaleS-2 mixed-signal neuromorphic\nsystem that adopts a physical-model approach with a 1000-fold acceleration of\nspiking neural network dynamics relative to biological real time. Using the\nembedded plasticity processor, we both simulate the Pong arcade video game and\nimplement a local plasticity rule that enables reinforcement learning, allowing\nthe on-chip neural network to learn to play the game. The experiment\ndemonstrates key aspects of the employed approach, such as accelerated and\nflexible learning, high energy efficiency and resilience to noise.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:29:30 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 09:02:01 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wunderlich", "Timo C.", ""], ["Kungl", "Akos F.", ""], ["M\u00fcller", "Eric", ""], ["Schemmel", "Johannes", ""], ["Petrovici", "Mihai", ""]]}, {"id": "1909.11391", "submitter": "Shinnosuke Takamichi", "authors": "Kazuki Fujii, Yuki Saito, Shinnosuke Takamichi, Yukino Baba, Hiroshi\n  Saruwatari", "title": "HumanGAN: generative adversarial network with human-based discriminator\n  and its evaluation in speech perception modeling", "comments": "Submitted to IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the HumanGAN, a generative adversarial network (GAN) incorporating\nhuman perception as a discriminator. A basic GAN trains a generator to\nrepresent a real-data distribution by fooling the discriminator that\ndistinguishes real and generated data. Therefore, the basic GAN cannot\nrepresent the outside of a real-data distribution. In the case of speech\nperception, humans can recognize not only human voices but also processed\n(i.e., a non-existent human) voices as human voice. Such a human-acceptable\ndistribution is typically wider than a real-data one and cannot be modeled by\nthe basic GAN. To model the human-acceptable distribution, we formulate a\nbackpropagation-based generator training algorithm by regarding human\nperception as a black-boxed discriminator. The training efficiently iterates\ngenerator training by using a computer and discrimination by crowdsourcing. We\nevaluate our HumanGAN in speech naturalness modeling and demonstrate that it\ncan represent a human-acceptable distribution that is wider than a real-data\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 10:32:41 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Fujii", "Kazuki", ""], ["Saito", "Yuki", ""], ["Takamichi", "Shinnosuke", ""], ["Baba", "Yukino", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1909.11483", "submitter": "Jordan Ott", "authors": "Jordan Ott, Erik Linstead, Nicholas LaHaye, Pierre Baldi", "title": "Learning in the Machine: To Share or Not to Share?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight-sharing is one of the pillars behind Convolutional Neural Networks and\ntheir successes. However, in physical neural systems such as the brain,\nweight-sharing is implausible. This discrepancy raises the fundamental question\nof whether weight-sharing is necessary. If so, to which degree of precision? If\nnot, what are the alternatives? The goal of this study is to investigate these\nquestions, primarily through simulations where the weight-sharing assumption is\nrelaxed. Taking inspiration from neural circuitry, we explore the use of Free\nConvolutional Networks and neurons with variable connection patterns. Using\nFree Convolutional Networks, we show that while weight-sharing is a pragmatic\noptimization approach, it is not a necessity in computer vision applications.\nFurthermore, Free Convolutional Networks match the performance observed in\nstandard architectures when trained using properly translated data (akin to\nvideo). Under the assumption of translationally augmented data, Free\nConvolutional Networks learn translationally invariant representations that\nyield an approximate form of weight sharing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 20:10:50 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 19:58:04 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ott", "Jordan", ""], ["Linstead", "Erik", ""], ["LaHaye", "Nicholas", ""], ["Baldi", "Pierre", ""]]}, {"id": "1909.11655", "submitter": "AkshatKumar Nigam Mr", "authors": "AkshatKumar Nigam, Pascal Friederich, Mario Krenn, Al\\'an Aspuru-Guzik", "title": "Augmenting Genetic Algorithms with Deep Neural Networks for Exploring\n  the Chemical Space", "comments": "9+3 Pages, 7+4 figures, 2 tables. Comments are welcome! (code is\n  available at: https://github.com/aspuru-guzik-group/GA)", "journal-ref": "International Conference on Learning Representations (ICLR-2020)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG physics.chem-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenges in natural sciences can often be phrased as optimization problems.\nMachine learning techniques have recently been applied to solve such problems.\nOne example in chemistry is the design of tailor-made organic materials and\nmolecules, which requires efficient methods to explore the chemical space. We\npresent a genetic algorithm (GA) that is enhanced with a neural network (DNN)\nbased discriminator model to improve the diversity of generated molecules and\nat the same time steer the GA. We show that our algorithm outperforms other\ngenerative models in optimization tasks. We furthermore present a way to\nincrease interpretability of genetic algorithms, which helped us to derive\ndesign principles.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:59:17 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 23:41:14 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 22:13:45 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2020 23:23:23 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Nigam", "AkshatKumar", ""], ["Friederich", "Pascal", ""], ["Krenn", "Mario", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1909.11700", "submitter": "Claudius Gros", "authors": "Claudius Gros", "title": "A generic framework for task selection driven by synthetic emotions", "comments": "IEEE conference on Humanized Computing, Laguna Hills 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a certain complexity level, humanized agents may select from a wide\nrange of possible tasks, with each activity corresponding to a transient goal.\nIn general there will be no overarching credit assignment scheme allowing to\ncompare available options with respect to expected utilities. For this\nsituation we propose a task selection framework that is based on time\nallocation via emotional stationarity (TAES). Emotions are argued to correspond\nto abstract criteria, such as satisfaction, challenge and boredom, along which\nactivities that have been carried out can be evaluated. The resulting timeline\nof experienced emotions is then compared with the `character' of the agent,\nwhich is defined in terms of a preferred distribution of emotional states. The\nlong-term goal of the agent, to align experience with character, is achieved by\noptimizing the frequency for selecting the individual tasks. Upon optimization,\nthe statistics of emotion experience becomes stationary.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:35:11 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Gros", "Claudius", ""]]}, {"id": "1909.11804", "submitter": "Shusen Liu", "authors": "Shusen Liu, Rushil Anirudh, Jayaraman J. Thiagarajan and Peer-Timo\n  Bremer", "title": "Function Preserving Projection for Scalable Exploration of\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present function preserving projections (FPP), a scalable linear\nprojection technique for discovering interpretable relationships in\nhigh-dimensional data. Conventional dimension reduction methods aim to\nmaximally preserve the global and/or local geometric structure of a dataset.\nHowever, in practice one is often more interested in determining how one or\nmultiple user-selected response function(s) can be explained by the data. To\nintuitively connect the responses to the data, FPP constructs 2D linear\nembeddings optimized to reveal interpretable yet potentially non-linear\npatterns of the response functions. More specifically, FPP is designed to (i)\nproduce human-interpretable embeddings; (ii) capture non-linear relationships;\n(iii) allow the simultaneous use of multiple response functions; and (iv) scale\nto millions of samples. Using FPP on real-world datasets, one can obtain\nfundamentally new insights about high-dimensional relationships in large-scale\ndata that could not be achieved using existing dimension reduction methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 22:40:47 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Liu", "Shusen", ""], ["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1909.11849", "submitter": "Alexander Ororbia", "authors": "AbdElRahman A. ElSaid, Alexander G. Ororbia and Travis J. Desell", "title": "The Ant Swarm Neuro-Evolution Procedure for Optimizing Recurrent\n  Networks", "comments": "15 pages, 22 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand-crafting effective and efficient structures for recurrent neural\nnetworks (RNNs) is a difficult, expensive, and time-consuming process. To\naddress this challenge, we propose a novel neuro-evolution algorithm based on\nant colony optimization (ACO), called ant swarm neuro-evolution (ASNE), for\ndirectly optimizing RNN topologies. The procedure selects from multiple modern\nrecurrent cell types such as Delta-RNN, GRU, LSTM, MGU and UGRNN cells, as well\nas recurrent connections which may span multiple layers and/or steps of time.\nIn order to introduce an inductive bias that encourages the formation of\nsparser synaptic connectivity patterns, we investigate several variations of\nthe core algorithm. We do so primarily by formulating different functions that\ndrive the underlying pheromone simulation process (which mimic L1 and L2\nregularization in standard machine learning) as well as by introducing ant\nagents with specialized roles (inspired by how real ant colonies operate),\ni.e., explorer ants that construct the initial feed forward structure and\nsocial ants which select nodes from the feed forward connections to\nsubsequently craft recurrent memory structures. We also incorporate a\nLamarckian strategy for weight initialization which reduces the number of\nbackpropagation epochs required to locally train candidate RNNs, speeding up\nthe neuro-evolution process. Our results demonstrate that the sparser RNNs\nevolved by ASNE significantly outperform traditional one and two layer\narchitectures consisting of modern memory cells, as well as the well-known NEAT\nalgorithm. Furthermore, we improve upon prior state-of-the-art results on the\ntime series dataset utilized in our experiments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 02:27:11 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 19:11:48 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["ElSaid", "AbdElRahman A.", ""], ["Ororbia", "Alexander G.", ""], ["Desell", "Travis J.", ""]]}, {"id": "1909.11890", "submitter": "Kapil Sharma Prof.", "authors": "Ashish Kumar Tripathi, Kapil Sharma, Manju Bala", "title": "Military Dog Based Optimizer and its Application to Fake Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last three decades more then sixty meta-heuristic algorithms have\nbeen proposed by the various authors. Such algorithms are inspired from\nphysical phenomena, animal behavior or evolutionary concepts. These algorithms\nhave been widely used for solving the various real world optimization problems.\nResearchers are continuously working to improve the existing algorithms and\nalso proposing new algorithms that are giving competitive results as compared\nto the existing algorithms present in the literature. In this paper a novel\nmeta heuristic algorithm based on military dogs squad is introduced. The\nproposed algorithm mimics the searching capability of the trained military\ndogs. Military dogs have strong smell senses by which they are able to search\nthe suspicious objects like bombs, wildlife scats, currency, or blood as well\nas they can communicate with each other by their barking. The performance of\nthe proposed algorithm is tested on 17 benchmark functions and compared with\nfive other meta-heuristics namely particle swarm optimization (PSO), multiverse\noptimizer (MVO), genetic algorithm (GA), probability based learning (PBIL) and\nevolutionary strategy (ES). The results are validated in terms of mean and\nstandard deviation of the fitness value. The convergence behavior and\nconsistency of the results have been also validated by plotting convergence\ngraphs and BoxPlots. Further the, proposed algorithm is successfully utilized\nto solve the real world fake review detection problem. The experimental results\ndemonstrate that the proposed algorithm outperforms the other considered\nalgorithms on the majority of performance parameters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 04:49:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Tripathi", "Ashish Kumar", ""], ["Sharma", "Kapil", ""], ["Bala", "Manju", ""]]}, {"id": "1909.11977", "submitter": "Patrik Reizinger", "authors": "Patrik Reizinger, B\\'alint Gyires-T\\'oth", "title": "Stochastic Weight Matrix-based Regularization Methods for Deep Neural\n  Networks", "comments": "Preprint of an LOD 2019 conference paper, 12 pages, 5 figures", "journal-ref": "Springer LNCS. 11943 (2020) 45-57", "doi": "10.1007/978-3-030-37599-7_5", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to introduce two widely applicable regularization\nmethods based on the direct modification of weight matrices. The first method,\nWeight Reinitialization, utilizes a simplified Bayesian assumption with\npartially resetting a sparse subset of the parameters. The second one, Weight\nShuffling, introduces an entropy- and weight distribution-invariant non-white\nnoise to the parameters. The latter can also be interpreted as an ensemble\napproach. The proposed methods are evaluated on benchmark datasets, such as\nMNIST, CIFAR-10 or the JSB Chorales database, and also on time series modeling\ntasks. We report gains both regarding performance and entropy of the analyzed\nnetworks. We also made our code available as a GitHub repository\n(https://github.com/rpatrik96/lod-wmm-2019).\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:38:55 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Reizinger", "Patrik", ""], ["Gyires-T\u00f3th", "B\u00e1lint", ""]]}, {"id": "1909.12072", "submitter": "Wojciech Samek", "authors": "Wojciech Samek and Klaus-Robert M\\\"uller", "title": "Towards Explainable Artificial Intelligence", "comments": "19 pages", "journal-ref": null, "doi": "10.1007/978-3-030-28954-6_1", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning (ML) has become a key enabling technology\nfor the sciences and industry. Especially through improvements in methodology,\nthe availability of large databases and increased computational power, today's\nML algorithms are able to achieve excellent performance (at times even\nexceeding the human level) on an increasing number of complex tasks. Deep\nlearning models are at the forefront of this development. However, due to their\nnested non-linear structure, these powerful models have been generally\nconsidered \"black boxes\", not providing any information about what exactly\nmakes them arrive at their predictions. Since in many applications, e.g., in\nthe medical domain, such lack of transparency may be not acceptable, the\ndevelopment of methods for visualizing, explaining and interpreting deep\nlearning models has recently attracted increasing attention. This introductory\npaper presents recent developments and applications in this field and makes a\nplea for a wider use of explainable learning algorithms in practice.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:05:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1909.12114", "submitter": "Leila Arras", "authors": "Leila Arras, Jose A. Arjona-Medina, Michael Widrich, Gr\\'egoire\n  Montavon, Michael Gillhofer, Klaus-Robert M\\\"uller, Sepp Hochreiter and\n  Wojciech Samek", "title": "Explaining and Interpreting LSTMs", "comments": "28 pages, 7 figures, book chapter, In: Explainable AI: Interpreting,\n  Explaining and Visualizing Deep Learning, LNCS volume 11700, Springer 2019.\n  arXiv admin note: text overlap with arXiv:1806.07857", "journal-ref": null, "doi": "10.1007/978-3-030-28954-6_11", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks have acted as a strong unifying force in the design of\nmodern AI systems, the neural network architectures themselves remain highly\nheterogeneous due to the variety of tasks to be solved. In this chapter, we\nexplore how to adapt the Layer-wise Relevance Propagation (LRP) technique used\nfor explaining the predictions of feed-forward networks to the LSTM\narchitecture used for sequential data modeling and forecasting. The special\naccumulators and gated interactions present in the LSTM require both a new\npropagation scheme and an extension of the underlying theoretical framework to\ndeliver faithful explanations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 11:45:43 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Arras", "Leila", ""], ["Arjona-Medina", "Jose A.", ""], ["Widrich", "Michael", ""], ["Montavon", "Gr\u00e9goire", ""], ["Gillhofer", "Michael", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Hochreiter", "Sepp", ""], ["Samek", "Wojciech", ""]]}, {"id": "1909.12486", "submitter": "Fu-Ming Guo", "authors": "Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin and Yanzhi Wang", "title": "Reweighted Proximal Pruning for Large-Scale Language Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained language representation flourishes as the mainstay of\nthe natural language understanding community, e.g., BERT. These pre-trained\nlanguage representations can create state-of-the-art results on a wide range of\ndownstream tasks. Along with continuous significant performance improvement,\nthe size and complexity of these pre-trained neural models continue to increase\nrapidly. Is it possible to compress these large-scale language representation\nmodels? How will the pruned language representation affect the downstream\nmulti-task transfer learning objectives? In this paper, we propose Reweighted\nProximal Pruning (RPP), a new pruning method specifically designed for a\nlarge-scale language representation model. Through experiments on SQuAD and the\nGLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for\nboth the pre-training task and the downstream multiple fine-tuning tasks at\nhigh prune ratio. RPP provides a new perspective to help us analyze what\nlarge-scale language representation might learn. Additionally, RPP makes it\npossible to deploy a large state-of-the-art language representation model such\nas BERT on a series of distinct devices (e.g., online servers, mobile phones,\nand edge devices).\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 04:10:10 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 01:23:53 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Guo", "Fu-Ming", ""], ["Liu", "Sijia", ""], ["Mungall", "Finlay S.", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1909.12612", "submitter": "Bisser Raytchev", "authors": "Shohei Hayashi and Bisser Raytchev and Toru Tamaki and Kazufumi Kaneda", "title": "Biomedical Image Segmentation by Retina-like Sequential Attention\n  Mechanism Using Only A Few Training Images", "comments": "Submitted to MLMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel deep learning-based algorithm for biomedical\nimage segmentation which uses a sequential attention mechanism able to shift\nthe focus of attention across the image in a selective way, allowing subareas\nwhich are more difficult to classify to be processed at increased resolution.\nThe spatial distribution of class information in each subarea is learned using\na retina-like representation where resolution decreases with distance from the\ncenter of attention. The final segmentation is achieved by averaging class\npredictions over overlapping subareas, utilizing the power of ensemble learning\nto increase segmentation accuracy. Experimental results for semantic\nsegmentation task for which only a few training images are available show that\na CNN using the proposed method outperforms both a patch-based classification\nCNN and a fully convolutional-based method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 10:55:24 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Hayashi", "Shohei", ""], ["Raytchev", "Bisser", ""], ["Tamaki", "Toru", ""], ["Kaneda", "Kazufumi", ""]]}, {"id": "1909.13030", "submitter": "Benjamin Evans", "authors": "Benjamin Patrick Evans, Harith Al-Sahaf, Bing Xue, Mengjie Zhang", "title": "Genetic Programming and Gradient Descent: A Memetic Approach to Binary\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is an essential task in computer vision, which aims to\ncategorise a set of images into different groups based on some visual criteria.\nExisting methods, such as convolutional neural networks, have been successfully\nutilised to perform image classification. However, such methods often require\nhuman intervention to design a model. Furthermore, such models are difficult to\ninterpret and it is challenging to analyse the patterns of different classes.\nThis paper presents a hybrid (memetic) approach combining genetic programming\n(GP) and Gradient-based optimisation for image classification to overcome the\nlimitations mentioned. The performance of the proposed method is compared to a\nbaseline version (without local search) on four binary classification image\ndatasets to provide an insight into the usefulness of local search mechanisms\nfor enhancing the performance of GP.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:42:22 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Evans", "Benjamin Patrick", ""], ["Al-Sahaf", "Harith", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1909.13183", "submitter": "Carl Yang", "authors": "Carl Yang, Lingrui Gan, Zongyi Wang, Jiaming Shen, Jinfeng Xiao,\n  Jiawei Han", "title": "Query-Specific Knowledge Summarization with Entity Evolutionary Networks", "comments": "published in CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a query, unlike traditional IR that finds relevant documents or\nentities, in this work, we focus on retrieving both entities and their\nconnections for insightful knowledge summarization. For example, given a query\n\"computer vision\" on a CS literature corpus, rather than returning a list of\nrelevant entities like \"cnn\", \"imagenet\" and \"svm\", we are interested in the\nconnections among them, and furthermore, the evolution patterns of such\nconnections along particular ordinal dimensions such as time. Particularly, we\nhope to provide structural knowledge relevant to the query, such as \"svm\" is\nrelated to \"imagenet\" but not \"cnn\". Moreover, we aim to model the changing\ntrends of the connections, such as \"cnn\" becomes highly related to \"imagenet\"\nafter 2010, which enables the tracking of knowledge evolutions. In this work,\nto facilitate such a novel insightful search system, we propose\n\\textsc{SetEvolve}, which is a unified framework based on nonparanomal\ngraphical models for evolutionary network construction from large text corpora.\nSystematic experiments on synthetic data and insightful case studies on\nreal-world corpora demonstrate the utility of \\textsc{SetEvolve}.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 01:48:32 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yang", "Carl", ""], ["Gan", "Lingrui", ""], ["Wang", "Zongyi", ""], ["Shen", "Jiaming", ""], ["Xiao", "Jinfeng", ""], ["Han", "Jiawei", ""]]}, {"id": "1909.13265", "submitter": "Fangwen Tu", "authors": "Fangwen Tu, Shuzhi Sam Ge, Yoo Sang Choo and Chang Chieh Hang", "title": "Adaptive Control for Marine Vessels Against Harsh Environmental\n  Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, robust control with sea state observer and dynamic thrust\nallocation is proposed for the Dynamic Positioning (DP) of an accommodation\nvessel in the presence of unknown hydrodynamic force variation and the input\ntime delay. In order to overcome the huge force variation due to the adjoining\nFloating Production Storage and Offloading (FPSO) and accommodation vessel, a\nnovel sea state observer is designed. The sea observer can effectively monitor\nthe variation of the drift wave-induced force on the vessel and activate Neural\nNetwork (NN) compensator in the controller when large wave force is identified.\nMoreover, the wind drag coefficients can be adaptively approximated in the sea\nobserver so that a feedforward control can be achieved. Based on this, a robust\nconstrained control is developed to guarantee a safe operation. The time delay\ninside the control input is also considered. Dynamic thrust allocation module\nis presented to distribute the generalized control input among azimuth\nthrusters. Under the proposed sea observer and control, the boundedness of all\nthe closed-loop signals are demonstrated via rigorous Lyapunov analysis. A set\nof simulation studies are conducted to verify the effectiveness of the proposed\ncontrol scheme.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 12:10:43 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Tu", "Fangwen", ""], ["Ge", "Shuzhi Sam", ""], ["Choo", "Yoo Sang", ""], ["Hang", "Chang Chieh", ""]]}, {"id": "1909.13354", "submitter": "Parsa Esfahanian", "authors": "Parsa Esfahanian, Mohammad Akhavan", "title": "GACNN: Training Deep Convolutional Neural Networks with Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have gained a significant attraction in\nthe recent years due to their increasing real-world applications. Their\nperformance is highly dependent to the network structure and the selected\noptimization method for tuning the network parameters. In this paper, we\npropose novel yet efficient methods for training convolutional neural networks.\nThe most of current state of the art learning method for CNNs are based on\nGradient decent. In contrary to the traditional CNN training methods, we\npropose to optimize the CNNs using methods based on Genetic Algorithms (GAs).\nThese methods are carried out using three individual GA schemes, Steady-State,\nGenerational, and Elitism. We present new genetic operators for crossover,\nmutation and also an innovative encoding paradigm of CNNs to chromosomes aiming\nto reduce the resulting chromosome's size by a large factor. We compare the\neffectiveness and scalability of our encoding with the traditional encoding.\nFurthermore, the performance of individual GA schemes used for training the\nnetworks were compared with each other in means of convergence rate and overall\naccuracy. Finally, our new encoding alongside the superior GA-based training\nscheme is compared to Backpropagation training with Adam optimization.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 19:55:10 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Esfahanian", "Parsa", ""], ["Akhavan", "Mohammad", ""]]}, {"id": "1909.13374", "submitter": "Neehar Peri", "authors": "Neehar Peri, Neal Gupta, W. Ronny Huang, Liam Fowl, Chen Zhu, Soheil\n  Feizi, Tom Goldstein, John P. Dickerson", "title": "Deep k-NN Defense against Clean-label Data Poisoning Attacks", "comments": "Accepted to ECCV 2020 Workshop - Adversarial Robustness in the Real\n  World (AROW). First three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted clean-label data poisoning is a type of adversarial attack on\nmachine learning systems in which an adversary injects a few correctly-labeled,\nminimally-perturbed samples into the training data, causing a model to\nmisclassify a particular test sample during inference. Although defenses have\nbeen proposed for general poisoning attacks, no reliable defense for\nclean-label attacks has been demonstrated, despite the attacks' effectiveness\nand realistic applications. In this work, we propose a simple, yet\nhighly-effective Deep k-NN defense against both feature collision and convex\npolytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our\nproposed strategy is able to detect over 99% of poisoned examples in both\nattacks and remove them without compromising model performance. Additionally,\nthrough ablation studies, we discover simple guidelines for selecting the value\nof k as well as for implementing the Deep k-NN defense on real-world datasets\nwith class imbalance. Our proposed defense shows that current clean-label\npoisoning attack strategies can be annulled, and serves as a strong yet\nsimple-to-implement baseline defense to test future clean-label poisoning\nattacks. Our code is available at https://github.com/neeharperi/DeepKNNDefense\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 21:47:14 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 02:38:02 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 05:47:23 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Peri", "Neehar", ""], ["Gupta", "Neal", ""], ["Huang", "W. Ronny", ""], ["Fowl", "Liam", ""], ["Zhu", "Chen", ""], ["Feizi", "Soheil", ""], ["Goldstein", "Tom", ""], ["Dickerson", "John P.", ""]]}, {"id": "1909.13446", "submitter": "Xinlin Li", "authors": "Xinlin Li, Vahid Partovi Nia", "title": "Random Bias Initialization Improves Quantized Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks improve computationally efficiency of deep models with\na large margin. However, there is still a performance gap between a successful\nfull-precision training and binary training. We bring some insights about why\nthis accuracy drop exists and call for a better understanding of binary network\ngeometry. We start with analyzing full-precision neural networks with ReLU\nactivation and compare it with its binarized version. This comparison suggests\nto initialize networks with random bias, a counter-intuitive remedy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 04:01:13 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 19:50:23 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Xinlin", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1909.13465", "submitter": "Takumi Ichimura", "authors": "Shin Kamada and Takumi Ichimura", "title": "An Object Detection by using Adaptive Structural Learning of Deep Belief\n  Network", "comments": "8 pages, 14 figures, The International Joint Conference on Neural\n  Networks (IJCNN 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning forms a hierarchical network structure for representation of\nmultiple input features. The adaptive structural learning method of Deep Belief\nNetwork (DBN) can realize a high classification capability while searching the\noptimal network structure during the training. The method can find the optimal\nnumber of hidden neurons for given input data in a Restricted Boltzmann Machine\n(RBM) by neuron generation-annihilation algorithm. Moreover, it can generate a\nnew hidden layer in DBN by the layer generation algorithm to actualize a deep\ndata representation. The proposed method showed higher classification accuracy\nfor image benchmark data sets than several deep learning methods including\nwell-known CNN methods. In this paper, a new object detection method for the\nDBN architecture is proposed for localization and category of objects. The\nmethod is a task for finding semantic objects in images as Bounding Box\n(B-Box). To investigate the effectiveness of the proposed method, the adaptive\nstructural learning of DBN and the object detection were evaluated on the Chest\nX-ray image benchmark data set (CXR8), which is one of the most commonly\naccessible radio-logical examination for many lung diseases. The proposed\nmethod showed higher performance for both classification (more than 94.5%\nclassification for test data) and localization (more than 90.4% detection for\ntest data) than the other CNN methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 05:49:02 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1909.13480", "submitter": "Takumi Ichimura", "authors": "Shin Kamada and Takumi Ichimura", "title": "A Video Recognition Method by using Adaptive Structural Learning of Long\n  Short Term Memory based Deep Belief Network", "comments": "6 pages, 7 figures, IEEE 11th International Workshop on Computational\n  Intelligence and Applications (IWCIA2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning builds deep architectures such as multi-layered artificial\nneural networks to effectively represent multiple features of input patterns.\nThe adaptive structural learning method of Deep Belief Network (DBN) can\nrealize a high classification capability while searching the optimal network\nstructure during the training. The method can find the optimal number of hidden\nneurons of a Restricted Boltzmann Machine (RBM) by neuron\ngeneration-annihilation algorithm to train the given input data, and then it\ncan make a new layer in DBN by the layer generation algorithm to actualize a\ndeep data representation. Moreover, the learning algorithm of Adaptive RBM and\nAdaptive DBN was extended to the time-series analysis by using the idea of LSTM\n(Long Short Term Memory). In this paper, our proposed prediction method was\napplied to Moving MNIST, which is a benchmark data set for video recognition.\nWe challenge to reveal the power of our proposed method in the video\nrecognition research field, since video includes rich source of visual\ninformation. Compared with the LSTM model, our method showed higher prediction\nperformance (more than 90% predication accuracy for test data).\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:57:55 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1909.13481", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura and Shin Kamada", "title": "Re-learning of Child Model for Misclassified data by using KL Divergence\n  in AffectNet: A Database for Facial Expression", "comments": "6 pages, 7 figures, IEEE 11th International Workshop on Computational\n  Intelligence and Applications (IWCIA2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AffectNet contains more than 1,000,000 facial images which manually annotated\nfor the presence of eight discrete facial expressions and the intensity of\nvalence and arousal. Adaptive structural learning method of DBN (Adaptive DBN)\nis positioned as a top Deep learning model of classification capability for\nsome large image benchmark databases. The Convolutional Neural Network and\nAdaptive DBN were trained for AffectNet and classification capability was\ncompared. Adaptive DBN showed higher classification ratio. However, the model\nwas not able to classify some test cases correctly because human emotions\ncontain many ambiguous features or patterns leading wrong answer which includes\nthe possibility of being a factor of adversarial examples, due to two or more\nannotators answer different subjective judgment for an image. In order to\ndistinguish such cases, this paper investigated a re-learning model of Adaptive\nDBN with two or more child models, where the original trained model can be seen\nas a parent model and then new child models are generated for some\nmisclassified cases. In addition, an appropriate child model was generated\naccording to difference between two models by using KL divergence. The\ngenerated child models showed better performance to classify two emotion\ncategories: `Disgust' and `Anger'.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:58:27 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ichimura", "Takumi", ""], ["Kamada", "Shin", ""]]}, {"id": "1909.13522", "submitter": "Yang Shunzhi South China Normal University", "authors": "Shunzhi Yang, Zheng Gong, Kai Ye, Yungen Wei, Zheng Huang, Zhenhua\n  Huang", "title": "EdgeCNN: Convolutional Neural Network Classification Model with small\n  inputs for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Internet of Things (IoT), data is increasingly\nappearing on the edge of the network. Processing tasks on the edge of the\nnetwork can effectively solve the problems of personal privacy leaks and server\noverload. As a result, it has attracted a great deal of attention and made\nsubstantial progress. This progress includes efficient convolutional neural\nnetwork (CNN) models such as MobileNet and ShuffleNet. However, all of these\nnetworks appear as a common network model and they usually need to identify\nmultiple targets when applied. So the size of the input is very large. In some\nspecific cases, only the target needs to be classified. Therefore, a small\ninput network can be designed to reduce computation. In addition, other\nefficient neural network models are primarily designed for mobile phones.\nMobile phones have faster memory access, which allows them to use group\nconvolution. In particular, this paper finds that the recently widely used\ngroup convolution is not suitable for devices with very slow memory access.\nTherefore, the EdgeCNN of this paper is designed for edge computing devices\nwith low memory access speed and low computing resources. EdgeCNN has been run\nsuccessfully on the Raspberry Pi 3B+ at a speed of 1.37 frames per second. The\naccuracy of facial expression classification for the FER-2013 and RAF-DB\ndatasets outperforms other proposed networks that are compatible with the\nRaspberry Pi 3B+. The implementation of EdgeCNN is available at\nhttps://github.com/yangshunzhi1994/EdgeCNN\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:45:03 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yang", "Shunzhi", ""], ["Gong", "Zheng", ""], ["Ye", "Kai", ""], ["Wei", "Yungen", ""], ["Huang", "Zheng", ""], ["Huang", "Zhenhua", ""]]}, {"id": "1909.13567", "submitter": "Ke Li Kl", "authors": "Ke Li, Minhui Liao, Kalyanmoy Deb, Geyong Min, Xin Yao", "title": "Does Preference Always Help? A Holistic Study on Preference-Based\n  Evolutionary Multi-Objective Optimisation Using Reference Points", "comments": "32 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of multi-objective optimisation is to help a decision maker\n(DM) identify solution(s) of interest (SOI) achieving satisfactory trade-offs\namong multiple conflicting criteria. This can be realised by leveraging DM's\npreference information in evolutionary multi-objective optimisation (EMO). No\nconsensus has been reached on the effectiveness brought by incorporating\npreference in EMO (either a priori or interactively) versus a posteriori\ndecision making after a complete run of an EMO algorithm. Bearing this\nconsideration in mind, this paper i) provides a pragmatic overview of the\nexisting developments of preference-based EMO; and ii) conducts a series of\nexperiments to investigate the effectiveness brought by preference\nincorporation in EMO for approximating various SOI. In particular, the DM's\npreference information is elicited as a reference point, which represents\nher/his aspirations for different objectives. Experimental results demonstrate\nthat preference incorporation in EMO does not always lead to a desirable\napproximation of SOI if the DM's preference information is not well utilised,\nnor does the DM elicit invalid preference information, which is not uncommon\nwhen encountering a black-box system. To a certain extent, this issue can be\nremedied through an interactive preference elicitation. Last but not the least,\nwe find that a preference-based EMO algorithm is able to be generalised to\napproximate the whole PF given an appropriate setup of preference information.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 10:21:50 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Li", "Ke", ""], ["Liao", "Minhui", ""], ["Deb", "Kalyanmoy", ""], ["Min", "Geyong", ""], ["Yao", "Xin", ""]]}, {"id": "1909.13698", "submitter": "Yuyang Gao", "authors": "Yuyang Gao, Giorgio A. Ascoli, Liang Zhao", "title": "BEAN: Interpretable Representation Learning with Biologically-Enhanced\n  Artificial Neuronal Assembly Regularization", "comments": "Accepted as an original research paper at Frontiers in Neurorobotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known for extracting useful information from\nlarge amounts of data. However, the representations learned in DNNs are\ntypically hard to interpret, especially in dense layers. One crucial issue of\nthe classical DNN model such as multilayer perceptron (MLP) is that neurons in\nthe same layer of DNNs are conditionally independent of each other, which makes\nco-training and emergence of higher modularity difficult. In contrast to DNNs,\nbiological neurons in mammalian brains display substantial dependency patterns.\nSpecifically, biological neural networks encode representations by so-called\nneuronal assemblies: groups of neurons interconnected by strong synaptic\ninteractions and sharing joint semantic content. The resulting population\ncoding is essential for human cognitive and mnemonic processes. Here, we\npropose a novel Biologically Enhanced Artificial Neuronal assembly (BEAN)\nregularization to model neuronal correlations and dependencies, inspired by\ncell assembly theory from neuroscience. Experimental results show that BEAN\nenables the formation of interpretable neuronal functional clusters and\nconsequently promotes a sparse, memory/computation-efficient network without\nloss of model performance. Moreover, our few-shot learning experiments\ndemonstrate that BEAN could also enhance the generalizability of the model when\ntraining samples are extremely limited.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 14:54:30 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 14:18:45 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gao", "Yuyang", ""], ["Ascoli", "Giorgio A.", ""], ["Zhao", "Liang", ""]]}]