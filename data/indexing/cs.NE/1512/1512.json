[{"id": "1512.00035", "submitter": "Benoit Girard", "authors": "Jean Li\\'enard (ISIR), Beno\\^it Girard (ISIR)", "title": "A biologically constrained model of the whole basal ganglia addressing\n  the paradoxes of connections and selection", "comments": "\\&lt;http://link.springer.com/article/10.1007%2Fs10827-013-0476-2\\&gt;.\n  \\&lt;10.1007/s10827-013-0476-2\\&gt", "journal-ref": "Journal of Computational Neuroscience, Springer Verlag (Germany),\n  2014, 36 (3), pp.445-468", "doi": "10.1007/s10827-013-0476-2", "report-no": null, "categories": "q-bio.NC cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basal ganglia nuclei form a complex network of nuclei often assumed to\nperform selection, yet their individual roles and how they influence each other\nis still largely unclear. In particular, the ties between the external and\ninternal parts of the globus pallidus are paradoxical, as anatomical data\nsuggest a potent inhibitory projection between them while electrophys-iological\nrecordings indicate that they have similar activities. Here we introduce a\ntheoretical study that reconciles both views on the intra-pallidal projection,\nby providing a plausible characterization of the relationship between the\nexternal and internal globus pallidus. Specifically, we developed a mean-field\nmodel of the whole basal ganglia, whose parameterization is optimized to\nrespect best a collection of numerous anatomical and electrophysiological data.\nWe first obtained models respecting all our constraints, hence anatomical and\nelectrophysiological data on the intrapallidal projection are globally\nconsistent. This model furthermore predicts that both aforementioned views\nabout the intra-pallidal projection may be reconciled when this projection is\nweakly inhibitory, thus making it possible to support similar neural activity\nin both nuclei and for the entire basal ganglia to select between actions.\nSecond, we predicts that afferent projections are substantially unbalanced\ntowards the external segment, as it receives the strongest excitation from STN\nand the weakest inhibition from the striatum. Finally, our study strongly\nsuggest that the intrapallidal connection pattern is not focused but diffuse,\nas this latter pattern is more efficient for the overall selection performed in\nthe basal ganglia.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 07:45:55 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Li\u00e9nard", "Jean", "", "ISIR"], ["Girard", "Beno\u00eet", "", "ISIR"]]}, {"id": "1512.00177", "submitter": "Yiming Cui", "authors": "Yiming Cui, Shijin Wang, Jianfeng Li", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "comments": "6 pages, accepted by NAACL2016 short paper", "journal-ref": null, "doi": "10.18653/v1/N16-1112", "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are powerful models, which have been widely\napplied into many aspects of machine translation, such as language modeling and\ntranslation modeling. Though notable improvements have been made in these\nareas, the reordering problem still remains a challenge in statistical machine\ntranslations. In this paper, we present a novel neural reordering model that\ndirectly models word pairs and alignment. By utilizing LSTM recurrent neural\nnetworks, much longer context could be learned for reordering prediction.\nExperimental results on NIST OpenMT12 Arabic-English and Chinese-English\n1000-best rescoring task show that our LSTM neural reordering feature is robust\nand achieves significant improvements over various baseline systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 08:43:19 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 01:17:22 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 10:01:49 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Cui", "Yiming", ""], ["Wang", "Shijin", ""], ["Li", "Jianfeng", ""]]}, {"id": "1512.00242", "submitter": "Haibing Wu", "authors": "Haibing Wu and Xiaodong Gu", "title": "Towards Dropout Training for Convolutional Neural Networks", "comments": "This paper has been published in Neural Networks,\n  http://www.sciencedirect.com/science/article/pii/S0893608015001446", "journal-ref": "Neural Networks 71: 1-10 (2015)", "doi": "10.1016/j.neunet.2015.07.007", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dropout has seen increasing use in deep learning. For deep\nconvolutional neural networks, dropout is known to work well in fully-connected\nlayers. However, its effect in convolutional and pooling layers is still not\nclear. This paper demonstrates that max-pooling dropout is equivalent to\nrandomly picking activation based on a multinomial distribution at training\ntime. In light of this insight, we advocate employing our proposed\nprobabilistic weighted pooling, instead of commonly used max-pooling, to act as\nmodel averaging at test time. Empirical evidence validates the superiority of\nprobabilistic weighted pooling. We also empirically show that the effect of\nconvolutional dropout is not trivial, despite the dramatically reduced\npossibility of over-fitting due to the convolutional architecture. Elaborately\ndesigning dropout training simultaneously in max-pooling and fully-connected\nlayers, we achieve state-of-the-art performance on MNIST, and very competitive\nresults on CIFAR-10 and CIFAR-100, relative to other approaches without data\naugmentation. Finally, we compare max-pooling dropout and stochastic pooling,\nboth of which introduce stochasticity based on multinomial distributions at\npooling stage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 12:46:11 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1512.00708", "submitter": "Totok R. Biyanto", "authors": "Totok Ruki Biyanto, Henokh Yernias Fibrianto, Gunawan Nugroho, Erny\n  Listijorini, Titik Budiati, Hairul Huda", "title": "Duelist Algorithm: An Algorithm Inspired by How Duelist Improve Their\n  Capabilities in a Duel", "comments": "This paper under submission to the Journal of Swarm and Evolutionary\n  Computation, consist of 7 pages and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an optimization algorithm based on how human fight and\nlearn from each duelist. Since this algorithm is based on population, the\nproposed algorithm starts with an initial set of duelists. The duel is to\ndetermine the winner and loser. The loser learns from the winner, while the\nwinner try their new skill or technique that may improve their fighting\ncapabilities. A few duelists with highest fighting capabilities are called as\nchampion. The champion train a new duelists such as their capabilities. The new\nduelist will join the tournament as a representative of each champion. All\nduelist are re-evaluated, and the duelists with worst fighting capabilities is\neliminated to maintain the amount of duelists. Two optimization problem is\napplied for the proposed algorithm, together with genetic algorithm, particle\nswarm optimization and imperialist competitive algorithm. The results show that\nthe proposed algorithm is able to find the better global optimum and faster\niteration.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 14:29:13 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Biyanto", "Totok Ruki", ""], ["Fibrianto", "Henokh Yernias", ""], ["Nugroho", "Gunawan", ""], ["Listijorini", "Erny", ""], ["Budiati", "Titik", ""], ["Huda", "Hairul", ""]]}, {"id": "1512.00883", "submitter": "Totok R. Biyanto", "authors": "Totok R. Biyanto, Sumitra Wira Suganda, Matraji, Yerry Susatio, Heri\n  Justiono, Sarwono", "title": "Cleaning Schedule Optimization of Heat Exchanger Networks Using Particle\n  Swarm Optimization", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oil refinery is one of industries that require huge energy consumption. The\ntoday technology advance requires energy saving. Heat integration is a method\nused to minimize the energy comsumption though the implementation of Heat\nExchanger Network (HEN). CPT is one of types of Heat Exchanger Network (HEN)\nthat functions to recover the heat in the flow of product or waste. HEN\ncomprises a number of heat exchangers (HEs) that are serially connected.\nHowever, the presence of fouling in the heat exchanger has caused the decline\nof the performance of both heat exchangers and all heat exchanger networks.\nFouling can not be avoided. However, it can be mitigated. In industry, periodic\nheat exchanger cleaning is the most effective and widely used mitigation\ntechnique. On the other side, a very frequent cleaning of heat exchanger can be\nmuch costly in maintenance and lost of production. In this way, an accurate\noptimization technique of cleaning schedule interval of heat exchanger is very\nessential. Commonly, this technique involves three elements: model to simulate\nthe heat exchanger network, representative fouling model to describe the\nfouling behavior and suitable optimization algorithm to solve the problem of\nclening schedule interval for heat exchanger network. This paper describe the\noptimization of interval cleaning schedule of HEN within the 44-month period\nusing PSO (particle swarm optimization). The number of iteration used to\nachieve the convergent is 100 iterations and the fitness value in PSO\ncorrelated with the amount of heat recovery, cleaning cost, and additional\npumping cost. The saving after the optimization of cleaning schedule of HEN in\nthis research achieved at $ 1.236 millions or 23% of maximum potential savings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 22:08:09 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Biyanto", "Totok R.", ""], ["Suganda", "Sumitra Wira", ""], ["Matraji", "", ""], ["Susatio", "Yerry", ""], ["Justiono", "Heri", ""], ["Sarwono", "", ""]]}, {"id": "1512.00961", "submitter": "Roshan Gopalakrishnan", "authors": "Roshan Gopalakrishnan, Arindam Basu", "title": "Triplet Spike Time Dependent Plasticity: A floating-gate Implementation", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synapse plays an important role of learning in a neural network; the learning\nrules which modify the synaptic strength based on the timing difference between\nthe pre- and post-synaptic spike occurrence is termed as Spike Time Dependent\nPlasticity (STDP). The most commonly used rule posits weight change based on\ntime difference between one pre- and one post spike and is hence termed doublet\nSTDP (DSTDP). However, D-STDP could not reproduce results of many biological\nexperiments; a triplet STDP (T-STDP) that considers triplets of spikes as the\nfundamental unit has been proposed recently to explain these observations. This\npaper describes the compact implementation of a synapse using single\nfloating-gate (FG) transistor that can store a weight in a nonvolatile manner\nand demonstrate the triplet STDP (T-STDP) learning rule by modifying drain\nvoltages according to triplets of spikes. We describe a mathematical procedure\nto obtain control voltages for the FG device for T-STDP and also show\nmeasurement results from a FG synapse fabricated in TSMC 0.35um CMOS process to\nsupport the theory. Possible VLSI implementation of drain voltage waveform\ngenerator circuits are also presented with simulation results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 06:12:05 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 06:03:16 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2015 05:12:44 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Gopalakrishnan", "Roshan", ""], ["Basu", "Arindam", ""]]}, {"id": "1512.00965", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao", "title": "Neural Enquirer: Learning to Query Tables with Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed Neural Enquirer as a neural network architecture to execute a\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\nNeural Enquirer finds the distributed representation of a query and then\nexecutes it on knowledge-base tables to obtain the answer as one of the values\nin the tables. Unlike similar efforts in end-to-end training of semantic\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\ndistributional representation of the query and the knowledge-base, but also\nrealizes the execution of compositional queries as a series of differentiable\noperations, with intermediate results (consisting of annotations of the tables\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\ntrained with gradient descent, with which not only the parameters of the\ncontrolling components and semantic parsing component, but also the embeddings\nof the tables and query words can be learned from scratch. The training can be\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\nstep-by-step supervision for complicated queries, and benefit from it. Neural\nEnquirer is one step towards building neural network systems which seek to\nunderstand language by executing it on real-world. Our experiments show that\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\nwith rich structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 06:46:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 02:46:25 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Yin", "Pengcheng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Kao", "Ben", ""]]}, {"id": "1512.01274", "submitter": "Mu Li", "authors": "Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,\n  Tianjun Xiao, Bing Xu, Chiyuan Zhang and Zheng Zhang", "title": "MXNet: A Flexible and Efficient Machine Learning Library for\n  Heterogeneous Distributed Systems", "comments": "In Neural Information Processing Systems, Workshop on Machine\n  Learning Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MS cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MXNet is a multi-language machine learning (ML) library to ease the\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\nthe host language, it blends declarative symbolic expression with imperative\ntensor computation. It offers auto differentiation to derive gradients. MXNet\nis computation and memory efficient and runs on various heterogeneous systems,\nranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of\nMXNet, and explains how embedding of both symbolic expression and tensor\noperation is handled in a unified fashion. Our preliminary experiments reveal\npromising results on large scale deep neural network applications using\nmultiple GPU machines.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 22:49:21 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Chen", "Tianqi", ""], ["Li", "Mu", ""], ["Li", "Yutian", ""], ["Lin", "Min", ""], ["Wang", "Naiyan", ""], ["Wang", "Minjie", ""], ["Xiao", "Tianjun", ""], ["Xu", "Bing", ""], ["Zhang", "Chiyuan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1512.01289", "submitter": "Edward Grant", "authors": "Edward Grant, Stephan Sahm, Mariam Zabihi, Marcel van Gerven", "title": "Predicting and visualizing psychological attributions with a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judgments about personality based on facial appearance are strong effectors\nin social decision making, and are known to have impact on areas from\npresidential elections to jury decisions. Recent work has shown that it is\npossible to predict perception of memorability, trustworthiness, intelligence\nand other attributes in human face images. The most successful of these\napproaches require face images expertly annotated with key facial landmarks. We\ndemonstrate a Convolutional Neural Network (CNN) model that is able to perform\nthe same task without the need for landmark features, thereby greatly\nincreasing efficiency. The model has high accuracy, surpassing human-level\nperformance in some cases. Furthermore, we use a deconvolutional approach to\nvisualize important features for perception of 22 attributes and demonstrate a\nnew method for separately visualizing positive and negative features.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 00:24:16 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 01:06:35 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Grant", "Edward", ""], ["Sahm", "Stephan", ""], ["Zabihi", "Mariam", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1512.01314", "submitter": "Subhrajit Roy", "authors": "Subhrajit Roy and Arindam Basu", "title": "An Online Unsupervised Structural Plasticity Algorithm for Spiking\n  Neural Networks", "comments": "11 pages, 10 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel Winner-Take-All (WTA) architecture\nemploying neurons with nonlinear dendrites and an online unsupervised\nstructural plasticity rule for training it. Further, to aid hardware\nimplementations, our network employs only binary synapses. The proposed\nlearning rule is inspired by spike time dependent plasticity (STDP) but differs\nfor each dendrite based on its activation level. It trains the WTA network\nthrough formation and elimination of connections between inputs and synapses.\nTo demonstrate the performance of the proposed network and learning rule, we\nemploy it to solve two, four and six class classification of random Poisson\nspike time inputs. The results indicate that by proper tuning of the inhibitory\ntime constant of the WTA, a trade-off between specificity and sensitivity of\nthe network can be achieved. We use the inhibitory time constant to set the\nnumber of subpatterns per pattern we want to detect. We show that while the\npercentage of successful trials are 92%, 88% and 82% for two, four and six\nclass classification when no pattern subdivisions are made, it increases to\n100% when each pattern is subdivided into 5 or 10 subpatterns. However, the\nformer scenario of no pattern subdivision is more jitter resilient than the\nlater ones.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 04:39:11 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Roy", "Subhrajit", ""], ["Basu", "Arindam", ""]]}, {"id": "1512.01322", "submitter": "Sungho Shin", "authors": "Sungho Shin, Kyuyeon Hwang, and Wonyong Sung", "title": "Fixed-Point Performance Analysis of Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2015.2411564", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have shown excellent performance in many\napplications, however they require increased complexity in hardware or software\nbased implementations. The hardware complexity can be much lowered by\nminimizing the word-length of weights and signals. This work analyzes the\nfixed-point performance of recurrent neural networks using a retrain based\nquantization method. The quantization sensitivity of each layer in RNNs is\nstudied, and the overall fixed-point optimization results minimizing the\ncapacity of weights while not sacrificing the performance are presented. A\nlanguage model and a phoneme recognition examples are used.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 06:07:28 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 11:53:34 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 11:42:34 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Shin", "Sungho", ""], ["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.01332", "submitter": "Naoto Yoshida", "authors": "Naoto Yoshida", "title": "Q-Networks for Binary Vector Actions", "comments": "9 pages, 5 figures, accepted for Deep Reinforcement Learning\n  Workshop, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper reinforcement learning with binary vector actions was\ninvestigated. We suggest an effective architecture of the neural networks for\napproximating an action-value function with binary vector actions. The proposed\narchitecture approximates the action-value function by a linear function with\nrespect to the action vector, but is still non-linear with respect to the state\ninput. We show that this approximation method enables the efficient calculation\nof greedy action selection and softmax action selection. Using this\narchitecture, we suggest an online algorithm based on Q-learning. The empirical\nresults in the grid world and the blocker task suggest that our approximation\narchitecture would be effective for the RL problems with large discrete action\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 07:51:48 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Yoshida", "Naoto", ""]]}, {"id": "1512.01362", "submitter": "Collins Achepsah Leke", "authors": "Collins Leke, Tshilidzi Marwala and Satyakama Paul", "title": "Proposition of a Theoretical Model for Missing Data Imputation using\n  Deep Learning and Evolutionary Algorithms", "comments": "14 Pages, 4 figures, journal, experiments will be added testing the\n  hypotheses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last couple of decades, there has been major advancements in the\ndomain of missing data imputation. The techniques in the domain include amongst\nothers: Expectation Maximization, Neural Networks with Evolutionary Algorithms\nor optimization techniques and K-Nearest Neighbor approaches to solve the\nproblem. The presence of missing data entries in databases render the tasks of\ndecision-making and data analysis nontrivial. As a result this area has\nattracted a lot of research interest with the aim being to yield accurate and\ntime efficient and sensitive missing data imputation techniques especially when\ntime sensitive applications are concerned like power plants and winding\nprocesses. In this article, considering arbitrary and monotone missing data\npatterns, we hypothesize that the use of deep neural networks built using\nautoencoders and denoising autoencoders in conjunction with genetic algorithms,\nswarm intelligence and maximum likelihood estimator methods as novel data\nimputation techniques will lead to better imputed values than existing\ntechniques. Also considered are the missing at random, missing completely at\nrandom and missing not at random missing data mechanisms. We also intend to use\nfuzzy logic in tandem with deep neural networks to perform the missing data\nimputation tasks, as well as different building blocks for the deep neural\nnetworks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to\ntest our hypothesis. The motivation behind this article is the need for missing\ndata imputation techniques that lead to better imputed values than existing\nmethods with higher accuracies and lower errors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 10:39:59 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Leke", "Collins", ""], ["Marwala", "Tshilidzi", ""], ["Paul", "Satyakama", ""]]}, {"id": "1512.01400", "submitter": "Haibing Wu", "authors": "Haibing Wu and Xiaodong Gu", "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "comments": "The journal version of this paper [arXiv:1512.00242] has been\n  published in Neural Networks,\n  http://www.sciencedirect.com/science/article/pii/S0893608015001446", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dropout has seen increasing use in deep learning. For deep\nconvolutional neural networks, dropout is known to work well in fully-connected\nlayers. However, its effect in pooling layers is still not clear. This paper\ndemonstrates that max-pooling dropout is equivalent to randomly picking\nactivation based on a multinomial distribution at training time. In light of\nthis insight, we advocate employing our proposed probabilistic weighted\npooling, instead of commonly used max-pooling, to act as model averaging at\ntest time. Empirical evidence validates the superiority of probabilistic\nweighted pooling. We also compare max-pooling dropout and stochastic pooling,\nboth of which introduce stochasticity based on multinomial distributions at\npooling stage.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 13:18:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1512.01537", "submitter": "Elliot Meyerson", "authors": "Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, Risto\n  Miikkulainen", "title": "Reuse of Neural Modules for General Video Game Playing", "comments": "Accepted at AAAI 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach to knowledge transfer is introduced in which an agent\ncontrolled by a neural network adapts how it reuses existing networks as it\nlearns in a new domain. Networks trained for a new domain can improve their\nperformance by routing activation selectively through previously learned neural\nstructure, regardless of how or for what it was learned. A neuroevolution\nimplementation of this approach is presented with application to\nhigh-dimensional sequential decision-making domains. This approach is more\ngeneral than previous approaches to neural transfer for reinforcement learning.\nIt is domain-agnostic and requires no prior assumptions about the nature of\ntask relatedness or mappings. The method is analyzed in a stochastic version of\nthe Arcade Learning Environment, demonstrating that it improves performance in\nsome of the more complex Atari 2600 games, and that the success of transfer can\nbe predicted based on a high-level characterization of game dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 20:43:30 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Braylan", "Alexander", ""], ["Hollenbeck", "Mark", ""], ["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1512.01596", "submitter": "Volodymyr Turchenko", "authors": "Volodymyr Turchenko, Artur Luczak", "title": "Creation of a Deep Convolutional Auto-Encoder in Caffe", "comments": "9 pages, 7 figures, 5 tables, 34 references in the list; Added\n  references, corrected Table 3, changed several paragraphs in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a deep (stacked) convolutional auto-encoder in the Caffe\ndeep learning framework is presented in this paper. We describe simple\nprinciples which we used to create this model in Caffe. The proposed model of\nconvolutional auto-encoder does not have pooling/unpooling layers yet. The\nresults of our experimental research show comparable accuracy of dimensionality\nreduction in comparison with a classic auto-encoder on the example of MNIST\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 23:58:47 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 01:51:14 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 03:20:41 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Turchenko", "Volodymyr", ""], ["Luczak", "Artur", ""]]}, {"id": "1512.01613", "submitter": "Fei Gao", "authors": "Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li", "title": "A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony\n  Algorithm", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ramsey number is of vital importance in Ramsey's theorem. This paper\nproposed a novel methodology for constructing Ramsey graphs about R(3,10),\nwhich uses Artificial Bee Colony optimization(ABC) to raise the lower bound of\nRamsey number R(3,10). The r(3,10)-graph contains two limitations, that is,\nneither complete graphs of order 3 nor independent sets of order 10. To resolve\nthese limitations, a special mathematical model is put in the paradigm to\nconvert the problems into discrete optimization whose smaller minimizers are\ncorrespondent to bigger lower bound as approximation of inf R(3,10). To\ndemonstrate the potential of the proposed method, simulations are done to to\nminimize the amount of these two types of graphs. For the first time, four\nr(3,9,39) graphs with best approximation for inf R(3,10) are reported in\nsimulations to support the current lower bound for R(3,10). The experiments'\nresults show that the proposed paradigm for Ramsey number's calculation driven\nby ABC is a successful method with the advantages of high precision and\nrobustness.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 02:42:28 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:03:26 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Mao", "Wei-Hao", ""], ["Gao", "Fei", ""], ["Dong", "Yi-Jin", ""], ["Li", "Wen-Ming", ""]]}, {"id": "1512.01712", "submitter": "Konstantin Lopyrev", "authors": "Konstantin Lopyrev", "title": "Generating News Headlines with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an application of an encoder-decoder recurrent neural network\nwith LSTM units and attention to generating headlines from the text of news\narticles. We find that the model is quite effective at concisely paraphrasing\nnews articles. Furthermore, we study how the neural network decides which input\nwords to pay attention to, and specifically we identify the function of the\ndifferent neurons in a simplified attention mechanism. Interestingly, our\nsimplified attention mechanism performs better that the more complex attention\nmechanism on a held out set of articles.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 23:41:22 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Lopyrev", "Konstantin", ""]]}, {"id": "1512.01834", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "title": "Linear Readout of Object Manifolds", "comments": "5 pages, 3 figures, accepted in Physical Review E as Rapid\n  Communication on 14th May. 2016", "journal-ref": "Phys. Rev. E 93, 060301 (R) (2016)", "doi": "10.1103/PhysRevE.93.060301", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects are represented in sensory systems by continuous manifolds due to\nsensitivity of neuronal responses to changes in physical features such as\nlocation, orientation, and intensity. What makes certain sensory\nrepresentations better suited for invariant decoding of objects by downstream\nnetworks? We present a theory that characterizes the ability of a linear\nreadout network, the perceptron, to classify objects from variable neural\nresponses. We show how the readout perceptron capacity depends on the\ndimensionality, size, and shape of the object manifolds in its input neural\nrepresentation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 21:00:02 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 05:50:13 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chung", "SueYeon", ""], ["Lee", "Daniel D.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1512.02047", "submitter": "Anton Eremeev", "authors": "Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre", "title": "Level-Based Analysis of Genetic Algorithms for Combinatorial\n  Optimization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to upper bounds on run-time of Non-Elitist Genetic\nAlgorithms until some target subset of solutions is visited for the first time.\nIn particular, we consider the sets of optimal solutions and the sets of local\noptima as the target subsets. Previously known upper bounds are improved by\nmeans of drift analysis. Finally, we propose conditions ensuring that a\nNon-Elitist Genetic Algorithm efficiently finds approximate solutions with\nconstant approximation ratio on the class of combinatorial optimization\nproblems with guaranteed local optima (GLO).\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 14:03:43 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 03:22:10 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Dang", "Duc-Cuong", ""], ["Eremeev", "Anton V.", ""], ["Lehre", "Per Kristian", ""]]}, {"id": "1512.02100", "submitter": "Tim Taylor", "authors": "Tim Taylor, Alan Dorin, Kevin Korb", "title": "Digital Genesis: Computers, Evolution and Artificial Life", "comments": "Extended abstract of talk presented at the 7th Munich-Sydney-Tilburg\n  Philosophy of Science Conference: Evolutionary Thinking, University of\n  Sydney, 20-22 March 2014. Presentation slides from talk available at\n  http://www.tim-taylor.com/papers/digital-genesis-presentation.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of evolution in the digital realm, with the goal of creating\nartificial intelligence and artificial life, has a history as long as that of\nthe digital computer itself. We illustrate the intertwined history of these\nideas, starting with the early theoretical work of John von Neumann and the\npioneering experimental work of Nils Aall Barricelli. We argue that\nevolutionary thinking and artificial life will continue to play an integral\nrole in the future development of the digital world.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:53:48 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Taylor", "Tim", ""], ["Dorin", "Alan", ""], ["Korb", "Kevin", ""]]}, {"id": "1512.02497", "submitter": "Francisco Massa", "authors": "Francisco Massa, Bryan Russell, Mathieu Aubry", "title": "Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end convolutional neural network (CNN) for\n2D-3D exemplar detection. We demonstrate that the ability to adapt the features\nof natural images to better align with those of CAD rendered views is critical\nto the success of our technique. We show that the adaptation can be learned by\ncompositing rendered views of textured object models on natural images. Our\napproach can be naturally incorporated into a CNN detection pipeline and\nextends the accuracy and speed benefits from recent advances in deep learning\nto 2D-3D exemplar detection. We applied our method to two tasks: instance\ndetection, where we evaluated on the IKEA dataset, and object category\ndetection, where we out-perform Aubry et al. for \"chair\" detection on a subset\nof the Pascal VOC dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 15:04:46 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 13:14:22 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Massa", "Francisco", ""], ["Russell", "Bryan", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1512.02693", "submitter": "John Jameson", "authors": "John W. Jameson", "title": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present incremental learning methods are limited in the ability to achieve\nreliable credit assignment over a large number time steps (or events). However,\nthis situation is typical for cases where the dynamical system to be controlled\nrequires relatively frequent control updates in order to maintain stability or\nrobustness yet has some action-consequences which must be established over\nrelatively long periods of time. To address this problem, the learning\ncapabilities of a control architecture comprised of two Backpropagated Adaptive\nCritics (BACs) in a two-level hierarchy with continuous actions are explored.\nThe high-level BAC updates less frequently than the low-level BAC and controls\nthe latter to some degree. The response of the low-level to high-level signals\ncan either be determined a priori or it can emerge during learning. A general\napproach called Response Induction Learning is introduced to address the latter\ncase.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 23:11:54 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Jameson", "John W.", ""]]}, {"id": "1512.02767", "submitter": "Michael Maire", "authors": "Michael Maire, Takuya Narihira, Stella X. Yu", "title": "Affinity CNN: Learning Pixel-Centric Pairwise Relations for\n  Figure/Ground Embedding", "comments": "minor updates; extended version of CVPR 2016 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding provides a framework for solving perceptual organization\nproblems, including image segmentation and figure/ground organization. From an\naffinity matrix describing pairwise relationships between pixels, it clusters\npixels into regions, and, using a complex-valued extension, orders pixels\naccording to layer. We train a convolutional neural network (CNN) to directly\npredict the pairwise relationships that define this affinity matrix. Spectral\nembedding then resolves these predictions into a globally-consistent\nsegmentation and figure/ground organization of the scene. Experiments\ndemonstrate significant benefit to this direct coupling compared to prior works\nwhich use explicit intermediate stages, such as edge detection, on the pathway\nfrom image to affinities. Our results suggest spectral embedding as a powerful\nalternative to the conditional random field (CRF)-based globalization schemes\ntypically coupled to deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 06:45:23 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 22:03:38 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Maire", "Michael", ""], ["Narihira", "Takuya", ""], ["Yu", "Stella X.", ""]]}, {"id": "1512.02930", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa, Giacomo Indiveri", "title": "Stochastic Interpretation of Quasi-periodic Event-based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many networks used in machine learning and as models of biological neural\nnetworks make use of stochastic neurons or neuron-like units. We show that\nstochastic artificial neurons can be realized on silicon chips by exploiting\nthe quasi-periodic behavior of mismatched analog oscillators to approximate the\nneuron's stochastic activation function. We represent neurons by finite state\nmachines (FSMs) that communicate using digital events and whose transitions are\nevent-triggered. The event generation times of each neuron are controlled by an\nanalog oscillator internal to that neuron/FSM and the frequencies of the\noscillators in different FSMs are incommensurable. We show that within this\nquasi-periodic system, the transition graph of a FSM can be interpreted as the\ntransition graph of a Markov chain and we show that by using different FSMs, we\ncan obtain approximations of different stochastic activation functions. We\ninvestigate the quality of the stochastic interpretation of such a\ndeterministic system and we use the system to realize and sample from a\nrestricted Boltzmann machine. We implemented the quasi-periodic event-based\nsystem on a custom silicon chip and we show that the chip behavior can be used\nto closely approximate a stochastic sampling task.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 16:31:59 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Mostafa", "Hesham", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1512.03025", "submitter": "Ilia Zintchenko", "authors": "Ilia Zintchenko, Matthew Hastings, Nathan Wiebe, Ethan Brown, Matthias\n  Troyer", "title": "Partial Reinitialisation for Optimisers", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic optimisers which search for an optimal configuration of variables\nrelative to an objective function often get stuck in local optima where the\nalgorithm is unable to find further improvement. The standard approach to\ncircumvent this problem involves periodically restarting the algorithm from\nrandom initial configurations when no further improvement can be found. We\npropose a method of partial reinitialization, whereby, in an attempt to find a\nbetter solution, only sub-sets of variables are re-initialised rather than the\nwhole configuration. Much of the information gained from previous runs is hence\nretained. This leads to significant improvements in the quality of the solution\nfound in a given time for a variety of optimisation problems in machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 20:08:43 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zintchenko", "Ilia", ""], ["Hastings", "Matthew", ""], ["Wiebe", "Nathan", ""], ["Brown", "Ethan", ""], ["Troyer", "Matthias", ""]]}, {"id": "1512.03466", "submitter": "Roberto Santana", "authors": "Roberto Santana, Alexander Mendiburu, Jose A. Lozano", "title": "Computing factorized approximations of Pareto-fronts using\n  mNM-landscapes and Boltzmann distributions", "comments": "Accepted for CAEPIA-2015 conference, Albacete, Spain. 11 pages, 3\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NM-landscapes have been recently introduced as a class of tunable rugged\nmodels. They are a subset of the general interaction models where all the\ninteractions are of order less or equal $M$. The Boltzmann distribution has\nbeen extensively applied in single-objective evolutionary algorithms to\nimplement selection and study the theoretical properties of model-building\nalgorithms. In this paper we propose the combination of the multi-objective\nNM-landscape model and the Boltzmann distribution to obtain Pareto-front\napproximations. We investigate the joint effect of the parameters of the\nNM-landscapes and the probabilistic factorizations in the shape of the Pareto\nfront approximations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 22:21:03 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Santana", "Roberto", ""], ["Mendiburu", "Alexander", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1512.03549", "submitter": "Pranjal Singh", "authors": "Pranjal Singh, Amitabha Mukerjee", "title": "Words are not Equal: Graded Weighting Model for building Composite\n  Document Vectors", "comments": "10 Pages, 2 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of distributional semantics, composing phrases from word\nvectors remains an important challenge. Several methods have been tried for\nbenchmark tasks such as sentiment classification, including word vector\naveraging, matrix-vector approaches based on parsing, and on-the-fly learning\nof paragraph vectors. Most models usually omit stop words from the composition.\nInstead of such an yes-no decision, we consider several graded schemes where\nwords are weighted according to their discriminatory relevance with respect to\nits use in the document (e.g., idf). Some of these methods (particularly\ntf-idf) are seen to result in a significant improvement in performance over\nprior state of the art. Further, combining such approaches into an ensemble\nbased on alternate classifiers such as the RNN model, results in an 1.6%\nperformance improvement on the standard IMDB movie review dataset, and a 7.01%\nimprovement on Amazon product reviews. Since these are language free models and\ncan be obtained in an unsupervised manner, they are of interest also for\nunder-resourced languages such as Hindi as well and many more languages. We\ndemonstrate the language free aspects by showing a gain of 12% for two review\ndatasets over earlier results, and also release a new larger dataset for future\ntesting (Singh,2015).\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 08:44:45 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Singh", "Pranjal", ""], ["Mukerjee", "Amitabha", ""]]}, {"id": "1512.03850", "submitter": "Junghyo Jo", "authors": "Marissa Pastor and Juyong Song and Danh-Tai Hoang and Junghyo Jo", "title": "Minimal Perceptrons for Memorizing Complex Patterns", "comments": "14 pages, 5 figures", "journal-ref": "Physica A 462:31-37 (2016)", "doi": "10.1016/j.physa.2016.06.025", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedforward neural networks have been investigated to understand learning and\nmemory, as well as applied to numerous practical problems in pattern\nclassification. It is a rule of thumb that more complex tasks require larger\nnetworks. However, the design of optimal network architectures for specific\ntasks is still an unsolved fundamental problem. In this study, we consider\nthree-layered neural networks for memorizing binary patterns. We developed a\nnew complexity measure of binary patterns, and estimated the minimal network\nsize for memorizing them as a function of their complexity. We formulated the\nminimal network size for regular, random, and complex patterns. In particular,\nthe minimal size for complex patterns, which are neither ordered nor\ndisordered, was predicted by measuring their Hamming distances from known\nordered patterns. Our predictions agreed with simulations based on the\nback-propagation algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 00:08:27 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Pastor", "Marissa", ""], ["Song", "Juyong", ""], ["Hoang", "Danh-Tai", ""], ["Jo", "Junghyo", ""]]}, {"id": "1512.03965", "submitter": "Ohad Shamir", "authors": "Ronen Eldan and Ohad Shamir", "title": "The Power of Depth for Feedforward Neural Networks", "comments": "Accepted to COLT 2016; Fixed a bug in the proof of claim 2 (now\n  requiring the mild assumption that the activations are polynomially bounded);\n  Other minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is a simple (approximately radial) function on $\\reals^d$,\nexpressible by a small 3-layer feedforward neural networks, which cannot be\napproximated by any 2-layer network, to more than a certain constant accuracy,\nunless its width is exponential in the dimension. The result holds for\nvirtually all known activation functions, including rectified linear units,\nsigmoids and thresholds, and formally demonstrates that depth -- even if\nincreased by 1 -- can be exponentially more valuable than width for standard\nfeedforward neural networks. Moreover, compared to related results in the\ncontext of Boolean functions, our result requires fewer assumptions, and the\nproof techniques and construction are very different.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 21:41:24 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 16:21:24 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2015 03:47:28 GMT"}, {"version": "v4", "created": "Mon, 9 May 2016 02:16:54 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Eldan", "Ronen", ""], ["Shamir", "Ohad", ""]]}, {"id": "1512.04280", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech\n  Recognition", "comments": "5 pages, 3 figures, fixed typo, accepted by Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and application\ndomains. However, compared to the conventional Gaussian mixture models,\nDNN-based acoustic models usually have much larger number of model parameters,\nmaking it challenging for their applications in resource constrained platforms,\ne.g., mobile devices. In this paper, we study the application of the recently\nproposed highway network to train small-footprint DNNs, which are {\\it thinner}\nand {\\it deeper}, and have significantly smaller number of model parameters\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\nspeech transcription corpus which has around 70 hours of audio data. The\nhighway neural networks constantly outperformed their plain DNN counterparts,\nand the number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 12:29:32 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 12:14:06 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 10:30:54 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 15:17:27 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1512.04295", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2592330", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing number of computer vision and image/video processing\nchallenges are being approached using deep convolutional neural networks,\nobtaining state-of-the-art results in object recognition and detection,\nsemantic segmentation, action recognition, optical flow and superresolution.\nHardware acceleration of these algorithms is essential to adopt these\nimprovements in embedded and mobile computer vision systems. We present a new\narchitecture, design and implementation as well as the first reported silicon\nmeasurements of such an accelerator, outperforming previous work in terms of\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\nthe first architecture scalable to TOp/s performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 13:06:43 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 22:56:41 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1512.04509", "submitter": "Kumar Eswaran Dr.", "authors": "K.Eswaran and K.Damodhar Rao", "title": "On non-iterative training of a neural classifier", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently an algorithm, was discovered, which separates points in n-dimension\nby planes in such a manner that no two points are left un-separated by at least\none plane{[}1-3{]}. By using this new algorithm we show that there are two ways\nof classification by a neural network, for a large dimension feature space,\nboth of which are non-iterative and deterministic. To demonstrate the power of\nboth these methods we apply them exhaustively to the classical pattern\nrecognition problem: The Fisher-Anderson's, IRIS flower data set and present\nthe results.\n  It is expected these methods will now be widely used for the training of\nneural networks for Deep Learning not only because of their non-iterative and\ndeterministic nature but also because of their efficiency and speed and will\nsupersede other classification methods which are iterative in nature and rely\non error minimization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 20:44:12 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 04:32:01 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Eswaran", "K.", ""], ["Rao", "K. Damodhar", ""]]}, {"id": "1512.04639", "submitter": "Michael Bukatin", "authors": "Michael Bukatin and Steve Matthews", "title": "Linear Models of Computation and Program Learning", "comments": "13 pages; September 3, 2015 version; to appear in the Proceedings of\n  GCAI 2015, Tbilisi, Georgia, Oct.16-18, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two classes of computations which admit taking linear\ncombinations of execution runs: probabilistic sampling and generalized\nanimation. We argue that the task of program learning should be more tractable\nfor these architectures than for conventional deterministic programs. We look\nat the recent advances in the \"sampling the samplers\" paradigm in higher-order\nprobabilistic programming. We also discuss connections between partial\ninconsistency, non-monotonic inference, and vector semantics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 04:13:51 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Bukatin", "Michael", ""], ["Matthews", "Steve", ""]]}, {"id": "1512.05055", "submitter": "Yu Chen", "authors": "Yu Chen and Xiufen Zou", "title": "Inferring Gene Regulatory Network Using An Evolutionary Multi-Objective\n  Method", "comments": "8pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of gene regulatory networks (GRNs) based on experimental data is a\nchallenging task in bioinformatics. In this paper, we present a bi-objective\nminimization model (BoMM) for inference of GRNs, where one objective is the\nfitting error of derivatives, and the other is the number of connections in the\nnetwork. To solve the BoMM efficiently, we propose a multi-objective\nevolutionary algorithm (MOEA), and utilize the separable parameter estimation\nmethod (SPEM) decoupling the ordinary differential equation (ODE) system. Then,\nthe Akaike Information Criterion (AIC) is employed to select one inference\nresult from the obtained Pareto set. Taking the S-system as the investigated\nGRN model, our method can properly identify the topologies and parameter values\nof benchmark systems. There is no need to preset problem-dependent parameter\nvalues to obtain appropriate results, and thus, our method could be applicable\nto inference of various GRNs models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 05:23:03 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Chen", "Yu", ""], ["Zou", "Xiufen", ""]]}, {"id": "1512.05245", "submitter": "Fergal Byrne", "authors": "Fergal Byrne", "title": "Symphony from Synapses: Neocortex as a Universal Dynamical Systems\n  Modeller using Hierarchical Temporal Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Reverse engineering the brain is proving difficult, perhaps impossible. While\nmany believe that this is just a matter of time and effort, a different\napproach might help. Here, we describe a very simple idea which explains the\npower of the brain as well as its structure, exploiting complex dynamics rather\nthan abstracting it away. Just as a Turing Machine is a Universal Digital\nComputer operating in a world of symbols, we propose that the brain is a\nUniversal Dynamical Systems Modeller, evolved bottom-up (itself using nested\nnetworks of interconnected, self-organised dynamical systems) to prosper in a\nworld of dynamical systems.\n  Recent progress in Applied Mathematics has produced startling evidence of\nwhat happens when abstract Dynamical Systems interact. Key latent information\ndescribing system A can be extracted by system B from very simple signals, and\nsignals can be used by one system to control and manipulate others. Using these\nfacts, we show how a region of the neocortex uses its dynamics to intrinsically\n\"compute\" about the external and internal world.\n  Building on an existing \"static\" model of cortical computation (Hawkins'\nHierarchical Temporal Memory - HTM), we describe how a region of neocortex can\nbe viewed as a network of components which together form a Dynamical Systems\nmodelling module, connected via sensory and motor pathways to the external\nworld, and forming part of a larger dynamical network in the brain.\n  Empirical modelling and simulations of Dynamical HTM are possible with simple\nextensions and combinations of currently existing open source software. We list\na number of relevant projects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:58:06 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Byrne", "Fergal", ""]]}, {"id": "1512.05449", "submitter": "Wei Du", "authors": "Wei Du, Sunney Yung Sun Leung, Yang Tang, Athanasios V. Vasilakos", "title": "Differential Evolution with Event-Triggered Impulsive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential evolution (DE) is a simple but powerful evolutionary algorithm,\nwhich has been widely and successfully used in various areas. In this paper, an\nevent-triggered impulsive control scheme (ETI) is introduced to improve the\nperformance of DE. Impulsive control, the concept of which derives from control\ntheory, aims at regulating the states of a network by instantly adjusting the\nstates of a fraction of nodes at certain instants, and these instants are\ndetermined by event-triggered mechanism (ETM). By introducing impulsive control\nand ETM into DE, we hope to change the search performance of the population in\na positive way after revising the positions of some individuals at certain\nmoments. At the end of each generation, the impulsive control operation is\ntriggered when the update rate of the population declines or equals to zero. In\ndetail, inspired by the concepts of impulsive control, two types of impulses\nare presented within the framework of DE in this paper: stabilizing impulses\nand destabilizing impulses. Stabilizing impulses help the individuals with\nlower rankings instantly move to a desired state determined by the individuals\nwith better fitness values. Destabilizing impulses randomly alter the positions\nof inferior individuals within the range of the current population. By means of\nintelligently modifying the positions of a part of individuals with these two\nkinds of impulses, both exploitation and exploration abilities of the whole\npopulation can be meliorated. In addition, the proposed ETI is flexible to be\nincorporated into several state-of-the-art DE variants. Experimental results\nover the CEC 2014 benchmark functions exhibit that the developed scheme is\nsimple yet effective, which significantly improves the performance of the\nconsidered DE algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 03:07:12 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 07:43:51 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Du", "Wei", ""], ["Leung", "Sunney Yung Sun", ""], ["Tang", "Yang", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "1512.05463", "submitter": "Subutai Ahmad", "authors": "Yuwei Cui, Subutai Ahmad, and Jeff Hawkins", "title": "Continuous online sequence learning with an unsupervised neural network\n  model", "comments": "Preprint of journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to recognize and predict temporal sequences of sensory inputs is\nvital for survival in natural environments. Based on many known properties of\ncortical neurons, hierarchical temporal memory (HTM) sequence memory is\nrecently proposed as a theoretical framework for sequence learning in the\ncortex. In this paper, we analyze properties of HTM sequence memory and apply\nit to sequence learning and prediction problems with streaming data. We show\nthe model is able to continuously learn a large number of variable-order\ntemporal sequences using an unsupervised Hebbian-like learning rule. The sparse\ntemporal codes formed by the model can robustly handle branching temporal\nsequences by maintaining multiple predictions until there is sufficient\ndisambiguating evidence. We compare the HTM sequence memory with other sequence\nlearning algorithms, including statistical methods: autoregressive integrated\nmoving average (ARIMA), feedforward neural networks: online sequential extreme\nlearning machine (ELM), and recurrent neural networks: long short-term memory\n(LSTM) and echo-state networks (ESN), on sequence prediction problems with both\nartificial and real-world data. The HTM model achieves comparable accuracy to\nother state-of-the-art algorithms. The model also exhibits properties that are\ncritical for sequence learning, including continuous online learning, the\nability to handle multiple predictions and branching sequences with high order\nstatistics, robustness to sensor noise and fault tolerance, and good\nperformance without task-specific hyper- parameters tuning. Therefore the HTM\nsequence memory not only advances our understanding of how the brain may solve\nthe sequence learning problem, but is also applicable to a wide range of\nreal-world problems such as discrete and continuous sequence prediction,\nanomaly detection, and sequence classification.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 04:45:31 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 21:00:52 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Cui", "Yuwei", ""], ["Ahmad", "Subutai", ""], ["Hawkins", "Jeff", ""]]}, {"id": "1512.05509", "submitter": "Denis Steckelmacher", "authors": "Denis Steckelmacher and Peter Vrancx", "title": "An Empirical Comparison of Neural Architectures for Reinforcement\n  Learning in Partially Observable Environments", "comments": "Presented at the 27th Benelux Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the performance of fitted neural Q iteration for\nreinforcement learning in several partially observable environments, using\nthree recurrent neural network architectures: Long Short-Term Memory, Gated\nRecurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of\nseveral thousands candidate architectures. A variant of fitted Q iteration,\nbased on Advantage values instead of Q values, is also explored. The results\nshow that GRU performs significantly better than LSTM and MUT1 for most of the\nproblems considered, requiring less training episodes and less CPU time before\nlearning a very good policy. Advantage learning also tends to produce better\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 09:45:51 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Steckelmacher", "Denis", ""], ["Vrancx", "Peter", ""]]}, {"id": "1512.05702", "submitter": "Adam Trischler", "authors": "Adam Trischler, Gabriele MT D'Eleuterio", "title": "Synthesis of recurrent neural networks for dynamical system simulation", "comments": "To be published in Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review several of the most widely used techniques for training recurrent\nneural networks to approximate dynamical systems, then describe a novel\nalgorithm for this task. The algorithm is based on an earlier theoretical\nresult that guarantees the quality of the network approximation. We show that a\nfeedforward neural network can be trained on the vector field representation of\na given dynamical system using backpropagation, then recast, using matrix\nmanipulations, as a recurrent network that replicates the original system's\ndynamics. After detailing this algorithm and its relation to earlier\napproaches, we present numerical examples that demonstrate its capabilities.\nOne of the distinguishing features of our approach is that both the original\ndynamical systems and the recurrent networks that simulate them operate in\ncontinuous time.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 18:08:33 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 20:18:03 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Trischler", "Adam", ""], ["D'Eleuterio", "Gabriele MT", ""]]}, {"id": "1512.05726", "submitter": "Tao Lei", "authors": "Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi Jaakkola, Katerina\n  Tymoshenko, Alessandro Moschitti, Lluis Marquez", "title": "Semi-supervised Question Retrieval with Gated Convolutions", "comments": "NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering forums are rapidly growing in size with no effective\nautomated ability to refer to and reuse answers already available for previous\nposted questions. In this paper, we develop a methodology for finding\nsemantically related questions. The task is difficult since 1) key pieces of\ninformation are often buried in extraneous details in the question body and 2)\navailable annotations on similar questions are scarce and fragmented. We design\na recurrent and convolutional model (gated convolution) to effectively map\nquestions to their semantic representations. The models are pre-trained within\nan encoder-decoder framework (from body to title) on the basis of the entire\nraw corpus, and fine-tuned discriminatively from limited annotations. Our\nevaluation demonstrates that our model yields substantial gains over a standard\nIR baseline and various neural network architectures (including CNNs, LSTMs and\nGRUs).\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:14:20 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 00:29:15 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Lei", "Tao", ""], ["Joshi", "Hrishikesh", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""], ["Tymoshenko", "Katerina", ""], ["Moschitti", "Alessandro", ""], ["Marquez", "Lluis", ""]]}, {"id": "1512.05986", "submitter": "Vlado Menkovski", "authors": "Vlado Menkovski, Zharko Aleksovski, Axel Saalbach, Hannes Nickisch", "title": "Can Pretrained Neural Networks Detect Anatomy?", "comments": "NIPS 2015 Workshop on Machine Learning in Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks demonstrated outstanding empirical results in\ncomputer vision and speech recognition tasks where labeled training data is\nabundant. In medical imaging, there is a huge variety of possible imaging\nmodalities and contrasts, where annotated data is usually very scarce. We\npresent two approaches to deal with this challenge. A network pretrained in a\ndifferent domain with abundant data is used as a feature extractor, while a\nsubsequent classifier is trained on a small target dataset; and a deep\narchitecture trained with heavy augmentation and equipped with sophisticated\nregularization methods. We test the approaches on a corpus of X-ray images to\ndesign an anatomy detection system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 15:16:31 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Menkovski", "Vlado", ""], ["Aleksovski", "Zharko", ""], ["Saalbach", "Axel", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1512.06612", "submitter": "Lili Mou", "authors": "Lili Mou, Rui Yan, Ge Li, Lu Zhang, Zhi Jin", "title": "Backward and Forward Language Modeling for Constrained Sentence\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent language models, especially those based on recurrent neural networks\n(RNNs), make it possible to generate natural language from a learned\nprobability. Language generation has wide applications including machine\ntranslation, summarization, question answering, conversation systems, etc.\nExisting methods typically learn a joint probability of words conditioned on\nadditional information, which is (either statically or dynamically) fed to\nRNN's hidden layer. In many applications, we are likely to impose hard\nconstraints on the generated texts, i.e., a particular word must appear in the\nsentence. Unfortunately, existing approaches could not solve this problem. In\nthis paper, we propose a novel backward and forward language model. Provided a\nspecific word, we use RNNs to generate previous words and future words, either\nsimultaneously or asynchronously, resulting in two model variants. In this way,\nthe given word could appear at any position in the sentence. Experimental\nresults show that the generated texts are comparable to sequential LMs in\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 13:07:31 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 20:15:44 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1512.06757", "submitter": "Jiaji Huang", "authors": "Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro", "title": "GraphConnect: A Regularization Framework for Neural Networks", "comments": "Theorems need more validation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proved very successful in domains where large\ntraining sets are available, but when the number of training samples is small,\ntheir performance suffers from overfitting. Prior methods of reducing\noverfitting such as weight decay, Dropout and DropConnect are data-independent.\nThis paper proposes a new method, GraphConnect, that is data-dependent, and is\nmotivated by the observation that data of interest lie close to a manifold. The\nnew method encourages the relationships between the learned decisions to\nresemble a graph representing the manifold structure. Essentially GraphConnect\nis designed to learn attributes that are present in data samples in contrast to\nweight decay, Dropout and DropConnect which are simply designed to make it more\ndifficult to fit to random error or noise. Empirical Rademacher complexity is\nused to connect the generalization error of the neural network to spectral\nproperties of the graph learned from the input data. This framework is used to\nshow that GraphConnect is superior to weight decay. Experimental results on\nseveral benchmark datasets validate the theoretical analysis, and show that\nwhen the number of training samples is small, GraphConnect is able to\nsignificantly improve performance over weight decay.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 18:42:45 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 03:21:15 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Huang", "Jiaji", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1512.07108", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy,\n  Bing Shuai, Ting Liu, Xingxing Wang, Li Wang, Gang Wang, Jianfei Cai, Tsuhan\n  Chen", "title": "Recent Advances in Convolutional Neural Networks", "comments": "Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, deep learning has led to very good performance on a\nvariety of problems, such as visual recognition, speech recognition and natural\nlanguage processing. Among different types of deep neural networks,\nconvolutional neural networks have been most extensively studied. Leveraging on\nthe rapid growth in the amount of the annotated data and the great improvements\nin the strengths of graphics processor units, the research on convolutional\nneural networks has been emerged swiftly and achieved state-of-the-art results\non various tasks. In this paper, we provide a broad survey of the recent\nadvances in convolutional neural networks. We detailize the improvements of CNN\non different aspects, including layer design, activation function, loss\nfunction, regularization, optimization and fast computation. Besides, we also\nintroduce various applications of convolutional neural networks in computer\nvision, speech and natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 14:54:34 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 11:39:16 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 01:54:59 GMT"}, {"version": "v4", "created": "Sat, 6 Aug 2016 12:38:35 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 05:45:53 GMT"}, {"version": "v6", "created": "Thu, 19 Oct 2017 16:34:35 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Wang", "Zhenhua", ""], ["Kuen", "Jason", ""], ["Ma", "Lianyang", ""], ["Shahroudy", "Amir", ""], ["Shuai", "Bing", ""], ["Liu", "Ting", ""], ["Wang", "Xingxing", ""], ["Wang", "Li", ""], ["Wang", "Gang", ""], ["Cai", "Jianfei", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1512.07450", "submitter": "Hector Zenil", "authors": "Alyssa Adams, Hector Zenil, Eduardo Hermo Reyes, Joost Joosten", "title": "Interacting Behavior and Emerging Complexity", "comments": "11 pages, 5 figures (in this version a minor typo corrected).\n  Presented at AUTOMATA 2015 forthcoming in journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC nlin.CG q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we quantify the change of complexity throughout evolutionary processes?\nWe attempt to address this question through an empirical approach. In very\ngeneral terms, we simulate two simple organisms on a computer that compete over\nlimited available resources. We implement Global Rules that determine the\ninteraction between two Elementary Cellular Automata on the same grid. Global\nRules change the complexity of the state evolution output which suggests that\nsome complexity is intrinsic to the interaction rules themselves. The largest\nincreases in complexity occurred when the interacting elementary rules had very\nlittle complexity, suggesting that they are able to accept complexity through\ninteraction only. We also found that some Class 3 or 4 CA rules are more\nfragile than others to Global Rules, while others are more robust, hence\nsuggesting some intrinsic properties of the rules independent of the Global\nRule choice. We provide statistical mappings of Elementary Cellular Automata\nexposed to Global Rules and different initial conditions onto different\ncomplexity classes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:29:01 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2015 01:13:46 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 18:21:46 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Adams", "Alyssa", ""], ["Zenil", "Hector", ""], ["Reyes", "Eduardo Hermo", ""], ["Joosten", "Joost", ""]]}, {"id": "1512.07679", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold and Richard Evans and Hado van Hasselt and Peter\n  Sunehag and Timothy Lillicrap and Jonathan Hunt and Timothy Mann and\n  Theophane Weber and Thomas Degris and Ben Coppin", "title": "Deep Reinforcement Learning in Large Discrete Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to reason in an environment with a large number of discrete\nactions is essential to bringing reinforcement learning to a larger class of\nproblems. Recommender systems, industrial plants and language models are only\nsome of the many real-world tasks involving large numbers of discrete actions\nfor which current methods are difficult or even often impossible to apply. An\nability to generalize over the set of actions as well as sub-linear complexity\nrelative to the size of the set are both necessary to handle such tasks.\nCurrent approaches are not able to provide both of these, which motivates the\nwork in this paper. Our proposed approach leverages prior information about the\nactions to embed them in a continuous space upon which it can generalize.\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\nlookup complexity relative to the number of actions, which is necessary for\ntime-wise tractable training. This combined approach allows reinforcement\nlearning methods to be applied to large-scale learning problems previously\nintractable with current methods. We demonstrate our algorithm's abilities on a\nseries of tasks having up to one million actions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:31:40 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 11:27:36 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Evans", "Richard", ""], ["van Hasselt", "Hado", ""], ["Sunehag", "Peter", ""], ["Lillicrap", "Timothy", ""], ["Hunt", "Jonathan", ""], ["Mann", "Timothy", ""], ["Weber", "Theophane", ""], ["Degris", "Thomas", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.07783", "submitter": "Aakash Patil", "authors": "Aakash Patil, Shanlan Shen, Enyi Yao and Arindam Basu", "title": "Hardware Architecture for Large Parallel Array of Random Feature\n  Extractors applied to Image Recognition", "comments": "Submitted for ELM special issue in Neurocomputing, 18 pages, 7\n  figures, 3 tables. ACM class: \"Hardware/Emerging Technologies\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a low-power and compact hardware implementation of Random\nFeature Extractor (RFE) core. With complex tasks like Image Recognition\nrequiring a large set of features, we show how weight reuse technique can allow\nto virtually expand the random features available from RFE core. Further, we\nshow how to avoid computation cost wasted for propagating \"incognizant\" or\nredundant random features. For proof of concept, we validated our approach by\nusing our RFE core as the first stage of Extreme Learning Machine (ELM)--a two\nlayer neural network--and were able to achieve $>97\\%$ accuracy on MNIST\ndatabase of handwritten digits. ELM's first stage of RFE is done on an analog\nASIC occupying $5$mm$\\times5$mm area in $0.35\\mu$m CMOS and consuming $5.95$\n$\\mu$J/classify while using $\\approx 5000$ effective hidden neurons. The ELM\nsecond stage consisting of just adders can be implemented as digital circuit\nwith estimated power consumption of $20.9$ nJ/classify. With a total energy\nconsumption of only $5.97$ $\\mu$J/classify, this low-power mixed signal ASIC\ncan act as a co-processor in portable electronic gadgets with cameras.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 10:49:28 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Patil", "Aakash", ""], ["Shen", "Shanlan", ""], ["Yao", "Enyi", ""], ["Basu", "Arindam", ""]]}, {"id": "1512.07980", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Shahryar Rahnamayan, Hamid R. Tizhoosh", "title": "Diversity Enhancement for Micro-Differential Evolution", "comments": "Developed version is submitted for review to Applied soft computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The differential evolution (DE) algorithm suffers from high computational\ntime due to slow nature of evaluation. In contrast, micro-DE (MDE) algorithms\nemploy a very small population size, which can converge faster to a reasonable\nsolution. However, these algorithms are vulnerable to a premature convergence\nas well as to high risk of stagnation. In this paper, MDE algorithm with\nvectorized random mutation factor (MDEVM) is proposed, which utilizes the small\nsize population benefit while empowers the exploration ability of mutation\nfactor through randomizing it in the decision variable level. The idea is\nsupported by analyzing mutation factor using Monte-Carlo based simulations. To\nfacilitate the usage of MDE algorithms with very-small population sizes, new\nmutation schemes for population sizes less than four are also proposed.\nFurthermore, comprehensive comparative simulations and analysis on performance\nof the MDE algorithms over various mutation schemes, population sizes, problem\ntypes (i.e. uni-modal, multi-modal, and composite), problem dimensionalities,\nand mutation factor ranges are conducted by considering population diversity\nanalysis for stagnation and trapping in local optimum situations. The studies\nare conducted on 28 benchmark functions provided for the IEEE CEC-2013\ncompetition. Experimental results demonstrate high performance and convergence\nspeed of the proposed MDEVM algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 09:12:52 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 15:57:41 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Rahnamayan", "Shahryar", ""], ["Tizhoosh", "Hamid R.", ""]]}, {"id": "1512.07982", "submitter": "Miltiadis Allamanis", "authors": "Fani A. Tzima, Miltiadis Allamanis, Alexandros Filotheou, Pericles A.\n  Mitkas", "title": "Inducing Generalized Multi-Label Rules with Learning Classifier Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi-label classification has attracted a significant body\nof research, motivated by real-life applications, such as text classification\nand medical diagnoses. Although sparsely studied in this context, Learning\nClassifier Systems are naturally well-suited to multi-label classification\nproblems, whose search space typically involves multiple highly specific\nniches. This is the motivation behind our current work that introduces a\ngeneralized multi-label rule format -- allowing for flexible label-dependency\nmodeling, with no need for explicit knowledge of which correlations to search\nfor -- and uses it as a guide for further adapting the general Michigan-style\nsupervised Learning Classifier System framework. The integration of the\naforementioned rule format and framework adaptations results in a novel\nalgorithm for multi-label classification whose behavior is studied through a\nset of properly defined artificial problems. The proposed algorithm is also\nthoroughly evaluated on a set of multi-label datasets and found competitive to\nother state-of-the-art multi-label classification methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 10:03:55 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Tzima", "Fani A.", ""], ["Allamanis", "Miltiadis", ""], ["Filotheou", "Alexandros", ""], ["Mitkas", "Pericles A.", ""]]}, {"id": "1512.08030", "submitter": "Sukru Burc Eryilmaz", "authors": "Sukru Burc Eryilmaz, Duygu Kuzum, Shimeng Yu, H.-S. Philip Wong", "title": "Device and System Level Design Considerations for\n  Analog-Non-Volatile-Memory Based Neuromorphic Architectures", "comments": "4 pages, In Electron Devices Meeting (IEDM), 2015 IEEE International\n  (pp. 4.1). IEEE. Original paper can be found here:\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7409622. Abstract can\n  be found here:\n  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7409622&refinements%3D4224410500%26filter%3DAND%28p_IS_Number%3A7409598%29", "journal-ref": "Electron Devices Meeting (IEDM), IEEE International\n  ,pp.4.1.1-4.1.4, 2015", "doi": "10.1109/IEDM.2015.7409622", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an overview of recent progress in the brain inspired\ncomputing field with a focus on implementation using emerging memories as\nelectronic synapses. Design considerations and challenges such as requirements\nand design targets on multilevel states, device variability, programming\nenergy, array-level connectivity, fan-in/fanout, wire energy, and IR drop are\npresented. Wires are increasingly important in design decisions, especially for\nlarge systems, and cycle-to-cycle variations have large impact on learning\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 19:43:05 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 18:01:43 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Eryilmaz", "Sukru Burc", ""], ["Kuzum", "Duygu", ""], ["Yu", "Shimeng", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1512.08301", "submitter": "ShiLiang Zhang", "authors": "Shiliang Zhang and Cong Liu and Hui Jiang and Si Wei and Lirong Dai\n  and Yu Hu", "title": "Feedforward Sequential Memory Networks: A New Structure to Learn\n  Long-term Dependency", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel neural network structure, namely\n\\emph{feedforward sequential memory networks (FSMN)}, to model long-term\ndependency in time series without using recurrent feedback. The proposed FSMN\nis a standard fully-connected feedforward neural network equipped with some\nlearnable memory blocks in its hidden layers. The memory blocks use a\ntapped-delay line structure to encode the long context information into a\nfixed-size representation as short-term memory mechanism. We have evaluated the\nproposed FSMNs in several standard benchmark tasks, including speech\nrecognition and language modelling. Experimental results have shown FSMNs\nsignificantly outperform the conventional recurrent neural networks (RNN),\nincluding LSTMs, in modeling sequential signals like speech or language.\nMoreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs\ndue to the inherent non-recurrent model structure.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 02:07:48 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 02:40:41 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Zhang", "Shiliang", ""], ["Liu", "Cong", ""], ["Jiang", "Hui", ""], ["Wei", "Si", ""], ["Dai", "Lirong", ""], ["Hu", "Yu", ""]]}, {"id": "1512.08457", "submitter": "Joel Leibo", "authors": "Joel Z. Leibo, Julien Cornebise, Sergio G\\'omez, Demis Hassabis", "title": "Approximate Hubel-Wiesel Modules and the Data Structures of Neural\n  Computation", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a framework for modeling the interface between\nperception and memory on the algorithmic level of analysis. It is consistent\nwith phenomena associated with many different brain regions. These include\nview-dependence (and invariance) effects in visual psychophysics and\ninferotemporal cortex physiology, as well as episodic memory recall\ninterference effects associated with the medial temporal lobe. The perspective\ndeveloped here relies on a novel interpretation of Hubel and Wiesel's\nconjecture for how receptive fields tuned to complex objects, and invariant to\ndetails, could be achieved. It complements existing accounts of two-speed\nlearning systems in neocortex and hippocampus (e.g., McClelland et al. 1995)\nwhile significantly expanding their scope to encompass a unified view of the\nentire pathway from V1 to hippocampus.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 16:53:35 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Leibo", "Joel Z.", ""], ["Cornebise", "Julien", ""], ["G\u00f3mez", "Sergio", ""], ["Hassabis", "Demis", ""]]}, {"id": "1512.08571", "submitter": "Sajid Anwar", "authors": "Sajid Anwar, Kyuyeon Hwang and Wonyong Sung", "title": "Structured Pruning of Deep Convolutional Neural Networks", "comments": "11 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time application of deep learning algorithms is often hindered by high\ncomputational complexity and frequent memory accesses. Network pruning is a\npromising technique to solve this problem. However, pruning usually results in\nirregular network connections that not only demand extra representation efforts\nbut also do not fit well on parallel computation. We introduce structured\nsparsity at various scales for convolutional neural networks, which are channel\nwise, kernel wise and intra kernel strided sparsity. This structured sparsity\nis very advantageous for direct computational resource savings on embedded\ncomputers, parallel computing environments and hardware based systems. To\ndecide the importance of network connections and paths, the proposed method\nuses a particle filtering approach. The importance weight of each particle is\nassigned by computing the misclassification rate with corresponding\nconnectivity pattern. The pruned network is re-trained to compensate for the\nlosses due to pruning. While implementing convolutions as matrix products, we\nparticularly show that intra kernel strided sparsity with a simple constraint\ncan significantly reduce the size of kernel and feature map matrices. The\npruned network is finally fixed point optimized with reduced word length\nprecision. This results in significant reduction in the total storage size\nproviding advantages for on-chip memory based implementations of deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 01:21:08 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Anwar", "Sajid", ""], ["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.08756", "submitter": "Colin Raffel", "authors": "Colin Raffel and Daniel P. W. Ellis", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simplified model of attention which is applicable to\nfeed-forward neural networks and demonstrate that the resulting model can solve\nthe synthetic \"addition\" and \"multiplication\" long-term memory problems for\nsequence lengths which are both longer and more widely varying than the best\npublished results for these tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 19:03:43 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 17:52:46 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 01:52:06 GMT"}, {"version": "v4", "created": "Mon, 28 Mar 2016 02:41:21 GMT"}, {"version": "v5", "created": "Tue, 20 Sep 2016 05:02:22 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Raffel", "Colin", ""], ["Ellis", "Daniel P. W.", ""]]}, {"id": "1512.08806", "submitter": "Uri Shaham", "authors": "Uri Shaham, Roy Lederman", "title": "Common Variable Learning and Invariant Representation Learning using\n  Siamese Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical problem of learning common source of variability\nin data which are synchronously captured by multiple sensors, and demonstrate\nthat Siamese neural networks can be naturally applied to this problem. This\napproach is useful in particular in exploratory, data-driven applications,\nwhere neither a model nor label information is available. In recent years, many\nresearchers have successfully applied Siamese neural networks to obtain an\nembedding of data which corresponds to a \"semantic similarity\". We present an\ninterpretation of this \"semantic similarity\" as learning of equivalence\nclasses. We discuss properties of the embedding obtained by Siamese networks\nand provide empirical results that demonstrate the ability of Siamese networks\nto learn common variability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:06:00 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 03:28:46 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 17:56:32 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Shaham", "Uri", ""], ["Lederman", "Roy", ""]]}, {"id": "1512.08849", "submitter": "Shuohang Wang", "authors": "Shuohang Wang and Jing Jiang", "title": "Learning Natural Language Inference with LSTM", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is a fundamentally important task in natural\nlanguage processing that has many applications. The recently released Stanford\nNatural Language Inference (SNLI) corpus has made it possible to develop and\nevaluate learning-centered methods such as deep neural networks for natural\nlanguage inference (NLI). In this paper, we propose a special long short-term\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\nproposed neural attention model for NLI but is based on a significantly\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\nhypothesis to be used for classification, our solution uses a match-LSTM to\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\nable to place more emphasis on important word-level matching results. In\nparticular, we observe that this LSTM remembers important mismatches that are\ncritical for predicting the contradiction or the neutral relationship label. On\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 05:02:53 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 11:54:29 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wang", "Shuohang", ""], ["Jiang", "Jing", ""]]}, {"id": "1512.08903", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Minjae Lee, Wonyong Sung", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a context-aware keyword spotting model employing a\ncharacter-level recurrent neural network (RNN) for spoken term detection in\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\nclassification (CTC) to generate the probabilities of character and\nword-boundary labels. There is no need for the phonetic transcription, senone\nmodeling, or system dictionary in training and testing. Also, keywords can\neasily be added and modified by editing the text based keyword list without\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\nlong input audio streams without pre-segmentation and keywords are detected\nwith low-latency before the utterance is finished. Experimental results show\nthat the proposed keyword spotter significantly outperforms the deep neural\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\nwith less computations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 10:32:12 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Lee", "Minjae", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.09251", "submitter": "Wolfgang Konen K", "authors": "Samineh Bagheri and Wolfgang Konen and Michael Emmerich and Thomas\n  B\\\"ack", "title": "Solving the G-problems in less than 500 iterations: Improved efficient\n  constrained optimization by surrogate modeling and adaptive parameter control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained optimization of high-dimensional numerical problems plays an\nimportant role in many scientific and industrial applications. Function\nevaluations in many industrial applications are severely limited and no\nanalytical information about objective function and constraint functions is\navailable. For such expensive black-box optimization tasks, the constraint\noptimization algorithm COBRA was proposed, making use of RBF surrogate modeling\nfor both the objective and the constraint functions. COBRA has shown remarkable\nsuccess in solving reliably complex benchmark problems in less than 500\nfunction evaluations. Unfortunately, COBRA requires careful adjustment of\nparameters in order to do so.\n  In this work we present a new self-adjusting algorithm SACOBRA, which is\nbased on COBRA and capable to achieve high-quality results with very few\nfunction evaluations and no parameter tuning. It is shown with the help of\nperformance profiles on a set of benchmark problems (G-problems, MOPTA08) that\nSACOBRA consistently outperforms any COBRA algorithm with fixed parameter\nsetting. We analyze the importance of the several new elements in SACOBRA and\nfind that each element of SACOBRA plays a role to boost up the overall\noptimization performance. We discuss the reasons behind and get in this way a\nbetter understanding of high-quality RBF surrogate modeling.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 10:30:21 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Bagheri", "Samineh", ""], ["Konen", "Wolfgang", ""], ["Emmerich", "Michael", ""], ["B\u00e4ck", "Thomas", ""]]}]