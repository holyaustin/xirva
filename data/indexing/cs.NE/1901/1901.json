[{"id": "1901.00100", "submitter": "Zhiri Tang", "authors": "Zhiri Tang, Ruohua Zhu, Peng Lin, Jin He, Hao Wang, Qijun Huang, Sheng\n  Chang, Qiming Ma", "title": "A Hardware Friendly Unsupervised Memristive Neural Network with Weight\n  Sharing Mechanism", "comments": "10 pages, 11 figures", "journal-ref": "Neurocomputing 2019", "doi": "10.1016/j.neucom.2018.12.049", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristive neural networks (MNNs), which use memristors as neurons or\nsynapses, have become a hot research topic recently. However, most memristors\nare not compatible with mainstream integrated circuit technology and their\nstabilities in large-scale are not very well so far. In this paper, a hardware\nfriendly MNN circuit is introduced, in which the memristive characteristics are\nimplemented by digital integrated circuit. Through this method, spike timing\ndependent plasticity (STDP) and unsupervised learning are realized. A weight\nsharing mechanism is proposed to bridge the gap of network scale and hardware\nresource. Experiment results show the hardware resource is significantly saved\nwith it, maintaining good recognition accuracy and high speed. Moreover, the\ntendency of resource increase is slower than the expansion of network scale,\nwhich infers our method's potential on large scale neuromorphic network's\nrealization.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 05:56:34 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Tang", "Zhiri", ""], ["Zhu", "Ruohua", ""], ["Lin", "Peng", ""], ["He", "Jin", ""], ["Wang", "Hao", ""], ["Huang", "Qijun", ""], ["Chang", "Sheng", ""], ["Ma", "Qiming", ""]]}, {"id": "1901.00109", "submitter": "Ranjan Mondal", "authors": "Ranjan Mondal, Soumendu Sundar Mukherjee, Sanchayan Santra and\n  Bhabatosh Chanda", "title": "Morphological Network: How Far Can We Go with Morphological Neurons?", "comments": "35 pages, 19 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the idea of using morphological operations as networks has\nreceived much attention. Mathematical morphology provides very efficient and\nuseful image processing and image analysis tools based on basic operators like\ndilation and erosion, defined in terms of kernels. Many other morphological\noperations are built up using the dilation and erosion operations. Although the\nlearning of structuring elements such as dilation or erosion using the\nbackpropagation algorithm is not new, the order and the way these morphological\noperations are used is not standard. In this paper, we have theoretically\nanalyzed the use of morphological operations for processing 1D feature vectors\nand shown that this gets extended to the 2D case in a simple manner. Our\ntheoretical results show that a morphological block represents a sum of hinge\nfunctions. Hinge functions are used in many places for classification and\nregression tasks (Breiman (1993)). We have also proved a universal\napproximation theorem -- a stack of two morphological blocks can approximate\nany continuous function over arbitrary compact sets. To experimentally validate\nthe efficacy of this network in real-life applications, we have evaluated its\nperformance on satellite image classification datasets since morphological\noperations are very sensitive to geometrical shapes and structures. We have\nalso shown results on a few tasks like segmentation of blood vessels from\nfundus images, segmentation of lungs from chest x-ray and image dehazing. The\nresults are encouraging and further establishes the potential of morphological\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 07:52:24 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 19:19:40 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 22:21:21 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mondal", "Ranjan", ""], ["Mukherjee", "Soumendu Sundar", ""], ["Santra", "Sanchayan", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "1901.00121", "submitter": "Ahmad Shawahna", "authors": "Ahmad Shawahna, Sadiq M. Sait, and Aiman El-Maleh", "title": "FPGA-based Accelerators of Deep Learning Networks for Learning and\n  Classification: A Review", "comments": "This article has been accepted for publication in IEEE Access\n  (December, 2018)", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2890150", "report-no": null, "categories": "cs.NE cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 09:17:51 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Shawahna", "Ahmad", ""], ["Sait", "Sadiq M.", ""], ["El-Maleh", "Aiman", ""]]}, {"id": "1901.00128", "submitter": "Roshan Gopalakrishnan", "authors": "Roshan Gopalakrishnan, Ashish Jith Sreejith Kumar and Yansong Chua", "title": "MaD: Mapping and debugging framework for implementing deep neural\n  network onto a neuromorphic chip with crossbar array of synapses", "comments": "7 pages, 6 figures and 6 tables. Submitting to IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic systems or dedicated hardware for neuromorphic computing is\ngetting popular with the advancement in research on different device materials\nfor synapses, especially in crossbar architecture and also algorithms specific\nor compatible to neuromorphic hardware. Hence, an automated mapping of any deep\nneural network onto the neuromorphic chip with crossbar array of synapses and\nan efficient debugging framework is very essential. Here, mapping is defined as\nthe deployment of a section of deep neural network layer onto a neuromorphic\ncore and the generation of connection lists among population of neurons to\nspecify the connectivity between various neuromorphic cores on the neuromorphic\nchip. Debugging is the verification of computations performed on the\nneuromorphic chip during inferencing. Together the framework becomes Mapping\nand Debugging (MaD) framework. MaD framework is quite general in usage as it is\na Python wrapper which can be integrated with almost every simulator tools for\nneuromorphic chips. This paper illustrates the MaD framework in detail,\nconsidering some optimizations while mapping onto a single neuromorphic core. A\nclassification task on MNIST and CIFAR-10 datasets are considered for test case\nimplementation of MaD framework.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 10:06:45 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gopalakrishnan", "Roshan", ""], ["Kumar", "Ashish Jith Sreejith", ""], ["Chua", "Yansong", ""]]}, {"id": "1901.00243", "submitter": "Mohammad Kachuee Mr.", "authors": "Mohammad Kachuee, Orpaz Goldstein, Kimmo Karkkainen, Sajad Darabi,\n  Majid Sarrafzadeh", "title": "Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data\n  Streams", "comments": "https://openreview.net/forum?id=S1eOHo09KX", "journal-ref": "International Conference on Learning Representations (ICLR), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world learning scenarios, features are only acquirable at a cost\nconstrained under a budget. In this paper, we propose a novel approach for\ncost-sensitive feature acquisition at the prediction-time. The suggested method\nacquires features incrementally based on a context-aware feature-value\nfunction. We formulate the problem in the reinforcement learning paradigm, and\nintroduce a reward function based on the utility of each feature. Specifically,\nMC dropout sampling is used to measure expected variations of the model\nuncertainty which is used as a feature-value function. Furthermore, we suggest\nsharing representations between the class predictor and value function\nestimator networks. The suggested approach is completely online and is readily\napplicable to stream learning setups. The solution is evaluated on three\ndifferent datasets including the well-known MNIST dataset as a benchmark as\nwell as two cost-sensitive datasets: Yahoo Learning to Rank and a dataset in\nthe medical domain for diabetes classification. According to the results, the\nproposed method is able to efficiently acquire features and make accurate\npredictions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 02:33:54 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 02:42:39 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kachuee", "Mohammad", ""], ["Goldstein", "Orpaz", ""], ["Karkkainen", "Kimmo", ""], ["Darabi", "Sajad", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1901.00266", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Junichi Murata, Hirotaka Takano, Alexandre\n  Claudio Botazzo Delbem", "title": "General Subpopulation Framework and Taming the Conflict Inside\n  Populations", "comments": null, "journal-ref": "Evolutionary computation 23 (1), 1-36, 2015", "doi": "10.1162/EVCO_a_00118", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured evolutionary algorithms have been investigated for some time.\nHowever, they have been under-explored specially in the field of\nmulti-objective optimization. Despite their good results, the use of complex\ndynamics and structures make their understanding and adoption rate low. Here,\nwe propose the general subpopulation framework that has the capability of\nintegrating optimization algorithms without restrictions as well as aid the\ndesign of structured algorithms. The proposed framework is capable of\ngeneralizing most of the structured evolutionary algorithms, such as cellular\nalgorithms, island models, spatial predator-prey and restricted mating based\nalgorithms under its formalization. Moreover, we propose two algorithms based\non the general subpopulation framework, demonstrating that with the simple\naddition of a number of single-objective differential evolution algorithms for\neach objective the results improve greatly, even when the combined algorithms\nbehave poorly when evaluated alone at the tests. Most importantly, the\ncomparison between the subpopulation algorithms and their related panmictic\nalgorithms suggests that the competition between different strategies inside\none population can have deleterious consequences for an algorithm and reveal a\nstrong benefit of using the subpopulation framework.\n  The code for SAN, the proposed multi-objective algorithm which has the\ncurrent best results in the hardest benchmark, is available at the following\nhttps://github.com/zweifel/zweifel\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 05:16:21 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Murata", "Junichi", ""], ["Takano", "Hirotaka", ""], ["Delbem", "Alexandre Claudio Botazzo", ""]]}, {"id": "1901.00279", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Leslie Pack Kaelbling", "title": "Elimination of All Bad Local Minima in Deep Learning", "comments": "Accepted to appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we theoretically prove that adding one special neuron per\noutput unit eliminates all suboptimal local minima of any deep neural network,\nfor multi-class classification, binary classification, and regression with an\narbitrary loss function, under practical assumptions. At every local minimum of\nany deep neural network with these added neurons, the set of parameters of the\noriginal neural network (without added neurons) is guaranteed to be a global\nminimum of the original neural network. The effects of the added neurons are\nproven to automatically vanish at every local minimum. Moreover, we provide a\nnovel theoretical characterization of a failure mode of eliminating suboptimal\nlocal minima via an additional theorem and several examples. This paper also\nintroduces a novel proof technique based on the perturbable gradient basis\n(PGB) necessary condition of local minima, which provides new insight into the\nelimination of local minima and is applicable to analyze various models and\ntransformations of objective functions beyond the elimination of local minima.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 06:40:36 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 22:20:50 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1901.00434", "submitter": "Roman Vershynin", "authors": "Pierre Baldi, Roman Vershynin", "title": "The capacity of feedforward neural networks", "comments": "49 pages. Introduction is expanded and conclusion is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long standing open problem in the theory of neural networks is the\ndevelopment of quantitative methods to estimate and compare the capabilities of\ndifferent architectures. Here we define the capacity of an architecture by the\nbinary logarithm of the number of functions it can compute, as the synaptic\nweights are varied. The capacity provides an upper bound on the number of bits\nthat can be extracted from the training data and stored in the architecture\nduring learning. We study the capacity of layered, fully-connected,\narchitectures of linear threshold neurons with $L$ layers of size $n_1,n_2,\n\\ldots, n_L$ and show that in essence the capacity is given by a cubic\npolynomial in the layer sizes: $C(n_1,\\ldots, n_L)=\\sum_{k=1}^{L-1}\n\\min(n_1,\\ldots,n_k)n_kn_{k+1}$, where layers that are smaller than all\nprevious layers act as bottlenecks. In proving the main result, we also develop\nnew techniques (multiplexing, enrichment, and stacking) as well as new bounds\non the capacity of finite sets. We use the main result to identify\narchitectures with maximal or minimal capacity under a number of natural\nconstraints. This leads to the notion of structural regularization for deep\narchitectures. While in general, everything else being equal, shallow networks\ncompute more functions than deep networks, the functions computed by deep\nnetworks are more regular and \"interesting\".\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 16:05:28 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 21:06:06 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Baldi", "Pierre", ""], ["Vershynin", "Roman", ""]]}, {"id": "1901.00516", "submitter": "Peter He", "authors": "Peter He, Alexis Gkantiragas, Gerard Glowacki", "title": "Honey Authentication with Machine Learning Augmented Bright-Field\n  Microscopy", "comments": "Accepted at the 'AI for Social Good' workshop at the 32nd Conference\n  on Neural Information Processing Systems (NeurIPS2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Honey has been collected and used by humankind as both a food and medicine\nfor thousands of years. However, in the modern economy, honey has become\nsubject to mislabelling and adulteration making it the third most faked food\nproduct in the world. The international scale of fraudulent honey has had both\neconomic and environmental ramifications. In this paper, we propose a novel\nmethod of identifying fraudulent honey using machine learning augmented\nmicroscopy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 13:01:25 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["He", "Peter", ""], ["Gkantiragas", "Alexis", ""], ["Glowacki", "Gerard", ""]]}, {"id": "1901.00518", "submitter": "Diogo Lopes", "authors": "Diogo Lopes, St\\'ephane Clain and Ant\\'onio Ramires Fernandes", "title": "Parameters identification method for breast biomechanical numerical\n  model", "comments": "22 pages including references, 7 figures, 8 tables. Paper submitted\n  to Journal of Medical & Biological Engineering & Computing and waiting for an\n  answer", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-mechanical breast simulations are based on a gravity free geometry as a\nreference domain and a nonlinear mechanical model parameterised by physical\ncoefficients. As opposed to complex models proposed in the literature based on\nmedical imagery, we propose a simple but yet realistic model that uses a basic\nset of measurements easy to realise in the context of routinely operations.\nBoth the mechanical system and the geometry are controlled with parameters we\nshall identify in an optimisation procedure. We give a detailed presentation of\nthe model together with the optimisation method and the associated\ndiscretisation. Sensitivity analysis is then carried out to evaluate the\nrobustness of the method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:58:25 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:29:09 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Lopes", "Diogo", ""], ["Clain", "St\u00e9phane", ""], ["Fernandes", "Ant\u00f3nio Ramires", ""]]}, {"id": "1901.00525", "submitter": "Fathi Salem", "authors": "Daniel Kent and Fathi M.Salem", "title": "Performance of Three Slim Variants of The Long Short-Term Memory (LSTM)\n  Layer", "comments": "4 pages, 2 Tables, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Long Short-Term Memory (LSTM) layer is an important advancement in the\nfield of neural networks and machine learning, allowing for effective training\nand impressive inference performance. LSTM-based neural networks have been\nsuccessfully employed in various applications such as speech processing and\nlanguage translation. The LSTM layer can be simplified by removing certain\ncomponents, potentially speeding up training and runtime with limited change in\nperformance. In particular, the recently introduced variants, called SLIM\nLSTMs, have shown success in initial experiments to support this view. Here, we\nperform computational analysis of the validation accuracy of a convolutional\nplus recurrent neural network architecture using comparatively the standard\nLSTM and three SLIM LSTM layers. We have found that some realizations of the\nSLIM LSTM layers can potentially perform as well as the standard LSTM layer for\nour considered architecture.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 20:28:23 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Kent", "Daniel", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1901.00577", "submitter": "Guizeng You", "authors": "Xinwu Yang, Guizeng You, Chong Zhao, Mengfei Dou and Xinian Guo", "title": "An Improved multi-objective genetic algorithm based on orthogonal design\n  and adaptive clustering pruning strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two important characteristics of multi-objective evolutionary algorithms are\ndistribution and convergency. As a classic multi-objective genetic algorithm,\nNSGA-II is widely used in multi-objective optimization fields. However, in\nNSGA-II, the random population initialization and the strategy of population\nmaintenance based on distance cannot maintain the distribution or convergency\nof the population well. To dispose these two deficiencies, this paper proposes\nan improved algorithm, OTNSGA-II II, which has a better performance on\ndistribution and convergency. The new algorithm adopts orthogonal experiment,\nwhich selects individuals in manner of a new discontinuing non-dominated\nsorting and crowding distance, to produce the initial population. And a new\npruning strategy based on clustering is proposed to self-adaptively prunes\nindividuals with similar features and poor performance in non-dominated sorting\nand crowding distance, or to individuals are far away from the Pareto Front\naccording to the degree of intra-class aggregation of clustering results. The\nnew pruning strategy makes population to converge to the Pareto Front more\neasily and maintain the distribution of population. OTNSGA-II and NSGA-II are\ncompared on various types of test functions to verify the improvement of\nOTNSGA-II in terms of distribution and convergency.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 02:12:07 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Yang", "Xinwu", ""], ["You", "Guizeng", ""], ["Zhao", "Chong", ""], ["Dou", "Mengfei", ""], ["Guo", "Xinian", ""]]}, {"id": "1901.00602", "submitter": "Yun Feng", "authors": "Yun Feng, Bing-Chuan Wang", "title": "Weights Adaptation Optimization of Heterogeneous Epidemic Spreading\n  Networks: A Constrained Cooperative Coevolution Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the dynamic constrained optimization problem of weights\nadaptation for heterogeneous epidemic spreading networks is investigated. Due\nto the powerful ability of searching global optimum, evolutionary algorithms\nare employed as the optimizers. One major difficulty is that the dimension of\nthe problem is increasing exponentially with the network size and most existing\nevolutionary algorithms cannot achieve satisfiable performance on large-scale\noptimization problems. To address this issue, a novel constrained cooperative\ncoevolution ($C^3$) strategy, which can separate the original large-scale\nproblem into different subcomponents, is employed to achieve the trade-off\nbetween the constraint and objective function.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 03:51:56 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 00:50:06 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Feng", "Yun", ""], ["Wang", "Bing-Chuan", ""]]}, {"id": "1901.00686", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Stefan Heinrich, Stefan Wermter", "title": "Generating Multiple Objects at Spatially Distinct Locations", "comments": "Published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent improvements to Generative Adversarial Networks (GANs) have made it\npossible to generate realistic images in high resolution based on natural\nlanguage descriptions such as image captions. Furthermore, conditional GANs\nallow us to control the image generation process through labels or even natural\nlanguage descriptions. However, fine-grained control of the image layout, i.e.\nwhere in the image specific objects should be located, is still difficult to\nachieve. This is especially true for images that should contain multiple\ndistinct objects at different spatial locations. We introduce a new approach\nwhich allows us to control the location of arbitrarily many objects within an\nimage by adding an object pathway to both the generator and the discriminator.\nOur approach does not need a detailed semantic layout but only bounding boxes\nand the respective labels of the desired objects are needed. The object pathway\nfocuses solely on the individual objects and is iteratively applied at the\nlocations specified by the bounding boxes. The global pathway focuses on the\nimage background and the general image layout. We perform experiments on the\nMulti-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show\nthat through the use of the object pathway we can control object locations\nwithin images and can model complex scenes with multiple objects at various\nlocations. We further show that the object pathway focuses on the individual\nobjects and learns features relevant for these, while the global pathway\nfocuses on global image characteristics and the image background.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 11:18:52 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Hinz", "Tobias", ""], ["Heinrich", "Stefan", ""], ["Wermter", "Stefan", ""]]}, {"id": "1901.00794", "submitter": "Leonardo Enzo Brito Da Silva", "authors": "Leonardo Enzo Brito da Silva, Islam Elnabarawy, Donald C. Wunsch II", "title": "Distributed dual vigilance fuzzy adaptive resonance theory learns\n  online, retrieves arbitrarily-shaped clusters, and mitigates order dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel adaptive resonance theory (ART)-based modular\narchitecture for unsupervised learning, namely the distributed dual vigilance\nfuzzy ART (DDVFA). DDVFA consists of a global ART system whose nodes are local\nfuzzy ART modules. It is equipped with the distinctive features of distributed\nhigher-order activation and match functions, using dual vigilance parameters\nresponsible for cluster similarity and data quantization. Together, these allow\nDDVFA to perform unsupervised modularization, create multi-prototype clustering\nrepresentations, retrieve arbitrarily-shaped clusters, and control its\ncompactness. Another important contribution is the reduction of\norder-dependence, an issue that affects any agglomerative clustering method.\nThis paper demonstrates two approaches for mitigating order-dependence:\npreprocessing using visual assessment of cluster tendency (VAT) or\npostprocessing using a novel Merge ART module. The former is suitable for batch\nprocessing, whereas the latter can be used in online learning. Experimental\nresults in the online learning mode carried out on 30 benchmark data sets show\nthat DDVFA cascaded with Merge ART statistically outperformed the best other\nART-based systems when samples were randomly presented. Conversely, they were\nfound to be statistically equivalent in the offline mode when samples were\npre-processed using VAT. Remarkably, performance comparisons to non-ART-based\nclustering algorithms show that DDVFA (which learns incrementally) was also\nstatistically equivalent to the non-incremental (offline) methods of DBSCAN,\nsingle linkage hierarchical agglomerative clustering (HAC), and k-means, while\nretaining the appealing properties of ART. Links to the source code and data\nare provided. Considering the algorithm's simplicity, online learning\ncapability, and performance, it is an ideal choice for many agglomerative\nclustering applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 19:02:35 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["da Silva", "Leonardo Enzo Brito", ""], ["Elnabarawy", "Islam", ""], ["Wunsch", "Donald C.", "II"]]}, {"id": "1901.00862", "submitter": "Duo Xu", "authors": "Duo Xu", "title": "Learning Nonlinear State Space Models with Hamiltonian Sequential Monte\n  Carlo Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State space models (SSM) have been widely applied for the analysis and\nvisualization of large sequential datasets. Sequential Monte Carlo (SMC) is a\nvery popular particle-based method to sample latent states from intractable\nposteriors. However, SSM is significantly influenced by the choice of the\nproposal. Recently Hamiltonian Monte Carlo (HMC) sampling has shown success in\nmany practical problems. In this paper, we propose an SMC augmented by HMC\n(HSMC) for inference and model learning of nonlinear SSM, which can exempt us\nfrom learning proposals and reduce the model complexity significantly. Based on\nthe measure preserving property of HMC, the particles directly generated by\ntransition function can approximate the posterior of latent states arbitrarily\nwell. In order to better adapt to the local geometry of latent space, the HMC\nis conducted on Riemannian manifold defined by a positive definite metric. In\naddition, we show that the proposed HSMC method can improve SSMs realized by\nboth Gaussian Processes (GP) and Neural Network (NN).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 18:23:25 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Xu", "Duo", ""]]}, {"id": "1901.00884", "submitter": "Jeremiah Johnson", "authors": "Jeremiah Johnson", "title": "Subspace Match Probably Does Not Accurately Assess the Similarity of\n  Learned Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.AC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning informative representations of data is one of the primary goals of\ndeep learning, but there is still little understanding as to what\nrepresentations a neural network actually learns. To better understand this,\nsubspace match was recently proposed as a method for assessing the similarity\nof the representations learned by neural networks. It has been shown that two\nnetworks with the same architecture trained from different initializations\nlearn representations that at hidden layers show low similarity when assessed\nwith subspace match, even when the output layers show high similarity and the\nnetworks largely exhibit similar performance on classification tasks. In this\nnote, we present a simple example motivated by standard results in commutative\nalgebra to illustrate how this can happen, and show that although the subspace\nmatch at a hidden layer may be 0, the representations learned may be isomorphic\nas vector spaces. This leads us to conclude that a subspace match comparison of\nlearned representations may well be uninformative, and it points to the need\nfor better methods of understanding learned representations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:17:45 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Johnson", "Jeremiah", ""]]}, {"id": "1901.00943", "submitter": "Carlos Florensa", "authors": "Carlos Florensa, Jonas Degrave, Nicolas Heess, Jost Tobias\n  Springenberg, Martin Riedmiller", "title": "Self-supervised Learning of Image Embedding for Continuous Control", "comments": "Contributed talk at Inference to Control workshop at NeurIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating directly from raw high dimensional sensory inputs like images is\nstill a challenge for robotic control. Recently, Reinforcement Learning methods\nhave been proposed to solve specific tasks end-to-end, from pixels to torques.\nHowever, these approaches assume the access to a specified reward which may\nrequire specialized instrumentation of the environment. Furthermore, the\nobtained policy and representations tend to be task specific and may not\ntransfer well. In this work we investigate completely self-supervised learning\nof a general image embedding and control primitives, based on finding the\nshortest time to reach any state. We also introduce a new structure for the\nstate-action value function that builds a connection between model-free and\nmodel-based methods, and improves the performance of the learning algorithm. We\nexperimentally demonstrate these findings in three simulated robotic tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 23:49:09 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Florensa", "Carlos", ""], ["Degrave", "Jonas", ""], ["Heess", "Nicolas", ""], ["Springenberg", "Jost Tobias", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1901.00945", "submitter": "Stephane Deny", "authors": "Jack Lindsey, Samuel A. Ocko, Surya Ganguli, Stephane Deny", "title": "A Unified Theory of Early Visual Representations from Retina to Cortex\n  through Anatomically Constrained Deep CNNs", "comments": null, "journal-ref": "International Conference on Learning Representations, 2019\n  https://openreview.net/forum?id=S1xq3oR5tQ", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The visual system is hierarchically organized to process visual information\nin successive stages. Neural representations vary drastically across the first\nstages of visual processing: at the output of the retina, ganglion cell\nreceptive fields (RFs) exhibit a clear antagonistic center-surround structure,\nwhereas in the primary visual cortex, typical RFs are sharply tuned to a\nprecise orientation. There is currently no unified theory explaining these\ndifferences in representations across layers. Here, using a deep convolutional\nneural network trained on image recognition as a model of the visual system, we\nshow that such differences in representation can emerge as a direct consequence\nof different neural resource constraints on the retinal and cortical networks,\nand we find a single model from which both geometries spontaneously emerge at\nthe appropriate stages of visual processing. The key constraint is a reduced\nnumber of neurons at the retinal output, consistent with the anatomy of the\noptic nerve as a stringent bottleneck. Second, we find that, for simple\ncortical networks, visual representations at the retinal output emerge as\nnonlinear and lossy feature detectors, whereas they emerge as linear and\nfaithful encoders of the visual scene for more complex cortices. This result\npredicts that the retinas of small vertebrates should perform sophisticated\nnonlinear computations, extracting features directly relevant to behavior,\nwhereas retinas of large animals such as primates should mostly encode the\nvisual scene linearly and respond to a much broader range of stimuli. These\npredictions could reconcile the two seemingly incompatible views of the retina\nas either performing feature extraction or efficient coding of natural scenes,\nby suggesting that all vertebrates lie on a spectrum between these two\nobjectives, depending on the degree of neural resources allocated to their\nvisual system.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 23:51:38 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Lindsey", "Jack", ""], ["Ocko", "Samuel A.", ""], ["Ganguli", "Surya", ""], ["Deny", "Stephane", ""]]}, {"id": "1901.00983", "submitter": "Satyarth Vaidya", "authors": "Satyarth Vaidya, Arshveer Kaur, Lavika Goel", "title": "Brief Review of Computational Intelligence Algorithms", "comments": "major error and re-ordering needed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Intelligence algorithms have gained a lot of attention of\nresearchers in the recent years due to their ability to deliver near optimal\nsolutions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 05:21:29 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 09:50:23 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 13:41:54 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Vaidya", "Satyarth", ""], ["Kaur", "Arshveer", ""], ["Goel", "Lavika", ""]]}, {"id": "1901.01074", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Bo Zhang and Ruijun Xu and Hailong Ma", "title": "Multi-Objective Reinforced Evolution in Mobile Neural Architecture\n  Search", "comments": "Deep Learning, Neural Architecture Search, Multi-objective,\n  Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabricating neural models for a wide range of mobile devices demands for a\nspecific design of networks due to highly constrained resources. Both evolution\nalgorithms (EA) and reinforced learning methods (RL) have been dedicated to\nsolve neural architecture search problems. However, these combinations usually\nconcentrate on a single objective such as the error rate of image\nclassification. They also fail to harness the very benefits from both sides. In\nthis paper, we present a new multi-objective oriented algorithm called MoreMNAS\n(Multi-Objective Reinforced Evolution in Mobile Neural Architecture Search) by\nleveraging good virtues from both EA and RL. In particular, we incorporate a\nvariant of multi-objective genetic algorithm NSGA-II, in which the search space\nis composed of various cells so that crossovers and mutations can be performed\nat the cell level. Moreover, reinforced control is mixed with a natural\nmutating process to regulate arbitrary mutation, maintaining a delicate balance\nbetween exploration and exploitation. Therefore, not only does our method\nprevent the searched models from degrading during the evolution process, but it\nalso makes better use of learned knowledge. Our experiments conducted in\nSuper-resolution domain (SR) deliver rivalling models compared to some\nstate-of-the-art methods with fewer FLOPS.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 12:21:56 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 10:50:40 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 15:07:10 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Xu", "Ruijun", ""], ["Ma", "Hailong", ""]]}, {"id": "1901.01140", "submitter": "Gabriel Nallathambi", "authors": "Gabriel Nallathambi and Jose C. Principe", "title": "Theory and Algorithms for Pulse Signal Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrate and fire converter transforms an analog signal into train of\nbiphasic pulses. The pulse train has information encoded in the timing and\npolarity of pulses. While it has been shown that any finite bandwidth analog\nsignal can be reconstructed from these pulse trains with an error as small as\ndesired, there is a need for fundamental signal processing techniques to\noperate directly on pulse trains without signal reconstruction. In this paper,\nthe feasibility of performing online the signal processing operations of\naddition, multiplication, and convolution of analog signals using their pulses\ntrain representations is explored. Theoretical framework to perform signal\nprocessing with pulse trains imposing minimal restrictions is derived, and\nalgorithms for online implementation of the operators are developed.\nPerformance of the algorithms in processing simulated data is studied. An\napplication of noise subtraction and representation of relevant features of\ninterest in electrocardiogram signal is demonstrated with mean pulse rate less\nthan 20 pulses per second.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:29:01 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Nallathambi", "Gabriel", ""], ["Principe", "Jose C.", ""]]}, {"id": "1901.01216", "submitter": "Marc Tanti", "authors": "Marc Tanti, Albert Gatt, Kenneth P. Camilleri", "title": "Transfer learning from language models to image caption generators:\n  Better models may not transfer better", "comments": "17 pages, 4 figures, 3 tables, unpublished (comments welcome)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing a neural caption generator, a convolutional neural network can\nbe used to extract image features. Is it possible to also use a neural language\nmodel to extract sentence prefix features? We answer this question by trying\ndifferent ways to transfer the recurrent neural network and embedding layer\nfrom a neural language model to an image caption generator. We find that image\ncaption generators with transferred parameters perform better than those\ntrained from scratch, even when simply pre-training them on the text of the\nsame captions dataset it will later be trained on. We also find that the best\nlanguage models (in terms of perplexity) do not result in the best caption\ngenerators after transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 20:23:40 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Camilleri", "Kenneth P.", ""]]}, {"id": "1901.01347", "submitter": "Thai Hung Le", "authors": "Hung Le, Truyen Tran, Svetha Venkatesh", "title": "Learning to Remember More with Less Memorization", "comments": "Accepted to ICLR'19, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-augmented neural networks consisting of a neural controller and an\nexternal memory have shown potentials in long-term sequential learning. Current\nRAM-like memory models maintain memory accessing every timesteps, thus they do\nnot effectively leverage the short-term memory held in the controller. We\nhypothesize that this scheme of writing is suboptimal in memory utilization and\nintroduces redundant computation. To validate our hypothesis, we derive a\ntheoretical bound on the amount of information stored in a RAM-like system and\nformulate an optimization problem that maximizes the bound. The proposed\nsolution dubbed Uniform Writing is proved to be optimal under the assumption of\nequal timestep contributions. To relax this assumption, we introduce\nmodifications to the original solution, resulting in a solution termed Cached\nUniform Writing. This method aims to balance between maximizing memorization\nand forgetting via overwriting mechanisms. Through an extensive set of\nexperiments, we empirically demonstrate the advantages of our solutions over\nother recurrent architectures, claiming the state-of-the-arts in various\nsequential modeling tasks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 00:56:09 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 03:13:22 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Le", "Hung", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1901.01462", "submitter": "Usman Ahmad", "authors": "Usman Ahmad, Hong Song, Awais Bilal, Shahid Mahmood, Asad Ullah, Uzair\n  Saeed", "title": "Rethinking the Artificial Neural Networks: A Mesh of Subnets with a\n  Central Mechanism for Storing and Predicting the Data", "comments": "SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING\n  SYSTEMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Artificial Neural Networks (ANNs) have been originally designed to\nfunction like a biological neural network, but does an ANN really work in the\nsame way as a biological neural network? As we know, the human brain holds\ninformation in its memory cells, so if the ANNs use the same model as our\nbrains, they should store datasets in a similar manner. The most popular type\nof ANN architecture is based on a layered structure of neurons, whereas a human\nbrain has trillions of complex interconnections of neurons continuously\nestablishing new connections, updating existing ones, and removing the\nirrelevant connections across different parts of the brain. In this paper, we\npropose a novel approach to building ANNs which are truly inspired by the\nbiological network containing a mesh of subnets controlled by a central\nmechanism. A subnet is a network of neurons that hold the dataset values. We\nattempt to address the following fundamental questions: (1) What is the\narchitecture of the ANN model? Whether the layered architecture is the most\nappropriate choice? (2) Whether a neuron is a process or a memory cell? (3)\nWhat is the best way of interconnecting neurons and what weight-assignment\nmechanism should be used? (4) How to incorporate prior knowledge, bias, and\ngeneralizations for features extraction and prediction? Our proposed ANN\narchitecture leverages the accuracy on textual data and our experimental\nfindings confirm the effectiveness of our model. We also collaborate with the\nconstruction of the ANN model for storing and processing the images.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 20:00:16 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ahmad", "Usman", ""], ["Song", "Hong", ""], ["Bilal", "Awais", ""], ["Mahmood", "Shahid", ""], ["Ullah", "Asad", ""], ["Saeed", "Uzair", ""]]}, {"id": "1901.01549", "submitter": "Guojun Chen", "authors": "Guojun Chen, Xianghong Lin and Guoen Wang", "title": "An online supervised learning algorithm based on triple spikes for\n  spiking neural networks", "comments": "11 pages, 5 figures and 5 tables, partly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using precise times of every spike, spiking supervised learning has more\neffects on complex spatial-temporal pattern than supervised learning only\nthrough neuronal firing rates. The purpose of spiking supervised learning after\nspatial-temporal encoding is to emit desired spike trains with precise times.\nExisting algorithms of spiking supervised learning have excellent performances,\nbut mechanisms of them still have some problems, such as the limitation of\nneuronal types and complex computation. Based on an online regulative mechanism\nof biological synapses, this paper proposes an online supervised learning\nalgorithm of multiple spike trains for spiking neural networks. The proposed\nalgorithm with a spatial-temporal transformation can make a simple direct\nregulation of synaptic weights as soon as firing time of an output spike is\nobtained. Besides, it is also not restricted by types of spiking neuron models.\nRelationship among desired output, actual output and input spike trains is\nfirstly analyzed and synthesized to simply select a unit of pair-spike for a\ndirect regulation. And then a computational method is constructed based on\nsimple triple spikes using this direct regulation. Compared with other learning\nalgorithms, results of experiments show that proposed algorithm has higher\nlearning accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 15:11:10 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 06:27:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Chen", "Guojun", ""], ["Lin", "Xianghong", ""], ["Wang", "Guoen", ""]]}, {"id": "1901.01753", "submitter": "Rui Wang", "authors": "Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley", "title": "Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly\n  Complex and Diverse Learning Environments and Their Solutions", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the history of machine learning so far largely encompasses a series of\nproblems posed by researchers and algorithms that learn their solutions, an\nimportant question is whether the problems themselves can be generated by the\nalgorithm at the same time as they are being solved. Such a process would in\neffect build its own diverse and expanding curricula, and the solutions to\nproblems at various stages would become stepping stones towards solving even\nmore challenging problems later in the process. The Paired Open-Ended\nTrailblazer (POET) algorithm introduced in this paper does just that: it pairs\nthe generation of environmental challenges and the optimization of agents to\nsolve those challenges. It simultaneously explores many different paths through\nthe space of possible problems and solutions and, critically, allows these\nstepping-stone solutions to transfer between problems if better, catalyzing\ninnovation. The term open-ended signifies the intriguing potential for\nalgorithms like POET to continue to create novel and increasingly complex\ncapabilities without bound. Our results show that POET produces a diverse range\nof sophisticated behaviors that solve a wide range of environmental challenges,\nmany of which cannot be solved by direct optimization alone, or even through a\ndirect-path curriculum-building control algorithm introduced to highlight the\ncritical role of open-endedness in solving ambitious challenges. The ability to\ntransfer solutions from one environment to another proves essential to\nunlocking the full potential of the system as a whole, demonstrating the\nunpredictable nature of fortuitous stepping stones. We hope that POET will\ninspire a new push towards open-ended discovery across many domains, where\nalgorithms like POET can blaze a trail through their interesting possible\nmanifestations and solutions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 11:28:36 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 10:32:21 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 01:17:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Wang", "Rui", ""], ["Lehman", "Joel", ""], ["Clune", "Jeff", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "1901.01965", "submitter": "Lingchuan Meng", "authors": "Lingchuan Meng, John Brothers", "title": "Efficient Winograd Convolution via Integer Arithmetic", "comments": "9 pages, 5 figures, and quite a few matrices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is the core operation for many deep neural networks. The Winograd\nconvolution algorithms have been shown to accelerate the widely-used small\nconvolution sizes. Quantized neural networks can effectively reduce model sizes\nand improve inference speed, which leads to a wide variety of kernels and\nhardware accelerators that work with integer data. The state-of-the-art\nWinograd algorithms pose challenges for efficient implementation and execution\nby the integer kernels and accelerators. We introduce a new class of Winograd\nalgorithms by extending the construction to the field of complex and propose\noptimizations that reduce the number of general multiplications. The new\nalgorithm achieves an arithmetic complexity reduction of $3.13$x over the\ndirect method and an efficiency gain up to $17.37\\%$ over the rational\nalgorithms. Furthermore, we design and implement an integer-based filter\nscaling scheme to effectively reduce the filter bit width by $30.77\\%$ without\nany significant accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 18:46:03 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Meng", "Lingchuan", ""], ["Brothers", "John", ""]]}, {"id": "1901.01989", "submitter": "Fernando Corbacho", "authors": "Fernando J. Corbacho", "title": "Towards Self-constructive Artificial Intelligence: Algorithmic basis\n  (Part I)", "comments": "arXiv admin note: substantial text overlap with arXiv:1608.02229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence frameworks should allow for ever more autonomous and\ngeneral systems in contrast to very narrow and restricted (human pre-defined)\ndomain systems, in analogy to how the brain works. Self-constructive Artificial\nIntelligence ($SCAI$) is one such possible framework. We herein propose that\n$SCAI$ is based on three principles of organization: self-growing,\nself-experimental and self-repairing. Self-growing: the ability to autonomously\nand incrementally construct structures and functionality as needed to solve\nencountered (sub)problems. Self-experimental: the ability to internally\nsimulate, anticipate and take decisions based on these expectations.\nSelf-repairing: the ability to autonomously re-construct a previously\nsuccessful functionality or pattern of interaction lost from a possible\nsub-component failure (damage). To implement these principles of organization,\na constructive architecture capable of evolving adaptive autonomous agents is\nrequired. We present Schema-based learning as one such architecture capable of\nincrementally constructing a myriad of internal models of three kinds:\npredictive schemas, dual (inverse models) schemas and goal schemas as they are\nnecessary to autonomously develop increasing functionality.\n  We claim that artificial systems, whether in the digital or in the physical\nworld, can benefit very much form this constructive architecture and should be\norganized around these principles of organization. To illustrate the generality\nof the proposed framework, we include several test cases in structural adaptive\nnavigation in artificial intelligence systems in Paper II of this series, and\nresilient robot motor control in Paper III of this series. Paper IV of this\nseries will also include $SCAI$ for problem structural discovery in predictive\nBusiness Intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 19:57:24 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Corbacho", "Fernando J.", ""]]}, {"id": "1901.02104", "submitter": "Hanie Sedghi", "authors": "Philip M. Long and Hanie Sedghi", "title": "On the effect of the activation function on the distribution of hidden\n  nodes in a deep network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the joint probability distribution on the lengths of the vectors\nof hidden variables in different layers of a fully connected deep network, when\nthe weights and biases are chosen randomly according to Gaussian distributions,\nand the input is in $\\{ -1, 1\\}^N$. We show that, if the activation function\n$\\phi$ satisfies a minimal set of assumptions, satisfied by all activation\nfunctions that we know that are used in practice, then, as the width of the\nnetwork gets large, the `length process' converges in probability to a length\nmap that is determined as a simple function of the variances of the random\nweights and biases, and the activation function $\\phi$. We also show that this\nconvergence may fail for $\\phi$ that violate our assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 23:33:14 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Long", "Philip M.", ""], ["Sedghi", "Hanie", ""]]}, {"id": "1901.02132", "submitter": "Jiecao Yu", "authors": "Jiecao Yu, Jongsoo Park, Maxim Naumov", "title": "Spatial-Winograd Pruning Enabling Sparse Winograd Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are deployed in various\napplications but demand immense computational requirements. Pruning techniques\nand Winograd convolution are two typical methods to reduce the CNN computation.\nHowever, they cannot be directly combined because Winograd transformation fills\nin the sparsity resulting from pruning. Li et al. (2017) propose sparse\nWinograd convolution in which weights are directly pruned in the Winograd\ndomain, but this technique is not very practical because Winograd-domain\nretraining requires low learning rates and hence significantly longer training\ntime. Besides, Liu et al. (2018) move the ReLU function into the Winograd\ndomain, which can help increase the weight sparsity but requires changes in the\nnetwork structure. To achieve a high Winograd-domain weight sparsity without\nchanging network structures, we propose a new pruning method, spatial-Winograd\npruning. As the first step, spatial-domain weights are pruned in a structured\nway, which efficiently transfers the spatial-domain sparsity into the Winograd\ndomain and avoids Winograd-domain retraining. For the next step, we also\nperform pruning and retraining directly in the Winograd domain but propose to\nuse an importance factor matrix to adjust weight importance and weight\ngradients. This adjustment makes it possible to effectively retrain the pruned\nWinograd-domain network without changing the network structure. For the three\nmodels on the datasets of CIFAR10, CIFAR-100, and ImageNet, our proposed method\ncan achieve the Winograd domain sparsities of 63%, 50%, and 74%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 02:17:44 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Yu", "Jiecao", ""], ["Park", "Jongsoo", ""], ["Naumov", "Maxim", ""]]}, {"id": "1901.02172", "submitter": "Yu Song", "authors": "Yu Song, Shengping Gong", "title": "Solar-Sail Trajectory Design for Multiple Near Earth Asteroid\n  Exploration Based on Deep Neural Networks", "comments": "34 pages, 19 figures", "journal-ref": "Aerospace Scienceand Technology 91 (2019) 28-40", "doi": "10.1016/j.ast.2019.04.056", "report-no": null, "categories": "cs.CE astro-ph.IM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the preliminary trajectory design of the multi-target rendezvous problem,\na model that can quickly estimate the cost of the orbital transfer is\nessential. The estimation of the transfer time using solar sail between two\narbitrary orbits is difficult and usually requires to solve an optimal control\nproblem. Inspired by the successful applications of the deep neural network in\nnonlinear regression, this work explores the possibility and effectiveness of\nmapping the transfer time for solar sail from the orbital characteristics using\nthe deep neural network. Furthermore, the Monte Carlo Tree Search method is\ninvestigated and used to search the optimal sequence considering a\nmulti-asteroid exploration problem. The obtained sequences from preliminary\ndesign will be solved and verified by sequentially solving the optimal control\nproblem. Two examples of different application backgrounds validate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 06:40:13 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 14:51:26 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 07:24:16 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Song", "Yu", ""], ["Gong", "Shengping", ""]]}, {"id": "1901.02302", "submitter": "Anna Bosman", "authors": "Anna Sergeevna Bosman, Andries Engelbrecht, Mard\\'e Helbig", "title": "Visualising Basins of Attraction for the Cross-Entropy and the Squared\n  Error Neural Network Loss Functions", "comments": "Preprint submitted to the Neural Networks journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of the stationary points and the associated basins of\nattraction of neural network loss surfaces is an important step towards a\nbetter understanding of neural network loss surfaces at large. This work\nproposes a novel method to visualise basins of attraction together with the\nassociated stationary points via gradient-based random sampling. The proposed\ntechnique is used to perform an empirical study of the loss surfaces generated\nby two different error metrics: quadratic loss and entropic loss. The empirical\nobservations confirm the theoretical hypothesis regarding the nature of neural\nnetwork attraction basins. Entropic loss is shown to exhibit stronger gradients\nand fewer stationary points than quadratic loss, indicating that entropic loss\nhas a more searchable landscape. Quadratic loss is shown to be more resilient\nto overfitting than entropic loss. Both losses are shown to exhibit local\nminima, but the number of local minima is shown to decrease with an increase in\ndimensionality. Thus, the proposed visualisation technique successfully\ncaptures the local minima properties exhibited by the neural network loss\nsurfaces, and can be used for the purpose of fitness landscape analysis of\nneural networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 13:34:36 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 09:42:12 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Bosman", "Anna Sergeevna", ""], ["Engelbrecht", "Andries", ""], ["Helbig", "Mard\u00e9", ""]]}, {"id": "1901.02358", "submitter": "Aditya Kusupati", "authors": "Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain\n  and Manik Varma", "title": "FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated\n  Recurrent Neural Network", "comments": "23 pages, 10 figures, Published at Advances in Neural Information\n  Processing Systems (NeurIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the FastRNN and FastGRNN algorithms to address the twin\nRNN limitations of inaccurate training and inefficient prediction. Previous\napproaches have improved accuracy at the expense of prediction costs making\nthem infeasible for resource-constrained and real-time applications. Unitary\nRNNs have increased accuracy somewhat by restricting the range of the state\ntransition matrix's singular values but have also increased the model size as\nthey require a larger number of hidden units to make up for the loss in\nexpressive power. Gated RNNs have obtained state-of-the-art accuracies by\nadding extra parameters thereby resulting in even larger models. FastRNN\naddresses these limitations by adding a residual connection that does not\nconstrain the range of the singular values explicitly and has only two extra\nscalar parameters. FastGRNN then extends the residual connection to a gate by\nreusing the RNN matrices to match state-of-the-art gated RNN accuracies but\nwith a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse\nand quantized resulted in accurate models that could be up to 35x smaller than\nleading gated and unitary RNNs. This allowed FastGRNN to accurately recognize\nthe \"Hey Cortana\" wakeword with a 1 KB model and to be deployed on severely\nresource-constrained IoT microcontrollers too tiny to store other RNN models.\nFastGRNN's code is available at https://github.com/Microsoft/EdgeML/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 15:19:13 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Kusupati", "Aditya", ""], ["Singh", "Manish", ""], ["Bhatia", "Kush", ""], ["Kumar", "Ashish", ""], ["Jain", "Prateek", ""], ["Varma", "Manik", ""]]}, {"id": "1901.02369", "submitter": "Dario  Izzo", "authors": "Dharmesh Tailor, Dario Izzo", "title": "Learning the optimal state-feedback via supervised imitation learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is a control design paradigm that seeks to learn a control\npolicy reproducing demonstrations from expert agents. By substituting expert\ndemonstrations for optimal behaviours, the same paradigm leads to the design of\ncontrol policies closely approximating the optimal state-feedback. This\napproach requires training a machine learning algorithm (in our case deep\nneural networks) directly on state-control pairs originating from optimal\ntrajectories. We have shown in previous work that, when restricted to\nlow-dimensional state and control spaces, this approach is very successful in\nseveral deterministic, non-linear problems in continuous-time. In this work, we\nrefine our previous studies using as a test case a simple quadcopter model with\nquadratic and time-optimal objective functions. We describe in detail the best\nlearning pipeline we have developed, that is able to approximate via deep\nneural networks the state-feedback map to a very high accuracy. We introduce\nthe use of the softplus activation function in the hidden units of neural\nnetworks showing that it results in a smoother control profile whilst retaining\nthe benefits of rectifiers. We show how to evaluate the optimality of the\ntrained state-feedback, and find that already with two layers the objective\nfunction reached and its optimal value differ by less than one percent. We\nlater consider also an additional metric linked to the system asymptotic\nbehaviour - time taken to converge to the policy's fixed point. With respect to\nthese metrics, we show that improvements in the mean absolute error do not\nnecessarily correspond to better policies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 14:11:06 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 09:56:14 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Tailor", "Dharmesh", ""], ["Izzo", "Dario", ""]]}, {"id": "1901.02438", "submitter": "Sergij Goncharov", "authors": "Sergij V. Goncharov", "title": "Using fuzzy bits and neural networks to partially invert few rounds of\n  some cryptographic hash functions", "comments": "26 pages, 31 figures, 48 refs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fuzzy, or continuous, bits, which take values in [0;1] and (-1;1]\ninstead of {0;1}, and operations on them (NOT, XOR etc.) and on their sequences\n(ADD), to obtain the generalization of cryptographic hash functions, CHFs, for\nthe messages consisting of fuzzy bits, so that CHFs become smooth and\nnon-constant functions of each bit of the message. We then train the neural\nnetworks to predict the message that has a given hash, where the loss function\nfor the hash of predicted message and given true hash is backpropagatable. The\nresults of the trainings for the standard CHFs - MD5, SHA1, SHA2-256, and\nSHA3/Keccak - with small number of (optionally weakened) rounds are presented\nand compared.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:35:13 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Goncharov", "Sergij V.", ""]]}, {"id": "1901.02671", "submitter": "Steffen Eger", "authors": "Steffen Eger and Paul Youssef and Iryna Gurevych", "title": "Is it Time to Swish? Comparing Deep Learning Activation Functions Across\n  NLP tasks", "comments": "Published at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions play a crucial role in neural networks because they are\nthe nonlinearities which have been attributed to the success story of deep\nlearning. One of the currently most popular activation functions is ReLU, but\nseveral competitors have recently been proposed or 'discovered', including\nLReLU functions and swish. While most works compare newly proposed activation\nfunctions on few tasks (usually from image classification) and against few\ncompetitors (usually ReLU), we perform the first large-scale comparison of 21\nactivation functions across eight different NLP tasks. We find that a largely\nunknown activation function performs most stably across all tasks, the\nso-called penalized tanh function. We also show that it can successfully\nreplace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage\npoint (pp) improvement over the standard choices on a challenging NLP task.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 10:45:20 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Eger", "Steffen", ""], ["Youssef", "Paul", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1901.03545", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Relationships between dilemma strength and fixation properties in\n  coevolutionary games", "comments": null, "journal-ref": "Proc. ICNC-FSKD 2019, Kunming, China", "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether or not cooperation is favored over defection in evolutionary games\ncan be assigned by structure coefficients for any arrangement of cooperators\nand defectors on any network modeled as a regular graph. We study how these\nstructure coefficients relate to a scaling of dilemma strength in social\ndilemma games. It is shown that some graphs permit certain arrangements of\ncooperators and defectors to possess particularly large structure coefficients.\nMoreover, these large coefficients imply particularly large sections of a\nbounded parameter plane spanned by scaling gamble-intending and risk-averting\ndilemma strength.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:41:30 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 10:40:37 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "1901.03690", "submitter": "Bipin Rajendran", "authors": "Bipin Rajendran, Abu Sebastian, Michael Schmuker, Narayan Srinivasa,\n  and Evangelos Eleftheriou", "title": "Low-Power Neuromorphic Hardware for Signal Processing Applications", "comments": "24 pages, 7 figures, accepted for the Special Issue on Learning\n  Algorithms and Signal Processing for Brain-Inspired Computing in the IEEE\n  Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has emerged as the dominant tool for implementing complex\ncognitive tasks that require supervised, unsupervised, and reinforcement\nlearning. While the resulting machines have demonstrated in some cases even\nsuper-human performance, their energy consumption has often proved to be\nprohibitive in the absence of costly super-computers. Most state-of-the-art\nmachine learning solutions are based on memory-less models of neurons. This is\nunlike the neurons in the human brain, which encode and process information\nusing temporal information in spike events. The different computing principles\nunderlying biological neurons and how they combine together to efficiently\nprocess information is believed to be a key factor behind their superior\nefficiency compared to current machine learning systems. Inspired by the\ntime-encoding mechanism used by the brain, third generation spiking neural\nnetworks (SNNs) are being studied for building a new class of information\nprocessing engines.\n  Modern computing systems based on the von Neumann architecture, however, are\nill-suited for efficiently implementing SNNs, since their performance is\nlimited by the need to constantly shuttle data between physically separated\nlogic and memory units. Hence, novel computational architectures that address\nthe von Neumann bottleneck are necessary in order to build systems that can\nimplement SNNs with low energy budgets. In this paper, we review some of the\narchitectural and system level design aspects involved in developing a new\nclass of brain-inspired information processing engines that mimic the\ntime-based information encoding and processing aspects of the brain.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 18:54:20 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 00:15:47 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 14:11:40 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Rajendran", "Bipin", ""], ["Sebastian", "Abu", ""], ["Schmuker", "Michael", ""], ["Srinivasa", "Narayan", ""], ["Eleftheriou", "Evangelos", ""]]}, {"id": "1901.03775", "submitter": "Risto Miikkulainen", "authors": "Risto Miikkulainen", "title": "Creative AI Through Evolutionary Computation", "comments": null, "journal-ref": "In Banzhaf et al. (editors), Evolution in Action---Past, Present\n  and Future. New York: Springer. 2020", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main power of artificial intelligence is not in modeling what we already\nknow, but in creating solutions that are new. Such solutions exist in extremely\nlarge, high-dimensional, and complex search spaces. Population-based search\ntechniques, i.e. variants of evolutionary computation, are well suited to\nfinding them. These techniques are also well positioned to take advantage of\nlarge-scale parallel computing resources, making creative AI through\nevolutionary computation the likely \"next deep learning\".\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 00:26:13 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 23:15:46 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Miikkulainen", "Risto", ""]]}, {"id": "1901.03900", "submitter": "Aaron Vose", "authors": "Aaron Vose, Jacob Balma, Alex Heye, Alessandro Rigazzi, Charles\n  Siegel, Diana Moise, Benjamin Robbins, Rangan Sukumar", "title": "Recombination of Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a genetic algorithm (GA) for hyperparameter optimization of\nartificial neural networks which includes chromosomal crossover as well as a\ndecoupling of parameters (i.e., weights and biases) from hyperparameters (e.g.,\nlearning rate, weight decay, and dropout) during sexual reproduction. Children\nare produced from three parents; two contributing hyperparameters and one\ncontributing the parameters. Our version of population-based training (PBT)\ncombines traditional gradient-based approaches such as stochastic gradient\ndescent (SGD) with our GA to optimize both parameters and hyperparameters\nacross SGD epochs. Our improvements over traditional PBT provide an increased\nspeed of adaptation and a greater ability to shed deleterious genes from the\npopulation. Our methods improve final accuracy as well as time to fixed\naccuracy on a wide range of deep neural network architectures including\nconvolutional neural networks, recurrent neural networks, dense neural\nnetworks, and capsule networks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 21:11:50 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Vose", "Aaron", ""], ["Balma", "Jacob", ""], ["Heye", "Alex", ""], ["Rigazzi", "Alessandro", ""], ["Siegel", "Charles", ""], ["Moise", "Diana", ""], ["Robbins", "Benjamin", ""], ["Sukumar", "Rangan", ""]]}, {"id": "1901.04016", "submitter": "Basel  Magableh  Dr", "authors": "Basel Magableh", "title": "Context Oriented Software Middleware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Our middleware approach, Context-Oriented Software Middleware (COSM),\nsupports context-dependent software with self-adaptability and dependability in\na mobile computing environment. The COSM-middleware is a generic and\nplatform-independent adaptation engine, which performs a runtime composition of\nthe software's context-dependent behaviours based on the execution contexts.\nOur middleware distinguishes between the context-dependent and\ncontext-independent functionality of software systems. This enables the\nCOSM-middleware to adapt the application behaviour by composing a set of\ncontext-oriented components, that implement the context-dependent functionality\nof the software. Accordingly, the software dependability is achieved by\nconsidering the functionality of the COSM-middleware and the adaptation\nimpact/costs. The COSM-middleware uses a dynamic policy-based engine to\nevaluate the adaptation outputs and verify the fitness of the adaptation output\nwith the application's objectives, goals and the architecture quality\nattributes. These capabilities are demonstrated through an empirical evaluation\nof a case study implementation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 16:57:38 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Magableh", "Basel", ""]]}, {"id": "1901.04024", "submitter": "Pengfei Sun", "authors": "Pengfei Sun, David A. Moses, and Edward Chang", "title": "Modeling neural dynamics during speech production using a state space\n  variational autoencoder", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the neural encoding of behavior remains a challenging task in\nmany research areas due in part to complex and noisy spatiotemporal dynamics of\nevoked brain activity. An important aspect of modeling these neural encodings\ninvolves separation of robust, behaviorally relevant signals from background\nactivity, which often contains signals from irrelevant brain processes and\ndecaying information from previous behavioral events. To achieve this\nseparation, we develop a two-branch State Space Variational AutoEncoder (SSVAE)\nmodel to individually describe the instantaneous evoked foreground signals and\nthe context-dependent background signals. We modeled the spontaneous\nspeech-evoked brain dynamics using smoothed Gaussian mixture models. By\napplying the proposed SSVAE model to track ECoG dynamics in one participant\nover multiple hours, we find that the model can predict speech-related dynamics\nmore accurately than other latent factor inference algorithms. Our results\ndemonstrate that separately modeling the instantaneous speech-evoked and slow\ncontext-dependent brain dynamics can enhance tracking performance, which has\nimportant implications for the development of advanced neural encoding and\ndecoding models in various neuroscience sub-disciplines.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 17:26:11 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sun", "Pengfei", ""], ["Moses", "David A.", ""], ["Chang", "Edward", ""]]}, {"id": "1901.04169", "submitter": "Helge Spieker", "authors": "Helge Spieker, Arnaud Gotlieb", "title": "Towards Testing of Deep Learning Systems with Training Set Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing the implementation of deep learning systems and their training\nroutines is crucial to maintain a reliable code base. Modern software\ndevelopment employs processes, such as Continuous Integration, in which changes\nto the software are frequently integrated and tested. However, testing the\ntraining routines requires running them and fully training a deep learning\nmodel can be resource-intensive, when using the full data set. Using only a\nsubset of the training data can improve test run time, but can also reduce its\neffectiveness. We evaluate different ways for training set reduction and their\nability to mimic the characteristics of model training with the original full\ndata set. Our results underline the usefulness of training set reduction,\nespecially in resource-constrained environments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 07:55:00 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Spieker", "Helge", ""], ["Gotlieb", "Arnaud", ""]]}, {"id": "1901.04321", "submitter": "Thom Lake", "authors": "Thom Lake, Sinead A. Williamson, Alexander T. Hawk, Christopher C.\n  Johnson, Benjamin P. Wing", "title": "Large-scale Collaborative Filtering with Product Embeddings", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of machine learning techniques to large-scale personalized\nrecommendation problems is a challenging task. Such systems must make sense of\nenormous amounts of implicit feedback in order to understand user preferences\nacross numerous product categories. This paper presents a deep learning based\nsolution to this problem within the collaborative filtering with implicit\nfeedback framework. Our approach combines neural attention mechanisms, which\nallow for context dependent weighting of past behavioral signals, with\nrepresentation learning techniques to produce models which obtain extremely\nhigh coverage, can easily incorporate new information as it becomes available,\nand are computationally efficient. Offline experiments demonstrate significant\nperformance improvements when compared to several alternative methods from the\nliterature. Results from an online setting show that the approach compares\nfavorably with current production techniques used to produce personalized\nproduct recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 17:28:59 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Lake", "Thom", ""], ["Williamson", "Sinead A.", ""], ["Hawk", "Alexander T.", ""], ["Johnson", "Christopher C.", ""], ["Wing", "Benjamin P.", ""]]}, {"id": "1901.04392", "submitter": "Pierre Tirilly", "authors": "Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne,\n  Pierre Boulet", "title": "Unsupervised Visual Feature Learning with Spike-timing-dependent\n  Plasticity: How Far are we from Traditional Feature Learning Approaches?", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2019.04.016", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) equipped with latency coding and spike-timing\ndependent plasticity rules offer an alternative to solve the data and energy\nbottlenecks of standard computer vision approaches: they can learn visual\nfeatures without supervision and can be implemented by ultra-low power hardware\narchitectures. However, their performance in image classification has never\nbeen evaluated on recent image datasets. In this paper, we compare SNNs to\nauto-encoders on three visual recognition datasets, and extend the use of SNNs\nto color images. The analysis of the results helps us identify some bottlenecks\nof SNNs: the limits of on-center/off-center coding, especially for color\nimages, and the ineffectiveness of current inhibition mechanisms. These issues\nshould be addressed to build effective SNNs for image recognition.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:42:30 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 11:04:24 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Falez", "Pierre", ""], ["Tirilly", "Pierre", ""], ["Bilasco", "Ioan Marius", ""], ["Devienne", "Philippe", ""], ["Boulet", "Pierre", ""]]}, {"id": "1901.05123", "submitter": "Simon Denman", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Memory Augmented Deep Generative models for Forecasting the Next Shot\n  Location in Tennis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for predicting shot location and type\nin tennis. Inspired by recent neuroscience discoveries we incorporate neural\nmemory modules to model the episodic and semantic memory components of a tennis\nplayer. We propose a Semi Supervised Generative Adversarial Network\narchitecture that couples these memory models with the automatic feature\nlearning power of deep neural networks and demonstrate methodologies for\nlearning player level behavioural patterns with the proposed framework. We\nevaluate the effectiveness of the proposed model on tennis tracking data from\nthe 2012 Australian Tennis open and exhibit applications of the proposed method\nin discovering how players adapt their style depending on the match context.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 03:16:28 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1901.05214", "submitter": "Lukas Kades", "authors": "Lukas Kades and Jan M. Pawlowski", "title": "The Discrete Langevin Machine: Bridging the Gap Between Thermodynamic\n  and Neuromorphic Systems", "comments": "25 pages, 16 figures", "journal-ref": "Phys. Rev. E 101, 063304 (2020)", "doi": "10.1103/PhysRevE.101.063304", "report-no": null, "categories": "cs.NE hep-lat physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formulation of Langevin dynamics for discrete systems is derived as a class\nof generic stochastic processes. The dynamics simplify for a two-state system\nand suggest a network architecture which is implemented by the Langevin\nmachine. The Langevin machine represents a promising approach to compute\nsuccessfully quantitative exact results of Boltzmann distributed systems by LIF\nneurons. Besides a detailed introduction of the dynamics, different simplified\nmodels of a neuromorphic hardware system are studied with respect to a control\nof emerging sources of errors.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 10:33:38 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 13:35:12 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 15:41:05 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Kades", "Lukas", ""], ["Pawlowski", "Jan M.", ""]]}, {"id": "1901.05512", "submitter": "Felipe Viana", "authors": "Renato Giorgiani Nascimento and Felipe A. C. Viana", "title": "Fleet Prognosis with Physics-informed Recurrent Neural Networks", "comments": "Data and codes (including our implementation for both the multi-layer\n  perceptron, the stress intensity and Paris law layers, the cumulative damage\n  cell, as well as python driver scripts) used in this manuscript are publicly\n  available on GitHub at https://github.com/PML-UCF/pinn. The data and code are\n  released under the MIT License", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Services and warranties of large fleets of engineering assets is a very\nprofitable business. The success of companies in that area is often related to\npredictive maintenance driven by advanced analytics. Therefore, accurate\nmodeling, as a way to understand how the complex interactions between operating\nconditions and component capability define useful life, is key for services\nprofitability. Unfortunately, building prognosis models for large fleets is a\ndaunting task as factors such as duty cycle variation, harsh environments,\ninadequate maintenance, and problems with mass production can lead to large\ndiscrepancies between designed and observed useful lives. This paper introduces\na novel physics-informed neural network approach to prognosis by extending\nrecurrent neural networks to cumulative damage models. We propose a new\nrecurrent neural network cell designed to merge physics-informed and\ndata-driven layers. With that, engineers and scientists have the chance to use\nphysics-informed layers to model parts that are well understood (e.g., fatigue\ncrack growth) and use data-driven layers to model parts that are poorly\ncharacterized (e.g., internal loads). A simple numerical experiment is used to\npresent the main features of the proposed physics-informed recurrent neural\nnetwork for damage accumulation. The test problem consist of predicting fatigue\ncrack length for a synthetic fleet of airplanes subject to different mission\nmixes. The model is trained using full observation inputs (far-field loads) and\nvery limited observation of outputs (crack length at inspection for only a\nportion of the fleet). The results demonstrate that our proposed hybrid\nphysics-informed recurrent neural network is able to accurately model fatigue\ncrack growth even when the observed distribution of crack length does not match\nwith the (unobservable) fleet distribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 19:58:35 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Nascimento", "Renato Giorgiani", ""], ["Viana", "Felipe A. C.", ""]]}, {"id": "1901.05573", "submitter": "Carola Doerr", "authors": "Furong Ye, Carola Doerr, Thomas B\\\"ack", "title": "Interpolating Local and Global Search by Controlling the Variance of\n  Standard Bit Mutation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key property underlying the success of evolutionary algorithms (EAs) is\ntheir global search behavior, which allows the algorithms to `jump' from a\ncurrent state to other parts of the search space, thereby avoiding to get stuck\nin local optima. This property is obtained through a random choice of the\nradius at which offspring are sampled from previously evaluated solutions. It\nis well known that, thanks to this global search behavior, the probability that\nan EA using standard bit mutation finds a global optimum of an arbitrary\nfunction $f:\\{0,1\\}^n \\to \\mathbb{R}$ tends to one as the number of function\nevaluations grows. This advantage over heuristics using a fixed search radius,\nhowever, comes at the cost of using non-optimal step sizes also in those\nregimes in which the optimal rate is stable for a long time. This downside\nresults in significant performance losses for many standard benchmark problems.\n  We introduce in this work a simple way to interpolate between the random\nglobal search of EAs and their deterministic counterparts which sample from a\nfixed radius only. To this end, we introduce \\emph{normalized standard bit\nmutation}, in which the binomial choice of the search radius is replaced by a\nnormal distribution. Normalized standard bit mutation allows a straightforward\nway to control its variance, and hence the degree of randomness involved. We\nexperiment with a self-adjusting choice of this variance, and demonstrate its\neffectiveness for the two classic benchmark problems LeadingOnes and OneMax.\nOur work thereby also touches a largely ignored question in discrete\nevolutionary computation: multi-dimensional parameter control.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 00:57:28 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Ye", "Furong", ""], ["Doerr", "Carola", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1901.05704", "submitter": "Jean-Baptiste Mouret", "authors": "David Howard and Agoston E. Eiben and Danielle Frances Kennedy and\n  Jean-Baptiste Mouret and Philip Valencia and Dave Winkler", "title": "Evolving embodied intelligence from materials to machines", "comments": null, "journal-ref": "Nature Machine Intelligence. Vol. 1, Number 1, pages 12--19. 2019", "doi": "10.1038/s42256-018-0009-9", "report-no": null, "categories": "cs.RO cond-mat.mtrl-sci cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural lifeforms specialise to their environmental niches across many\nlevels; from low-level features such as DNA and proteins, through to\nhigher-level artefacts including eyes, limbs, and overarching body plans. We\npropose Multi-Level Evolution (MLE), a bottom-up automatic process that designs\nrobots across multiple levels and niches them to tasks and environmental\nconditions. MLE concurrently explores constituent molecular and material\n'building blocks', as well as their possible assemblies into specialised\nmorphological and sensorimotor configurations. MLE provides a route to fully\nharness a recent explosion in available candidate materials and ongoing\nadvances in rapid manufacturing processes. We outline a feasible MLE\narchitecture that realises this vision, highlight the main roadblocks and how\nthey may be overcome, and show robotic applications to which MLE is\nparticularly suited. By forming a research agenda to stimulate discussion\nbetween researchers in related fields, we hope to inspire the pursuit of\nmulti-level robotic design all the way from material to machine.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 09:51:15 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Howard", "David", ""], ["Eiben", "Agoston E.", ""], ["Kennedy", "Danielle Frances", ""], ["Mouret", "Jean-Baptiste", ""], ["Valencia", "Philip", ""], ["Winkler", "Dave", ""]]}, {"id": "1901.05737", "submitter": "Jan Scholz", "authors": "Jan Scholz", "title": "Genetic Algorithms and the Traveling Salesman Problem a historical\n  Review", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.22632.78088/1", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a highly abstracted view on the historical development of\nGenetic Algorithms for the Traveling Salesman Problem is given. In a meta-data\nanalysis three phases in the development can be distinguished. First\nexponential growth in interest till 1996 can be observed, growth stays linear\ntill 2011 and after that publications deteriorate. These three phases are\nexamined and the major milestones are presented. Lastly an outlook to future\nwork in this field is infered.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 11:35:13 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Scholz", "Jan", ""]]}, {"id": "1901.05755", "submitter": "Hao Tong", "authors": "Hao Tong, Changwu Huang, Jialin Liu and Xin Yao", "title": "Voronoi-based Efficient Surrogate-assisted Evolutionary Algorithm for\n  Very Expensive Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very expensive problems are very common in practical system that one fitness\nevaluation costs several hours or even days. Surrogate assisted evolutionary\nalgorithms (SAEAs) have been widely used to solve this crucial problem in the\npast decades. However, most studied SAEAs focus on solving problems with a\nbudget of at least ten times of the dimension of problems which is unacceptable\nin many very expensive real-world problems. In this paper, we employ Voronoi\ndiagram to boost the performance of SAEAs and propose a novel framework named\nVoronoi-based efficient surrogate assisted evolutionary algorithm (VESAEA) for\nvery expensive problems, in which the optimization budget, in terms of fitness\nevaluations, is only 5 times of the problem's dimension. In the proposed\nframework, the Voronoi diagram divides the whole search space into several\nsubspace and then the local search is operated in some potentially better\nsubspace. Additionally, in order to trade off the exploration and exploitation,\nthe framework involves a global search stage developed by combining\nleave-one-out cross-validation and radial basis function surrogate model. A\nperformance selector is designed to switch the search dynamically and\nautomatically between the global and local search stages. The empirical results\non a variety of benchmark problems demonstrate that the proposed framework\nsignificantly outperforms several state-of-art algorithms with extremely\nlimited fitness evaluations. Besides, the efficacy of Voronoi-diagram is\nfurtherly analyzed, and the results show its potential to optimize very\nexpensive problems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 12:25:58 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 10:31:53 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Tong", "Hao", ""], ["Huang", "Changwu", ""], ["Liu", "Jialin", ""], ["Yao", "Xin", ""]]}, {"id": "1901.05988", "submitter": "Ahmed Aly", "authors": "Ahmed Aly, David Weikersdorfer, Claire Delaunay", "title": "Optimizing Deep Neural Networks with Multiple Search Neuroevolution", "comments": "Submitted to IEEE CEC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an evolutionary metaheuristic called Multiple Search\nNeuroevolution (MSN) to optimize deep neural networks. The algorithm attempts\nto search multiple promising regions in the search space simultaneously,\nmaintaining sufficient distance between them. It is tested by training neural\nnetworks for two tasks, and compared with other optimization algorithms. The\nfirst task is to solve Global Optimization functions with challenging\ntopographies. We found to MSN to outperform classic optimization algorithms\nsuch as Evolution Strategies, reducing the number of optimization steps\nperformed by at least 2X.\n  The second task is to train a convolutional neural network (CNN) on the\npopular MNIST dataset. Using 3.33% of the training set, MSN reaches a\nvalidation accuracy of 90%. Stochastic Gradient Descent (SGD) was able to match\nthe same accuracy figure, while taking 7X less optimization steps. Despite\nlagging, the fact that the MSN metaheurisitc trains a 4.7M-parameter CNN\nsuggests promise for future development. This is by far the largest network\never evolved using a pool of only 50 samples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 19:28:21 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Aly", "Ahmed", ""], ["Weikersdorfer", "David", ""], ["Delaunay", "Claire", ""]]}, {"id": "1901.06153", "submitter": "Fabio Caraffini", "authors": "Fabio Caraffini and Anna V. Kononova and David Corne", "title": "Infeasibility and structural bias in Differential Evolution", "comments": "Journal paper, 30 pages plus further extended results made available\n  online at www.cse.dmu.ac.uk/~fcaraf00/BIAS/BIAS_DE_EXTENDED_RESULTS.pdf", "journal-ref": null, "doi": "10.1016/j.ins.2019.05.019", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper thoroughly investigates a range of popular DE configurations to\nidentify components responsible for the emergence of structural bias - recently\nidentified tendency of the algorithm to prefer some regions of the search space\nfor reasons directly unrelated to the objective function values. Such tendency\nwas already studied in GA and PSO where a connection was established between\nthe strength of structural bias and population sizes and potential weaknesses\nof these algorithms was highlighted. For DE, this study goes further and\nextends the range of aspects that can contribute to presence of structural bias\nby including algorithmic component which is usually overlooked - constraint\nhandling technique. A wide range of DE configurations were subjected to the\nprotocol for testing for bias. Results suggest that triggering mechanism for\nthe bias in DE differs to the one previously found for GA and PSO - no clear\ndependency on population size exists. Setting of DE parameters is based on a\nseparate study which on its own leads to interesting directions of new\nresearch. Overall, DE turned out to be robust against structural bias - only\nDE/current-to-best/1/bin is clearly biased but this effect is mitigated by the\nuse of penalty constraint handling technique.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 09:53:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Caraffini", "Fabio", ""], ["Kononova", "Anna V.", ""], ["Corne", "David", ""]]}, {"id": "1901.06240", "submitter": "Ajinkya Gorad", "authors": "Ajinkya Gorad, Vivek Saraswat and Udayan Ganguly", "title": "Predicting Performance using Approximate State Space Model for Liquid\n  State Machines", "comments": "Submitted to IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid State Machine (LSM) is a brain-inspired architecture used for solving\nproblems like speech recognition and time series prediction. LSM comprises of a\nrandomly connected recurrent network of spiking neurons. This network\npropagates the non-linear neuronal and synaptic dynamics. Maass et al. have\nargued that the non-linear dynamics of LSMs is essential for its performance as\na universal computer. Lyapunov exponent (mu), used to characterize the\n\"non-linearity\" of the network, correlates well with LSM performance. We\npropose a complementary approach of approximating the LSM dynamics with a\nlinear state space representation. The spike rates from this model are well\ncorrelated to the spike rates from LSM. Such equivalence allows the extraction\nof a \"memory\" metric (tau_M) from the state transition matrix. tau_M displays\nhigh correlation with performance. Further, high tau_M system require lesser\nepochs to achieve a given accuracy. Being computationally cheap (1800x time\nefficient compared to LSM), the tau_M metric enables exploration of the vast\nparameter design space. We observe that the performance correlation of the\ntau_M surpasses the Lyapunov exponent (mu), (2-4x improvement) in the\nhigh-performance regime over multiple datasets. In fact, while mu increases\nmonotonically with network activity, the performance reaches a maxima at a\nspecific activity described in literature as the \"edge of chaos\". On the other\nhand, tau_M remains correlated with LSM performance even as mu increases\nmonotonically. Hence, tau_M captures the useful memory of network activity that\nenables LSM performance. It also enables rapid design space exploration and\nfine-tuning of LSM parameters for high performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 14:12:40 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gorad", "Ajinkya", ""], ["Saraswat", "Vivek", ""], ["Ganguly", "Udayan", ""]]}, {"id": "1901.06274", "submitter": "Jyoti Prakash Singh", "authors": "Sunil Saumya, Jyoti Prakash Singh, Abdullah Mohammed Baabdullah,\n  Nripendra P. Rana, Yogesh k. Dwivedi", "title": "Ranking Online Consumer Reviews", "comments": null, "journal-ref": "Electronic Commerce Research and Applications, 2018", "doi": "10.1016/j.elerap.2018.03.008", "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The product reviews are posted online in the hundreds and even in the\nthousands for some popular products. Handling such a large volume of\ncontinuously generated online content is a challenging task for buyers,\nsellers, and even researchers. The purpose of this study is to rank the\noverwhelming number of reviews using their predicted helpfulness score. The\nhelpfulness score is predicted using features extracted from review text data,\nproduct description data and customer question-answer data of a product using\nrandom-forest classifier and gradient boosting regressor. The system is made to\nclassify the reviews into low or high quality by random-forest classifier. The\nhelpfulness score of the high-quality reviews is only predicted using gradient\nboosting regressor. The helpfulness score of the low-quality reviews is not\ncalculated because they are never going to be in the top k reviews. They are\njust added at the end of the review list to the review-listing website. The\nproposed system provides fair review placement on review listing pages and\nmaking all high-quality reviews visible to customers on the top. The\nexperimental results on data from two popular Indian e-commerce websites\nvalidate our claim, as 3-4 new high-quality reviews are placed in the top ten\nreviews along with 5-6 old reviews based on review helpfulness. Our findings\nindicate that inclusion of features from product description data and customer\nquestion-answer data improves the prediction accuracy of the helpfulness score.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 13:33:21 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Saumya", "Sunil", ""], ["Singh", "Jyoti Prakash", ""], ["Baabdullah", "Abdullah Mohammed", ""], ["Rana", "Nripendra P.", ""], ["Dwivedi", "Yogesh k.", ""]]}, {"id": "1901.06401", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Slim LSTM networks: LSTM_6 and LSTM_C6", "comments": "6 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have shown previously that our parameter-reduced variants of Long\nShort-Term Memory (LSTM) Recurrent Neural Networks (RNN) are comparable in\nperformance to the standard LSTM RNN on the MNIST dataset. In this study, we\nshow that this is also the case for two diverse benchmark datasets, namely, the\nreview sentiment IMDB and the 20 Newsgroup datasets. Specifically, we focus on\ntwo of the simplest variants, namely LSTM_6 (i.e., standard LSTM with three\nconstant fixed gates) and LSTM_C6 (i.e., LSTM_6 with further reduced cell body\ninput block). We demonstrate that these two aggressively reduced-parameter\nvariants are competitive with the standard LSTM when hyper-parameters, e.g.,\nlearning parameter, number of hidden units and gate constants are set properly.\nThese architectures enable speeding up training computations and hence, these\nnetworks would be more suitable for online training and inference onto portable\ndevices with relatively limited computational resources.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 19:36:41 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1901.06566", "submitter": "Shaeke Salman", "authors": "Shaeke Salman and Xiuwen Liu", "title": "Overfitting Mechanism and Avoidance in Deep Neural Networks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assisted by the availability of data and high performance computing, deep\nlearning techniques have achieved breakthroughs and surpassed human performance\nempirically in difficult tasks, including object recognition, speech\nrecognition, and natural language processing. As they are being used in\ncritical applications, understanding underlying mechanisms for their successes\nand limitations is imperative. In this paper, we show that overfitting, one of\nthe fundamental issues in deep neural networks, is due to continuous gradient\nupdating and scale sensitiveness of cross entropy loss. By separating samples\ninto correctly and incorrectly classified ones, we show that they behave very\ndifferently, where the loss decreases in the correct ones and increases in the\nincorrect ones. Furthermore, by analyzing dynamics during training, we propose\na consensus-based classification algorithm that enables us to avoid overfitting\nand significantly improve the classification accuracy especially when the\nnumber of training samples is limited. As each trained neural network depends\non extrinsic factors such as initial values as well as training data, requiring\nconsensus among multiple models reduces extrinsic factors substantially; for\nstatistically independent models, the reduction is exponential. Compared to\nensemble algorithms, the proposed algorithm avoids overgeneralization by not\nclassifying ambiguous inputs. Systematic experimental results demonstrate the\neffectiveness of the proposed algorithm. For example, using only 1000 training\nsamples from MNIST dataset, the proposed algorithm achieves 95% accuracy,\nsignificantly higher than any of the individual models, with 90% of the test\nsamples classified.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 18:08:55 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Salman", "Shaeke", ""], ["Liu", "Xiuwen", ""]]}, {"id": "1901.06610", "submitter": "Luis Fred", "authors": "Jader Abreu, Luis Fred, David Mac\\^edo, Cleber Zanchettin", "title": "Hierarchical Attentional Hybrid Neural Networks for Document\n  Classification", "comments": "Paper accepted at International Conference on Artificial Neural\n  Networks - ICANN 2019", "journal-ref": "2019 International Conference on Artificial Neural Networks\n  (ICANN)", "doi": "10.1007/978-3-030-30493-5_39", "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Document classification is a challenging task with important applications.\nThe deep learning approaches to the problem have gained much attention\nrecently. Despite the progress, the proposed models do not incorporate the\nknowledge of the document structure in the architecture efficiently and not\ntake into account the contexting importance of words and sentences. In this\npaper, we propose a new approach based on a combination of convolutional neural\nnetworks, gated recurrent units, and attention mechanisms for document\nclassification tasks. The main contribution of this work is the use of\nconvolution layers to extract more meaningful, generalizable and abstract\nfeatures by the hierarchical representation. The proposed method in this paper\nimproves the results of the current attention-based approaches for document\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 01:48:43 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 17:49:18 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Abreu", "Jader", ""], ["Fred", "Luis", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1901.06775", "submitter": "Jack Collins", "authors": "Jack Collins, Ben Cottier and David Howard", "title": "Comparing Direct and Indirect Representations for Environment-Specific\n  Robot Component Design", "comments": "8 pages submitted to the 2019 IEEE CONGRESS ON EVOLUTIONARY\n  COMPUTATION (Under Review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two representations used to define the morphology of legs for a\nhexapod robot, which are subsequently 3D printed. A leg morphology occupies a\nset of voxels in a voxel grid. One method, a direct representation, uses a\ncollection of Bezier splines. The second, an indirect method, utilises\nCPPN-NEAT. In our first experiment, we investigate two strategies to\npost-process the CPPN output and ensure leg length constraints are met. The\nfirst uses an adaptive threshold on the output neuron, the second, previously\nreported in the literature, scales the largest generated artefact to our\ndesired length. In our second experiment, we build on our past work that\nevolves the tibia of a hexapod to provide environment-specific performance\nbenefits. We compare the performance of our direct and indirect legs across\nthree distinct environments, represented in a high-fidelity simulator. Results\nare significant and support our hypothesis that the indirect representation\nallows for further exploration of the design space leading to improved fitness.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 02:56:08 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Collins", "Jack", ""], ["Cottier", "Ben", ""], ["Howard", "David", ""]]}, {"id": "1901.06834", "submitter": "Mahmoud Salamati", "authors": "Mahmoud Salamati, Sadegh Soudjani and Rupak Majumdar", "title": "Perception-in-the-Loop Adversarial Examples", "comments": "13 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable, black box, perception-in-the-loop technique to find\nadversarial examples for deep neural network classifiers. Black box means that\nour procedure only has input-output access to the classifier, and not to the\ninternal structure, parameters, or intermediate confidence values.\nPerception-in-the-loop means that the notion of proximity between inputs can be\ndirectly queried from human participants rather than an arbitrarily chosen\nmetric. Our technique is based on covariance matrix adaptation evolution\nstrategy (CMA-ES), a black box optimization approach. CMA-ES explores the\nsearch space iteratively in a black box manner, by generating populations of\ncandidates according to a distribution, choosing the best candidates according\nto a cost function, and updating the posterior distribution to favor the best\ncandidates. We run CMA-ES using human participants to provide the fitness\nfunction, using the insight that the choice of best candidates in CMA-ES can be\nnaturally modeled as a perception task: pick the top $k$ inputs perceptually\nclosest to a fixed input. We empirically demonstrate that finding adversarial\nexamples is feasible using small populations and few iterations. We compare the\nperformance of CMA-ES on the MNIST benchmark with other black-box approaches\nusing $L_p$ norms as a cost function, and show that it performs favorably both\nin terms of success in finding adversarial examples and in minimizing the\ndistance between the original and the adversarial input. In experiments on the\nMNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find\nperceptually similar adversarial inputs with a small number of iterations and\nsmall population sizes when using perception-in-the-loop. Finally, we show that\nnetworks trained specifically to be robust against $L_\\infty$ norm can still be\nsusceptible to perceptually similar adversarial examples.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 09:09:35 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Salamati", "Mahmoud", ""], ["Soudjani", "Sadegh", ""], ["Majumdar", "Rupak", ""]]}, {"id": "1901.07066", "submitter": "Zhiwen Zuo", "authors": "Zhiwen Zuo, Lei Zhao, Liwen Zuo, Feng Jiang, Wei Xing, Dongming Lu", "title": "On Compression of Unsupervised Neural Nets by Pruning Weak Connections", "comments": "This paper needs to be further revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep\nBelif Networks(DBNs), are powerful in automatic feature extraction,unsupervised\nweight initialization and density estimation. In this paper,we demonstrate that\nthe parameters of these neural nets can be dramatically reduced without\naffecting their performance. We describe a method to reduce the parameters\nrequired by RBM which is the basic building block for deep architectures.\nFurther we propose an unsupervised sparse deep architectures selection\nalgorithm to form sparse deep neural networks.Experimental results show that\nthere is virtually no loss in either generative or discriminative performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:19:27 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 17:05:28 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 14:19:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zuo", "Zhiwen", ""], ["Zhao", "Lei", ""], ["Zuo", "Liwen", ""], ["Jiang", "Feng", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "1901.07298", "submitter": "Jimmy Gaudreault", "authors": "Jimmy Gaudreault, Arunabh Saxena and Hideaki Shimazaki", "title": "Online Estimation of Multiple Dynamic Graphs in Pattern Sequences", "comments": "8 pages, 4 figures v2: IJCNN 2019, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequences of correlated binary patterns can represent many time-series data\nincluding text, movies, and biological signals. These patterns may be described\nby weighted combinations of a few dominant structures that underpin specific\ninteractions among the binary elements. To extract the dominant correlation\nstructures and their contributions to generating data in a time-dependent\nmanner, we model the dynamics of binary patterns using the state-space model of\nan Ising-type network that is composed of multiple undirected graphs. We\nprovide a sequential Bayes algorithm to estimate the dynamics of weights on the\ngraphs while gaining the graph structures online. This model can uncover\noverlapping graphs underlying the data better than a traditional orthogonal\ndecomposition method, and outperforms an original time-dependent Ising model.\nWe assess the performance of the method by simulated data, and demonstrate that\nspontaneous activity of cultured hippocampal neurons is represented by dynamics\nof multiple graphs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:05:53 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 03:38:11 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Gaudreault", "Jimmy", ""], ["Saxena", "Arunabh", ""], ["Shimazaki", "Hideaki", ""]]}, {"id": "1901.07589", "submitter": "Christoph Adami", "authors": "Ali Tehrani-Saleh and Christoph Adami (Michigan State University)", "title": "Can Transfer Entropy Infer Information Flow in Neuronal Circuits for\n  Cognitive Processing?", "comments": "16 pages, 7 figures. Significantly changed version including new\n  results. Title changed", "journal-ref": null, "doi": "10.3390/e22040385", "report-no": null, "categories": "cs.NE nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To infer information flow in any network of agents, it is important first and\nforemost to establish causal temporal relations between the nodes. Practical\nand automated methods that can infer causality are difficult to find, and the\nsubject of ongoing research. While Shannon information only detects\ncorrelation, there are several information-theoretic notions of \"directed\ninformation\" that have successfully detected causality in some systems, in\nparticular in the neuroscience community. However, recent work has shown that\nsome directed information measures can sometimes inadequately estimate the\nextent of causal relations, or even fail to identify existing cause-effect\nrelations between components of systems, especially if neurons contribute in a\ncryptographic manner to influence the effector neuron. Here, we test how often\ncryptographic logic emerges in an evolutionary process that generates\nartificial neural circuits for two fundamental cognitive tasks: motion\ndetection and sound localization. We also test whether activity time-series\nrecorded from behaving digital brains can infer information flow using the\ntransfer entropy concept, when compared to a ground-truth model of causal\ninfluence constructed from connectivity and circuit logic. Our results suggest\nthat transfer entropy will sometimes fail to infer causality when it exists,\nand sometimes suggest a causal connection when there is none. However, the\nextent of incorrect inference strongly depends on the cognitive task\nconsidered. These results emphasize the importance of understanding the\nfundamental logic processes that contribute to information flow in cognitive\nprocessing, and quantifying their relevance in any given nervous system.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 19:19:11 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 04:20:26 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Tehrani-Saleh", "Ali", "", "Michigan State University"], ["Adami", "Christoph", "", "Michigan State University"]]}, {"id": "1901.07714", "submitter": "Li Li", "authors": "Li Li, Minjie Fan, Rishabh Singh, Patrick Riley", "title": "Neural-Guided Symbolic Regression with Asymptotic Constraints", "comments": null, "journal-ref": "NeurIPS 2019 Workshop on Knowledge Representation & Reasoning\n  Meets Machine Learning", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression is a type of discrete optimization problem that involves\nsearching expressions that fit given data points. In many cases, other\nmathematical constraints about the unknown expression not only provide more\ninformation beyond just values at some inputs, but also effectively constrain\nthe search space. We identify the asymptotic constraints of leading polynomial\npowers as the function approaches zero and infinity as useful constraints and\ncreate a system to use them for symbolic regression. The first part of the\nsystem is a conditional production rule generating neural network which\npreferentially generates production rules to construct expressions with the\ndesired leading powers, producing novel expressions outside the training\ndomain. The second part, which we call Neural-Guided Monte Carlo Tree Search,\nuses the network during a search to find an expression that conforms to a set\nof data points and desired leading powers. Lastly, we provide an extensive\nexperimental validation on thousands of target expressions showing the efficacy\nof our system compared to exiting methods for finding unknown functions outside\nof the training set.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 04:15:10 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 02:51:25 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Li", "Li", ""], ["Fan", "Minjie", ""], ["Singh", "Rishabh", ""], ["Riley", "Patrick", ""]]}, {"id": "1901.07718", "submitter": "Edward Frady", "authors": "E. Paxon Frady and Friedrich T. Sommer", "title": "Robust computation with rhythmic spike patterns", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information coding by precise timing of spikes can be faster and more\nenergy-efficient than traditional rate coding. However, spike-timing codes are\noften brittle, which has limited their use in theoretical neuroscience and\ncomputing applications. Here, we propose a novel type of attractor neural\nnetwork in complex state space, and show how it can be leveraged to construct\nspiking neural networks with robust computational properties through a\nphase-to-timing mapping. Building on Hebbian neural associative memories, like\nHopfield networks, we first propose threshold phasor associative memory (TPAM)\nnetworks. Complex phasor patterns whose components can assume continuous-valued\nphase angles and binary magnitudes can be stored and retrieved as stable fixed\npoints in the network dynamics. TPAM achieves high memory capacity when storing\nsparse phasor patterns, and we derive the energy function that governs its\nfixed point attractor dynamics. Second, through simulation experiments we show\nhow the complex algebraic computations in TPAM can be approximated by a\nbiologically plausible network of integrate-and-fire neurons with synaptic\ndelays and recurrently connected inhibitory interneurons. The fixed points of\nTPAM in the complex domain are commensurate with stable periodic states of\nprecisely timed spiking activity that are robust to perturbation. The link\nestablished between rhythmic firing patterns and complex attractor dynamics has\nimplications for the interpretation of spike patterns seen in neuroscience, and\ncan serve as a framework for computation in emerging neuromorphic devices.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 04:31:04 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Frady", "E. Paxon", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "1901.07766", "submitter": "Yu Ji", "authors": "Yu Ji, Zixin Liu, Xing Hu, Peiqi Wang, Youhui Zhang", "title": "Programmable Neural Network Trojan for Pre-Trained Feature Extractor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) trojaning attack is an emerging and important attack\nmodel that can broadly damage the system deployed with NN models. Existing\nstudies have explored the outsourced training attack scenario and transfer\nlearning attack scenario in some small datasets for specific domains, with\nlimited numbers of fixed target classes. In this paper, we propose a more\npowerful trojaning attack method for both outsourced training attack and\ntransfer learning attack, which outperforms existing studies in the capability,\ngenerality, and stealthiness. First, The attack is programmable that the\nmalicious misclassification target is not fixed and can be generated on demand\neven after the victim's deployment. Second, our trojan attack is not limited in\na small domain; one trojaned model on a large-scale dataset can affect\napplications of different domains that reuse its general features. Thirdly, our\ntrojan design is hard to be detected or eliminated even if the victims\nfine-tune the whole model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:18:48 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Ji", "Yu", ""], ["Liu", "Zixin", ""], ["Hu", "Xing", ""], ["Wang", "Peiqi", ""], ["Zhang", "Youhui", ""]]}, {"id": "1901.07871", "submitter": "Patrick Spettel", "authors": "Patrick Spettel and Hans-Georg Beyer", "title": "Analysis of the $(\\mu/\\mu_I,\\lambda)$-CSA-ES with Repair by Projection\n  Applied to a Conically Constrained Problem", "comments": "This is a PREPRINT of an article that has been accepted for\n  publication in the journal MIT Press Evolutionary Computation (ECJ). 25 pages\n  + supplementary material. The work was supported by the Austrian Science Fund\n  FWF under grant P29651-N32", "journal-ref": null, "doi": "10.1162/evco_a_00261", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical analyses of evolution strategies are indispensable for gaining a\ndeep understanding of their inner workings. For constrained problems, rather\nsimple problems are of interest in the current research. This work presents a\ntheoretical analysis of a multi-recombinative evolution strategy with\ncumulative step size adaptation applied to a conically constrained linear\noptimization problem. The state of the strategy is modeled by random variables\nand a stochastic iterative mapping is introduced. For the analytical treatment,\nfluctuations are neglected and the mean value iterative system is considered.\nNon-linear difference equations are derived based on one-generation progress\nrates. Based on that, expressions for the steady state of the mean value\niterative system are derived. By comparison with real algorithm runs, it is\nshown that for the considered assumptions, the theoretical derivations are able\nto predict the dynamics and the steady state values of the real runs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 13:32:06 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 12:58:20 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Spettel", "Patrick", ""], ["Beyer", "Hans-Georg", ""]]}, {"id": "1901.07927", "submitter": "Sara Mandelli", "authors": "Sara Mandelli, Vincenzo Lipari, Paolo Bestagini, Stefano Tubaro", "title": "Interpolation and Denoising of Seismic Data using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic data processing algorithms greatly benefit from regularly sampled and\nreliable data. Therefore, interpolation and denoising play a fundamental role\nas one of the starting steps of most seismic processing workflows. We exploit\nconvolutional neural networks for the joint tasks of interpolation and random\nnoise attenuation of 2D common shot gathers. Inspired by the great\ncontributions achieved in image processing and computer vision, we investigate\na particular architecture of convolutional neural network referred to as U-net,\nwhich implements a convolutional autoencoder able to describe the complex\nfeatures of clean and regularly sampled data for reconstructing the corrupted\nones. In training phase we exploit part of the data for tailoring the network\nto the specific tasks of interpolation, denoising and joint\ndenoising/interpolation, while during the system deployment we are able to\nrecover the remaining corrupted shot gathers in a computationally efficient\nprocedure. We consider a plurality of data corruptions in our numerical\nexperiments, including different noise models and different distributions of\nmissing traces. Several examples on synthetic and field data illustrate the\nappealing features of the aforementioned strategy. Comparative examples show\nimprovements with respect to recently proposed solutions for joint denoising\nand interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 14:47:06 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 10:49:18 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 10:00:01 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 09:31:09 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mandelli", "Sara", ""], ["Lipari", "Vincenzo", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1901.08013", "submitter": "Fei Qi", "authors": "Fei Qi, Zhaohui Xia, Gaoyang Tang, Hang Yang, Yu Song, Guangrui Qian,\n  Xiong An, Chunhuan Lin, Guangming Shi", "title": "DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine\n  Learning", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging field, Automated Machine Learning (AutoML) aims to reduce or\neliminate manual operations that require expertise in machine learning. In this\npaper, a graph-based architecture is employed to represent flexible\ncombinations of ML models, which provides a large searching space compared to\ntree-based and stacking-based architectures. Based on this, an evolutionary\nalgorithm is proposed to search for the best architecture, where the mutation\nand heredity operators are the key for architecture evolution. With Bayesian\nhyper-parameter optimization, the proposed approach can automate the workflow\nof machine learning. On the PMLB dataset, the proposed approach shows the\nstate-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn.\nSome of the optimized models are with complex structures which are difficult to\nobtain in manual design.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:42:41 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Qi", "Fei", ""], ["Xia", "Zhaohui", ""], ["Tang", "Gaoyang", ""], ["Yang", "Hang", ""], ["Song", "Yu", ""], ["Qian", "Guangrui", ""], ["An", "Xiong", ""], ["Lin", "Chunhuan", ""], ["Shi", "Guangming", ""]]}, {"id": "1901.08263", "submitter": "Peiqi Wang", "authors": "Peiqi Wang, Dongsheng Wang, Yu Ji, Xinfeng Xie, Haoxuan Song, XuXin\n  Liu, Yongqiang Lyu, Yuan Xie", "title": "QGAN: Quantized Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensive computation and memory requirements of generative adversarial\nneural networks (GANs) hinder its real-world deployment on edge devices such as\nsmartphones. Despite the success in model reduction of CNNs, neural network\nquantization methods have not yet been studied on GANs, which are mainly faced\nwith the issues of both the effectiveness of quantization algorithms and the\ninstability of training GAN models. In this paper, we start with an extensive\nstudy on applying existing successful methods to quantize GANs. Our observation\nreveals that none of them generates samples with reasonable quality because of\nthe underrepresentation of quantized values in model weights, and the generator\nand discriminator networks show different sensitivities upon quantization\nmethods. Motivated by these observations, we develop a novel quantization\nmethod for GANs based on EM algorithms, named as QGAN. We also propose a\nmulti-precision algorithm to help find the optimal number of bits of quantized\nGAN models in conjunction with corresponding result qualities. Experiments on\nCIFAR-10 and CelebA show that QGAN can quantize GANs to even 1-bit or 2-bit\nrepresentations with results of quality comparable to original models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 07:40:30 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Wang", "Peiqi", ""], ["Wang", "Dongsheng", ""], ["Ji", "Yu", ""], ["Xie", "Xinfeng", ""], ["Song", "Haoxuan", ""], ["Liu", "XuXin", ""], ["Lyu", "Yongqiang", ""], ["Xie", "Yuan", ""]]}, {"id": "1901.08296", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky", "title": "Deep Learning on Attributed Graphs: A Journey from Graphs to Their\n  Embeddings and Back", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is a powerful concept for representation of relations between pairs\nof entities. Data with underlying graph structure can be found across many\ndisciplines and there is a natural desire for understanding such data better.\nDeep learning (DL) has achieved significant breakthroughs in a variety of\nmachine learning tasks in recent years, especially where data is structured on\na grid, such as in text, speech, or image understanding. However, surprisingly\nlittle has been done to explore the applicability of DL on arbitrary\ngraph-structured data directly.\n  The goal of this thesis is to investigate architectures for DL on graphs and\nstudy how to transfer, adapt or generalize concepts that work well on\nsequential and image data to this domain. We concentrate on two important\nprimitives: embedding graphs or their nodes into a continuous vector space\nrepresentation (encoding) and, conversely, generating graphs from such vectors\nback (decoding). To that end, we make the following contributions.\n  First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like\noperation on graphs performed in the spatial domain where filters are\ndynamically generated based on edge attributes. The method is used to encode\ngraphs with arbitrary and varying structure.\n  Second, we propose SuperPoint Graph, an intermediate point cloud\nrepresentation with rich edge attributes encoding the contextual relationship\nbetween object parts. Based on this representation, ECC is employed to segment\nlarge-scale point clouds without major sacrifice in fine details.\n  Third, we present GraphVAE, a graph generator allowing us to decode graphs\nwith variable but upper-bounded number of nodes making use of approximate graph\nmatching for aligning the predictions of an autoencoder with its inputs. The\nmethod is applied to the task of molecule generation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 09:12:33 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Simonovsky", "Martin", ""]]}, {"id": "1901.08455", "submitter": "Li Yue", "authors": "Li Yue and Zhao Weibin and Shang Lin", "title": "Really should we pruning after model be totally trained? Pruning based\n  on a small amount of training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training of models in pruning algorithms plays an important role in\npruning decision-making. We find that excessive pre-training is not necessary\nfor pruning algorithms. According to this idea, we propose a pruning\nalgorithm---Incremental pruning based on less training (IPLT). Compared with\nthe traditional pruning algorithm based on a large number of pre-training, IPLT\nhas competitive compression effect than the traditional pruning algorithm under\nthe same simple pruning strategy. On the premise of ensuring accuracy, IPLT can\nachieve 8x-9x compression for VGG-19 on CIFAR-10 and only needs to pre-train\nfew epochs. For VGG-19 on CIFAR-10, we can not only achieve 10 times test\nacceleration, but also about 10 times training acceleration. At present, the\nresearch mainly focuses on the compression and acceleration in the application\nstage of the model, while the compression and acceleration in the training\nstage are few. We newly proposed a pruning algorithm that can compress and\naccelerate in the training stage. It is novel to consider the amount of\npre-training required by pruning algorithm. Our results have implications: Too\nmuch pre-training may be not necessary for pruning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 15:30:54 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Yue", "Li", ""], ["Weibin", "Zhao", ""], ["Lin", "Shang", ""]]}, {"id": "1901.08584", "submitter": "Wei Hu", "authors": "Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruosong Wang", "title": "Fine-Grained Analysis of Optimization and Generalization for\n  Overparameterized Two-Layer Neural Networks", "comments": "In ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have cast some light on the mystery of why deep nets fit any\ndata and generalize despite being very overparametrized. This paper analyzes\ntraining and generalization for a simple 2-layer ReLU net with random\ninitialization, and provides the following improvements over recent works:\n  (i) Using a tighter characterization of training speed than recent papers, an\nexplanation for why training a neural net with random labels leads to slower\ntraining, as originally observed in [Zhang et al. ICLR'17].\n  (ii) Generalization bound independent of network size, using a data-dependent\ncomplexity measure. Our measure distinguishes clearly between random labels and\ntrue labels on MNIST and CIFAR, as shown by experiments. Moreover, recent\npapers require sample complexity to increase (slowly) with the size, while our\nsample complexity is completely independent of the network size.\n  (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets\ntrained via gradient descent.\n  The key idea is to track dynamics of training and generalization via\nproperties of a related kernel.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:58:16 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 04:22:57 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Arora", "Sanjeev", ""], ["Du", "Simon S.", ""], ["Hu", "Wei", ""], ["Li", "Zhiyuan", ""], ["Wang", "Ruosong", ""]]}, {"id": "1901.08619", "submitter": "Hassen Dhrif", "authors": "Hassen Dhrif, Luis G. Sanchez Giraldo, Miroslav Kubat, Stefan Wuchty", "title": "A Stable Combinatorial Particle Swarm Optimization for Scalable Feature\n  Selection in Gene Expression Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary computation (EC) algorithms, such as discrete and\nmulti-objective versions of particle swarm optimization (PSO), have been\napplied to solve the Feature selection (FS) problem, tackling the combinatorial\nexplosion of search spaces that are peppered with local minima. Furthermore,\nhigh-dimensional FS problems such as finding a small set of biomarkers to make\na diagnostic call add an additional challenge as such methods ability to pick\nout the most important features must remain unchanged in decision spaces of\nincreasing dimensions and presence of irrelevant features. We developed a\ncombinatorial PSO algorithm, called COMB-PSO, that scales up to\nhigh-dimensional gene expression data while still selecting the smallest\nsubsets of genes that allow reliable classification of samples. In particular,\nCOMB-PSO enhances the encoding, speed of convergence, control of divergence and\ndiversity of the conventional PSO algorithm, balancing exploration and\nexploitation of the search space. Applying our approach on real gene expression\ndata of different cancers, COMB-PSO finds gene sets of smallest size that allow\na reliable classification of the underlying disease classes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 19:30:43 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Dhrif", "Hassen", ""], ["Giraldo", "Luis G. Sanchez", ""], ["Kubat", "Miroslav", ""], ["Wuchty", "Stefan", ""]]}, {"id": "1901.08644", "submitter": "Richard Meyes", "authors": "Richard Meyes, Melanie Lu, Constantin Waubert de Puiseau, Tobias\n  Meisen", "title": "Ablation Studies in Artificial Neural Networks", "comments": "19 pages, 23 figures, intention to submit as a conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ablation studies have been widely used in the field of neuroscience to tackle\ncomplex biological systems such as the extensively studied Drosophila central\nnervous system, the vertebrate brain and more interestingly and most\ndelicately, the human brain. In the past, these kinds of studies were utilized\nto uncover structure and organization in the brain, i.e. a mapping of features\ninherent to external stimuli onto different areas of the neocortex. considering\nthe growth in size and complexity of state-of-the-art artificial neural\nnetworks (ANNs) and the corresponding growth in complexity of the tasks that\nare tackled by these networks, the question arises whether ablation studies may\nbe used to investigate these networks for a similar organization of their inner\nrepresentations. In this paper, we address this question and performed two\nablation studies in two fundamentally different ANNs to investigate their inner\nrepresentations of two well-known benchmark datasets from the computer vision\ndomain. We found that features distinct to the local and global structure of\nthe data are selectively represented in specific parts of the network.\nFurthermore, some of these representations are redundant, awarding the network\na certain robustness to structural damages. We further determined the\nimportance of specific parts of the network for the classification task solely\nbased on the weight structure of single units. Finally, we examined the ability\nof damaged networks to recover from the consequences of ablations by means of\nrecovery training. We argue that ablations studies are a feasible method to\ninvestigate knowledge representations in ANNs and are especially helpful to\nexamine a networks robustness to structural damages, a feature of ANNs that\nwill become increasingly important for future safety-critical applications.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 21:11:59 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 09:39:19 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Meyes", "Richard", ""], ["Lu", "Melanie", ""], ["de Puiseau", "Constantin Waubert", ""], ["Meisen", "Tobias", ""]]}, {"id": "1901.09002", "submitter": "Tai Sing Lee", "authors": "Jielin Qiu, Ge Huang, Tai Sing Lee", "title": "A Neurally-Inspired Hierarchical Prediction Network for Spatiotemporal\n  Sequence Learning and Prediction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we developed a hierarchical network model, called Hierarchical\nPrediction Network (HPNet), to understand how spatiotemporal memories might be\nlearned and encoded in the recurrent circuits in the visual cortical hierarchy\nfor predicting future video frames. This neurally inspired model operates in\nthe analysis-by-synthesis framework. It contains a feed-forward path that\ncomputes and encodes spatiotemporal features of successive complexity and a\nfeedback path for the successive levels to project their interpretations to the\nlevel below. Within each level, the feed-forward path and the feedback path\nintersect in a recurrent gated circuit, instantiated in a LSTM module, to\ngenerate a prediction or explanation of the incoming signals. The network\nlearns its internal model of the world by minimizing the errors of its\nprediction of the incoming signals at each level of the hierarchy. We found\nthat hierarchical interaction in the network increases semantic clustering of\nglobal movement patterns in the population codes of the units along the\nhierarchy, even in the earliest module. This facilitates the learning of\nrelationships among movement patterns, yielding state-of-the-art performance in\nlong range video sequence predictions in the benchmark datasets. The network\nmodel automatically reproduces a variety of prediction suppression and\nfamiliarity suppression neurophysiological phenomena observed in the visual\ncortex, suggesting that hierarchical prediction might indeed be an important\nprinciple for representational learning in the visual cortex.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 18:03:17 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Qiu", "Jielin", ""], ["Huang", "Ge", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1901.09006", "submitter": "Martin Engelcke", "authors": "Edward Wagstaff, Fabian B. Fuchs, Martin Engelcke, Ingmar Posner,\n  Michael Osborne", "title": "On the Limitations of Representing Functions on Sets", "comments": "Published at the International Conference on Machine Learning (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on the representation of functions on sets has considered the use\nof summation in a latent space to enforce permutation invariance. In\nparticular, it has been conjectured that the dimension of this latent space may\nremain fixed as the cardinality of the sets under consideration increases.\nHowever, we demonstrate that the analysis leading to this conjecture requires\nmappings which are highly discontinuous and argue that this is only of limited\npractical use. Motivated by this observation, we prove that an implementation\nof this model via continuous mappings (as provided by e.g. neural networks or\nGaussian processes) actually imposes a constraint on the dimensionality of the\nlatent space. Practical universal function representation for set inputs can\nonly be achieved with a latent dimension at least the size of the maximum\nnumber of input elements.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 18:11:52 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 10:12:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wagstaff", "Edward", ""], ["Fuchs", "Fabian B.", ""], ["Engelcke", "Martin", ""], ["Posner", "Ingmar", ""], ["Osborne", "Michael", ""]]}, {"id": "1901.09049", "submitter": "Guillaume Bellec", "authors": "Guillaume Bellec, Franz Scherr, Elias Hajek, Darjan Salaj, Robert\n  Legenstein, Wolfgang Maass", "title": "Biologically inspired alternatives to backpropagation through time for\n  learning in recurrent neural nets", "comments": "We changed in this version 2 of the paper the name of the new\n  learning algorithms to e-prop, corrected minor errors, added details --\n  especially for resulting new rules for synaptic plasticity, edited the\n  notation, and included new results for TIMIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way how recurrently connected networks of spiking neurons in the brain\nacquire powerful information processing capabilities through learning has\nremained a mystery. This lack of understanding is linked to a lack of learning\nalgorithms for recurrent networks of spiking neurons (RSNNs) that are both\nfunctionally powerful and can be implemented by known biological mechanisms.\nSince RSNNs are simultaneously a primary target for implementations of\nbrain-inspired circuits in neuromorphic hardware, this lack of algorithmic\ninsight also hinders technological progress in that area. The gold standard for\nlearning in recurrent neural networks in machine learning is back-propagation\nthrough time (BPTT), which implements stochastic gradient descent with regard\nto a given loss function. But BPTT is unrealistic from a biological\nperspective, since it requires a transmission of error signals backwards in\ntime and in space, i.e., from post- to presynaptic neurons. We show that an\nonline merging of locally available information during a computation with\nsuitable top-down learning signals in real-time provides highly capable\napproximations to BPTT. For tasks where information on errors arises only late\nduring a network computation, we enrich locally available information through\nfeedforward eligibility traces of synapses that can easily be computed in an\nonline manner. The resulting new generation of learning algorithms for\nrecurrent neural networks provides a new understanding of network learning in\nthe brain that can be tested experimentally. In addition, these algorithms\nprovide efficient methods for on-chip training of RSNNs in neuromorphic\nhardware.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 19:07:36 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 17:15:18 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Bellec", "Guillaume", ""], ["Scherr", "Franz", ""], ["Hajek", "Elias", ""], ["Salaj", "Darjan", ""], ["Legenstein", "Robert", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1901.09135", "submitter": "Alexander Wong", "authors": "Zhong Qiu Lin and Alexander Wong", "title": "Progressive Label Distillation: Learning Input-Efficient Deep Neural\n  Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the focus in the area of knowledge distillation has been on\ndistilling knowledge from a larger teacher network to a smaller student\nnetwork. However, there has been little research on how the concept of\ndistillation can be leveraged to distill the knowledge encapsulated in the\ntraining data itself into a reduced form. In this study, we explore the concept\nof progressive label distillation, where we leverage a series of\nteacher-student network pairs to progressively generate distilled training data\nfor learning deep neural networks with greatly reduced input dimensions. To\ninvestigate the efficacy of the proposed progressive label distillation\napproach, we experimented with learning a deep limited vocabulary speech\nrecognition network based on generated 500ms input utterances distilled\nprogressively from 1000ms source training data, and demonstrated a significant\nincrease in test accuracy of almost 78% compared to direct learning.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 01:22:14 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Lin", "Zhong Qiu", ""], ["Wong", "Alexander", ""]]}, {"id": "1901.09181", "submitter": "Shiwei Liu", "authors": "Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram\n  Matavalam, Yulong Pei, Mykola Pechenizkiy", "title": "Sparse evolutionary Deep Learning with over one million artificial\n  neurons on commodity hardware", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) have emerged as hot topics in the research\ncommunity. Despite the success of ANNs, it is challenging to train and deploy\nmodern ANNs on commodity hardware due to the ever-increasing model size and the\nunprecedented growth in the data volumes. Particularly for microarray data, the\nvery-high dimensionality and the small number of samples make it difficult for\nmachine learning techniques to handle. Furthermore, specialized hardware such\nas Graphics Processing Unit (GPU) is expensive. Sparse neural networks are the\nleading approaches to address these challenges. However, off-the-shelf sparsity\ninducing techniques either operate from a pre-trained model or enforce the\nsparse structure via binary masks. The training efficiency of sparse neural\nnetworks cannot be obtained practically. In this paper, we introduce a\ntechnique allowing us to train truly sparse neural networks with fixed\nparameter count throughout training. Our experimental results demonstrate that\nour method can be applied directly to handle high dimensional data, while\nachieving higher accuracy than the traditional two phases approaches. Moreover,\nwe have been able to create truly sparse MultiLayer Perceptrons (MLPs) models\nwith over one million neurons and to train them on a typical laptop without\nGPU, this being way beyond what is possible with any state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 09:14:01 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 17:27:54 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 19:17:33 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2021 01:02:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Shiwei", ""], ["Mocanu", "Decebal Constantin", ""], ["Matavalam", "Amarsagar Reddy Ramapuram", ""], ["Pei", "Yulong", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1901.09208", "submitter": "Shiwei Liu", "authors": "Shiwei Liu, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "Intrinsically Sparse Long Short-Term Memory Networks", "comments": "9 pages, 8 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) has achieved state-of-the-art performances on a\nwide range of tasks. Its outstanding performance is guaranteed by the long-term\nmemory ability which matches the sequential data perfectly and the gating\nstructure controlling the information flow. However, LSTMs are prone to be\nmemory-bandwidth limited in realistic applications and need an unbearable\nperiod of training and inference time as the model size is ever-increasing. To\ntackle this problem, various efficient model compression methods have been\nproposed. Most of them need a big and expensive pre-trained model which is a\nnightmare for resource-limited devices where the memory budget is strictly\nlimited. To remedy this situation, in this paper, we incorporate the Sparse\nEvolutionary Training (SET) procedure into LSTM, proposing a novel model dubbed\nSET-LSTM. Rather than starting with a fully-connected architecture, SET-LSTM\nhas a sparse topology and dramatically fewer parameters in both phases,\ntraining and inference. Considering the specific architecture of LSTMs, we\nreplace the LSTM cells and embedding layers with sparse structures and further\non, use an evolutionary strategy to adapt the sparse connectivity to the data.\nAdditionally, we find that SET-LSTM can provide many different good\ncombinations of sparse connectivity to substitute the overparameterized\noptimization problem of dense neural networks. Evaluated on four sentiment\nanalysis classification datasets, the results demonstrate that our proposed\nmodel is able to achieve usually better performance than its fully connected\ncounterpart while having less than 4\\% of its parameters.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 13:17:27 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Liu", "Shiwei", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1901.09292", "submitter": "Najwa Kouka", "authors": "Najwa Kouka, Raja Fdhila and Adel M. Alimi", "title": "Multi Objective Particle Swarm Optimization based Cooperative Agents\n  with Automated Negotiation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-70093-9_28", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper investigates a new hybridization of multi-objective particle swarm\noptimization (MOPSO) and cooperative agents (MOPSO-CA) to handle the problem of\nstagnation encounters in MOPSO, which leads solutions to trap in local optima.\nThe proposed approach involves a new distribution strategy based on the idea of\nhaving a set of a sub-population, each of which is processed by one agent. The\nnumber of the sub-population and agents are adjusted dynamically through the\nPareto ranking. This method allocates a dynamic number of sub-population as\nrequired to improve diversity in the search space. Additionally, agents are\nused for better management for the exploitation within a sub-population, and\nfor exploration among sub-populations. Furthermore, we investigate the\nautomated negotiation within agents in order to share the best knowledge. To\nvalidate our approach, several benchmarks are performed. The results show that\nthe introduced variant ensures the trade-off between the exploitation and\nexploration with respect to the comparative algorithms\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 00:27:56 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kouka", "Najwa", ""], ["Fdhila", "Raja", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1901.09491", "submitter": "Stanislav Fort", "authors": "Stanislav Fort, Pawe{\\l} Krzysztof Nowak, Stanislaw Jastrzebski, Srini\n  Narayanan", "title": "Stiffness: A New Perspective on Generalization in Neural Networks", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new perspective on generalization of neural\nnetworks by proposing and investigating the concept of a neural network\nstiffness. We measure how stiff a network is by looking at how a small gradient\nstep in the network's parameters on one example affects the loss on another\nexample. Higher stiffness suggests that a network is learning features that\ngeneralize. In particular, we study how stiffness depends on 1) class\nmembership, 2) distance between data points in the input space, 3) training\niteration, and 4) learning rate. We present experiments on MNIST, FASHION\nMNIST, and CIFAR-10/100 using fully-connected and convolutional neural\nnetworks, as well as on a transformer-based NLP model. We demonstrate the\nconnection between stiffness and generalization, and observe its dependence on\nlearning rate. When training on CIFAR-100, the stiffness matrix exhibits a\ncoarse-grained behavior indicative of the model's awareness of super-class\nmembership. In addition, we measure how stiffness between two data points\ndepends on their mutual input-space distance, and establish the concept of a\ndynamical critical length -- a distance below which a parameter update based on\na data point influences its neighbors.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 02:49:46 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 14:29:53 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 23:33:57 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Fort", "Stanislav", ""], ["Nowak", "Pawe\u0142 Krzysztof", ""], ["Jastrzebski", "Stanislaw", ""], ["Narayanan", "Srini", ""]]}, {"id": "1901.09614", "submitter": "Doyun Kim", "authors": "Doyun Kim, Kyoung-Young Kim, Sangsoo Ko, Sanghyuck Ha", "title": "A Simple Method to Reduce Off-chip Memory Accesses on Convolutional\n  Neural Networks", "comments": "9 pages, 10 figures, under review (by ICML2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For convolutional neural networks, a simple algorithm to reduce off-chip\nmemory accesses is proposed by maximally utilizing on-chip memory in a neural\nprocess unit. Especially, the algorithm provides an effective way to process a\nmodule which consists of multiple branches and a merge layer. For Inception-V3\non Samsung's NPU in Exynos, our evaluation shows that the proposed algorithm\nmakes off-chip memory accesses reduced by 1/50, and accordingly achieves 97.59\n% reduction in the amount of feature-map data to be transferred from/to\noff-chip memory.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 11:43:25 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kim", "Doyun", ""], ["Kim", "Kyoung-Young", ""], ["Ko", "Sangsoo", ""], ["Ha", "Sanghyuck", ""]]}, {"id": "1901.09821", "submitter": "David Mac\\^edo", "authors": "Andr\\'ea B. Duque, Lu\\~a L\\'azaro J. Santos, David Mac\\^edo, Cleber\n  Zanchettin", "title": "Squeezed Very Deep Convolutional Neural Networks for Text Classification", "comments": null, "journal-ref": "2019 International Conference on Artificial Neural Networks\n  (ICANN)", "doi": "10.1007/978-3-030-30487-4_16", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the research in convolutional neural networks has focused on\nincreasing network depth to improve accuracy, resulting in a massive number of\nparameters which restricts the trained network to platforms with memory and\nprocessing constraints. We propose to modify the structure of the Very Deep\nConvolutional Neural Networks (VDCNN) model to fit mobile platforms constraints\nand keep performance. In this paper, we evaluate the impact of Temporal\nDepthwise Separable Convolutions and Global Average Pooling in the network\nparameters, storage size, and latency. The squeezed model (SVDCNN) is between\n10x and 20x smaller, depending on the network depth, maintaining a maximum size\nof 6MB. Regarding accuracy, the network experiences a loss between 0.4% and\n1.3% and obtains lower latencies compared to the baseline model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 17:14:12 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Duque", "Andr\u00e9a B.", ""], ["Santos", "Lu\u00e3 L\u00e1zaro J.", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1901.09904", "submitter": "Yu Ji", "authors": "Yu Ji, Youyang Zhang, Xinfeng Xie, Shuangchen Li, Peiqi Wang, Xing Hu,\n  Youhui Zhang, Yuan Xie", "title": "FPSA: A Full System Stack Solution for Reconfigurable ReRAM-based NN\n  Accelerator Architecture", "comments": "Accepted by ASPLOS 2019", "journal-ref": null, "doi": "10.1145/3297858.3304048", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Network (NN) accelerators with emerging ReRAM (resistive random access\nmemory) technologies have been investigated as one of the promising solutions\nto address the \\textit{memory wall} challenge, due to the unique capability of\n\\textit{processing-in-memory} within ReRAM-crossbar-based processing elements\n(PEs). However, the high efficiency and high density advantages of ReRAM have\nnot been fully utilized due to the huge communication demands among PEs and the\noverhead of peripheral circuits.\n  In this paper, we propose a full system stack solution, composed of a\nreconfigurable architecture design, Field Programmable Synapse Array (FPSA) and\nits software system including neural synthesizer, temporal-to-spatial mapper,\nand placement & routing. We highly leverage the software system to make the\nhardware design compact and efficient. To satisfy the high-performance\ncommunication demand, we optimize it with a reconfigurable routing architecture\nand the placement & routing tool. To improve the computational density, we\ngreatly simplify the PE circuit with the spiking schema and then adopt neural\nsynthesizer to enable the high density computation-resources to support\ndifferent kinds of NN operations. In addition, we provide spiking memory blocks\n(SMBs) and configurable logic blocks (CLBs) in hardware and leverage the\ntemporal-to-spatial mapper to utilize them to balance the storage and\ncomputation requirements of NN. Owing to the end-to-end software system, we can\nefficiently deploy existing deep neural networks to FPSA. Evaluations show\nthat, compared to one of state-of-the-art ReRAM-based NN accelerators, PRIME,\nthe computational density of FPSA improves by 31x; for representative NNs, its\ninference performance can achieve up to 1000x speedup.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 03:06:38 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ji", "Yu", ""], ["Zhang", "Youyang", ""], ["Xie", "Xinfeng", ""], ["Li", "Shuangchen", ""], ["Wang", "Peiqi", ""], ["Hu", "Xing", ""], ["Zhang", "Youhui", ""], ["Xie", "Yuan", ""]]}, {"id": "1901.09948", "submitter": "Emre Neftci", "authors": "Emre O. Neftci, Hesham Mostafa, Friedemann Zenke", "title": "Surrogate Gradient Learning in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks are nature's versatile solution to fault-tolerant and\nenergy efficient signal processing. To translate these benefits into hardware,\na growing number of neuromorphic spiking neural network processors attempt to\nemulate biological neural networks. These developments have created an imminent\nneed for methods and tools to enable such systems to solve real-world signal\nprocessing problems. Like conventional neural networks, spiking neural networks\ncan be trained on real, domain specific data. However, their training requires\novercoming a number of challenges linked to their binary and dynamical nature.\nThis article elucidates step-by-step the problems typically encountered when\ntraining spiking neural networks, and guides the reader through the key\nconcepts of synaptic plasticity and data-driven learning in the spiking\nsetting. To that end, it gives an overview of existing approaches and provides\nan introduction to surrogate gradient methods, specifically, as a particularly\nflexible and efficient method to overcome the aforementioned challenges.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:13:55 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 16:24:45 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Neftci", "Emre O.", ""], ["Mostafa", "Hesham", ""], ["Zenke", "Friedemann", ""]]}, {"id": "1901.09972", "submitter": "David Mac\\^edo", "authors": "Jefferson L. P. Lima, David Mac\\^edo, Cleber Zanchettin", "title": "Heartbeat Anomaly Detection using Adversarial Oversampling", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN.2019.8852242", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular diseases are one of the most common causes of death in the\nworld. Prevention, knowledge of previous cases in the family, and early\ndetection is the best strategy to reduce this fact. Different machine learning\napproaches to automatic diagnostic are being proposed to this task. As in most\nhealth problems, the imbalance between examples and classes is predominant in\nthis problem and affects the performance of the automated solution. In this\npaper, we address the classification of heartbeats images in different\ncardiovascular diseases. We propose a two-dimensional Convolutional Neural\nNetwork for classification after using a InfoGAN architecture for generating\nsynthetic images to unbalanced classes. We call this proposal Adversarial\nOversampling and compare it with the classical oversampling methods as SMOTE,\nADASYN, and RandomOversampling. The results show that the proposed approach\nimproves the classifier performance for the minority classes without harming\nthe performance in the balanced classes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:55:42 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Lima", "Jefferson L. P.", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1901.10277", "submitter": "Samuli Laine", "authors": "Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila", "title": "High-Quality Self-Supervised Deep Image Denoising", "comments": "NeurIPS 2019 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:37:16 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 09:59:59 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 12:36:21 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Laine", "Samuli", ""], ["Karras", "Tero", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "1901.10397", "submitter": "Marko Angjelichinoski", "authors": "Marko Angjelichinoski, Taposh Banerjee, John Choi, Bijan Pesaran,\n  Vahid Tarokh", "title": "Minimax-optimal decoding of movement goals from local field potentials\n  using complex spectral features", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting eye movement goals from local field\npotentials (LFP) recorded through a multielectrode array in the macaque\nprefrontal cortex. The monkey is tasked with performing memory-guided saccades\nto one of eight targets during which LFP activity is recorded and used to train\na decoder. Previous reports have mainly relied on the spectral amplitude of the\nLFPs as a feature in the decoding step to limited success, while neglecting the\nphase without proper theoretical justification. This paper formulates the\nproblem of decoding eye movement intentions in a statistically optimal\nframework and uses Gaussian sequence modeling and Pinsker's theorem to generate\nminimax-optimal estimates of the LFP signals which are later used as features\nin the decoding step. The approach is shown to act as a low-pass filter and\neach LFP in the feature space is represented via its complex Fourier\ncoefficients after appropriate shrinking such that higher frequency components\nare attenuated; this way, the phase information inherently present in the LFP\nsignal is naturally embedded into the feature space. The proposed complex\nspectrum-based decoder achieves prediction accuracy of up to $94\\%$ at\nsuperficial electrode depths near the surface of the prefrontal cortex, which\nmarks a significant performance improvement over conventional power\nspectrum-based decoders.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 17:08:25 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Angjelichinoski", "Marko", ""], ["Banerjee", "Taposh", ""], ["Choi", "John", ""], ["Pesaran", "Bijan", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1901.10603", "submitter": "Charles Frye", "authors": "Charles G. Frye, Neha S. Wadia, Michael R. DeWeese, and Kristofer E.\n  Bouchard", "title": "Numerically Recovering the Critical Points of a Deep Linear Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerically locating the critical points of non-convex surfaces is a\nlong-standing problem central to many fields. Recently, the loss surfaces of\ndeep neural networks have been explored to gain insight into outstanding\nquestions in optimization, generalization, and network architecture design.\nHowever, the degree to which recently-proposed methods for numerically\nrecovering critical points actually do so has not been thoroughly evaluated. In\nthis paper, we examine this issue in a case for which the ground truth is\nknown: the deep linear autoencoder. We investigate two sub-problems associated\nwith numerical critical point identification: first, because of large parameter\ncounts, it is infeasible to find all of the critical points for contemporary\nneural networks, necessitating sampling approaches whose characteristics are\npoorly understood; second, the numerical tolerance for accurately identifying a\ncritical point is unknown, and conservative tolerances are difficult to\nsatisfy. We first identify connections between recently-proposed methods and\nwell-understood methods in other fields, including chemical physics, economics,\nand algebraic geometry. We find that several methods work well at recovering\ncertain information about loss surfaces, but fail to take an unbiased sample of\ncritical points. Furthermore, numerical tolerance must be very strict to ensure\nthat numerically-identified critical points have similar properties to true\nanalytical critical points. We also identify a recently-published Newton method\nfor optimization that outperforms previous methods as a critical point-finding\nalgorithm. We expect our results will guide future attempts to numerically\nstudy critical points in large nonlinear neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 23:05:54 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Frye", "Charles G.", ""], ["Wadia", "Neha S.", ""], ["DeWeese", "Michael R.", ""], ["Bouchard", "Kristofer E.", ""]]}, {"id": "1901.10723", "submitter": "Martha Lewis", "authors": "Martha Lewis", "title": "Compositionality for Recursive Neural Networks", "comments": "presented at NeSy2018, Thirteenth International Workshop on\n  Neural-Symbolic Learning and Reasoning, co-located with Human-Level AI 2018,\n  Prague, CZ, August 23-24, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling compositionality has been a longstanding area of research in the\nfield of vector space semantics. The categorical approach to compositionality\nmaps grammar onto vector spaces in a principled way, but comes under fire for\nrequiring the formation of very high-dimensional matrices and tensors, and\ntherefore being computationally infeasible. In this paper I show how a linear\nsimplification of recursive neural tensor network models can be mapped directly\nonto the categorical approach, giving a way of computing the required matrices\nand tensors. This mapping suggests a number of lines of research for both\ncategorical compositional vector space models of meaning and for recursive\nneural network models of compositionality.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 09:32:51 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Lewis", "Martha", ""]]}, {"id": "1901.10738", "submitter": "Jean-Yves Franceschi", "authors": "Jean-Yves Franceschi (MLIA), Aymeric Dieuleveut (CMAP), Martin Jaggi", "title": "Unsupervised Scalable Representation Learning for Multivariate Time\n  Series", "comments": null, "journal-ref": "Thirty-third Conference on Neural Information Processing Systems,\n  Neural Information Processing Systems Foundation, Dec 2019, Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series constitute a challenging data type for machine learning\nalgorithms, due to their highly variable lengths and sparse labeling in\npractice. In this paper, we tackle this challenge by proposing an unsupervised\nmethod to learn universal embeddings of time series. Unlike previous works, it\nis scalable with respect to their length and we demonstrate the quality,\ntransferability and practicability of the learned representations with thorough\nexperiments and comparisons. To this end, we combine an encoder based on causal\ndilated convolutions with a novel triplet loss employing time-based negative\nsampling, obtaining general-purpose representations for variable length and\nmultivariate time series.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 10:07:45 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 14:45:05 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 13:38:25 GMT"}, {"version": "v4", "created": "Fri, 3 Jan 2020 14:22:57 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Franceschi", "Jean-Yves", "", "MLIA"], ["Dieuleveut", "Aymeric", "", "CMAP"], ["Jaggi", "Martin", ""]]}, {"id": "1901.10798", "submitter": "Doron Friedman", "authors": "Ori Tal and Doron Friedman", "title": "Recurrent Neural Networks for P300-based BCI", "comments": null, "journal-ref": "Int'l BCI Conf Graz, Austria, 2017", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P300-based spellers are one of the main methods for EEG-based brain-computer\ninterface, and the detection of the P300 target event with high accuracy is an\nimportant prerequisite. The rapid serial visual presentation (RSVP) protocol is\nof high interest because it can be used by patients who have lost control over\ntheir eyes. In this study we wish to explore the suitability of recurrent\nneural networks (RNNs) as a machine learning method for identifying the P300\nsignal in RSVP data. We systematically compare RNN with alternative methods\nsuch as linear discriminant analysis (LDA) and convolutional neural network\n(CNN). Our results indicate that LDA performs as well as the neural network\nmodels or better on single subject data, but a network combining CNN and RNN\nhas advantages when transferring learning among subejcts, and is significantly\nmore resilient to temporal noise than other methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:04:31 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Tal", "Ori", ""], ["Friedman", "Doron", ""]]}, {"id": "1901.10826", "submitter": "David Mac\\^edo", "authors": "Jo\\~ao Ant\\^onio Chagas Nunes, David Mac\\^edo, Cleber Zanchettin", "title": "Additive Margin SincNet for Speaker Recognition", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN.2019.8852112", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker Recognition is a challenging task with essential applications such as\nauthentication, automation, and security. The SincNet is a new deep learning\nbased model which has produced promising results to tackle the mentioned task.\nTo train deep learning systems, the loss function is essential to the network\nperformance. The Softmax loss function is a widely used function in deep\nlearning methods, but it is not the best choice for all kind of problems. For\ndistance-based problems, one new Softmax based loss function called Additive\nMargin Softmax (AM-Softmax) is proving to be a better choice than the\ntraditional Softmax. The AM-Softmax introduces a margin of separation between\nthe classes that forces the samples from the same class to be closer to each\nother and also maximizes the distance between classes. In this paper, we\npropose a new approach for speaker recognition systems called AM-SincNet, which\nis based on the SincNet but uses an improved AM-Softmax layer. The proposed\nmethod is evaluated in the TIMIT dataset and obtained an improvement of\napproximately 40% in the Frame Error Rate compared to SincNet.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:16:34 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Nunes", "Jo\u00e3o Ant\u00f4nio Chagas", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1901.10860", "submitter": "Karlson Pfannschmidt", "authors": "Karlson Pfannschmidt, Pritha Gupta, Eyke H\\\"ullermeier", "title": "Learning Choice Functions: Concepts and Architectures", "comments": "arXiv admin note: text overlap with arXiv:1803.05796", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning choice functions, which play an important\nrole in various domains of application, most notably in the field of economics.\nFormally, a choice function is a mapping from sets to sets: Given a set of\nchoice alternatives as input, a choice function identifies a subset of most\npreferred elements. Learning choice functions from suitable training data comes\nwith a number of challenges. For example, the sets provided as input and the\nsubsets produced as output can be of any size. Moreover, since the order in\nwhich alternatives are presented is irrelevant, a choice function should be\nsymmetric. Perhaps most importantly, choice functions are naturally\ncontext-dependent, in the sense that the preference in favor of an alternative\nmay depend on what other options are available. We formalize the problem of\nlearning choice functions and present two general approaches based on two\nrepresentations of context-dependent utility functions. Both approaches are\ninstantiated by means of appropriate neural network architectures, and their\nperformance is demonstrated on suitable benchmark tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 12:59:00 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 08:20:39 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 16:37:24 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Pfannschmidt", "Karlson", ""], ["Gupta", "Pritha", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1901.10997", "submitter": "Hongxu Yin", "authors": "Hongxu Yin, Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, and\n  Niraj K. Jha", "title": "Hardware-Guided Symbiotic Training for Compact, Accurate, yet\n  Execution-Efficient LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many long short-term memory (LSTM) applications need fast yet compact models.\nNeural network compression approaches, such as the grow-and-prune paradigm,\nhave proved to be promising for cutting down network complexity by skipping\ninsignificant weights. However, current compression strategies are mostly\nhardware-agnostic and network complexity reduction does not always translate\ninto execution efficiency. In this work, we propose a hardware-guided symbiotic\ntraining methodology for compact, accurate, yet execution-efficient inference\nmodels. It is based on our observation that hardware may introduce substantial\nnon-monotonic behavior, which we call the latency hysteresis effect, when\nevaluating network size vs. inference latency. This observation raises question\nabout the mainstream smaller-dimension-is-better compression strategy, which\noften leads to a sub-optimal model architecture. By leveraging the\nhardware-impacted hysteresis effect and sparsity, we are able to achieve the\nsymbiosis of model compactness and accuracy with execution efficiency, thus\nreducing LSTM latency while increasing its accuracy. We have evaluated our\nalgorithms on language modeling and speech recognition applications. Relative\nto the traditional stacked LSTM architecture obtained for the Penn Treebank\ndataset, we reduce the number of parameters by 18.0x (30.5x) and measured\nrun-time latency by up to 2.4x (5.2x) on Nvidia GPUs (Intel Xeon CPUs) without\nany accuracy degradation. For the DeepSpeech2 architecture obtained for the AN4\ndataset, we reduce the number of parameters by 7.0x (19.4x), word error rate\nfrom 12.9% to 9.9% (10.4%), and measured run-time latency by up to 1.7x (2.4x)\non Nvidia GPUs (Intel Xeon CPUs). Thus, our method yields compact, accurate,\nyet execution-efficient inference models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 18:41:54 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Yin", "Hongxu", ""], ["Chen", "Guoyang", ""], ["Li", "Yingmin", ""], ["Che", "Shuai", ""], ["Zhang", "Weifeng", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1901.11090", "submitter": "David Landaeta", "authors": "David Landaeta", "title": "Neuroevolution with Perceptron Turing Machines", "comments": "Patent pending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the perceptron Turing machine and show how it can be used to\ncreate a system of neuroevolution. Advantages of this approach include\nautomatic scaling of solutions to larger problem sizes, the ability to\nexperiment with hand-coded solutions, and an enhanced potential for\nunderstanding evolved solutions. Hand-coded solutions may be implemented in the\nlow-level language of Turing machines, which is the genotype used in\nneuroevolution, but a high-level language called Lopro is introduced to make\nthe job easier.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 20:37:40 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Landaeta", "David", ""]]}, {"id": "1901.11115", "submitter": "David Landaeta", "authors": "David Landaeta", "title": "Code Farming: A Process for Creating Generic Computational Building\n  Blocks", "comments": "Patent pending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a desire to improve on the current state of the art in genetic\nprogramming, and aided by recent progress in understanding the computational\naspects of evolutionary systems, we describe a process that creates a set of\ngeneric computational building blocks for the purpose of seeding initial\npopulations of programs in any genetic programming system. This provides an\nadvantage over the standard approach of initializing the population purely\nrandomly in that it avoids the need to constantly rediscover such building\nblocks. It is also better than seeding the initial population with hand-coded\nbuilding blocks, since it lessens the amount of human intervention required by\nthe system.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 21:46:42 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 01:49:22 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Landaeta", "David", ""]]}, {"id": "1901.11117", "submitter": "David So", "authors": "David R. So and Chen Liang and Quoc V. Le", "title": "The Evolved Transformer", "comments": "ICML version with SOTA results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent works have highlighted the strength of the Transformer architecture on\nsequence tasks while, at the same time, neural architecture search (NAS) has\nbegun to outperform human-designed models. Our goal is to apply NAS to search\nfor a better alternative to the Transformer. We first construct a large search\nspace inspired by the recent advances in feed-forward sequence models and then\nrun evolutionary architecture search with warm starting by seeding our initial\npopulation with the Transformer. To directly search on the computationally\nexpensive WMT 2014 English-German translation task, we develop the Progressive\nDynamic Hurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models. The architecture found in our experiments\n-- the Evolved Transformer -- demonstrates consistent improvement over the\nTransformer on four well-established language tasks: WMT 2014 English-German,\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\noriginal \"big\" Transformer with 37.6% less parameters and outperforms the\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 22:03:01 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 21:35:28 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 22:23:12 GMT"}, {"version": "v4", "created": "Fri, 17 May 2019 19:47:49 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["So", "David R.", ""], ["Liang", "Chen", ""], ["Le", "Quoc V.", ""]]}, {"id": "1901.11120", "submitter": "Ke Li Kl", "authors": "Ke Li, Zilin Xiang, Kay Chen Tan", "title": "Which Surrogate Works for Empirical Performance Modelling? A Case Study\n  with Differential Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not uncommon that meta-heuristic algorithms contain some intrinsic\nparameters, the optimal configuration of which is crucial for achieving their\npeak performance. However, evaluating the effectiveness of a configuration is\nexpensive, as it involves many costly runs of the target algorithm. Perhaps\nsurprisingly, it is possible to build a cheap-to-evaluate surrogate that models\nthe algorithm's empirical performance as a function of its parameters. Such\nsurrogates constitute an important building block for understanding algorithm\nperformance, algorithm portfolio/selection, and the automatic algorithm\nconfiguration. In principle, many off-the-shelf machine learning techniques can\nbe used to build surrogates. In this paper, we take the differential evolution\n(DE) as the baseline algorithm for proof-of-concept study. Regression models\nare trained to model the DE's empirical performance given a parameter\nconfiguration. In particular, we evaluate and compare four popular regression\nalgorithms both in terms of how well they predict the empirical performance\nwith respect to a particular parameter configuration, and also how well they\napproximate the parameter versus the empirical performance landscapes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 22:13:13 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Li", "Ke", ""], ["Xiang", "Zilin", ""], ["Tan", "Kay Chen", ""]]}, {"id": "1901.11271", "submitter": "Louis Faury", "authors": "Louis Faury, Clement Calauzenes, Olivier Fercoq, Syrine Krichen", "title": "Improving Evolutionary Strategies with Generative Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Strategies (ES) are a popular family of black-box zeroth-order\noptimization algorithms which rely on search distributions to efficiently\noptimize a large variety of objective functions. This paper investigates the\npotential benefits of using highly flexible search distributions in classical\nES algorithms, in contrast to standard ones (typically Gaussians). We model\nsuch distributions with Generative Neural Networks (GNNs) and introduce a new\ntraining algorithm that leverages their expressiveness to accelerate the ES\nprocedure. We show that this tailored algorithm can readily incorporate\nexisting ES algorithms, and outperforms the state-of-the-art on diverse\nobjective functions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:00:33 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Faury", "Louis", ""], ["Calauzenes", "Clement", ""], ["Fercoq", "Olivier", ""], ["Krichen", "Syrine", ""]]}]