[{"id": "1301.0048", "submitter": "Yukihiro Kamada", "authors": "Yukihiro Kamada, Kiyonori Miyasaki", "title": "Generating High-Order Threshold Functions with Multiple Thresholds", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider situations in which a given logical function is\nrealized by a multithreshold threshold function. In such situations, constant\nfunctions can be easily obtained from multithreshold threshold functions, and\ntherefore, we can show that it becomes possible to optimize a class of\nhigh-order neural networks. We begin by proposing a generating method for\nthreshold functions in which we use a vector that determines the boundary\nbetween the linearly separable function and the high-order threshold function.\nBy applying this method to high-order threshold functions, we show that\nfunctions with the same weight as, but a different threshold than, a threshold\nfunction generated by the generation process can be easily obtained. We also\nshow that the order of the entire network can be extended while maintaining the\nstructure of given functions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 02:20:31 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Kamada", "Yukihiro", ""], ["Miyasaki", "Kiyonori", ""]]}, {"id": "1301.0254", "submitter": "Andrew Clark", "authors": "Andrew Clark", "title": "Group theory, group actions, evolutionary algorithms, and global\n  optimization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.DS math.OC math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use group, action and orbit to understand how evolutionary\nsolve nonconvex optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 23:04:01 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2013 02:07:32 GMT"}, {"version": "v3", "created": "Fri, 3 May 2013 16:05:32 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Clark", "Andrew", ""]]}, {"id": "1301.0785", "submitter": "Dilip Aldar", "authors": "Dilip S Aldar", "title": "Adaptive Intelligent Cooperative Spectrum Sensing In Cognitive Radio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio Spectrum is most precious and scarce resource and must be utilized\nefficiently and effectively. Cognitive radio is the promising solutions for the\noptimum utilization of the scared natural resource. The spectrum owned by the\nprimary user should be shared among the secondary user, but primary user should\nnot be interfered by the secondary user. In order to utilize the primary user\nspectrum, secondary user must detect accurately, the existence of primary in\nthe band of interest. In cooperative spectrum sensing, the channel between the\nsecondary users and the cognitive radio base station is non stationary and\ncauses interference in the decision in decision fusion and in information in\ninformation due to multipath fading. In this paper neural network based\ncooperative spectrum sensing method is proposed, the performance of proposed\nmethod is evaluated and observed that, the neural network based scheme\nperformance improve significantly over the AND,OR and Majority rule\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2013 18:08:30 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Aldar", "Dilip S", ""]]}, {"id": "1301.0929", "submitter": "Iztok Fister", "authors": "Iztok Fister, Marjan Mernik, Janez Brest", "title": "Hybridization of Evolutionary Algorithms", "comments": null, "journal-ref": "Iztok Fister, Marjan Mernik and Janez Brest (2011). Hybridization\n  of Evolutionary Algorithms, Evolutionary Algorithms, Eisuke Kita (Ed.), ISBN:\n  978-953-307-171-8, InTech", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms are good general problem solver but suffer from a\nlack of domain specific knowledge. However, the problem specific knowledge can\nbe added to evolutionary algorithms by hybridizing. Interestingly, all the\nelements of the evolutionary algorithms can be hybridized. In this chapter, the\nhybridization of the three elements of the evolutionary algorithms is\ndiscussed: the objective function, the survivor selection operator and the\nparameter settings. As an objective function, the existing heuristic function\nthat construct the solution of the problem in traditional way is used. However,\nthis function is embedded into the evolutionary algorithm that serves as a\ngenerator of new solutions. In addition, the objective function is improved by\nlocal search heuristics. The new neutral selection operator has been developed\nthat is capable to deal with neutral solutions, i.e. solutions that have the\ndifferent representation but expose the equal values of objective function. The\naim of this operator is to directs the evolutionary search into a new\nundiscovered regions of the search space. To avoid of wrong setting of\nparameters that control the behavior of the evolutionary algorithm, the\nself-adaptation is used. Finally, such hybrid self-adaptive evolutionary\nalgorithm is applied to the two real-world NP-hard problems: the graph\n3-coloring and the optimization of markers in the clothing industry. Extensive\nexperiments shown that these hybridization improves the results of the\nevolutionary algorithms a lot. Furthermore, the impact of the particular\nhybridizations is analyzed in details as well.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2013 18:38:14 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Fister", "Iztok", ""], ["Mernik", "Marjan", ""], ["Brest", "Janez", ""]]}, {"id": "1301.0930", "submitter": "Saptarshi Das", "authors": "Sayan Saha, Saptarshi Das, Anindya Pakhira, Sumit Mukherjee, Indranil\n  Pan", "title": "Comparative Studies on Decentralized Multiloop PID Controller Design\n  Using Evolutionary Algorithms", "comments": "6 pages, 9 figures", "journal-ref": "Engineering and Systems (SCES), 2012 Students Conference on;\n  Allahabad, March 2012", "doi": "10.1109/SCES.2012.6199122", "report-no": null, "categories": "cs.SY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized PID controllers have been designed in this paper for\nsimultaneous tracking of individual process variables in multivariable systems\nunder step reference input. The controller design framework takes into account\nthe minimization of a weighted sum of Integral of Time multiplied Squared Error\n(ITSE) and Integral of Squared Controller Output (ISCO) so as to balance the\noverall tracking errors for the process variables and required variation in the\ncorresponding manipulated variables. Decentralized PID gains are tuned using\nthree popular Evolutionary Algorithms (EAs) viz. Genetic Algorithm (GA),\nEvolutionary Strategy (ES) and Cultural Algorithm (CA). Credible simulation\ncomparisons have been reported for four benchmark 2x2 multivariable processes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2013 18:45:01 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Saha", "Sayan", ""], ["Das", "Saptarshi", ""], ["Pakhira", "Anindya", ""], ["Mukherjee", "Sumit", ""], ["Pan", "Indranil", ""]]}, {"id": "1301.0939", "submitter": "Iztok Fister", "authors": "Iztok Fister, Marjan Mernik, Bogdan Filipi\\v{c}", "title": "Graph 3-coloring with a hybrid self-adaptive evolutionary algorithm", "comments": null, "journal-ref": null, "doi": "10.1007/s10589-012-9496-5", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hybrid self-adaptive evolutionary algorithm for graph\ncoloring that is hybridized with the following novel elements: heuristic\ngenotype-phenotype mapping, a swap local search heuristic, and a neutral\nsurvivor selection operator. This algorithm was compared with the evolutionary\nalgorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertz\nand de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. The\nperformance of these algorithms were tested on a test suite consisting of\nrandomly generated 3-colorable graphs of various structural features, such as\ngraph size, type, edge density, and variability in sizes of color classes.\nFurthermore, the test graphs were generated including the phase transition\nwhere the graphs are hard to color. The purpose of the extensive experimental\nwork was threefold: to investigate the behavior of the tested algorithms in the\nphase transition, to identify what impact hybridization with the DSatur\ntraditional heuristic has on the evolutionary algorithm, and to show how graph\nstructural features influence the performance of the graph-coloring algorithms.\nThe results indicate that the performance of the hybrid self-adaptive\nevolutionary algorithm is comparable with, or better than, the performance of\nthe hybrid evolutionary algorithm which is one of the best graph-coloring\nalgorithms today. Moreover, the fact that all the considered algorithms\nperformed poorly on flat graphs confirms that this type of graphs is really the\nhardest to color.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2013 20:11:43 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Fister", "Iztok", ""], ["Mernik", "Marjan", ""], ["Filipi\u010d", "Bogdan", ""]]}, {"id": "1301.1555", "submitter": "Amir Hesam Salavati", "authors": "Amin Karbasi, Amir Hesam Salavati, and Amin Shokrollahi", "title": "Coupled Neural Associative Memories", "comments": "A shorter version of this paper is going to be submitted to\n  International symposium on Information Theory (ISIT 2013) in Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture to design a neural associative memory that is\ncapable of learning a large number of patterns and recalling them later in\npresence of noise. It is based on dividing the neurons into local clusters and\nparallel plains, very similar to the architecture of the visual cortex of\nmacaque brain. The common features of our proposed architecture with those of\nspatially-coupled codes enable us to show that the performance of such networks\nin eliminating noise is drastically better than the previous approaches while\nmaintaining the ability of learning an exponentially large number of patterns.\nPrevious work either failed in providing good performance during the recall\nphase or in offering large pattern retrieval (storage) capacities. We also\npresent computational experiments that lend additional support to the\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 14:55:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 22:57:53 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2013 12:48:16 GMT"}, {"version": "v4", "created": "Tue, 21 May 2013 15:02:19 GMT"}, {"version": "v5", "created": "Fri, 23 Aug 2013 14:26:16 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Karbasi", "Amin", ""], ["Salavati", "Amir Hesam", ""], ["Shokrollahi", "Amin", ""]]}, {"id": "1301.2638", "submitter": "Tina Yu", "authors": "Tina Yu, Dave Wilkinson, Julian Clark and Morgan Sullivan", "title": "Computational Intelligence for Deepwater Reservoir Depositional\n  Environments Interpretation", "comments": null, "journal-ref": "Journal of Natural Gas Science & Engineering, Volume 3, Issue 6,\n  pages 716-728, Elsevier, 2011", "doi": "10.1016/j.jngse.2011.07.014", "report-no": null, "categories": "cs.NE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting oil recovery efficiency of a deepwater reservoir is a challenging\ntask. One approach to characterize a deepwater reservoir and to predict its\nproducibility is by analyzing its depositional information. This research\nproposes a deposition-based stratigraphic interpretation framework for\ndeepwater reservoir characterization. In this framework, one critical task is\nthe identification and labeling of the stratigraphic components in the\nreservoir, according to their depositional environments. This interpretation\nprocess is labor intensive and can produce different results depending on the\nstratigrapher who performs the analysis. To relieve stratigrapher's workload\nand to produce more consistent results, we have developed a novel methodology\nto automate this process using various computational intelligence techniques.\nUsing a well log data set, we demonstrate that the developed methodology and\nthe designed workflow can produce finite state transducer models that interpret\ndeepwater reservoir depositional environments adequately.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 01:23:48 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Yu", "Tina", ""], ["Wilkinson", "Dave", ""], ["Clark", "Julian", ""], ["Sullivan", "Morgan", ""]]}, {"id": "1301.2959", "submitter": "Jean Piniello", "authors": "Jean Piniello", "title": "New elements for a network (including brain) general theory during\n  learning period", "comments": "The Version 3 (in english) brought relevant improvements to Version 2\n  (in french). The Version 4 is similar to Version 3, except from the fact that\n  it includes REFERENCES BIBLIOGRAPHY. The Version 5 brings some considerations\n  about ARTIFICIAL INTELLIGENCE", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.NE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study deals with the evolution of the so called 'intelligent' networks\n(insect society without leader, cells of an organism, brain,...) during their\nlearning period. First we summarize briefly the Version 2 (published in\nFrench), whose the main characteristics are: 1) A network connected to its\nenvironment is considered as immersed into an information field created by this\nenvironment which so dictates to it the learning constraints. 2) The used\nformalism draws one's inspiration from the one of the Quantum field theory\n(Principle of stationary action, gauge fields, invariance by symmetry\ntransformations,...). 3) We obtain Lagrange equations whose solutions describe\nthe network evolution during the whole learning period. 4) Then, while\nproceeding with the same formalism inspiration, we suggest other study ways\ncapable of evolving the knowledge in the considered scope. In a second part,\nafter a reminder of the points to be improved, we exhibit the Version 5 which\nbrings, we think, relevant improvements. Indeed: 5) We consider the weighted\naverages of the variables; this introduces probabilities. 6) We define two\nobservables (L average of information flux and A activity of the network) which\ncould be measured and so be compared with experimental results. 7) We find that\nL , weighted average of information flows, is an invariant. 8) Finally, we\npropose two expressions for the conactance, from which we deduce the\ncorresponding Lagrange equations which have to be solved to know the evolution\nof the considered weighted averages. But, at the present stage, we think that\nwe can progress only by carrying out experiments (see projects like Human brain\nproject) and discovering invariants, symmetries which would allow us, like in\nPhysics, to classify networks and above all to understand better the\nconnections between them. Indeed, and that is what we propose among the future\nresearch ways, the underlying problem is to understand how, after their\nlearning period, several networks can connect together to produce, in the brain\ncase for instance, what we call mental states.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 12:58:14 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 09:11:44 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 08:05:49 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Piniello", "Jean", ""]]}, {"id": "1301.3195", "submitter": "Zhen Hu", "authors": "Zhen Hu and Kun Fu and Changshui Zhang", "title": "Audio Classical Composer Identification by Deep Neural Network", "comments": "I will update it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Classical Composer Identification (ACC) is an important problem in\nMusic Information Retrieval (MIR) which aims at identifying the composer for\naudio classical music clips. The famous annual competition, Music Information\nRetrieval Evaluation eXchange (MIREX), also takes it as one of the four\ntraining&testing tasks. We built a hybrid model based on Deep Belief Network\n(DBN) and Stacked Denoising Autoencoder (SDA) to identify the composer from\naudio signal. As a matter of copyright, sponsors of MIREX cannot publish their\ndata set. We built a comparable data set to test our model. We got an accuracy\nof 76.26% in our data set which is better than some pure models and shallow\nmodels. We think our method is promising even though we test it in a different\ndata set, since our data set is comparable to that in MIREX by size. We also\nfound that samples from different classes become farther away from each other\nwhen transformed by more layers in our model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 01:25:24 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 12:13:31 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2013 02:02:58 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2013 08:31:34 GMT"}, {"version": "v5", "created": "Thu, 14 Mar 2013 08:51:14 GMT"}, {"version": "v6", "created": "Wed, 27 Mar 2013 06:36:07 GMT"}, {"version": "v7", "created": "Wed, 16 Mar 2016 03:31:56 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Hu", "Zhen", ""], ["Fu", "Kun", ""], ["Zhang", "Changshui", ""]]}, {"id": "1301.3530", "submitter": "Charles Cadieu", "authors": "Charles F. Cadieu, Ha Hong, Dan Yamins, Nicolas Pinto, Najib J. Majaj,\n  James J. DiCarlo", "title": "The Neural Representation Benchmark and its Evaluation on Brain and\n  Machine", "comments": "The v1 version contained incorrectly computed kernel analysis curves\n  and KA-AUC values for V4, IT, and the HT-L3 models. They have been corrected\n  in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for the development of effective learning representations\nis their evaluation and comparison to representations we know to be effective.\nIn natural sensory domains, the community has viewed the brain as a source of\ninspiration and as an implicit benchmark for success. However, it has not been\npossible to directly test representational learning algorithms directly against\nthe representations contained in neural systems. Here, we propose a new\nbenchmark for visual representations on which we have directly tested the\nneural representation in multiple visual cortical areas in macaque (utilizing\ndata from [Majaj et al., 2012]), and on which any computer vision algorithm\nthat produces a feature space can be tested. The benchmark measures the\neffectiveness of the neural or machine representation by computing the\nclassification loss on the ordered eigendecomposition of a kernel matrix\n[Montavon et al., 2011]. In our analysis we find that the neural representation\nin visual area IT is superior to visual area V4. In our analysis of\nrepresentational learning algorithms, we find that three-layer models approach\nthe representational performance of V4 and the algorithm in [Le et al., 2012]\nsurpasses the performance of V4. Impressively, we find that a recent supervised\nalgorithm [Krizhevsky et al., 2012] achieves performance comparable to that of\nIT for an intermediate level of image variation difficulty, and surpasses IT at\na higher difficulty level. We believe this result represents a major milestone:\nit is the first learning algorithm we have found that exceeds our current\nestimate of IT representation performance. We hope that this benchmark will\nassist the community in matching the representational performance of visual\ncortex and will serve as an initial rallying point for further correspondence\nbetween representations derived in brains and machines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 23:42:21 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2013 20:39:46 GMT"}], "update_date": "2013-01-28", "authors_parsed": [["Cadieu", "Charles F.", ""], ["Hong", "Ha", ""], ["Yamins", "Dan", ""], ["Pinto", "Nicolas", ""], ["Majaj", "Najib J.", ""], ["DiCarlo", "James J.", ""]]}, {"id": "1301.3533", "submitter": "Xanadu Halkias", "authors": "Xanadu Halkias, Sebastien Paris, Herve Glotin", "title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "comments": "8 pages, 7 figures (including subfigures), ICleaR conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Belief Networks (DBN) have been successfully applied on popular machine\nlearning tasks. Specifically, when applied on hand-written digit recognition,\nDBNs have achieved approximate accuracy rates of 98.8%. In an effort to\noptimize the data representation achieved by the DBN and maximize their\ndescriptive power, recent advances have focused on inducing sparse constraints\nat each layer of the DBN. In this paper we present a theoretical approach for\nsparse constraints in the DBN using the mixed norm for both non-overlapping and\noverlapping groups. We explore how these constraints affect the classification\naccuracy for digit recognition in three different datasets (MNIST, USPS, RIMES)\nand provide initial estimations of their usefulness by altering different\nparameters such as the group size and overlap percentage.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 00:12:21 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 10:18:15 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Halkias", "Xanadu", ""], ["Paris", "Sebastien", ""], ["Glotin", "Herve", ""]]}, {"id": "1301.3545", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Razvan Pascanu, Aaron Courville and Yoshua\n  Bengio", "title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for\ntraining Boltzmann Machines. Similar in spirit to the Hessian-Free method of\nMartens [8], our algorithm belongs to the family of truncated Newton methods\nand exploits an efficient matrix-vector product to avoid explicitely storing\nthe natural gradient metric $L$. This metric is shown to be the expected second\nderivative of the log-partition function (under the model distribution), or\nequivalently, the variance of the vector of partial derivatives of the energy\nfunction. We evaluate our method on the task of joint-training a 3-layer Deep\nBoltzmann Machine and show that MFNG does indeed have faster per-epoch\nconvergence compared to Stochastic Maximum Likelihood with centering, though\nwall-clock performance is currently not competitive.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 01:40:20 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 16:07:12 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Pascanu", "Razvan", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1301.3557", "submitter": "Matthew Zeiler", "authors": "Matthew D. Zeiler and Rob Fergus", "title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\n  Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and effective method for regularizing large\nconvolutional neural networks. We replace the conventional deterministic\npooling operations with a stochastic procedure, randomly picking the activation\nwithin each pooling region according to a multinomial distribution, given by\nthe activities within the pooling region. The approach is hyper-parameter free\nand can be combined with other regularization approaches, such as dropout and\ndata augmentation. We achieve state-of-the-art performance on four image\ndatasets, relative to other approaches that do not utilize data augmentation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 02:12:07 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Zeiler", "Matthew D.", ""], ["Fergus", "Rob", ""]]}, {"id": "1301.3605", "submitter": "Michael Seltzer", "authors": "Dong Yu, Michael L. Seltzer, Jinyu Li, Jui-Ting Huang, Frank Seide", "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition\n  Tasks", "comments": "ICLR 2013, 9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that deep neural networks (DNNs) perform\nsignificantly better than shallow networks and Gaussian mixture models (GMMs)\non large vocabulary speech recognition tasks. In this paper, we argue that the\nimproved accuracy achieved by the DNNs is the result of their ability to\nextract discriminative internal representations that are robust to the many\nsources of variability in speech signals. We show that these representations\nbecome increasingly insensitive to small perturbations in the input with\nincreasing network depth, which leads to better speech recognition performance\nwith deeper networks. We also show that DNNs cannot extrapolate to test samples\nthat are substantially different from the training examples. If the training\ndata are sufficiently representative, however, internal features learned by the\nDNN are relatively stable with respect to speaker differences, bandwidth\ndifferences, and environment distortion. This enables DNN-based recognizers to\nperform as well or better than state-of-the-art systems based on GMMs or\nshallow networks without the need for explicit model adaptation or feature\nnormalization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 07:23:19 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 07:42:07 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2013 19:42:37 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yu", "Dong", ""], ["Seltzer", "Michael L.", ""], ["Li", "Jinyu", ""], ["Huang", "Jui-Ting", ""], ["Seide", "Frank", ""]]}, {"id": "1301.3641", "submitter": "Ryan Kiros", "authors": "Ryan Kiros", "title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "comments": "11 pages, ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hessian-free (HF) optimization has been successfully used for training deep\nautoencoders and recurrent networks. HF uses the conjugate gradient algorithm\nto construct update directions through curvature-vector products that can be\ncomputed on the same order of time as gradients. In this paper we exploit this\nproperty and study stochastic HF with gradient and curvature mini-batches\nindependent of the dataset size. We modify Martens' HF for these settings and\nintegrate dropout, a method for preventing co-adaptation of feature detectors,\nto guard against overfitting. Stochastic Hessian-free optimization gives an\nintermediary between SGD and HF that achieves competitive performance on both\nclassification and deep autoencoder experiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 10:10:23 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2013 05:51:37 GMT"}, {"version": "v3", "created": "Wed, 1 May 2013 06:57:50 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Kiros", "Ryan", ""]]}, {"id": "1301.3833", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu, Nando de Freitas, Arnaud Doucet", "title": "Reversible Jump MCMC Simulated Annealing for Neural Networks", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-11-18", "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated\nannealing algorithm to optimize radial basis function (RBF) networks. This\nalgorithm enables us to maximize the joint posterior distribution of the\nnetwork parameters and the number of basis functions. It performs a global\nsearch in the joint space of the parameters and number of parameters, thereby\nsurmounting the problem of local minima. We also show that by calibrating a\nBayesian model, we can obtain the classical AIC, BIC and MDL model selection\ncriteria within a penalized likelihood framework. Finally, we show\ntheoretically and empirically that the algorithm converges to the modes of the\nfull posterior distribution in an efficient way.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:48:42 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Andrieu", "Christophe", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1301.4083", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre and Yoshua Bengio", "title": "Knowledge Matters: Importance of Prior Information for Optimization", "comments": "37 Pages, 5 figures, 5 tables JMLR Special Topics on Representation\n  Learning Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We explore the effect of introducing prior information into the intermediate\nlevel of neural networks for a learning task on which all the state-of-the-art\nmachine learning algorithms tested failed to learn. We motivate our work from\nthe hypothesis that humans learn such intermediate concepts from other\nindividuals via a form of supervision or guidance using a curriculum. The\nexperiments we have conducted provide positive evidence in favor of this\nhypothesis. In our experiments, a two-tiered MLP architecture is trained on a\ndataset with 64x64 binary inputs images, each image with three sprites. The\nfinal task is to decide whether all the sprites are the same or one of them is\ndifferent. Sprites are pentomino tetris shapes and they are placed in an image\nwith different locations using scaling and rotation transformations. The first\npart of the two-tiered MLP is pre-trained with intermediate-level targets being\nthe presence of sprites at each location, while the second part takes the\noutput of the first part as input and predicts the final task's target binary\nevent. The two-tiered MLP architecture, with a few tens of thousand examples,\nwas able to learn the task perfectly, whereas all other algorithms (include\nunsupervised pre-training, but also traditional algorithms like SVMs, decision\ntrees and boosting) all perform no better than chance. We hypothesize that the\noptimization difficulty involved when the intermediate pre-training is not\nperformed is due to the {\\em composition} of two highly non-linear tasks. Our\nfindings are also consistent with hypotheses on cultural learning inspired by\nthe observations of optimization problems with deep learning, presumably\nbecause of effective local minima.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 13:06:52 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2013 05:43:57 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2013 17:11:19 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2013 20:13:08 GMT"}, {"version": "v5", "created": "Fri, 15 Mar 2013 05:41:47 GMT"}, {"version": "v6", "created": "Sat, 13 Jul 2013 16:38:36 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["G\u00fcl\u00e7ehre", "\u00c7a\u011flar", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1301.4096", "submitter": "Anton Eremeev", "authors": "Benjamin Doerr, Anton Eremeev, Frank Neumann, Madeleine Theile,\n  Christian Thyssen", "title": "Evolutionary Algorithms and Dynamic Programming", "comments": "This is an updated version of journal publication where few misprints\n  are fixed", "journal-ref": "Theoretical Computer Science, Vol. 412, Issue 43, 2011,\n  P.6020-6035", "doi": "10.1016/j.tcs.2011.07.024", "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been proven that evolutionary algorithms produce good\nresults for a wide range of combinatorial optimization problems. Some of the\nconsidered problems are tackled by evolutionary algorithms that use a\nrepresentation which enables them to construct solutions in a dynamic\nprogramming fashion. We take a general approach and relate the construction of\nsuch algorithms to the development of algorithms using dynamic programming\ntechniques. Thereby, we give general guidelines on how to develop evolutionary\nalgorithms that have the additional ability of carrying out dynamic programming\nsteps. Finally, we show that for a wide class of the so-called DP-benevolent\nproblems (which are known to admit FPTAS) there exists a fully polynomial-time\nrandomized approximation scheme based on an evolutionary algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 13:50:25 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Doerr", "Benjamin", ""], ["Eremeev", "Anton", ""], ["Neumann", "Frank", ""], ["Theile", "Madeleine", ""], ["Thyssen", "Christian", ""]]}, {"id": "1301.4194", "submitter": "Ankit Dangi Mr.", "authors": "Ankit Dangi", "title": "Financial Portfolio Optimization: Computationally guided agents to\n  investigate, analyse and invest!?", "comments": "Thesis work under the guidance of Dr. Abhijit Kulkarni, Advanced\n  Analytics Lab. (SSO), SAS Research & Development, India. Submitted at Centre\n  for Modeling and Simulation, University of Pune for completion of Master of\n  Technology (M. Tech.) in Modeling and Simulation (M&S)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.CE cs.NE q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial portfolio optimization is a widely studied problem in mathematics,\nstatistics, financial and computational literature. It adheres to determining\nan optimal combination of weights associated with financial assets held in a\nportfolio. In practice, it faces challenges by virtue of varying math.\nformulations, parameters, business constraints and complex financial\ninstruments. Empirical nature of data is no longer one-sided; thereby\nreflecting upside and downside trends with repeated yet unidentifiable cyclic\nbehaviours potentially caused due to high frequency volatile movements in asset\ntrades. Portfolio optimization under such circumstances is theoretically and\ncomputationally challenging. This work presents a novel mechanism to reach an\noptimal solution by encoding a variety of optimal solutions in a solution bank\nto guide the search process for the global investment objective formulation. It\nconceptualizes the role of individual solver agents that contribute optimal\nsolutions to a bank of solutions, a super-agent solver that learns from the\nsolution bank, and, thus reflects a knowledge-based computationally guided\nagents approach to investigate, analyse and reach to optimal solution for\ninformed investment decisions.\n  Conceptual understanding of classes of solver agents that represent varying\nproblem formulations and, mathematically oriented deterministic solvers along\nwith stochastic-search driven evolutionary and swarm-intelligence based\ntechniques for optimal weights are discussed. Algorithmic implementation is\npresented by an enhanced neighbourhood generation mechanism in Simulated\nAnnealing algorithm. A framework for inclusion of heuristic knowledge and human\nexpertise from financial literature related to investment decision making\nprocess is reflected via introduction of controlled perturbation strategies\nusing a decision matrix for neighbourhood generation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 19:26:02 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Dangi", "Ankit", ""]]}, {"id": "1301.4662", "submitter": "Yusuf Perwej", "authors": "Yusuf Perwej", "title": "Recurrent Neural Network Method in Arabic Words Recognition System", "comments": "6 Pages, 5 Figures, Vol. 3, Issue 11, pages 43-48", "journal-ref": "International Journal of Computer Science and Telecommunications\n  (IJCST)UK, London,(http://www.ijcst.org) Vol. 3, Issue 11, pages 43-48,\n  November 2012, http://www.ijcst.org/Volume3/Issue11/p8_3_11.pdf", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of unconstrained handwriting continues to be a difficult task\nfor computers despite active research for several decades. This is because\nhandwritten text offers great challenges such as character and word\nsegmentation, character recognition, variation between handwriting styles,\ndifferent character size and no font constraints as well as the background\nclarity. In this paper primarily discussed Online Handwriting Recognition\nmethods for Arabic words which being often used among then across the Middle\nEast and North Africa people. Because of the characteristic of the whole body\nof the Arabic words, namely connectivity between the characters, thereby the\nsegmentation of An Arabic word is very difficult. We introduced a recurrent\nneural network to online handwriting Arabic word recognition. The key\ninnovation is a recently produce recurrent neural networks objective function\nknown as connectionist temporal classification. The system consists of an\nadvanced recurrent neural network with an output layer designed for sequence\nlabeling, partially combined with a probabilistic language model. Experimental\nresults show that unconstrained Arabic words achieve recognition rates about\n79%, which is significantly higher than the about 70% using a previously\ndeveloped hidden markov model based recognition system.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2013 14:29:56 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Perwej", "Yusuf", ""]]}, {"id": "1301.4862", "submitter": "Pierre-Yves Oudeyer", "authors": "Adrien Baranes and Pierre-Yves Oudeyer", "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal\n  Exploration in Robots", "comments": null, "journal-ref": "Baranes, A., Oudeyer, P-Y. (2013) Active Learning of Inverse\n  Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and\n  Autonomous Systems, 61(1), pp. 49-73", "doi": "10.1016/j.robot.2012.05.008", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive\nCuriosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal\nexploration mechanism which allows active learning of inverse models in\nhigh-dimensional redundant robots. This allows a robot to efficiently and\nactively learn distributions of parameterized motor skills/policies that solve\na corresponding distribution of parameterized tasks/goals. The architecture\nmakes the robot sample actively novel parameterized tasks in the task space,\nbased on a measure of competence progress, each of which triggers low-level\ngoal-directed learning of the motor policy pa- rameters that allow to solve it.\nFor both learning and generalization, the system leverages regression\ntechniques which allow to infer the motor policy parameters corresponding to a\ngiven novel parameterized task, and based on the previously learnt\ncorrespondences between policy and task parameters. We present experiments with\nhigh-dimensional continuous sensorimotor spaces in three different robotic\nsetups: 1) learning the inverse kinematics in a highly-redundant robotic arm,\n2) learning omnidirectional locomotion with motor primitives in a quadruped\nrobot, 3) an arm learning to control a fishing rod with a flexible wire. We\nshow that 1) exploration in the task space can be a lot faster than exploration\nin the actuator space for learning inverse models in redundant robots; 2)\nselecting goals maximizing competence progress creates developmental\ntrajectories driving the robot to progressively focus on tasks of increasing\ncomplexity and is statistically significantly more efficient than selecting\ntasks randomly, as well as more efficient than different standard active motor\nbabbling methods; 3) this architecture allows the robot to actively discover\nwhich parts of its task space it can learn to reach and which part it cannot.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 13:26:07 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Baranes", "Adrien", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1301.6265", "submitter": "Amir Hesam Salavati", "authors": "Amin Karbasi and Amir Hesam Salavati and Amin Shokrollahi and Lav\n  Varshney", "title": "Neural Networks Built from Unreliable Components", "comments": "This paper has been withdrawn by the author due to some personal\n  restrictions on the publication policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in associative memory design through strutured pattern sets\nand graph-based inference algorithms have allowed the reliable learning and\nretrieval of an exponential number of patterns. Both these and classical\nassociative memories, however, have assumed internally noiseless computational\nnodes. This paper considers the setting when internal computations are also\nnoisy. Even if all components are noisy, the final error probability in recall\ncan often be made exceedingly small, as we characterize. There is a threshold\nphenomenon. We also show how to optimize inference algorithm parameters when\nknowing statistical properties of internal noise.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 16:20:46 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 20:17:05 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2013 13:49:40 GMT"}, {"version": "v4", "created": "Fri, 31 May 2013 20:24:23 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Karbasi", "Amin", ""], ["Salavati", "Amir Hesam", ""], ["Shokrollahi", "Amin", ""], ["Varshney", "Lav", ""]]}]