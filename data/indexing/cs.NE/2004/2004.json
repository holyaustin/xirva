[{"id": "2004.00058", "submitter": "Larissa Albantakis", "authors": "Larissa Albantakis, Francesco Massari, Maggie Beheler-Amass and Giulio\n  Tononi", "title": "A macro agent and its actions", "comments": "18 pages, 5 figures; to appear as a chapter in \"Top-Down Causation\n  and Emergence\" published by Springer as part of the Synthese Library Book\n  Series; F.M. and M.B. contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In science, macro level descriptions of the causal interactions within\ncomplex, dynamical systems are typically deemed convenient, but ultimately\nreducible to a complete causal account of the underlying micro constituents.\nYet, such a reductionist perspective is hard to square with several issues\nrelated to autonomy and agency: (1) agents require (causal) borders that\nseparate them from the environment, (2) at least in a biological context,\nagents are associated with macroscopic systems, and (3) agents are supposed to\nact upon their environment. Integrated information theory (IIT) (Oizumi et al.,\n2014) offers a quantitative account of causation based on a set of causal\nprinciples, including notions such as causal specificity, composition, and\nirreducibility, that challenges the reductionist perspective in multiple ways.\nFirst, the IIT formalism provides a complete account of a system's causal\nstructure, including irreducible higher-order mechanisms constituted of\nmultiple system elements. Second, a system's amount of integrated information\n($\\Phi$) measures the causal constraints a system exerts onto itself and can\npeak at a macro level of description (Hoel et al., 2016; Marshall et al.,\n2018). Finally, the causal principles of IIT can also be employed to identify\nand quantify the actual causes of events (\"what caused what\"), such as an\nagent's actions (Albantakis et al., 2019). Here, we demonstrate this framework\nby example of a simulated agent, equipped with a small neural network, that\nforms a maximum of $\\Phi$ at a macro scale.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:51:18 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Albantakis", "Larissa", ""], ["Massari", "Francesco", ""], ["Beheler-Amass", "Maggie", ""], ["Tononi", "Giulio", ""]]}, {"id": "2004.00151", "submitter": "Jacob Schrum", "authors": "Jacob Schrum, Jake Gutierrez, Vanessa Volz, Jialin Liu, Simon Lucas,\n  and Sebastian Risi", "title": "Interactive Evolution and Exploration Within Latent Level-Design Space\n  of Generative Adversarial Networks", "comments": "GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are an emerging form of indirect\nencoding. The GAN is trained to induce a latent space on training data, and a\nreal-valued evolutionary algorithm can search that latent space. Such Latent\nVariable Evolution (LVE) has recently been applied to game levels. However, it\nis hard for objective scores to capture level features that are appealing to\nplayers. Therefore, this paper introduces a tool for interactive LVE of\ntile-based levels for games. The tool also allows for direct exploration of the\nlatent dimensions, and allows users to play discovered levels. The tool works\nfor a variety of GAN models trained for both Super Mario Bros. and The Legend\nof Zelda, and is easily generalizable to other games. A user study shows that\nboth the evolution and latent space exploration features are appreciated, with\na slight preference for direct exploration, but combining these features allows\nusers to discover even better levels. User feedback also indicates how this\nsystem could eventually grow into a commercial design tool, with the addition\nof a few enhancements.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:52:17 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Schrum", "Jacob", ""], ["Gutierrez", "Jake", ""], ["Volz", "Vanessa", ""], ["Liu", "Jialin", ""], ["Lucas", "Simon", ""], ["Risi", "Sebastian", ""]]}, {"id": "2004.00221", "submitter": "Alvin Wan", "authors": "Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Henry Jin,\n  Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez", "title": "NBDT: Neural-Backed Decision Trees", "comments": "8 pages, 7 figures, accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications such as finance and medicine demand accurate\nand justifiable predictions, barring most deep learning methods from use. In\nresponse, previous work combines decision trees with deep learning, yielding\nmodels that (1) sacrifice interpretability for accuracy or (2) sacrifice\naccuracy for interpretability. We forgo this dilemma by jointly improving\naccuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs\nreplace a neural network's final linear layer with a differentiable sequence of\ndecisions and a surrogate loss. This forces the model to learn high-level\nconcepts and lessens reliance on highly-uncertain decisions, yielding (1)\naccuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet\nand better generalize to unseen classes by up to 16%. Furthermore, our\nsurrogate loss improves the original model's accuracy by up to 2%. NBDTs also\nafford (2) interpretability: improving human trustby clearly identifying model\nmistakes and assisting in dataset debugging. Code and pretrained NBDTs are at\nhttps://github.com/alvinwan/neural-backed-decision-trees.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:04:03 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 22:33:20 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 03:06:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wan", "Alvin", ""], ["Dunlap", "Lisa", ""], ["Ho", "Daniel", ""], ["Yin", "Jihan", ""], ["Lee", "Scott", ""], ["Jin", "Henry", ""], ["Petryk", "Suzanne", ""], ["Bargal", "Sarah Adel", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2004.00302", "submitter": "Filipe Assun\\c{c}\\~ao", "authors": "Filipe Assun\\c{c}\\~ao, Nuno Louren\\c{c}o, Bernardete Ribeiro, Penousal\n  Machado", "title": "Incremental Evolution and Development of Deep Artificial Neural Networks", "comments": "European Conference on Genetic Programming (EuroGP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NeuroEvolution (NE) methods are known for applying Evolutionary Computation\nto the optimisation of Artificial Neural Networks(ANNs). Despite aiding\nnon-expert users to design and train ANNs, the vast majority of NE approaches\ndisregard the knowledge that is gathered when solving other tasks, i.e.,\nevolution starts from scratch for each problem, ultimately delaying the\nevolutionary process. To overcome this drawback, we extend Fast Deep\nEvolutionary Network Structured Representation (Fast-DENSER) to incremental\ndevelopment. We hypothesise that by transferring the knowledge gained from\nprevious tasks we can attain superior results and speedup evolution. The\nresults show that the average performance of the models generated by\nincremental development is statistically superior to the non-incremental\naverage performance. In case the number of evaluations performed by incremental\ndevelopment is smaller than the performed by non-incremental development the\nattained results are similar in performance, which indicates that incremental\ndevelopment speeds up evolution. Lastly, the models generated using incremental\ndevelopment generalise better, and thus, without further evolution, report a\nsuperior performance on unseen problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:23:15 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Assun\u00e7\u00e3o", "Filipe", ""], ["Louren\u00e7o", "Nuno", ""], ["Ribeiro", "Bernardete", ""], ["Machado", "Penousal", ""]]}, {"id": "2004.00307", "submitter": "Filipe Assun\\c{c}\\~ao", "authors": "Filipe Assun\\c{c}\\~ao, Nuno Louren\\c{c}o, Bernardete Ribeiro, and\n  Penousal Machado", "title": "Evolution of Scikit-Learn Pipelines with Dynamic Structured Grammatical\n  Evolution", "comments": "EvoApps 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Machine Learning (ML) models is a difficult and\ntime-consuming job that comprises a series of sequential and correlated tasks\nthat go from the data pre-processing, and the design and extraction of\nfeatures, to the choice of the ML algorithm and its parameterisation. The task\nis even more challenging considering that the design of features is in many\ncases problem specific, and thus requires domain-expertise. To overcome these\nlimitations Automated Machine Learning (AutoML) methods seek to automate, with\nfew or no human-intervention, the design of pipelines, i.e., automate the\nselection of the sequence of methods that have to be applied to the raw data.\nThese methods have the potential to enable non-expert users to use ML, and\nprovide expert users with solutions that they would unlikely consider. In\nparticular, this paper describes AutoML-DSGE - a novel grammar-based framework\nthat adapts Dynamic Structured Grammatical Evolution (DSGE) to the evolution of\nScikit-Learn classification pipelines. The experimental results include\ncomparing AutoML-DSGE to another grammar-based AutoML framework, Resilient\nClassificationPipeline Evolution (RECIPE), and show that the average\nperformance of the classification pipelines generated by AutoML-DSGE is always\nsuperior to the average performance of RECIPE; the differences are\nstatistically significant in 3 out of the 10 used datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:31:34 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Assun\u00e7\u00e3o", "Filipe", ""], ["Louren\u00e7o", "Nuno", ""], ["Ribeiro", "Bernardete", ""], ["Machado", "Penousal", ""]]}, {"id": "2004.00327", "submitter": "Per Kristian Lehre", "authors": "Brendan Case and Per Kristian Lehre", "title": "Self-adaptation in non-Elitist Evolutionary Algorithms on Discrete\n  Problems with Unknown Structure", "comments": "To appear in IEEE Transactions of Evolutionary Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key challenge to make effective use of evolutionary algorithms is to choose\nappropriate settings for their parameters. However, the appropriate parameter\nsetting generally depends on the structure of the optimisation problem, which\nis often unknown to the user. Non-deterministic parameter control mechanisms\nadjust parameters using information obtained from the evolutionary process.\nSelf-adaptation -- where parameter settings are encoded in the chromosomes of\nindividuals and evolve through mutation and crossover -- is a popular parameter\ncontrol mechanism in evolutionary strategies. However, there is little\ntheoretical evidence that self-adaptation is effective, and self-adaptation has\nlargely been ignored by the discrete evolutionary computation community.\n  Here we show through a theoretical runtime analysis that a non-elitist,\ndiscrete evolutionary algorithm which self-adapts its mutation rate not only\noutperforms EAs which use static mutation rates on \\leadingones, but also\nimproves asymptotically on an EA using a state-of-the-art control mechanism.\nThe structure of this problem depends on a parameter $k$, which is \\emph{a\npriori} unknown to the algorithm, and which is needed to appropriately set a\nfixed mutation rate. The self-adaptive EA achieves the same asymptotic runtime\nas if this parameter was known to the algorithm beforehand, which is an\nasymptotic speedup for this problem compared to all other EAs previously\nstudied. An experimental study of how the mutation-rates evolve show that they\nrespond adequately to a diverse range of problem structures.\n  These results suggest that self-adaptation should be adopted more broadly as\na parameter control mechanism in discrete, non-elitist evolutionary algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 10:35:45 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Case", "Brendan", ""], ["Lehre", "Per Kristian", ""]]}, {"id": "2004.00476", "submitter": "Christopher Cleghorn", "authors": "Christopher W Cleghorn, Belinda Stapelberg", "title": "Particle Swarm Optimization: Stability Analysis using N-Informers under\n  Arbitrary Coefficient Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives, under minimal modelling assumptions, a simple to use\ntheorem for obtaining both order-$1$ and order-$2$ stability criteria for a\ncommon class of particle swarm optimization (PSO) variants. Specifically, PSO\nvariants that can be rewritten as a finite sum of stochastically weighted\ndifference vectors between a particle's position and swarm informers are\ncovered by the theorem. Additionally, the use of the derived theorem allows a\nPSO practitioner to obtain stability criteria that contains no artificial\nrestriction on the relationship between control coefficients. Almost all\nprevious PSO stability results have provided stability criteria under the\nrestriction that the social and cognitive control coefficients are equal; such\nrestrictions are not present when using the derived theorem. Using the derived\ntheorem, as demonstration of its ease of use, stability criteria are derived\nwithout the imposed restriction on the relation between the control\ncoefficients for three popular PSO variants.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:45:02 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Cleghorn", "Christopher W", ""], ["Stapelberg", "Belinda", ""]]}, {"id": "2004.00502", "submitter": "Vinayakumar R", "authors": "Simran K, Sriram S, Vinayakumar R, Soman KP", "title": "Deep Learning Approach for Intelligent Named Entity Recognition of Cyber\n  Security", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the amount of Cyber Security data generated in the form of\nunstructured texts, for example, social media resources, blogs, articles, and\nso on has exceptionally increased. Named Entity Recognition (NER) is an initial\nstep towards converting this unstructured data into structured data which can\nbe used by a lot of applications. The existing methods on NER for Cyber\nSecurity data are based on rules and linguistic characteristics. A Deep\nLearning (DL) based approach embedded with Conditional Random Fields (CRFs) is\nproposed in this paper. Several DL architectures are evaluated to find the most\noptimal architecture. The combination of Bidirectional Gated Recurrent Unit\n(Bi-GRU), Convolutional Neural Network (CNN), and CRF performed better compared\nto various other DL frameworks on a publicly available benchmark dataset. This\nmay be due to the reason that the bidirectional structures preserve the\nfeatures related to the future and previous words in a sequence.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:36:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["K", "Simran", ""], ["S", "Sriram", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "2004.00503", "submitter": "Vinayakumar R", "authors": "Simran K, Prathiksha Balakrishna, Vinayakumar R, Soman KP", "title": "Deep Learning Approach for Enhanced Cyber Threat Indicators in Twitter\n  Stream", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent days, the amount of Cyber Security text data shared via social\nmedia resources mainly Twitter has increased. An accurate analysis of this data\ncan help to develop cyber threat situational awareness framework for a cyber\nthreat. This work proposes a deep learning based approach for tweet data\nanalysis. To convert the tweets into numerical representations, various text\nrepresentations are employed. These features are feed into deep learning\narchitecture for optimal feature extraction as well as classification. Various\nhyperparameter tuning approaches are used for identifying optimal text\nrepresentation method as well as optimal network parameters and network\nstructures for deep learning models. For comparative analysis, the classical\ntext representation method with classical machine learning algorithm is\nemployed. From the detailed analysis of experiments, we found that the deep\nlearning architecture with advanced text representation methods performed\nbetter than the classical text representation and classical machine learning\nalgorithms. The primary reason for this is that the advanced text\nrepresentation methods have the capability to learn sequential properties which\nexist among the textual data and deep learning architectures learns the optimal\nfeatures along with decreasing the feature size.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:29:42 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["K", "Simran", ""], ["Balakrishna", "Prathiksha", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "2004.00719", "submitter": "Harbir Antil", "authors": "Harbir Antil, Ratna Khatri, Rainald L\\\"ohner, and Deepanshu Verma", "title": "Fractional Deep Neural Network via Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel algorithmic framework for a deep neural network\n(DNN), which in a mathematically rigorous manner, allows us to incorporate\nhistory (or memory) into the network -- it ensures all layers are connected to\none another. This DNN, called Fractional-DNN, can be viewed as a\ntime-discretization of a fractional in time nonlinear ordinary differential\nequation (ODE). The learning problem then is a minimization problem subject to\nthat fractional ODE as constraints. We emphasize that an analogy between the\nexisting DNN and ODEs, with standard time derivative, is well-known by now. The\nfocus of our work is the Fractional-DNN. Using the Lagrangian approach, we\nprovide a derivation of the backward propagation and the design equations. We\ntest our network on several datasets for classification problems.\nFractional-DNN offers various advantages over the existing DNN. The key\nbenefits are a significant improvement to the vanishing gradient issue due to\nthe memory effect, and better handling of nonsmooth data due to the network's\nability to approximate non-smooth functions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 21:58:21 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Antil", "Harbir", ""], ["Khatri", "Ratna", ""], ["L\u00f6hner", "Rainald", ""], ["Verma", "Deepanshu", ""]]}, {"id": "2004.00802", "submitter": "Christopher H. Bennett", "authors": "Christopher H. Bennett, T. Patrick Xiao, Ryan Dellana, Vineet Agrawal,\n  Ben Feinberg, Venkatraman Prabhakar, Krishnaswamy Ramkumar, Long Hinh,\n  Swatilekha Saha, Vijay Raghavan, Ramesh Chettuvetty, Sapan Agarwal, and\n  Matthew J. Marinella", "title": "Device-aware inference operations in SONOS nonvolatile memory arrays", "comments": "To be presented at IEEE International Physics Reliability Symposium\n  (IRPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory arrays can deploy pre-trained neural network models for\nedge inference. However, these systems are affected by device-level noise and\nretention issues. Here, we examine damage caused by these effects, introduce a\nmitigation strategy, and demonstrate its use in fabricated array of SONOS\n(Silicon-Oxide-Nitride-Oxide-Silicon) devices. On MNIST, fashion-MNIST, and\nCIFAR-10 tasks, our approach increases resilience to synaptic noise and drift.\nWe also show strong performance can be realized with ADCs of 5-8 bits\nprecision.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 04:04:37 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Bennett", "Christopher H.", ""], ["Xiao", "T. Patrick", ""], ["Dellana", "Ryan", ""], ["Agrawal", "Vineet", ""], ["Feinberg", "Ben", ""], ["Prabhakar", "Venkatraman", ""], ["Ramkumar", "Krishnaswamy", ""], ["Hinh", "Long", ""], ["Saha", "Swatilekha", ""], ["Raghavan", "Vijay", ""], ["Chettuvetty", "Ramesh", ""], ["Agarwal", "Sapan", ""], ["Marinella", "Matthew J.", ""]]}, {"id": "2004.00858", "submitter": "Wenjing Li", "authors": "Wenjing Li and Wei Bian", "title": "Projection Neural Network for a Class of Sparse Regression Problems with\n  Cardinality Penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of sparse regression problems, whose\nobjective function is the summation of a convex loss function and a cardinality\npenalty. By constructing a smoothing function for the cardinality function, we\npropose a projected neural network and design a correction method for solving\nthis problem. The solution of the proposed neural network is unique, global\nexistent, bounded and globally Lipschitz continuous. Besides, we prove that all\naccumulation points of the proposed neural network have a common support set\nand a unified lower bound for the nonzero entries. Combining the proposed\nneural network with the correction method, any corrected accumulation point is\na local minimizer of the considered sparse regression problem. Moreover, we\nanalyze the equivalent relationship on the local minimizers between the\nconsidered sparse regression problem and another regression sparse problem.\nFinally, some numerical experiments are provided to show the efficiency of the\nproposed neural networks in solving some sparse regression problems in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:05:20 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 09:29:24 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 04:44:19 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 09:55:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Wenjing", ""], ["Bian", "Wei", ""]]}, {"id": "2004.00930", "submitter": "Dimitrije Markovic", "authors": "Sascha Fr\\\"olich, Dimitrije Markovi\\'c, and Stefan J. Kiebel", "title": "Neuronal Sequence Models for Bayesian Online Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sequential neuronal activity underlies a wide range of processes in the\nbrain. Neuroscientific evidence for neuronal sequences has been reported in\ndomains as diverse as perception, motor control, speech, spatial navigation and\nmemory. Consequently, different dynamical principles have been proposed as\npossible sequence-generating mechanisms. Combining experimental findings with\ncomputational concepts like the Bayesian brain hypothesis and predictive coding\nleads to the interesting possibility that predictive and inferential processes\nin the brain are grounded on generative processes which maintain a sequential\nstructure. While probabilistic inference about ongoing sequences is a useful\ncomputational model for both the analysis of neuroscientific data and a wide\nrange of problems in artificial recognition and motor control, research on the\nsubject is relatively scarce and distributed over different fields in the\nneurosciences. Here we review key findings about neuronal sequences and relate\nthese to the concept of online inference on sequences as a model of\nsensory-motor processing and recognition. We propose that describing sequential\nneuronal activity as an expression of probabilistic inference over sequences\nmay lead to novel perspectives on brain function. Importantly, it is promising\nto translate the key idea of probabilistic inference on sequences to machine\nlearning, in order to address challenges in the real-time recognition of speech\nand human motion.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 10:52:54 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Fr\u00f6lich", "Sascha", ""], ["Markovi\u0107", "Dimitrije", ""], ["Kiebel", "Stefan J.", ""]]}, {"id": "2004.01181", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Simon Alford, Vijay Gadepally, Michael Jones, Lauren\n  Milechin, Albert Reuther, Ryan Robinett, Sid Samsi", "title": "GraphChallenge.org Sparse Deep Neural Network Performance", "comments": "7 pages, 7 figures, 80 references, to be submitted to IEEE HPEC 2020.\n  This work reports new updated results on prior work reported in\n  arXiv:1909.05631. arXiv admin note: substantial text overlap with\n  arXiv:1807.03165, arXiv:1708.02937. arXiv admin note: text overlap with\n  arXiv:2003.09269", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286253", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to\ndeveloping new solutions for analyzing graphs and sparse data. Sparse AI\nanalytics present unique scalability difficulties. The Sparse Deep Neural\nNetwork (DNN) Challenge draws upon prior challenges from machine learning, high\nperformance computing, and visual analytics to create a challenge that is\nreflective of emerging sparse AI systems. The sparse DNN challenge is based on\na mathematically well-defined DNN inference computation and can be implemented\nin any programming environment. In 2019 several sparse DNN challenge\nsubmissions were received from a wide range of authors and organizations. This\npaper presents a performance analysis of the best performers of these\nsubmissions. These submissions show that their state-of-the-art sparse DNN\nexecution time, $T_{\\rm DNN}$, is a strong function of the number of DNN\noperations performed, $N_{\\rm op}$. The sparse DNN challenge provides a clear\npicture of current sparse DNN systems and underscores the need for new\ninnovations to achieve high performance on very large sparse DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 00:29:12 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 02:38:52 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kepner", "Jeremy", ""], ["Alford", "Simon", ""], ["Gadepally", "Vijay", ""], ["Jones", "Michael", ""], ["Milechin", "Lauren", ""], ["Reuther", "Albert", ""], ["Robinett", "Ryan", ""], ["Samsi", "Sid", ""]]}, {"id": "2004.01190", "submitter": "Gadi Naveh", "authors": "Gadi Naveh, Oded Ben-David, Haim Sompolinsky and Zohar Ringel", "title": "Predicting the outputs of finite networks trained with noisy gradients", "comments": "8 pages + appendix, 7 figures overall", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of works studied wide deep neural networks (DNNs) by\napproximating them as Gaussian Processes (GPs). A DNN trained with gradient\nflow was shown to map to a GP governed by the Neural Tangent Kernel (NTK),\nwhereas earlier works showed that a DNN with an i.i.d. prior over its weights\nmaps to the so-called Neural Network Gaussian Process (NNGP). Here we consider\na DNN training protocol, involving noise, weight decay and finite width, whose\noutcome corresponds to a certain non-Gaussian stochastic process. An analytical\nframework is then introduced to analyze this non-Gaussian process, whose\ndeviation from a GP is controlled by the finite width. Our contribution is\nthree-fold: (i) In the infinite width limit, we establish a correspondence\nbetween DNNs trained with noisy gradients and the NNGP, not the NTK. (ii) We\nprovide a general analytical form for the finite width correction (FWC) for\nDNNs with arbitrary activation functions and depth and use it to predict the\noutputs of empirical finite networks with high accuracy. Analyzing the FWC\nbehavior as a function of $n$, the training set size, we find that it is\nnegligible for both the very small $n$ regime, and, surprisingly, for the large\n$n$ regime (where the GP error scales as $O(1/n)$). (iii) We flesh-out\nalgebraically how these FWCs can improve the performance of finite\nconvolutional neural networks (CNNs) relative to their GP counterparts on image\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 18:00:01 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 10:17:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Naveh", "Gadi", ""], ["Ben-David", "Oded", ""], ["Sompolinsky", "Haim", ""], ["Ringel", "Zohar", ""]]}, {"id": "2004.01254", "submitter": "Constantin Waubert de Puiseau", "authors": "Richard Meyes, Constantin Waubert de Puiseau, Andres Posada-Moreno,\n  Tobias Meisen", "title": "Under the Hood of Neural Networks: Characterizing Learned\n  Representations by Functional Neuron Populations and Network Ablations", "comments": "17 pages, 14 figures, adjusted \"under review\" statement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for more transparency of the decision-making processes in artificial\nneural networks steadily increases driven by their applications in safety\ncritical and ethically challenging domains such as autonomous driving or\nmedical diagnostics. We address today's lack of transparency of neural networks\nand shed light on the roles of single neurons and groups of neurons within the\nnetwork fulfilling a learned task. Inspired by research in the field of\nneuroscience, we characterize the learned representations by activation\npatterns and network ablations, revealing functional neuron populations that a)\nact jointly in response to specific stimuli or b) have similar impact on the\nnetwork's performance after being ablated. We find that neither a neuron's\nmagnitude or selectivity of activation, nor its impact on network performance\nare sufficient stand-alone indicators for its importance for the overall task.\nWe argue that such indicators are essential for future advances in transfer\nlearning and modern neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 20:45:01 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 09:09:15 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Meyes", "Richard", ""], ["de Puiseau", "Constantin Waubert", ""], ["Posada-Moreno", "Andres", ""], ["Meisen", "Tobias", ""]]}, {"id": "2004.01274", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Does Comma Selection Help To Cope With Local Optima", "comments": "36 pages. Full version of a paper that appeared at GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3389823", "report-no": null, "categories": "cs.NE cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One hope when using non-elitism in evolutionary computation is that the\nability to abandon the current-best solution aids leaving local optima. To\nimprove our understanding of this mechanism, we perform a rigorous runtime\nanalysis of a basic non-elitist evolutionary algorithm (EA), the\n$(\\mu,\\lambda)$ EA, on the most basic benchmark function with a local optimum,\nthe jump function. We prove that for all reasonable values of the parameters\nand the problem, the expected runtime of the $(\\mu,\\lambda)$~EA is, apart from\nlower order terms, at least as large as the expected runtime of its elitist\ncounterpart, the $(\\mu+\\lambda)$~EA (for which we conduct the first runtime\nanalysis on jump functions to allow this comparison). Consequently, the ability\nof the $(\\mu,\\lambda)$~EA to leave local optima to inferior solutions does not\nlead to a runtime advantage.\n  We complement this lower bound with an upper bound that, for broad ranges of\nthe parameters, is identical to our lower bound apart from lower order terms.\nThis is the first runtime result for a non-elitist algorithm on a multi-modal\nproblem that is tight apart from lower order terms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 21:39:33 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 11:47:10 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "2004.01395", "submitter": "Binxin Ru", "authors": "Binxin Ru, Pedro Esperanca, Fabio Carlucci", "title": "Neural Architecture Generator Optimization", "comments": "20 pages, 9 figures, neural architecture search, Thirty-fourth\n  Conference on Neural Information Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) was first proposed to achieve\nstate-of-the-art performance through the discovery of new architecture\npatterns, without human intervention. An over-reliance on expert knowledge in\nthe search space design has however led to increased performance (local optima)\nwithout significant architectural breakthroughs, thus preventing truly novel\nsolutions from being reached. In this work we 1) are the first to investigate\ncasting NAS as a problem of finding the optimal network generator and 2) we\npropose a new, hierarchical and graph-based search space capable of\nrepresenting an extremely large variety of network types, yet only requiring\nfew continuous hyper-parameters. This greatly reduces the dimensionality of the\nproblem, enabling the effective use of Bayesian Optimisation as a search\nstrategy. At the same time, we expand the range of valid architectures,\nmotivating a multi-objective learning approach. We demonstrate the\neffectiveness of this strategy on six benchmark datasets and show that our\nsearch space generates extremely lightweight yet highly competitive models.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 06:38:07 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 16:22:40 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 16:28:40 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ru", "Binxin", ""], ["Esperanca", "Pedro", ""], ["Carlucci", "Fabio", ""]]}, {"id": "2004.01481", "submitter": "Pitoyo Hartono", "authors": "Pitoyo Hartono", "title": "Generating Similarity Map for COVID-19 Transmission Dynamics with\n  Topological Autoencoder", "comments": null, "journal-ref": "Informatics in Medicine Unlocked, Vol. 20, 100386, 2020", "doi": "10.1016/j.imu.2020.100386", "report-no": null, "categories": "physics.soc-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the beginning of 2020 the world has seen the initial outbreak of COVID-19,\na disease caused by SARS-CoV2 virus in China. The World Health Organization\n(WHO) declared this disease as a pandemic on March 11 2020. As the disease\nspread globally, it becomes difficult to tract the transmission dynamics of\nthis disease in all countries, as they may differ in geographical, demographic\nand strategical aspects. In this short note, the author proposes the\nutilization of a type of neural network to generate a global topological map\nfor these dynamics, in which countries that share similar dynamics are mapped\nadjacently, while countries with significantly different dynamics are mapped\nfar from each other. The author believes that this kind of topological map can\nbe useful for further analyzing and comparing the correlation between the\ndiseases dynamics with strategies to mitigate this global crisis in an\nintuitive manner. Some initial experiments with with time series of patients\nnumbers in more than 240 countries are explained in this note.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 11:43:55 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 06:26:13 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hartono", "Pitoyo", ""]]}, {"id": "2004.01656", "submitter": "Christoph Ostrau", "authors": "Christoph Ostrau, Jonas Homburg, Christian Klarhorst, Michael Thies,\n  Ulrich R\\\"uckert", "title": "Benchmarking Deep Spiking Neural Networks on Neuromorphic Hardware", "comments": "The final authenticated publication is available at Springer via\n  https://doi.org/10.1007/978-3-030-61616-8_49", "journal-ref": "Farka\\v{s} I., Masulli P., Wermter S. (eds) Artificial Neural\n  Networks and Machine Learning -- ICANN 2020. ICANN 2020. Lecture Notes in\n  Computer Science, vol 12397", "doi": "10.1007/978-3-030-61616-8_49", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more and more event-based neuromorphic hardware systems being developed\nat universities and in industry, there is a growing need for assessing their\nperformance with domain specific measures. In this work, we use the methodology\nof converting pre-trained non-spiking to spiking neural networks to evaluate\nthe performance loss and measure the energy-per-inference for three\nneuromorphic hardware systems (BrainScaleS, Spikey, SpiNNaker) and common\nsimulation frameworks for CPU (NEST) and CPU/GPU (GeNN). For analog hardware we\nfurther apply a re-training technique known as hardware-in-the-loop training to\ncope with device mismatch. This analysis is performed for five different\nnetworks, including three networks that have been found by an automated\noptimization with a neural architecture search framework. We demonstrate that\nthe conversion loss is usually below one percent for digital implementations,\nand moderately higher for analog systems with the benefit of much lower\nenergy-per-inference costs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:25:49 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:59:33 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 21:20:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ostrau", "Christoph", ""], ["Homburg", "Jonas", ""], ["Klarhorst", "Christian", ""], ["Thies", "Michael", ""], ["R\u00fcckert", "Ulrich", ""]]}, {"id": "2004.01680", "submitter": "Alexander Hvatov", "authors": "Alexander Hvatov and Mikhail Maslyaev", "title": "The data-driven physical-based equations discovery using evolutionary\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern machine learning methods allow one to obtain the data-driven\nmodels in various ways. However, the more complex the model is, the harder it\nis to interpret. In the paper, we describe the algorithm for the mathematical\nequations discovery from the given observations data. The algorithm combines\ngenetic programming with the sparse regression.\n  This algorithm allows obtaining different forms of the resulting models. As\nan example, it could be used for governing analytical equation discovery as\nwell as for partial differential equations (PDE) discovery.\n  The main idea is to collect a bag of the building blocks (it may be simple\nfunctions or their derivatives of arbitrary order) and consequently take them\nfrom the bag to create combinations, which will represent terms of the final\nequation. The selected terms pass to the evolutionary algorithm, which is used\nto evolve the selection. The evolutionary steps are combined with the sparse\nregression to pick only the significant terms. As a result, we obtain a short\nand interpretable expression that describes the physical process that lies\nbeyond the data.\n  In the paper, two examples of the algorithm application are described: the\nPDE discovery for the metocean processes and the function discovery for the\nacoustics.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:21:57 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hvatov", "Alexander", ""], ["Maslyaev", "Mikhail", ""]]}, {"id": "2004.01703", "submitter": "Jacob Schrum", "authors": "Jacob Schrum and Vanessa Volz and Sebastian Risi", "title": "CPPN2GAN: Combining Compositional Pattern Producing Networks and GANs\n  for Large-scale Pattern Generation", "comments": "GECCO 2020. arXiv admin note: text overlap with arXiv:2004.00151", "journal-ref": null, "doi": "10.1145/3377930.3389822", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are proving to be a powerful indirect\ngenotype-to-phenotype mapping for evolutionary search, but they have\nlimitations. In particular, GAN output does not scale to arbitrary dimensions,\nand there is no obvious way of combining multiple GAN outputs into a cohesive\nwhole, which would be useful in many areas, such as the generation of video\ngame levels. Game levels often consist of several segments, sometimes repeated\ndirectly or with variation, organized into an engaging pattern. Such patterns\ncan be produced with Compositional Pattern Producing Networks (CPPNs).\nSpecifically, a CPPN can define latent vector GAN inputs as a function of\ngeometry, which provides a way to organize level segments output by a GAN into\na complete level. This new CPPN2GAN approach is validated in both Super Mario\nBros. and The Legend of Zelda. Specifically, divergent search via MAP-Elites\ndemonstrates that CPPN2GAN can better cover the space of possible levels. The\nlayouts of the resulting levels are also more cohesive and aesthetically\nconsistent.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 04:29:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Schrum", "Jacob", ""], ["Volz", "Vanessa", ""], ["Risi", "Sebastian", ""]]}, {"id": "2004.01899", "submitter": "Xuefei Ning", "authors": "Xuefei Ning, Yin Zheng, Tianchen Zhao, Yu Wang, and Huazhong Yang", "title": "A Generic Graph-based Neural Architecture Encoding Scheme for\n  Predictor-based NAS", "comments": "14 pages main text; 10 pages appendix", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme,\na.k.a. GATES, to improve the predictor-based neural architecture search.\nSpecifically, different from existing graph-based schemes, GATES models the\noperations as the transformation of the propagating information, which mimics\nthe actual data processing of neural architecture. GATES is a more reasonable\nmodeling of the neural architectures, and can encode architectures from both\nthe \"operation on node\" and \"operation on edge\" cell search spaces\nconsistently. Experimental results on various search spaces confirm GATES's\neffectiveness in improving the performance predictor. Furthermore, equipped\nwith the improved performance predictor, the sample efficiency of the\npredictor-based neural architecture search (NAS) flow is boosted. Codes are\navailable at https://github.com/walkerning/aw_nas.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 09:54:49 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 03:19:33 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 01:06:51 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ning", "Xuefei", ""], ["Zheng", "Yin", ""], ["Zhao", "Tianchen", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "2004.01902", "submitter": "Nicolas Boull\\'e", "authors": "Nicolas Boull\\'e, Yuji Nakatsukasa, Alex Townsend", "title": "Rational neural networks", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider neural networks with rational activation functions. The choice of\nthe nonlinear activation function in deep learning architectures is crucial and\nheavily impacts the performance of a neural network. We establish optimal\nbounds in terms of network complexity and prove that rational neural networks\napproximate smooth functions more efficiently than ReLU networks with\nexponentially smaller depth. The flexibility and smoothness of rational\nactivation functions make them an attractive alternative to ReLU, as we\ndemonstrate with numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 10:36:11 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:16:55 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Boull\u00e9", "Nicolas", ""], ["Nakatsukasa", "Yuji", ""], ["Townsend", "Alex", ""]]}, {"id": "2004.02094", "submitter": "Neda Tavakoli", "authors": "Neda Tavakoli", "title": "Locality Sensitive Hashing-based Sequence Alignment Using Deep\n  Bidirectional LSTM Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional Long Short-Term Memory (LSTM) is a special kind of Recurrent\nNeural Network (RNN) architecture which is designed to model sequences and\ntheir long-range dependencies more precisely than RNNs. This paper proposes to\nuse deep bidirectional LSTM for sequence modeling as an approach to perform\nlocality-sensitive hashing (LSH)-based sequence alignment. In particular, we\nuse the deep bidirectional LSTM to learn features of LSH. The obtained LSH is\nthen can be utilized to perform sequence alignment. We demonstrate the\nfeasibility of the modeling sequences using the proposed LSTM-based model by\naligning the short read queries over the reference genome. We use the human\nreference genome as our training dataset, in addition to a set of short reads\ngenerated using Illumina sequencing technology. The ultimate goal is to align\nquery sequences into a reference genome. We first decompose the reference\ngenome into multiple sequences. These sequences are then fed into the\nbidirectional LSTM model and then mapped into fixed-length vectors. These\nvectors are what we call the trained LSH, which can then be used for sequence\nalignment. The case study shows that using the introduced LSTM-based model, we\nachieve higher accuracy with the number of epochs.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 05:13:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Tavakoli", "Neda", ""]]}, {"id": "2004.02115", "submitter": "Yongsheng Liang", "authors": "Zhigang Ren, Yongsheng Liang, Muyi Wang, Yang Yang, An Chen", "title": "An Eigenspace Divide-and-Conquer Approach for Large-Scale Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide-and-conquer-based (DC-based) evolutionary algorithms (EAs) have\nachieved notable success in dealing with large-scale optimization problems\n(LSOPs). However, the appealing performance of this type of algorithms\ngenerally requires a high-precision decomposition of the optimization problem,\nwhich is still a challenging task for existing decomposition methods. This\nstudy attempts to address the above issue from a different perspective and\nproposes an eigenspace divide-and-conquer (EDC) approach. Different from\nexisting DC-based algorithms that perform decomposition and optimization in the\noriginal decision space, EDC first establishes an eigenspace by conducting\nsingular value decomposition on a set of high-quality solutions selected from\nrecent generations. Then it transforms the optimization problem into the\neigenspace, and thus significantly weakens the dependencies among the\ncorresponding eigenvariables. Accordingly, these eigenvariables can be\nefficiently grouped by a simple random strategy and each of the resulting\nsubproblems can be addressed more easily by a traditional EA. To verify the\nefficiency of EDC, comprehensive experimental studies were conducted on two\nsets of benchmark functions. Experimental results indicate that EDC is robust\nto its parameters and has good scalability to the problem dimension. The\ncomparison with several state-of-the-art algorithms further confirms that EDC\nis pretty competitive and performs better on complicated LSOPs.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 07:29:44 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ren", "Zhigang", ""], ["Liang", "Yongsheng", ""], ["Wang", "Muyi", ""], ["Yang", "Yang", ""], ["Chen", "An", ""]]}, {"id": "2004.02203", "submitter": "Steffen Goebbels", "authors": "Steffen Goebbels", "title": "On Sharpness of Error Bounds for Multivariate Neural Network\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single hidden layer feedforward neural networks can represent multivariate\nfunctions that are sums of ridge functions. These ridge functions are defined\nvia an activation function and customizable weights. The paper deals with best\nnon-linear approximation by such sums of ridge functions. Error bounds are\npresented in terms of moduli of smoothness. The main focus, however, is to\nprove that the bounds are best possible. To this end, counterexamples are\nconstructed with a non-linear, quantitative extension of the uniform\nboundedness principle. They show sharpness with respect to Lipschitz classes\nfor the logistic activation function and for certain piecewise polynomial\nactivation functions. The paper is based on univariate results in (Goebbels,\nSt.: On sharpness of error bounds for univariate approximation by single hidden\nlayer feedforward neural networks. Results Math 75 (3), 2020, article 109,\nhttps://rdcu.be/b5mKH).\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:00:52 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 09:26:16 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 15:28:36 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Goebbels", "Steffen", ""]]}, {"id": "2004.02457", "submitter": "Anna Kazeykina", "authors": "Giovanni Conforti (CMAP), Anna Kazeykina (LMO), Zhenjie Ren (CEREMADE)", "title": "Game on Random Environment, Mean-field Langevin System and Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.NE math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a type of games regularized by the relative entropy,\nwhere the players' strategies are coupled through a random environment\nvariable. Besides the existence and the uniqueness of equilibria of such games,\nwe prove that the marginal laws of the corresponding mean-field Langevin\nsystems can converge towards the games' equilibria in different settings. As\napplications, the dynamic games can be treated as games on a random environment\nwhen one treats the time horizon as the environment. In practice, our results\ncan be applied to analysing the stochastic gradient descent algorithm for deep\nneural networks in the context of supervised learning as well as for the\ngenerative adversarial networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:59:05 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 21:52:22 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Conforti", "Giovanni", "", "CMAP"], ["Kazeykina", "Anna", "", "LMO"], ["Ren", "Zhenjie", "", "CEREMADE"]]}, {"id": "2004.02535", "submitter": "Piotr Antonik", "authors": "Piotr Antonik, Nicolas Marsal, Daniel Brunner, Damien Rontani", "title": "Bayesian optimisation of large-scale photonic reservoir computers", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction. Reservoir computing is a growing paradigm for simplified\ntraining of recurrent neural networks, with a high potential for hardware\nimplementations. Numerous experiments in optics and electronics yield\ncomparable performance to digital state-of-the-art algorithms. Many of the most\nrecent works in the field focus on large-scale photonic systems, with tens of\nthousands of physical nodes and arbitrary interconnections. While this trend\nsignificantly expands the potential applications of photonic reservoir\ncomputing, it also complicates the optimisation of the high number of\nhyper-parameters of the system. Methods. In this work, we propose the use of\nBayesian optimisation for efficient exploration of the hyper-parameter space in\na minimum number of iteration. Results. We test this approach on a previously\nreported large-scale experimental system, compare it to the commonly used grid\nsearch, and report notable improvements in performance and the number of\nexperimental iterations required to optimise the hyper-parameters. Conclusion.\nBayesian optimisation thus has the potential to become the standard method for\ntuning the hyper-parameters in photonic reservoir computing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:11:03 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Antonik", "Piotr", ""], ["Marsal", "Nicolas", ""], ["Brunner", "Daniel", ""], ["Rontani", "Damien", ""]]}, {"id": "2004.02542", "submitter": "Piotr Antonik", "authors": "Piotr Antonik, Nicolas Marsal, Damien Rontani", "title": "Large-scale spatiotemporal photonic reservoir computer for image\n  classification", "comments": "12 pages, 9 figures", "journal-ref": "IEEE Journal of Selected Topics in Quantum Electronics (Volume: 26\n  , Issue: 1 , Jan.-Feb. 2020)", "doi": "10.1109/jstqe.2019.2924138", "report-no": null, "categories": "cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable photonic architecture for implementation of feedforward\nand recurrent neural networks to perform the classification of handwritten\ndigits from the MNIST database. Our experiment exploits off-the-shelf optical\nand electronic components to currently achieve a network size of 16,384 nodes.\nBoth network types are designed within the the reservoir computing paradigm\nwith randomly weighted input and hidden layers. Using various feature\nextraction techniques (e.g. histograms of oriented gradients, zoning, Gabor\nfilters) and a simple training procedure consisting of linear regression and\nwinner-takes-all decision strategy, we demonstrate numerically and\nexperimentally that a feedforward network allows for classification error rate\nof 1%, which is at the state-of-the-art for experimental implementations and\nremains competitive with more advanced algorithmic approaches. We also\ninvestigate recurrent networks in numerical simulations by explicitly\nactivating the temporal dynamics, and predict a performance improvement over\nthe feedforward configuration.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:22:31 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Antonik", "Piotr", ""], ["Marsal", "Nicolas", ""], ["Rontani", "Damien", ""]]}, {"id": "2004.02545", "submitter": "Piotr Antonik", "authors": "Piotr Antonik, Nicolas Marsal, Daniel Brunner, Damien Rontani", "title": "Human action recognition with a large-scale brain-inspired photonic\n  computer", "comments": "Authors' version before final reviews (12 pages, 4 figures)", "journal-ref": "Nat Mach Intell 1, 530-537 (2019)", "doi": "10.1038/s42256-019-0110-8", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of human actions in video streams is a challenging task in\ncomputer vision, with cardinal applications in e.g. brain-computer interface\nand surveillance. Deep learning has shown remarkable results recently, but can\nbe found hard to use in practice, as its training requires large datasets and\nspecial purpose, energy-consuming hardware. In this work, we propose a scalable\nphotonic neuro-inspired architecture based on the reservoir computing paradigm,\ncapable of recognising video-based human actions with state-of-the-art\naccuracy. Our experimental optical setup comprises off-the-shelf components,\nand implements a large parallel recurrent neural network that is easy to train\nand can be scaled up to hundreds of thousands of nodes. This work paves the way\ntowards simply reconfigurable and energy-efficient photonic information\nprocessing systems for real-time video processing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:39:10 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Antonik", "Piotr", ""], ["Marsal", "Nicolas", ""], ["Brunner", "Daniel", ""], ["Rontani", "Damien", ""]]}, {"id": "2004.02555", "submitter": "Yinglong Ma", "authors": "Jingpeng Zhao and Yinglong Ma", "title": "Joint Embedding of Words and Category Labels for Hierarchical\n  Multi-label Text Classification", "comments": "The submitted paper (Identifier: arXiv:2004.02555) has a problem of\n  authorship disputes within a collaboration of authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification has become increasingly challenging due to the continuous\nrefinement of classification label granularity and the expansion of\nclassification label scale. To address that, some research has been applied\nonto strategies that exploit the hierarchical structure in problems with a\nlarge number of categories. At present, hierarchical text classification (HTC)\nhas received extensive attention and has broad application prospects. Making\nfull use of the relationship between parent category and child category in text\nclassification task can greatly improve the performance of classification. In\nthis paper, We propose a joint embedding of text and parent category based on\nhierarchical fine-tuning ordered neurons LSTM (HFT-ONLSTM) for HTC. Our method\nmakes full use of the connection between the upper-level and lower-level\nlabels. Experiments show that our model outperforms the state-of-the-art\nhierarchical model at a lower computation cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:06:08 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 06:35:41 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 03:20:30 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhao", "Jingpeng", ""], ["Ma", "Yinglong", ""]]}, {"id": "2004.02688", "submitter": "Davide Mazzini", "authors": "Alessio Elmi, Davide Mazzini and Pietro Tortella", "title": "Light3DPose: Real-time Multi-Person 3D PoseEstimation from Multiple\n  Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to perform 3D pose estimation of multiple people from\na few calibrated camera views. Our architecture, leveraging the recently\nproposed unprojection layer, aggregates feature-maps from a 2D pose estimator\nbackbone into a comprehensive representation of the 3D scene. Such intermediate\nrepresentation is then elaborated by a fully-convolutional volumetric network\nand a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our\nmethod achieves state of the art MPJPE on the CMU Panoptic dataset using a few\nunseen views and obtains competitive results even with a single input view. We\nalso assess the transfer learning capabilities of the model by testing it\nagainst the publicly available Shelf dataset obtaining good performance\nmetrics. The proposed method is inherently efficient: as a pure bottom-up\napproach, it is computationally independent of the number of people in the\nscene. Furthermore, even though the computational burden of the 2D part scales\nlinearly with the number of input views, the overall architecture is able to\nexploit a very lightweight 2D backbone which is orders of magnitude faster than\nthe volumetric counterpart, resulting in fast inference time. The system can\nrun at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:12:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Elmi", "Alessio", ""], ["Mazzini", "Davide", ""], ["Tortella", "Pietro", ""]]}, {"id": "2004.02720", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Conditions for Open-Ended Evolution in Immigration Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Immigration Game (invented by Don Woods in 1971) extends the solitaire\nGame of Life (invented by John Conway in 1970) to enable two-player\ncompetition. The Immigration Game can be used in a model of evolution by\nnatural selection, where fitness is measured with competitions. The rules for\nthe Game of Life belong to the family of semitotalistic rules, a family with\n262,144 members. Woods' method for converting the Game of Life into a\ntwo-player game generalizes to 8,192 members of the family of semitotalistic\nrules. In this paper, we call the original Immigration Game the Life\nImmigration Game and we call the 8,192 generalizations Immigration Games\n(including the Life Immigration Game). The question we examine here is, what\nare the conditions for one of the 8,192 Immigration Games to be suitable for\nmodeling open-ended evolution? Our focus here is specifically on conditions for\nthe rules, as opposed to conditions for other aspects of the model of\nevolution. In previous work, it was conjectured that Turing-completeness of the\nrules for the Game of Life may have been necessary for the success of evolution\nusing the Life Immigration Game. Here we present evidence that\nTuring-completeness is a sufficient condition on the rules of Immigration\nGames, but not a necessary condition. The evidence suggests that a necessary\nand sufficient condition on the rules of Immigration Games, for open-ended\nevolution, is that the rules should allow growth.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:00:13 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "2004.02755", "submitter": "Dingkang Wang", "authors": "Dingkang Wang, Lucas Magee, Bing-Xing Huo, Samik Banerjee, Xu Li,\n  Jaikishan Jayakumar, Meng Kuan Lin, Keerthi Ram, Suyi Wang, Yusu Wang, Partha\n  P. Mitra", "title": "Detection and skeletonization of single neurons and tracer injections\n  using topological methods", "comments": "20 pages (14 pages main-text and 6 pages supplementary information).\n  5 main-text figures. 5 supplementary figures. 2 supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neuroscientific data analysis has traditionally relied on linear algebra and\nstochastic process theory. However, the tree-like shapes of neurons cannot be\ndescribed easily as points in a vector space (the subtraction of two neuronal\nshapes is not a meaningful operation), and methods from computational topology\nare better suited to their analysis. Here we introduce methods from Discrete\nMorse (DM) Theory to extract the tree-skeletons of individual neurons from\nvolumetric brain image data, and to summarize collections of neurons labelled\nby tracer injections. Since individual neurons are topologically trees, it is\nsensible to summarize the collection of neurons using a consensus tree-shape\nthat provides a richer information summary than the traditional regional\n'connectivity matrix' approach. The conceptually elegant DM approach lacks\nhand-tuned parameters and captures global properties of the data as opposed to\nprevious approaches which are inherently local. For individual skeletonization\nof sparsely labelled neurons we obtain substantial performance gains over\nstate-of-the-art non-topological methods (over 10% improvements in precision\nand faster proofreading). The consensus-tree summary of tracer injections\nincorporates the regional connectivity matrix information, but in addition\ncaptures the collective collateral branching patterns of the set of neurons\nconnected to the injection site, and provides a bridge between single-neuron\nmorphology and tracer-injection data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 20:58:38 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Dingkang", ""], ["Magee", "Lucas", ""], ["Huo", "Bing-Xing", ""], ["Banerjee", "Samik", ""], ["Li", "Xu", ""], ["Jayakumar", "Jaikishan", ""], ["Lin", "Meng Kuan", ""], ["Ram", "Keerthi", ""], ["Wang", "Suyi", ""], ["Wang", "Yusu", ""], ["Mitra", "Partha P.", ""]]}, {"id": "2004.02881", "submitter": "Luciano Melodia", "authors": "Luciano Melodia, Richard Lenz", "title": "Estimate of the Neural Network Dimension using Algebraic Topology and\n  Lie Theory", "comments": "The title of this article was formerly \"Parameterization of Neural\n  Networks with Connected Abelian Lie Groups as Data Manifold\"", "journal-ref": "Img.Mine.Theo.Appl.VII 2021 (15-29)", "doi": "10.1007/978-3-030-68821-9_2", "report-no": null, "categories": "stat.ML cs.CG cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to determine the smallest possible\nnumber of neurons in a layer of a neural network in such a way that the\ntopology of the input space can be learned sufficiently well. We introduce a\ngeneral procedure based on persistent homology to investigate topological\ninvariants of the manifold on which we suspect the data set. We specify the\nrequired dimensions precisely, assuming that there is a smooth manifold on or\nnear which the data are located. Furthermore, we require that this space is\nconnected and has a commutative group structure in the mathematical sense.\nThese assumptions allow us to derive a decomposition of the underlying space\nwhose topology is well known. We use the representatives of the $k$-dimensional\nhomology groups from the persistence landscape to determine an integer\ndimension for this decomposition. This number is the dimension of the embedding\nthat is capable of capturing the topology of the data manifold. We derive the\ntheory and validate it experimentally on toy data sets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:15:05 GMT"}, {"version": "v10", "created": "Sun, 13 Dec 2020 14:36:48 GMT"}, {"version": "v11", "created": "Thu, 31 Dec 2020 07:27:13 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 09:32:13 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 13:09:50 GMT"}, {"version": "v4", "created": "Sat, 1 Aug 2020 08:58:54 GMT"}, {"version": "v5", "created": "Thu, 22 Oct 2020 06:17:04 GMT"}, {"version": "v6", "created": "Wed, 28 Oct 2020 08:45:41 GMT"}, {"version": "v7", "created": "Tue, 3 Nov 2020 15:02:55 GMT"}, {"version": "v8", "created": "Tue, 10 Nov 2020 14:29:01 GMT"}, {"version": "v9", "created": "Mon, 16 Nov 2020 09:14:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Melodia", "Luciano", ""], ["Lenz", "Richard", ""]]}, {"id": "2004.02967", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Andrew Brock, Karen Simonyan, Quoc V. Le", "title": "Evolving Normalization-Activation Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization layers and activation functions are fundamental components in\ndeep networks and typically co-locate with each other. Here we propose to\ndesign them using an automated approach. Instead of designing them separately,\nwe unify them into a single tensor-to-tensor computation graph, and evolve its\nstructure starting from basic mathematical functions. Examples of such\nmathematical functions are addition, multiplication and statistical moments.\nThe use of low-level mathematical functions, in contrast to the use of\nhigh-level modules in mainstream NAS, leads to a highly sparse and large search\nspace which can be challenging for search methods. To address the challenge, we\ndevelop efficient rejection protocols to quickly filter out candidate layers\nthat do not work well. We also use multi-objective evolution to optimize each\nlayer's performance across many architectures to prevent overfitting. Our\nmethod leads to the discovery of EvoNorms, a set of new\nnormalization-activation layers with novel, and sometimes surprising structures\nthat go beyond existing design patterns. For example, some EvoNorms do not\nassume that normalization and activation functions must be applied\nsequentially, nor need to center the feature maps, nor require explicit\nactivation functions. Our experiments show that EvoNorms work well on image\nclassification models including ResNets, MobileNets and EfficientNets but also\ntransfer well to Mask R-CNN with FPN/SpineNet for instance segmentation and to\nBigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers\nin many cases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:52:48 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 02:58:37 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 16:29:08 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 22:59:31 GMT"}, {"version": "v5", "created": "Fri, 17 Jul 2020 04:42:59 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Hanxiao", ""], ["Brock", "Andrew", ""], ["Simonyan", "Karen", ""], ["Le", "Quoc V.", ""]]}, {"id": "2004.03010", "submitter": "Nikolay Nikitin", "authors": "Nikolay O. Nikitin, Iana S. Polonskaia, Anna V. Kalyuzhnaya, Alexander\n  V. Boukhanovsky", "title": "The multi-objective optimisation of breakwaters using evolutionary\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering practice, it is often necessary to increase the effectiveness\nof existing protective constructions for ports and coasts (i. e. breakwaters)\nby extending their configuration, because existing configurations don't provide\nthe appropriate environmental conditions. That extension task can be considered\nas an optimisation problem. In the paper, the multi-objective evolutionary\napproach for the breakwaters optimisation is proposed. Also, a greedy heuristic\nis implemented and included to algorithm, that allows achieving the appropriate\nsolution faster. The task of the identification of the attached breakwaters\noptimal variant that provides the safe ship parking and manoeuvring in large\nBlack Sea Port of Sochi has been used as a case study. The results of the\nexperiments demonstrated the possibility to apply the proposed multi-objective\nevolutionary approach in real-world engineering problems. It allows identifying\nthe Pareto-optimal set of the possible configuration, which can be analysed by\ndecision makers and used for final construction\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 21:48:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nikitin", "Nikolay O.", ""], ["Polonskaia", "Iana S.", ""], ["Kalyuzhnaya", "Anna V.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "2004.03035", "submitter": "Pawel Kalczynski", "authors": "Tammy Drezner, Zvi Drezner and Pawel Kalczynski", "title": "Directional approach to gradual cover: the continuous case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the cover location models is covering demand by facilities\nwithin a given distance. The gradual (or partial) cover replaces abrupt drop\nfrom full cover to no cover by defining gradual decline in cover. In this paper\nwe use a recently proposed rule for calculating the joint cover of a demand\npoint by several facilities termed \"directional gradual cover\". Contrary to all\ngradual cover models, the joint cover depends on the facilities' directions. In\norder to calculate the joint cover, existing models apply the partial cover by\neach facility disregarding their direction. We develop a genetic algorithm to\nsolve the facilities location problem and also solve the problem for facilities\nthat can be located anywhere in the plane. The proposed modifications were\nextensively tested on a case study of covering Orange County, California.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 23:01:41 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Drezner", "Tammy", ""], ["Drezner", "Zvi", ""], ["Kalczynski", "Pawel", ""]]}, {"id": "2004.03156", "submitter": "Alessio Quaglino PhD", "authors": "Giorgio Giannone, Asha Anoosheh, Alessio Quaglino, Pierluca D'Oro,\n  Marco Gallieri, Jonathan Masci", "title": "Real-time Classification from Short Event-Camera Streams using\n  Input-filtering Neural ODEs", "comments": "Submitted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are novel, efficient sensors inspired by the human vision\nsystem, generating an asynchronous, pixel-wise stream of data. Learning from\nsuch data is generally performed through heavy preprocessing and event\nintegration into images. This requires buffering of possibly long sequences and\ncan limit the response time of the inference system. In this work, we instead\npropose to directly use events from a DVS camera, a stream of intensity changes\nand their spatial coordinates. This sequence is used as the input for a novel\n\\emph{asynchronous} RNN-like architecture, the Input-filtering Neural ODEs\n(INODE). This is inspired by the dynamical systems and filtering literature.\nINODE is an extension of Neural ODEs (NODE) that allows for input signals to be\ncontinuously fed to the network, like in filtering. The approach naturally\nhandles batches of time series with irregular time-stamps by implementing a\nbatch forward Euler solver. INODE is trained like a standard RNN, it learns to\ndiscriminate short event sequences and to perform event-by-event online\ninference. We demonstrate our approach on a series of classification tasks,\ncomparing against a set of LSTM baselines. We show that, independently of the\ncamera resolution, INODE can outperform the baselines by a large margin on the\nASL task and it's on par with a much larger LSTM for the NCALTECH task.\nFinally, we show that INODE is accurate even when provided with very few\nevents.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:58:38 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Giannone", "Giorgio", ""], ["Anoosheh", "Asha", ""], ["Quaglino", "Alessio", ""], ["D'Oro", "Pierluca", ""], ["Gallieri", "Marco", ""], ["Masci", "Jonathan", ""]]}, {"id": "2004.03205", "submitter": "Yue Xie", "authors": "Yue Xie, Aneta Neumann, Frank Neumann", "title": "Specific Single- and Multi-Objective Evolutionary Algorithms for the\n  Chance-Constrained Knapsack Problem", "comments": "Accepted for oral presentation at GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chance-constrained knapsack problem is a variant of the classical\nknapsack problem where each item has a weight distribution instead of a\ndeterministic weight. The objective is to maximize the total profit of the\nselected items under the condition that the weight of the selected items only\nexceeds the given weight bound with a small probability of $\\alpha$. In this\npaper, consider problem-specific single-objective and multi-objective\napproaches for the problem. We examine the use of heavy-tail mutations and\nintroduce a problem-specific crossover operator to deal with the\nchance-constrained knapsack problem. Empirical results for single-objective\nevolutionary algorithms show the effectiveness of our operators compared to the\nuse of classical operators. Moreover, we introduce a new effective\nmulti-objective model for the chance-constrained knapsack problem. We use this\nmodel in combination with the problem-specific crossover operator in\nmulti-objective evolutionary algorithms to solve the problem. Our experimental\nresults show that this leads to significant performance improvements when using\nthe approach in evolutionary multi-objective algorithms such as GSEMO and\nNSGA-II.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:46:51 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:27:55 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Xie", "Yue", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""]]}, {"id": "2004.03237", "submitter": "Richard Meyes", "authors": "Richard Meyes, Moritz Schneider, Tobias Meisen", "title": "How Do You Act? An Empirical Study to Understand Behavior of Deep\n  Reinforcement Learning Agents", "comments": "16 pages, currently under review for publication for the ECMLPKDD\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for more transparency of decision-making processes of deep\nreinforcement learning agents is greater than ever, due to their increased use\nin safety critical and ethically challenging domains such as autonomous\ndriving. In this empirical study, we address this lack of transparency\nfollowing an idea that is inspired by research in the field of neuroscience. We\ncharacterize the learned representations of an agent's policy network through\nits activation space and perform partial network ablations to compare the\nrepresentations of the healthy and the intentionally damaged networks. We show\nthat the healthy agent's behavior is characterized by a distinct correlation\npattern between the network's layer activation and the performed actions during\nan episode and that network ablations, which cause a strong change of this\npattern, lead to the agent failing its trained control task. Furthermore, the\nlearned representation of the healthy agent is characterized by a distinct\npattern in its activation space reflecting its different behavioral stages\nduring an episode, which again, when distorted by network ablations, leads to\nthe agent failing its trained control task. Concludingly, we argue in favor of\na new perspective on artificial neural networks as objects of empirical\ninvestigations, just as biological neural systems in neuroscientific studies,\npaving the way towards a new standard of scientific falsifiability with respect\nto research on transparency and interpretability of artificial neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:08:55 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Meyes", "Richard", ""], ["Schneider", "Moritz", ""], ["Meisen", "Tobias", ""]]}, {"id": "2004.03266", "submitter": "Amirhossein Rajabi", "authors": "Amirhossein Rajabi and Carsten Witt", "title": "Self-Adjusting Evolutionary Algorithms for Multimodal Optimization", "comments": "26 pages. Full version of a paper appearing at GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3389833", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical research has shown that self-adjusting and self-adaptive\nmechanisms can provably outperform static settings in evolutionary algorithms\nfor binary search spaces. However, the vast majority of these studies focuses\non unimodal functions which do not require the algorithm to flip several bits\nsimultaneously to make progress. In fact, existing self-adjusting algorithms\nare not designed to detect local optima and do not have any obvious benefit to\ncross large Hamming gaps.\n  We suggest a mechanism called stagnation detection that can be added as a\nmodule to existing evolutionary algorithms (both with and without prior\nself-adjusting algorithms). Added to a simple (1+1) EA, we prove an expected\nruntime on the well-known Jump benchmark that corresponds to an asymptotically\noptimal parameter setting and outperforms other mechanisms for multimodal\noptimization like heavy-tailed mutation. We also investigate the module in the\ncontext of a self-adjusting (1+$\\lambda$) EA and show that it combines the\nprevious benefits of this algorithm on unimodal problems with more efficient\nmultimodal optimization.\n  To explore the limitations of the approach, we additionally present an\nexample where both self-adjusting mechanisms, including stagnation detection,\ndo not help to find a beneficial setting of the mutation rate. Finally, we\ninvestigate our module for stagnation detection experimentally.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:02:10 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 12:01:29 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Rajabi", "Amirhossein", ""], ["Witt", "Carsten", ""]]}, {"id": "2004.03333", "submitter": "Haotong Qin", "authors": "Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu\n  Sebe", "title": "Binary Neural Networks: A Survey", "comments": null, "journal-ref": "Pattern Recognition (2020) 107281", "doi": "10.1016/j.patcog.2020.107281", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary neural network, largely saving the storage and computation, serves\nas a promising technique for deploying deep models on resource-limited devices.\nHowever, the binarization inevitably causes severe information loss, and even\nworse, its discontinuity brings difficulty to the optimization of the deep\nnetwork. To address these issues, a variety of algorithms have been proposed,\nand achieved satisfying progress in recent years. In this paper, we present a\ncomprehensive survey of these algorithms, mainly categorized into the native\nsolutions directly conducting binarization, and the optimized ones using\ntechniques like minimizing the quantization error, improving the network loss\nfunction, and reducing the gradient error. We also investigate other practical\naspects of binary neural networks such as the hardware-friendly design and the\ntraining tricks. Then, we give the evaluation and discussions on different\ntasks, including image classification, object detection and semantic\nsegmentation. Finally, the challenges that may be faced in future research are\nprospected.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 16:47:20 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Qin", "Haotong", ""], ["Gong", "Ruihao", ""], ["Liu", "Xianglong", ""], ["Bai", "Xiao", ""], ["Song", "Jingkuan", ""], ["Sebe", "Nicu", ""]]}, {"id": "2004.03343", "submitter": "Loick Bonniot", "authors": "Lo\\\"ick Bonniot (WIDE), Christoph Neumann, Fran\\c{c}ois Ta\\\"iani\n  (WIDE)", "title": "DiagNet: towards a generic, Internet-scale root cause analysis solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing problems in Internet-scale services remains particularly difficult\nand costly for both content providers and ISPs. Because the Internet is\ndecentralized, the cause of such problems might lie anywhere between an\nend-user's device and the service datacenters. Further, the set of possible\nproblems and causes is not known in advance, making it impossible in practice\nto train a classifier with all combinations of problems, causes and locations.\nIn this paper, we explore how different machine learning techniques can be used\nfor Internet-scale root cause analysis using measurements taken from end-user\ndevices. We show how to build generic models that (i) are agnostic to the\nunderlying network topology, (ii) do not require to define the full set of\npossible causes during training, and (iii) can be quickly adapted to diagnose\nnew services. Our solution, DiagNet, adapts concepts from image processing\nresearch to handle network and system metrics. We evaluate DiagNet with a\nmulti-cloud deployment of online services with injected faults and emulated\nclients with automated browsers. We demonstrate promising root cause analysis\ncapabilities, with a recall of 73.9% including causes only being introduced at\ninference time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:21:32 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bonniot", "Lo\u00efck", "", "WIDE"], ["Neumann", "Christoph", "", "WIDE"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE"]]}, {"id": "2004.03383", "submitter": "Subhashini Venugopalan", "authors": "Shawn Xu, Subhashini Venugopalan, Mukund Sundararajan", "title": "Attribution in Scale and Space", "comments": "CVPR 2020 camera-ready. Code is available at\n  https://github.com/PAIR-code/saliency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the attribution problem [28] for deep networks applied to perception\ntasks. For vision tasks, attribution techniques attribute the prediction of a\nnetwork to the pixels of the input image. We propose a new technique called\n\\emph{Blur Integrated Gradients}. This technique has several advantages over\nother methods. First, it can tell at what scale a network recognizes an object.\nIt produces scores in the scale/frequency dimension, that we find captures\ninteresting phenomena. Second, it satisfies the scale-space axioms [14], which\nimply that it employs perturbations that are free of artifact. We therefore\nproduce explanations that are cleaner and consistent with the operation of deep\nnetworks. Third, it eliminates the need for a 'baseline' parameter for\nIntegrated Gradients [31] for perception tasks. This is desirable because the\nchoice of baseline has a significant effect on the explanations. We compare the\nproposed technique against previous techniques and demonstrate application on\nthree tasks: ImageNet object recognition, Diabetic Retinopathy prediction, and\nAudioSet audio event identification.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:04:16 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 16:41:12 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Xu", "Shawn", ""], ["Venugopalan", "Subhashini", ""], ["Sundararajan", "Mukund", ""]]}, {"id": "2004.03438", "submitter": "Mohammad Majid Al-Rifaie", "authors": "Mohammad Majid al-Rifaie and Marc Cavazza", "title": "Beer Organoleptic Optimisation: Utilising Swarm Intelligence and\n  Evolutionary Computation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customisation in food properties is a challenging task involving optimisation\nof the production process with the demand to support computational creativity\nwhich is geared towards ensuring the presence of alternatives. This paper\naddresses the personalisation of beer properties in the specific case of craft\nbeers where the production process is more flexible. We investigate the problem\nby using three swarm intelligence and evolutionary computation techniques that\nenable brewers to map physico-chemical properties to target organoleptic\nproperties to design a specific brew. While there are several tools, using the\noriginal mathematical and chemistry formulas, or machine learning models that\ndeal with the process of determining beer properties based on the\npre-determined quantities of ingredients, the next step is to investigate an\nautomated quantitative ingredient selection approach. The process is\nillustrated by a number of experiments designing craft beers where the results\nare investigated by \"cloning\" popular commercial brands based on their known\nproperties. Algorithms performance is evaluated using accuracy, efficiency,\nreliability, population-diversity, iteration-based improvements and solution\ndiversity. The proposed approach allows for the discovery of new recipes,\npersonalisation and alternative high-fidelity reproduction of existing ones.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:39:11 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["al-Rifaie", "Mohammad Majid", ""], ["Cavazza", "Marc", ""]]}, {"id": "2004.03573", "submitter": "Maxwell Crouse", "authors": "Maxwell Crouse, Constantine Nakos, Ibrahim Abdelaziz, Kenneth Forbus", "title": "Neural Analogical Matching", "comments": "AAAI version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogy is core to human cognition. It allows us to solve problems based on\nprior experience, it governs the way we conceptualize new information, and it\neven influences our visual perception. The importance of analogy to humans has\nmade it an active area of research in the broader field of artificial\nintelligence, resulting in data-efficient models that learn and reason in\nhuman-like ways. While cognitive perspectives of analogy and deep learning have\ngenerally been studied independently of one another, the integration of the two\nlines of research is a promising step towards more robust and efficient\nlearning techniques. As part of a growing body of research on such an\nintegration, we introduce the Analogical Matching Network: a neural\narchitecture that learns to produce analogies between structured, symbolic\nrepresentations that are largely consistent with the principles of\nStructure-Mapping Theory.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:50:52 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 04:49:40 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 15:01:42 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 22:31:43 GMT"}, {"version": "v5", "created": "Tue, 15 Dec 2020 16:59:33 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Crouse", "Maxwell", ""], ["Nakos", "Constantine", ""], ["Abdelaziz", "Ibrahim", ""], ["Forbus", "Kenneth", ""]]}, {"id": "2004.03590", "submitter": "Ke Li", "authors": "Ke Li, Shichong Peng, Tianhao Zhang, Jitendra Malik", "title": "Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood\n  Estimation", "comments": "To appear in International Journal of Computer Vision (IJCV). arXiv\n  admin note: text overlap with arXiv:1811.12373", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in computer vision and graphics fall within the framework of\nconditional image synthesis. In recent years, generative adversarial nets\n(GANs) have delivered impressive advances in quality of synthesized images.\nHowever, it remains a challenge to generate both diverse and plausible images\nfor the same input, due to the problem of mode collapse. In this paper, we\ndevelop a new generic multimodal conditional image synthesis method based on\nImplicit Maximum Likelihood Estimation (IMLE) and demonstrate improved\nmultimodal image synthesis performance on two tasks, single image\nsuper-resolution and image synthesis from scene layouts. We make our\nimplementation publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:06:55 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Zhang", "Tianhao", ""], ["Malik", "Jitendra", ""]]}, {"id": "2004.03698", "submitter": "Umut \\\"Ozkaya", "authors": "Umut Ozkaya, Saban Ozturk, Mucahid Barstugan", "title": "Coronavirus (COVID-19) Classification using Deep Features Fusion and\n  Ranking Technique", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus (COVID-19) emerged towards the end of 2019. World Health\nOrganization (WHO) was identified it as a global epidemic. Consensus occurred\nin the opinion that using Computerized Tomography (CT) techniques for early\ndiagnosis of pandemic disease gives both fast and accurate results. It was\nstated by expert radiologists that COVID-19 displays different behaviours in CT\nimages. In this study, a novel method was proposed as fusing and ranking deep\nfeatures to detect COVID-19 in early phase. 16x16 (Subset-1) and 32x32\n(Subset-2) patches were obtained from 150 CT images to generate sub-datasets.\nWithin the scope of the proposed method, 3000 patch images have been labelled\nas CoVID-19 and No finding for using in training and testing phase. Feature\nfusion and ranking method have been applied in order to increase the\nperformance of the proposed method. Then, the processed data was classified\nwith a Support Vector Machine (SVM). According to other pre-trained\nConvolutional Neural Network (CNN) models used in transfer learning, the\nproposed method shows high performance on Subset-2 with 98.27% accuracy, 98.93%\nsensitivity, 97.60% specificity, 97.63% precision, 98.28% F1-score and 96.54%\nMatthews Correlation Coefficient (MCC) metrics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:43:44 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Ozkaya", "Umut", ""], ["Ozturk", "Saban", ""], ["Barstugan", "Mucahid", ""]]}, {"id": "2004.03717", "submitter": "Anup Das", "authors": "Shihao Song, Adarsha Balaji, Anup Das, Nagarajan Kandasamy, and James\n  Shackleford", "title": "Compiling Spiking Neural Networks to Neuromorphic Hardware", "comments": "10 pages, 17 figures, accepted at 21st ACM SIGPLAN/SIGBED\n  International Conference on Languages, Compilers, and Tools for Embedded\n  Systems (LCTES 2020)", "journal-ref": null, "doi": "10.1145/3372799.3394364", "report-no": null, "categories": "cs.DC cs.AR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning applications that are implemented with spike-based\ncomputation model, e.g., Spiking Neural Network (SNN), have a great potential\nto lower the energy consumption when they are executed on a neuromorphic\nhardware. However, compiling and mapping an SNN to the hardware is challenging,\nespecially when compute and storage resources of the hardware (viz. crossbar)\nneed to be shared among the neurons and synapses of the SNN. We propose an\napproach to analyze and compile SNNs on a resource-constrained neuromorphic\nhardware, providing guarantee on key performance metrics such as execution time\nand throughput. Our approach makes the following three key contributions.\nFirst, we propose a greedy technique to partition an SNN into clusters of\nneurons and synapses such that each cluster can fit on to the resources of a\ncrossbar. Second, we exploit the rich semantics and expressiveness of\nSynchronous Dataflow Graphs (SDFGs) to represent a clustered SNN and analyze\nits performance using Max-Plus Algebra, considering the available compute and\nstorage capacities, buffer sizes, and communication bandwidth. Third, we\npropose a self-timed execution-based fast technique to compile and admit\nSNN-based applications to a neuromorphic hardware at run-time, adapting\ndynamically to the available resources on the hardware. We evaluate our\napproach with standard SNN-based applications and demonstrate a significant\nperformance improvement compared to current practices.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:13:27 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:02:31 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Song", "Shihao", ""], ["Balaji", "Adarsha", ""], ["Das", "Anup", ""], ["Kandasamy", "Nagarajan", ""], ["Shackleford", "James", ""]]}, {"id": "2004.03761", "submitter": "Shakti Kumar", "authors": "Shakti Kumar, Jerrod Parker, Panteha Naderian", "title": "Adaptive Transformers in RL", "comments": "10 pages with 9 figures and 4 tables. Main text is 6 pages, appendix\n  is 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in Transformers have opened new interesting areas of\nresearch in partially observable reinforcement learning tasks. Results from\nlate 2019 showed that Transformers are able to outperform LSTMs on both memory\nintense and reactive tasks. In this work we first partially replicate the\nresults shown in Stabilizing Transformers in RL on both reactive and memory\nbased environments. We then show performance improvement coupled with reduced\ncomputation when adding adaptive attention span to this Stable Transformer on a\nchallenging DMLab30 environment. The code for all our experiments and models is\navailable at https://github.com/jerrodparker20/adaptive-transformers-in-rl.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 01:03:10 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Kumar", "Shakti", ""], ["Parker", "Jerrod", ""], ["Naderian", "Panteha", ""]]}, {"id": "2004.03839", "submitter": "Zhi-Hua Zhou", "authors": "Shao-Qun Zhang and Zhi-Hua Zhou", "title": "Flexible Transmitter Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current neural networks are mostly built upon the MP model, which usually\nformulates the neuron as executing an activation function on the real-valued\nweighted aggregation of signals received from other neurons. In this paper, we\npropose the Flexible Transmitter (FT) model, a novel bio-plausible neuron model\nwith flexible synaptic plasticity. The FT model employs a pair of parameters to\nmodel the transmitters between neurons and puts up a neuron-exclusive variable\nto record the regulated neurotrophin density, which leads to the formulation of\nthe FT model as a two-variable two-valued function, taking the commonly-used MP\nneuron model as its special case. This modeling manner makes the FT model not\nonly biologically more realistic, but also capable of handling complicated\ndata, even time series. To exhibit its power and potential, we present the\nFlexible Transmitter Network (FTNet), which is built on the most common\nfully-connected feed-forward architecture taking the FT model as the basic\nbuilding block. FTNet allows gradient calculation and can be implemented by an\nimproved back-propagation algorithm in the complex-valued domain. Experiments\non a board range of tasks show the superiority of the proposed FTNet. This\nstudy provides an alternative basic building block in neural networks and\nexhibits the feasibility of developing artificial neural networks with neuronal\nplasticity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:55:12 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 17:36:21 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 14:18:18 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zhang", "Shao-Qun", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "2004.03953", "submitter": "Ana Stanojevic", "authors": "Ana Stanojevic, Giovanni Cherubini, Timoleon Moraitis, Abu Sebastian", "title": "File Classification Based on Spiking Neural Networks", "comments": "5 pages. 5 figures. Accepted at ISCAS 2020 for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a system for file classification in large data sets\nbased on spiking neural networks (SNNs). File information contained in\nkey-value metadata pairs is mapped by a novel correlative temporal encoding\nscheme to spike patterns that are input to an SNN. The correlation between\ninput spike patterns is determined by a file similarity measure. Unsupervised\ntraining of such networks using spike-timing-dependent plasticity (STDP) is\naddressed first. Then, supervised SNN training is considered by backpropagation\nof an error signal that is obtained by comparing the spike pattern at the\noutput neurons with a target pattern representing the desired class. The\nclassification accuracy is measured for various publicly available data sets\nwith tens of thousands of elements, and compared with other learning\nalgorithms, including logistic regression and support vector machines.\nSimulation results indicate that the proposed SNN-based system using memristive\nsynapses may represent a valid alternative to classical machine learning\nalgorithms for inference tasks, especially in environments with asynchronous\ningest of input data and limited resources.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 11:50:29 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Stanojevic", "Ana", ""], ["Cherubini", "Giovanni", ""], ["Moraitis", "Timoleon", ""], ["Sebastian", "Abu", ""]]}, {"id": "2004.04030", "submitter": "Ayobami Adewale Mr", "authors": "Ayobami E. Adewale and Amnir Hadachi", "title": "Neural Networks Model for Travel Time Prediction Based on ODTravel Time\n  Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public transportation system commuters are often interested in getting\naccurate travel time information to plan their daily activities. However, this\ninformation is often difficult to predict accurately due to the irregularities\nof road traffic, caused by factors such as weather conditions, road accidents,\nand traffic jams. In this study, two neural network models namely\nmulti-layer(MLP) perceptron and long short-term model(LSTM) are developed for\npredicting link travel time of a busy route with input generated using\nOrigin-Destination travel time matrix derived from a historical GPS dataset.\nThe experiment result showed that both models can make near-accurate\npredictions however, LSTM is more susceptible to noise as time step increases.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 15:01:13 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Adewale", "Ayobami E.", ""], ["Hadachi", "Amnir", ""]]}, {"id": "2004.04077", "submitter": "Andrea Cossu", "authors": "Andrea Cossu, Antonio Carta, Davide Bacciu", "title": "Continual Learning with Gated Incremental Memories for sequential data\n  processing", "comments": "Accepted as a conference paper at 2020 International Joint Conference\n  on Neural Networks (IJCNN 2020). Part of 2020 IEEE World Congress on\n  Computational Intelligence (IEEE WCCI 2020)", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207550", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn in dynamic, nonstationary environments without\nforgetting previous knowledge, also known as Continual Learning (CL), is a key\nenabler for scalable and trustworthy deployments of adaptive solutions. While\nthe importance of continual learning is largely acknowledged in machine vision\nand reinforcement learning problems, this is mostly under-documented for\nsequence processing tasks. This work proposes a Recurrent Neural Network (RNN)\nmodel for CL that is able to deal with concept drift in input distribution\nwithout forgetting previously acquired knowledge. We also implement and test a\npopular CL approach, Elastic Weight Consolidation (EWC), on top of two\ndifferent types of RNNs. Finally, we compare the performances of our enhanced\narchitecture against EWC and RNNs on a set of standard CL benchmarks, adapted\nto the sequential data processing scenario. Results show the superior\nperformance of our architecture and highlight the need for special solutions\ndesigned to address CL in RNNs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:00:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Cossu", "Andrea", ""], ["Carta", "Antonio", ""], ["Bacciu", "Davide", ""]]}, {"id": "2004.04114", "submitter": "Andrei Velichko", "authors": "A. A. Velichko, D. V. Ryabokon, S. D. Khanin, A. V. Sidorenko, A. G.\n  Rikkiev", "title": "Reservoir Computing using High Order Synchronization of Coupled\n  Oscillators", "comments": "8 pages, 7 figures", "journal-ref": "IOP Conf. Ser.: Mater. Sci. Eng. 862 052062 (2020)", "doi": "10.1088/1757-899X/862/5/052062", "report-no": null, "categories": "cs.ET cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a concept for reservoir computing on oscillators using the\nhigh-order synchronization effect. The reservoir output is presented in the\nform of oscillator synchronization metrics: fractional high-order\nsynchronization value and synchronization efficiency, expressed as a\npercentage. Using two coupled relaxation oscillators built on VO2 switches, we\ncreated an oscillator reservoir that allows simulating the XOR operation. The\nreservoir can operate as with static input data (power currents, coupling\nforces), as with dynamic data in the form of spike sequences. Having a small\nnumber of oscillators and significant non-linearity, the reservoir expresses a\nwide range of dynamic states. The proposed computing concept can be implemented\non oscillators of diverse nature.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:47:25 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Velichko", "A. A.", ""], ["Ryabokon", "D. V.", ""], ["Khanin", "S. D.", ""], ["Sidorenko", "A. V.", ""], ["Rikkiev", "A. G.", ""]]}, {"id": "2004.04249", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Mohammad Samragh, Tara Javidi, Farinaz Koushanfar", "title": "GeneCAI: Genetic Evolution for Acquiring Compact AI", "comments": null, "journal-ref": null, "doi": "10.1145/3377930.3390226", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving\ntowards more complex architectures to achieve higher inference accuracy. Model\ncompression techniques can be leveraged to efficiently deploy such\ncompute-intensive architectures on resource-limited mobile devices. Such\nmethods comprise various hyper-parameters that require per-layer customization\nto ensure high accuracy. Choosing such hyper-parameters is cumbersome as the\npertinent search space grows exponentially with model layers. This paper\nintroduces GeneCAI, a novel optimization method that automatically learns how\nto tune per-layer compression hyper-parameters. We devise a bijective\ntranslation scheme that encodes compressed DNNs to the genotype space. The\noptimality of each genotype is measured using a multi-objective score based on\naccuracy and number of floating point operations. We develop customized genetic\noperations to iteratively evolve the non-dominated solutions towards the\noptimal Pareto front, thus, capturing the optimal trade-off between model\naccuracy and complexity. GeneCAI optimization method is highly scalable and can\nachieve a near-linear performance boost on distributed multi-GPU platforms. Our\nextensive evaluations demonstrate that GeneCAI outperforms existing rule-based\nand reinforcement learning methods in DNN compression by finding models that\nlie on a better accuracy-complexity Pareto curve.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:56:37 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 04:35:42 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Samragh", "Mohammad", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2004.04519", "submitter": "George Hall", "authors": "George T. Hall, Pietro Simone Oliveto, Dirk Sudholt", "title": "Analysis of the Performance of Algorithm Configurators for Search\n  Heuristics with Global Mutation Operators", "comments": "To appear at GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been proved that a simple algorithm configurator called\nParamRLS can efficiently identify the optimal neighbourhood size to be used by\nstochastic local search to optimise two standard benchmark problem classes. In\nthis paper we analyse the performance of algorithm configurators for tuning the\nmore sophisticated global mutation operator used in standard evolutionary\nalgorithms, which flips each of the $n$ bits independently with probability\n$\\chi/n$ and the best value for $\\chi$ has to be identified. We compare the\nperformance of configurators when the best-found fitness values within the\ncutoff time $\\kappa$ are used to compare configurations against the actual\noptimisation time for two standard benchmark problem classes, Ridge and\nLeadingOnes. We rigorously prove that all algorithm configurators that use\noptimisation time as performance metric require cutoff times that are at least\nas large as the expected optimisation time to identify the optimal\nconfiguration. Matters are considerably different if the fitness metric is\nused. To show this we prove that the simple ParamRLS-F configurator can\nidentify the optimal mutation rates even when using cutoff times that are\nconsiderably smaller than the expected optimisation time of the best parameter\nvalue for both problem classes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:42:30 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Hall", "George T.", ""], ["Oliveto", "Pietro Simone", ""], ["Sudholt", "Dirk", ""]]}, {"id": "2004.04560", "submitter": "Alexander Vandesompele", "authors": "Alexander Vandesompele, Gabriel Urbain, Francis wyffels, Joni Dambre", "title": "Populations of Spiking Neurons for Reservoir Computing: Closed Loop\n  Control of a Compliant Quadruped", "comments": null, "journal-ref": null, "doi": "10.1016/j.cogsys.2019.08.002", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compliant robots can be more versatile than traditional robots, but their\ncontrol is more complex. The dynamics of compliant bodies can however be turned\ninto an advantage using the physical reservoir computing frame-work. By feeding\nsensor signals to the reservoir and extracting motor signals from the\nreservoir, closed loop robot control is possible. Here, we present a novel\nframework for implementing central pattern generators with spiking neural\nnetworks to obtain closed loop robot control. Using the FORCE learning\nparadigm, we train a reservoir of spiking neuron populations to act as a\ncentral pattern generator. We demonstrate the learning of predefined gait\npatterns, speed control and gait transition on a simulated model of a compliant\nquadrupedal robot.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 14:32:49 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 13:29:35 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Vandesompele", "Alexander", ""], ["Urbain", "Gabriel", ""], ["wyffels", "Francis", ""], ["Dambre", "Joni", ""]]}, {"id": "2004.04633", "submitter": "Jamal Toutouh", "authors": "Emiliano Perez, Sergio Nesmachnow, Jamal Toutouh, Erik Hemberg,\n  Una-May O'Reilly", "title": "Parallel/distributed implementation of cellular training for generative\n  adversarial neural networks", "comments": "This article has been accepted for publication in IEEE International\n  Parallel and Distributed Processing Symposium, Parallel and Distributed\n  Combinatorics and Optimization, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are widely used to learn generative\nmodels. GANs consist of two networks, a generator and a discriminator, that\napply adversarial learning to optimize their parameters. This article presents\na parallel/distributed implementation of a cellular competitive coevolutionary\nmethod to train two populations of GANs. A distributed memory parallel\nimplementation is proposed for execution in high performance/supercomputing\ncenters. Efficient results are reported on addressing the generation of\nhandwritten digits (MNIST dataset samples). Moreover, the proposed\nimplementation is able to reduce the training times and scale properly when\nconsidering different grid sizes for training.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 16:01:58 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 15:12:43 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 17:55:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Perez", "Emiliano", ""], ["Nesmachnow", "Sergio", ""], ["Toutouh", "Jamal", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2004.04642", "submitter": "Jamal Toutouh", "authors": "Jamal Toutouh, Una-May O'Reilly, Erik Hemberg", "title": "Data Dieting in GAN Training", "comments": "Chapter 14 of the Book \"Deep Neural Evolution - Deep Learning with\n  Evolutionary Computation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate training Generative Adversarial Networks, GANs, with less\ndata. Subsets of the training dataset can express empirical sample diversity\nwhile reducing training resource requirements, e.g. time and memory. We ask how\nmuch data reduction impacts generator performance and gauge the additive value\nof generator ensembles. In addition to considering stand-alone GAN training and\nensembles of generator models, we also consider reduced data training on an\nevolutionary GAN training framework named Redux-Lipizzaner. Redux-Lipizzaner\nmakes GAN training more robust and accurate by exploiting overlapping\nneighborhood-based training on a spatial 2D grid. We conduct empirical\nexperiments on Redux-Lipizzaner using the MNIST and CelebA data sets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:39:42 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Toutouh", "Jamal", ""], ["O'Reilly", "Una-May", ""], ["Hemberg", "Erik", ""]]}, {"id": "2004.04687", "submitter": "Majid Farzaneh", "authors": "Majid Farzaneh and Rahil Mahdian Toroghi", "title": "GGA-MG: Generative Genetic Algorithm for Music Generation", "comments": "14 pages, Submitted to Journal of Evolutionary Intelligence", "journal-ref": null, "doi": "10.13140/RG.2.2.16677.24805", "report-no": null, "categories": "cs.SD cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music Generation (MG) is an interesting research topic that links the art of\nmusic and Artificial Intelligence (AI). The goal is to train an artificial\ncomposer to generate infinite, fresh, and pleasurable musical pieces. Music has\ndifferent parts such as melody, harmony, and rhythm. In this paper, we propose\na Generative Genetic Algorithm (GGA) to produce a melody automatically. The\nmain GGA uses a Long Short-Term Memory (LSTM) recurrent neural network as the\nobjective function, which should be trained by a spectrum of bad-to-good\nmelodies. These melodies have to be provided by another GGA with a different\nobjective function. Good melodies have been provided by CAMPINs collection. We\nhave considered the rhythm in this work, too. The experimental results clearly\nshow that the proposed GGA method is able to generate eligible melodies with\nnatural transitions and without rhythm error.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 22:45:42 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Farzaneh", "Majid", ""], ["Toroghi", "Rahil Mahdian", ""]]}, {"id": "2004.04796", "submitter": "Victor Costa", "authors": "Victor Costa, Nuno Louren\\c{c}o, Jo\\~ao Correia, Penousal Machado", "title": "Using Skill Rating as Fitness on the Evolution of GANs", "comments": "Published in EvoApplications 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are an adversarial model that achieved\nimpressive results on generative tasks. In spite of the relevant results, GANs\npresent some challenges regarding stability, making the training usually a\nhit-and-miss process. To overcome these challenges, several improvements were\nproposed to better handle the internal characteristics of the model, such as\nalternative loss functions or architectural changes on the neural networks used\nby the generator and the discriminator. Recent works proposed the use of\nevolutionary algorithms on GAN training, aiming to solve these challenges and\nto provide an automatic way to find good models. In this context, COEGAN\nproposes the use of coevolution and neuroevolution to orchestrate the training\nof GANs. However, previous experiments detected that some of the fitness\nfunctions used to guide the evolution are not ideal. In this work we propose\nthe evaluation of a game-based fitness function to be used within the COEGAN\nmethod. Skill rating is a metric to quantify the skill of players in a game and\nhas already been used to evaluate GANs. We extend this idea using the skill\nrating in an evolutionary algorithm to train GANs. The results show that skill\nrating can be used as fitness to guide the evolution in COEGAN without the\ndependence of an external evaluator.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:26:51 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 19:54:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Costa", "Victor", ""], ["Louren\u00e7o", "Nuno", ""], ["Correia", "Jo\u00e3o", ""], ["Machado", "Penousal", ""]]}, {"id": "2004.04812", "submitter": "Vinayakumar R", "authors": "Simran K, Prathiksha Balakrishna, Vinayakumar Ravi, Soman KP", "title": "Deep Learning based Frameworks for Handling Imbalance in DGA, Email, and\n  URL Data Analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE cs.SI eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning is a state of the art method for a lot of applications. The\nmain issue is that most of the real-time data is highly imbalanced in nature.\nIn order to avoid bias in training, cost-sensitive approach can be used. In\nthis paper, we propose cost-sensitive deep learning based frameworks and the\nperformance of the frameworks is evaluated on three different Cyber Security\nuse cases which are Domain Generation Algorithm (DGA), Electronic mail (Email),\nand Uniform Resource Locator (URL). Various experiments were performed using\ncost-insensitive as well as cost-sensitive methods and parameters for both of\nthese methods are set based on hyperparameter tuning. In all experiments, the\ncost-sensitive deep learning methods performed better than the cost-insensitive\napproaches. This is mainly due to the reason that cost-sensitive approach gives\nimportance to the classes which have a very less number of samples during\ntraining and this helps to learn all the classes in a more efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:22:25 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 08:12:19 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["K", "Simran", ""], ["Balakrishna", "Prathiksha", ""], ["Ravi", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "2004.04902", "submitter": "Rohan Jagtap", "authors": "Rohan Jagtap, Dr. Sudhir N. Dhage", "title": "An In-depth Walkthrough on Evolution of Neural Machine Translation", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) methodologies have burgeoned from using\nsimple feed-forward architectures to the state of the art; viz. BERT model. The\nuse cases of NMT models have been broadened from just language translations to\nconversational agents (chatbots), abstractive text summarization, image\ncaptioning, etc. which have proved to be a gem in their respective\napplications. This paper aims to study the major trends in Neural Machine\nTranslation, the state of the art models in the domain and a high level\ncomparison between them.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 04:21:05 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Jagtap", "Rohan", ""], ["Dhage", "Dr. Sudhir N.", ""]]}, {"id": "2004.04978", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr, Martin Krejca", "title": "A Simplified Run Time Analysis of the Univariate Marginal Distribution\n  Algorithm on LeadingOnes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With elementary means, we prove a stronger run time guarantee for the\nunivariate marginal distribution algorithm (UMDA) optimizing the LeadingOnes\nbenchmark function in the desirable regime with low genetic drift. If the\npopulation size is at least quasilinear, then, with high probability, the UMDA\nsamples the optimum within a number of iterations that is linear in the problem\nsize divided by the logarithm of the UMDA's selection rate. This improves over\nthe previous guarantee, obtained by Dang and Lehre (2015) via the deep\nlevel-based population method, both in terms of the run time and by\ndemonstrating further run time gains from small selection rates. With similar\narguments as in our upper-bound analysis, we also obtain the first lower bound\nfor this problem. Under similar assumptions, we prove that a bound that matches\nour upper bound up to constant factors holds with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:20:05 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Doerr", "Benjamin", ""], ["Krejca", "Martin", ""]]}, {"id": "2004.05068", "submitter": "Stef Maree", "authors": "S. C. Maree and T. Alderliesten and P. A. N. Bosman", "title": "Uncrowded Hypervolume-based Multi-objective Optimization with Gene-pool\n  Optimal Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domination-based multi-objective (MO) evolutionary algorithms (EAs) are today\narguably the most frequently used type of MOEA. These methods however stagnate\nwhen the majority of the population becomes non-dominated, preventing\nconvergence to the Pareto set. Hypervolume-based MO optimization has shown\npromising results to overcome this. Direct use of the hypervolume however\nresults in no selection pressure for dominated solutions. The recently\nintroduced Sofomore framework overcomes this by solving multiple interleaved\nsingle-objective dynamic problems that iteratively improve a single\napproximation set, based on the uncrowded hypervolume improvement (UHVI). It\nthereby however loses many advantages of population-based MO optimization, such\nas handling multimodality. Here, we reformulate the UHVI as a quality measure\nfor approximation sets, called the uncrowded hypervolume (UHV), which can be\nused to directly solve MO optimization problems with a single-objective\noptimizer. We use the state-of-the-art gene-pool optimal mixing evolutionary\nalgorithm (GOMEA) that is capable of efficiently exploiting the intrinsically\navailable grey-box properties of this problem. The resulting algorithm,\nUHV-GOMEA, is compared to Sofomore equipped with GOMEA, and the\ndomination-based MO-GOMEA. In doing so, we investigate in which scenarios\neither domination-based or hypervolume-based methods are preferred. Finally, we\nconstruct a simple hybrid approach that combines MO-GOMEA with UHV-GOMEA and\noutperforms both.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:14:54 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Maree", "S. C.", ""], ["Alderliesten", "T.", ""], ["Bosman", "P. A. N.", ""]]}, {"id": "2004.05084", "submitter": "Aboul Ella Hassanien Abo", "authors": "Dalia Ezzat, Aboul ell Hassanien, Hassan Aboul Ella (Scientific\n  Research Group in Egypt)", "title": "GSA-DenseNet121-COVID-19: a Hybrid Deep Learning Architecture for the\n  Diagnosis of COVID-19 Disease based on Gravitational Search Optimization\n  Algorithm", "comments": "29 pages - 7 Figures - 5 Tables", "journal-ref": "Applied Soft Computing, Volume 98, January 2021, 106742", "doi": "10.1016/j.asoc.2020.106742", "report-no": null, "categories": "eess.IV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach called GSA-DenseNet121-COVID-19 based on a\nhybrid convolutional neural network (CNN) architecture is proposed using an\noptimization algorithm. The CNN architecture that was used is called\nDenseNet121 and the optimization algorithm that was used is called the\ngravitational search algorithm (GSA). The GSA is adapted to determine the best\nvalues for the hyperparameters of the DenseNet121 architecture, and to achieve\na high level of accuracy in diagnosing COVID-19 disease through chest x-ray\nimage analysis. The obtained results showed that the proposed approach was able\nto correctly classify 98% of the test set. To test the efficacy of the GSA in\nsetting the optimum values for the hyperparameters of DenseNet121, it was\ncompared to another optimization algorithm called social ski driver (SSD). The\ncomparison results demonstrated the efficacy of the proposed\nGSA-DenseNet121-COVID-19 and its ability to better diagnose COVID-19 disease\nthan the SSD-DenseNet121 as the second was able to diagnose only 94% of the\ntest set. As well as, the proposed approach was compared to an approach based\non a CNN architecture called Inception-v3 and the manual search method for\ndetermining the values of the hyperparameters. The results of the comparison\nshowed that the GSA-DenseNet121 was able to beat the other approach, as the\nsecond was able to classify only 95% of the test set samples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 13:30:11 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ezzat", "Dalia", "", "Scientific\n  Research Group in Egypt"], ["Hassanien", "Aboul ell", "", "Scientific\n  Research Group in Egypt"], ["Ella", "Hassan Aboul", "", "Scientific\n  Research Group in Egypt"]]}, {"id": "2004.05328", "submitter": "Javad PourMostafa Roshan Sharami", "authors": "Javad PourMostafa Roshan Sharami, Parsa Abbasi Sarabestani, Seyed\n  Abolghasem Mirroshandel", "title": "DeepSentiPers: Novel Deep Learning Models Trained Over Proposed\n  Augmented Persian Sentiment Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on how to extract opinions over each Persian\nsentence-level text. Deep learning models provided a new way to boost the\nquality of the output. However, these architectures need to feed on big\nannotated data as well as an accurate design. To best of our knowledge, we do\nnot merely suffer from lack of well-annotated Persian sentiment corpus, but\nalso a novel model to classify the Persian opinions in terms of both multiple\nand binary classification. So in this work, first we propose two novel deep\nlearning architectures comprises of bidirectional LSTM and CNN. They are a part\nof a deep hierarchy designed precisely and also able to classify sentences in\nboth cases. Second, we suggested three data augmentation techniques for the\nlow-resources Persian sentiment corpus. Our comprehensive experiments on three\nbaselines and two different neural word embedding methods show that our data\naugmentation methods and intended models successfully address the aims of the\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 07:45:35 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Sharami", "Javad PourMostafa Roshan", ""], ["Sarabestani", "Parsa Abbasi", ""], ["Mirroshandel", "Seyed Abolghasem", ""]]}, {"id": "2004.05380", "submitter": "Fabio Caraffini PhD", "authors": "Johana Florez-Lozano, Fabio Caraffini, Carlos Parra and Mario Gongora", "title": "Training Data Set Assessment for Decision-Making in a Multiagent\n  Landmine Detection Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world problems such as landmine detection require multiple sources of\ninformation to reduce the uncertainty of decision-making. A novel approach to\nsolve these problems includes distributed systems, as presented in this work\nbased on hardware and software multi-agent systems. To achieve a high rate of\nlandmine detection, we evaluate the performance of a trained system over the\ndistribution of samples between training and validation sets. Additionally, a\ngeneral explanation of the data set is provided, presenting the samples\ngathered by a cooperative multi-agent system developed for detecting improvised\nexplosive devices. The results show that input samples affect the performance\nof the output decisions, and a decision-making system can be less sensitive to\nsensor noise with intelligent systems obtained from a diverse and suitably\norganised training set.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:05:30 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Florez-Lozano", "Johana", ""], ["Caraffini", "Fabio", ""], ["Parra", "Carlos", ""], ["Gongora", "Mario", ""]]}, {"id": "2004.05450", "submitter": "Justin Ting", "authors": "Justin Ting, Yan Fang, Ashwin Sanjay Lele, Arijit Raychowdhury", "title": "Bio-inspired Gait Imitation of Hexapod Robot Using Event-Based Vision\n  Sensor and Spiking Neural Network", "comments": "7 pages, 9 figures, to be published in proceeding of IEEE WCCI/IJCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to walk is a sophisticated neurological task for most animals.\nIn order to walk, the brain must synthesize multiple cortices, neural circuits,\nand diverse sensory inputs. Some animals, like humans, imitate surrounding\nindividuals to speed up their learning. When humans watch their peers, visual\ndata is processed through a visual cortex in the brain. This complex problem of\nimitation-based learning forms associations between visual data and muscle\nactuation through Central Pattern Generation (CPG). Reproducing this imitation\nphenomenon on low power, energy-constrained robots that are learning to walk\nremains challenging and unexplored. We propose a bio-inspired feed-forward\napproach based on neuromorphic computing and event-based vision to address the\ngait imitation problem. The proposed method trains a \"student\" hexapod to walk\nby watching an \"expert\" hexapod moving its legs. The student processes the flow\nof Dynamic Vision Sensor (DVS) data with a one-layer Spiking Neural Network\n(SNN). The SNN of the student successfully imitates the expert within a small\nconvergence time of ten iterations and exhibits energy efficiency at the\nsub-microjoule level.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 17:55:34 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ting", "Justin", ""], ["Fang", "Yan", ""], ["Lele", "Ashwin Sanjay", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "2004.05479", "submitter": "Cengiz Pehlevan", "authors": "Alper T. Erdogan, Cengiz Pehlevan", "title": "Blind Bounded Source Separation Using Neural Networks with Local\n  Learning Rules", "comments": "ICASSP 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053114", "report-no": null, "categories": "eess.SP cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem encountered by both natural and engineered signal\nprocessing systems is blind source separation. In many instances of the\nproblem, the sources are bounded by their nature and known to be so, even\nthough the particular bound may not be known. To separate such bounded sources\nfrom their mixtures, we propose a new optimization problem, Bounded Similarity\nMatching (BSM). A principled derivation of an adaptive BSM algorithm leads to a\nrecurrent neural network with a clipping nonlinearity. The network adapts by\nlocal learning rules, satisfying an important constraint for both biological\nplausibility and implementability in neuromorphic hardware.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:20:22 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Erdogan", "Alper T.", ""], ["Pehlevan", "Cengiz", ""]]}, {"id": "2004.05488", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Laurent Rodriguez, Benoit Miramond", "title": "Brain-inspired self-organization with cellular neuromorphic computing\n  for multimodal unsupervised learning", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical plasticity is one of the main features that enable our ability to\nlearn and adapt in our environment. Indeed, the cerebral cortex self-organizes\nitself through structural and synaptic plasticity mechanisms that are very\nlikely at the basis of an extremely interesting characteristic of the human\nbrain development: the multimodal association. In spite of the diversity of the\nsensory modalities, like sight, sound and touch, the brain arrives at the same\nconcepts (convergence). Moreover, biological observations show that one\nmodality can activate the internal representation of another modality when both\nare correlated (divergence). In this work, we propose the Reentrant\nSelf-Organizing Map (ReSOM), a brain-inspired neural system based on the\nreentry theory using Self-Organizing Maps and Hebbian-like learning. We propose\nand compare different computational methods for unsupervised learning and\ninference, then quantify the gain of the ReSOM in a multimodal classification\ntask. The divergence mechanism is used to label one modality based on the\nother, while the convergence mechanism is used to improve the overall accuracy\nof the system. We perform our experiments on a constructed written/spoken\ndigits database and a DVS/EMG hand gestures database. The proposed model is\nimplemented on a cellular neuromorphic architecture that enables distributed\ncomputing with local connectivity. We show the gain of the so-called hardware\nplasticity induced by the ReSOM, where the system's topology is not fixed by\nthe user but learned along the system's experience through self-organization.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 21:02:45 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 15:36:46 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 17:10:21 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Khacef", "Lyes", ""], ["Rodriguez", "Laurent", ""], ["Miramond", "Benoit", ""]]}, {"id": "2004.05531", "submitter": "Tianyun Zhang", "authors": "Tianyun Zhang, Xiaolong Ma, Zheng Zhan, Shanglin Zhou, Minghai Qin,\n  Fei Sun, Yen-Kuang Chen, Caiwen Ding, Makan Fardad and Yanzhi Wang", "title": "A Unified DNN Weight Compression Framework Using Reweighted Optimization\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the large model size and intensive computation requirement of deep\nneural networks (DNNs), weight pruning techniques have been proposed and\ngenerally fall into two categories, i.e., static regularization-based pruning\nand dynamic regularization-based pruning. However, the former method currently\nsuffers either complex workloads or accuracy degradation, while the latter one\ntakes a long time to tune the parameters to achieve the desired pruning rate\nwithout accuracy loss. In this paper, we propose a unified DNN weight pruning\nframework with dynamically updated regularization terms bounded by the\ndesignated constraint, which can generate both non-structured sparsity and\ndifferent kinds of structured sparsity. We also extend our method to an\nintegrated framework for the combination of different DNN compression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 02:59:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Tianyun", ""], ["Ma", "Xiaolong", ""], ["Zhan", "Zheng", ""], ["Zhou", "Shanglin", ""], ["Qin", "Minghai", ""], ["Sun", "Fei", ""], ["Chen", "Yen-Kuang", ""], ["Ding", "Caiwen", ""], ["Fardad", "Makan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2004.05565", "submitter": "Alvin Wan", "authors": "Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian,\n  Saining Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, Peter Vajda, Joseph E.\n  Gonzalez", "title": "FBNetV2: Differentiable Neural Architecture Search for Spatial and\n  Channel Dimensions", "comments": "8 pages, 10 figures, accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Neural Architecture Search (DNAS) has demonstrated great\nsuccess in designing state-of-the-art, efficient neural networks. However,\nDARTS-based DNAS's search space is small when compared to other search\nmethods', since all candidate network layers must be explicitly instantiated in\nmemory. To address this bottleneck, we propose a memory and computationally\nefficient DNAS variant: DMaskingNAS. This algorithm expands the search space by\nup to $10^{14}\\times$ over conventional DNAS, supporting searches over spatial\nand channel dimensions that are otherwise prohibitively expensive: input\nresolution and number of filters. We propose a masking mechanism for feature\nmap reuse, so that memory and computational costs stay nearly constant as the\nsearch space expands. Furthermore, we employ effective shape propagation to\nmaximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield\nstate-of-the-art performance when compared with all previous architectures.\nWith up to 421$\\times$ less search cost, DMaskingNAS finds models with 0.9%\nhigher accuracy, 15% fewer FLOPs than MobileNetV3-Small; and with similar\naccuracy but 20% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2\noutperforms MobileNetV3 by 2.6% in accuracy, with equivalent model size.\nFBNetV2 models are open-sourced at\nhttps://github.com/facebookresearch/mobile-vision.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 08:52:15 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Wan", "Alvin", ""], ["Dai", "Xiaoliang", ""], ["Zhang", "Peizhao", ""], ["He", "Zijian", ""], ["Tian", "Yuandong", ""], ["Xie", "Saining", ""], ["Wu", "Bichen", ""], ["Yu", "Matthew", ""], ["Xu", "Tao", ""], ["Chen", "Kan", ""], ["Vajda", "Peter", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2004.05733", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Exponential Upper Bounds for the Runtime of Randomized Search Heuristics", "comments": "Extended version of a paper appearing at PPSN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that proven exponential upper bounds on runtimes, an established\narea in classic algorithms, are interesting also in heuristic search and we\nprove several such results. We show that any of the algorithms randomized local\nsearch, Metropolis algorithm, simulated annealing, and (1+1) evolutionary\nalgorithm can optimize any pseudo-Boolean weakly monotonic function under a\nlarge set of noise assumptions in a runtime that is at most exponential in the\nproblem dimension~$n$. This drastically extends a previous such result, limited\nto the (1+1) EA, the LeadingOnes function, and one-bit or bit-wise prior noise\nwith noise probability at most $1/2$, and at the same time simplifies its\nproof. With the same general argument, among others, we also derive a\nsub-exponential upper bound for the runtime of the $(1,\\lambda)$ evolutionary\nalgorithm on the OneMax problem when the offspring population size $\\lambda$ is\nlogarithmic, but below the efficiency threshold. To show that our approach can\nalso deal with non-trivial parent population sizes, we prove an exponential\nupper bound for the runtime of the mutation-based version of the simple genetic\nalgorithm on the OneMax benchmark, matching a known exponential lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 00:24:33 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 13:52:58 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 12:44:31 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "2004.05795", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai and Nuno Vasconcelos", "title": "Rethinking Differentiable Search for Mixed-Precision Neural Networks", "comments": "accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision networks, with weights and activations quantized to low\nbit-width, are widely used to accelerate inference on edge devices. However,\ncurrent solutions are uniform, using identical bit-width for all filters. This\nfails to account for the different sensitivities of different filters and is\nsuboptimal. Mixed-precision networks address this problem, by tuning the\nbit-width to individual filter requirements. In this work, the problem of\noptimal mixed-precision network search (MPS) is considered. To circumvent its\ndifficulties of discrete search space and combinatorial optimization, a new\ndifferentiable search architecture is proposed, with several novel\ncontributions to advance the efficiency by leveraging the unique properties of\nthe MPS problem. The resulting Efficient differentiable MIxed-Precision network\nSearch (EdMIPS) method is effective at finding the optimal bit allocation for\nmultiple popular networks, and can search a large model, e.g. Inception-V3,\ndirectly on ImageNet without proxy task in a reasonable amount of time. The\nlearned mixed-precision networks significantly outperform their uniform\ncounterparts.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:02:23 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Cai", "Zhaowei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2004.05930", "submitter": "Francesco Conti", "authors": "Francesco Conti", "title": "Technical Report: NEMO DNN Quantization for Deployment Model", "comments": "12 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report aims at defining a formal framework for Deep Neural\nNetwork (DNN) layer-wise quantization, focusing in particular on the problems\nrelated to the final deployment. It also acts as a documentation for the NEMO\n(NEural Minimization for pytOrch) framework. It describes the four DNN\nrepresentations used in NEMO (FullPrecision, FakeQuantized, QuantizedDeployable\nand IntegerDeployable), focusing in particular on a formal definition of the\nlatter two. An important feature of this model, and in particular the\nIntegerDeployable representation, is that it enables DNN inference using purely\nintegers - without resorting to real-valued numbers in any part of the\ncomputation and without relying on an explicit fixed-point numerical\nrepresentation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 13:23:27 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Conti", "Francesco", ""]]}, {"id": "2004.05960", "submitter": "Aboul Ella Hassanien Abo", "authors": "Rizk M. Rizk-Allah and Aboul Ella Hassanien (Scientific Research Group\n  in Egypt)", "title": "COVID-19 forecasting based on an improved interior search algorithm and\n  multi-layer feed forward neural network", "comments": "24 pages, 3 Tables and 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is a novel coronavirus that was emerged in December 2019 within\nWuhan, China. As the crisis of its serious increasing dynamic outbreak in all\nparts of the globe, the forecast maps and analysis of confirmed cases (CS)\nbecomes a vital great changeling task. In this study, a new forecasting model\nis presented to analyze and forecast the CS of COVID-19 for the coming days\nbased on the reported data since 22 Jan 2020. The proposed forecasting model,\nnamed ISACL-MFNN, integrates an improved interior search algorithm (ISA) based\non chaotic learning (CL) strategy into a multi-layer feed-forward neural\nnetwork (MFNN). The ISACL incorporates the CL strategy to enhance the\nperformance of ISA and avoid the trapping in the local optima. By this\nmethodology, it is intended to train the neural network by tuning its\nparameters to optimal values and thus achieving high-accuracy level regarding\nforecasted results. The ISACL-MFNN model is investigated on the official data\nof the COVID-19 reported by the World Health Organization (WHO) to analyze the\nconfirmed cases for the upcoming days. The performance regarding the proposed\nforecasting model is validated and assessed by introducing some indices\nincluding the mean absolute error (MAE), root mean square error (RMSE) and mean\nabsolute percentage error (MAPE) and the comparisons with other optimization\nalgorithms are presented. The proposed model is investigated in the most\naffected countries (i.e., USA, Italy, and Spain). The experimental simulations\nillustrate that the proposed ISACL-MFNN provides promising performance rather\nthan the other algorithms while forecasting task for the candidate countries.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 12:08:10 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Rizk-Allah", "Rizk M.", "", "Scientific Research Group\n  in Egypt"], ["Hassanien", "Aboul Ella", "", "Scientific Research Group\n  in Egypt"]]}, {"id": "2004.06243", "submitter": "Priyabrata Saha", "authors": "Priyabrata Saha, Saurabh Dash, Saibal Mukhopadhyay", "title": "Physics-Incorporated Convolutional Recurrent Neural Networks for Source\n  Identification and Forecasting of Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal dynamics of physical processes are generally modeled using\npartial differential equations (PDEs). Though the core dynamics follows some\nprinciples of physics, real-world physical processes are often driven by\nunknown external sources. In such cases, developing a purely analytical model\nbecomes very difficult and data-driven modeling can be of assistance. In this\npaper, we present a hybrid framework combining physics-based numerical models\nwith deep learning for source identification and forecasting of spatio-temporal\ndynamical systems with unobservable time-varying external sources. We formulate\nour model PhICNet as a convolutional recurrent neural network (RNN) which is\nend-to-end trainable for spatio-temporal evolution prediction of dynamical\nsystems and learns the source behavior as an internal state of the RNN.\nExperimental results show that the proposed model can forecast the dynamics for\na relatively long time and identify the sources as well.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 00:27:18 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 02:19:38 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Saha", "Priyabrata", ""], ["Dash", "Saurabh", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "2004.06395", "submitter": "Yusuke Nojima", "authors": "Koen van der Blom, Timo M. Deist, Tea Tu\\v{s}ar, Mariapia Marchi,\n  Yusuke Nojima, Akira Oyama, Vanessa Volz, Boris Naujoks", "title": "Towards Realistic Optimization Benchmarks: A Questionnaire on the\n  Properties of Real-World Problems", "comments": "2 pages, GECCO2020 Poster Paper", "journal-ref": null, "doi": "10.1145/3377929.3389974", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarks are a useful tool for empirical performance comparisons. However,\none of the main shortcomings of existing benchmarks is that it remains largely\nunclear how they relate to real-world problems. What does an algorithm's\nperformance on a benchmark say about its potential on a specific real-world\nproblem? This work aims to identify properties of real-world problems through a\nquestionnaire on real-world single-, multi-, and many-objective optimization\nproblems. Based on initial responses, a few challenges that have to be\nconsidered in the design of realistic benchmarks can already be identified. A\nkey point for future work is to gather more responses to the questionnaire to\nallow an analysis of common combinations of properties. In turn, such common\ncombinations can then be included in improved benchmark suites. To gather more\ndata, the reader is invited to participate in the questionnaire at:\nhttps://tinyurl.com/opt-survey\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 10:04:38 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["van der Blom", "Koen", ""], ["Deist", "Timo M.", ""], ["Tu\u0161ar", "Tea", ""], ["Marchi", "Mariapia", ""], ["Nojima", "Yusuke", ""], ["Oyama", "Akira", ""], ["Volz", "Vanessa", ""], ["Naujoks", "Boris", ""]]}, {"id": "2004.06538", "submitter": "Denis Antipov", "authors": "Denis Antipov, Maxim Buzdalov, Benjamin Doerr", "title": "Fast Mutation in Crossover-based Algorithms", "comments": "This is a version of the same paper presented at GECCO 2020 completed\n  with the proofs which were missing because of the page limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heavy-tailed mutation operator proposed in Doerr, Le, Makhmara, and\nNguyen (GECCO 2017), called \\emph{fast mutation} to agree with the previously\nused language, so far was proven to be advantageous only in mutation-based\nalgorithms. There, it can relieve the algorithm designer from finding the\noptimal mutation rate and nevertheless obtain a performance close to the one\nthat the optimal mutation rate gives.\n  In this first runtime analysis of a crossover-based algorithm using a\nheavy-tailed choice of the mutation rate, we show an even stronger impact. For\nthe $(1+(\\lambda,\\lambda))$ genetic algorithm optimizing the OneMax benchmark\nfunction, we show that with a heavy-tailed mutation rate a linear runtime can\nbe achieved. This is asymptotically faster than what can be obtained with any\nstatic mutation rate, and is asymptotically equivalent to the runtime of the\nself-adjusting version of the parameters choice of the $(1+(\\lambda,\\lambda))$\ngenetic algorithm. This result is complemented by an empirical study which\nshows the effectiveness of the fast mutation also on random satisfiable\nMax-3SAT instances.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:16:42 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:54:49 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 14:02:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Antipov", "Denis", ""], ["Buzdalov", "Maxim", ""], ["Doerr", "Benjamin", ""]]}, {"id": "2004.06559", "submitter": "Eneko Osaba", "authors": "Eneko Osaba, Aritz D. Martinez, Akemi Galvez, Andres Iglesias, Javier\n  Del Ser", "title": "dMFEA-II: An Adaptive Multifactorial Evolutionary Algorithm for\n  Permutation-based Discrete Optimization Problems", "comments": "7 pages, 0 figures, Camera-ready version of the paper accepted for\n  presentation in The Genetic and Evolutionary Computation Conference 2020\n  (GECCO 2020)", "journal-ref": null, "doi": "10.1145/3377929.3398084", "report-no": null, "categories": "cs.AI cs.DM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging research paradigm coined as multitasking optimization aims to\nsolve multiple optimization tasks concurrently by means of a single search\nprocess. For this purpose, the exploitation of complementarities among the\ntasks to be solved is crucial, which is often achieved via the transfer of\ngenetic material, thereby forging the Transfer Optimization field. In this\ncontext, Evolutionary Multitasking addresses this paradigm by resorting to\nconcepts from Evolutionary Computation. Within this specific branch, approaches\nsuch as the Multifactorial Evolutionary Algorithm (MFEA) has lately gained a\nnotable momentum when tackling multiple optimization tasks. This work\ncontributes to this trend by proposing the first adaptation of the recently\nintroduced Multifactorial Evolutionary Algorithm II (MFEA-II) to\npermutation-based discrete optimization environments. For modeling this\nadaptation, some concepts cannot be directly applied to discrete search spaces,\nsuch as parent-centric interactions. In this paper we entirely reformulate such\nconcepts, making them suited to deal with permutation-based search spaces\nwithout loosing the inherent benefits of MFEA-II. The performance of the\nproposed solver has been assessed over 5 different multitasking setups,\ncomposed by 8 datasets of the well-known Traveling Salesman (TSP) and\nCapacitated Vehicle Routing Problems (CVRP). The obtained results and their\ncomparison to those by the discrete version of the MFEA confirm the good\nperformance of the developed dMFEA-II, and concur with the insights drawn in\nprevious studies for continuous optimization.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:42:47 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 14:28:56 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 15:35:08 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Osaba", "Eneko", ""], ["Martinez", "Aritz D.", ""], ["Galvez", "Akemi", ""], ["Iglesias", "Andres", ""], ["Del Ser", "Javier", ""]]}, {"id": "2004.06564", "submitter": "Yali Wang", "authors": "Yali Wang, Bas van Stein, Michael T.M. Emmerich, Thomas B\\\"ack", "title": "A Tailored NSGA-III Instantiation for Flexible Job Shop Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A customized multi-objective evolutionary algorithm (MOEA) is proposed for\nthe multi-objective flexible job shop scheduling problem (FJSP). It uses smart\ninitialization approaches to enrich the first generated population, and\nproposes various crossover operators to create a better diversity of offspring.\nEspecially, the MIP-EGO configurator, which can tune algorithm parameters, is\nadopted to automatically tune operator probabilities. Furthermore, different\nlocal search strategies are employed to explore the neighborhood for better\nsolutions. In general, the algorithm enhancement strategy can be integrated\nwith any standard EMO algorithm. In this paper, it has been combined with\nNSGA-III to solve benchmark multi-objective FJSPs, whereas an off-the-shelf\nimplementation of NSGA-III is not capable of solving the FJSP. The experimental\nresults show excellent performance with less computing budget.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:49:36 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Wang", "Yali", ""], ["van Stein", "Bas", ""], ["Emmerich", "Michael T. M.", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2004.06581", "submitter": "Elena Guti\\'errez Viedma", "authors": "Elena Guti\\'errez, Takamasa Okudono, Masaki Waga, Ichiro Hasuo", "title": "Genetic Algorithm for the Weight Maximization Problem on Weighted\n  Automata", "comments": "Accepted at GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weight maximization problem (WMP) is the problem of finding the word of\nhighest weight on a weighted finite state automaton (WFA). It is an essential\nquestion that emerges in many optimization problems in automata theory.\nUnfortunately, the general problem can be shown to be undecidable, whereas its\nbounded decisional version is NP-complete. Designing efficient algorithms that\nproduce approximate solutions to the WMP in reasonable time is an appealing\nresearch direction that can lead to several new applications including formal\nverification of systems abstracted as WFAs. In particular, in combination with\na recent procedure that translates a recurrent neural network into a weighted\nautomaton, an algorithm for the WMP can be used to analyze and verify the\nnetwork by exploiting the simpler and more compact automata model. In this\nwork, we propose, implement and evaluate a metaheuristic based on genetic\nalgorithms to approximate solutions to the WMP. We experimentally evaluate its\nperformance on examples from the literature and show its potential on different\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 09:30:11 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Guti\u00e9rrez", "Elena", ""], ["Okudono", "Takamasa", ""], ["Waga", "Masaki", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "2004.06632", "submitter": "Leonid Datta", "authors": "Leonid Datta", "title": "A Survey on Activation Functions and their relation with Xavier and He\n  Normal Initialization", "comments": "17 Pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In artificial neural network, the activation function and the weight\ninitialization method play important roles in training and performance of a\nneural network. The question arises is what properties of a function are\nimportant/necessary for being a well-performing activation function. Also, the\nmost widely used weight initialization methods - Xavier and He normal\ninitialization have fundamental connection with activation function. This\nsurvey discusses the important/necessary properties of activation function and\nthe most widely used activation functions (sigmoid, tanh, ReLU, LReLU and\nPReLU). This survey also explores the relationship between these activation\nfunctions and the two weight initialization methods - Xavier and He normal\ninitialization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:17:56 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Datta", "Leonid", ""]]}, {"id": "2004.06674", "submitter": "Ashish Rana", "authors": "Ashish Rana, Taranveer Singh, Harpreet Singh, Neeraj Kumar and\n  Prashant Singh Rana", "title": "Systematically designing better instance counting models on cell images\n  with Neural Arithmetic Logic Units", "comments": "* code repository for project:\n  https://github.com/ashishrana160796/nalu-cell-counting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The big problem for neural network models which are trained to count\ninstances is that whenever test range goes high training range generalization\nerror increases i.e. they are not good generalizers outside training range.\nConsider the case of automating cell counting process where more dense images\nwith higher cell counts are commonly encountered as compared to images used in\ntraining data. By making better predictions for higher ranges of cell count we\nare aiming to create better generalization systems for cell counting. With\narchitecture proposal of neural arithmetic logic units (NALU) for arithmetic\noperations, task of counting has become feasible for higher numeric ranges\nwhich were not included in training data with better accuracy. As a part of our\nstudy we used these units and different other activation functions for learning\ncell counting task with two different architectures namely Fully Convolutional\nRegression Network and U-Net. These numerically biased units are added in the\nform of residual concatenated layers to original architectures and a\ncomparative experimental study is done with these newly proposed changes. This\ncomparative study is described in terms of optimizing regression loss problem\nfrom these models trained with extensive data augmentation techniques. We were\nable to achieve better results in our experiments of cell counting tasks with\nintroduction of these numerically biased units to already existing\narchitectures in the form of residual layer concatenation connections. Our\nresults confirm that above stated numerically biased units does help models to\nlearn numeric quantities for better generalization results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:23:37 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 07:44:46 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rana", "Ashish", ""], ["Singh", "Taranveer", ""], ["Singh", "Harpreet", ""], ["Kumar", "Neeraj", ""], ["Rana", "Prashant Singh", ""]]}, {"id": "2004.06702", "submitter": "Denis Antipov", "authors": "Denis Antipov, Benjamin Doerr, Vitalii Karavaev", "title": "A Rigorous Runtime Analysis of the $(1 + (\\lambda, \\lambda))$ GA on Jump\n  Functions", "comments": "The full version of the paper \"The $(1 + (\\lambda, \\lambda))$ GA Is\n  Even Faster on Multimodal Problems\" presented at GECCO 2020 containing all\n  the proofs omitted in the conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $(1 + (\\lambda,\\lambda))$ genetic algorithm is a younger evolutionary\nalgorithm trying to profit also from inferior solutions. Rigorous runtime\nanalyses on unimodal fitness functions showed that it can indeed be faster than\nclassical evolutionary algorithms, though on these simple problems the gains\nwere only moderate.\n  In this work, we conduct the first runtime analysis of this algorithm on a\nmultimodal problem class, the jump functions benchmark. We show that with the\nright parameters, the \\ollga optimizes any jump function with jump size $2 \\le\nk \\le n/4$ in expected time $O(n^{(k+1)/2} e^{O(k)} k^{-k/2})$, which\nsignificantly and already for constant~$k$ outperforms standard mutation-based\nalgorithms with their $\\Theta(n^k)$ runtime and standard crossover-based\nalgorithms with their $O(n^{k-1})$ runtime guarantee.\n  For the isolated problem of leaving the local optimum of jump functions, we\ndetermine provably optimal parameters that lead to a runtime of $(n/k)^{k/2}\ne^{\\Theta(k)}$. This suggests some general advice on how to set the parameters\nof the \\ollga, which might ease the further use of this algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:54:12 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 19:06:58 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 14:18:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Antipov", "Denis", ""], ["Doerr", "Benjamin", ""], ["Karavaev", "Vitalii", ""]]}, {"id": "2004.06874", "submitter": "Jon McCormack", "authors": "Jon McCormack and Andy Lomas", "title": "Understanding Aesthetic Evaluation using Deep Learning", "comments": "Presented at EvoMUSART 2020 Conference", "journal-ref": "In: Romero J., et al.(eds) Artificial Intelligence in Music,\n  Sound, Art and Design. EvoMUSART 2020. LNCS vol 12103. Springer, Cham (2020)", "doi": "10.1007/978-3-030-43859-3_9", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bottleneck in any evolutionary art system is aesthetic evaluation. Many\ndifferent methods have been proposed to automate the evaluation of aesthetics,\nincluding measures of symmetry, coherence, complexity, contrast and grouping.\nThe interactive genetic algorithm (IGA) relies on human-in-the-loop, subjective\nevaluation of aesthetics, but limits possibilities for large search due to user\nfatigue and small population sizes. In this paper we look at how recent\nadvances in deep learning can assist in automating personal aesthetic\njudgement. Using a leading artist's computer art dataset, we use dimensionality\nreduction methods to visualise both genotype and phenotype space in order to\nsupport the exploration of new territory in any generative system.\nConvolutional Neural Networks trained on the user's prior aesthetic evaluations\nare used to suggest new possibilities similar or between known high quality\ngenotype-phenotype mappings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 04:18:38 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["McCormack", "Jon", ""], ["Lomas", "Andy", ""]]}, {"id": "2004.06930", "submitter": "Uma K", "authors": "D.Sabari Nathan, K.Uma, D Synthiya Vinothini, B. Sathya Bama, S. M. Md\n  Mansoor Roomi", "title": "Light Weight Residual Dense Attention Net for Spectral Reconstruction\n  from RGB Images", "comments": "6pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Hyperspectral Imaging is the acquisition of spectral and spatial information\nof a particular scene. Capturing such information from a specialized\nhyperspectral camera remains costly. Reconstructing such information from the\nRGB image achieves a better solution in both classification and object\nrecognition tasks. This work proposes a novel light weight network with very\nless number of parameters about 233,059 parameters based on Residual dense\nmodel with attention mechanism to obtain this solution. This network uses\nCoordination Convolutional Block to get the spatial information. The weights\nfrom this block are shared by two independent feature extraction mechanisms,\none by dense feature extraction and the other by the multiscale hierarchical\nfeature extraction. Finally, the features from both the feature extraction\nmechanisms are globally fused to produce the 31 spectral bands. The network is\ntrained with NTIRE 2020 challenge dataset and thus achieved 0.0457 MRAE metric\nvalue with less computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 07:58:15 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 03:04:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nathan", "D. Sabari", ""], ["Uma", "K.", ""], ["Vinothini", "D Synthiya", ""], ["Bama", "B. Sathya", ""], ["Roomi", "S. M. Md Mansoor", ""]]}, {"id": "2004.06941", "submitter": "Yali Wang", "authors": "Yali Wang, Andr\\'e Deutz, Thomas B\\\"ack, and Michael Emmerich", "title": "Improving Many-Objective Evolutionary Algorithms by Means of\n  Edge-Rotated Cones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point in $m$-dimensional objective space, any $\\varepsilon$-ball of a\npoint can be partitioned into the incomparable, the dominated and dominating\nregion. The ratio between the size of the incomparable region, and the\ndominated (and dominating) region decreases proportionally to $1/2^{m-1}$,\ni.e., the volume of the Pareto dominating orthant as compared to all other\nvolumes. Due to this reason, it gets increasingly unlikely that dominating\npoints can be found by random, isotropic mutations. As a remedy to stagnation\nof search in many objective optimization, in this paper, we suggest to enhance\nthe Pareto dominance order by involving an obtuse convex dominance cone in the\nconvergence phase of an evolutionary optimization algorithm. We propose\nedge-rotated cones as generalizations of Pareto dominance cones for which the\nopening angle can be controlled by a single parameter only. The approach is\nintegrated in several state-of-the-art multi-objective evolutionary algorithms\n(MOEAs) and tested on benchmark problems with four, five, six and eight\nobjectives. Computational experiments demonstrate the ability of these\nedge-rotated cones to improve the performance of MOEAs on many-objective\noptimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 08:48:06 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 17:53:13 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Wang", "Yali", ""], ["Deutz", "Andr\u00e9", ""], ["B\u00e4ck", "Thomas", ""], ["Emmerich", "Michael", ""]]}, {"id": "2004.06961", "submitter": "Geoffrey Pruvost", "authors": "Geoffrey Pruvost (BONUS), Bilel Derbel (BONUS), Arnaud Liefooghe\n  (BONUS), Ke Li, Qingfu Zhang (CUHK)", "title": "On the Combined Impact of Population Size and Sub-problem Selection in\n  MOEA/D", "comments": "European Conference on Evolutionary Computation in Combinatorial\n  Optimization, Apr 2020, Seville, Spain", "journal-ref": null, "doi": "10.1007/978-3-030-43680-3_9", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper intends to understand and to improve the working principle of\ndecomposition-based multi-objective evolutionary algorithms. We review the\ndesign of the well-established Moea/d framework to support the smooth\nintegration of different strategies for sub-problem selection, while\nemphasizing the role of the population size and of the number of offspring\ncreated at each generation. By conducting a comprehensive empirical analysis on\na wide range of multi-and many-objective combinatorial NK landscapes, we\nprovide new insights into the combined effect of those parameters on the\nanytime performance of the underlying search process. In particular, we show\nthat even a simple random strategy selecting sub-problems at random outperforms\nexisting sophisticated strategies. We also study the sensitivity of such\nstrategies with respect to the ruggedness and the objective space dimension of\nthe target problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:13:32 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Pruvost", "Geoffrey", "", "BONUS"], ["Derbel", "Bilel", "", "BONUS"], ["Liefooghe", "Arnaud", "", "BONUS"], ["Li", "Ke", "", "CUHK"], ["Zhang", "Qingfu", "", "CUHK"]]}, {"id": "2004.07085", "submitter": "Lukas Faber", "authors": "Lukas Faber and Roger Wattenhofer", "title": "Neural Status Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Neural Networks can learn mathematical operations, but they do not\nextrapolate. Extrapolation means that the model can apply to larger numbers,\nwell beyond those observed during training. Recent architectures tackle\narithmetic operations and can extrapolate; however, the equally important\nproblem of quantitative reasoning remains unaddressed. In this work, we propose\na novel architectural element, the Neural Status Register (NSR), for\nquantitative reasoning over numbers. Our NSR relaxes the discrete bit logic of\nphysical status registers to continuous numbers and allows end-to-end learning\nwith gradient descent. Experiments show that the NSR achieves solutions that\nextrapolate to numbers many orders of magnitude larger than those in the\ntraining set. We successfully train the NSR on number comparisons, piecewise\ndiscontinuous functions, counting in sequences, recurrently finding minimums,\nfinding shortest paths in graphs, and comparing digits in images.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:34:37 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:58:29 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Faber", "Lukas", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2004.07136", "submitter": "Albert Susanto", "authors": "Albert Susanto, Herman, Tjeng Wawan Cenggoro, Suharjito, Bens\n  Pardamean", "title": "Transfer-Learning-Aware Neuro-Evolution for Diseases Detection in Chest\n  X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The neural network needs excessive costs of time because of the complexity of\narchitecture when trained on images. Transfer learning and fine-tuning can help\nimprove time and cost efficiency when training a neural network. Yet, Transfer\nlearning and fine-tuning needs a lot of experiment to try with. Therefore, a\nmethod to find the best architecture for transfer learning and fine-tuning is\nneeded. To overcome this problem, neuro-evolution using a genetic algorithm can\nbe used to find the best architecture for transfer learning. To check the\nperformance of this study, dataset ChestX-Ray 14 and DenseNet-121 as a base\nneural network model are used. This study used the AUC score, differences in\nexecution time for training, and McNemar's test to the significance test. In\nterms of result, this study got a 5% difference in the AUC score, 3 % faster in\nterms of execution time, and significance in most of the disease detection.\nFinally, this study gives a concrete summary of how neuro-evolution transfer\nlearning can help in terms of transfer learning and fine-tuning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:06:30 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Susanto", "Albert", ""], ["Herman", "", ""], ["Cenggoro", "Tjeng Wawan", ""], ["Suharjito", "", ""], ["Pardamean", "Bens", ""]]}, {"id": "2004.07141", "submitter": "Weijie Zheng", "authors": "Benjamin Doerr and Weijie Zheng", "title": "From Understanding Genetic Drift to a Smart-Restart Parameter-less\n  Compact Genetic Algorithm", "comments": "4 figures. Extended version of a paper appearing at GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key difficulties in using estimation-of-distribution algorithms is\nchoosing the population size(s) appropriately: Too small values lead to genetic\ndrift, which can cause enormous difficulties. In the regime with no genetic\ndrift, however, often the runtime is roughly proportional to the population\nsize, which renders large population sizes inefficient.\n  Based on a recent quantitative analysis which population sizes lead to\ngenetic drift, we propose a parameter-less version of the compact genetic\nalgorithm that automatically finds a suitable population size without spending\ntoo much time in situations unfavorable due to genetic drift.\n  We prove a mathematical runtime guarantee for this algorithm and conduct an\nextensive experimental analysis on four classic benchmark problems both without\nand with additive centered Gaussian posterior noise. The former shows that\nunder a natural assumption, our algorithm has a performance very similar to the\none obtainable from the best problem-specific population size. The latter\nconfirms that missing the right population size in the original cGA can be\ndetrimental and that previous theory-based suggestions for the population size\ncan be far away from the right values; it also shows that our algorithm as well\nas a previously proposed parameter-less variant of the cGA based on parallel\nruns avoid such pitfalls. Comparing the two parameter-less approaches, ours\nprofits from its ability to abort runs which are likely to be stuck in a\ngenetic drift situation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:12:01 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 10:27:56 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 07:16:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Doerr", "Benjamin", ""], ["Zheng", "Weijie", ""]]}, {"id": "2004.07294", "submitter": "Marc Goerigk", "authors": "Martin Hughes, Marc Goerigk, Trivikram Dokka", "title": "Automatic Generation of Algorithms for Black-Box Robust Optimisation\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop algorithms capable of tackling robust black-box optimisation\nproblems, where the number of model runs is limited. When a desired solution\ncannot be implemented exactly the aim is to find a robust one, where the worst\ncase in an uncertainty neighbourhood around a solution still performs well.\nThis requires a local maximisation within a global minimisation.\n  To investigate improved optimisation methods for robust problems, and remove\nthe need to manually determine an effective heuristic and parameter settings,\nwe employ an automatic generation of algorithms approach: Grammar-Guided\nGenetic Programming. We develop algorithmic building blocks to be implemented\nin a Particle Swarm Optimisation framework, define the rules for constructing\nheuristics from these components, and evolve populations of search algorithms.\nOur algorithmic building blocks combine elements of existing techniques and new\nfeatures, resulting in the investigation of a novel heuristic solution space.\n  As a result of this evolutionary process we obtain algorithms which improve\nupon the current state of the art. We also analyse the component level\nbreakdowns of the populations of algorithms developed against their\nperformance, to identify high-performing heuristic components for robust\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:51:33 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hughes", "Martin", ""], ["Goerigk", "Marc", ""], ["Dokka", "Trivikram", ""]]}, {"id": "2004.07561", "submitter": "Xiangzhi Wei", "authors": "Haohao Zhou, Zhi-Hui Zhan, Zhi-Xin Yang, Xiangzhi Wei", "title": "AMPSO: Artificial Multi-Swarm Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel artificial multi-swarm PSO which consists of\nan exploration swarm, an artificial exploitation swarm and an artificial\nconvergence swarm. The exploration swarm is a set of equal-sized sub-swarms\nrandomly distributed around the particles space, the exploitation swarm is\nartificially generated from a perturbation of the best particle of exploration\nswarm for a fixed period of iterations, and the convergence swarm is\nartificially generated from a Gaussian perturbation of the best particle in the\nexploitation swarm as it is stagnated. The exploration and exploitation\noperations are alternatively carried out until the evolution rate of the\nexploitation is smaller than a threshold or the maximum number of iterations is\nreached. An adaptive inertia weight strategy is applied to different swarms to\nguarantee their performances of exploration and exploitation. To guarantee the\naccuracy of the results, a novel diversity scheme based on the positions and\nfitness values of the particles is proposed to control the exploration,\nexploitation and convergence processes of the swarms. To mitigate the\ninefficiency issue due to the use of diversity, two swarm update techniques are\nproposed to get rid of lousy particles such that nice results can be achieved\nwithin a fixed number of iterations. The effectiveness of AMPSO is validated on\nall the functions in the CEC2015 test suite, by comparing with a set of\ncomprehensive set of 16 algorithms, including the most recently well-performing\nPSO variants and some other non-PSO optimization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:59:45 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 07:09:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhou", "Haohao", ""], ["Zhan", "Zhi-Hui", ""], ["Yang", "Zhi-Xin", ""], ["Wei", "Xiangzhi", ""]]}, {"id": "2004.07607", "submitter": "Jeff Hajewski", "authors": "Jeff Hajewski, Suely Oliveira, and Xiaoyu Xing", "title": "Distributed Evolution of Deep Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders have seen wide success in domains ranging from feature selection\nto information retrieval. Despite this success, designing an autoencoder for a\ngiven task remains a challenging undertaking due to the lack of firm intuition\non how the backing neural network architectures of the encoder and decoder\nimpact the overall performance of the autoencoder. In this work we present a\ndistributed system that uses an efficient evolutionary algorithm to design a\nmodular autoencoder. We demonstrate the effectiveness of this system on the\ntasks of manifold learning and image denoising. The system beats random search\nby nearly an order of magnitude on both tasks while achieving near linear\nhorizontal scaling as additional worker nodes are added to the system.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 11:31:18 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hajewski", "Jeff", ""], ["Oliveira", "Suely", ""], ["Xing", "Xiaoyu", ""]]}, {"id": "2004.07707", "submitter": "Declan Oller", "authors": "Declan Oller, Tobias Glasmachers, Giuseppe Cuccu", "title": "Analyzing Reinforcement Learning Benchmarks with Random Weight Guessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for analyzing and visualizing the complexity of\nstandard reinforcement learning (RL) benchmarks based on score distributions. A\nlarge number of policy networks are generated by randomly guessing their\nparameters, and then evaluated on the benchmark task; the study of their\naggregated results provide insights into the benchmark complexity. Our method\nguarantees objectivity of evaluation by sidestepping learning altogether: the\npolicy network parameters are generated using Random Weight Guessing (RWG),\nmaking our method agnostic to (i) the classic RL setup, (ii) any learning\nalgorithm, and (iii) hyperparameter tuning. We show that this approach isolates\nthe environment complexity, highlights specific types of challenges, and\nprovides a proper foundation for the statistical analysis of the task's\ndifficulty. We test our approach on a variety of classic control benchmarks\nfrom the OpenAI Gym, where we show that small untrained networks can provide a\nrobust baseline for a variety of tasks. The networks generated often show good\nperformance even without gradual learning, incidentally highlighting the\ntriviality of a few popular benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:32:52 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Oller", "Declan", ""], ["Glasmachers", "Tobias", ""], ["Cuccu", "Giuseppe", ""]]}, {"id": "2004.07802", "submitter": "Mikhail Khodak", "authors": "Liam Li, Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar", "title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "comments": "ICLR 2021 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art methods for neural architecture search (NAS) exploit\ngradient-based optimization by relaxing the problem into continuous\noptimization over architectures and shared-weights, a noisy process that\nremains poorly understood. We argue for the study of single-level empirical\nrisk minimization to understand NAS with weight-sharing, reducing the design of\nNAS methods to devising optimizers and regularizers that can quickly obtain\nhigh-quality solutions to this problem. Invoking the theory of mirror descent,\nwe present a geometry-aware framework that exploits the underlying structure of\nthis optimization to return sparse architectural parameters, leading to simple\nyet novel algorithms that enjoy fast convergence guarantees and achieve\nstate-of-the-art accuracy on the latest NAS benchmarks in computer vision.\nNotably, we exceed the best published results for both CIFAR and ImageNet on\nboth the DARTS search space and NAS-Bench201; on the latter we achieve\nnear-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory\nand experiments demonstrate a principled way to co-design optimizers and\ncontinuous relaxations of discrete NAS search spaces.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:46:39 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 16:03:42 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 22:20:48 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 14:44:17 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 17:47:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Li", "Liam", ""], ["Khodak", "Mikhail", ""], ["Balcan", "Maria-Florina", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2004.07864", "submitter": "Shuo Tian", "authors": "Shuo Tian, Lianhua Qu, Kai Hu, Nan Li, Lei Wang and Weixia Xu", "title": "A Neural Architecture Search based Framework for Liquid State Machine\n  Design", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid State Machine (LSM), also known as the recurrent version of Spiking\nNeural Networks (SNN), has attracted great research interests thanks to its\nhigh computational power, biological plausibility from the brain, simple\nstructure and low training complexity. By exploring the design space in network\narchitectures and parameters, recent works have demonstrated great potential\nfor improving the accuracy of LSM model with low complexity. However, these\nworks are based on manually-defined network architectures or predefined\nparameters. Considering the diversity and uniqueness of brain structure, the\ndesign of LSM model should be explored in the largest search space possible. In\nthis paper, we propose a Neural Architecture Search (NAS) based framework to\nexplore both architecture and parameter design space for automatic\ndataset-oriented LSM model. To handle the exponentially-increased design space,\nwe adopt a three-step search for LSM, including multi-liquid architecture\nsearch, variation on the number of neurons and parameters search such as\npercentage connectivity and excitatory neuron ratio within each liquid.\nBesides, we propose to use Simulated Annealing (SA) algorithm to implement the\nthree-step heuristic search. Three datasets, including image dataset of MNIST\nand NMNIST and speech dataset of FSDD, are used to test the effectiveness of\nour proposed framework. Simulation results show that our proposed framework can\nproduce the dataset-oriented optimal LSM models with high accuracy and low\ncomplexity. The best classification accuracy on the three datasets is 93.2%,\n92.5% and 84% respectively with only 1000 spiking neurons, and the network\nconnections can be averagely reduced by 61.4% compared with a single LSM.\nMoreover, we find that the total quantity of neurons in optimal LSM models on\nthree datasets can be further reduced by 20% with only about 0.5% accuracy\nloss.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:55:05 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tian", "Shuo", ""], ["Qu", "Lianhua", ""], ["Hu", "Kai", ""], ["Li", "Nan", ""], ["Wang", "Lei", ""], ["Xu", "Weixia", ""]]}, {"id": "2004.07903", "submitter": "Jeremy Tan", "authors": "Jeremy Tan and Bernhard Kainz", "title": "Divergent Search for Few-Shot Image Classification", "comments": "Submitted to GECCO2020 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is unlabelled and the target task is not known a priori, divergent\nsearch offers a strategy for learning a wide range of skills. Having such a\nrepertoire allows a system to adapt to new, unforeseen tasks. Unlabelled image\ndata is plentiful, but it is not always known which features will be required\nfor downstream tasks. We propose a method for divergent search in the few-shot\nimage classification setting and evaluate with Omniglot and Mini-ImageNet. This\nhigh-dimensional behavior space includes all possible ways of partitioning the\ndata. To manage divergent search in this space, we rely on a meta-learning\nframework to integrate useful features from diverse tasks into a single model.\nThe final layer of this model is used as an index into the `archive' of all\npast behaviors. We search for regions in the behavior space that the current\narchive cannot reach. As expected, divergent search is outperformed by models\nwith a strong bias toward the evaluation tasks. But it is able to match and\nsometimes exceed the performance of models that have a weak bias toward the\ntarget task or none at all. This demonstrates that divergent search is a viable\napproach, even in high-dimensional behavior spaces.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:47:50 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tan", "Jeremy", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2004.08057", "submitter": "Gerard Howard", "authors": "David Howard, Thomas Lowe, Wade Geles", "title": "Diversity-based Design Assist for Large Legged Robots", "comments": "Expanded version of poster for GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine MAP-Elites and highly parallelisable simulation to explore the\ndesign space of a class of large legged robots, which stand at around 2m tall\nand whose design and construction is not well-studied. The simulation is\nmodified to account for factors such as motor torque and weight, and presents a\nreasonable fidelity search space. A novel robot encoding allows for\nbio-inspired features such as legs scaling along the length of the body. The\nimpact of three possible control generation schemes are assessed in the context\nof body-brain co-evolution, showing that even constrained problems benefit\nstrongly from coupling-promoting mechanisms. A two stage process in\nimplemented. In the first stage, a library of possible robots is generated,\ntreating user requirements as constraints. In the second stage, the most\npromising robot niches are analysed and a suite of human-understandable design\nrules generated related to the values of their feature variables. These rules,\ntogether with the library, are then ready to be used by a (human) robot\ndesigner as a Design Assist tool.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:59:17 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Howard", "David", ""], ["Lowe", "Thomas", ""], ["Geles", "Wade", ""]]}, {"id": "2004.08114", "submitter": "Philip John Gorinski", "authors": "Gabriel Gordon-Hall, Philip John Gorinski, Gerasimos Lampouras,\n  Ignacio Iacobacci", "title": "Show Us the Way: Learning to Manage Dialog from Demonstrations", "comments": "8 pages + 2 pages references, 4 figures, 4 tables, accepted to DSTC8\n  Workshop at AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our submission to the End-to-End Multi-Domain Dialog Challenge\nTrack of the Eighth Dialog System Technology Challenge. Our proposed dialog\nsystem adopts a pipeline architecture, with distinct components for Natural\nLanguage Understanding, Dialog State Tracking, Dialog Management and Natural\nLanguage Generation. At the core of our system is a reinforcement learning\nalgorithm which uses Deep Q-learning from Demonstrations to learn a dialog\npolicy with the help of expert examples. We find that demonstrations are\nessential to training an accurate dialog policy where both state and action\nspaces are large. Evaluation of our Dialog Management component shows that our\napproach is effective - beating supervised and reinforcement learning\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:41:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Gordon-Hall", "Gabriel", ""], ["Gorinski", "Philip John", ""], ["Lampouras", "Gerasimos", ""], ["Iacobacci", "Ignacio", ""]]}, {"id": "2004.08140", "submitter": "Jhe-Yu Liou", "authors": "Jhe-Yu Liou, Xiaodong Wang, Stephanie Forrest, Carole-Jean Wu", "title": "GEVO: GPU Code Optimization using Evolutionary Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are a key enabler of the revolution in machine learning and high\nperformance computing, functioning as de facto co-processors to accelerate\nlarge-scale computation. As the programming stack and tool support have\nmatured, GPUs have also become accessible to programmers, who may lack detailed\nknowledge of the underlying architecture and fail to fully leverage the GPU's\ncomputation power. GEVO (Gpu optimization using EVOlutionary computation) is a\ntool for automatically discovering optimization opportunities and tuning the\nperformance of GPU kernels in the LLVM representation. GEVO uses\npopulation-based search to find edits to GPU code compiled to LLVM-IR and\nimproves performance on desired criteria while retaining required\nfunctionality. We demonstrate that GEVO improves the execution time of the GPU\nprograms in the Rodinia benchmark suite and the machine learning models, SVM\nand ResNet18, on NVIDIA Tesla P100. For the Rodinia benchmarks, GEVO improves\nGPU kernel runtime performance by an average of 49.48% and by as much as 412%\nover the fully compiler-optimized baseline. If kernel output accuracy is\nrelaxed to tolerate up to 1% error, GEVO can find kernel variants that\noutperform the baseline version by an average of 51.08%. For the machine\nlearning workloads, GEVO achieves kernel performance improvement for SVM on the\nMNIST handwriting recognition (3.24X) and the a9a income prediction (2.93X)\ndatasets with no loss of model accuracy. GEVO achieves 1.79X kernel performance\nimprovement on image classification using ResNet18/CIFAR-10, with less than 1%\nmodel accuracy reduction.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 09:36:17 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 21:30:52 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Liou", "Jhe-Yu", ""], ["Wang", "Xiaodong", ""], ["Forrest", "Stephanie", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2004.08170", "submitter": "Javier Del Ser Dr.", "authors": "Javier Del Ser, Ibai Lana, Eric L. Manibardo, Izaskun Oregi, Eneko\n  Osaba, Jesus L. Lobo, Miren Nekane Bilbao, Eleni I. Vlahogianni", "title": "Deep Echo State Networks for Short-Term Traffic Forecasting: Performance\n  Comparison and Statistical Assessment", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In short-term traffic forecasting, the goal is to accurately predict future\nvalues of a traffic parameter of interest occurring shortly after the\nprediction is queried. The activity reported in this long-standing research\nfield has been lately dominated by different Deep Learning approaches, yielding\noverly complex forecasting models that in general achieve accuracy gains of\nquestionable practical utility. In this work we elaborate on the performance of\nDeep Echo State Networks for this particular task. The efficient learning\nalgorithm and simpler parametric configuration of these alternative modeling\napproaches make them emerge as a competitive traffic forecasting method for\nreal ITS applications deployed in devices and systems with stringently limited\ncomputational resources. An extensive comparison benchmark is designed with\nreal traffic data captured over the city of Madrid (Spain), amounting to more\nthan 130 automatic Traffic Readers (ATRs) and several shallow learning,\nensembles and Deep Learning models. Results from this comparison benchmark and\nthe analysis of the statistical significance of the reported performance gaps\nare decisive: Deep Echo State Networks achieve more accurate traffic forecasts\nthan the rest of considered modeling counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:07:25 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Del Ser", "Javier", ""], ["Lana", "Ibai", ""], ["Manibardo", "Eric L.", ""], ["Oregi", "Izaskun", ""], ["Osaba", "Eneko", ""], ["Lobo", "Jesus L.", ""], ["Bilbao", "Miren Nekane", ""], ["Vlahogianni", "Eleni I.", ""]]}, {"id": "2004.08193", "submitter": "Markus Wagner", "authors": "Sebastian Baltes, Markus Wagner", "title": "An Annotated Dataset of Stack Overflow Post Edits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve software engineering, software repositories have been mined for\ncode snippets and bug fixes. Typically, this mining takes place at the level of\nfiles or commits. To be able to dig deeper and to extract insights at a higher\nresolution, we hereby present an annotated dataset that contains over 7 million\nedits of code and text on Stack Overflow. Our preliminary study indicates that\nthese edits might be a treasure trove for mining information about fine-grained\npatches, e.g., for the optimisation of non-functional properties.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:59:35 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 00:36:16 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Baltes", "Sebastian", ""], ["Wagner", "Markus", ""]]}, {"id": "2004.08433", "submitter": "Tom Hartmann", "authors": "Daniel Abitz, Tom Hartmann, Martin Middendorf", "title": "A Weighted Population Update Rule for PACO Applied to the Single Machine\n  Total Weighted Tardiness Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new population update rule for population based ant colony\noptimization (PACO) is proposed. PACO is a well known alternative to the\nstandard ant colony optimization algorithm. The new update rule allows to\nweight different parts of the solutions. PACO with the new update rule is\nevaluated for the example of the single machine total weighted tardiness\nproblem (SMTWTP). This is an $\\mathcal{NP}$-hard optimization problem where the\naim is to schedule jobs on a single machine such that their total weighted\ntardiness is minimized. PACO with the new population update rule is evaluated\nwith several benchmark instances from the OR-Library. Moreover, the impact of\nthe weights of the jobs on the solutions in the population and on the\nconvergence of the algorithm are analyzed experimentally. The results show that\nPACO with the new update rule has on average better solution quality than PACO\nwith the standard update rule.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:52:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Abitz", "Daniel", ""], ["Hartmann", "Tom", ""], ["Middendorf", "Martin", ""]]}, {"id": "2004.08664", "submitter": "Maxim Buzdalov", "authors": "Anton Bassin and Maxim Buzdalov", "title": "The $(1+(\\lambda,\\lambda))$ Genetic Algorithm for Permutations", "comments": "This contribution is a slightly extended version of the paper\n  accepted to the GECCO 2020 workshop on permutation-based problems", "journal-ref": null, "doi": "10.1145/3377929.3398148", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $(1+(\\lambda,\\lambda))$ genetic algorithm is a bright example of an\nevolutionary algorithm which was developed based on the insights from\ntheoretical findings. This algorithm uses crossover, and it was shown to\nasymptotically outperform all mutation-based evolutionary algorithms even on\nsimple problems like OneMax. Subsequently it was studied on a number of other\nproblems, but all of these were pseudo-Boolean.\n  We aim at improving this situation by proposing an adaptation of the\n$(1+(\\lambda,\\lambda))$ genetic algorithm to permutation-based problems. Such\nan adaptation is required, because permutations are noticeably different from\nbit strings in some key aspects, such as the number of possible mutations and\ntheir mutual dependence. We also present the first runtime analysis of this\nalgorithm on a permutation-based problem called Ham whose properties resemble\nthose of OneMax. On this problem, where the simple mutation-based algorithms\nhave the running time of $\\Theta(n^2 \\log n)$ for problem size $n$, the\n$(1+(\\lambda,\\lambda))$ genetic algorithm finds the optimum in $O(n^2)$ fitness\nqueries. We augment this analysis with experiments, which show that this\nalgorithm is also fast in practice.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 17:04:57 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 13:13:44 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bassin", "Anton", ""], ["Buzdalov", "Maxim", ""]]}, {"id": "2004.08861", "submitter": "Jie Fu", "authors": "Jie Fu, Xue Geng, Zhijian Duan, Bohan Zhuang, Xingdi Yuan, Adam\n  Trischler, Jie Lin, Chris Pal, Hao Dong", "title": "Role-Wise Data Augmentation for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) is a common method for transferring the\n``knowledge'' learned by one machine learning model (the \\textit{teacher}) into\nanother model (the \\textit{student}), where typically, the teacher has a\ngreater capacity (e.g., more parameters or higher bit-widths). To our\nknowledge, existing methods overlook the fact that although the student absorbs\nextra knowledge from the teacher, both models share the same input data -- and\nthis data is the only medium by which the teacher's knowledge can be\ndemonstrated. Due to the difference in model capacities, the student may not\nbenefit fully from the same data points on which the teacher is trained. On the\nother hand, a human teacher may demonstrate a piece of knowledge with\nindividualized examples adapted to a particular student, for instance, in terms\nof her cultural background and interests. Inspired by this behavior, we design\ndata augmentation agents with distinct roles to facilitate knowledge\ndistillation. Our data augmentation agents generate distinct training data for\nthe teacher and student, respectively. We find empirically that specially\ntailored data points enable the teacher's knowledge to be demonstrated more\neffectively to the student. We compare our approach with existing KD methods on\ntraining popular neural architectures and demonstrate that role-wise data\naugmentation improves the effectiveness of KD over strong prior approaches. The\ncode for reproducing our results can be found at\nhttps://github.com/bigaidream-projects/role-kd\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 14:22:17 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Fu", "Jie", ""], ["Geng", "Xue", ""], ["Duan", "Zhijian", ""], ["Zhuang", "Bohan", ""], ["Yuan", "Xingdi", ""], ["Trischler", "Adam", ""], ["Lin", "Jie", ""], ["Pal", "Chris", ""], ["Dong", "Hao", ""]]}, {"id": "2004.08900", "submitter": "Or Sharir", "authors": "Or Sharir, Barak Peleg and Yoav Shoham", "title": "The Cost of Training NLP Models: A Concise Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the cost of training large-scale language models, and the drivers\nof these costs. The intended audience includes engineers and scientists\nbudgeting their model-training experiments, as well as non-practitioners trying\nto make sense of the economics of modern-day Natural Language Processing (NLP).\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:28:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sharir", "Or", ""], ["Peleg", "Barak", ""], ["Shoham", "Yoav", ""]]}, {"id": "2004.08914", "submitter": "Sima Sinaei", "authors": "Seyed Ahmad Mirsalari, Sima Sinaei, Mostafa E. Salehi, Masoud\n  Daneshtalab", "title": "MuBiNN: Multi-Level Binarized Recurrent Neural Network for EEG signal\n  Classification", "comments": "To appear in IEEE International Symposium on Circuits & Systems in\n  2020. arXiv admin note: text overlap with arXiv:1807.04093 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) are widely used for learning sequences in\napplications such as EEG classification. Complex RNNs could be hardly deployed\non wearable devices due to their computation and memory-intensive processing\npatterns. Generally, reduction in precision leads much more efficiency and\nbinarized RNNs are introduced as energy-efficient solutions. However, naive\nbinarization methods lead to significant accuracy loss in EEG classification.\nIn this paper, we propose a multi-level binarized LSTM, which significantly\nreduces computations whereas ensuring an accuracy pretty close to the full\nprecision LSTM. Our method reduces the delay of the 3-bit LSTM cell operation\n47* with less than 0.01% accuracy loss.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 17:24:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Mirsalari", "Seyed Ahmad", ""], ["Sinaei", "Sima", ""], ["Salehi", "Mostafa E.", ""], ["Daneshtalab", "Masoud", ""]]}, {"id": "2004.08925", "submitter": "Benjamin Paassen", "authors": "Benjamin Paassen, Irena Koprinska, Kalina Yacef", "title": "Tree Echo State Autoencoders with Grammars", "comments": "accepted at the 2020 International Joint Conference on Neural\n  Networks (IJCNN 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree data occurs in many forms, such as computer programs, chemical\nmolecules, or natural language. Unfortunately, the non-vectorial and discrete\nnature of trees makes it challenging to construct functions with tree-formed\noutput, complicating tasks such as optimization or time series prediction.\nAutoencoders address this challenge by mapping trees to a vectorial latent\nspace, where tasks are easier to solve, and then mapping the solution back to a\ntree structure. However, existing autoencoding approaches for tree data fail to\ntake the specific grammatical structure of tree domains into account and rely\non deep learning, thus requiring large training datasets and long training\ntimes. In this paper, we propose tree echo state autoencoders (TES-AE), which\nare guided by a tree grammar and can be trained within seconds by virtue of\nreservoir computing. In our evaluation on three datasets, we demonstrate that\nour proposed approach is not only much faster than a state-of-the-art deep\nlearning autoencoding approach (D-VAE) but also has less autoencoding error if\nlittle data and time is given.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 18:04:33 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Paassen", "Benjamin", ""], ["Koprinska", "Irena", ""], ["Yacef", "Kalina", ""]]}, {"id": "2004.08996", "submitter": "Tom Den Ottelander", "authors": "T. Den Ottelander, A. Dushatskiy, M. Virgolin, P. A. N. Bosman", "title": "Local Search is a Remarkably Strong Baseline for Neural Architecture\n  Search", "comments": "20 pages, 15 figures; Added analysis of LS on single-objective NAS\n  tasks in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS), i.e., the automation of neural network\ndesign, has gained much popularity in recent years with increasingly complex\nsearch algorithms being proposed. Yet, solid comparisons with simple baselines\nare often missing. At the same time, recent retrospective studies have found\nmany new algorithms to be no better than random search (RS). In this work we\nconsider, for the first time, a simple Local Search (LS) algorithm for NAS. We\nparticularly consider a multi-objective NAS formulation, with network accuracy\nand network complexity as two objectives, as understanding the trade-off\nbetween these two objectives is arguably the most interesting aspect of NAS.\nThe proposed LS algorithm is compared with RS and two evolutionary algorithms\n(EAs), as these are often heralded as being ideal for multi-objective\noptimization. To promote reproducibility, we create and release two benchmark\ndatasets, named MacroNAS-C10 and MacroNAS-C100, containing 200K saved network\nevaluations for two established image classification tasks, CIFAR-10 and\nCIFAR-100. Our benchmarks are designed to be complementary to existing\nbenchmarks, especially in that they are better suited for multi-objective\nsearch. We additionally consider a version of the problem with a much larger\narchitecture space. While we find and show that the considered algorithms\nexplore the search space in fundamentally different ways, we also find that LS\nsubstantially outperforms RS and even performs nearly as good as\nstate-of-the-art EAs. We believe that this provides strong evidence that LS is\ntruly a competitive baseline for NAS against which new NAS algorithms should be\nbenchmarked.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 00:08:34 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 12:44:01 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 11:04:47 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ottelander", "T. Den", ""], ["Dushatskiy", "A.", ""], ["Virgolin", "M.", ""], ["Bosman", "P. A. N.", ""]]}, {"id": "2004.09043", "submitter": "Eric Zelikman", "authors": "Eric Zelikman, William Yin, Kenneth Wang", "title": "Learning as Reinforcement: Applying Principles of Neuroscience for More\n  General Reinforcement Learning Agents", "comments": "Originally completed as part of Stanford's CS 234 \"Reinforcement\n  Learning.\" Presented at the California Cognitive Science Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant challenge in developing AI that can generalize well is\ndesigning agents that learn about their world without being told what to learn,\nand apply that learning to challenges with sparse rewards. Moreover, most\ntraditional reinforcement learning approaches explicitly separate learning and\ndecision making in a way that does not correspond to biological learning. We\nimplement an architecture founded in principles of experimental neuroscience,\nby combining computationally efficient abstractions of biological algorithms.\nOur approach is inspired by research on spike-timing dependent plasticity, the\ntransition between short and long term memory, and the role of various\nneurotransmitters in rewarding curiosity. The Neurons-in-a-Box architecture can\nlearn in a wholly generalizable manner, and demonstrates an efficient way to\nbuild and apply representations without explicitly optimizing over a set of\ncriteria or actions. We find it performs well in many environments including\nOpenAI Gym's Mountain Car, which has no reward besides touching a hard-to-reach\nflag on a hill, Inverted Pendulum, where it learns simple strategies to improve\nthe time it holds a pendulum up, a video stream, where it spontaneously learns\nto distinguish an open and closed hand, as well as other environments like\nGoogle Chrome's Dinosaur Game.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:06:21 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zelikman", "Eric", ""], ["Yin", "William", ""], ["Wang", "Kenneth", ""]]}, {"id": "2004.09188", "submitter": "Anh Viet Do", "authors": "Anh Viet Do, Jakob Bossek, Aneta Neumann, Frank Neumann", "title": "Evolving Diverse Sets of Tours for the Travelling Salesperson Problem", "comments": "11 pages, 3 tables, 3 figures, to be published in GECCO '20", "journal-ref": null, "doi": "10.1145/3377930.3389844", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolving diverse sets of high quality solutions has gained increasing\ninterest in the evolutionary computation literature in recent years. With this\npaper, we contribute to this area of research by examining evolutionary\ndiversity optimisation approaches for the classical Traveling Salesperson\nProblem (TSP). We study the impact of using different diversity measures for a\ngiven set of tours and the ability of evolutionary algorithms to obtain a\ndiverse set of high quality solutions when adopting these measures. Our studies\nshow that a large variety of diverse high quality tours can be achieved by\nusing our approaches. Furthermore, we compare our approaches in terms of\ntheoretical properties and the final set of tours obtained by the evolutionary\ndiversity optimisation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:34:07 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Do", "Anh Viet", ""], ["Bossek", "Jakob", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""]]}, {"id": "2004.09274", "submitter": "Hossein Shirazi", "authors": "Ehsan Farzadnia, Hossein Shirazi, Alireza Nowroozi", "title": "A New Intrusion Detection System using the Improved Dendritic Cell\n  Algorithm", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.17223.65442", "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dendritic Cell Algorithm (DCA) as one of the emerging evolutionary\nalgorithms is based on the behavior of the specific immune agents; known as\nDendritic Cells (DCs). DCA has several potentially beneficial features for\nbinary classification problems. In this paper, we aim at providing a new\nversion of this immune-inspired mechanism acts as a semi-supervised classifier\nwhich can be a defensive shield in network intrusion detection problem. Till\nnow, no strategy or idea has already been adopted on the GetAntigen() function\non detection phase, but randomly sampling entails the DCA to provide\nundesirable results in several cycles in each time. This leads to uncertainty.\nWhereas it must be accomplished by biological behaviors of DCs in tissues, we\nhave proposed a novel strategy which exactly acts based on its immunological\nfunctionalities of dendritic cells. The proposed mechanism focuses on two\nitems: First, to obviate the challenge of needing to have a preordered antigen\nset for computing danger signal, and the second, to provide a novel\nimmune-inspired idea in order to non-random data sampling. A variable\nfunctional migration threshold is also computed cycle by cycle that shows\nnecessity of the Migration threshold (MT) flexibility. A significant criterion\nso called capability of intrusion detection (CID) used for tests. All of the\ntests have been performed in a new benchmark dataset named UNSW-NB15.\nExperimental consequences demonstrate that the present schema dominates the\nstandard DCA and has higher CID in comparison with other approaches found in\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:21:34 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Farzadnia", "Ehsan", ""], ["Shirazi", "Hossein", ""], ["Nowroozi", "Alireza", ""]]}, {"id": "2004.09416", "submitter": "Hyeryung Jang", "authors": "Hyeryung Jang, Nicolas Skatchkovsky and Osvaldo Simeone", "title": "VOWEL: A Local Online Learning Rule for Recurrent Networks of\n  Probabilistic Spiking Winner-Take-All Circuits", "comments": "14 pages, submitted for possible conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks of spiking neurons and Winner-Take-All spiking circuits (WTA-SNNs)\ncan detect information encoded in spatio-temporal multi-valued events. These\nare described by the timing of events of interest, e.g., clicks, as well as by\ncategorical numerical values assigned to each event, e.g., like or dislike.\nOther use cases include object recognition from data collected by neuromorphic\ncameras, which produce, for each pixel, signed bits at the times of\nsufficiently large brightness variations. Existing schemes for training\nWTA-SNNs are limited to rate-encoding solutions, and are hence able to detect\nonly spatial patterns. Developing more general training algorithms for\narbitrary WTA-SNNs inherits the challenges of training (binary) Spiking Neural\nNetworks (SNNs). These amount, most notably, to the non-differentiability of\nthreshold functions, to the recurrent behavior of spiking neural models, and to\nthe difficulty of implementing backpropagation in neuromorphic hardware. In\nthis paper, we develop a variational online local training rule for WTA-SNNs,\nreferred to as VOWEL, that leverages only local pre- and post-synaptic\ninformation for visible circuits, and an additional common reward signal for\nhidden circuits. The method is based on probabilistic generalized linear neural\nmodels, control variates, and variational regularization. Experimental results\non real-world neuromorphic datasets with multi-valued events demonstrate the\nadvantages of WTA-SNNs over conventional binary SNNs trained with\nstate-of-the-art methods, especially in the presence of limited computing\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:21:18 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jang", "Hyeryung", ""], ["Skatchkovsky", "Nicolas", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2004.09491", "submitter": "Anton Eremeev", "authors": "Anton V. Eremeev", "title": "On Non-Elitist Evolutionary Algorithms Optimizing Fitness Functions with\n  a Plateau", "comments": "14 pages, accepted for proceedings of Mathematical Optimization\n  Theory and Operations Research (MOTOR 2020). arXiv admin note: text overlap\n  with arXiv:1908.08686", "journal-ref": "Proc. of MOTOR 2020, LNCS, vol. 12095, Springer, Cham. 2020.\n  pp.329-342", "doi": "10.1007/978-3-030-49988-4_23", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the expected runtime of non-elitist evolutionary algorithms\n(EAs), when they are applied to a family of fitness functions with a plateau of\nsecond-best fitness in a Hamming ball of radius r around a unique global\noptimum. On one hand, using the level-based theorems, we obtain polynomial\nupper bounds on the expected runtime for some modes of non-elitist EA based on\nunbiased mutation and the bitwise mutation in particular. On the other hand, we\nshow that the EA with fitness proportionate selection is inefficient if the\nbitwise mutation is used with the standard settings of mutation probability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 03:20:14 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Eremeev", "Anton V.", ""]]}, {"id": "2004.09613", "submitter": "Carola Doerr", "authors": "Maxim Buzdalov and Benjamin Doerr and Carola Doerr and Dmitry\n  Vinokurov", "title": "Fixed-Target Runtime Analysis", "comments": "This is an extended version of a paper which appeared at GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3390184", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime analysis aims at contributing to our understanding of evolutionary\nalgorithms through mathematical analyses of their runtimes. In the context of\ndiscrete optimization problems, runtime analysis classically studies the time\nneeded to find an optimal solution. However, both from a practical and a\ntheoretical viewpoint, more fine-grained performance measures are needed. Two\ncomplementary approaches have been suggested: fixed-budget analysis and\nfixed-target analysis.\n  In this work, we conduct an in-depth study on the advantages and limitations\nof fixed-target analyses. We show that, different from fixed-budget analyses,\nmany classical methods from the runtime analysis of discrete evolutionary\nalgorithms yield fixed-target results without greater effort. We use this to\nconduct a number of new fixed-target analyses. However, we also point out\nexamples where an extension of the existing runtime result to a fixed-target\nresult is highly non-trivial.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:26:35 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 22:07:26 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Buzdalov", "Maxim", ""], ["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Vinokurov", "Dmitry", ""]]}, {"id": "2004.09838", "submitter": "Yiming Peng", "authors": "Yiming Peng, Hisao Ishibuchi", "title": "A Decomposition-based Large-scale Multi-modal Multi-objective\n  Optimization Algorithm", "comments": "8 pages, 8 figures, 3 tables. Accepted by the 2020 IEEE Congress on\n  Evolutionary Computation (IEEE CEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-modal multi-objective optimization problem is a special kind of\nmulti-objective optimization problem with multiple Pareto subsets. In this\npaper, we propose an efficient multi-modal multi-objective optimization\nalgorithm based on the widely used MOEA/D algorithm. In our proposed algorithm,\neach weight vector has its own sub-population. With a clearing mechanism and a\ngreedy removal strategy, our proposed algorithm can effectively preserve\nequivalent Pareto optimal solutions (i.e., different Pareto optimal solutions\nwith same objective values). Experimental results show that our proposed\nalgorithm can effectively preserve the diversity of solutions in the decision\nspace when handling large-scale multi-modal multi-objective optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:18:54 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Peng", "Yiming", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2004.09949", "submitter": "Johannes Lengler", "authors": "Johannes Lengler, Jonas Meier", "title": "Large Population Sizes and Crossover Help in Dynamic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic linear functions on the hypercube are functions which assign to each\nbit a positive weight, but the weights change over time. Throughout\noptimization, these functions maintain the same global optimum, and never have\ndefecting local optima. Nevertheless, it was recently shown [Lengler, Schaller,\nFOCI 2019] that the $(1+1)$-Evolutionary Algorithm needs exponential time to\nfind or approximate the optimum for some algorithm configurations. In this\npaper, we study the effect of larger population sizes for Dynamic BinVal, the\nextremal form of dynamic linear functions. We find that moderately increased\npopulation sizes extend the range of efficient algorithm configurations, and\nthat crossover boosts this positive effect substantially. Remarkably, similar\nto the static setting of monotone functions in [Lengler, Zou, FOGA 2019], the\nhardest region of optimization for $(\\mu+1)$-EA is not close the optimum, but\nfar away from it. In contrast, for the $(\\mu+1)$-GA, the region around the\noptimum is the hardest region in all studied cases.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:26:39 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lengler", "Johannes", ""], ["Meier", "Jonas", ""]]}, {"id": "2004.09969", "submitter": "Javier Del Ser Dr.", "authors": "Antonio LaTorre, Daniel Molina, Eneko Osaba, Javier Del Ser, Francisco\n  Herrera", "title": "Fairness in Bio-inspired Optimization Research: A Prescription of\n  Methodological Guidelines for Comparing Meta-heuristics", "comments": "43 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-inspired optimization (including Evolutionary Computation and Swarm\nIntelligence) is a growing research topic with many competitive bio-inspired\nalgorithms being proposed every year. In such an active area, preparing a\nsuccessful proposal of a new bio-inspired algorithm is not an easy task. Given\nthe maturity of this research field, proposing a new optimization technique\nwith innovative elements is no longer enough. Apart from the novelty, results\nreported by the authors should be proven to achieve a significant advance over\nprevious outcomes from the state of the art. Unfortunately, not all new\nproposals deal with this requirement properly. Some of them fail to select an\nappropriate benchmark or reference algorithms to compare with. In other cases,\nthe validation process carried out is not defined in a principled way (or is\neven not done at all). Consequently, the significance of the results presented\nin such studies cannot be guaranteed. In this work we review several\nrecommendations in the literature and propose methodological guidelines to\nprepare a successful proposal, taking all these issues into account. We expect\nthese guidelines to be useful not only for authors, but also for reviewers and\neditors along their assessment of new contributions to the field.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 04:46:45 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["LaTorre", "Antonio", ""], ["Molina", "Daniel", ""], ["Osaba", "Eneko", ""], ["Del Ser", "Javier", ""], ["Herrera", "Francisco", ""]]}, {"id": "2004.10042", "submitter": "Fengyang Sun", "authors": "Chunxiuzi Liu and Fengyang Sun and Qingrui Ni and Lin Wang and Bo Yang", "title": "A Novel Graphic Bending Transformation on Benchmark", "comments": "11 pages, 7 figures, to be published in Proceedings of 2020 IEEE\n  International Conference on Systems, Man and Cybernetics (SMC 2020), 2020.\n  Chunxiuzi Liu and Fengyang Sun contribute equally to this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical benchmark problems utilize multiple transformation techniques to\nincrease optimization difficulty, e.g., shift for anti centering effect and\nrotation for anti dimension sensitivity. Despite testing the transformation\ninvariance, however, such operations do not really change the landscape's\n\"shape\", but rather than change the \"view point\". For instance, after rotated,\nill conditional problems are turned around in terms of orientation but still\nkeep proportional components, which, to some extent, does not create much\nobstacle in optimization. In this paper, inspired from image processing, we\ninvestigate a novel graphic conformal mapping transformation on benchmark\nproblems to deform the function shape. The bending operation does not alter the\nfunction basic properties, e.g., a unimodal function can almost maintain its\nunimodality after bent, but can modify the shape of interested area in the\nsearch space. Experiments indicate the same optimizer spends more search budget\nand encounter more failures on the conformal bent functions than the rotated\nversion. Several parameters of the proposed function are also analyzed to\nreveal performance sensitivity of the evolutionary algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:31:07 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 12:37:40 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Liu", "Chunxiuzi", ""], ["Sun", "Fengyang", ""], ["Ni", "Qingrui", ""], ["Wang", "Lin", ""], ["Yang", "Bo", ""]]}, {"id": "2004.10048", "submitter": "Nassim Dehouche", "authors": "Nassim Dehouche", "title": "Devolutionary genetic algorithms with application to the minimum\n  labeling Steiner tree problem", "comments": null, "journal-ref": "Evolving Systems volume 9(2018), pages 157-168", "doi": "10.1007/s12530-017-9182-z", "report-no": null, "categories": "math.OC cs.AI cs.NE math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper characterizes and discusses devolutionary genetic algorithms and\nevaluates their performances in solving the minimum labeling Steiner tree\n(MLST) problem. We define devolutionary algorithms as the process of reaching a\nfeasible solution by devolving a population of super-optimal unfeasible\nsolutions over time. We claim that distinguishing them from the widely used\nevolutionary algorithms is relevant. The most important distinction lies in the\nfact that in the former type of processes, the value function decreases over\nsuccessive generation of solutions, thus providing a natural stopping condition\nfor the computation process. We show how classical evolutionary concepts, such\nas crossing, mutation and fitness can be adapted to aim at reaching an optimal\nor close-to-optimal solution among the first generations of feasible solutions.\nWe additionally introduce a novel integer linear programming formulation of the\nMLST problem and a valid constraint used for speeding up the devolutionary\nprocess. Finally, we conduct an experiment comparing the performances of\ndevolutionary algorithms to those of state of the art approaches used for\nsolving randomly generated instances of the MLST problem. Results of this\nexperiment support the use of devolutionary algorithms for the MLST problem and\ntheir development for other NP-hard combinatorial optimization problems.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 13:27:28 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Dehouche", "Nassim", ""]]}, {"id": "2004.10061", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Exploring Distributed Control with the NK Model", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.07429,\n  arXiv:1808.03471, arXiv:1811.04073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NK model has been used widely to explore aspects of natural evolution and\ncomplex systems. This paper introduces a modified form of the NK model for\nexploring distributed control in complex systems such as organisations, social\nnetworks, collective robotics, etc. Initial results show how varying the size\nand underlying functional structure of a given system affects the performance\nof different distributed control structures and decision making, including\nwithin dynamically formed structures and those with differing numbers of\ncontrol nodes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:01:24 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "2004.10424", "submitter": "Vahid Roostapour", "authors": "Vahid Roostapour, Jakob Bossek, Frank Neumann", "title": "Runtime Analysis of Evolutionary Algorithms with Biased Mutation for the\n  Multi-Objective Minimum Spanning Tree Problem", "comments": "To be presented at GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3390168", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) are general-purpose problem solvers that\nusually perform an unbiased search. This is reasonable and desirable in a\nblack-box scenario. For combinatorial optimization problems, often more\nknowledge about the structure of optimal solutions is given, which can be\nleveraged by means of biased search operators. We consider the Minimum Spanning\nTree (MST) problem in a single- and multi-objective version, and introduce a\nbiased mutation, which puts more emphasis on the selection of edges of low rank\nin terms of low domination number. We present example graphs where the biased\nmutation can significantly speed up the expected runtime until (Pareto-)optimal\nsolutions are found. On the other hand, we demonstrate that bias can lead to\nexponential runtime if heavy edges are necessarily part of an optimal solution.\nHowever, on general graphs in the single-objective setting, we show that a\ncombined mutation operator which decides for unbiased or biased edge selection\nin each step with equal probability exhibits a polynomial upper bound -- as\nunbiased mutation -- in the worst case and benefits from bias if the\ncircumstances are favorable.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 07:47:00 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Roostapour", "Vahid", ""], ["Bossek", "Jakob", ""], ["Neumann", "Frank", ""]]}, {"id": "2004.10489", "submitter": "Fabio Caraffini PhD", "authors": "Anna V. Kononova, Fabio Caraffini and Thomas B\\\"ack", "title": "Differential evolution outside the box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how often the popular configurations of Differential\nEvolution generate solutions outside the feasible domain. Following previous\npublications in the field, we argue that what the algorithm does with such\nsolutions and how often this has to happen is important for the overall\nperformance of the algorithm and interpretation of results. Significantly more\nsolutions than what is usually assumed by practitioners have to undergo some\nsort of 'correction' to conform with the definition of the problem's search\ndomain. A wide range of popular Differential Evolution configurations is\nconsidered in this study. Conclusions are made regarding the effect the\nDifferential Evolution components and parameter settings have on the\ndistribution of percentages of infeasible solutions generated in a series of\nindependent runs. Results shown in this study suggest strong dependencies\nbetween percentages of generated infeasible solutions and every aspect\nmentioned above. Further investigation of the distribution of percentages of\ngenerated infeasible solutions is required.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 10:58:05 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kononova", "Anna V.", ""], ["Caraffini", "Fabio", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2004.10558", "submitter": "Eatai Roth", "authors": "Saber Sheybani, Eduardo J. Izquierdo, Eatai Roth", "title": "Evolving Dyadic Strategies for a Cooperative Physical Task", "comments": "6 pages, 4 figures, IEEE Haptics Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cooperative physical tasks require that individuals play specialized\nroles (e.g., leader-follower). Humans are adept cooperators, negotiating these\nroles and transitions between roles innately. Yet how roles are delegated and\nreassigned is not well understood. Using a genetic algorithm, we evolve\nsimulated agents to explore a space of feasible role-switching policies.\nApplying these switching policies in a cooperative manual task, agents process\nvisual and haptic cues to decide when to switch roles. We then analyze the\nevolved virtual population for attributes typically associated with\ncooperation: load sharing and temporal coordination. We find that the best\nperforming dyads exhibit high temporal coordination (anti-synchrony). And in\nturn, anti-synchrony is correlated to symmetry between the parameters of the\ncooperative agents. These simulations furnish hypotheses as to how human\ncooperators might mediate roles in dyadic tasks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:23:12 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sheybani", "Saber", ""], ["Izquierdo", "Eduardo J.", ""], ["Roth", "Eatai", ""]]}, {"id": "2004.10701", "submitter": "John Ram\\'irez", "authors": "John Ram\\'irez-Figueroa, Carlos Mart\\'in-Barreiro, Ana B.\n  Nieto-Librero, Victor Leiva-S\\'anchez, Purificaci\\'on Galindo-Villard\\'on", "title": "Disjoint principal component analysis by constrained binary particle\n  swarm optimization", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an alternative method to the disjoint principal\ncomponent analysis. The method consists of a principal component analysis with\nconstraints, which allows us to determine disjoint components that are linear\ncombinations of disjoint subsets of the original variables. The proposed method\nis named constrained binary optimization by particle swarm disjoint principal\ncomponent analysis, since it is based on the particle swarm optimization. The\nmethod uses stochastic optimization to find solutions in cases of high\ncomputational complexity. The algorithm associated with the method starts\ngenerating randomly a particle population which iteratively evolves until\nattaining a global optimum which is function of the disjoint components.\nNumerical results are provided to confirm the quality of the solutions attained\nby the proposed method. Illustrative examples with real data are conducted to\nshow the potential applications of the method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:07:19 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Ram\u00edrez-Figueroa", "John", ""], ["Mart\u00edn-Barreiro", "Carlos", ""], ["Nieto-Librero", "Ana B.", ""], ["Leiva-S\u00e1nchez", "Victor", ""], ["Galindo-Villard\u00f3n", "Purificaci\u00f3n", ""]]}, {"id": "2004.10854", "submitter": "Dionysios Diamantopoulos", "authors": "Dionysios Diamantopoulos, Burkhard Ringlein, Mitra Purandare,\n  Gagandeep Singh, and Christoph Hagleitner", "title": "Agile Autotuning of a Transprecision Tensor Accelerator Overlay for TVM\n  Compiler Stack", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Specialized accelerators for tensor-operations, such as blocked-matrix\noperations and multi-dimensional convolutions, have been emerged as powerful\narchitecture choices for high-performance Deep-Learning computing. The rapid\ndevelopment of frameworks, models, and precision options challenges the\nadaptability of such tensor-accelerators since the adaptation to new\nrequirements incurs significant engineering costs. Programmable tensor\naccelerators offer a promising alternative by allowing reconfiguration of a\nvirtual architecture that overlays on top of the physical FPGA configurable\nfabric. We propose an overlay ({\\tau}-VTA) and an optimization method guided by\nagile-inspired auto-tuning techniques. We achieve higher performance and faster\nconvergence than state-of-art.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:12:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Diamantopoulos", "Dionysios", ""], ["Ringlein", "Burkhard", ""], ["Purandare", "Mitra", ""], ["Singh", "Gagandeep", ""], ["Hagleitner", "Christoph", ""]]}, {"id": "2004.10874", "submitter": "Ke Li Kl", "authors": "Lei Sun and Ke Li", "title": "Adaptive Operator Selection Based on Dynamic Thompson Sampling for\n  MOEA/D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary computation, different reproduction operators have various\nsearch dynamics. To strike a well balance between exploration and exploitation,\nit is attractive to have an adaptive operator selection (AOS) mechanism that\nautomatically chooses the most appropriate operator on the fly according to the\ncurrent status. This paper proposes a new AOS mechanism for multi-objective\nevolutionary algorithm based on decomposition (MOEA/D). More specifically, the\nAOS is formulated as a multi-armed bandit problem where the dynamic Thompson\nsampling (DYTS) is applied to adapt the bandit learning model, originally\nproposed with an assumption of a fixed award distribution, to a non-stationary\nsetup. In particular, each arm of our bandit learning model represents a\nreproduction operator and is assigned with a prior reward distribution. The\nparameters of these reward distributions will be progressively updated\naccording to the performance of its performance collected from the evolutionary\nprocess. When generating an offspring, an operator is chosen by sampling from\nthose reward distribution according to the DYTS. Experimental results fully\ndemonstrate the effectiveness and competitiveness of our proposed AOS mechanism\ncompared with other four state-of-the-art MOEA/D variants.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 21:41:57 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Sun", "Lei", ""], ["Li", "Ke", ""]]}, {"id": "2004.10883", "submitter": "Aaron Tuor", "authors": "Aaron Tuor, Jan Drgona, Draguna Vrabie", "title": "Constrained Neural Ordinary Differential Equations with Stability\n  Guarantees", "comments": "4 pages, Appendix", "journal-ref": "Presented at DEEPDIFFEQ 2020 : ICLR Workshop on Integration of\n  Deep Neural Models and Differential Equations", "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential equations are frequently used in engineering domains, such as\nmodeling and control of industrial systems, where safety and performance\nguarantees are of paramount importance. Traditional physics-based modeling\napproaches require domain expertise and are often difficult to tune or adapt to\nnew systems. In this paper, we show how to model discrete ordinary differential\nequations (ODE) with algebraic nonlinearities as deep neural networks with\nvarying degrees of prior knowledge. We derive the stability guarantees of the\nnetwork layers based on the implicit constraints imposed on the weight's\neigenvalues. Moreover, we show how to use barrier methods to generically handle\nadditional inequality constraints. We demonstrate the prediction accuracy of\nlearned neural ODEs evaluated on open-loop simulations compared to ground truth\ndynamics with bi-linear terms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 22:07:57 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Tuor", "Aaron", ""], ["Drgona", "Jan", ""], ["Vrabie", "Draguna", ""]]}, {"id": "2004.10978", "submitter": "Bao Trung Nguyen", "authors": "Trung B. Nguyen, Will N. Browne, Mengjie Zhang", "title": "Constructing Complexity-efficient Features in XCS with Tree-based Rule\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major goal of machine learning is to create techniques that abstract away\nirrelevant information. The generalisation property of standard Learning\nClassifier System (LCS) removes such information at the feature level but not\nat the feature interaction level. Code Fragments (CFs), a form of tree-based\nprograms, introduced feature manipulation to discover important interactions,\nbut they often contain irrelevant information, which causes structural\ninefficiency. XOF is a recently introduced LCS that uses CFs to encode building\nblocks of knowledge about feature interaction. This paper aims to optimise the\nstructural efficiency of CFs in XOF. We propose two measures to improve\nconstructing CFs to achieve this goal. Firstly, a new CF-fitness update\nestimates the applicability of CFs that also considers the structural\ncomplexity. The second measure we can use is a niche-based method of generating\nCFs. These approaches were tested on Even-parity and Hierarchical problems,\nwhich require highly complex combinations of input features to capture the data\npatterns. The results show that the proposed methods significantly increase the\nstructural efficiency of CFs, which is estimated by the rule \"generality rate\".\nThis results in faster learning performance in the Hierarchical Majority-on\nproblem. Furthermore, a user-set depth limit for CF generation is not needed as\nthe learning agent will not adopt higher-level CFs once optimal CFs are\nconstructed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:41:41 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Nguyen", "Trung B.", ""], ["Browne", "Will N.", ""], ["Zhang", "Mengjie", ""]]}, {"id": "2004.11018", "submitter": "Vojtech Mrazek", "authors": "David Hodan, Vojtech Mrazek, Zdenek Vasicek", "title": "Semantically-Oriented Mutation Operator in Cartesian Genetic Programming\n  for Evolutionary Circuit Design", "comments": "Accepted for Genetic and Evolutionary Computation Conference (GECCO\n  '20), July 8--12, 2020, Canc\\'un, Mexico", "journal-ref": null, "doi": "10.1145/3377930.3390188", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite many successful applications, Cartesian Genetic Programming (CGP)\nsuffers from limited scalability, especially when used for evolutionary circuit\ndesign. Considering the multiplier design problem, for example, the 5x5-bit\nmultiplier represents the most complex circuit evolved from a randomly\ngenerated initial population. The efficiency of CGP highly depends on the\nperformance of the point mutation operator, however, this operator is purely\nstochastic. This contrasts with the recent developments in Genetic Programming\n(GP), where advanced informed approaches such as semantic-aware operators are\nincorporated to improve the search space exploration capability of GP. In this\npaper, we propose a semantically-oriented mutation operator (SOMO) suitable for\nthe evolutionary design of combinational circuits. SOMO uses semantics to\ndetermine the best value for each mutated gene. Compared to the common CGP and\nits variants as well as the recent versions of Semantic GP, the proposed method\nconverges on common Boolean benchmarks substantially faster while keeping the\nphenotype size relatively small. The successfully evolved instances presented\nin this paper include 10-bit parity, 10+10-bit adder and 5x5-bit multiplier.\nThe most complex circuits were evolved in less than one hour with a\nsingle-thread implementation running on a common CPU.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:15:48 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Hodan", "David", ""], ["Mrazek", "Vojtech", ""], ["Vasicek", "Zdenek", ""]]}, {"id": "2004.11054", "submitter": "Philip John Gorinski", "authors": "Gabriel Gordon-Hall, Philip John Gorinski, Shay B. Cohen", "title": "Learning Dialog Policies from Weak Demonstrations", "comments": "9 pages + 2 pages references + 1 page appendices, 6 figures, 2\n  tables, 1 algorithm, accepted as long paper at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is a promising approach to training a dialog\nmanager, but current methods struggle with the large state and action spaces of\nmulti-domain dialog systems. Building upon Deep Q-learning from Demonstrations\n(DQfD), an algorithm that scores highly in difficult Atari games, we leverage\ndialog data to guide the agent to successfully respond to a user's requests. We\nmake progressively fewer assumptions about the data needed, using labeled,\nreduced-labeled, and even unlabeled data to train expert demonstrators. We\nintroduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to\novercome the domain gap between the datasets and the environment. Experiments\nin a challenging multi-domain dialog system framework validate our approaches,\nand get high success rates even when trained on out-of-domain data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:22:16 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 16:02:03 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gordon-Hall", "Gabriel", ""], ["Gorinski", "Philip John", ""], ["Cohen", "Shay B.", ""]]}, {"id": "2004.11055", "submitter": "Alma Rahat PhD", "authors": "Alma Rahat and Michael Wood", "title": "On Bayesian Search for the Feasible Space Under Computationally\n  Expensive Constraints", "comments": "Accepted at The Sixth International Conference on Machine Learning,\n  Optimization, and Data Science. Main content 12 pages, a total of 19 pages\n  with supplementary. 3 Figures and 2 tables. Python code for Bayesian search\n  is available at: http://bitbucket.org/arahat/lod-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are often interested in identifying the feasible subset of a decision\nspace under multiple constraints to permit effective design exploration. If\ndetermining feasibility required computationally expensive simulations, the\ncost of exploration would be prohibitive. Bayesian search is data-efficient for\nsuch problems: starting from a small dataset, the central concept is to use\nBayesian models of constraints with an acquisition function to locate promising\nsolutions that may improve predictions of feasibility when the dataset is\naugmented. At the end of this sequential active learning approach with a\nlimited number of expensive evaluations, the models can accurately predict the\nfeasibility of any solution obviating the need for full simulations. In this\npaper, we propose a novel acquisition function that combines the probability\nthat a solution lies at the boundary between feasible and infeasible spaces\n(representing exploitation) and the entropy in predictions (representing\nexploration). Experiments confirmed the efficacy of the proposed function.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:22:32 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:00:05 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Rahat", "Alma", ""], ["Wood", "Michael", ""]]}, {"id": "2004.11120", "submitter": "Varun Bhatt", "authors": "Varun Bhatt, Shalini Shrivastava, Tanmay Chavan, Udayan Ganguly", "title": "Software-Level Accuracy Using Stochastic Computing With\n  Charge-Trap-Flash Based Weight Matrix", "comments": "8 pages, 8 figures, submitted to the International Joint Conference\n  on Neural Networks (IJCNN) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The in-memory computing paradigm with emerging memory devices has been\nrecently shown to be a promising way to accelerate deep learning. Resistive\nprocessing unit (RPU) has been proposed to enable the vector-vector outer\nproduct in a crossbar array using a stochastic train of identical pulses to\nenable one-shot weight update, promising intense speed-up in matrix\nmultiplication operations, which form the bulk of training neural networks.\nHowever, the performance of the system suffers if the device does not satisfy\nthe condition of linear conductance change over around 1,000 conductance\nlevels. This is a challenge for nanoscale memories. Recently, Charge Trap Flash\n(CTF) memory was shown to have a large number of levels before saturation, but\nvariable non-linearity. In this paper, we explore the trade-off between the\nrange of conductance change and linearity. We show, through simulations, that\nat an optimum choice of the range, our system performs nearly as well as the\nmodels trained using exact floating point operations, with less than 1%\nreduction in the performance. Our system reaches an accuracy of 97.9% on MNIST\ndataset, 89.1% and 70.5% accuracy on CIFAR-10 and CIFAR-100 datasets (using\npre-extracted features). We also show its use in reinforcement learning, where\nit is used for value function approximation in Q-Learning, and learns to\ncomplete an episode the mountain car control problem in around 146 steps.\nBenchmarked to state-of-the-art, the CTF based RPU shows best in class\nperformance to enable software equivalent performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:45:58 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Bhatt", "Varun", ""], ["Shrivastava", "Shalini", ""], ["Chavan", "Tanmay", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2004.11137", "submitter": "Jeff Hajewski", "authors": "Jeff Hajewski and Suely Oliveira and David E. Stewart and Laura Weiler", "title": "gBeam-ACO: a greedy and faster variant of Beam-ACO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beam-ACO, a modification of the traditional Ant Colony Optimization (ACO)\nalgorithms that incorporates a modified beam search, is one of the most\neffective ACO algorithms for solving the Traveling Salesman Problem (TSP).\nAlthough adding beam search to the ACO heuristic search process is effective,\nit also increases the amount of work (in terms of partial paths) done by the\nalgorithm at each step. In this work, we introduce a greedy variant of Beam-ACO\nthat uses a greedy path selection heuristic. The exploitation of the greedy\npath selection is offset by the exploration required in maintaining the beam of\npaths. This approach has the added benefit of avoiding costly calls to a random\nnumber generator and reduces the algorithms internal state, making it simpler\nto parallelize. Our experiments demonstrate that not only is our greedy\nBeam-ACO (gBeam-ACO) faster than traditional Beam-ACO, in some cases by an\norder of magnitude, but it does not sacrifice quality of the found solution,\nespecially on large TSP instances. We also found that our greedy algorithm,\nwhich we refer to as gBeam-ACO, was less dependent on hyperparameter settings.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:35:27 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Hajewski", "Jeff", ""], ["Oliveira", "Suely", ""], ["Stewart", "David E.", ""], ["Weiler", "Laura", ""]]}, {"id": "2004.11184", "submitter": "Aaron Tuor", "authors": "Jan Drgona, Aaron Tuor, Draguna Vrabie", "title": "Learning Stable Adaptive Explicit Differentiable Predictive Control for\n  Unknown Linear Systems", "comments": "11 pages. Code for reproducing our experiments is available at:\n  https://github.com/pnnl/deps_arXiv20204", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present differentiable predictive control (DPC), a method for learning\nconstrained adaptive neural control policies and dynamical models of unknown\nlinear systems. DPC presents an approximate data-driven solution approach to\nthe explicit Model Predictive Control (MPC) problem as a scalable alternative\nto computationally expensive multiparametric programming solvers. DPC is\nformulated as a constrained deep learning problem whose architecture is\ninspired by the structure of classical MPC. The optimization of the neural\ncontrol policy is based on automatic differentiation of the MPC-inspired loss\nfunction through a differentiable closed-loop system model. This novel solution\napproach can optimize adaptive neural control policies for time-varying\nreferences while obeying state and input constraints without the prior need of\nan MPC controller. We show that DPC can learn to stabilize constrained neural\ncontrol policies for systems with unstable dynamics. Moreover, we provide\nsufficient conditions for asymptotic stability of generic closed-loop system\ndynamics with neural feedback policies. In simulation case studies, we assess\nthe performance of the proposed DPC method in terms of reference tracking,\nrobustness, and computational and memory footprints compared against classical\nmodel-based and data-driven control approaches. We demonstrate that DPC scales\nlinearly with problem size, compared to exponential scalability of classical\nexplicit MPC based on multiparametric programming.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:24:44 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 00:38:27 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 22:14:24 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2020 12:37:15 GMT"}, {"version": "v5", "created": "Fri, 23 Jul 2021 16:48:34 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Drgona", "Jan", ""], ["Tuor", "Aaron", ""], ["Vrabie", "Draguna", ""]]}, {"id": "2004.11204", "submitter": "Keshab Parhi", "authors": "Lulu Ge and Keshab K. Parhi", "title": "Classification using Hyperdimensional Computing: A Review", "comments": "IEEE Circuits and Systems Magazine (2020)", "journal-ref": "IEEE Circuits and Systems Magazine, 20(2), pp. 30-47, June 2020", "doi": "10.1109/MCAS.2020.2988388", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperdimensional (HD) computing is built upon its unique data type referred\nto as hypervectors. The dimension of these hypervectors is typically in the\nrange of tens of thousands. Proposed to solve cognitive tasks, HD computing\naims at calculating similarity among its data. Data transformation is realized\nby three operations, including addition, multiplication and permutation. Its\nultra-wide data representation introduces redundancy against noise. Since\ninformation is evenly distributed over every bit of the hypervectors, HD\ncomputing is inherently robust. Additionally, due to the nature of those three\noperations, HD computing leads to fast learning ability, high energy efficiency\nand acceptable accuracy in learning and classification tasks. This paper\nintroduces the background of HD computing, and reviews the data representation,\ndata transformation, and similarity measurement. The orthogonality in high\ndimensions presents opportunities for flexible computing. To balance the\ntradeoff between accuracy and efficiency, strategies include but are not\nlimited to encoding, retraining, binarization and hardware acceleration.\nEvaluations indicate that HD computing shows great potential in addressing\nproblems using data in the form of letters, signals and images. HD computing\nespecially shows significant promise to replace machine learning algorithms as\na light-weight classifier in the field of internet of things (IoTs).\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 23:51:44 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ge", "Lulu", ""], ["Parhi", "Keshab K.", ""]]}, {"id": "2004.11234", "submitter": "Juan-Pablo Ortega", "authors": "Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega", "title": "Memory and forecasting capacities of nonlinear recurrent networks", "comments": "27 pages, 1 figure. To appear in Physica D", "journal-ref": null, "doi": "10.1016/j.physd.2020.132721", "report-no": null, "categories": "math.OC cs.LG cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of memory capacity, originally introduced for echo state and\nlinear networks with independent inputs, is generalized to nonlinear recurrent\nnetworks with stationary but dependent inputs. The presence of dependence in\nthe inputs makes natural the introduction of the network forecasting capacity,\nthat measures the possibility of forecasting time series values using network\nstates. Generic bounds for memory and forecasting capacities are formulated in\nterms of the number of neurons of the nonlinear recurrent network and the\nautocovariance function or the spectral density of the input. These bounds\ngeneralize well-known estimates in the literature to a dependent inputs setup.\nFinally, for the particular case of linear recurrent networks with independent\ninputs it is proved that the memory capacity is given by the rank of the\nassociated controllability matrix, a fact that has been for a long time assumed\nto be true without proof by the community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 10:53:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Gonon", "Lukas", ""], ["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "2004.11300", "submitter": "Luca Mariot", "authors": "Domagoj Jakobovic, Luca Manzoni, Luca Mariot, Stjepan Picek, Mauro\n  Castelli", "title": "CoInGP: Convolutional Inpainting with Genetic Programming", "comments": "21 pages, 8 figures, updated pre-print accepted at GECCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of Genetic Programming (GP) as a convolutional\npredictor for missing pixels in images. The training phase is performed by\nsweeping a sliding window over an image, where the pixels on the border\nrepresent the inputs of a GP tree. The output of the tree is taken as the\npredicted value for the central pixel. We consider two topologies for the\nsliding window, namely the Moore and the Von Neumann neighborhood. The best GP\ntree scoring the lowest prediction error over the training set is then used to\npredict the pixels in the test set. We experimentally assess our approach\nthrough two experiments. In the first one, we train a GP tree over a subset of\n1000 complete images from the MNIST dataset. The results show that GP can learn\nthe distribution of the pixels with respect to a simple baseline predictor,\nwith no significant differences observed between the two neighborhoods. In the\nsecond experiment, we train a GP convolutional predictor on two degraded\nimages, removing around 20% of their pixels. In this case, we observe that the\nMoore neighborhood works better, although the Von Neumann neighborhood allows\nfor a larger training set.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:31:58 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 10:23:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jakobovic", "Domagoj", ""], ["Manzoni", "Luca", ""], ["Mariot", "Luca", ""], ["Picek", "Stjepan", ""], ["Castelli", "Mauro", ""]]}, {"id": "2004.11331", "submitter": "Luca Mariot", "authors": "Luca Manzoni, Luca Mariot, Eva Tuba", "title": "Tip the Balance: Improving Exploration of Balanced Crossover Operators\n  by Adaptive Bias", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of balanced crossover operators in Genetic Algorithms (GA) ensures\nthat the binary strings generated as offsprings have the same Hamming weight of\nthe parents, a constraint which is sought in certain discrete optimization\nproblems. Although this method reduces the size of the search space, the\nresulting fitness landscape often becomes more difficult for the GA to explore\nand to discover optimal solutions. This issue has been studied in this paper by\napplying an adaptive bias strategy to a counter-based crossover operator that\nintroduces unbalancedness in the offspring with a certain probability, which is\ndecreased throughout the evolutionary process. Experiments show that improving\nthe exploration of the search space with this adaptive bias strategy is\nbeneficial for the GA performances in terms of the number of optimal solutions\nfound for the balanced nonlinear Boolean functions problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:26:43 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Manzoni", "Luca", ""], ["Mariot", "Luca", ""], ["Tuba", "Eva", ""]]}, {"id": "2004.11514", "submitter": "Brian Hutchinson", "authors": "Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda\n  Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor", "title": "Systematic Evaluation of Backdoor Data Poisoning Attacks on Image\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor data poisoning attacks have recently been demonstrated in computer\nvision research as a potential safety risk for machine learning (ML) systems.\nTraditional data poisoning attacks manipulate training data to induce\nunreliability of an ML model, whereas backdoor data poisoning attacks maintain\nsystem performance unless the ML model is presented with an input containing an\nembedded \"trigger\" that provides a predetermined response advantageous to the\nadversary. Our work builds upon prior backdoor data-poisoning research for ML\nimage classifiers and systematically assesses different experimental conditions\nincluding types of trigger patterns, persistence of trigger patterns during\nretraining, poisoning strategies, architectures (ResNet-50, NasNet,\nNasNet-Mobile), datasets (Flowers, CIFAR-10), and potential defensive\nregularization techniques (Contrastive Loss, Logit Squeezing, Manifold Mixup,\nSoft-Nearest-Neighbors Loss). Experiments yield four key findings. First, the\nsuccess rate of backdoor poisoning attacks varies widely, depending on several\nfactors, including model architecture, trigger pattern and regularization\ntechnique. Second, we find that poisoned models are hard to detect through\nperformance inspection alone. Third, regularization typically reduces backdoor\nsuccess rate, although it can have no effect or even slightly increase it,\ndepending on the form of regularization. Finally, backdoors inserted through\ndata poisoning can be rendered ineffective after just a few epochs of\nadditional training on a small set of clean data without affecting the model's\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 02:58:22 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Truong", "Loc", ""], ["Jones", "Chace", ""], ["Hutchinson", "Brian", ""], ["August", "Andrew", ""], ["Praggastis", "Brenda", ""], ["Jasper", "Robert", ""], ["Nichols", "Nicole", ""], ["Tuor", "Aaron", ""]]}, {"id": "2004.11545", "submitter": "Seyed Iman Mirzadeh", "authors": "Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Hassan Ghasemzadeh", "title": "Dropout as an Implicit Gating Mechanism For Continual Learning", "comments": "CVPR 2020 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural networks have demonstrated an outstanding ability to\nachieve complex learning tasks across various domains. However, they suffer\nfrom the \"catastrophic forgetting\" problem when they face a sequence of\nlearning tasks, where they forget the old ones as they learn new tasks. This\nproblem is also highly related to the \"stability-plasticity dilemma\". The more\nplastic the network, the easier it can learn new tasks, but the faster it also\nforgets previous ones. Conversely, a stable network cannot learn new tasks as\nfast as a very plastic network. However, it is more reliable to preserve the\nknowledge it has learned from the previous tasks. Several solutions have been\nproposed to overcome the forgetting problem by making the neural network\nparameters more stable, and some of them have mentioned the significance of\ndropout in continual learning. However, their relationship has not been\nsufficiently studied yet. In this paper, we investigate this relationship and\nshow that a stable network with dropout learns a gating mechanism such that for\ndifferent tasks, different paths of the network are active. Our experiments\nshow that the stability achieved by this implicit gating plays a very critical\nrole in leading to performance comparable to or better than other involved\ncontinual learning algorithms to overcome catastrophic forgetting.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 06:04:15 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mirzadeh", "Seyed-Iman", ""], ["Farajtabar", "Mehrdad", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2004.11685", "submitter": "Laurent Meunier", "authors": "Laurent Meunier, Yann Chevaleyre, Jeremy Rapin, Cl\\'ement W. Royer,\n  Olivier Teytaud", "title": "On averaging the best samples in evolutionary computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing the right selection rate is a long standing issue in evolutionary\ncomputation. In the continuous unconstrained case, we prove mathematically that\na single parent $\\mu=1$ leads to a sub-optimal simple regret in the case of the\nsphere function. We provide a theoretically-based selection rate $\\mu/\\lambda$\nthat leads to better progress rates. With our choice of selection rate, we get\na provable regret of order $O(\\lambda^{-1})$ which has to be compared with\n$O(\\lambda^{-2/d})$ in the case where $\\mu=1$. We complete our study with\nexperiments to confirm our theoretical claims.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:25:39 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 13:22:44 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 18:32:33 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Meunier", "Laurent", ""], ["Chevaleyre", "Yann", ""], ["Rapin", "Jeremy", ""], ["Royer", "Cl\u00e9ment W.", ""], ["Teytaud", "Olivier", ""]]}, {"id": "2004.11687", "submitter": "Laurent Meunier", "authors": "Laurent Meunier, Carola Doerr, Jeremy Rapin, Olivier Teytaud", "title": "Variance Reduction for Better Sampling in Continuous Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of experiments, random search, initialization of population-based\nmethods, or sampling inside an epoch of an evolutionary algorithm use a sample\ndrawn according to some probability distribution for approximating the location\nof an optimum. Recent papers have shown that the optimal search distribution,\nused for the sampling, might be more peaked around the center of the\ndistribution than the prior distribution modelling our uncertainty about the\nlocation of the optimum. We confirm this statement, provide explicit values for\nthis reshaping of the search distribution depending on the population size\n$\\lambda$ and the dimension $d$, and validate our results experimentally.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:25:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Meunier", "Laurent", ""], ["Doerr", "Carola", ""], ["Rapin", "Jeremy", ""], ["Teytaud", "Olivier", ""]]}, {"id": "2004.11898", "submitter": "Nathaniel Bastian PhD", "authors": "Elie Alhajjar and Paul Maxwell and Nathaniel D. Bastian", "title": "Adversarial Machine Learning in Network Intrusion Detection Systems", "comments": "25 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are inputs to a machine learning system intentionally\ncrafted by an attacker to fool the model into producing an incorrect output.\nThese examples have achieved a great deal of success in several domains such as\nimage recognition, speech recognition and spam detection. In this paper, we\nstudy the nature of the adversarial problem in Network Intrusion Detection\nSystems (NIDS). We focus on the attack perspective, which includes techniques\nto generate adversarial examples capable of evading a variety of machine\nlearning models. More specifically, we explore the use of evolutionary\ncomputation (particle swarm optimization and genetic algorithm) and deep\nlearning (generative adversarial networks) as tools for adversarial example\ngeneration. To assess the performance of these algorithms in evading a NIDS, we\napply them to two publicly available data sets, namely the NSL-KDD and\nUNSW-NB15, and we contrast them to a baseline perturbation method: Monte Carlo\nsimulation. The results show that our adversarial example generation techniques\ncause high misclassification rates in eleven different machine learning models,\nalong with a voting classifier. Our work highlights the vulnerability of\nmachine learning based NIDS in the face of adversarial perturbation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 19:47:43 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Alhajjar", "Elie", ""], ["Maxwell", "Paul", ""], ["Bastian", "Nathaniel D.", ""]]}, {"id": "2004.11946", "submitter": "Fei Sun", "authors": "Fei Sun, Minghai Qin, Tianyun Zhang, Liu Liu, Yen-Kuang Chen, Yuan Xie", "title": "Computation on Sparse Neural Networks: an Inspiration for Future\n  Hardware", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models are widely used in solving many challenging problems,\nsuch as computer vision, personalized recommendation, and natural language\nprocessing. Those models are very computationally intensive and reach the\nhardware limit of the existing server and IoT devices. Thus, finding better\nmodel architectures with much less amount of computation while maximally\npreserving the accuracy is a popular research topic. Among various mechanisms\nthat aim to reduce the computation complexity, identifying the zero values in\nthe model weights and in the activations to avoid computing them is a promising\ndirection.\n  In this paper, we summarize the current status of the research on the\ncomputation of sparse neural networks, from the perspective of the sparse\nalgorithms, the software frameworks, and the hardware accelerations. We observe\nthat the search for the sparse structure can be a general methodology for\nhigh-quality model explorations, in addition to a strategy for high-efficiency\nmodel execution. We discuss the model accuracy influenced by the number of\nweight parameters and the structure of the model. The corresponding models are\ncalled to be located in the weight dominated and structure dominated regions,\nrespectively. We show that for practically complicated problems, it is more\nbeneficial to search large and sparse models in the weight dominated region. In\norder to achieve the goal, new approaches are required to search for proper\nsparse structures, and new sparse training hardware needs to be developed to\nfacilitate fast iterations of sparse models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:13:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sun", "Fei", ""], ["Qin", "Minghai", ""], ["Zhang", "Tianyun", ""], ["Liu", "Liu", ""], ["Chen", "Yen-Kuang", ""], ["Xie", "Yuan", ""]]}, {"id": "2004.11947", "submitter": "Jiri Kubalik", "authors": "J. Kubal\\'ik, E. Derner, R. Babu\\v{s}ka", "title": "Symbolic Regression Driven by Training Data and Prior Knowledge", "comments": "9 pages, 4 figures, 3 tables", "journal-ref": null, "doi": "10.1145/3377930.3390152", "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In symbolic regression, the search for analytic models is typically driven\npurely by the prediction error observed on the training data samples. However,\nwhen the data samples do not sufficiently cover the input space, the prediction\nerror does not provide sufficient guidance toward desired models. Standard\nsymbolic regression techniques then yield models that are partially incorrect,\nfor instance, in terms of their steady-state characteristics or local behavior.\nIf these properties were considered already during the search process, more\naccurate and relevant models could be produced. We propose a multi-objective\nsymbolic regression approach that is driven by both the training data and the\nprior knowledge of the properties the desired model should manifest. The\nproperties given in the form of formal constraints are internally represented\nby a set of discrete data samples on which candidate models are exactly\nchecked. The proposed approach was experimentally evaluated on three test\nproblems with results clearly demonstrating its capability to evolve realistic\nmodels that fit the training data well while complying with the prior knowledge\nof the desired model characteristics at the same time. It outperforms standard\nsymbolic regression by several orders of magnitude in terms of the mean squared\ndeviation from a reference model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:15:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kubal\u00edk", "J.", ""], ["Derner", "E.", ""], ["Babu\u0161ka", "R.", ""]]}, {"id": "2004.12045", "submitter": "Markus Wagner", "authors": "Ragav Sachdeva, Frank Neumann, Markus Wagner", "title": "The Dynamic Travelling Thief Problem: Benchmarks and Performance of\n  Evolutionary Algorithms", "comments": "Accepted for publication and presentation at ICONIP 2020,\n  https://iconip2020.apnns.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world optimisation problems involve dynamic and stochastic\ncomponents. While problems with multiple interacting components are omnipresent\nin inherently dynamic domains like supply-chain optimisation and logistics,\nmost research on dynamic problems focuses on single-component problems. With\nthis article, we define a number of scenarios based on the Travelling Thief\nProblem to enable research on the effect of dynamic changes to sub-components.\nOur investigations of 72 scenarios and seven algorithms show that -- depending\non the instance, the magnitude of the change, and the algorithms in the\nportfolio -- it is preferable to either restart the optimisation from scratch\nor to continue with the previously valid solutions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 02:54:17 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 10:04:36 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 01:10:59 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Sachdeva", "Ragav", ""], ["Neumann", "Frank", ""], ["Wagner", "Markus", ""]]}, {"id": "2004.12117", "submitter": "Reza Refaei", "authors": "Reza Refaei Afshar and Yingqian Zhang and Murat Firat and Uzay Kaymak", "title": "A State Aggregation Approach for Solving Knapsack Problem with Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Deep Reinforcement Learning (DRL) approach for solving\nknapsack problem. The proposed method consists of a state aggregation step\nbased on tabular reinforcement learning to extract features and construct\nstates. The state aggregation policy is applied to each problem instance of the\nknapsack problem, which is used with Advantage Actor Critic (A2C) algorithm to\ntrain a policy through which the items are sequentially selected at each time\nstep. The method is a constructive solution approach and the process of\nselecting items is repeated until the final solution is obtained. The\nexperiments show that our approach provides close to optimal solutions for all\ntested instances, outperforms the greedy algorithm, and is able to handle\nlarger instances and more flexible than an existing DRL approach. In addition,\nthe results demonstrate that the proposed model with the state aggregation\nstrategy not only gives better solutions but also learns in less timesteps,\nthan the one without state aggregation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 11:52:24 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Afshar", "Reza Refaei", ""], ["Zhang", "Yingqian", ""], ["Firat", "Murat", ""], ["Kaymak", "Uzay", ""]]}, {"id": "2004.12188", "submitter": "Amiram Moshaiov", "authors": "Amiram Moshaiov and Michael Zadok", "title": "On the Generalization Capability of Evolved Counter-propagation\n  Neuro-controllers for Robot Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolving Counter-Propagation Neuro-Controllers (CPNCs), rather than the\ntraditional Feed-Forward Neuro-Controllers (FFNCs), has recently been suggested\nand tested using simulated robot navigation. It has been demon-strated that\nboth convergence rate and final performance obtained by evolving CPNCs are\nsuperior to those obtained by evolving FFNCs. In this paper the maze\ngeneralization features of both types of evolved navigation controllers are\nexamined. For this purpose the controllers are tested in an environment that\ndrastically differs from the one used for their training. Moreover, a\ncomparison is carried out of results obtained by single-objective and\nmulti-objective evolution approaches. Using a simulated case-study, the maze\ngeneralization capability of the evolved CPNCs is highlighted in both the\nsingle and multi-objective cases. In contrast, the evolved FFNCs are found to\nlack such capabilities in both approaches.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:55:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Moshaiov", "Amiram", ""], ["Zadok", "Michael", ""]]}, {"id": "2004.12304", "submitter": "Weijie Zheng", "authors": "Weijie Zheng, Huanhuan Chen, Xin Yao", "title": "Analysis of Evolutionary Algorithms on Fitness Function with\n  Time-linkage Property", "comments": "Accepted for publication in the IEEE Transactions on Evolutionary\n  Computation", "journal-ref": null, "doi": "10.1109/TEVC.2021.3061442", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications, many optimization problems have the time-linkage\nproperty, that is, the objective function value relies on the current solution\nas well as the historical solutions. Although the rigorous theoretical analysis\non evolutionary algorithms has rapidly developed in recent two decades, it\nremains an open problem to theoretically understand the behaviors of\nevolutionary algorithms on time-linkage problems. This paper takes the first\nstep to rigorously analyze evolutionary algorithms for time-linkage functions.\nBased on the basic OneMax function, we propose a time-linkage function where\nthe first bit value of the last time step is integrated but has a different\npreference from the current first bit. We prove that with probability $1-o(1)$,\nrandomized local search and $(1+1)$ EA cannot find the optimum, and with\nprobability $1-o(1)$, $(\\mu+1)$ EA is able to reach the optimum.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:56:40 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:21:11 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 10:58:45 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 01:13:45 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zheng", "Weijie", ""], ["Chen", "Huanhuan", ""], ["Yao", "Xin", ""]]}, {"id": "2004.12337", "submitter": "Jaros{\\l}aw Miszczak", "authors": "Mateusz \\.Zarski, Bartosz W\\'ojcik, Jaros{\\l}aw Adam Miszczak", "title": "KrakN: Transfer Learning framework for thin crack detection in\n  infrastructure maintenance", "comments": "23 pages, 15 figures and flowcharts, software available at\n  https://github.com/MatZar01/KrakN, https://doi.org/10.5281/zenodo.3764697,\n  and https://doi.org/10.5281/zenodo.3755452, dataset available from\n  https://doi.org/10.5281/zenodo.3759845", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monitoring the technical condition of infrastructure is a crucial element to\nits maintenance. Currently applied methods are outdated, labour-intensive and\ninaccurate. At the same time, the latest methods using Artificial Intelligence\ntechniques are severely limited in their application due to two main factors --\nlabour-intensive gathering of new datasets and high demand for computing power.\nWe propose to utilize custom made framework -- KrakN, to overcome these\nlimiting factors. It enables the development of unique infrastructure defects\ndetectors on digital images, achieving the accuracy of above 90%. The framework\nsupports semi-automatic creation of new datasets and has modest computing power\nrequirements. It is implemented in the form of a ready-to-use software package\nopenly distributed to the public. Thus, it can be used to immediately implement\nthe methods proposed in this paper in the process of infrastructure management\nby government units, regardless of their financial capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:57:36 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 17:15:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["\u017barski", "Mateusz", ""], ["W\u00f3jcik", "Bartosz", ""], ["Miszczak", "Jaros\u0142aw Adam", ""]]}, {"id": "2004.12357", "submitter": "Hui Wang", "authors": "Hui Wang, Mike Preuss, Aske Plaat", "title": "Warm-Start AlphaZero Self-Play Search Enhancements", "comments": null, "journal-ref": "PPSN 2020", "doi": "10.1007/978-3-030-58115-2_37", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, AlphaZero has achieved landmark results in deep reinforcement\nlearning, by providing a single self-play architecture that learned three\ndifferent games at super human level. AlphaZero is a large and complicated\nsystem with many parameters, and success requires much compute power and\nfine-tuning. Reproducing results in other games is a challenge, and many\nresearchers are looking for ways to improve results while reducing\ncomputational demands. AlphaZero's design is purely based on self-play and\nmakes no use of labeled expert data ordomain specific enhancements; it is\ndesigned to learn from scratch. We propose a novel approach to deal with this\ncold-start problem by employing simple search enhancements at the beginning\nphase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE)\nand dynamically weighted combinations of these with the neural network, and\nRolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that\nmost of these enhancements improve the performance of their baseline player in\nthree different (small) board games, with especially RAVE based variants\nplaying strongly.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 11:48:53 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wang", "Hui", ""], ["Preuss", "Mike", ""], ["Plaat", "Aske", ""]]}, {"id": "2004.12538", "submitter": "Sohee Cho", "authors": "Sohee Cho, Ginkyeng Lee, Wonjoon Chang and Jaesik Choi", "title": "Interpretation of Deep Temporal Representations by Selective\n  Visualization of Internally Activated Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks demonstrate competitive performances in\nclassification and regression tasks for many temporal or sequential data.\nHowever, it is still hard to understand the classification mechanisms of\ntemporal deep neural networks. In this paper, we propose two new frameworks to\nvisualize temporal representations learned from deep neural networks. Given\ninput data and output, our algorithm interprets the decision of temporal neural\nnetwork by extracting highly activated periods and visualizes a sub-sequence of\ninput data which contributes to activate the units. Furthermore, we\ncharacterize such sub-sequences with clustering and calculate the uncertainty\nof the suggested type and actual data. We also suggest Layer-wise Relevance\nfrom the output of a unit, not from the final output, with backward Monte-Carlo\ndropout to show the relevance scores of each input point to activate units with\nproviding a visual representation of the uncertainty about this impact.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:45:55 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 05:08:29 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Cho", "Sohee", ""], ["Lee", "Ginkyeng", ""], ["Chang", "Wonjoon", ""], ["Choi", "Jaesik", ""]]}, {"id": "2004.12574", "submitter": "Vahid Roostapour", "authors": "Vahid Roostapour, Aneta Neumann, Frank Neumann", "title": "Evolutionary Multi-Objective Optimization for the Dynamic Knapsack\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms are bio-inspired algorithms that can easily adapt to\nchanging environments. In this paper, we study single- and multi-objective\nbaseline evolutionary algorithms for the classical knapsack problem where the\ncapacity of the knapsack varies over time. We establish different benchmark\nscenarios where the capacity changes every $\\tau$ iterations according to a\nuniform or normal distribution. Our experimental investigations analyze the\nbehavior of our algorithms in terms of the magnitude of changes determined by\nparameters of the chosen distribution, the frequency determined by $\\tau$, and\nthe class of knapsack instance under consideration. Our results show that the\nmulti-objective approaches using a population that caters for dynamic changes\nhave a clear advantage in many benchmarks scenarios when the frequency of\nchanges is not too high. Furthermore, we demonstrate that the distribution\nhandling techniques in advance algorithms such as NSGA-II and SPEA2 do not\nnecessarily result in better performance and even prevent these algorithms from\nfinding good quality solutions in comparison with simple multi-objective\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 03:50:24 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Roostapour", "Vahid", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""]]}, {"id": "2004.12691", "submitter": "Edward Frady", "authors": "E. Paxon Frady, Garrick Orchard, David Florey, Nabil Imam, Ruokun Liu,\n  Joyesh Mishra, Jonathan Tse, Andreas Wild, Friedrich T. Sommer, Mike Davies", "title": "Neuromorphic Nearest-Neighbor Search Using Intel's Pohoiki Springs", "comments": "9 pages, 8 figures, 3 tables, submission to NICE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing applies insights from neuroscience to uncover\ninnovations in computing technology. In the brain, billions of interconnected\nneurons perform rapid computations at extremely low energy levels by leveraging\nproperties that are foreign to conventional computing systems, such as temporal\nspiking codes and finely parallelized processing units integrating both memory\nand computation. Here, we showcase the Pohoiki Springs neuromorphic system, a\nmesh of 768 interconnected Loihi chips that collectively implement 100 million\nspiking neurons in silicon. We demonstrate a scalable approximate k-nearest\nneighbor (k-NN) algorithm for searching large databases that exploits\nneuromorphic principles. Compared to state-of-the-art conventional CPU-based\nimplementations, we achieve superior latency, index build time, and energy\nefficiency when evaluated on several standard datasets containing over 1\nmillion high-dimensional patterns. Further, the system supports adding new data\npoints to the indexed database online in O(1) time unlike all but brute force\nconventional k-NN implementations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:23:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Frady", "E. Paxon", ""], ["Orchard", "Garrick", ""], ["Florey", "David", ""], ["Imam", "Nabil", ""], ["Liu", "Ruokun", ""], ["Mishra", "Joyesh", ""], ["Tse", "Jonathan", ""], ["Wild", "Andreas", ""], ["Sommer", "Friedrich T.", ""], ["Davies", "Mike", ""]]}, {"id": "2004.12750", "submitter": "Mohamed El Yafrani", "authors": "Mohamed El Yafrani, Marcella Scoczynski Ribeiro Martins, Inkyung Sung,\n  Markus Wagner, Carola Doerr, and Peter Nielsen", "title": "MATE: A Model-based Algorithm Tuning Engine", "comments": "16 pages. Submitted to Evo* 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a Model-based Algorithm Turning Engine, namely\nMATE, where the parameters of an algorithm are represented as expressions of\nthe features of a target optimisation problem. In contrast to most static\n(feature-independent) algorithm tuning engines such as irace and SPOT, our\napproach aims to derive the best parameter configuration of a given algorithm\nfor a specific problem, exploiting the relationships between the algorithm\nparameters and the features of the problem. We formulate the problem of finding\nthe relationships between the parameters and the problem features as a symbolic\nregression problem and we use genetic programming to extract these expressions.\nFor the evaluation, we apply our approach to configuration of the (1+1) EA and\nRLS algorithms for the OneMax, LeadingOnes, BinValue and Jump optimisation\nproblems, where the theoretically optimal algorithm parameters to the problems\nare available as functions of the features of the problems. Our study shows\nthat the found relationships typically comply with known theoretical results,\nthus demonstrating a new opportunity to consider model-based parameter tuning\nas an effective alternative to the static algorithm tuning engines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:50:48 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 09:38:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Yafrani", "Mohamed El", ""], ["Martins", "Marcella Scoczynski Ribeiro", ""], ["Sung", "Inkyung", ""], ["Wagner", "Markus", ""], ["Doerr", "Carola", ""], ["Nielsen", "Peter", ""]]}, {"id": "2004.12762", "submitter": "Marcella Martins", "authors": "Marko Durasevic, Domagoj Jakobovic, Marcella Scoczynski Ribeiro\n  Martins, Stjepan Picek, and Markus Wagner", "title": "Fitness Landscape Analysis of Dimensionally-Aware Genetic Programming\n  Featuring Feynman Equations", "comments": "14 pages. Submitted to PPSN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic programming is an often-used technique for symbolic regression:\nfinding symbolic expressions that match data from an unknown function. To make\nthe symbolic regression more efficient, one can also use dimensionally-aware\ngenetic programming that constrains the physical units of the equation.\nNevertheless, there is no formal analysis of how much dimensionality awareness\nhelps in the regression process. In this paper, we conduct a fitness landscape\nanalysis of dimensionallyaware genetic programming search spaces on a subset of\nequations from Richard Feynmans well-known lectures. We define an\ninitialisation procedure and an accompanying set of neighbourhood operators for\nconducting the local search within the physical unit constraints. Our\nexperiments show that the added information about the variable dimensionality\ncan efficiently guide the search algorithm. Still, further analysis of the\ndifferences between the dimensionally-aware and standard genetic programming\nlandscapes is needed to help in the design of efficient evolutionary operators\nto be used in a dimensionally-aware regression.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:05:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Durasevic", "Marko", ""], ["Jakobovic", "Domagoj", ""], ["Martins", "Marcella Scoczynski Ribeiro", ""], ["Picek", "Stjepan", ""], ["Wagner", "Markus", ""]]}, {"id": "2004.12794", "submitter": "Mehdi Neshat", "authors": "Mehdi Neshat, Meysam Majidi Nezhad, Ehsan Abbasnejad, Daniele Groppi,\n  Azim Heydari, Lina Bertling Tjernberg, Davide Astiaso Garcia, Bradley\n  Alexander and Markus Wagner", "title": "Hybrid Neuro-Evolutionary Method for Predicting Wind Turbine Power\n  Output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable wind turbine power prediction is imperative to the planning,\nscheduling and control of wind energy farms for stable power production. In\nrecent years Machine Learning (ML) methods have been successfully applied in a\nwide range of domains, including renewable energy. However, due to the\nchallenging nature of power prediction in wind farms, current models are far\nshort of the accuracy required by industry. In this paper, we deploy a\ncomposite ML approach--namely a hybrid neuro-evolutionary algorithm--for\naccurate forecasting of the power output in wind-turbine farms. We use\nhistorical data in the supervisory control and data acquisition (SCADA) systems\nas input to estimate the power output from an onshore wind farm in Sweden. At\nthe beginning stage, the k-means clustering method and an Autoencoder are\nemployed, respectively, to detect and filter noise in the SCADA measurements.\nNext, with the prior knowledge that the underlying wind patterns are highly\nnon-linear and diverse, we combine a self-adaptive differential evolution\n(SaDE) algorithm as a hyper-parameter optimizer, and a recurrent neural network\n(RNN) called Long Short-term memory (LSTM) to model the power curve of a wind\nturbine in a farm. Two short time forecasting horizons, including ten-minutes\nahead and one-hour ahead, are considered in our experiments. We show that our\napproach outperforms its counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 04:22:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Neshat", "Mehdi", ""], ["Nezhad", "Meysam Majidi", ""], ["Abbasnejad", "Ehsan", ""], ["Groppi", "Daniele", ""], ["Heydari", "Azim", ""], ["Tjernberg", "Lina Bertling", ""], ["Garcia", "Davide Astiaso", ""], ["Alexander", "Bradley", ""], ["Wagner", "Markus", ""]]}, {"id": "2004.12814", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Michele Scarpiniti, Enzo Baccarelli, Aurelio Uncini", "title": "Why should we add early exits to neural networks?", "comments": "Published in Cognitive Computation", "journal-ref": "Cognitive Computation, 2020", "doi": "10.1007/s12559-020-09734-4", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are generally designed as a stack of differentiable\nlayers, in which a prediction is obtained only after running the full stack.\nRecently, some contributions have proposed techniques to endow the networks\nwith early exits, allowing to obtain predictions at intermediate points of the\nstack. These multi-output networks have a number of advantages, including: (i)\nsignificant reductions of the inference time, (ii) reduced tendency to\noverfitting and vanishing gradients, and (iii) capability of being distributed\nover multi-tier computation platforms. In addition, they connect to the wider\nthemes of biological plausibility and layered cognitive reasoning. In this\npaper, we provide a comprehensive introduction to this family of neural\nnetworks, by describing in a unified fashion the way these architectures can be\ndesigned, trained, and actually deployed in time-constrained scenarios. We also\ndescribe in-depth their application scenarios in 5G and Fog computing\nenvironments, as long as some of the open research questions connected to them.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:53:16 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 07:42:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Scardapane", "Simone", ""], ["Scarpiniti", "Michele", ""], ["Baccarelli", "Enzo", ""], ["Uncini", "Aurelio", ""]]}, {"id": "2004.12846", "submitter": "Eseoghene Ben-Iwhiwhu", "authors": "Eseoghene Ben-Iwhiwhu, Pawel Ladosz, Jeffery Dick, Wen-Hua Chen,\n  Praveen Pilly, Andrea Soltoggio", "title": "Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems", "comments": "9 pages. Accepted as a full paper in the Genetic and Evolutionary\n  Computation Conference (GECCO 2020)", "journal-ref": null, "doi": "10.1145/3377930.3390214", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid online adaptation to changing tasks is an important problem in machine\nlearning and, recently, a focus of meta-reinforcement learning. However,\nreinforcement learning (RL) algorithms struggle in POMDP environments because\nthe state of the system, essential in a RL framework, is not always visible.\nAdditionally, hand-designed meta-RL architectures may not include suitable\ncomputational structures for specific learning problems. The evolution of\nonline learning mechanisms, on the contrary, has the ability to incorporate\nlearning strategies into an agent that can (i) evolve memory when required and\n(ii) optimize adaptation speed to specific online learning problems. In this\npaper, we exploit the highly adaptive nature of neuromodulated neural networks\nto evolve a controller that uses the latent space of an autoencoder in a POMDP.\nThe analysis of the evolved networks reveals the ability of the proposed\nalgorithm to acquire inborn knowledge in a variety of aspects such as the\ndetection of cues that reveal implicit rewards, and the ability to evolve\nlocation neurons that help with navigation. The integration of inborn knowledge\nand online plasticity enabled fast adaptation and better performance in\ncomparison to some non-evolutionary meta-reinforcement learning algorithms. The\nalgorithm proved also to succeed in the 3D gaming environment Malmo Minecraft.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:55:08 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:04:56 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ben-Iwhiwhu", "Eseoghene", ""], ["Ladosz", "Pawel", ""], ["Dick", "Jeffery", ""], ["Chen", "Wen-Hua", ""], ["Pilly", "Praveen", ""], ["Soltoggio", "Andrea", ""]]}, {"id": "2004.12886", "submitter": "Miad Zandavi Dr", "authors": "Seid Miad Zandavi, Vera Chung, Ali Anaissi", "title": "Control Design of Autonomous Drone Using Deep Learning Based Image\n  Understanding Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new framework to use images as the inputs for the\ncontroller to have autonomous flight, considering the noisy indoor environment\nand uncertainties. A new Proportional-Integral-Derivative-Accelerated (PIDA)\ncontrol with a derivative filter is proposed to improves drone/quadcopter\nflight stability within a noisy environment and enables autonomous flight using\nobject and depth detection techniques. The mathematical model is derived from\nan accurate model with a high level of fidelity by addressing the problems of\nnon-linearity, uncertainties, and coupling. The proposed PIDA controller is\ntuned by Stochastic Dual Simplex Algorithm (SDSA) to support autonomous flight.\nThe simulation results show that adapting the deep learning-based image\nunderstanding techniques (RetinaNet ant colony detection and PSMNet) to the\nproposed controller can enable the generation and tracking of the desired point\nin the presence of environmental disturbances.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:50:04 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 00:52:47 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 01:23:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zandavi", "Seid Miad", ""], ["Chung", "Vera", ""], ["Anaissi", "Ali", ""]]}, {"id": "2004.13121", "submitter": "Amir Javadpour", "authors": "Amir Javadpour, Samira Rezaei, Kuan-Ching Li and Guojun Wang", "title": "A Scalable Feature Selection and Opinion Miner Using Whale Optimization\n  Algorithm", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-15-4828-4_20", "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fast-growing volume of text documents and reviews in recent years,\ncurrent analyzing techniques are not competent enough to meet the users' needs.\nUsing feature selection techniques not only support to understand data better\nbut also lead to higher speed and also accuracy. In this article, the Whale\nOptimization algorithm is considered and applied to the search for the optimum\nsubset of features. As known, F-measure is a metric based on precision and\nrecall that is very popular in comparing classifiers. For the evaluation and\ncomparison of the experimental results, PART, random tree, random forest, and\nRBF network classification algorithms have been applied to the different number\nof features. Experimental results show that the random forest has the best\naccuracy on 500 features. Keywords: Feature selection, Whale Optimization\nalgorithm, Selecting optimal, Classification algorithm\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 01:08:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Javadpour", "Amir", ""], ["Rezaei", "Samira", ""], ["Li", "Kuan-Ching", ""], ["Wang", "Guojun", ""]]}, {"id": "2004.13181", "submitter": "Sheldon Tan", "authors": "Wentian Jin, Sheriff Sadiqbatcha, Jinwei Zhang, Sheldon X.-D. Tan", "title": "EM-GAN: Fast Stress Analysis for Multi-Segment Interconnect Using\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fast transient hydrostatic stress analysis for\nelectromigration (EM) failure assessment for multi-segment interconnects using\ngenerative adversarial networks (GANs). Our work leverages the image synthesis\nfeature of GAN-based generative deep neural networks. The stress evaluation of\nmulti-segment interconnects, modeled by partial differential equations, can be\nviewed as time-varying 2D-images-to-image problem where the input is the\nmulti-segment interconnects topology with current densities and the output is\nthe EM stress distribution in those wire segments at the given aging time.\nBased on this observation, we train conditional GAN model using the images of\nmany self-generated multi-segment wires and wire current densities and aging\ntime (as conditions) against the COMSOL simulation results. Different\nhyperparameters of GAN were studied and compared. The proposed algorithm,\ncalled {\\it EM-GAN}, can quickly give accurate stress distribution of a general\nmulti-segment wire tree for a given aging time, which is important for\nfull-chip fast EM failure assessment. Our experimental results show that the\nEM-GAN shows 6.6\\% averaged error compared to COMSOL simulation results with\norders of magnitude speedup. It also delivers 8.3X speedup over\nstate-of-the-art analytic based EM analysis solver.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:18:11 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jin", "Wentian", ""], ["Sadiqbatcha", "Sheriff", ""], ["Zhang", "Jinwei", ""], ["Tan", "Sheldon X. -D.", ""]]}, {"id": "2004.13271", "submitter": "Zhaohe Liao", "authors": "Zhaohe Liao", "title": "Trainable Activation Function in Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current research of neural networks, the activation function is\nmanually specified by human and not able to change themselves during training.\nThis paper focus on how to make the activation function trainable for deep\nneural networks. We use series and linear combination of different activation\nfunctions make activation functions continuously variable. Also, we test the\nperformance of CNNs with Fourier series simulated activation(Fourier-CNN) and\nCNNs with linear combined activation function (LC-CNN) on Cifar-10 dataset. The\nresult shows our trainable activation function reveals better performance than\nthe most used ReLU activation function. Finally, we improves the performance of\nFourier-CNN with Autoencoder, and test the performance of PSO algorithm in\noptimizing the parameters of networks\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 03:50:53 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 09:05:35 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Liao", "Zhaohe", ""]]}, {"id": "2004.13282", "submitter": "William La Cava", "authors": "William La Cava and Jason H. Moore", "title": "Genetic programming approaches to learning fair classifiers", "comments": "9 pages, 7 figures. GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3390157", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Society has come to rely on algorithms like classifiers for important\ndecision making, giving rise to the need for ethical guarantees such as\nfairness. Fairness is typically defined by asking that some statistic of a\nclassifier be approximately equal over protected groups within a population. In\nthis paper, current approaches to fairness are discussed and used to motivate\nalgorithmic proposals that incorporate fairness into genetic programming for\nclassification. We propose two ideas. The first is to incorporate a fairness\nobjective into multi-objective optimization. The second is to adapt lexicase\nselection to define cases dynamically over intersections of protected groups.\nWe describe why lexicase selection is well suited to pressure models to perform\nwell across the potentially infinitely many subgroups over which fairness is\ndesired. We use a recent genetic programming approach to construct models on\nfour datasets for which fairness constraints are necessary, and empirically\ncompare performance to prior methods utilizing game-theoretic solutions.\nMethods are assessed based on their ability to generate trade-offs of subgroup\nfairness and accuracy that are Pareto optimal. The result show that genetic\nprogramming methods in general, and random search in particular, are well\nsuited to this task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:20:25 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["La Cava", "William", ""], ["Moore", "Jason H.", ""]]}, {"id": "2004.13291", "submitter": "Rodrigo Canaan", "authors": "Rodrigo Canaan, Xianbo Gao, Youjin Chung, Julian Togelius, Andy Nealen\n  and Stefan Menzel", "title": "Evaluating the Rainbow DQN Agent in Hanabi with Unseen Partners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hanabi is a cooperative game that challenges exist-ing AI techniques due to\nits focus on modeling the mental states ofother players to interpret and\npredict their behavior. While thereare agents that can achieve near-perfect\nscores in the game byagreeing on some shared strategy, comparatively little\nprogresshas been made in ad-hoc cooperation settings, where partnersand\nstrategies are not known in advance. In this paper, we showthat agents trained\nthrough self-play using the popular RainbowDQN architecture fail to cooperate\nwell with simple rule-basedagents that were not seen during training and,\nconversely, whenthese agents are trained to play with any individual\nrule-basedagent, or even a mix of these agents, they fail to achieve\ngoodself-play scores.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:24:44 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Canaan", "Rodrigo", ""], ["Gao", "Xianbo", ""], ["Chung", "Youjin", ""], ["Togelius", "Julian", ""], ["Nealen", "Andy", ""], ["Menzel", "Stefan", ""]]}, {"id": "2004.13431", "submitter": "Yiming Hu", "authors": "Yiming Hu, Yuding Liang, Zichao Guo, Ruosi Wan, Xiangyu Zhang, Yichen\n  Wei, Qingyi Gu, Jian Sun", "title": "Angle-based Search Space Shrinking for Neural Architecture Search", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a simple and general search space shrinking method,\ncalled Angle-Based search space Shrinking (ABS), for Neural Architecture Search\n(NAS). Our approach progressively simplifies the original search space by\ndropping unpromising candidates, thus can reduce difficulties for existing NAS\nmethods to find superior architectures. In particular, we propose an\nangle-based metric to guide the shrinking process. We provide comprehensive\nevidences showing that, in weight-sharing supernet, the proposed metric is more\nstable and accurate than accuracy-based and magnitude-based metrics to predict\nthe capability of child models. We also show that the angle-based metric can\nconverge fast while training supernet, enabling us to get promising shrunk\nsearch spaces efficiently. ABS can easily apply to most of NAS approaches (e.g.\nSPOS, FairNAS, ProxylessNAS, DARTS and PDARTS). Comprehensive experiments show\nthat ABS can dramatically enhance existing NAS approaches by providing a\npromising shrunk search space.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:26:46 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 13:04:04 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 14:45:22 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hu", "Yiming", ""], ["Liang", "Yuding", ""], ["Guo", "Zichao", ""], ["Wan", "Ruosi", ""], ["Zhang", "Xiangyu", ""], ["Wei", "Yichen", ""], ["Gu", "Qingyi", ""], ["Sun", "Jian", ""]]}, {"id": "2004.13532", "submitter": "Richard Gerum", "authors": "Richard C. Gerum, Achim Schilling", "title": "Integration of Leaky-Integrate-and-Fire-Neurons in Deep Learning\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up to now, modern Machine Learning is mainly based on fitting high\ndimensional functions to enormous data sets, taking advantage of huge hardware\nresources. We show that biologically inspired neuron models such as the\nLeaky-Integrate-and-Fire (LIF) neurons provide novel and efficient ways of\ninformation encoding. They can be integrated in Machine Learning models, and\nare a potential target to improve Machine Learning performance.\n  Thus, we derived simple update-rules for the LIF units from the differential\nequations, which are easy to numerically integrate. We apply a novel approach\nto train the LIF units supervisedly via backpropagation, by assigning a\nconstant value to the derivative of the neuron activation function exclusively\nfor the backpropagation step. This simple mathematical trick helps to\ndistribute the error between the neurons of the pre-connected layer. We apply\nour method to the IRIS blossoms image data set and show that the training\ntechnique can be used to train LIF neurons on image classification tasks.\nFurthermore, we show how to integrate our method in the KERAS (tensorflow)\nframework and efficiently run it on GPUs. To generate a deeper understanding of\nthe mechanisms during training we developed interactive illustrations, which we\nprovide online.\n  With this study we want to contribute to the current efforts to enhance\nMachine Intelligence by integrating principles from biology.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:57:42 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 13:27:06 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Gerum", "Richard C.", ""], ["Schilling", "Achim", ""]]}, {"id": "2004.13710", "submitter": "Rodrigo Canaan", "authors": "Rodrigo Canaan, Xianbo Gao, Julian Togelius, Andy Nealen and Stefan\n  Menzel", "title": "Generating and Adapting to Diverse Ad-Hoc Cooperation Agents in Hanabi", "comments": "arXiv admin note: text overlap with arXiv:1907.03840", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hanabi is a cooperative game that brings the problem of modeling other\nplayers to the forefront. In this game, coordinated groups of players can\nleverage pre-established conventions to great effect, but playing in an ad-hoc\nsetting requires agents to adapt to its partner's strategies with no previous\ncoordination. Evaluating an agent in this setting requires a diverse population\nof potential partners, but so far, the behavioral diversity of agents has not\nbeen considered in a systematic way. This paper proposes Quality Diversity\nalgorithms as a promising class of algorithms to generate diverse populations\nfor this purpose, and generates a population of diverse Hanabi agents using\nMAP-Elites. We also postulate that agents can benefit from a diverse population\nduring training and implement a simple \"meta-strategy\" for adapting to an\nagent's perceived behavioral niche. We show this meta-strategy can work better\nthan generalist strategies even outside the population it was trained with if\nits partner's behavioral niche can be correctly inferred, but in practice a\npartner's behavior depends and interferes with the meta-agent's own behavior,\nsuggesting an avenue for future research in characterizing another agent's\nbehavior during gameplay.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:03:19 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 01:49:53 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Canaan", "Rodrigo", ""], ["Gao", "Xianbo", ""], ["Togelius", "Julian", ""], ["Nealen", "Andy", ""], ["Menzel", "Stefan", ""]]}, {"id": "2004.13832", "submitter": "Luca Mariot", "authors": "Luca Manzoni, Domagoj Jakobovic, Luca Mariot, Stjepan Picek, Mauro\n  Castelli", "title": "Towards an evolutionary-based approach for natural language processing", "comments": "18 pages, 7 figures, 2 tables. Accepted for publication at the\n  Genetic and Evolutionary Computation Conference (GECCO 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks related to Natural Language Processing (NLP) have recently been the\nfocus of a large research endeavor by the machine learning community. The\nincreased interest in this area is mainly due to the success of deep learning\nmethods. Genetic Programming (GP), however, was not under the spotlight with\nrespect to NLP tasks. Here, we propose a first proof-of-concept that combines\nGP with the well established NLP tool word2vec for the next word prediction\ntask. The main idea is that, once words have been moved into a vector space,\ntraditional GP operators can successfully work on vectors, thus producing\nmeaningful words as the output. To assess the suitability of this approach, we\nperform an experimental evaluation on a set of existing newspaper headlines.\nIndividuals resulting from this (pre-)training phase can be employed as the\ninitial population in other NLP tasks, like sentence generation, which will be\nthe focus of future investigations, possibly employing adversarial\nco-evolutionary approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:44:12 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Manzoni", "Luca", ""], ["Jakobovic", "Domagoj", ""], ["Mariot", "Luca", ""], ["Picek", "Stjepan", ""], ["Castelli", "Mauro", ""]]}, {"id": "2004.13925", "submitter": "Jeisson Prieto", "authors": "Jeisson Prieto, Jonatan Gomez", "title": "Hybrid Adaptive Evolutionary Algorithm for Multi-objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The major difficulty in Multi-objective Optimization Evolutionary Algorithms\n(MOEAs) is how to find an appropriate solution that is able to converge towards\nthe true Pareto Front with high diversity. Most existing methodologies, which\nhave demonstrated their niche on various practical problems involving two and\nthree objectives, face significant challenges in the dependency of the\nselection of the EA parameters. Moreover, the process of setting such\nparameters is considered time-consuming, and several research works have tried\nto deal with this problem. This paper proposed a new Multi-objective Algorithm\nas an extension of the Hybrid Adaptive Evolutionary algorithm (HAEA) called\nMoHAEA. MoHAEA allows dynamic adaptation of the application of operator\nprobabilities (rates) to evolve with the solution of the multi-objective\nproblems combining the dominance- and decomposition-based approaches. MoHAEA is\ncompared with four states of the art MOEAs, namely MOEA/D, pa$\\lambda$-MOEA/D,\nMOEA/D-AWA, and NSGA-II on ten widely used multi-objective test problems.\nExperimental results indicate that MoHAEA outperforms the benchmark algorithms\nin terms of how it is able to find a well-covered and well-distributed set of\npoints on the Pareto Front.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:16:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Prieto", "Jeisson", ""], ["Gomez", "Jonatan", ""]]}, {"id": "2004.13936", "submitter": "Marcella Martins", "authors": "Marcella Scoczynski Ribeiro Martins, Mohamed El Yafrani, Myriam R. B.\n  S. Delgado, and Ricardo Luders", "title": "Multi-layer local optima networks for the analysis of advanced local\n  search-based algorithms", "comments": "Accepted in GECCO2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Local Optima Network (LON) is a graph model that compresses the fitness\nlandscape of a particular combinatorial optimization problem based on a\nspecific neighborhood operator and a local search algorithm. Determining which\nand how landscape features affect the effectiveness of search algorithms is\nrelevant for both predicting their performance and improving the design\nprocess. This paper proposes the concept of multi-layer LONs as well as a\nmethodology to explore these models aiming at extracting metrics for fitness\nlandscape analysis. Constructing such models, extracting and analyzing their\nmetrics are the preliminary steps into the direction of extending the study on\nsingle neighborhood operator heuristics to more sophisticated ones that use\nmultiple operators. Therefore, in the present paper we investigate a twolayer\nLON obtained from instances of a combinatorial problem using bitflip and swap\noperators. First, we enumerate instances of NK-landscape model and use the hill\nclimbing heuristic to build the corresponding LONs. Then, using LON metrics, we\nanalyze how efficiently the search might be when combining both strategies. The\nexperiments show promising results and demonstrate the ability of multi-layer\nLONs to provide useful information that could be used for in metaheuristics\nbased on multiple operators such as Variable Neighborhood Search.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 03:20:01 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Martins", "Marcella Scoczynski Ribeiro", ""], ["Yafrani", "Mohamed El", ""], ["Delgado", "Myriam R. B. S.", ""], ["Luders", "Ricardo", ""]]}, {"id": "2004.13971", "submitter": "Youssef Hammadi", "authors": "Youssef Hammadi (MAT), David Ryckelynck (LMSP), Amin El-Bakkali", "title": "Reduced Bond Graph via machine learning for nonlinear multiphysics\n  dynamic systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NA cs.NE math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a machine learning approach aiming at reducing Bond Graphs. The\noutput of the machine learning is a hybrid modeling that contains a reduced\nBond Graph coupled to a simple artificial neural network. The proposed coupling\nenables knowledge continuity in machine learning. In this paper, a neural\nnetwork is obtained by a linear calibration procedure. We propose a method that\ncontains two training steps. First, the method selects the components of the\noriginal Bond Graph that are kept in the Reduced Bond Graph. Secondly, the\nmethod builds an artificial neural network that supplements the reduced Bond\nGraph. Because the output of the machine learning is a hybrid model, not solely\ndata, it becomes difficult to use a usual Backpropagation Through Time to\ncalibrate the weights of the neural network. So, in a first attempt, a very\nsimple neural network is proposed by following a model reduction approach. We\nconsider the modeling of the automotive cabins thermal behavior. The data used\nfor the training step are obtained via solutions of differential algebraic\nequations by using a design of experiment. Simple cooling simulations are run\nduring the training step. We show a simulation speed-up when the reduced bond\ngraph is used to simulate the driving cycle of the WLTP vehicles homologation\nprocedure, while preserving accuracy on output variables. The variables of the\noriginal Bond Graph are split into a set of primary variables, a set of\nsecondary variables and a set of tertiary variables. The reduced bond graph\ncontains all the primary variables, but none of the tertiary variables.\nSecondary variables are coupled to primary ones via an artificial neural\nnetwork. We discuss the extension of this coupling approach to more complex\nartificial neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:19:14 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Hammadi", "Youssef", "", "MAT"], ["Ryckelynck", "David", "", "LMSP"], ["El-Bakkali", "Amin", ""]]}, {"id": "2004.14014", "submitter": "Olivier Teytaud", "authors": "Jialin Liu, Antoine Moreau, Mike Preuss, Baptiste Roziere, Jeremy\n  Rapin, Fabien Teytaud, Olivier Teytaud", "title": "Versatile Black-Box Optimization", "comments": "Accepted at GECCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing automatically the right algorithm using problem descriptors is a\nclassical component of combinatorial optimization. It is also a good tool for\nmaking evolutionary algorithms fast, robust and versatile. We present Shiwa, an\nalgorithm good at both discrete and continuous, noisy and noise-free,\nsequential and parallel, black-box optimization. Our algorithm is\nexperimentally compared to competitors on YABBOB, a BBOB comparable testbed,\nand on some variants of it, and then validated on several real world testbeds.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:20:36 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Liu", "Jialin", ""], ["Moreau", "Antoine", ""], ["Preuss", "Mike", ""], ["Roziere", "Baptiste", ""], ["Rapin", "Jeremy", ""], ["Teytaud", "Fabien", ""], ["Teytaud", "Olivier", ""]]}, {"id": "2004.14765", "submitter": "Enzo Tartaglione", "authors": "Enzo Tartaglione, Andrea Bragagnolo and Marco Grangetto", "title": "Pruning artificial neural networks: a way to find well-generalizing,\n  high-entropy sharp minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a race towards the simplification of deep networks has begun,\nshowing that it is effectively possible to reduce the size of these models with\nminimal or no performance loss. However, there is a general lack in\nunderstanding why these pruning strategies are effective. In this work, we are\ngoing to compare and analyze pruned solutions with two different pruning\napproaches, one-shot and gradual, showing the higher effectiveness of the\nlatter. In particular, we find that gradual pruning allows access to narrow,\nwell-generalizing minima, which are typically ignored when using one-shot\napproaches. In this work we also propose PSP-entropy, a measure to understand\nhow a given neuron correlates to some specific learned classes. Interestingly,\nwe observe that the features extracted by iteratively-pruned models are less\ncorrelated to specific classes, potentially making these models a better fit in\ntransfer learning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:29:37 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Tartaglione", "Enzo", ""], ["Bragagnolo", "Andrea", ""], ["Grangetto", "Marco", ""]]}, {"id": "2004.14827", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "On the Baldwin Effect under Coevolution", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.10061,\n  arXiv:1903.07429, arXiv:1808.03471, arXiv:1811.04073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potentially beneficial interaction between learning and evolution, the\nBaldwin effect, has long been established. This paper considers their\ninteraction within a coevolutionary scenario, ie, where the adaptations of one\nspecies typically affects the fitness of others. Using the NKCS model, which\nallows the systematic exploration of the effects of fitness landscape size,\nruggedness, and degree of coupling, it is shown how the amount of learning and\nthe relative rate of evolution can alter behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:45:41 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 07:14:01 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "2004.14870", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty, Lav R. Varshney, Min-Yen Kan", "title": "Mind Your Inflections! Improving NLP for Non-Standard Englishes with\n  Base-Inflection Encoding", "comments": "Published in the Proceedings of the 2020 Conference on Empirical\n  Methods in Natural Language Processing", "journal-ref": "2020.emnlp-main.455", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inflectional variation is a common feature of World Englishes such as\nColloquial Singapore English and African American Vernacular English. Although\ncomprehension by human readers is usually unimpaired by non-standard\ninflections, current NLP systems are not yet robust. We propose Base-Inflection\nEncoding (BITE), a method to tokenize English text by reducing inflected words\nto their base forms before reinjecting the grammatical information as special\nsymbols. Fine-tuning pretrained NLP models for downstream tasks using our\nencoding defends against inflectional adversaries while maintaining performance\non clean data. Models using BITE generalize better to dialects with\nnon-standard inflections without explicit training and translation models\nconverge faster when trained with BITE. Finally, we show that our encoding\nimproves the vocabulary efficiency of popular data-driven subword tokenizers.\nSince there has been no prior work on quantitatively evaluating vocabulary\nefficiency, we propose metrics to do so.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:15:40 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 18:54:40 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 05:20:28 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 06:16:31 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""], ["Varshney", "Lav R.", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2004.14878", "submitter": "Zdenek Straka", "authors": "Zdenek Straka, Tomas Svoboda, Matej Hoffmann", "title": "PreCNet: Next Frame Video Prediction Based on Predictive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding, currently a highly influential theory in neuroscience, has\nnot been widely adopted in machine learning yet. In this work, we transform the\nseminal model of Rao and Ballard (1999) into a modern deep learning framework\nwhile remaining maximally faithful to the original schema. The resulting\nnetwork we propose (PreCNet) is tested on a widely used next frame video\nprediction benchmark, which consists of images from an urban environment\nrecorded from a car-mounted camera. On this benchmark (training: 41k images\nfrom KITTI dataset; testing: Caltech Pedestrian dataset), we achieve to our\nknowledge the best performance to date when measured with the Structural\nSimilarity Index (SSIM). Performance on all measures was further improved when\na larger training set (2M images from BDD100k), pointing to the limitations of\nthe KITTI training set. This work demonstrates that an architecture carefully\nbased in a neuroscience model, without being explicitly tailored to the task at\nhand, can exhibit unprecedented performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:31:24 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 13:58:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Straka", "Zdenek", ""], ["Svoboda", "Tomas", ""], ["Hoffmann", "Matej", ""]]}, {"id": "2004.14884", "submitter": "Arthur Bra\\v{z}inskas", "authors": "Arthur Bra\\v{z}inskas, Mirella Lapata, Ivan Titov", "title": "Few-Shot Learning for Opinion Summarization", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion summarization is the automatic creation of text reflecting subjective\ninformation expressed in multiple documents, such as user reviews of a product.\nThe task is practically important and has attracted a lot of attention.\nHowever, due to the high cost of summary production, datasets large enough for\ntraining supervised models are lacking. Instead, the task has been\ntraditionally approached with extractive methods that learn to select text\nfragments in an unsupervised or weakly-supervised way. Recently, it has been\nshown that abstractive summaries, potentially more fluent and better at\nreflecting conflicting information, can also be produced in an unsupervised\nfashion. However, these models, not being exposed to actual summaries, fail to\ncapture their essential properties. In this work, we show that even a handful\nof summaries is sufficient to bootstrap generation of the summary text with all\nexpected properties, such as writing style, informativeness, fluency, and\nsentiment preservation. We start by training a conditional Transformer language\nmodel to generate a new product review given other available reviews of the\nproduct. The model is also conditioned on review properties that are directly\nrelated to summaries; the properties are derived from reviews with no manual\neffort. In the second stage, we fine-tune a plug-in module that learns to\npredict property values on a handful of summaries. This lets us switch the\ngenerator to the summarization mode. We show on Amazon and Yelp datasets that\nour approach substantially outperforms previous extractive and abstractive\nmethods in automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:37:38 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 19:45:25 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 06:30:38 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bra\u017einskas", "Arthur", ""], ["Lapata", "Mirella", ""], ["Titov", "Ivan", ""]]}, {"id": "2004.14942", "submitter": "Adnan Mehonic", "authors": "Adnan Mehonic, Abu Sebastian, Bipin Rajendran, Osvaldo Simeone, Eleni\n  Vasilaki, Anthony J. Kenyon", "title": "Memristors -- from In-memory computing, Deep Learning Acceleration,\n  Spiking Neural Networks, to the Future of Neuromorphic and Bio-inspired\n  Computing", "comments": "Keywords: memristor, neuromorphic, AI, deep learning, spiking neural\n  networks, in-memory computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning, particularly in the form of deep learning, has driven most\nof the recent fundamental developments in artificial intelligence. Deep\nlearning is based on computational models that are, to a certain extent,\nbio-inspired, as they rely on networks of connected simple computing units\noperating in parallel. Deep learning has been successfully applied in areas\nsuch as object/pattern recognition, speech and natural language processing,\nself-driving vehicles, intelligent self-diagnostics tools, autonomous robots,\nknowledgeable personal assistants, and monitoring. These successes have been\nmostly supported by three factors: availability of vast amounts of data,\ncontinuous growth in computing power, and algorithmic innovations. The\napproaching demise of Moore's law, and the consequent expected modest\nimprovements in computing power that can be achieved by scaling, raise the\nquestion of whether the described progress will be slowed or halted due to\nhardware limitations. This paper reviews the case for a novel beyond CMOS\nhardware technology, memristors, as a potential solution for the implementation\nof power-efficient in-memory computing, deep learning accelerators, and spiking\nneural networks. Central themes are the reliance on non-von-Neumann computing\narchitectures and the need for developing tailored learning and inference\nalgorithms. To argue that lessons from biology can be useful in providing\ndirections for further progress in artificial intelligence, we briefly discuss\nan example based reservoir computing. We conclude the review by speculating on\nthe big picture view of future neuromorphic and brain-inspired computing\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:49:03 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mehonic", "Adnan", ""], ["Sebastian", "Abu", ""], ["Rajendran", "Bipin", ""], ["Simeone", "Osvaldo", ""], ["Vasilaki", "Eleni", ""], ["Kenyon", "Anthony J.", ""]]}]