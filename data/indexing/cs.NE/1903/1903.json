[{"id": "1903.00068", "submitter": "Xinyun Zou", "authors": "Xinyun Zou, Soheil Kolouri, Praveen K. Pilly, Jeffrey L. Krichmar", "title": "Neuromodulated Goal-Driven Perception in Uncertain Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In uncertain domains, the goals are often unknown and need to be predicted by\nthe organism or system. In this paper, contrastive excitation backprop (c-EB)\nwas used in a goal-driven perception task with pairs of noisy MNIST digits,\nwhere the system had to increase attention to one of the two digits\ncorresponding to a goal (i.e., even, odd, low value, or high value) and\ndecrease attention to the distractor digit or noisy background pixels. Because\nthe valid goal was unknown, an online learning model based on the cholinergic\nand noradrenergic neuromodulatory systems was used to predict a noisy goal\n(expected uncertainty) and re-adapt when the goal changed (unexpected\nuncertainty). This neurobiologically plausible model demonstrates how\nneuromodulatory systems can predict goals in uncertain domains and how\nattentional mechanisms can enhance the perception of that goal.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 19:46:09 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Zou", "Xinyun", ""], ["Kolouri", "Soheil", ""], ["Pilly", "Praveen K.", ""], ["Krichmar", "Jeffrey L.", ""]]}, {"id": "1903.00342", "submitter": "Yu Li", "authors": "Yu Li, Chao Huang, Lizhong Ding, Zhongxiao Li, Yijie Pan, Xin Gao", "title": "Deep learning in bioinformatics: introduction, application, and\n  perspective in big data era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, which is especially formidable in handling big data, has\nachieved great success in various fields, including bioinformatics. With the\nadvances of the big data era in biology, it is foreseeable that deep learning\nwill become increasingly important in the field and will be incorporated in\nvast majorities of analysis pipelines. In this review, we provide both the\nexoteric introduction of deep learning, and concrete examples and\nimplementations of its representative applications in bioinformatics. We start\nfrom the recent achievements of deep learning in the bioinformatics field,\npointing out the problems which are suitable to use deep learning. After that,\nwe introduce deep learning in an easy-to-understand fashion, from shallow\nneural networks to legendary convolutional neural networks, legendary recurrent\nneural networks, graph neural networks, generative adversarial networks,\nvariational autoencoder, and the most recent state-of-the-art architectures.\nAfter that, we provide eight examples, covering five bioinformatics research\ndirections and all the four kinds of data type, with the implementation written\nin Tensorflow and Keras. Finally, we discuss the common issues, such as\noverfitting and interpretability, that users will encounter when adopting deep\nlearning methods and provide corresponding suggestions. The implementations are\nfreely available at \\url{https://github.com/lykaust15/Deep_learning_examples}.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:51:05 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Li", "Yu", ""], ["Huang", "Chao", ""], ["Ding", "Lizhong", ""], ["Li", "Zhongxiao", ""], ["Pan", "Yijie", ""], ["Gao", "Xin", ""]]}, {"id": "1903.00568", "submitter": "Tianyu Li", "authors": "Tianyu Li, Bolun Dai", "title": "GRP Model for Sensorimotor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from complex demonstrations is challenging, especially when the\ndemonstration consists of different strategies. A popular approach is to use a\ndeep neural network to perform imitation learning. However, the structure of\nthat deep neural network has to be ``deep\" enough to capture all possible\nscenarios. Besides the machine learning issue, how humans learn in the sense of\nphysiology has rarely been addressed and relevant works on spinal cord learning\nare rarer. In this work, we develop a novel modular learning architecture, the\nGenerator and Responsibility Predictor (GRP) model, which automatically learns\nthe sub-task policies from an unsegmented controller demonstration and learns\nto switch between the policies. We also introduce a more physiological based\nneural network architecture. We implemented our GRP model and our proposed\nneural network to form a model the transfers the swing leg control from the\nbrain to the spinal cord. Our result suggests that by using the GRP model the\nbrain can successfully transfer the target swing leg control to the spinal cord\nand the resulting model can switch between sub-control policies automatically.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 22:41:51 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Tianyu", ""], ["Dai", "Bolun", ""]]}, {"id": "1903.00742", "submitter": "Joel Leibo", "authors": "Joel Z. Leibo, Edward Hughes, Marc Lanctot, Thore Graepel", "title": "Autocurricula and the Emergence of Innovation from Social Interaction: A\n  Manifesto for Multi-Agent Intelligence Research", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution has produced a multi-scale mosaic of interacting adaptive units.\nInnovations arise when perturbations push parts of the system away from stable\nequilibria into new regimes where previously well-adapted solutions no longer\nwork. Here we explore the hypothesis that multi-agent systems sometimes display\nintrinsic dynamics arising from competition and cooperation that provide a\nnaturally emergent curriculum, which we term an autocurriculum. The solution of\none social task often begets new social tasks, continually generating novel\nchallenges, and thereby promoting innovation. Under certain conditions these\nchallenges may become increasingly complex over time, demanding that agents\naccumulate ever more innovations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 18:13:25 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 15:25:43 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Leibo", "Joel Z.", ""], ["Hughes", "Edward", ""], ["Lanctot", "Marc", ""], ["Graepel", "Thore", ""]]}, {"id": "1903.00884", "submitter": "Yasir Hussain", "authors": "Yasir Hussain, Zhiqiu Huang, Yu Zhou, Senzhang Wang", "title": "CodeGRU: Context-aware Deep Learning with Gated Recurrent Unit for\n  Source Code Modeling", "comments": null, "journal-ref": "Information and Software Technology. Volume 125, September 2020,\n  106309", "doi": "10.1016/j.infsof.2020.106309", "report-no": null, "categories": "cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning based Natural Language Processing (NLP) models have\nshown great potential in the modeling of source code. However, a major\nlimitation of these approaches is that they take source code as simple tokens\nof text and ignore its contextual, syntactical and structural dependencies. In\nthis work, we present CodeGRU, a gated recurrent unit based source code\nlanguage model that is capable of capturing source code's contextual,\nsyntactical and structural dependencies. We introduce a novel approach which\ncan capture the source code context by leveraging the source code token types.\nFurther, we adopt a novel approach which can learn variable size context by\ntaking into account source code's syntax, and structural information. We\nevaluate CodeGRU with real-world data set and it shows that CodeGRU outperforms\nthe state-of-the-art language models and help reduce the vocabulary size up to\n24.93\\%. Unlike previous works, we tested CodeGRU with an independent test set\nwhich suggests that our methodology does not requisite the source code comes\nfrom the same domain as training data while providing suggestions. We further\nevaluate CodeGRU with two software engineering applications: source code\nsuggestion, and source code completion. Our experiment confirms that the source\ncode's contextual information can be vital and can help improve the software\nlanguage models. The extensive evaluation of CodeGRU shows that it outperforms\nthe state-of-the-art models. The results further suggest that the proposed\napproach can help reduce the vocabulary size and is of practical use for\nsoftware developers.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 11:44:08 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 12:12:00 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hussain", "Yasir", ""], ["Huang", "Zhiqiu", ""], ["Zhou", "Yu", ""], ["Wang", "Senzhang", ""]]}, {"id": "1903.01093", "submitter": "David Sprunger", "authors": "David Sprunger and Shin-ya Katsumata", "title": "Differentiable Causal Computations via Delayed Trace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.NE math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate causal computations taking sequences of inputs to sequences of\noutputs where the $n$th output depends on the first $n$ inputs only. We model\nthese in category theory via a construction taking a Cartesian category $C$ to\nanother category $St(C)$ with a novel trace-like operation called \"delayed\ntrace\", which misses yanking and dinaturality axioms of the usual trace. The\ndelayed trace operation provides a feedback mechanism in $St(C)$ with an\nimplicit guardedness guarantee.\n  When $C$ is equipped with a Cartesian differential operator, we construct a\ndifferential operator for $St(C)$ using an abstract version of backpropagation\nthrough time, a technique from machine learning based on unrolling of\nfunctions. This obtains a swath of properties for backpropagation through time,\nincluding a chain rule and Schwartz theorem. Our differential operator is also\nable to compute the derivative of a stateful network without requiring the\nnetwork to be unrolled.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 07:05:27 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Sprunger", "David", ""], ["Katsumata", "Shin-ya", ""]]}, {"id": "1903.01180", "submitter": "Huanneng Qiu", "authors": "Huanneng Qiu, Matthew Garratt, David Howard and Sreenatha Anavatti", "title": "Evolving Spiking Neural Networks for Nonlinear Control Problems", "comments": "conference: ssci 2018", "journal-ref": null, "doi": "10.1109/SSCI.2018.8628848", "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks are powerful computational modelling tools that have\nattracted much interest because of the bioinspired modelling of synaptic\ninteractions between neurons. Most of the research employing spiking neurons\nhas been non-behavioural and discontinuous. Comparatively, this paper presents\na recurrent spiking controller that is capable of solving nonlinear control\nproblems in continuous domains using a popular topology evolution algorithm as\nthe learning mechanism. We propose two mechanisms necessary to the decoding of\ncontinuous signals from discrete spike transmission: (i) a background current\ncomponent to maintain frequency sufficiency for spike rate decoding, and (ii) a\ngeneral network structure that derives strength from topology evolution. We\ndemonstrate that the proposed spiking controller can learn significantly faster\nto discover functional solutions than sigmoidal neural networks in solving a\nclassic nonlinear control problem.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:30:53 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Qiu", "Huanneng", ""], ["Garratt", "Matthew", ""], ["Howard", "David", ""], ["Anavatti", "Sreenatha", ""]]}, {"id": "1903.01217", "submitter": "Mien Brabeeba Wang", "authors": "Nancy Lynch and Mien Brabeeba Wang", "title": "Integrating Temporal Information to Spatial Information in a Neural\n  Circuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider networks of deterministic spiking neurons, firing\nsynchronously at discrete times; such spiking neural networks are inspired by\nnetworks of neurons and synapses that occur in brains. We consider the problem\nof translating temporal information into spatial information in such networks,\nan important task that is carried out by actual brains.\n  Specifically, we define two problems: \"First Consecutive Spikes Counting\n(FCSC)\" and \"Total Spikes Counting (TSC)\", which model spike and rate coding\naspects of translating temporal information into spatial information\nrespectively. Assuming an upper bound of $T$ on the length of the temporal\ninput signal, we design two networks that solve these two problems, each using\n$O(\\log T)$ neurons and terminating in time $1$. We also prove that there is no\nnetwork with less than $T$ neurons that solves either question in time $0$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 03:59:59 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 19:17:14 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 18:20:39 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Lynch", "Nancy", ""], ["Wang", "Mien Brabeeba", ""]]}, {"id": "1903.01341", "submitter": "Federico Galatolo", "authors": "Federico A. Galatolo, Mario G. C. A. Cimino, Gigliola Vaglini", "title": "Using stigmergy as a computational memory in the design of recurrent\n  neural networks", "comments": null, "journal-ref": null, "doi": "10.5220/0007581508300836", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel architecture of Recurrent Neural Network (RNN) is\ndesigned and experimented. The proposed RNN adopts a computational memory based\non the concept of stigmergy. The basic principle of a Stigmergic Memory (SM) is\nthat the activity of deposit/removal of a quantity in the SM stimulates the\nnext activities of deposit/removal. Accordingly, subsequent SM activities tend\nto reinforce/weaken each other, generating a coherent coordination between the\nSM activities and the input temporal stimulus. We show that, in a problem of\nsupervised classification, the SM encodes the temporal input in an emergent\nrepresentational model, by coordinating the deposit, removal and classification\nactivities. This study lays down a basic framework for the derivation of a\nSM-RNN. A formal ontology of SM is discussed, and the SM-RNN architecture is\ndetailed. To appreciate the computational power of an SM-RNN, comparative NNs\nhave been selected and trained to solve the MNIST handwritten digits\nrecognition benchmark in its two variants: spatial (sequences of bitmap rows)\nand temporal (sequences of pen strokes).\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 22:26:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Galatolo", "Federico A.", ""], ["Cimino", "Mario G. C. A.", ""], ["Vaglini", "Gigliola", ""]]}, {"id": "1903.01456", "submitter": "Borko Bo\\v{s}kovi\\'c", "authors": "Borko Bo\\v{s}kovi\\'c and Janez Brest", "title": "Two-phase protein folding optimization on a three-dimensional AB\n  off-lattice model", "comments": "20 pages, 13 tables, 6 figures. arXiv admin note: text overlap with\n  arXiv:1710.07031", "journal-ref": "Borko Bo\\v{s}kovi\\'c, Janez Brest. Two-phase protein folding\n  optimization on a three-dimensional AB off-lattice model. Swarm and\n  Evolutionary Computation, 2020, vol. 57, pp. 100708", "doi": "10.1016/j.swevo.2020.100708", "report-no": null, "categories": "cs.NE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a two-phase protein folding optimization on a\nthree-dimensional AB off-lattice model. The first phase is responsible for\nforming conformations with a good hydrophobic core or a set of compact\nhydrophobic amino acid positions. These conformations are forwarded to the\nsecond phase, where an accurate search is performed with the aim of locating\nconformations with the best energy value. The optimization process switches\nbetween these two phases until the stopping condition is satisfied. An\nauxiliary fitness function was designed for the first phase, while the original\nfitness function is used in the second phase. The auxiliary fitness function\nincludes an expression about the quality of the hydrophobic core. This\nexpression is crucial for leading the search process to the promising solutions\nthat have a good hydrophobic core and, consequently, improves the efficiency of\nthe whole optimization process. Our differential evolution algorithm was used\nfor demonstrating the efficiency of two-phase optimization. It was analyzed on\nwell-known amino acid sequences that are used frequently in the literature. The\nobtained experimental results show that the employed two-phase optimization\nimproves the efficiency of our algorithm significantly and that the proposed\nalgorithm is superior to other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:15:53 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 08:37:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bo\u0161kovi\u0107", "Borko", ""], ["Brest", "Janez", ""]]}, {"id": "1903.01548", "submitter": "Namwoo Kang", "authors": "Sangeun Oh, Yongsu Jung, Seongsin Kim, Ikjin Lee, Namwoo Kang", "title": "Deep Generative Design: Integration of Topology Optimization and\n  Generative Models", "comments": null, "journal-ref": "Journal of Mechanical Design, 141(11), 111405", "doi": "10.1115/1.4044229", "report-no": null, "categories": "cs.LG cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently been applied to various research areas of design\noptimization. This study presents the need and effectiveness of adopting deep\nlearning for generative design (or design exploration) research area. This work\nproposes an artificial intelligent (AI)-based design automation framework that\nis capable of generating numerous design options which are not only aesthetic\nbut also optimized for engineering performance. The proposed framework\nintegrates topology optimization and deep generative models (e.g., generative\nadversarial networks (GANs)) in an iterative manner to explore new design\noptions, thus generating a large number of designs starting from limited\nprevious design data. In addition, anomaly detection can evaluate the novelty\nof generated designs, thus helping designers choose among design options. The\n2D wheel design problem is applied as a case study for validation of the\nproposed framework. The framework manifests better aesthetics, diversity, and\nrobustness of generated designs than previous generative design methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 07:38:41 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 14:07:42 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Oh", "Sangeun", ""], ["Jung", "Yongsu", ""], ["Kim", "Seongsin", ""], ["Lee", "Ikjin", ""], ["Kang", "Namwoo", ""]]}, {"id": "1903.01567", "submitter": "Jayesh Gupta", "authors": "Bohan Wu, Jayesh K. Gupta, Mykel J. Kochenderfer", "title": "Model Primitive Hierarchical Lifelong Reinforcement Learning", "comments": "9 pages, 10 figures. Accepted as a full paper at AAMAS 2019", "journal-ref": "International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning interpretable and transferable subpolicies and performing task\ndecomposition from a single, complex task is difficult. Some traditional\nhierarchical reinforcement learning techniques enforce this decomposition in a\ntop-down manner, while meta-learning techniques require a task distribution at\nhand to learn such decompositions. This paper presents a framework for using\ndiverse suboptimal world models to decompose complex task solutions into\nsimpler modular subpolicies. This framework performs automatic decomposition of\na single source task in a bottom up manner, concurrently learning the required\nmodular subpolicies as well as a controller to coordinate them. We perform a\nseries of experiments on high dimensional continuous action control tasks to\ndemonstrate the effectiveness of this approach at both complex single task\nlearning and lifelong learning. Finally, we perform ablation studies to\nunderstand the importance and robustness of different elements in the framework\nand limitations to this approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 22:14:23 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Wu", "Bohan", ""], ["Gupta", "Jayesh K.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1903.01635", "submitter": "Brian Hoskins", "authors": "Brian D. Hoskins, Matthew W. Daniels, Siyuan Huang, Advait Madhavan,\n  Gina C. Adam, Nikolai Zhitenev, Jabez J. McClelland, Mark D. Stiles", "title": "Streaming Batch Eigenupdates for Hardware Neuromorphic Networks", "comments": "13 pages, 5 figures", "journal-ref": "Frontiers in Neuroscience 13 (2019): 793", "doi": "10.3389/fnins.2019.00793", "report-no": null, "categories": "cs.LG cs.ET cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neuromorphic networks based on nanodevices, such as metal oxide memristors,\nphase change memories, and flash memory cells, have generated considerable\ninterest for their increased energy efficiency and density in comparison to\ngraphics processing units (GPUs) and central processing units (CPUs). Though\nimmense acceleration of the training process can be achieved by leveraging the\nfact that the time complexity of training does not scale with the network size,\nit is limited by the space complexity of stochastic gradient descent, which\ngrows quadratically. The main objective of this work is to reduce this space\ncomplexity by using low-rank approximations of stochastic gradient descent.\nThis low spatial complexity combined with streaming methods allows for\nsignificant reductions in memory and compute overhead, opening the doors for\nimprovements in area, time and energy efficiency of training. We refer to this\nalgorithm and architecture to implement it as the streaming batch eigenupdate\n(SBE) approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 02:35:52 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Hoskins", "Brian D.", ""], ["Daniels", "Matthew W.", ""], ["Huang", "Siyuan", ""], ["Madhavan", "Advait", ""], ["Adam", "Gina C.", ""], ["Zhitenev", "Nikolai", ""], ["McClelland", "Jabez J.", ""], ["Stiles", "Mark D.", ""]]}, {"id": "1903.01715", "submitter": "Gaelle Loosli", "authors": "Isma\\\"ila Seck (LIMOS, LITIS), Ga\\\"elle Loosli (LIMOS), Stephane Canu\n  (LITIS)", "title": "L 1-norm double backpropagation adversarial defense", "comments": "ESANN - European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning, Apr 2019, Bruges, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are a challenging open problem for deep neural networks.\nWe propose in this paper to add a penalization term that forces the decision\nfunction to be at in some regions of the input space, such that it becomes, at\nleast locally, less sensitive to attacks. Our proposition is theoretically\nmotivated and shows on a first set of carefully conducted experiments that it\nbehaves as expected when used alone, and seems promising when coupled with\nadversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 08:04:34 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Seck", "Isma\u00efla", "", "LIMOS, LITIS"], ["Loosli", "Ga\u00eblle", "", "LIMOS"], ["Canu", "Stephane", "", "LITIS"]]}, {"id": "1903.01768", "submitter": "Ke Li Kl", "authors": "Huiru Gao, Haifeng Nie, Ke Li", "title": "Visualisation of Pareto Front Approximation: A Short Survey and\n  Empirical Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualisation is an effective way to facilitate the analysis and\nunderstanding of multivariate data. In the context of multi-objective\noptimisation, comparing to quantitative performance metrics, visualisation is,\nin principle, able to provide a decision maker better insights about Pareto\nfront approximation sets (e.g. the distribution of solutions, the geometric\ncharacteristics of Pareto front approximation) thus to facilitate the\ndecision-making (e.g. the exploration of trade-off relationship, the knee\nregion or region of interest). In this paper, we overview some currently\nprevalent visualisation techniques according to the way how data is\nrepresented. To have a better understanding of the pros and cons of different\nvisualisation techniques, we empirically compare six representative\nvisualisation techniques for the exploratory analysis of different Pareto front\napproximation sets obtained by four state-of-the-art evolutionary\nmulti-objective optimisation algorithms on the classic DTLZ benchmark test\nproblems. From the empirical results, we find that visual comparisons also\nfollow the \\textit{No-Free-Lunch} theorem where no single visualisation\ntechnique is able to provide a comprehensive understanding of the\ncharacteristics of a Pareto front approximation set. In other words, a specific\ntype of visualisation technique is only good at exploring a particular aspect\nof the data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 11:16:49 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Gao", "Huiru", ""], ["Nie", "Haifeng", ""], ["Li", "Ke", ""]]}, {"id": "1903.01885", "submitter": "M\\'ario C\\'esar San Felice", "authors": "Pedro H. D. B. Hokama, M\\'ario C. San Felice, Evandro C. Bracht,\n  F\\'abio L. Usberti", "title": "Evolutionary framework for two-stage stochastic resource allocation\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation problems are a family of problems in which resources must\nbe selected to satisfy given demands. This paper focuses on the two-stage\nstochastic generalization of resource allocation problems where future demands\nare expressed in a finite number of possible scenarios. The goal is to select\ncost effective resources to be acquired in the present time (first stage), and\nto implement a complete solution for each scenario (second stage), while\nminimizing the total expected cost of the choices in both stages.\n  We propose an evolutionary framework for solving general two-stage stochastic\nresource allocation problems. In each iteration of our framework, a local\nsearch algorithm selects resources to be acquired in the first stage. A genetic\nmetaheuristic then completes the solutions for each scenario and relevant\ninformation is passed onto the next iteration, thereby supporting the\nacquisition of promising resources in the following first stage.\nExperimentation on numerous instances of the two-stage stochastic Steiner tree\nproblem suggests that our evolutionary framework is powerful enough to address\nlarge instances of a wide variety of two-stage stochastic resource allocation\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:30:38 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Hokama", "Pedro H. D. B.", ""], ["Felice", "M\u00e1rio C. San", ""], ["Bracht", "Evandro C.", ""], ["Usberti", "F\u00e1bio L.", ""]]}, {"id": "1903.01886", "submitter": "Simyung Chang", "authors": "Simyung Chang, John Yang, Jaeseok Choi, Nojun Kwak", "title": "Genetic-Gated Networks for Deep Reinforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": "14pages, This paper is accepted at NIPS 2018", "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that\ncombine a gate vector composed of binary genetic genes in the hidden layer(s)\nof networks. Our method can take both advantages of gradient-free optimization\nand gradient-based optimization methods, of which the former is effective for\nproblems with multiple local minima, while the latter can quickly find local\nminima. In addition, multiple chromosomes can define different models, making\nit easy to construct multiple models and can be effectively applied to problems\nthat require multiple models. We show that this G2N can be applied to typical\nreinforcement learning algorithms to achieve a large improvement in sample\nefficiency and performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:56:05 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Chang", "Simyung", ""], ["Yang", "John", ""], ["Choi", "Jaeseok", ""], ["Kwak", "Nojun", ""]]}, {"id": "1903.01893", "submitter": "Pranshu Gupta", "authors": "Pranshu Gupta", "title": "Algorithms Inspired by Nature: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nature is known to be the best optimizer. Natural processes most often than\nnot reach an optimal equilibrium. Scientists have always strived to understand\nand model such processes.Thus, many algorithms exist today that are inspired by\nnature. Many of these algorithms and heuristics can be used to solve problems\nfor which no polynomial time algorithms exist,such as Job Shop Scheduling and\nmany other Combinatorial Optimization problems. We will discuss some of these\nalgorithms and heuristics and how they help us solve complex problems of\npractical importance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 07:27:39 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Gupta", "Pranshu", ""]]}, {"id": "1903.01894", "submitter": "Zhaoyang Ai", "authors": "Zhaoyang Ai, Chaodong Fan, Yingjie Zhang, Huigui Rong, Ze'an Tian,\n  Haibing Fu", "title": "Boundary Evolution Algorithm for SAT-NP", "comments": "8 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A boundary evolution Algorithm (BEA) is proposed by simultaneously taking\ninto account the bottom and the high-level crossover and mutation, ie., the\nboundary of the hierarchical genetic algorithm. Operators and optimal\nindividuals based on optional annealing are designed. Based on the numerous\nversions of genetic algorithm, the boundary evolution approach with crossover\nand mutation has been tested on the SAT problem and compared with two competing\nmethods: a traditional genetic algorithm and another traditional hierarchical\ngenetic algorithm, and among some others. The results of the comparative\nexperiments in solving SAT problem have proved that the new hierarchical\ngenetic algorithm based on simulated annealing and optimal individuals (BEA)\ncan improve the success rate and convergence speed considerably for SAT problem\ndue to its avoidance of both divergence and loss of optimal individuals, and by\ncoronary, conducive to NP problem. Though more extensive comparisons are to be\nmade on more algorithms, the consideration of the boundary elasticity of\nhierarchical genetic algorithm is an implication of evolutionary algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 08:12:34 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ai", "Zhaoyang", ""], ["Fan", "Chaodong", ""], ["Zhang", "Yingjie", ""], ["Rong", "Huigui", ""], ["Tian", "Ze'an", ""], ["Fu", "Haibing", ""]]}, {"id": "1903.01895", "submitter": "Marijn Van Knippenberg", "authors": "Marijn van Knippenberg, Vlado Menkovski, Sergio Consoli", "title": "Evolutionary Construction of Convolutional Neural Networks", "comments": null, "journal-ref": "Springer Lecture Notes in Computer Science 11331 (2018) 293-304", "doi": "10.1007/978-3-030-13709-0_25", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuro-Evolution is a field of study that has recently gained significantly\nincreased traction in the deep learning community. It combines deep neural\nnetworks and evolutionary algorithms to improve and/or automate the\nconstruction of neural networks. Recent Neuro-Evolution approaches have shown\npromising results, rivaling hand-crafted neural networks in terms of accuracy.\nA two-step approach is introduced where a convolutional autoencoder is created\nthat efficiently compresses the input data in the first step, and a\nconvolutional neural network is created to classify the compressed data in the\nsecond step. The creation of networks in both steps is guided by by an\nevolutionary process, where new networks are constantly being generated by\nmutating members of a collection of existing networks. Additionally, a method\nis introduced that considers the trade-off between compression and information\nloss of different convolutional autoencoders. This is used to select the\noptimal convolutional autoencoder from among those evolved to compress the data\nfor the second step. The complete framework is implemented, tested on the\npopular CIFAR-10 data set, and the results are discussed. Finally, a number of\npossible directions for future work with this particular framework in mind are\nconsidered, including opportunities to improve its efficiency and its\napplication in particular areas.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 12:30:51 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["van Knippenberg", "Marijn", ""], ["Menkovski", "Vlado", ""], ["Consoli", "Sergio", ""]]}, {"id": "1903.01896", "submitter": "Rodrigo Soto Garrido", "authors": "Guillermo Fuertes, Manuel Vargas, Miguel Alfaro, Rodrigo Soto-Garrido,\n  Jorge Sabattin and Maria Alejandra Peralta", "title": "Chaotic Genetic Algorithm and The Effects of Entropy in Performance\n  Optimization", "comments": "8 pages, 4 figures, 31 references. Accepted for publication in Chaos:\n  An Interdisciplinary Journal of Nonlinear Science", "journal-ref": null, "doi": "10.1063/1.5048299", "report-no": null, "categories": "cs.NE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new edge about the Chaotic Genetic Algorithm (CGA) and\nthe importance of the entropy in the initial population. Inspired by chaos\ntheory the CGA uses chaotic maps to modify the stochastic parameters of Genetic\nAlgorithm (GA). The algorithm modifies the parameters of the initial population\nusing chaotic series and then analyzes the entropy of such population. This\nstrategy exhibits the relationship between entropy and performance optimization\nin complex search spaces. Our study includes the optimization of nine benchmark\nfunctions using eight different chaotic maps for each of the benchmark\nfunctions. The numerical experiment demonstrates a direct relation between\nentropy and performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 15:54:22 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Fuertes", "Guillermo", ""], ["Vargas", "Manuel", ""], ["Alfaro", "Miguel", ""], ["Soto-Garrido", "Rodrigo", ""], ["Sabattin", "Jorge", ""], ["Peralta", "Maria Alejandra", ""]]}, {"id": "1903.01969", "submitter": "Saeed Amizadeh", "authors": "Saeed Amizadeh, Sergiy Matusevych, Markus Weimer", "title": "PDP: A General Neural Framework for Learning Constraint Satisfaction\n  Solvers", "comments": "Neuro-symbolic Methods, Neural Combinatorial Optimization, Geometric\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been recent efforts for incorporating Graph Neural Network models\nfor learning full-stack solvers for constraint satisfaction problems (CSP) and\nparticularly Boolean satisfiability (SAT). Despite the unique representational\npower of these neural embedding models, it is not clear how the search strategy\nin the learned models actually works. On the other hand, by fixing the search\nstrategy (e.g. greedy search), we would effectively deprive the neural models\nof learning better strategies than those given. In this paper, we propose a\ngeneric neural framework for learning CSP solvers that can be described in\nterms of probabilistic inference and yet learn search strategies beyond greedy\nsearch. Our framework is based on the idea of propagation, decimation and\nprediction (and hence the name PDP) in graphical models, and can be trained\ndirectly toward solving CSP in a fully unsupervised manner via energy\nminimization, as shown in the paper. Our experimental results demonstrate the\neffectiveness of our framework for SAT solving compared to both neural and the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:26:33 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Amizadeh", "Saeed", ""], ["Matusevych", "Sergiy", ""], ["Weimer", "Markus", ""]]}, {"id": "1903.02014", "submitter": "Zeyang Yu", "authors": "Zeyang Yu, Shengxi Li and Danilo Mandic", "title": "Widely Linear Complex-valued Autoencoder: Dealing with Noncircularity in\n  Generative-Discriminative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new structure for the complex-valued autoencoder by introducing\nadditional degrees of freedom into its design through a widely linear (WL)\ntransform. The corresponding widely linear backpropagation algorithm is also\ndeveloped using the $\\mathbb{CR}$ calculus, to unify the gradient calculation\nof the cost function and the underlying WL model. More specifically, all the\nexisting complex-valued autoencoders employ the strictly linear transform,\nwhich is optimal only when the complex-valued outputs of each network layer are\nindependent of the conjugate of the inputs. In addition, the widely linear\nmodel which underpins our work allows us to consider all the second-order\nstatistics of inputs. This provides more freedom in the design and enhanced\noptimization opportunities, as compared to the state-of-the-art. Furthermore,\nwe show that the most widely adopted cost function, i.e., the mean squared\nerror, is not best suited for the complex domain, as it is a real quantity with\na single degree of freedom, while both the phase and the amplitude information\nneed to be optimized. To resolve this issue, we design a new cost function,\nwhich is capable of controlling the balance between the phase and the amplitude\ncontribution to the solution. The experimental results verify the superior\nperformance of the proposed autoencoder together with the new cost function,\nespecially for the imaging scenarios where the phase preserves extensive\ninformation on edges and shapes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 19:04:09 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Yu", "Zeyang", ""], ["Li", "Shengxi", ""], ["Mandic", "Danilo", ""]]}, {"id": "1903.02079", "submitter": "Nazeeh Ghatasheh", "authors": "Nazeeh Ghatasheh, Hossam Faris, Ibrahim Aljarah, Rizik M. H. Al-Sayyed", "title": "Optimizing Software Effort Estimation Models Using Firefly Algorithm", "comments": "9 pages", "journal-ref": "Journal of Software Engineering and Applications, 8, 133-142\n  (2018)", "doi": "10.4236/jsea.2015.83014", "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software development effort estimation is considered a fundamental task for\nsoftware development life cycle as well as for managing project cost, time and\nquality. Therefore, accurate estimation is a substantial factor in projects\nsuccess and reducing the risks. In recent years, software effort estimation has\nreceived a considerable amount of attention from researchers and became a\nchallenge for software industry. In the last two decades, many researchers and\npractitioners proposed statistical and machine learning-based models for\nsoftware effort estimation. In this work, Firefly Algorithm is proposed as a\nmetaheuristic optimization method for optimizing the parameters of three\nCOCOMO-based models. These models include the basic COCOMO model and other two\nmodels proposed in the literature as extensions of the basic COCOMO model. The\ndeveloped estimation models are evaluated using different evaluation metrics.\nExperimental results show high accuracy and significant error minimization of\nFirefly Algorithm over other metaheuristic optimization algorithms including\nGenetic Algorithms and Particle Swarm Optimization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 23:34:43 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Ghatasheh", "Nazeeh", ""], ["Faris", "Hossam", ""], ["Aljarah", "Ibrahim", ""], ["Al-Sayyed", "Rizik M. H.", ""]]}, {"id": "1903.02080", "submitter": "Senthil Yogamani", "authors": "Sambit Mohapatra, Heinrich Gotzig, Senthil Yogamani, Stefan Milz and\n  Raoul Zollner", "title": "Exploring Deep Spiking Neural Networks for Automated Driving\n  Applications", "comments": "Accepted for Oral Presentation at VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become the standard model for various computer vision\ntasks in automated driving including semantic segmentation, moving object\ndetection, depth estimation, visual odometry, etc. The main flavors of neural\nnetworks which are used commonly are convolutional (CNN) and recurrent (RNN).\nIn spite of rapid progress in embedded processors, power consumption and cost\nis still a bottleneck. Spiking Neural Networks (SNNs) are gradually progressing\nto achieve low-power event-driven hardware architecture which has a potential\nfor high efficiency. In this paper, we explore the role of deep spiking neural\nnetworks (SNN) for automated driving applications. We provide an overview of\nprogress on SNN and argue how it can be a good fit for automated driving\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 18:40:42 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Mohapatra", "Sambit", ""], ["Gotzig", "Heinrich", ""], ["Yogamani", "Senthil", ""], ["Milz", "Stefan", ""], ["Zollner", "Raoul", ""]]}, {"id": "1903.02081", "submitter": "Samira Vafay Eslahi", "authors": "Samira Vafay Eslahi, Nader Jafarnia Dabanloo, Keivan Maghooli", "title": "A GA-based feature selection of the EEG signals by classification\n  evaluation: Application in BCI systems", "comments": "12 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In electroencephalogram (EEG) signal processing, finding the appropriate\ninformation from a dataset has been a big challenge for successful signal\nclassification. The feature selection methods make it possible to solve this\nproblem; however, the method selection is still under investigation to find out\nwhich feature can perform the best to extract the most proper features of the\nsignal to improve the classification performance. In this study, we use the\ngenetic algorithm (GA), a heuristic searching algorithm, to find the optimum\ncombination of the feature extraction methods and the classifiers, in the\nbrain-computer interface (BCI) applications. A BCI system can be practical if\nand only if it performs with high accuracy and high speed alongside each other.\nIn the proposed method, GA performs as a searching engine to find the best\ncombination of the features and classifications. The features used here are\nKatz, Higuchi, Petrosian, Sevcik, and box-counting dimension (BCD) feature\nextraction methods. These features are applied to the wavelet subbands and are\nclassified with four classifiers such as adaptive neuro-fuzzy inference system\n(ANFIS), fuzzy k-nearest neighbors (FKNN), support vector machine (SVM) and\nlinear discriminant analysis (LDA). Due to the huge number of features, the GA\noptimization is used to find the features with the optimum fitness value (FV).\nResults reveal that Katz fractal feature estimation method with LDA\nclassification has the best FV. Consequently, due to the low computation time\nof the first Daubechies wavelet transformation in comparison to the original\nsignal, the final selected methods contain the fractal features of the first\ncoefficient of the detail subbands.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 05:31:08 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Eslahi", "Samira Vafay", ""], ["Dabanloo", "Nader Jafarnia", ""], ["Maghooli", "Keivan", ""]]}, {"id": "1903.02082", "submitter": "Yifeng Zhang", "authors": "Yifeng Zhang, Ka-Ho Chow, S.-H. Gary Chan", "title": "DA-LSTM: A Long Short-Term Memory with Depth Adaptive to Non-uniform\n  Information Flow in Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much sequential data exhibits highly non-uniform information distribution.\nThis cannot be correctly modeled by traditional Long Short-Term Memory (LSTM).\nTo address that, recent works have extended LSTM by adding more activations\nbetween adjacent inputs. However, the approaches often use a fixed depth, which\nis at the step of the most information content. This one-size-fits-all\nworst-case approach is not satisfactory, because when little information is\ndistributed to some steps, shallow structures can achieve faster convergence\nand consume less computation resource. In this paper, we develop a\nDepth-Adaptive Long Short-Term Memory (DA-LSTM) architecture, which can\ndynamically adjust the structure depending on information distribution without\nprior knowledge. Experimental results on real-world datasets show that DA-LSTM\ncosts much less computation resource and substantially reduce convergence time\nby $41.78\\%$ and $46.01 \\%$, compared with Stacked LSTM and Deep Transition\nLSTM, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 14:21:12 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhang", "Yifeng", ""], ["Chow", "Ka-Ho", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "1903.02083", "submitter": "Brian Crafton Mr.", "authors": "Brian Crafton, Abhinav Parihar, Evan Gebhardt, Arijit Raychowdhury", "title": "Direct Feedback Alignment with Sparse Connections for Local Learning", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep neural networks (DNNs) owe their success to training\nalgorithms that use backpropagation and gradient-descent. Backpropagation,\nwhile highly effective on von Neumann architectures, becomes inefficient when\nscaling to large networks. Commonly referred to as the weight transport\nproblem, each neuron's dependence on the weights and errors located deeper in\nthe network require exhaustive data movement which presents a key problem in\nenhancing the performance and energy-efficiency of machine-learning hardware.\nIn this work, we propose a bio-plausible alternative to backpropagation drawing\nfrom advances in feedback alignment algorithms in which the error computation\nat a single synapse reduces to the product of three scalar values. Using a\nsparse feedback matrix, we show that a neuron needs only a fraction of the\ninformation previously used by the feedback alignment algorithms. Consequently,\nmemory and compute can be partitioned and distributed whichever way produces\nthe most efficient forward pass so long as a single error can be delivered to\neach neuron. Our results show orders of magnitude improvement in data movement\nand $2\\times$ improvement in multiply-and-accumulate operations over\nbackpropagation. Like previous work, we observe that any variant of feedback\nalignment suffers significant losses in classification accuracy on deep\nconvolutional neural networks. By transferring trained convolutional layers and\ntraining the fully connected layers using direct feedback alignment, we\ndemonstrate that direct feedback alignment can obtain results competitive with\nbackpropagation. Furthermore, we observe that using an extremely sparse\nfeedback matrix, rather than a dense one, results in a small accuracy drop\nwhile yielding hardware advantages. All the code and results are available\nunder https://github.com/bcrafton/ssdfa.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 16:33:08 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 14:13:29 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Crafton", "Brian", ""], ["Parihar", "Abhinav", ""], ["Gebhardt", "Evan", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1903.02130", "submitter": "Oren Segal", "authors": "Philip Colangelo, Oren Segal, Alexander Speicher and Martin Margala", "title": "Evolutionary Cell Aided Design for Neural Network Architectures", "comments": "Text and image edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical theory shows us that multilayer feedforward Artificial Neural\nNetworks(ANNs) are universal function approximators, capable of approximating\nany measurable function to any desired degree of accuracy. In practice\ndesigning practical and efficient neural network architectures require\nsignificant effort and expertise. We present a novel software framework called\nEvolutionary Cell Aided Design(ECAD) meant to aid in the exploration and design\nof efficient Neural Network Architectures(NNAs) for reconfigurable hardware.\nGiven a general neural network structure and a set of constraints and fitness\nfunctions, the framework will explore both the space of possible NNA and the\nspace of possible hardware designs, using evolutionary algorithms, and attempt\nto find the fittest co-design solutions according to a predefined set of goals.\nWe test the framework on an image classification task and use the MNIST data\nset of hand written digits with an Intel Arria 10 GX 1150 device as our target\nplatform. We design and implement a modular and scalable 2D systolic array with\nenhancements for machine learning that can be used by the framework for the\nhardware search space. Our results demonstrate the ability to pair neural\nnetwork design and hardware development together using an evolutionary\nalgorithm and removing traditional human-in-the-loop development tasks. By\nrunning various experiments of the fittest solutions for neural network and\nhardware searches, we demonstrate the full end-to-end capabilities of the ECAD\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 01:04:14 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 16:17:07 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 14:56:39 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Colangelo", "Philip", ""], ["Segal", "Oren", ""], ["Speicher", "Alexander", ""], ["Margala", "Martin", ""]]}, {"id": "1903.02140", "submitter": "Hui Jiang", "authors": "Hui Jiang", "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex\n  Optimization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present some theoretical work to explain why simple\ngradient descent methods are so successful in solving non-convex optimization\nproblems in learning large-scale neural networks (NN). After introducing a\nmathematical tool called canonical space, we have proved that the objective\nfunctions in learning NNs are convex in the canonical model space. We further\nelucidate that the gradients between the original NN model space and the\ncanonical space are related by a pointwise linear transformation, which is\nrepresented by the so-called disparity matrix. Furthermore, we have proved that\ngradient descent methods surely converge to a global minimum of zero loss\nprovided that the disparity matrices maintain full rank. If this full-rank\ncondition holds, the learning of NNs behaves in the same way as normal convex\noptimization. At last, we have shown that the chance to have singular disparity\nmatrices is extremely slim in large NNs. In particular, when over-parameterized\nNNs are randomly initialized, the gradient decent algorithms converge to a\nglobal minimum of zero loss in probability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 02:21:37 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Jiang", "Hui", ""]]}, {"id": "1903.02165", "submitter": "Jon McCormack", "authors": "Dilpreet Singh, Nina Rajcic, Simon Colton and Jon McCormack", "title": "Camera Obscurer: Generative Art for Design Inspiration", "comments": "Accepted for EvoMUSART 2019: 8th International Conference on\n  Computational Intelligence in Music, Sound, Art and Design. April 2019,\n  Leipzig, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate using generated decorative art as a source of inspiration for\ndesign tasks. Using a visual similarity search for image retrieval, the\n\\emph{Camera Obscurer} app enables rapid searching of tens of thousands of\ngenerated abstract images of various types. The seed for a visual similarity\nsearch is a given image, and the retrieved generated images share some visual\nsimilarity with the seed. Implemented in a hand-held device, the app empowers\nusers to use photos of their surroundings to search through the archive of\ngenerated images and other image archives. Being abstract in nature, the\nretrieved images supplement the seed image rather than replace it, providing\ndifferent visual stimuli including shapes, colours, textures and\njuxtapositions, in addition to affording their own interpretations. This\napproach can therefore be used to provide inspiration for a design task, with\nthe abstract images suggesting new ideas that might give direction to a graphic\ndesign project. We describe a crowdsourcing experiment with the app to estimate\nuser confidence in retrieved images, and we describe a pilot study where Camera\nObscurer provided inspiration for a design task. These experiments have enabled\nus to describe future improvements, and to begin to understand sources of\nvisual inspiration for design tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 04:05:47 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Singh", "Dilpreet", ""], ["Rajcic", "Nina", ""], ["Colton", "Simon", ""], ["McCormack", "Jon", ""]]}, {"id": "1903.02166", "submitter": "Jon McCormack", "authors": "Jon McCormack, Toby Gifford, Patrick Hutchings", "title": "Autonomy, Authenticity, Authorship and Intention in computer generated\n  art", "comments": "Accepted for EvoMUSART 2019: 8th International Conference on\n  Computational Intelligence in Music, Sound, Art and Design. April 2019,\n  Leipzig, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines five key questions surrounding computer generated art.\nDriven by the recent public auction of a work of `AI Art' we selectively\nsummarise many decades of research and commentary around topics of autonomy,\nauthenticity, authorship and intention in computer generated art, and use this\nresearch to answer contemporary questions often asked about art made by\ncomputers that concern these topics. We additionally reflect on whether current\ntechniques in deep learning and Generative Adversarial Networks significantly\nchange the answers provided by many decades of prior research.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 04:06:20 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["McCormack", "Jon", ""], ["Gifford", "Toby", ""], ["Hutchings", "Patrick", ""]]}, {"id": "1903.02167", "submitter": "Taimoor Akhtar", "authors": "Taimoor Akhtar, Christine A. Shoemaker", "title": "Efficient Multi-Objective Optimization through Population-based Parallel\n  Surrogate Search", "comments": "Submitted to IEEE Transactions on Evolutionary Computation. This work\n  is supported by the National Research Foundation, Prime Minister's Office,\n  Singapore under its Campus for Research Excellence and Technological\n  Enterprise (CREATE) programme and by the National Science Foundation (NSF)\n  grant CISE 1116298", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Objective Optimization (MOO) is very difficult for expensive functions\nbecause most current MOO methods rely on a large number of function evaluations\nto get an accurate solution. We address this problem with surrogate\napproximation and parallel computation. We develop an MOO algorithm MOPLS-N for\nexpensive functions that combines iteratively updated surrogate approximations\nof the objective functions with a structure for efficiently selecting a\npopulation of $N$ points so that the expensive objectives for all points are\nsimultaneously evaluated on $N$ processors in each iteration. MOPLS\nincorporates Radial Basis Function (RBF) approximation, Tabu Search and local\ncandidate search around multiple points to strike a balance between\nexploration, exploitation and diversification during each algorithm iteration.\nEleven test problems (with 8 to 24 decision variables and two real-world\nwatershed problems are used to compare performance of MOPLS to ParEGO, GOMORS,\nBorg, MOEA/D, and NSGA-III on a limited budget of evaluations with between 1\n(serial) and 64 processors. MOPLS in serial is better than all non-RBF serial\nmethods tested. Parallel speedup of MOPLS is higher than all other parallel\nalgorithms with 16 and 64 processors. With both algorithms on 64 processors\nMOPLS is at least 2 times faster than NSGA-III on the watershed problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 04:13:28 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Akhtar", "Taimoor", ""], ["Shoemaker", "Christine A.", ""]]}, {"id": "1903.02195", "submitter": "Vahid Roostapour", "authors": "Mojgan Pourhassan, Vahid Roostapour, and Frank Neumann", "title": "Runtime Analysis of RLS and (1+1) EA for the Dynamic Weighted Vertex\n  Cover Problem", "comments": "The paper has been accepted for Theoretical Computer Science journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform theoretical analyses on the behaviour of an\nevolutionary algorithm and a randomised search algorithm for the dynamic vertex\ncover problem based on its dual formulation. The dynamic vertex cover problem\nhas already been theoretically investigated to some extent and it has been\nshown that using its dual formulation to represent possible solutions can lead\nto a better approximation behaviour. We improve some of the existing results,\ni.e. we find a linear expected re-optimization time for a (1+1) EA to\nre-discover a 2-approximation when edges are dynamically deleted from the\ngraph. Furthermore, we investigate a different setting for applying the\ndynamism to the problem, in which a dynamic change happens at each step with a\nprobability $P_D$. We also expand these analyses to the weighted vertex cover\nproblem, in which weights are assigned to vertices and the goal is to find a\ncover set with minimum total weight. Similar to the classical case, the dynamic\nchanges that we consider on the weighted vertex cover problem are adding and\nremoving edges to and from the graph. We aim at finding a maximal solution for\nthe dual problem, which gives a 2-approximate solution for the vertex cover\nproblem. This is equivalent to the maximal matching problem for the classical\nvertex cover problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 06:35:06 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Pourhassan", "Mojgan", ""], ["Roostapour", "Vahid", ""], ["Neumann", "Frank", ""]]}, {"id": "1903.02440", "submitter": "Milad Mozafari", "authors": "Milad Mozafari, Mohammad Ganjtabesh, Abbas Nowzari-Dalini, Timoth\\'ee\n  Masquelier", "title": "SpykeTorch: Efficient Simulation of Convolutional Spiking Neural\n  Networks with at most one Spike per Neuron", "comments": null, "journal-ref": null, "doi": "10.3389/fnins.2019.00625", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of deep convolutional spiking neural networks (SNNs) to\nartificial intelligence (AI) tasks has recently gained a lot of interest since\nSNNs are hardware-friendly and energy-efficient. Unlike the non-spiking\ncounterparts, most of the existing SNN simulation frameworks are not\npractically efficient enough for large-scale AI tasks. In this paper, we\nintroduce SpykeTorch, an open-source high-speed simulation framework based on\nPyTorch. This framework simulates convolutional SNNs with at most one spike per\nneuron and the rank-order encoding scheme. In terms of learning rules, both\nspike-timing-dependent plasticity (STDP) and reward-modulated STDP (R-STDP) are\nimplemented, but other rules could be implemented easily. Apart from the\naforementioned properties, SpykeTorch is highly generic and capable of\nreproducing the results of various studies. Computations in the proposed\nframework are tensor-based and totally done by PyTorch functions, which in turn\nbrings the ability of just-in-time optimization for running on CPUs, GPUs, or\nMulti-GPU platforms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 15:15:53 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 15:50:20 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Mozafari", "Milad", ""], ["Ganjtabesh", "Mohammad", ""], ["Nowzari-Dalini", "Abbas", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1903.02504", "submitter": "Konstantinos Michmizos", "authors": "Guangzhi Tang, Arpit Shah, and Konstantinos P. Michmizos", "title": "Spiking Neural Network on Neuromorphic Hardware for Energy-Efficient\n  Unidimensional SLAM", "comments": "6 pages, 7 figures", "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-efficient simultaneous localization and mapping (SLAM) is crucial for\nmobile robots exploring unknown environments. The mammalian brain solves SLAM\nvia a network of specialized neurons, exhibiting asynchronous computations and\nevent-based communications, with very low energy consumption. We propose a\nbrain-inspired spiking neural network (SNN) architecture that solves the\nunidimensional SLAM by introducing spike-based reference frame transformation,\nvisual likelihood computation, and Bayesian inference. We integrated our\nneuromorphic algorithm to Intel's Loihi neuromorphic processor, a non-Von\nNeumann hardware that mimics the brain's computing paradigms. We performed\ncomparative analyses for accuracy and energy-efficiency between our\nneuromorphic approach and the GMapping algorithm, which is widely used in small\nenvironments. Our Loihi-based SNN architecture consumes 100 times less energy\nthan GMapping run on a CPU while having comparable accuracy in head direction\nlocalization and map-generation. These results pave the way for scaling our\napproach towards active-SLAM alternative solutions for Loihi-controlled\nautonomous robots.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:26:38 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 16:26:06 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Tang", "Guangzhi", ""], ["Shah", "Arpit", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "1903.02510", "submitter": "Shouyong Jiang", "authors": "Shouyong Jiang, Marcus Kaiser, Shengxiang Yang, Stefanos Kollias, and\n  Natalio Krasnogor", "title": "A Scalable Test Suite for Continuous Dynamic Multiobjective Optimisation", "comments": "19 pages, 22 figures and 7 tables", "journal-ref": "IEEE Transactions on Cybernetics, 2019", "doi": "10.1109/TCYB.2019.2896021", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic multiobjective optimisation has gained increasing attention in recent\nyears. Test problems are of great importance in order to facilitate the\ndevelopment of advanced algorithms that can handle dynamic environments well.\nHowever, many of existing dynamic multiobjective test problems have not been\nrigorously constructed and analysed, which may induce some unexpected bias when\nthey are used for algorithmic analysis. In this paper, some of these biases are\nidentified after a review of widely used test problems. These include poor\nscalability of objectives and, more importantly, problematic overemphasis of\nstatic properties rather than dynamics making it difficult to draw accurate\nconclusion about the strengths and weaknesses of the algorithms studied. A\ndiverse set of dynamics and features is then highlighted that a good test suite\nshould have. We further develop a scalable continuous test suite, which\nincludes a number of dynamics or features that have been rarely considered in\nliterature but frequently occur in real life. It is demonstrated with empirical\nstudies that the proposed test suite is more challenging to the dynamic\nmultiobjective optimisation algorithms found in the literature. The test suite\ncan also test algorithms in ways that existing test suites can not.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:34:50 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Jiang", "Shouyong", ""], ["Kaiser", "Marcus", ""], ["Yang", "Shengxiang", ""], ["Kollias", "Stefanos", ""], ["Krasnogor", "Natalio", ""]]}, {"id": "1903.02519", "submitter": "Roshan Gopalakrishnan", "authors": "Roshan Gopalakrishnan", "title": "RRAM based neuromorphic algorithms", "comments": "12 pages, 6 figures, A version of this survey report is submitting as\n  Springer book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This submission is a report on RRAM based neuromorphic algorithms. This\nreport basically gives an overview of the algorithms implemented on\nneuromorphic hardware with crossbar array of RRAM synapses. This report mainly\ntalks about the work on deep neural network to spiking neural network\nconversion and its significance.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 14:27:19 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Gopalakrishnan", "Roshan", ""]]}, {"id": "1903.02536", "submitter": "H. Sebastian Seung", "authors": "H. Sebastian Seung", "title": "Convergence of gradient descent-ascent analyzed as a Newtonian dynamical\n  system with dissipation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamical system is defined in terms of the gradient of a payoff function.\nDynamical variables are of two types, ascent and descent. The ascent variables\nmove in the direction of the gradient, while the descent variables move in the\nopposite direction. Dynamical systems of this form or very similar forms have\nbeen studied in diverse fields such as game theory, optimization, neural\nnetworks, and population biology. Gradient descent-ascent is approximated as a\nNewtonian dynamical system that conserves total energy, defined as the sum of\nthe kinetic energy and a potential energy that is proportional to the payoff\nfunction. The error of the approximation is a residual force that violates\nenergy conservation. If the residual force is purely dissipative, then the\nenergy serves as a Lyapunov function, and convergence of bounded trajectories\nto steady states is guaranteed. A previous convergence theorem due to Kose and\nUzawa required the payoff function to be convex in the descent variables, and\nconcave in the ascent variables. Here the assumption is relaxed, so that the\npayoff function need only be globally `less convex' or `more concave' in the\nascent variables than in the descent variables. Such relative convexity\nconditions allow the existence of multiple steady states, unlike the\nconvex-concave assumption. When combined with sufficient conditions that imply\nthe existence of a minimax equilibrium, boundedness of trajectories is also\nassured.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 03:09:20 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Seung", "H. Sebastian", ""]]}, {"id": "1903.02547", "submitter": "Xiujun Li", "authors": "Liyiming Ke and Xiujun Li and Yonatan Bisk and Ari Holtzman and Zhe\n  Gan and Jingjing Liu and Jianfeng Gao and Yejin Choi and Siddhartha Srinivasa", "title": "Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language\n  Navigation", "comments": "CVPR 2019 Oral, video demo: https://youtu.be/AD9TNohXoPA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Frontier Aware Search with backTracking (FAST) Navigator, a\ngeneral framework for action decoding, that achieves state-of-the-art results\non the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson\net. al. (2018). Given a natural language instruction and photo-realistic image\nviews of a previously unseen environment, the agent was tasked with navigating\nfrom source to target location as quickly as possible. While all current\napproaches make local action decisions or score entire trajectories using beam\nsearch, ours balances local and global signals when exploring an unobserved\nenvironment. Importantly, this lets us act greedily but use global signals to\nbacktrack when necessary. Applying FAST framework to existing state-of-the-art\nmodels achieved a 17% relative gain, an absolute 6% gain on Success rate\nweighted by Path Length (SPL).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:54:55 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 17:48:26 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ke", "Liyiming", ""], ["Li", "Xiujun", ""], ["Bisk", "Yonatan", ""], ["Holtzman", "Ari", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""], ["Choi", "Yejin", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1903.02683", "submitter": "Gang Wang", "authors": "Gang Wang", "title": "A Novel Neural Network Structure Constructed according to Logical\n  Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve more complex things, computer systems becomes more and more complex.\nIt becomes harder to be handled manually for various conditions and unknown new\nconditions in advance. This situation urgently requires the development of\ncomputer technology of automatic judgement and decision according to various\nconditions. Current ANN (Artificial Neural Network) models are good at\nperceptual intelligence while they are not good at cognitive intelligence such\nas logical representation, making them not deal with the above situation well.\nTherefore, researchers have tried to design novel models so as to represent and\nstore logical relations into the neural network structures, the type of which\nis called KBNN (Knowledge-Based Neural Network). In this type models, the\nneurons and links are designed specific for logical relation representation,\nand the neural network structures are constructed according to logical\nrelations, allowing us to construct automatically the rule libraries of expert\nsystems. In this paper, the further improvement is made based on KBNN by\nredesigning the neurons and links. This improvement can make neurons solely for\nrepresenting things while making links solely for representing logical\nrelations between things, and thus no extra logical neurons are needed.\nMoreover, the related construction and adjustment methods of the neural network\nstructure are also designed based on the redesigned neurons and links, making\nthe neural network structure dynamically constructed and adjusted according to\nthe logical relations. The probabilistic mechanism for the weight adjustment\ncan make the neural network model further represent logical relations in the\nuncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 01:34:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Gang", ""]]}, {"id": "1903.02809", "submitter": "Linu Pinto", "authors": "Linu Pinto and Dr. Sasi Gopalan", "title": "Limiting Network Size within Finite Bounds for Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Largest theoretical contribution to Neural Networks comes from VC Dimension\nwhich characterizes the sample complexity of classification model in a\nprobabilistic view and are widely used to study the generalization error. So\nfar in the literature the VC Dimension has only been used to approximate the\ngeneralization error bounds on different Neural Network architectures. VC\nDimension has not yet been implicitly or explicitly stated to fix the network\nsize which is important as the wrong configuration could lead to high\ncomputation effort in training and leads to over fitting. So there is a need to\nbound these units so that task can be computed with only sufficient number of\nparameters. For binary classification tasks shallow networks are used as they\nhave universal approximation property and it is enough to size the hidden layer\nwidth for such networks. The paper brings out a theoretical justification on\nrequired attribute size and its corresponding hidden layer dimension for a\ngiven sample set that gives an optimal binary classification results with\nminimum training complexity in a single layered feed forward network framework.\nThe paper also establishes proof on the existence of bounds on the width of the\nhidden layer and its range subjected to certain conditions. Findings in this\npaper are experimentally analyzed on three different dataset using Mathlab 2018\n(b) software.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 10:15:21 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Pinto", "Linu", ""], ["Gopalan", "Dr. Sasi", ""]]}, {"id": "1903.02915", "submitter": "Javier Del Ser Dr.", "authors": "Antonio Benitez-Hidalgo, Antonio J. Nebro, Jose Garcia-Nieto, Izaskun\n  Oregi and Javier Del Ser", "title": "jMetalPy: a Python Framework for Multi-Objective Optimization with\n  Metaheuristics", "comments": "13 pages, under review in Swarm and Evolutionary Computation journal.\n  Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes jMetalPy, an object-oriented Python-based framework for\nmulti-objective optimization with metaheuristic techniques. Building upon our\nexperiences with the well-known jMetal framework, we have developed a new\nmulti-objective optimization software platform aiming not only at replicating\nthe former one in a different programming language, but also at taking\nadvantage of the full feature set of Python, including its facilities for fast\nprototyping and the large amount of available libraries for data processing,\ndata analysis, data visualization, and high-performance computing. As a result,\njMetalPy provides an environment for solving multi-objective optimization\nproblems focused not only on traditional metaheuristics, but also on techniques\nsupporting preference articulation and dynamic problems, along with a rich set\nof features related to the automatic generation of statistical data from the\nresults generated, as well as the real-time and interactive visualization of\nthe Pareto front approximations produced by the algorithms. jMetalPy offers\nadditionally support for parallel computing in multicore and cluster systems.\nWe include some use cases to explore the main features of jMetalPy and to\nillustrate how to work with it.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 14:00:20 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 08:54:27 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Benitez-Hidalgo", "Antonio", ""], ["Nebro", "Antonio J.", ""], ["Garcia-Nieto", "Jose", ""], ["Oregi", "Izaskun", ""], ["Del Ser", "Javier", ""]]}, {"id": "1903.02974", "submitter": "Richard Droste", "authors": "Richard Droste, Yifan Cai, Harshita Sharma, Pierre Chatelain, Lior\n  Drukker, Aris T. Papageorghiou, J. Alison Noble", "title": "Ultrasound Image Representation Learning by Modeling Sonographer Visual\n  Attention", "comments": "Accepted at the international conference on Information Processing in\n  Medical Imaging (IPMI) 2019", "journal-ref": null, "doi": "10.1007/978-3-030-20351-1_46", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representations are commonly learned from class labels, which are a\nsimplistic approximation of human image understanding. In this paper we\ndemonstrate that transferable representations of images can be learned without\nmanual annotations by modeling human visual attention. The basis of our\nanalyses is a unique gaze tracking dataset of sonographers performing routine\nclinical fetal anomaly screenings. Models of sonographer visual attention are\nlearned by training a convolutional neural network (CNN) to predict gaze on\nultrasound video frames through visual saliency prediction or gaze-point\nregression. We evaluate the transferability of the learned representations to\nthe task of ultrasound standard plane detection in two contexts. Firstly, we\nperform transfer learning by fine-tuning the CNN with a limited number of\nlabeled standard plane images. We find that fine-tuning the saliency predictor\nis superior to training from random initialization, with an average F1-score\nimprovement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we\ntrain a simple softmax regression on the feature activations of each CNN layer\nin order to evaluate the representations independently of transfer learning\nhyper-parameters. We find that the attention models derive strong\nrepresentations, approaching the precision of a fully-supervised baseline model\nfor all but the last layer.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:05:31 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 11:55:31 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Droste", "Richard", ""], ["Cai", "Yifan", ""], ["Sharma", "Harshita", ""], ["Chatelain", "Pierre", ""], ["Drukker", "Lior", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1903.02988", "submitter": "Florian Strub", "authors": "Marie-Agathe Charpagne and Florian Strub and Tresa M. Pollock", "title": "Accurate reconstruction of EBSD datasets by a multimodal data approach\n  using an evolutionary algorithm", "comments": "A short version of this paper exists towards people working in\n  Machine Learning, namely arxiv:1903.02982", "journal-ref": "Materials Characterization, 2019", "doi": "10.1016/j.matchar.2019.01.033", "report-no": null, "categories": "physics.data-an cs.NE physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method has been developed for the correction of the distortions and/or\nenhanced phase differentiation in Electron Backscatter Diffraction (EBSD) data.\nUsing a multi-modal data approach, the method uses segmented images of the\nphase of interest (laths, precipitates, voids, inclusions) on images gathered\nby backscattered or secondary electrons of the same area as the EBSD map. The\nproposed approach then search for the best transformation to correct their\nrelative distortions and recombines the data in a new EBSD file. Speckles of\nthe features of interest are first segmented in both the EBSD and image data\nmodes. The speckle extracted from the EBSD data is then meshed, and the\nCovariance Matrix Adaptation Evolution Strategy (CMA-ES) is implemented to\ndistort the mesh until the speckles superimpose. The quality of the matching is\nquantified via a score that is linked to the number of overlapping pixels in\nthe speckles. The locations of the points of the distorted mesh are compared to\nthose of the initial positions to create pairs of matching points that are used\nto calculate the polynomial function that describes the distortion the best.\nThis function is then applied to un-distort the EBSD data, and the phase\ninformation is inferred using the data of the segmented speckle. Fast and\nversatile, this method does not require any human annotation and can be applied\nto large datasets and wide areas. Besides, this method requires very few\nassumptions concerning the shape of the distortion function. It can be used for\nthe single compensation of the distortions or combined with the phase\ndifferentiation. The accuracy of this method is of the order of the pixel size.\nSome application examples in multiphase materials with feature sizes down to 1\n$\\mu$m are presented, including Ti-6Al-4V Titanium alloy, Rene 65 and additive\nmanufactured Inconel 718 Nickel-base superalloys.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:26:28 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 14:24:17 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Charpagne", "Marie-Agathe", ""], ["Strub", "Florian", ""], ["Pollock", "Tresa M.", ""]]}, {"id": "1903.03494", "submitter": "Shalin Shah", "authors": "Shalin Shah", "title": "Genetic Algorithm for a class of Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 0/1 knapsack problem is weakly NP-hard in that there exist\npseudo-polynomial time algorithms based on dynamic programming that can solve\nit exactly. There are also the core branch and bound algorithms that can solve\nlarge randomly generated instances in a very short amount of time. However, as\nthe correlation between the variables is increased, the difficulty of the\nproblem increases. Recently a new class of knapsack problems was introduced by\nD. Pisinger called the spanner knapsack instances. These instances are\nunsolvable by the core branch and bound instances; and as the size of the\ncoefficients and the capacity constraint increase, the spanner instances are\nunsolvable even by dynamic programming based algorithms. In this paper, a\ngenetic algorithm is presented for spanner knapsack instances. Results show\nthat the algorithm is capable of delivering optimum solutions within a\nreasonable amount of computational duration.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 20:48:41 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Shah", "Shalin", ""]]}, {"id": "1903.03511", "submitter": "Zhenfeng Cao", "authors": "Zhenfeng Cao", "title": "Realizing Continual Learning through Modeling a Learning System as a\n  Fiber Bundle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A human brain is capable of continual learning by nature; however the current\nmainstream deep neural networks suffer from a phenomenon named catastrophic\nforgetting (i.e., learning a new set of patterns suddenly and completely would\nresult in fully forgetting what has already been learned). In this paper we\npropose a generic learning model, which regards a learning system as a fiber\nbundle. By comparing the learning performance of our model with conventional\nones whose neural networks are multilayer perceptrons through a variety of\nmachine-learning experiments, we found our proposed model not only enjoys a\ndistinguished capability of continual learning but also bears a high\ninformation capacity. In addition, we found in some learning scenarios the\nlearning performance can be further enhanced by making the learning time-aware\nto mimic the episodic memory in human brain. Last but not least, we found that\nthe properties of forgetting in our model correspond well to those of human\nmemory. This work may shed light on how a human brain learns.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 01:14:19 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Cao", "Zhenfeng", ""]]}, {"id": "1903.03523", "submitter": "Jos\\'e Esgario", "authors": "Jose G. M. Esgario, Iago E. da Silva and Renato A. Krohling", "title": "Application of Genetic Algorithms to the Multiple Team Formation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Allocating of people in multiple projects is an important issue considering\nthe efficiency of groups from the point of view of social interaction. In this\npaper, based on previous works, the Multiple Team Formation Problem (MTFP)\nbased on sociometric techniques is formulated as an optimization problem taking\ninto account the social interaction among team members. To solve the resulting\noptimization problem we propose a Genetic Algorithm due to the NP-hard nature\nof the problem. The social cohesion is an important issue that directly impacts\nthe productivity of the work environment. So, maintaining an appropriate level\nof cohesion keeps a group together, which will bring positive impacts on the\nresults of a project. The aim of the proposal is to ensure the best possible\neffectiveness from the point of view of social interaction. In this way, the\npresented algorithm serves as a decision-making tool for managers to build\nteams of people in multiple projects. In order to analyze the performance of\nthe proposed method, computational experiments with benchmarks were performed\nand compared with the exhaustive method. The results are promising and show\nthat the algorithm generally obtains near-optimal results within a short\ncomputational time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:56:53 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Esgario", "Jose G. M.", ""], ["da Silva", "Iago E.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1903.03536", "submitter": "Martin Wistuba", "authors": "Martin Wistuba, Tejaswini Pedapati", "title": "Inductive Transfer for Neural Architecture Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advent of automated neural network architecture search led to\nseveral methods that outperform state-of-the-art human-designed architectures.\nHowever, these approaches are computationally expensive, in extreme cases\nconsuming GPU years. We propose two novel methods which aim to expedite this\noptimization problem by transferring knowledge acquired from previous tasks to\nnew ones. First, we propose a novel neural architecture selection method which\nemploys this knowledge to identify strong and weak characteristics of neural\narchitectures across datasets. Thus, these characteristics do not need to be\nrediscovered in every search, a strong weakness of current state-of-the-art\nsearches. Second, we propose a method for learning curve extrapolation to\ndetermine if a training process can be terminated early. In contrast to\nexisting work, we propose to learn from learning curves of architectures\ntrained on other datasets to improve the prediction accuracy for novel\ndatasets. On five different image classification benchmarks, we empirically\ndemonstrate that both of our orthogonal contributions independently lead to an\nacceleration, without any significant loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:27:32 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Wistuba", "Martin", ""], ["Pedapati", "Tejaswini", ""]]}, {"id": "1903.03759", "submitter": "Zheqi Zhu", "authors": "Zheqi Zhu, Pingyi Fan", "title": "Machine Learning Based Prediction and Classification of Computational\n  Jobs in Cloud Computing Centers", "comments": null, "journal-ref": null, "doi": "10.1109/IWCMC.2019.8766558", "report-no": null, "categories": "cs.LG cs.IT cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of the data volume and the fast increasing of the\ncomputational model complexity in the scenario of cloud computing, it becomes\nan important topic that how to handle users' requests by scheduling\ncomputational jobs and assigning the resources in data center.\n  In order to have a better perception of the computing jobs and their requests\nof resources, we analyze its characteristics and focus on the prediction and\nclassification of the computing jobs with some machine learning approaches.\nSpecifically, we apply LSTM neural network to predict the arrival of the jobs\nand the aggregated requests for computing resources. Then we evaluate it on\nGoogle Cluster dataset and it shows that the accuracy has been improved\ncompared to the current existing methods. Additionally, to have a better\nunderstanding of the computing jobs, we use an unsupervised hierarchical\nclustering algorithm, BIRCH, to make classification and get some\ninterpretability of our results in the computing centers.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 08:02:18 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhu", "Zheqi", ""], ["Fan", "Pingyi", ""]]}, {"id": "1903.03854", "submitter": "Esteban Ricalde", "authors": "Esteban Ricalde", "title": "A Genetic Programming System with an Epigenetic Mechanism for Traffic\n  Signal Control", "comments": "PhD thesis, Computer Science Department, Memorial University of\n  Newfoundland. Defended on February 11th, 2019. 183 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic congestion is an increasing problem in most cities around the world.\nIt impacts businesses as well as commuters, small cities and large ones in\ndeveloping as well as developed economies. One approach to decrease urban\ntraffic congestion is to optimize the traffic signal behaviour in order to be\nadaptive to changes in the traffic conditions. From the perspective of\nintelligent transportation systems, this optimization problem is called the\ntraffic signal control problem and is considered a large combinatorial problem\nwith high complexity and uncertainty. A novel approach to the traffic signal\ncontrol problem is proposed in this thesis. The approach includes a new\nmechanism for Genetic Programming inspired by Epigenetics. Epigenetic\nmechanisms play an important role in biological processes such as phenotype\ndifferentiation, memory consolidation within generations and environmentally\ninduced epigenetic modification of behaviour. These properties lead us to\nconsider the implementation of epigenetic mechanisms as a way to improve the\nperformance of Evolutionary Algorithms in solution to real-world problems with\ndynamic environmental changes, such as the traffic control signal problem. The\nepigenetic mechanism proposed was evaluated in four traffic scenarios with\ndifferent properties and traffic conditions using two microscopic simulators.\nThe results of these experiments indicate that Genetic Programming was able to\ngenerate competitive actuated traffic signal controllers for all the scenarios\ntested. Furthermore, the use of the epigenetic mechanism improved the\nperformance of Genetic Programming in all the scenarios. The evolved\ncontrollers adapt to modifications in the traffic density and require less\nmonitoring and less human interaction than other solutions because they\ndynamically adjust the signal behaviour depending on the local traffic\nconditions at each intersection.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 19:21:26 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ricalde", "Esteban", ""]]}, {"id": "1903.03882", "submitter": "Vaneet Aggarwal", "authors": "Abubakr Alabbasi and Arnob Ghosh and Vaneet Aggarwal", "title": "DeepPool: Distributed Model-free Algorithm for Ride-sharing using Deep\n  Reinforcement Learning", "comments": null, "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, vol. 20,\n  no. 12, pp. 4714-4727, Dec. 2019", "doi": "10.1109/TITS.2019.2931830", "report-no": null, "categories": "cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of modern ride-sharing platforms crucially depends on the profit\nof the ride-sharing fleet operating companies, and how efficiently the\nresources are managed. Further, ride-sharing allows sharing costs and, hence,\nreduces the congestion and emission by making better use of vehicle capacities.\nIn this work, we develop a distributed model-free, DeepPool, that uses deep\nQ-network (DQN) techniques to learn optimal dispatch policies by interacting\nwith the environment. Further, DeepPool efficiently incorporates travel demand\nstatistics and deep learning models to manage dispatching vehicles for improved\nride sharing services. Using real-world dataset of taxi trip records in New\nYork City, DeepPool performs better than other strategies, proposed in the\nliterature, that do not consider ride sharing or do not dispatch the vehicles\nto regions where the future demand is anticipated. Finally, DeepPool can adapt\nrapidly to dynamic environments since it is implemented in a distributed manner\nin which each vehicle solves its own DQN individually without coordination.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 22:30:17 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Alabbasi", "Abubakr", ""], ["Ghosh", "Arnob", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1903.03893", "submitter": "Bin Wang", "authors": "Bin Wang, Yanan Sun, Bing Xue, Mengjie Zhang", "title": "A Hybrid GA-PSO Method for Evolving Architecture and Short Connections\n  of Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is a difficult machine learning task, where\nConvolutional Neural Networks (CNNs) have been applied for over 20 years in\norder to solve the problem. In recent years, instead of the traditional way of\nonly connecting the current layer with its next layer, shortcut connections\nhave been proposed to connect the current layer with its forward layers apart\nfrom its next layer, which has been proved to be able to facilitate the\ntraining process of deep CNNs. However, there are various ways to build the\nshortcut connections, it is hard to manually design the best shortcut\nconnections when solving a particular problem, especially given the design of\nthe network architecture is already very challenging.\n  In this paper, a hybrid evolutionary computation (EC) method is proposed to\n\\textit{automatically} evolve both the architecture of deep CNNs and the\nshortcut connections. Three major contributions of this work are: Firstly, a\nnew encoding strategy is proposed to encode a CNN, where the architecture and\nthe shortcut connections are encoded separately; Secondly, a hybrid two-level\nEC method, which combines particle swarm optimisation and genetic algorithms,\nis developed to search for the optimal CNNs; Lastly, an adjustable learning\nrate is introduced for the fitness evaluations, which provides a better\nlearning rate for the training process given a fixed number of epochs. The\nproposed algorithm is evaluated on three widely used benchmark datasets of\nimage classification and compared with 12 peer Non-EC based competitors and one\nEC based competitor. The experimental results demonstrate that the proposed\nmethod outperforms all of the peer competitors in terms of classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 00:51:19 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1903.04311", "submitter": "Clement Romac", "authors": "Cl\\'ement Romac, Vincent B\\'eraud", "title": "Deep Recurrent Q-Learning vs Deep Q-Learning on a simple Partially\n  Observable Markov Decision Process with Minecraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Q-Learning has been successfully applied to a wide variety of tasks in\nthe past several years. However, the architecture of the vanilla Deep Q-Network\nis not suited to deal with partially observable environments such as 3D video\ngames. For this, recurrent layers have been added to the Deep Q-Network in\norder to allow it to handle past dependencies. We here use Minecraft for its\ncustomization advantages and design two very simple missions that can be frames\nas Partially Observable Markov Decision Process. We compare on these missions\nthe Deep Q-Network and the Deep Recurrent Q-Network in order to see if the\nlatter, which is trickier and longer to train, is always the best architecture\nwhen the agent has to deal with partial observability.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 14:11:20 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 07:11:13 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Romac", "Cl\u00e9ment", ""], ["B\u00e9raud", "Vincent", ""]]}, {"id": "1903.04337", "submitter": "Daniel Wesierski", "authors": "Lukasz Czekaj, Wojciech Ziembla, Pawel Jezierski, Pawel Swiniarski,\n  Anna Kolodziejak, Pawel Ogniewski, Pawel Niedbalski, Anna Jezierska, Daniel\n  Wesierski", "title": "Labeler-hot Detection of EEG Epileptic Transients", "comments": "5 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preventing early progression of epilepsy and so the severity of seizures\nrequires an effective diagnosis. Epileptic transients indicate the ability to\ndevelop seizures but humans overlook such brief events in an\nelectroencephalogram (EEG) what compromises patient treatment. Traditionally,\ntraining of the EEG event detection algorithms has relied on ground truth\nlabels, obtained from the consensus of the majority of labelers. In this work,\nwe go beyond labeler consensus on EEG data. Our event descriptor integrates EEG\nsignal features with one-hot encoded labeler category that is a key to improved\ngeneralization performance. Notably, boosted decision trees take advantage of\nsingly-labeled but more varied training sets. Our quantitative experiments show\nthe proposed labeler-hot epileptic event detector consistently outperforms a\nconsensus-trained detector and maintains confidence bounds of the detection.\nThe results on our infant EEG recordings suggest datasets can gain higher event\nvariety faster and thus better performance by shifting available human effort\nfrom consensus-oriented to separate labeling when labels include both, the\nevent and the labeler category.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 14:48:49 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 12:01:55 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 08:59:41 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Czekaj", "Lukasz", ""], ["Ziembla", "Wojciech", ""], ["Jezierski", "Pawel", ""], ["Swiniarski", "Pawel", ""], ["Kolodziejak", "Anna", ""], ["Ogniewski", "Pawel", ""], ["Niedbalski", "Pawel", ""], ["Jezierska", "Anna", ""], ["Wesierski", "Daniel", ""]]}, {"id": "1903.04341", "submitter": "Johannes Christian Thiele", "authors": "Johannes C. Thiele, Olivier Bichler, Antoine Dupret, Sergio Solinas,\n  Giacomo Indiveri", "title": "A Spiking Network for Inference of Relations Trained with Neuromorphic\n  Backpropagation", "comments": "Accepted as a conference paper at IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing need for intelligent sensors in a wide range of everyday\nobjects requires the existence of low power information processing systems\nwhich can operate autonomously in their environment. In particular, merging and\nprocessing the outputs of different sensors efficiently is a necessary\nrequirement for mobile agents with cognitive abilities. In this work, we\npresent a multi-layer spiking neural network for inference of relations between\nstimuli patterns in dedicated neuromorphic systems. The system is trained with\na new version of the backpropagation algorithm adapted to on-chip learning in\nneuromorphic hardware: Error gradients are encoded as spike signals which are\npropagated through symmetric synapses, using the same integrate-and-fire\nhardware infrastructure as used during forward propagation. We demonstrate the\nstrength of the approach on an arithmetic relation inference task and on visual\nXOR on the MNIST dataset. Compared to previous, biologically-inspired\nimplementations of networks for learning and inference of relations, our\napproach is able to achieve better performance with less neurons. Our\narchitecture is the first spiking neural network architecture with on-chip\nlearning capabilities, which is able to perform relational inference on complex\nvisual stimuli. These features make our system interesting for sensor fusion\napplications and embedded learning in autonomous neuromorphic agents.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 14:55:44 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Thiele", "Johannes C.", ""], ["Bichler", "Olivier", ""], ["Dupret", "Antoine", ""], ["Solinas", "Sergio", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1903.04377", "submitter": "Bahareh Pourbabaee", "authors": "Bahareh Pourbabaee, Matthew Howe-Patterson, Matthew Patterson,\n  Frederic Benard", "title": "SleepNet: Automated Sleep Analysis via Dense Convolutional Neural\n  Network Using Physiological Time Series", "comments": "20 pages, 4 figures, Accepted to be published by Physiological\n  Measurement Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a dense recurrent convolutional neural network (DRCNN) was\nconstructed to detect sleep disorders including arousal, apnea and hypopnea\nusing Polysomnography (PSG) measurement channels provided in the 2018 Physionet\nchallenge database. Our model structure is composed of multiple dense\nconvolutional units (DCU) followed by a bidirectional long-short term memory\n(LSTM) layer followed by a softmax output layer. The sleep events including\nsleep stages, arousal regions and multiple types of apnea and hypopnea are\nmanually annotated by experts which enables us to train our proposed network\nusing a multi-task learning mechanism. Three binary cross-entropy loss\nfunctions corresponding to sleep/wake, target arousal and apnea-hypopnea/normal\ndetection tasks are summed up to generate our overall network loss function\nthat is optimized using the Adam method. Our model performance was evaluated\nusing two metrics: the area under the precision-recall curve (AUPRC) and the\narea under the receiver operating characteristic curve (AUROC). To measure our\nmodel generalization, 4-fold cross-validation was also performed. For training,\nour model was applied to full night recording data. Finally, the average AUPRC\nand AUROC values associated with the arousal detection task were 0.505 and\n0.922, respectively on our testing dataset. An ensemble of four models trained\non different data folds improved the AUPRC and AUROC to 0.543 and 0.931,\nrespectively. Our proposed algorithm achieved the first place in the official\nstage of the 2018 Physionet challenge for detecting sleep arousals with AUPRC\nof 0.54 on the blind testing dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 15:41:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 16:17:16 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Pourbabaee", "Bahareh", ""], ["Howe-Patterson", "Matthew", ""], ["Patterson", "Matthew", ""], ["Benard", "Frederic", ""]]}, {"id": "1903.04476", "submitter": "Siavash Golkar", "authors": "Siavash Golkar, Michael Kagan, Kyunghyun Cho", "title": "Continual Learning via Neural Pruning", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed\nat lifelong learning in fixed capacity models based on neuronal model\nsparsification. In this method, subsequent tasks are trained using the inactive\nneurons and filters of the sparsified network and cause zero deterioration to\nthe performance of previous tasks. In order to deal with the possible\ncompromise between model sparsity and performance, we formalize and incorporate\nthe concept of graceful forgetting: the idea that it is preferable to suffer a\nsmall amount of forgetting in a controlled manner if it helps regain network\ncapacity and prevents uncontrolled loss of performance during the training of\nfuture tasks. CLNP also provides simple continual learning diagnostic tools in\nterms of the number of free neurons left for the training of future tasks as\nwell as the number of neurons that are being reused. In particular, we see in\nexperiments that CLNP verifies and automatically takes advantage of the fact\nthat the features of earlier layers are more transferable. We show empirically\nthat CLNP leads to significantly improved results over current weight\nelasticity based methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:53:34 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Golkar", "Siavash", ""], ["Kagan", "Michael", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1903.04579", "submitter": "Ian Williamson", "authors": "Ian A. D. Williamson, Tyler W. Hughes, Momchil Minkov, Ben Bartlett,\n  Sunil Pai, Shanhui Fan", "title": "Reprogrammable Electro-Optic Nonlinear Activation Functions for Optical\n  Neural Networks", "comments": "12 pages, 6 figures", "journal-ref": "IEEE Journal of Selected Topics in Quantum Electronics, vol. 26,\n  no. 1, pp. 1-12, Jan. 2020", "doi": "10.1109/JSTQE.2019.2930455", "report-no": null, "categories": "eess.SP cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an electro-optic hardware platform for nonlinear activation\nfunctions in optical neural networks. The optical-to-optical nonlinearity\noperates by converting a small portion of the input optical signal into an\nanalog electric signal, which is used to intensity-modulate the original\noptical signal with no reduction in processing speed. Our scheme allows for\ncomplete nonlinear on-off contrast in transmission at relatively low optical\npower thresholds and eliminates the requirement of having additional optical\nsources between each layer of the network. Moreover, the activation function is\nreconfigurable via electrical bias, allowing it to be programmed or trained to\nsynthesize a variety of nonlinear responses. Using numerical simulations, we\ndemonstrate that this activation function significantly improves the\nexpressiveness of optical neural networks, allowing them to perform well on two\nbenchmark machine learning tasks: learning a multi-input exclusive-OR (XOR)\nlogic function and classification of images of handwritten numbers from the\nMNIST dataset. The addition of the nonlinear activation function improves test\naccuracy on the MNIST task from 85% to 94%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 04:02:25 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 02:17:08 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Williamson", "Ian A. D.", ""], ["Hughes", "Tyler W.", ""], ["Minkov", "Momchil", ""], ["Bartlett", "Ben", ""], ["Pai", "Sunil", ""], ["Fan", "Shanhui", ""]]}, {"id": "1903.04598", "submitter": "Henrique Lemos", "authors": "Henrique Lemos and Marcelo Prates and Pedro Avelar and Luis Lamb", "title": "Graph Colouring Meets Deep Learning: Effective Graph Neural Network\n  Models for Combinatorial Problems", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has consistently defied state-of-the-art techniques in many\nfields over the last decade. However, we are just beginning to understand the\ncapabilities of neural learning in symbolic domains. Deep learning\narchitectures that employ parameter sharing over graphs can produce models\nwhich can be trained on complex properties of relational data. These include\nhighly relevant NP-Complete problems, such as SAT and TSP. In this work, we\nshowcase how Graph Neural Networks (GNN) can be engineered -- with a very\nsimple architecture -- to solve the fundamental combinatorial problem of graph\ncolouring. Our results show that the model, which achieves high accuracy upon\ntraining on random instances, is able to generalise to graph distributions\ndifferent from those seen at training time. Further, it performs better than\nthe Neurosat, Tabucol and greedy baselines for some distributions. In addition,\nwe show how vertex embeddings can be clustered in multidimensional spaces to\nyield constructive solutions even though our model is only trained as a binary\nclassifier. In summary, our results contribute to shorten the gap in our\nunderstanding of the algorithms learned by GNNs, as well as hoarding empirical\nevidence for their capability on hard combinatorial problems. Our results thus\ncontribute to the standing challenge of integrating robust learning and\nsymbolic reasoning in Deep Learning systems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 20:46:47 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 19:00:53 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lemos", "Henrique", ""], ["Prates", "Marcelo", ""], ["Avelar", "Pedro", ""], ["Lamb", "Luis", ""]]}, {"id": "1903.04659", "submitter": "Ian Colbert", "authors": "Ian Colbert, Ken Kreutz-Delgado and Srinjoy Das", "title": "AX-DBN: An Approximate Computing Framework for the Design of Low-Power\n  Discriminative Deep Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power budget for embedded hardware implementations of Deep Learning\nalgorithms can be extremely tight. To address implementation challenges in such\ndomains, new design paradigms, like Approximate Computing, have drawn\nsignificant attention. Approximate Computing exploits the innate\nerror-resilience of Deep Learning algorithms, a property that makes them\namenable for deployment on low-power computing platforms. This paper describes\nan Approximate Computing design methodology, AX-DBN, for an architecture\nbelonging to the class of stochastic Deep Learning algorithms known as Deep\nBelief Networks (DBNs). Specifically, we consider procedures for efficiently\nimplementing the Discriminative Deep Belief Network (DDBN), a stochastic neural\nnetwork which is used for classification tasks, extending Approximation\nComputing from the analysis of deterministic to stochastic neural networks. For\nthe purpose of optimizing the DDBN for hardware implementations, we explore the\nuse of: (a)Limited precision of neurons and functional approximations of\nactivation functions; (b) Criticality analysis to identify nodes in the network\nwhich can operate at reduced precision while allowing the network to maintain\ntarget accuracy levels; and (c) A greedy search methodology with incremental\nretraining to determine the optimal reduction in precision for all neurons to\nmaximize power savings. Using the AX-DBN methodology proposed in this paper, we\npresent experimental results across several network architectures that show\nsignificant power savings under a user-specified accuracy loss constraint with\nrespect to ideal full precision implementations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 23:46:56 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 05:11:49 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Colbert", "Ian", ""], ["Kreutz-Delgado", "Ken", ""], ["Das", "Srinjoy", ""]]}, {"id": "1903.04671", "submitter": "Daniel Selsam", "authors": "Daniel Selsam, Nikolaj Bj{\\o}rner", "title": "Guiding High-Performance SAT Solvers with Unsat-Core Predictions", "comments": null, "journal-ref": "International Conference on Theory and Applications of\n  Satisfiability Testing. Springer, Cham, 2019", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NeuroSAT neural network architecture was recently introduced for\npredicting properties of propositional formulae. When trained to predict the\nsatisfiability of toy problems, it was shown to find solutions and\nunsatisfiable cores on its own. However, the authors saw \"no obvious path\" to\nusing the architecture to improve the state-of-the-art. In this work, we train\na simplified NeuroSAT architecture to directly predict the unsatisfiable cores\nof real problems. We modify several high-performance SAT solvers to\nperiodically replace their variable activity scores with NeuroSAT's prediction\nof how likely the variables are to appear in an unsatisfiable core. The\nmodified MiniSat solves 10% more problems on SAT-COMP 2018 within the standard\n5,000 second timeout than the original does. The modified Glucose solves 11%\nmore problems than the original, while the modified Z3 solves 6% more. The\ngains are even greater when the training is specialized for a specific\ndistribution of problems; on a benchmark of hard problems from a scheduling\ndomain, the modified Glucose solves 20% more problems than the original does\nwithin a one-hour timeout. Our results demonstrate that NeuroSAT can provide\neffective guidance to high-performance SAT solvers on real problems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 00:15:46 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 01:38:57 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 16:33:14 GMT"}, {"version": "v4", "created": "Tue, 19 Mar 2019 15:31:35 GMT"}, {"version": "v5", "created": "Thu, 4 Apr 2019 00:58:17 GMT"}, {"version": "v6", "created": "Sat, 13 Apr 2019 19:31:09 GMT"}, {"version": "v7", "created": "Fri, 19 Jul 2019 22:36:02 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Selsam", "Daniel", ""], ["Bj\u00f8rner", "Nikolaj", ""]]}, {"id": "1903.04711", "submitter": "Wentao Zhu", "authors": "Wentao Zhu", "title": "Deep Learning for Automated Medical Image Analysis", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical imaging is an essential tool in many areas of medical applications,\nused for both diagnosis and treatment. However, reading medical images and\nmaking diagnosis or treatment recommendations require specially trained medical\nspecialists. The current practice of reading medical images is labor-intensive,\ntime-consuming, costly, and error-prone. It would be more desirable to have a\ncomputer-aided system that can automatically make diagnosis and treatment\nrecommendations. Recent advances in deep learning enable us to rethink the ways\nof clinician diagnosis based on medical images. In this thesis, we will\nintroduce 1) mammograms for detecting breast cancers, the most frequently\ndiagnosed solid cancer for U.S. women, 2) lung CT images for detecting lung\ncancers, the most frequently diagnosed malignant cancer, and 3) head and neck\nCT images for automated delineation of organs at risk in radiotherapy. First,\nwe will show how to employ the adversarial concept to generate the hard\nexamples improving mammogram mass segmentation. Second, we will demonstrate how\nto use the weakly labeled data for the mammogram breast cancer diagnosis by\nefficiently design deep learning for multi-instance learning. Third, the thesis\nwill walk through DeepLung system which combines deep 3D ConvNets and GBM for\nautomated lung nodule detection and classification. Fourth, we will show how to\nuse weakly labeled data to improve existing lung nodule detection system by\nintegrating deep learning with a probabilistic graphic model. Lastly, we will\ndemonstrate the AnatomyNet which is thousands of times faster and more accurate\nthan previous methods on automated anatomy segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 03:28:37 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Zhu", "Wentao", ""]]}, {"id": "1903.05071", "submitter": "Jacob Reinier Maat", "authors": "Jacob Reinier Maat, Nikos Gianniotis, Pavlos Protopapas", "title": "Efficient Optimization of Echo State Networks for Time Series Datasets", "comments": null, "journal-ref": "2018 International Joint Conference on Neural Networks (IJCNN),\n  pp. 1-7. IEEE, 2018", "doi": "10.1109/IJCNN.2018.8489094", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo State Networks (ESNs) are recurrent neural networks that only train\ntheir output layer, thereby precluding the need to backpropagate gradients\nthrough time, which leads to significant computational gains. Nevertheless, a\ncommon issue in ESNs is determining its hyperparameters, which are crucial in\ninstantiating a well performing reservoir, but are often set manually or using\nheuristics. In this work we optimize the ESN hyperparameters using Bayesian\noptimization which, given a limited budget of function evaluations, outperforms\na grid search strategy. In the context of large volumes of time series data,\nsuch as light curves in the field of astronomy, we can further reduce the\noptimization cost of ESNs. In particular, we wish to avoid tuning\nhyperparameters per individual time series as this is costly; instead, we want\nto find ESNs with hyperparameters that perform well not just on individual time\nseries but rather on groups of similar time series without sacrificing\npredictive performance significantly. This naturally leads to a notion of\nclusters, where each cluster is represented by an ESN tuned to model a group of\ntime series of similar temporal behavior. We demonstrate this approach both on\nsynthetic datasets and real world light curves from the MACHO survey. We show\nthat our approach results in a significant reduction in the number of ESN\nmodels required to model a whole dataset, while retaining predictive\nperformance for the series in each cluster.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 17:27:19 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Maat", "Jacob Reinier", ""], ["Gianniotis", "Nikos", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1903.05174", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio and Alessio Micheli", "title": "Richness of Deep Echo State Network Dynamics", "comments": "Preprint of the paper accepted at IWANN 2019", "journal-ref": null, "doi": "10.1007/978-3-030-20521-8_40", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Computing (RC) is a popular methodology for the efficient design of\nRecurrent Neural Networks (RNNs). Recently, the advantages of the RC approach\nhave been extended to the context of multi-layered RNNs, with the introduction\nof the Deep Echo State Network (DeepESN) model. In this paper, we study the\nquality of state dynamics in progressively higher layers of DeepESNs, using\ntools from the areas of information theory and numerical analysis. Our\nexperimental results on RC benchmark datasets reveal the fundamental role\nplayed by the strength of inter-reservoir connections to increasingly enrich\nthe representations developed in higher layers. Our analysis also gives\ninteresting insights into the possibility of effective exploitation of training\nalgorithms based on stochastic gradient descent in the RC field.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 19:39:36 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 15:49:55 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""]]}, {"id": "1903.05537", "submitter": "Yan Jin", "authors": "Yan Jin, John H. Drake, Una Benlic, Kun He", "title": "Effective reinforcement learning based local search for the maximum\n  k-plex problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum k-plex problem is a computationally complex problem, which\nemerged from graph-theoretic social network studies. This paper presents an\neffective hybrid local search for solving the maximum k-plex problem that\ncombines the recently proposed breakout local search algorithm with a\nreinforcement learning strategy. The proposed approach includes distinguishing\nfeatures such as: a unified neighborhood search based on the swapping operator,\na distance-and-quality reward for actions and a new parameter control mechanism\nbased on reinforcement learning. Extensive experiments for the maximum k-plex\nproblem (k = 2, 3, 4, 5) on 80 benchmark instances from the second DIMACS\nChallenge demonstrate that the proposed approach can match the best-known\nresults from the literature in all but four problem instances. In addition, the\nproposed algorithm is able to find 32 new best solutions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 15:11:35 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Jin", "Yan", ""], ["Drake", "John H.", ""], ["Benlic", "Una", ""], ["He", "Kun", ""]]}, {"id": "1903.06070", "submitter": "Soheil Kolouri", "authors": "Soheil Kolouri, Nicholas Ketz, Xinyun Zou, Jeffrey Krichmar, Praveen\n  Pilly", "title": "Attention-Based Structural-Plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting/interference is a critical problem for lifelong\nlearning machines, which impedes the agents from maintaining their previously\nlearned knowledge while learning new tasks. Neural networks, in particular,\nsuffer plenty from the catastrophic forgetting phenomenon. Recently there has\nbeen several efforts towards overcoming catastrophic forgetting in neural\nnetworks. Here, we propose a biologically inspired method toward overcoming\ncatastrophic forgetting. Specifically, we define an attention-based selective\nplasticity of synapses based on the cholinergic neuromodulatory system in the\nbrain. We define synaptic importance parameters in addition to synaptic weights\nand then use Hebbian learning in parallel with backpropagation algorithm to\nlearn synaptic importances in an online and seamless manner. We test our\nproposed method on benchmark tasks including the Permuted MNIST and the Split\nMNIST problems and show competitive performance compared to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 22:23:35 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Kolouri", "Soheil", ""], ["Ketz", "Nicholas", ""], ["Zou", "Xinyun", ""], ["Krichmar", "Jeffrey", ""], ["Pilly", "Praveen", ""]]}, {"id": "1903.06127", "submitter": "Mahesh Patil", "authors": "Mahesh B. Patil, M. Naveen Naidu, A. Vasan, Murari R. R. Varma", "title": "Water Distribution System Design Using Multi-Objective Particle Swarm\n  Optimisation", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of the multi-objective particle swarm optimisation (MOPSO)\nalgorithm to design of water distribution systems is described. An earlier\nMOPSO algorithm is augmented with (a) local search, (b) a modified strategy for\nassigning the leader, and (c) a modified mutation scheme. For one of the\nbenchmark problems described in the literature, the effect of each of the above\nfeatures on the algorithm performance is demonstrated. The augmented MOPSO\nalgorithm (called MOPSO+) is applied to five benchmark problems, and in each\ncase, it finds non-dominated solutions not reported earlier. In addition, for\nthe purpose of comparing Pareto fronts (sets of non-dominated solutions)\nobtained by different algorithms, a new criterion is suggested, and its\nusefulness is pointed out with an example. Finally, some suggestions regarding\nfuture research directions are made.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 16:59:43 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Patil", "Mahesh B.", ""], ["Naidu", "M. Naveen", ""], ["Vasan", "A.", ""], ["Varma", "Murari R. R.", ""]]}, {"id": "1903.06379", "submitter": "Syed Shakib Sarwar", "authors": "Chankyu Lee, Syed Shakib Sarwar, Priyadarshini Panda, Gopalakrishnan\n  Srinivasan, Kaushik Roy", "title": "Enabling Spike-based Backpropagation for Training Deep Neural Network\n  Architectures", "comments": "Chankyu Lee and Syed Shakib Sarwar contributed equally to the work", "journal-ref": "Frontiers in Neuroscience, 14 (2020)", "doi": "10.3389/fnins.2020.00119", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have recently emerged as a prominent neural\ncomputing paradigm. However, the typical shallow SNN architectures have limited\ncapacity for expressing complex representations while training deep SNNs using\ninput spikes has not been successful so far. Diverse methods have been proposed\nto get around this issue such as converting off-the-shelf trained deep\nArtificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion\nscheme fails to capture the temporal dynamics of a spiking system. On the other\nhand, it is still a difficult problem to directly train deep SNNs using input\nspike events due to the discontinuous, non-differentiable nature of the spike\ngeneration function. To overcome this problem, we propose an approximate\nderivative method that accounts for the leaky behavior of LIF neurons. This\nmethod enables training deep convolutional SNNs directly (with input spike\nevents) using spike-based backpropagation. Our experiments show the\neffectiveness of the proposed spike-based learning on deep networks (VGG and\nResidual architectures) by achieving the best classification accuracies in\nMNIST, SVHN and CIFAR-10 datasets compared to other SNNs trained with a\nspike-based learning. Moreover, we analyze sparse event-based computations to\ndemonstrate the efficacy of the proposed SNN training method for inference\noperation in the spiking domain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 06:23:29 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 15:41:29 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 03:40:05 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 22:51:42 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Lee", "Chankyu", ""], ["Sarwar", "Syed Shakib", ""], ["Panda", "Priyadarshini", ""], ["Srinivasan", "Gopalakrishnan", ""], ["Roy", "Kaushik", ""]]}, {"id": "1903.06396", "submitter": "Dimo Brockhoff", "authors": "Ouassim Elhara (RANDOPT), Konstantinos Varelas (RANDOPT), Duc Nguyen\n  (HNUE), Tea Tusar (IJS), Dimo Brockhoff (RANDOPT), Nikolaus Hansen (RANDOPT),\n  Anne Auger (RANDOPT)", "title": "COCO: The Large Scale Black-Box Optimization Benchmarking\n  (bbob-largescale) Test Suite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bbob-largescale test suite, containing 24 single-objective functions in\ncontinuous domain, extends the well-known single-objective noiseless bbob test\nsuite, which has been used since 2009 in the BBOB workshop series, to large\ndimension. The core idea is to make the rotational transformations R, Q in\nsearch space that appear in the bbob test suite computationally cheaper while\nretaining some desired properties. This documentation presents an approach that\nreplaces a full rotational transformation with a combination of a\nblock-diagonal matrix and two permutation matrices in order to construct test\nfunctions whose computational and memory costs scale linearly in the dimension\nof the problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 07:47:05 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 15:24:13 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Elhara", "Ouassim", "", "RANDOPT"], ["Varelas", "Konstantinos", "", "RANDOPT"], ["Nguyen", "Duc", "", "HNUE"], ["Tusar", "Tea", "", "IJS"], ["Brockhoff", "Dimo", "", "RANDOPT"], ["Hansen", "Nikolaus", "", "RANDOPT"], ["Auger", "Anne", "", "RANDOPT"]]}, {"id": "1903.06413", "submitter": "Reza Hedayati Majdabadi", "authors": "Reza Hedayati Majdabadi and Saeed Sharifian Khortoomi", "title": "A proposed method to extract maximum possible power in the shortest time\n  on solar PV arrays under partial shadings using metaheuristic algorithms", "comments": "35 pages, 21 figures, 23 tables, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of fossil fuels to produce energy is leading to\nenvironmental problems. Hence, it has led the human society to move towards the\nuse of renewable energies, including solar energy. In recent years, one of the\nmost popular methods to gain energy is using photovoltaic arrays to produce\nsolar energy. Skyscrapers and different weather conditions cause shadings on\nthese PV arrays, which leads to less power generation. Various methods such as\nTCT and Sudoku patterns have been proposed to improve power generation for\npartial shading PV arrays, but these methods have some problems such as not\ngenerating maximum power and being designed for a specific dimension of PV\narrays. Therefore, we proposed a metaheuristic algorithm-based approach to\nextract maximum possible power in the shortest possible time. In this paper,\nfive algorithms which have proper results in most of the searching problems are\nchosen from different groups of metaheuristic algorithms. Also, four different\nstandard shading patterns are used for more realistic analysis. Results show\nthat the proposed method achieves better results in maximum power generation\ncompared to TCT arrangement (18.53%) and Sudoku arrangement (4.93%). Also, the\nresults show that GWO is the fastest metaheuristic algorithm to reach maximum\noutput power in PV arrays under partial shading condition. Thus, the authors\nbelieve that by using metaheuristic algorithms, an efficient, reliable, and\nfast solution is reached to solve partial shading PV arrays problem\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 08:52:18 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Majdabadi", "Reza Hedayati", ""], ["Khortoomi", "Saeed Sharifian", ""]]}, {"id": "1903.06493", "submitter": "Thomas Bohnstingl", "authors": "Thomas Bohnstingl, Franz Scherr, Christian Pehle, Karlheinz Meier,\n  Wolfgang Maass", "title": "Neuromorphic Hardware learns to learn", "comments": null, "journal-ref": "https://www.frontiersin.org/articles/10.3389/fnins.2019.00483/full", "doi": "10.3389/fnins.2019.00483", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameters and learning algorithms for neuromorphic hardware are usually\nchosen by hand. In contrast, the hyperparameters and learning algorithms of\nnetworks of neurons in the brain, which they aim to emulate, have been\noptimized through extensive evolutionary and developmental processes for\nspecific ranges of computing and learning tasks. Occasionally this process has\nbeen emulated through genetic algorithms, but these require themselves\nhand-design of their details and tend to provide a limited range of\nimprovements. We employ instead other powerful gradient-free optimization\ntools, such as cross-entropy methods and evolutionary strategies, in order to\nport the function of biological optimization processes to neuromorphic\nhardware. As an example, we show that this method produces neuromorphic agents\nthat learn very efficiently from rewards. In particular, meta-plasticity, i.e.,\nthe optimization of the learning rule which they use, substantially enhances\nreward-based learning capability of the hardware. In addition, we demonstrate\nfor the first time Learning-to-Learn benefits from such hardware, in\nparticular, the capability to extract abstract knowledge from prior learning\nexperiences that speeds up the learning of new but related tasks.\nLearning-to-Learn is especially suited for accelerated neuromorphic hardware,\nsince it makes it feasible to carry out the required very large number of\nnetwork computations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 12:39:35 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 08:22:21 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Bohnstingl", "Thomas", ""], ["Scherr", "Franz", ""], ["Pehle", "Christian", ""], ["Meier", "Karlheinz", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1903.06496", "submitter": "Valentin Vielzeuf", "authors": "Juan-Manuel P\\'erez-R\\'ua, Valentin Vielzeuf, St\\'ephane Pateux, Moez\n  Baccouche, Fr\\'ed\\'eric Jurie", "title": "MFAS: Multimodal Fusion Architecture Search", "comments": "CVPR 2019, Jun 2019, Long Beach, United States\n  http://cvpr2019.thecvf.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of finding good architectures for multimodal\nclassification problems. We propose a novel and generic search space that spans\na large number of possible fusion architectures. In order to find an optimal\narchitecture for a given dataset in the proposed search space, we leverage an\nefficient sequential model-based exploration approach that is tailored for the\nproblem. We demonstrate the value of posing multimodal fusion as a neural\narchitecture search problem by extensive experimentation on a toy dataset and\ntwo other real multimodal datasets. We discover fusion architectures that\nexhibit state-of-the-art performance for problems with different domain and\ndataset size, including the NTU RGB+D dataset, the largest multi-modal action\nrecognition dataset available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 12:45:13 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["P\u00e9rez-R\u00faa", "Juan-Manuel", ""], ["Vielzeuf", "Valentin", ""], ["Pateux", "St\u00e9phane", ""], ["Baccouche", "Moez", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1903.07107", "submitter": "Souma Chowdhury", "authors": "Amir Behjat and Sharat Chidambaran and Souma Chowdhury", "title": "Adaptive Genomic Evolution of Neural Network Topologies (AGENT) for\n  State-to-Action Mapping in Autonomous Agents", "comments": "Accepted for presentation in (and publication in the proceedings of)\n  the 2019 IEEE International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroevolution is a process of training neural networks (NN) through an\nevolutionary algorithm, usually to serve as a state-to-action mapping model in\ncontrol or reinforcement learning-type problems. This paper builds on the Neuro\nEvolution of Augmented Topologies (NEAT) formalism that allows designing\ntopology and weight evolving NNs. Fundamental advancements are made to the\nneuroevolution process to address premature stagnation and convergence issues,\ncentral among which is the incorporation of automated mechanisms to control the\npopulation diversity and average fitness improvement within the neuroevolution\nprocess. Insights into the performance and efficiency of the new algorithm is\nobtained by evaluating it on three benchmark problems from the Open AI platform\nand an Unmanned Aerial Vehicle (UAV) collision avoidance problem.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 15:34:11 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Behjat", "Amir", ""], ["Chidambaran", "Sharat", ""], ["Chowdhury", "Souma", ""]]}, {"id": "1903.07138", "submitter": "Decebal Constantin Mocanu", "authors": "Joost Pieterse and Decebal Constantin Mocanu", "title": "Evolving and Understanding Sparse Deep Neural Networks using Cosine\n  Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training sparse neural networks with adaptive connectivity is an active\nresearch topic. Such networks require less storage and have lower computational\ncomplexity compared to their dense counterparts. The Sparse Evolutionary\nTraining (SET) procedure uses weights magnitude to evolve efficiently the\ntopology of a sparse network to fit the dataset, while enabling it to have\nquadratically less parameters than its dense counterpart. To this end, we\npropose a novel approach that evolves a sparse network topology based on the\nbehavior of neurons in the network. More exactly, the cosine similarities\nbetween the activations of any two neurons are used to determine which\nconnections are added to or removed from the network. By integrating our\napproach within the SET procedure, we propose 5 new algorithms to train sparse\nneural networks. We argue that our approach has low additional computational\ncomplexity and we draw a parallel to Hebbian learning. Experiments are\nperformed on 8 datasets taken from various domains to demonstrate the general\napplicability of our approach. Even without optimizing hyperparameters for\nspecific datasets, the experiments show that our proposed training algorithms\nusually outperform SET and state-of-the-art dense neural network techniques.\nThe last but not the least, we show that the evolved connectivity patterns of\nthe input neurons reflect their impact on the classification task.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 17:45:10 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Pieterse", "Joost", ""], ["Mocanu", "Decebal Constantin", ""]]}, {"id": "1903.07209", "submitter": "Alexander Wong", "authors": "Alexander Wong, Zhong Qiu Lin, and Brendan Chwyl", "title": "AttoNets: Compact and Efficient Deep Neural Networks for the Edge via\n  Human-Machine Collaborative Design", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have achieved state-of-the-art performance across\na large number of complex tasks, it remains a big challenge to deploy such\nnetworks for practical, on-device edge scenarios such as on mobile devices,\nconsumer devices, drones, and vehicles. In this study, we take a deeper\nexploration into a human-machine collaborative design approach for creating\nhighly efficient deep neural networks through a synergy between principled\nnetwork design prototyping and machine-driven design exploration. The efficacy\nof human-machine collaborative design is demonstrated through the creation of\nAttoNets, a family of highly efficient deep neural networks for on-device edge\ndeep learning. Each AttoNet possesses a human-specified network-level\nmacro-architecture comprising of custom modules with unique machine-designed\nmodule-level macro-architecture and micro-architecture designs, all driven by\nhuman-specified design requirements. Experimental results for the task of\nobject recognition showed that the AttoNets created via human-machine\ncollaborative design has significantly fewer parameters and computational costs\nthan state-of-the-art networks designed for efficiency while achieving\nnoticeably higher accuracy (with the smallest AttoNet achieving ~1.8% higher\naccuracy while requiring ~10x fewer multiply-add operations and parameters than\nMobileNet-V1). Furthermore, the efficacy of the AttoNets is demonstrated for\nthe task of instance-level object segmentation and object detection, where an\nAttoNet-based Mask R-CNN network was constructed with significantly fewer\nparameters and computational costs (~5x fewer multiply-add operations and ~2x\nfewer parameters) than a ResNet-50 based Mask R-CNN network.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 00:16:04 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 16:05:35 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wong", "Alexander", ""], ["Lin", "Zhong Qiu", ""], ["Chwyl", "Brendan", ""]]}, {"id": "1903.07429", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Sex and Coevolution", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1811.04073, arXiv:1808.03471", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been suggested that the fundamental haploid-diploid cycle of\neukaryotic sex exploits a rudimentary form of the Baldwin effect. This paper\nuses the well-known NKCS model to explore the effects of coevolution upon the\nbehaviour of eukaryotes. It is shown how varying fitness landscape size,\nruggedness and connectedness can vary the conditions under which eukaryotic sex\nproves beneficial over asexual reproduction in haploids in a coevolutionary\ncontext. Moreover, eukaryotic sex is shown to be more sensitive to the relative\nrate of evolution exhibited by its partnering species than asexual haploids.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:12:43 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1903.07461", "submitter": "Geoff Nitschke", "authors": "David Jones and Anja Schroeder and Geoff Nitschke", "title": "Evolutionary Deep Learning to Identify Galaxies in the Zone of Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zone of Avoidance makes it difficult for astronomers to catalogue\ngalaxies at low latitudes to our galactic plane due to high star densities and\nextinction. However, having a complete sky map of galaxies is important in a\nnumber of fields of research in astronomy. There are many unclassified sources\nof light in the Zone of Avoidance and it is therefore important that there\nexists an accurate automated system to identify and classify galaxies in this\nregion. This study aims to evaluate the efficiency and accuracy of using an\nevolutionary algorithm to evolve the topology and configuration of\nConvolutional Neural Network (CNNs) to automatically identify galaxies in the\nZone of Avoidance. A supervised learning method is used with data containing\nnear-infrared images. Input image resolution and number of near-infrared\npassbands needed by the evolutionary algorithm is also analyzed while the\naccuracy of the best evolved CNN is compared to other CNN variants.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:53:27 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 07:22:29 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Jones", "David", ""], ["Schroeder", "Anja", ""], ["Nitschke", "Geoff", ""]]}, {"id": "1903.07526", "submitter": "Miguel Ib\\'a\\~nez Berganza", "authors": "Miguel Ib\\'a\\~nez-Berganza, Ambra Amico, Vittorio Loreto", "title": "Subjectivity and complexity of facial attractiveness", "comments": "15 pages, 5 figures. Supplementary information: 26 pages, 13 figures", "journal-ref": "Scientific Reports 9, Article number: 8364 (2019)", "doi": "10.1038/s41598-019-44655-9", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The origin and meaning of facial beauty represent a longstanding puzzle.\nDespite the profuse literature devoted to facial attractiveness, its very\nnature, its determinants and the nature of inter-person differences remain\ncontroversial issues. Here we tackle such questions proposing a novel\nexperimental approach in which human subjects, instead of rating natural faces,\nare allowed to efficiently explore the face-space and 'sculpt' their favorite\nvariation of a reference facial image. The results reveal that different\nsubjects prefer distinguishable regions of the face-space, highlighting the\nessential subjectivity of the phenomenon.The different sculpted facial vectors\nexhibit strong correlations among pairs of facial distances, characterising the\nunderlying universality and complexity of the cognitive processes, and the\nrelative relevance and robustness of the different facial distances.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:08:16 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 12:42:42 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Ib\u00e1\u00f1ez-Berganza", "Miguel", ""], ["Amico", "Ambra", ""], ["Loreto", "Vittorio", ""]]}, {"id": "1903.07533", "submitter": "Konstantinos Michmizos", "authors": "Ioannis E. Polykretis, Vladimir A. Ivanov, Konstantinos P. Michmizos", "title": "Computational Astrocyence: Astrocytes encode inhibitory activity into\n  the frequency and spatial extent of their calcium elevations", "comments": "4 pages, 3 figures, IEEE-EMBS International Conference on Biomedical\n  and Health Informatics (BHI '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.CB cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciphering the complex interactions between neurotransmission and astrocytic\n$Ca^{2+}$ elevations is a target promising a comprehensive understanding of\nbrain function. While the astrocytic response to excitatory synaptic activity\nhas been extensively studied, how inhibitory activity results to intracellular\n$Ca^{2+}$ waves remains elusive. In this study, we developed a compartmental\nastrocytic model that exhibits distinct levels of responsiveness to inhibitory\nactivity. Our model suggested that the astrocytic coverage of inhibitory\nterminals defines the spatial and temporal scale of their $Ca^{2+}$ elevations.\nUnderstanding the interplay between the synaptic pathways and the astrocytic\nresponses will help us identify how astrocytes work independently and\ncooperatively with neurons, in health and disease.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:21:55 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Polykretis", "Ioannis E.", ""], ["Ivanov", "Vladimir A.", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "1903.08011", "submitter": "Alexander Hvatov", "authors": "Michail Maslyaev and Alexander Hvatov and Anna Kalyuzhnaya", "title": "Data-driven PDE discovery with evolutionary approach", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-22750-0_61", "report-no": null, "categories": "cs.NE cs.LG math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data-driven models allow one to define the model structure in cases when\na priori information is not sufficient to build other types of models. The\npossible way to obtain physical interpretation is the data-driven differential\nequation discovery techniques. The existing methods of PDE (partial derivative\nequations) discovery are bound with the sparse regression. However, sparse\nregression is restricting the resulting model form, since the terms for PDE are\ndefined before regression. The evolutionary approach described in the article\nhas a symbolic regression as the background instead and thus has fewer\nrestrictions on the PDE form. The evolutionary method of PDE discovery (EPDE)\nis described and tested on several canonical PDEs. The question of robustness\nis examined on a noised data example.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:16:45 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 09:13:06 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Maslyaev", "Michail", ""], ["Hvatov", "Alexander", ""], ["Kalyuzhnaya", "Anna", ""]]}, {"id": "1903.08072", "submitter": "Samy Blusseau", "authors": "Yunxiang Zhang (CMM, LTCI), Samy Blusseau (CMM), Santiago\n  Velasco-Forero (CMM), Isabelle Bloch (LTCI), Jesus Angulo (CMM)", "title": "Max-plus Operators Applied to Filter Selection and Model Pruning in\n  Neural Networks", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology, Jul 2019,\n  Saarbr{\\\"u}cken, Germany", "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.LG cs.NE stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent advances in morphological neural networks, we propose to\nstudy in more depth how Max-plus operators can be exploited to define\nmorphological units and how they behave when incorporated in layers of\nconventional neural networks. Besides showing that they can be easily\nimplemented with modern machine learning frameworks , we confirm and extend the\nobservation that a Max-plus layer can be used to select important filters and\nreduce redundancy in its previous layer, without incurring performance loss.\nExperimental results demonstrate that the filter selection strategy enabled by\na Max-plus is highly efficient and robust, through which we successfully\nperformed model pruning on different neural network architectures. We also\npoint out that there is a close connection between Maxout networks and our\npruned Max-plus networks by comparing their respective characteristics. The\ncode for reproducing our experiments is available online.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 15:58:43 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 12:51:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Yunxiang", "", "CMM, LTCI"], ["Blusseau", "Samy", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Bloch", "Isabelle", "", "LTCI"], ["Angulo", "Jesus", "", "CMM"]]}, {"id": "1903.08228", "submitter": "Olaf Witkowski", "authors": "Olaf Witkowski, Takashi Ikegami", "title": "How to Make Swarms Open-Ended? Evolving Collective Intelligence Through\n  a Constricted Exploration of Adjacent Possibles", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach of open-ended evolution via the simulation of swarm\ndynamics. In nature, swarms possess remarkable properties, which allow many\norganisms, from swarming bacteria to ants and flocking birds, to form\nhigher-order structures that enhance their behavior as a group. Swarm\nsimulations highlight three important factors to create novelty and diversity:\n(a) communication generates combinatorial cooperative dynamics, (b) concurrency\nallows for separation of timescales, and (c) complexity and size increases push\nthe system towards transitions in innovation. We illustrate these three\ncomponents in a model computing the continuous evolution of a swarm of agents.\nThe results, divided in three distinct applications, show how emergent\nstructures are capable of filtering information through the bottleneck of their\nmemory, to produce meaningful novelty and diversity within their simulated\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:34:49 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Witkowski", "Olaf", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1903.08500", "submitter": "Yexin Yan", "authors": "Yexin Yan, David Kappel, Felix Neumaerker, Johannes Partzsch, Bernhard\n  Vogginger, Sebastian Hoeppner, Steve Furber, Wolfgang Maass, Robert\n  Legenstein, Christian Mayr", "title": "Efficient Reward-Based Structural Plasticity on a SpiNNaker 2 Prototype", "comments": "accepted by IEEE TBioCAS", "journal-ref": null, "doi": "10.1109/TBCAS.2019.2906401", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in neuroscience uncover the mechanisms employed by the brain to\nefficiently solve complex learning tasks with very limited resources. However,\nthe efficiency is often lost when one tries to port these findings to a silicon\nsubstrate, since brain-inspired algorithms often make extensive use of complex\nfunctions such as random number generators, that are expensive to compute on\nstandard general purpose hardware. The prototype chip of the 2nd generation\nSpiNNaker system is designed to overcome this problem. Low-power ARM processors\nequipped with a random number generator and an exponential function accelerator\nenable the efficient execution of brain-inspired algorithms. We implement the\nrecently introduced reward-based synaptic sampling model that employs\nstructural plasticity to learn a function or task. The numerical simulation of\nthe model requires to update the synapse variables in each time step including\nan explorative random term. To the best of our knowledge, this is the most\ncomplex synapse model implemented so far on the SpiNNaker system. By making\nefficient use of the hardware accelerators and numerical optimizations the\ncomputation time of one plasticity update is reduced by a factor of 2. This,\ncombined with fitting the model into to the local SRAM, leads to 62% energy\nreduction compared to the case without accelerators and the use of external\nDRAM. The model implementation is integrated into the SpiNNaker software\nframework allowing for scalability onto larger systems. The hardware-software\nsystem presented in this work paves the way for power-efficient mobile and\nbiomedical applications with biologically plausible brain-inspired algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 13:28:17 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Yan", "Yexin", ""], ["Kappel", "David", ""], ["Neumaerker", "Felix", ""], ["Partzsch", "Johannes", ""], ["Vogginger", "Bernhard", ""], ["Hoeppner", "Sebastian", ""], ["Furber", "Steve", ""], ["Maass", "Wolfgang", ""], ["Legenstein", "Robert", ""], ["Mayr", "Christian", ""]]}, {"id": "1903.08543", "submitter": "Chris Beeler", "authors": "Chris Beeler, Uladzimir Yahorau, Rory Coles, Kyle Mills, Stephen\n  Whitelam, and Isaac Tamblyn", "title": "Optimizing thermodynamic trajectories using evolutionary and\n  gradient-based reinforcement learning", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.stat-mech cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using a model heat engine we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We use\nan evolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. Given\nadditional irreversible processes this evolutionary scheme learns a previously\nunknown thermodynamic cycle. We also show how an evolutionary approach compares\nwith gradient-based reinforcement learning. Gradient-based reinforcement\nlearning is able to learn the Stirling cycle, whereas an evolutionary approach\nachieves the optimal Carnot cycle. Our results show how the reinforcement\nlearning strategies developed for game playing can be applied to solve physical\nproblems conditioned upon path-extensive order parameters.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:09:16 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 18:32:41 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 13:48:24 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Beeler", "Chris", ""], ["Yahorau", "Uladzimir", ""], ["Coles", "Rory", ""], ["Mills", "Kyle", ""], ["Whitelam", "Stephen", ""], ["Tamblyn", "Isaac", ""]]}, {"id": "1903.08763", "submitter": "Tarik A. Rashid", "authors": "Hardi M. Mohammed (1,3), Shahla U. Umar (1,4), Tarik A. Rashid (2)\n  ((1) Technical College of Informatics, Sulaimani Polytechnic University,\n  Sulaimani, KRG, Iraq. (2) Computer Science and Engineering, University of\n  Kurdistan Hewler (UKH), Erbil, KRG, Iraq. (3) Applied Computer Department,\n  College of Health and Applied Sciences, Charmo University, Sulaimani,\n  Chamchamal, KRG, Iraq. (4) Network Department, College of Computer Science\n  and Information Technology, Kirkuk University, Kirkuk, KRG, Iraq)", "title": "A Systematic and Meta-analysis Survey of Whale Optimization Algorithm", "comments": "Computational Intelligence and Neuroscience, Hindawi, Computational\n  Intelligence and Neuroscience, Article ID 8718571", "journal-ref": "Computational Intelligence and Neuroscience, 2019", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Whale Optimization Algorithm (WOA) is a nature-inspired meta-heuristic\noptimization algorithm, which was proposed by Mirjalili and Lewis in 2016. This\nalgorithm has shown its ability to solve many problems. Comprehensive surveys\nhave been conducted about some other nature-inspired algorithms, such as ABC,\nPSO, etc.Nonetheless, no survey search work has been conducted on WOA.\nTherefore, in this paper, a systematic and meta analysis survey of WOA is\nconducted to help researchers to use it in different areas or hybridize it with\nother common algorithms. Thus, WOA is presented in depth in terms of\nalgorithmic backgrounds, its characteristics, limitations, modifications,\nhybridizations, and applications. Next, WOA performances are presented to solve\ndifferent problems. Then, the statistical results of WOA modifications and\nhybridizations are established and compared with the most common optimization\nalgorithms and WOA. The survey's results indicate that WOA performs better than\nother common algorithms in terms of convergence speed and balancing between\nexploration and exploitation. WOA modifications and hybridizations also perform\nwell compared to WOA. In addition, our investigation paves a way to present a\nnew technique by hybridizing both WOA and BAT algorithms. The BAT algorithm is\nused for the exploration phase, whereas the WOA algorithm is used for the\nexploitation phase. Finally, statistical results obtained from WOA-BAT are very\ncompetitive and better than WOA in 16 benchmarks functions. WOA-BAT also\noutperforms well in 13 functions from CEC2005 and 7 functions from CEC2019.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 22:10:11 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 22:05:08 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 05:37:09 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Mohammed", "Hardi M.", ""], ["Umar", "Shahla U.", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "1903.08850", "submitter": "Aditya Grover", "authors": "Aditya Grover, Eric Wang, Aaron Zweig, Stefano Ermon", "title": "Stochastic Optimization of Sorting Networks via Continuous Relaxations", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting input objects is an important step in many machine learning\npipelines. However, the sorting operator is non-differentiable with respect to\nits inputs, which prohibits end-to-end gradient-based optimization. In this\nwork, we propose NeuralSort, a general-purpose continuous relaxation of the\noutput of the sorting operator from permutation matrices to the set of unimodal\nrow-stochastic matrices, where every row sums to one and has a distinct arg\nmax. This relaxation permits straight-through optimization of any computational\ngraph involve a sorting operation. Further, we use this relaxation to enable\ngradient-based stochastic optimization over the combinatorially large space of\npermutations by deriving a reparameterized gradient estimator for the\nPlackett-Luce family of distributions over permutations. We demonstrate the\nusefulness of our framework on three tasks that require learning semantic\norderings of high-dimensional objects, including a fully differentiable,\nparameterized extension of the k-nearest neighbors algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 07:05:44 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 07:56:18 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Grover", "Aditya", ""], ["Wang", "Eric", ""], ["Zweig", "Aaron", ""], ["Ermon", "Stefano", ""]]}, {"id": "1903.08941", "submitter": "Yexin Yan", "authors": "Sebastian Hoeppner, Bernhard Vogginger, Yexin Yan, Andreas Dixius,\n  Stefan Scholze, Johannes Partzsch, Felix Neumaerker, Stephan Hartmann, Stefan\n  Schiefer, Georg Ellguth, Love Cederstroem, Luis Plana, Jim Garside, Steve\n  Furber, Christian Mayr", "title": "Dynamic Power Management for Neuromorphic Many-Core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a dynamic power management architecture for neuromorphic\nmany core systems such as SpiNNaker. A fast dynamic voltage and frequency\nscaling (DVFS) technique is presented which allows the processing elements (PE)\nto change their supply voltage and clock frequency individually and\nautonomously within less than 100 ns. This is employed by the neuromorphic\nsimulation software flow, which defines the performance level (PL) of the PE\nbased on the actual workload within each simulation cycle. A test chip in 28 nm\nSLP CMOS technology has been implemented. It includes 4 PEs which can be scaled\nfrom 0.7 V to 1.0 V with frequencies from 125 MHz to 500 MHz at three distinct\nPLs. By measurement of three neuromorphic benchmarks it is shown that the total\nPE power consumption can be reduced by 75%, with 80% baseline power reduction\nand a 50% reduction of energy per neuron and synapse computation, all while\nmaintaining temporary peak system performance to achieve biological real-time\noperation of the system. A numerical model of this power management model is\nderived which allows DVFS architecture exploration for neuromorphics. The\nproposed technique is to be used for the second generation SpiNNaker\nneuromorphic many core system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 12:06:46 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Hoeppner", "Sebastian", ""], ["Vogginger", "Bernhard", ""], ["Yan", "Yexin", ""], ["Dixius", "Andreas", ""], ["Scholze", "Stefan", ""], ["Partzsch", "Johannes", ""], ["Neumaerker", "Felix", ""], ["Hartmann", "Stephan", ""], ["Schiefer", "Stefan", ""], ["Ellguth", "Georg", ""], ["Cederstroem", "Love", ""], ["Plana", "Luis", ""], ["Garside", "Jim", ""], ["Furber", "Steve", ""], ["Mayr", "Christian", ""]]}, {"id": "1903.09085", "submitter": "Yang Lou Dr", "authors": "Yang Lou, Shiu Yin Yuen, Guanrong Chen, Xin Zhang", "title": "On-line Search History-assisted Restart Strategy for Covariance Matrix\n  Adaptation Evolution Strategy", "comments": "8 pages, 9 figures", "journal-ref": "IEEE Congress on Evolutionary Computation (2019) 3142-3149", "doi": "10.1109/CEC.2019.8790368", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Restart strategy helps the covariance matrix adaptation evolution strategy\n(CMA-ES) to increase the probability of finding the global optimum in\noptimization, while a single run CMA-ES is easy to be trapped in local optima.\nIn this paper, the continuous non-revisiting genetic algorithm (cNrGA) is used\nto help CMA-ES to achieve multiple restarts from different sub-regions of the\nsearch space. The CMA-ES with on-line search history-assisted restart strategy\n(HR-CMA-ES) is proposed. The entire on-line search history of cNrGA is stored\nin a binary space partitioning (BSP) tree, which is effective for performing\nlocal search. The frequently sampled sub-region is reflected by a deep position\nin the BSP tree. When leaf nodes are located deeper than a threshold, the\ncorresponding sub-region is considered a region of interest (ROI). In\nHR-CMA-ES, cNrGA is responsible for global exploration and suggesting ROI for\nCMA-ES to perform an exploitation within or around the ROI. CMA-ES restarts\nindependently in each suggested ROI. The non-revisiting mechanism of cNrGA\navoids to suggest the same ROI for a second time. Experimental results on the\nCEC 2013 and 2017 benchmark suites show that HR-CMA-ES performs better than\nboth CMA-ES and cNrGA. A positive synergy is observed by the memetic\ncooperation of the two algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 15:11:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lou", "Yang", ""], ["Yuen", "Shiu Yin", ""], ["Chen", "Guanrong", ""], ["Zhang", "Xin", ""]]}, {"id": "1903.09297", "submitter": "Hussein Abbass A", "authors": "Alexander Gee and Hussein Abbass", "title": "Transparent Machine Education of Neural Networks for Swarm Shepherding\n  Using Curriculum Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm control is a difficult problem due to the need to guide a large number\nof agents simultaneously. We cast the problem as a shepherding problem, similar\nto biological dogs guiding a group of sheep towards a goal. The shepherd needs\nto deal with complex and dynamic environments and make decisions in order to\ndirect the swarm from one location to another. In this paper, we design a novel\ncurriculum to teach an artificial intelligence empowered agent to shepherd in\nthe presence of the large state space associated with the shepherding problem\nand in a transparent manner. The results show that a properly designed\ncurriculum could indeed enhance the speed of learning and the complexity of\nlearnt behaviours.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 00:04:04 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Gee", "Alexander", ""], ["Abbass", "Hussein", ""]]}, {"id": "1903.09304", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Junichi Murata, Hirotaka Takano", "title": "Tackling Unit Commitment and Load Dispatch Problems Considering All\n  Constraints with Evolutionary Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CE cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit commitment and load dispatch problems are important and complex problems\nin power system operations that have being traditionally solved separately. In\nthis paper, both problems are solved together without approximations or\nsimplifications. In fact, the problem solved has a massive amount of\ngrid-connected photovoltaic units, four pump-storage hydro plants as energy\nstorage units and ten thermal power plants, each with its own set of operation\nrequirements that need to be satisfied. To face such a complex constrained\noptimization problem an adaptive repair method is proposed. By including a\ngiven repair method itself as a parameter to be optimized, the proposed\nadaptive repair method avoid any bias in repair choices. Moreover, this results\nin a repair method that adapt to the problem and will improve together with the\nsolution during optimization. Experiments are conducted revealing that the\nproposed method is capable of surpassing exact method solutions on a simplified\nversion of the problem with approximations as well as solve the otherwise\nintractable complete problem without simplifications. Moreover, since the\nproposed approach can be applied to other problems in general and it may not be\nobvious how to choose the constraint handling for a certain constraint, a\nguideline is provided explaining the reasoning behind. Thus, this paper open\nfurther possibilities to deal with the ever changing types of generation units\nand other similarly complex operation/schedule optimization problems with many\ndifficult constraints.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 01:45:53 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Murata", "Junichi", ""], ["Takano", "Hirotaka", ""]]}, {"id": "1903.09386", "submitter": "Yuriria Cortes-Poza", "authors": "Yuriria Cortes-Poza and J. Rogelio Perez-Buendia", "title": "Forest structure in epigenetic landscapes", "comments": "26 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphogenesis is the biological process that causes the emergence and changes\nof patterns (tissues and organs) in living organisms. It is a robust,\nself-organising mechanism, governed by Genetic Regulatory Networks (GRN), that\nhasn't been thoroughly understood. In this work we propose Epigenetic Forests\nas a tool to study morphogenesis and to extract valuable information from GRN.\nOur method unfolds the richness and structure within the GRN.\n  As a case study, we analyze the GRN during cell fate determination during the\nearly stages of development of the flower Arabidopsis thaliana and its spatial\ndynamics. By using a genetic algorithm we optimize cell differentiation in our\nmodel and correctly recover the architecture of the flower.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 07:29:46 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Cortes-Poza", "Yuriria", ""], ["Perez-Buendia", "J. Rogelio", ""]]}, {"id": "1903.09393", "submitter": "Anil Yaman", "authors": "Anil Yaman, Giovanni Iacca, Decebal Constantin Mocanu, George\n  Fletcher, Mykola Pechenizkiy", "title": "Learning with Delayed Synaptic Plasticity", "comments": "GECCO2019", "journal-ref": null, "doi": "10.1145/3321707.3321723", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plasticity property of biological neural networks allows them to perform\nlearning and optimize their behavior by changing their configuration. Inspired\nby biology, plasticity can be modeled in artificial neural networks by using\nHebbian learning rules, i.e. rules that update synapses based on the neuron\nactivations and reinforcement signals. However, the distal reward problem\narises when the reinforcement signals are not available immediately after each\nnetwork output to associate the neuron activations that contributed to\nreceiving the reinforcement signal. In this work, we extend Hebbian plasticity\nrules to allow learning in distal reward cases. We propose the use of neuron\nactivation traces (NATs) to provide additional data storage in each synapse to\nkeep track of the activation of the neurons. Delayed reinforcement signals are\nprovided after each episode relative to the networks' performance during the\nprevious episode. We employ genetic algorithms to evolve delayed synaptic\nplasticity (DSP) rules and perform synaptic updates based on NATs and delayed\nreinforcement signals. We compare DSP with an analogous hill climbing algorithm\nthat does not incorporate domain knowledge introduced with the NATs, and show\nthat the synaptic updates performed by the DSP rules demonstrate more effective\ntraining performance relative to the HC algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 08:03:02 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 19:27:23 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Yaman", "Anil", ""], ["Iacca", "Giovanni", ""], ["Mocanu", "Decebal Constantin", ""], ["Fletcher", "George", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1903.09671", "submitter": "Konstantinos Michmizos", "authors": "Vladimir A. Ivanov, Ioannis E. Polykretis, Konstantinos P. Michmizos", "title": "Axonal Conduction Velocity Impacts Neuronal Network Oscillations", "comments": "4 pages, 5 figures, IEEE-EMBS International Conference on Biomedical\n  and Health Informatics (BHI '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing experimental evidence suggests that axonal action potential\nconduction velocity is a highly adaptive parameter in the adult central nervous\nsystem. Yet, the effects of this newfound plasticity on global brain dynamics\nis poorly understood. In this work, we analyzed oscillations in biologically\nplausible neuronal networks with different conduction velocity distributions.\nChanges of 1-2 (ms) in network mean signal transmission time resulted in\nsubstantial network oscillation frequency changes ranging in 0-120 (Hz). Our\nresults suggest that changes in axonal conduction velocity may significantly\naffect both the frequency and synchrony of brain rhythms, which have well\nestablished connections to learning, memory, and other cognitive processes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 18:38:12 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ivanov", "Vladimir A.", ""], ["Polykretis", "Ioannis E.", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "1903.09688", "submitter": "Erik Derner", "authors": "Ji\\v{r}\\'i Kubal\\'ik, Jan \\v{Z}egklitz, Erik Derner and Robert\n  Babu\\v{s}ka", "title": "Symbolic Regression Methods for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms can be used to optimally solve dynamic\ndecision-making and control problems. With continuous-valued state and input\nvariables, reinforcement learning algorithms must rely on function\napproximators to represent the value function and policy mappings. Commonly\nused numerical approximators, such as neural networks or basis function\nexpansions, have two main drawbacks: they are black-box models offering no\ninsight in the mappings learned, and they require significant trial and error\ntuning of their meta-parameters. In this paper, we propose a new approach to\nconstructing smooth value functions by means of symbolic regression. We\nintroduce three off-line methods for finding value functions based on a state\ntransition model: symbolic value iteration, symbolic policy iteration, and a\ndirect solution of the Bellman equation. The methods are illustrated on four\nnonlinear control problems: velocity control under friction, one-link and\ntwo-link pendulum swing-up, and magnetic manipulation. The results show that\nthe value functions not only yield well-performing policies, but also are\ncompact, human-readable and mathematically tractable. This makes them\npotentially suitable for further analysis of the closed-loop system. A\ncomparison with alternative approaches using neural networks shows that our\nmethod constructs well-performing value functions with substantially fewer\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 19:53:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kubal\u00edk", "Ji\u0159\u00ed", ""], ["\u017degklitz", "Jan", ""], ["Derner", "Erik", ""], ["Babu\u0161ka", "Robert", ""]]}, {"id": "1903.09769", "submitter": "Tianyun Zhang", "authors": "Shaokai Ye, Xiaoyu Feng, Tianyun Zhang, Xiaolong Ma, Sheng Lin,\n  Zhengang Li, Kaidi Xu, Wujie Wen, Sijia Liu, Jian Tang, Makan Fardad, Xue\n  Lin, Yongpan Liu and Yanzhi Wang", "title": "Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning\n  and Quantization Rates using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning and weight quantization are two important categories of DNN\nmodel compression. Prior work on these techniques are mainly based on\nheuristics. A recent work developed a systematic frame-work of DNN weight\npruning using the advanced optimization technique ADMM (Alternating Direction\nMethods of Multipliers), achieving one of state-of-art in weight pruning\nresults. In this work, we first extend such one-shot ADMM-based framework to\nguarantee solution feasibility and provide fast convergence rate, and\ngeneralize to weight quantization as well. We have further developed a\nmulti-step, progressive DNN weight pruning and quantization framework, with\ndual benefits of (i) achieving further weight pruning/quantization thanks to\nthe special property of ADMM regularization, and (ii) reducing the search space\nwithin each step. Extensive experimental results demonstrate the superior\nperformance compared with prior work. Some highlights: (i) we achieve 246x,36x,\nand 8x weight pruning on LeNet-5, AlexNet, and ResNet-50 models, respectively,\nwith (almost) zero accuracy loss; (ii) even a significant 61x weight pruning in\nAlexNet (ImageNet) results in only minor degradation in actual accuracy\ncompared with prior work; (iii) we are among the first to derive notable weight\npruning results for ResNet and MobileNet models; (iv) we derive the first\nlossless, fully binarized (for all layers) LeNet-5 for MNIST and VGG-16 for\nCIFAR-10; and (v) we derive the first fully binarized (for all layers) ResNet\nfor ImageNet with reasonable accuracy loss.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 05:54:26 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 03:27:38 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ye", "Shaokai", ""], ["Feng", "Xiaoyu", ""], ["Zhang", "Tianyun", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Li", "Zhengang", ""], ["Xu", "Kaidi", ""], ["Wen", "Wujie", ""], ["Liu", "Sijia", ""], ["Tang", "Jian", ""], ["Fardad", "Makan", ""], ["Lin", "Xue", ""], ["Liu", "Yongpan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1903.09800", "submitter": "Alejandro Baldominos", "authors": "Alejandro Baldominos and Yago Saez", "title": "Coin.AI: A Proof-of-Useful-Work Scheme for Blockchain-based Distributed\n  Deep Learning", "comments": "17 pages, 5 figures", "journal-ref": "Entropy 2019, 21, 723", "doi": "10.3390/e21080723", "report-no": null, "categories": "cs.NE cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One decade ago, Bitcoin was introduced, becoming the first cryptocurrency and\nestablishing the concept of \"blockchain\" as a distributed ledger. As of today,\nthere are many different implementations of cryptocurrencies working over a\nblockchain, with different approaches and philosophies. However, many of them\nshare one common feature: they require proof-of-work to support the generation\nof blocks (mining) and, eventually, the generation of money. This proof-of-work\nscheme often consists in the resolution of a cryptography problem, most\ncommonly breaking a hash value, which can only be achieved through brute-force.\nThe main drawback of proof-of-work is that it requires ridiculously large\namounts of energy which do not have any useful outcome beyond supporting the\ncurrency. In this paper, we present a theoretical proposal that introduces a\nproof-of-useful-work scheme to support a cryptocurrency running over a\nblockchain, which we named Coin.AI. In this system, the mining scheme requires\ntraining deep learning models, and a block is only mined when the performance\nof such model exceeds a threshold. The distributed system allows for nodes to\nverify the models delivered by miners in an easy way (certainly much more\nefficiently than the mining process itself), determining when a block is to be\ngenerated. Additionally, this paper presents a proof-of-storage scheme for\nrewarding users that provide storage for the deep learning models, as well as a\ntheoretical dissertation on how the mechanics of the system could be\narticulated with the ultimate goal of democratizing access to artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:15:21 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 10:19:06 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Baldominos", "Alejandro", ""], ["Saez", "Yago", ""]]}, {"id": "1903.09807", "submitter": "Hyungjun Kim", "authors": "Hyungjun Kim, Yulhwa Kim, Sungju Ryu, and Jae-Joon Kim", "title": "BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant computational cost and memory requirements for deep neural\nnetworks (DNNs) make it difficult to utilize DNNs in resource-constrained\nenvironments. Binary neural network (BNN), which uses binary weights and binary\nactivations, has been gaining interests for its hardware-friendly\ncharacteristics and minimal resource requirement. However, BNN usually suffers\nfrom accuracy degradation. In this paper, we introduce \"BitSplit-Net\", a neural\nnetwork which maintains the hardware-friendly characteristics of BNN while\nimproving accuracy by using multi-bit precision. In BitSplit-Net, each bit of\nmulti-bit activations propagates independently throughout the network before\nbeing merged at the end of the network. Thus, each bit path of the BitSplit-Net\nresembles BNN and hardware friendly features of BNN, such as bitwise binary\nactivation function, are preserved in our scheme. We demonstrate that the\nBitSplit version of LeNet-5, VGG-9, AlexNet, and ResNet-18 can be trained to\nhave similar classification accuracy at a lower computational cost compared to\nconventional multi-bit networks with low bit precision (<= 4-bit). We further\nevaluate BitSplit-Net on GPU with custom CUDA kernel, showing that BitSplit-Net\ncan achieve better hardware performance in comparison to conventional multi-bit\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:52:23 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kim", "Hyungjun", ""], ["Kim", "Yulhwa", ""], ["Ryu", "Sungju", ""], ["Kim", "Jae-Joon", ""]]}, {"id": "1903.09876", "submitter": "Hao Tang", "authors": "Hao Tang, Daniel R. Kim, Xiaohui Xie", "title": "Automated pulmonary nodule detection using 3D deep convolutional neural\n  networks", "comments": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of pulmonary nodules in computed tomography (CT) images is\nessential for successful outcomes among lung cancer patients. Much attention\nhas been given to deep convolutional neural network (DCNN)-based approaches to\nthis task, but models have relied at least partly on 2D or 2.5D components for\ninherently 3D data. In this paper, we introduce a novel DCNN approach,\nconsisting of two stages, that is fully three-dimensional end-to-end and\nutilizes the state-of-the-art in object detection. First, nodule candidates are\nidentified with a U-Net-inspired 3D Faster R-CNN trained using online hard\nnegative mining. Second, false positive reduction is performed by 3D DCNN\nclassifiers trained on difficult examples produced during candidate screening.\nFinally, we introduce a method to ensemble models from both stages via\nconsensus to give the final predictions. By using this framework, we ranked\nfirst of 2887 teams in Season One of Alibaba's 2017 TianChi AI Competition for\nHealthcare.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 20:20:15 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Tang", "Hao", ""], ["Kim", "Daniel R.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1903.10103", "submitter": "Cem C. Tutum", "authors": "Cameron R. Wolfe and Cem C. Tutum and Risto Miikkulainen", "title": "Functional Generative Design of Mechanisms with Recurrent Neural\n  Networks and Novelty Search", "comments": "7 pages, GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer-grade 3D printers have made it easier to fabricate aesthetic objects\nand static assemblies, opening the door to automated design of such objects.\nHowever, while static designs are easily produced with 3D printing, functional\ndesigns with moving parts are more difficult to generate: The search space is\ntoo high-dimensional, the resolution of the 3D-printed parts is not adequate,\nand it is difficult to predict the physical behavior of imperfect 3D-printed\nmechanisms. An example challenge is to produce a diverse set of reliable and\neffective gear mechanisms that could be used after production without extensive\npost-processing. To meet this challenge, an indirect encoding based on a\nRecurrent Neural Network (RNN) is created and evolved using novelty search. The\nelite solutions of each generation are 3D printed to evaluate their functional\nperformance on a physical test platform. The system is able to discover\nsequential design rules that are difficult to discover with other methods.\nCompared to direct encoding evolved with Genetic Algorithms (GAs), its designs\nare geometrically more diverse and functionally more effective. It therefore\nforms a promising foundation for the generative design of 3D-printed,\nfunctional mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 02:26:46 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wolfe", "Cameron R.", ""], ["Tutum", "Cem C.", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1903.10410", "submitter": "Sidney Pontes-Filho", "authors": "Sidney Pontes-Filho and Stefano Nichele", "title": "A Conceptual Bio-Inspired Framework for the Evolution of Artificial\n  General Intelligence", "comments": "7 pages, 2 figures, accepted to \"The 3rd Special Session on\n  Biologically Inspired Parallel and Distributed Computing, Algorithms and\n  Solutions\" (BICAS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a conceptual bio-inspired parallel and distributed learning\nframework for the emergence of general intelligence is proposed, where agents\nevolve through environmental rewards and learn throughout their lifetime\nwithout supervision, i.e., self-learning through embodiment. The chosen control\nmechanism for agents is a biologically plausible neuron model based on spiking\nneural networks. Network topologies become more complex through evolution,\ni.e., the topology is not fixed, while the synaptic weights of the networks\ncannot be inherited, i.e., newborn brains are not trained and have no innate\nknowledge of the environment. What is subject to the evolutionary process is\nthe network topology, the type of neurons, and the type of learning. This\nprocess ensures that controllers that are passed through the generations have\nthe intrinsic ability to learn and adapt during their lifetime in mutable\nenvironments. We envision that the described approach may lead to the emergence\nof the simplest form of artificial general intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:49:07 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 08:22:46 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 15:52:45 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 08:49:57 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Pontes-Filho", "Sidney", ""], ["Nichele", "Stefano", ""]]}, {"id": "1903.10545", "submitter": "Ahmad Beirami", "authors": "Yunqi Zhao, Igor Borovikov, Fernando de Mesentier Silva, Ahmad\n  Beirami, Jason Rupert, Caedmon Somers, Jesse Harder, John Kolen, Jervis\n  Pinto, Reza Pourabolghasem, James Pestrak, Harold Chaput, Mohsen Sardari,\n  Long Lin, Sundeep Narravula, Navid Aghdaie, Kazi Zaman", "title": "Winning Isn't Everything: Enhancing Game Development with Intelligent\n  Agents", "comments": "Accepted to IEEE Trans. Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been several high-profile achievements of agents\nlearning to play games against humans and beat them. In this paper, we study\nthe problem of training intelligent agents in service of game development.\nUnlike the agents built to \"beat the game\", our agents aim to produce\nhuman-like behavior to help with game evaluation and balancing. We discuss two\nfundamental metrics based on which we measure the human-likeness of agents,\nnamely skill and style, which are multi-faceted concepts with practical\nimplications outlined in this paper. We report four case studies in which the\nstyle and skill requirements inform the choice of algorithms and metrics used\nto train agents; ranging from A* search to state-of-the-art deep reinforcement\nlearning. We, further, show that the learning potential of state-of-the-art\ndeep RL models does not seamlessly transfer from the benchmark environments to\ntarget ones without heavily tuning their hyperparameters, leading to linear\nscaling of the engineering efforts and computational cost with the number of\ntarget domains.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:39:04 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 00:19:51 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 04:37:58 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2020 18:36:10 GMT"}, {"version": "v5", "created": "Tue, 28 Apr 2020 03:29:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhao", "Yunqi", ""], ["Borovikov", "Igor", ""], ["Silva", "Fernando de Mesentier", ""], ["Beirami", "Ahmad", ""], ["Rupert", "Jason", ""], ["Somers", "Caedmon", ""], ["Harder", "Jesse", ""], ["Kolen", "John", ""], ["Pinto", "Jervis", ""], ["Pourabolghasem", "Reza", ""], ["Pestrak", "James", ""], ["Chaput", "Harold", ""], ["Sardari", "Mohsen", ""], ["Lin", "Long", ""], ["Narravula", "Sundeep", ""], ["Aghdaie", "Navid", ""], ["Zaman", "Kazi", ""]]}, {"id": "1903.10574", "submitter": "Kathleen Hamilton", "authors": "Kathleen E. Hamilton and Tiffany M. Mintz and Catherine D. Schuman", "title": "Spike-based primitives for graph algorithms", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider graph algorithms and graphical analysis as a new\napplication for neuromorphic computing platforms. We demonstrate how the\nnonlinear dynamics of spiking neurons can be used to implement low-level graph\noperations. Our results are hardware agnostic, and we present multiple versions\nof routines that can utilize static synapses or require synapse plasticity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 19:57:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hamilton", "Kathleen E.", ""], ["Mintz", "Tiffany M.", ""], ["Schuman", "Catherine D.", ""]]}, {"id": "1903.10681", "submitter": "Ahlem Aboud", "authors": "Ahlem Aboud, Raja Fdhila and Adel M. Alimi", "title": "Dynamic Multi Objective Particle Swarm Optimization based on a New\n  Environment Change Detection Strategy", "comments": "10 pages, 5 figures, International Conference on Neural Information\n  Processing", "journal-ref": null, "doi": "10.1007/978-3-319-70093-9_27", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The dynamic of real-world optimization problems raises new challenges to the\ntraditional particle swarm optimization (PSO). Responding to these challenges,\nthe dynamic optimization has received considerable attention over the past\ndecade. This paper introduces a new dynamic multi-objective optimization based\nparticle swarm optimization (Dynamic-MOPSO).The main idea of this paper is to\nsolve such dynamic problem based on a new environment change detection strategy\nusing the advantage of the particle swarm optimization. In this way, our\napproach has been developed not just to obtain the optimal solution, but also\nto have a capability to detect the environment changes. Thereby, DynamicMOPSO\nensures the balance between the exploration and the exploitation in dynamic\nresearch space. Our approach is tested through the most popularized dynamic\nbenchmark's functions to evaluate its performance as a good method.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 10:05:28 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Aboud", "Ahlem", ""], ["Fdhila", "Raja", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1903.10951", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Ye Yuan and Yihua Tan", "title": "Optimize TSK Fuzzy Systems for Regression Problems: Mini-Batch Gradient\n  Descent with Regularization, DropRule and AdaBound (MBGD-RDA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Takagi-Sugeno-Kang (TSK) fuzzy systems are very useful machine learning\nmodels for regression problems. However, to our knowledge, there has not\nexisted an efficient and effective training algorithm that ensures their\ngeneralization performance, and also enables them to deal with big data.\nInspired by the connections between TSK fuzzy systems and neural networks, we\nextend three powerful neural network optimization techniques, i.e., mini-batch\ngradient descent, regularization, and AdaBound, to TSK fuzzy systems, and also\npropose three novel techniques (DropRule, DropMF, and DropMembership)\nspecifically for training TSK fuzzy systems. Our final algorithm, mini-batch\ngradient descent with regularization, DropRule and AdaBound (MBGD-RDA), can\nachieve fast convergence in training TSK fuzzy systems, and also superior\ngeneralization performance in testing. It can be used for training TSK fuzzy\nsystems on datasets of any size; however, it is particularly useful for big\ndatasets, on which currently no other efficient training algorithms exist.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:16:24 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 11:37:38 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 01:45:17 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 04:58:47 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wu", "Dongrui", ""], ["Yuan", "Ye", ""], ["Tan", "Yihua", ""]]}, {"id": "1903.10976", "submitter": "Dogan Corus", "authors": "Dogan Corus, Pietro S. Oliveto", "title": "On the Benefits of Populations on the Exploitation Speed of Standard\n  Steady-State Genetic Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally accepted that populations are useful for the global\nexploration of multi-modal optimisation problems. Indeed, several theoretical\nresults are available showing such advantages over single-trajectory search\nheuristics. In this paper we provide evidence that evolving populations via\ncrossover and mutation may also benefit the optimisation time for hillclimbing\nunimodal functions. In particular, we prove bounds on the expected runtime of\nthe standard ($\\mu$+1)~GA for OneMax that are lower than its unary black box\ncomplexity and decrease in the leading constant with the population size up to\n$\\mu=O(\\sqrt{\\log n})$. Our analysis suggests that the optimal mutation\nstrategy is to flip two bits most of the time. To achieve the results we\nprovide two interesting contributions to the theory of randomised search\nheuristics: 1) A novel application of drift analysis which compares absorption\ntimes of different Markov chains without defining an explicit potential\nfunction. 2) The inversion of fundamental matrices to calculate the absorption\ntimes of the Markov chains. The latter strategy was previously proposed in the\nliterature but to the best of our knowledge this is the first time is has been\nused to show non-trivial bounds on expected runtimes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:04:25 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Corus", "Dogan", ""], ["Oliveto", "Pietro S.", ""]]}, {"id": "1903.10983", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "A Tight Runtime Analysis for the cGA on Jump Functions---EDAs Can Cross\n  Fitness Valleys at No Extra Cost", "comments": "25 pages, full version of a paper to appear at GECCO 2019", "journal-ref": null, "doi": "10.1145/3321707.3321747", "report-no": null, "categories": "cs.NE cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the compact genetic algorithm (cGA) with hypothetical\npopulation size $\\mu = \\Omega(\\sqrt n \\log n) \\cap \\text{poly}(n)$ with high\nprobability finds the optimum of any $n$-dimensional jump function with jump\nsize $k < \\frac 1 {20} \\ln n$ in $O(\\mu \\sqrt n)$ iterations. Since it is known\nthat the cGA with high probability needs at least $\\Omega(\\mu \\sqrt n + n \\log\nn)$ iterations to optimize the unimodal OneMax function, our result shows that\nthe cGA in contrast to most classic evolutionary algorithms here is able to\ncross moderate-sized valleys of low fitness at no extra cost.\n  Our runtime guarantee improves over the recent upper bound $O(\\mu n^{1.5}\n\\log n)$ valid for $\\mu = \\Omega(n^{3.5+\\varepsilon})$ of Hasen\\\"ohrl and\nSutton (GECCO 2018). For the best choice of the hypothetical population size,\nthis result gives a runtime guarantee of $O(n^{5+\\varepsilon})$, whereas ours\ngives $O(n \\log n)$.\n  We also provide a simple general method based on parallel runs that, under\nmild conditions, (i)~overcomes the need to specify a suitable population size,\nbut gives a performance close to the one stemming from the best-possible\npopulation size, and (ii)~transforms EDAs with high-probability performance\nguarantees into EDAs with similar bounds on the expected runtime.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:12:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1903.11012", "submitter": "Devdhar Patel", "authors": "Devdhar Patel, Hananel Hazan, Daniel J. Saunders, Hava Siegelmann,\n  Robert Kozma", "title": "Improved robustness of reinforcement learning policies upon conversion\n  to spiking neuronal network platforms applied to ATARI games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Reinforcement Learning (RL) demonstrates excellent performance on tasks\nthat can be solved by trained policy. It plays a dominant role among\ncutting-edge machine learning approaches using multi-layer Neural networks\n(NNs). At the same time, Deep RL suffers from high sensitivity to noisy,\nincomplete, and misleading input data. Following biological intuition, we\ninvolve Spiking Neural Networks (SNNs) to address some deficiencies of deep RL\nsolutions. Previous studies in image classification domain demonstrated that\nstandard NNs (with ReLU nonlinearity) trained using supervised learning can be\nconverted to SNNs with negligible deterioration in performance. In this paper,\nwe extend those conversion results to the domain of Q-Learning NNs trained\nusing RL. We provide a proof of principle of the conversion of standard NN to\nSNN. In addition, we show that the SNN has improved robustness to occlusion in\nthe input image. Finally, we introduce results with converting full-scale Deep\nQ-network to SNN, paving the way for future research to robust Deep RL\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:53:09 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 22:36:43 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 14:46:13 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Patel", "Devdhar", ""], ["Hazan", "Hananel", ""], ["Saunders", "Daniel J.", ""], ["Siegelmann", "Hava", ""], ["Kozma", "Robert", ""]]}, {"id": "1903.11323", "submitter": "Rizwan Ahmed Khan", "authors": "Hamza Sharif, Rizwan Ahmed Khan", "title": "A novel machine learning based framework for detection of Autism\n  Spectrum Disorder (ASD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision and machine learning are the linchpin of field of automation.\nThe medicine industry has adopted numerous methods to discover the root causes\nof many diseases in order to automate detection process. But, the biomarkers of\nAutism Spectrum Disorder (ASD) are still unknown, let alone automating its\ndetection. Studies from the neuroscience domain highlighted the fact that\ncorpus callosum and intracranial brain volume holds significant information for\ndetection of ASD. Such results and studies are not tested and verified by\nscientists working in the domain of computer vision / machine learning. Thus,\nin this study we have proposed a machine learning based framework for automatic\ndetection of ASD using features extracted from corpus callosum and intracranial\nbrain volume from ABIDE dataset. Corpus callosum and intracranial brain volume\ndata is obtained from T1-weighted MRI scans. Our proposed framework first\ncalculates weights of features extracted from Corpus callosum and intracranial\nbrain volume data. This step ensures to utilize discriminative capabilities of\nonly those features that will help in robust recognition of ASD. Then,\nconventional machine learning algorithm (conventional refers to algorithms\nother than deep learning) is applied on features that are most significant in\nterms of discriminative capabilities for recognition of ASD. Finally, for\nbenchmarking and to verify potential of deep learning on analyzing neuroimaging\ndata i.e. T1-weighted MRI scans, we have done experiment with state of the art\ndeep learning architecture i.e. VGG16 . We have used transfer learning approach\nto use already trained VGG16 model for detection of ASD. This is done to help\nreaders understand benefits and bottlenecks of using deep learning approach for\nanalyzing neuroimaging data which is difficult to record in large enough\nquantity for deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 10:02:08 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 14:38:52 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 14:58:23 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sharif", "Hamza", ""], ["Khan", "Rizwan Ahmed", ""]]}, {"id": "1903.11359", "submitter": "Francesco Croce", "authors": "Francesco Croce, Jonas Rauber, Matthias Hein", "title": "Scaling up the randomized gradient-free adversarial attack reveals\n  overestimation of robustness using established attacks", "comments": "Accepted at International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are highly non-robust against adversarial\nmanipulation. A significant amount of work has been invested in techniques to\ncompute lower bounds on robustness through formal guarantees and to build\nprovably robust models. However, it is still difficult to get guarantees for\nlarger networks or robustness against larger perturbations. Thus attack\nstrategies are needed to provide tight upper bounds on the actual robustness.\nWe significantly improve the randomized gradient-free attack for ReLU networks\n[9], in particular by scaling it up to large networks. We show that our attack\nachieves similar or significantly smaller robust accuracy than state-of-the-art\nattacks like PGD or the one of Carlini and Wagner, thus revealing an\noverestimation of the robustness by these state-of-the-art methods. Our attack\nis not based on a gradient descent scheme and in this sense gradient-free,\nwhich makes it less sensitive to the choice of hyperparameters as no careful\nselection of the stepsize is required.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 11:41:27 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 17:04:43 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Croce", "Francesco", ""], ["Rauber", "Jonas", ""], ["Hein", "Matthias", ""]]}, {"id": "1903.11421", "submitter": "Richard Jiang", "authors": "Ziping Jiang, Paul L. Chazot, M. Emre Celebi, Danny Crookes and\n  Richard Jiang", "title": "Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN\n  Framework", "comments": null, "journal-ref": "IEEE Access 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioural phenotyping of Drosophila is an important means in biological and\nmedical research to identify genetic, pathologic or psychologic impact on\nanimal behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:41:17 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Jiang", "Ziping", ""], ["Chazot", "Paul L.", ""], ["Celebi", "M. Emre", ""], ["Crookes", "Danny", ""], ["Jiang", "Richard", ""]]}, {"id": "1903.11446", "submitter": "Xin-She Yang", "authors": "Xin-She Yang, Suash Deb, Sudhanshu K Mishra", "title": "Multi-Species Cuckoo Search Algorithm for Global Optimization", "comments": "15 pages, 1 figures", "journal-ref": "Cognitive Computation, vol. 10, number 6, 1085-1095 (2018)", "doi": "10.1007/s12559-018-9579-4", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization problems in science and engineering are highly nonlinear,\nand thus require sophisticated optimization techniques to solve. Traditional\ntechniques such as gradient-based algorithms are mostly local search methods,\nand often struggle to cope with such challenging optimization problems. Recent\ntrends tend to use nature-inspired optimization algorithms. This work extends\nthe standard cuckoo search (CS) by using the successful features of the\ncuckoo-host co-evolution with multiple interacting species, and the proposed\nmulti-species cuckoo search (MSCS) intends to mimic the multiple species of\ncuckoos that compete for the survival of the fittest, and they co-evolve with\nhost species with solution vectors being encoded as position vectors. The\nproposed algorithm is then validated by 15 benchmark functions as well as five\nnonlinear, multimodal design case studies in practical applications. Simulation\nresults suggest that the proposed algorithm can be effective for finding\noptimal solutions and in this case all optimal solutions are achievable. The\nresults for the test benchmarks are also compared with those obtained by other\nmethods such as the standard cuckoo search and genetic algorithm, which\ndemonstrated the efficiency of the present algorithm. Based on numerical\nexperiments and case studies, we can conclude that the proposed algorithm can\nbe more efficient in most cases, leading a potentially very effective tool for\nsolving nonlinear optimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:25:16 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Yang", "Xin-She", ""], ["Deb", "Suash", ""], ["Mishra", "Sudhanshu K", ""]]}, {"id": "1903.11483", "submitter": "Erik Derner", "authors": "Erik Derner, Ji\\v{r}\\'i Kubal\\'ik, Nicola Ancona and Robert\n  Babu\\v{s}ka", "title": "Constructing Parsimonious Analytic Models for Dynamic Systems via\n  Symbolic Regression", "comments": null, "journal-ref": "Applied Soft Computing, Volume 94, September 2020, 106432", "doi": "10.1016/j.asoc.2020.106432", "report-no": null, "categories": "cs.LG cs.NE cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing mathematical models of dynamic systems is central to many\ndisciplines of engineering and science. Models facilitate simulations, analysis\nof the system's behavior, decision making and design of automatic control\nalgorithms. Even inherently model-free control techniques such as reinforcement\nlearning (RL) have been shown to benefit from the use of models, typically\nlearned online. Any model construction method must address the tradeoff between\nthe accuracy of the model and its complexity, which is difficult to strike. In\nthis paper, we propose to employ symbolic regression (SR) to construct\nparsimonious process models described by analytic equations. We have equipped\nour method with two different state-of-the-art SR algorithms which\nautomatically search for equations that fit the measured data: Single Node\nGenetic Programming (SNGP) and Multi-Gene Genetic Programming (MGGP). In\naddition to the standard problem formulation in the state-space domain, we show\nhow the method can also be applied to input-output models of the NARX\n(nonlinear autoregressive with exogenous input) type. We present the approach\non three simulated examples with up to 14-dimensional state space: an inverted\npendulum, a mobile robot, and a bipedal walking robot. A comparison with deep\nneural networks and local linear regression shows that SR in most cases\noutperforms these commonly used alternative methods. We demonstrate on a real\npendulum system that the analytic model found enables a RL controller to\nsuccessfully perform the swing-up task, based on a model constructed from only\n100 data samples.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 15:22:38 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 09:17:27 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Derner", "Erik", ""], ["Kubal\u00edk", "Ji\u0159\u00ed", ""], ["Ancona", "Nicola", ""], ["Babu\u0161ka", "Robert", ""]]}, {"id": "1903.11598", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "A Simple Haploid-Diploid Evolutionary Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.07429,\n  arXiv:1811.04073, arXiv:1808.03471", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been suggested that evolution exploits a form of fitness\nlandscape smoothing within eukaryotic sex due to the haploid-diploid cycle.\nThis short paper presents a simple modification to the standard evolutionary\ncomputing algorithm to similarly benefit from the process. Using the well-known\nNK model of fitness landscapes it is shown that the benefit emerges as\nruggedness is increased.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 11:44:09 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1903.11621", "submitter": "Xin-She Yang", "authors": "Nunzia Palmieri, Xin-She Yang, Floriano De Rango, Amilcare Francesco\n  Santamaria", "title": "Self-adaptive decision-making mechanisms to balance the execution of\n  multiple tasks for a multi-robots team", "comments": "40 pages", "journal-ref": "Neurocomputing, vol. 306, 17-36 (2018)", "doi": "10.1016/j.neucom.2018.03.038", "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the coordination problem of multiple robots with the goal\nof finding specific hazardous targets in an unknown area and dealing with them\ncooperatively. The desired behaviour for the robotic system entails multiple\nrequirements, which may also be conflicting. The paper presents the problem as\na constrained bi-objective optimization problem in which mobile robots must\nperform two specific tasks of exploration and at same time cooperation and\ncoordination for disarming the hazardous targets. These objectives are opposed\ngoals, in which one may be favored, but only at the expense of the other.\nTherefore, a good trade-off must be found. For this purpose, a nature-inspired\napproach and an analytical mathematical model to solve this problem considering\na single equivalent weighted objective function are presented. The results of\nproposed coordination model, simulated in a two dimensional terrain, are showed\nin order to assess the behaviour of the proposed solution to tackle this\nproblem. We have analyzed the performance of the approach and the influence of\nthe weights of the objective function under different conditions: static and\ndynamic. In this latter situation, the robots may fail under the stringent\nlimited budget of energy or for hazardous events. The paper concludes with a\ncritical discussion of the experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 18:01:59 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Palmieri", "Nunzia", ""], ["Yang", "Xin-She", ""], ["De Rango", "Floriano", ""], ["Santamaria", "Amilcare Francesco", ""]]}, {"id": "1903.11674", "submitter": "Donya Yazdani", "authors": "Dogan Corus, Pietro S. Oliveto, Donya Yazdani", "title": "On Inversely Proportional Hypermutations with Mutation Potential", "comments": "This paper is accepted in GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Immune Systems (AIS) employing hypermutations with linear static\nmutation potential have recently been shown to be very effective at escaping\nlocal optima of combinatorial optimisation problems at the expense of being\nslower during the exploitation phase compared to standard evolutionary\nalgorithms. In this paper we prove that considerable speed-ups in the\nexploitation phase may be achieved with dynamic inversely proportional mutation\npotentials (IPM) and argue that the potential should decrease inversely to the\ndistance to the optimum rather than to the difference in fitness. Afterwards we\ndefine a simple (1+1)~Opt-IA, that uses IPM hypermutations and ageing, for\nrealistic applications where optimal solutions are unknown. The aim of the AIS\nis to approximate the ideal behaviour of the inversely proportional\nhypermutations better and better as the search space is explored. We prove that\nsuch desired behaviour, and related speed-ups, occur for a well-studied bimodal\nbenchmark function called \\textsc{TwoMax}. Furthermore, we prove that the\n(1+1)~Opt-IA with IPM efficiently optimises a third bimodal function,\n\\textsc{Cliff}, by escaping its local optima while Opt-IA with static potential\ncannot, thus requires exponential expected runtime in the distance between the\ncliff and the optimum.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 19:50:46 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Corus", "Dogan", ""], ["Oliveto", "Pietro S.", ""], ["Yazdani", "Donya", ""]]}, {"id": "1903.11691", "submitter": "Pietro Verzelli", "authors": "Pietro Verzelli and Cesare Alippi and Lorenzo Livi", "title": "Echo State Networks with Self-Normalizing Activations on the\n  Hyper-Sphere", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-019-50158-4", "report-no": null, "categories": "cs.NE cs.LG nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the various architectures of Recurrent Neural Networks, Echo State\nNetworks (ESNs) emerged due to their simplified and inexpensive training\nprocedure. These networks are known to be sensitive to the setting of\nhyper-parameters, which critically affect their behaviour. Results show that\ntheir performance is usually maximized in a narrow region of hyper-parameter\nspace called edge of chaos. Finding such a region requires searching in\nhyper-parameter space in a sensible way: hyper-parameter configurations\nmarginally outside such a region might yield networks exhibiting fully\ndeveloped chaos, hence producing unreliable computations. The performance gain\ndue to optimizing hyper-parameters can be studied by considering the\nmemory--nonlinearity trade-off, i.e., the fact that increasing the nonlinear\nbehavior of the network degrades its ability to remember past inputs, and\nvice-versa. In this paper, we propose a model of ESNs that eliminates critical\ndependence on hyper-parameters, resulting in networks that provably cannot\nenter a chaotic regime and, at the same time, denotes nonlinear behaviour in\nphase space characterised by a large memory of past inputs, comparable to the\none of linear networks. Our contribution is supported by experiments\ncorroborating our theoretical findings, showing that the proposed model\ndisplays dynamics that are rich-enough to approximate many common nonlinear\nsystems used for benchmarking.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:37:40 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 16:06:55 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Verzelli", "Pietro", ""], ["Alippi", "Cesare", ""], ["Livi", "Lorenzo", ""]]}, {"id": "1903.11712", "submitter": "Tarik A. Rashid", "authors": "Tarik A. Rashid (1, 2), Dosti K. Abbas (3), Yalin K. Turel (4) ((1)\n  Computer Science and Engineering Department, University of Kurdistan Hewler,\n  Kurdistan, Iraq, (2) Software and Informatics Engineering, Salahaddin\n  University-Erbil, Kurdistan, Iraq, (3) Faculty of Engineering, Soran\n  University, Kurdistan, Iraq, (4) Department of Computer Education and\n  Instructional Technology, Firat University, Elazig, Turkey)", "title": "A Multi Hidden Recurrent Neural Network with a Modified Grey Wolf\n  Optimizer", "comments": "34 pages, published in PLoS ONE", "journal-ref": "PLoS ONE 14(3): e0213237", "doi": "10.1371/journal.pone.0213237", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying university students' weaknesses results in better learning and\ncan function as an early warning system to enable students to improve. However,\nthe satisfaction level of existing systems is not promising. New and dynamic\nhybrid systems are needed to imitate this mechanism. A hybrid system (a\nmodified Recurrent Neural Network with an adapted Grey Wolf Optimizer) is used\nto forecast students' outcomes. This proposed system would improve instruction\nby the faculty and enhance the students' learning experiences. The results show\nthat a modified recurrent neural network with an adapted Grey Wolf Optimizer\nhas the best accuracy when compared with other models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 21:46:34 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Rashid", "Tarik A.", ""], ["Abbas", "Dosti K.", ""], ["Turel", "Yalin K.", ""]]}, {"id": "1903.11936", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr, Andrei Lissovoi, Pietro S. Oliveto", "title": "Evolving Boolean Functions with Conjunctions and Disjunctions via\n  Genetic Programming", "comments": "9 pages. To appear in the proceedings of GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been proved that simple GP systems can efficiently evolve the\nconjunction of $n$ variables if they are equipped with the minimal required\ncomponents. In this paper, we make a considerable step forward by analysing the\nbehaviour and performance of the GP system for evolving a Boolean function with\nunknown components, i.e., the function may consist of both conjunctions and\ndisjunctions. We rigorously prove that if the target function is the\nconjunction of $n$ variables, then the RLS-GP using the complete truth table to\nevaluate program quality evolves the exact target function in $O(\\ell n \\log^2\nn)$ iterations in expectation, where $\\ell \\geq n$ is a limit on the size of\nany accepted tree. When, as in realistic applications, only a polynomial sample\nof possible inputs is used to evaluate solution quality, we show how RLS-GP can\nevolve a conjunction with any polynomially small generalisation error with\nprobability $1 - O(\\log^2(n)/n)$. To produce our results we introduce a\nsuper-multiplicative drift theorem that gives significantly stronger runtime\nbounds when the expected progress is only slightly super-linear in the distance\nfrom the optimum.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 13:08:47 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:37:52 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Doerr", "Benjamin", ""], ["Lissovoi", "Andrei", ""], ["Oliveto", "Pietro S.", ""]]}, {"id": "1903.11991", "submitter": "Maximus Mutschler", "authors": "Maximus Mutschler and Andreas Zell", "title": "Parabolic Approximation Line Search for DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in current optimization research for deep learning is to\nautomatically find optimal step sizes for each update step. The optimal step\nsize is closely related to the shape of the loss in the update step direction.\nHowever, this shape has not yet been examined in detail. This work shows\nempirically that the batch loss over lines in negative gradient direction is\nmostly convex locally and well suited for one-dimensional parabolic\napproximations. By exploiting this parabolic property we introduce a simple and\nrobust line search approach, which performs loss-shape dependent update steps.\nOur approach combines well-known methods such as parabolic approximation, line\nsearch and conjugate gradient, to perform efficiently. It surpasses other step\nsize estimating methods and competes with common optimization methods on a\nlarge variety of experiments without the need of hand-designed step size\nschedules. Thus, it is of interest for objectives where step-size schedules are\nunknown or do not perform well. Our extensive evaluation includes multiple\ncomprehensive hyperparameter grid searches on several datasets and\narchitectures. Finally, we provide a general investigation of exact line\nsearches in the context of batch losses and exact losses, including their\nrelation to our line search approach.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:13:21 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 10:03:30 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 11:17:22 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 14:06:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Mutschler", "Maximus", ""], ["Zell", "Andreas", ""]]}, {"id": "1903.12011", "submitter": "Satish Gajawada", "authors": "Satish Gajawada and Hassan Mustafa", "title": "Novel Artificial Human Optimization Field Algorithms - The Beginning", "comments": "25 pages, 41 figures", "journal-ref": "Transactions on Machine Learning and Artificial Intelligence\n  (TMLAI), Volume 7, Issue 1, February 2019", "doi": "10.14738/tmlai.71.5712", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New Artificial Human Optimization (AHO) Field Algorithms can be created from\nscratch or by adding the concept of Artificial Humans into other existing\nOptimization Algorithms. Particle Swarm Optimization (PSO) has been very\npopular for solving complex optimization problems due to its simplicity. In\nthis work, new Artificial Human Optimization Field Algorithms are created by\nmodifying existing PSO algorithms with AHO Field Concepts. These Hybrid PSO\nAlgorithms comes under PSO Field as well as AHO Field. There are Hybrid PSO\nresearch articles based on Human Behavior, Human Cognition and Human Thinking\netc. But there are no Hybrid PSO articles which based on concepts like Human\nDisease, Human Kindness and Human Relaxation. This paper proposes new AHO Field\nalgorithms based on these research gaps. Some existing Hybrid PSO algorithms\nare given a new name in this work so that it will be easy for future AHO\nresearchers to find these novel Artificial Human Optimization Field Algorithms.\nA total of 6 Artificial Human Optimization Field algorithms titled \"Human\nSafety Particle Swarm Optimization (HuSaPSO)\", \"Human Kindness Particle Swarm\nOptimization (HKPSO)\", \"Human Relaxation Particle Swarm Optimization (HRPSO)\",\n\"Multiple Strategy Human Particle Swarm Optimization (MSHPSO)\", \"Human Thinking\nParticle Swarm Optimization (HTPSO)\" and \"Human Disease Particle Swarm\nOptimization (HDPSO)\" are tested by applying these novel algorithms on Ackley,\nBeale, Bohachevsky, Booth and Three-Hump Camel Benchmark Functions. Results\nobtained are compared with PSO algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 04:26:38 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Gajawada", "Satish", ""], ["Mustafa", "Hassan", ""]]}, {"id": "1903.12073", "submitter": "Vishakha Metre VaM", "authors": "Vishakha A Metre, Mr Pramod B Deshmukh", "title": "Scope of Research on Particle Swarm Optimization Based Data Clustering", "comments": "7 pages, 6 figures, 1 table, published with International Journal of\n  Computer Science Trends and Technology (IJCST)", "journal-ref": "IJCST V6(6): Page(87-93) Nov-Dec 2018. ISSN: 2347-8578", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization is nothing but a mathematical technique which finds maxima or\nminima of any function of concern in some realistic region. Different\noptimization techniques are proposed which are competing for the best solution.\nParticle Swarm Optimization (PSO) is a new, advanced, and most powerful\noptimization methodology that performs empirically well on several optimization\nproblems. It is the extensively used Swarm Intelligence (SI) inspired\noptimization algorithm used for finding the global optimal solution in a\nmultifaceted search region. Data clustering is one of the challenging real\nworld applications that invite the eminent research works in variety of fields.\nApplicability of different PSO variants to data clustering is studied in the\nliterature, and the analyzed research work shows that, PSO variants give poor\nresults for multidimensional data. This paper describes the different\nchallenges associated with multidimensional data clustering and scope of\nresearch on optimizing the clustering problems using PSO. We also propose a\nstrategy to use hybrid PSO variant for clustering multidimensional numerical,\ntext and image data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 17:05:28 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Metre", "Vishakha A", ""], ["Deshmukh", "Mr Pramod B", ""]]}, {"id": "1903.12272", "submitter": "Ruthvik Vaila", "authors": "Ruthvik Vaila, John Chiasson, Vishal Saxena", "title": "Deep Convolutional Spiking Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Spiking neural networks are biologically plausible counterparts of the\nartificial neural networks, artificial neural networks are usually trained with\nstochastic gradient descent and spiking neural networks are trained with spike\ntiming dependant plasticity. Training deep convolutional neural networks is a\nmemory and power intensive job. Spiking networks could potentially help in\nreducing the power usage. There is a large pool of tools for one to chose to\ntrain artificial neural networks of any size, on the other hand all the\navailable tools to simulate spiking neural networks are geared towards\ncomputational neuroscience applications and they are not suitable for real life\napplications. In this work we focus on implementing a spiking CNN using\nTensorflow to examine behaviour of the network and empirically study the effect\nof various parameters on learning capabilities and also study catastrophic\nforgetting in the spiking CNN and weight initialization problem in R-STDP using\nMNIST and N-MNIST data sets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 21:15:08 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 16:06:01 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Vaila", "Ruthvik", ""], ["Chiasson", "John", ""], ["Saxena", "Vishal", ""]]}, {"id": "1903.12330", "submitter": "Abhishek Ramdas Nair", "authors": "P. Kumar, A. R. Nair, O. Chatterjee, T. Paul, A. Ghosh, S.\n  Chakrabartty, C. S. Thakur", "title": "Neuromorphic In-Memory Computing Framework using Memtransistor Cross-bar\n  based Support Vector Machines", "comments": "4 pages, 5 figures, MWSCAS 2019", "journal-ref": "2019 IEEE 62nd International Midwest Symposium on Circuits and\n  Systems (MWSCAS)", "doi": "10.1109/MWSCAS.2019.8885180", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel framework for designing support vector machines\n(SVMs), which does not impose restriction on the SVM kernel to be\npositive-definite and allows the user to define memory constraint in terms of\nfixed template vectors. This makes the framework scalable and enables its\nimplementation for low-power, high-density and memory constrained embedded\napplication. An efficient hardware implementation of the same is also\ndiscussed, which utilizes novel low power memtransistor based cross-bar\narchitecture, and is robust to device mismatch and randomness. We used\nmemtransistor measurement data, and showed that the designed SVMs can achieve\nclassification accuracy comparable to traditional SVMs on both synthetic and\nreal-world benchmark datasets. This framework would be beneficial for design of\nSVM based wake-up systems for internet of things (IoTs) and edge devices where\nmemtransistors can be used to optimize system's energy-efficiency and perform\nin-memory matrix-vector multiplication (MVM).\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 02:38:07 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 16:09:35 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kumar", "P.", ""], ["Nair", "A. R.", ""], ["Chatterjee", "O.", ""], ["Paul", "T.", ""], ["Ghosh", "A.", ""], ["Chakrabartty", "S.", ""], ["Thakur", "C. S.", ""]]}, {"id": "1903.12366", "submitter": "Zehra Sura", "authors": "Zehra Sura, Tong Chen, and Hyojin Sung", "title": "Using Structured Input and Modularity for Improved Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for utilizing the known structure of input data to make\nlearning more efficient. Our work is in the domain of programming languages,\nand we use deep neural networks to do program analysis. Computer programs\ninclude a lot of structural information (such as loop nests, conditional\nblocks, and data scopes), which is pertinent to program analysis. In this case,\nthe neural network has to learn to recognize the structure, and also learn the\ntarget function for the problem. However, the structural information in this\ndomain is readily accessible to software with the availability of compiler\ntools and parsers for well-defined programming languages.\n  Our method for utilizing the known structure of input data includes: (1)\npre-processing the input data to expose relevant structures, and (2)\nconstructing neural networks by incorporating the structure of the input data\nas an integral part of the network design. The method has the effect of\nmodularizing the neural network which helps break down complexity, and results\nin more efficient training of the overall network. We apply this method to an\nexample code analysis problem, and show that it can achieve higher accuracy\nwith a smaller network size and fewer training examples. Further, the method is\nrobust, performing equally well on input data with different distributions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 06:30:32 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Sura", "Zehra", ""], ["Chen", "Tong", ""], ["Sung", "Hyojin", ""]]}, {"id": "1903.12527", "submitter": "Mahmood Amintoosi", "authors": "Hashem Ezzati and Mahmood Amintoosi and Hashem Tabasi", "title": "An Upper Bound for Minimum True Matches in Graph Isomorphism with\n  Simulated Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph matching is one of the most important problems in graph theory and\ncombinatorial optimization, with many applications in various domains. Although\nmeta-heuristic algorithms have had good performance on many NP-Hard and\nNP-Complete problems, for this problem there are not reported superior\nsolutions by these algorithms. The reason of this inefficiency has not been\ninvestigated yet. In this paper we show that simulated annealing as an\nstochastic optimization method is unlikely to be even close to the optimal\nsolution for this problem. In addition to theoretical discussion, the\nexperimental results also verified our idea; for example, in two sample graphs,\nthe probability of reaching to a solution with more than three correct matches\nis about $0.02$ in simulated annealing.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:06:19 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Ezzati", "Hashem", ""], ["Amintoosi", "Mahmood", ""], ["Tabasi", "Hashem", ""]]}, {"id": "1903.12575", "submitter": "Luana Ruiz", "authors": "Luana Ruiz, Fernando Gama, Antonio G. Marques, Alejandro Ribeiro", "title": "Invariance-Preserving Localized Activation Functions for Graph Neural\n  Networks", "comments": "Accepted at TSP", "journal-ref": null, "doi": "10.1109/TSP.2019.2955832", "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph signals are signals with an irregular structure that can be described\nby a graph. Graph neural networks (GNNs) are information processing\narchitectures tailored to these graph signals and made of stacked layers that\ncompose graph convolutional filters with nonlinear activation functions. Graph\nconvolutions endow GNNs with invariance to permutations of the graph nodes'\nlabels. In this paper, we consider the design of trainable nonlinear activation\nfunctions that take into consideration the structure of the graph. This is\naccomplished by using graph median filters and graph max filters, which mimic\nlinear graph convolutions and are shown to retain the permutation invariance of\nGNNs. We also discuss modifications to the backpropagation algorithm necessary\nto train local activation functions. The advantages of localized activation\nfunction architectures are demonstrated in four numerical experiments: source\nlocalization on synthetic graphs, authorship attribution of 19th century\nnovels, movie recommender systems and scientific article classification. In all\ncases, localized activation functions are shown to improve model capacity.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:35:01 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 18:59:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ruiz", "Luana", ""], ["Gama", "Fernando", ""], ["Marques", "Antonio G.", ""], ["Ribeiro", "Alejandro", ""]]}]