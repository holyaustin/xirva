[{"id": "1601.00013", "submitter": "Namig Guliyev", "authors": "Namig J. Guliyev and Vugar E. Ismailov", "title": "A single hidden layer feedforward network with only one neuron in the\n  hidden layer can approximate any univariate function", "comments": "12 pages, 1 figure; to be published in Neural Computation; for\n  associated SageMath worksheet, see\n  http://sites.google.com/site/njguliyev/papers/sigmoidal", "journal-ref": "Neural Computation, 28 (2016), no. 7, 1289-1304", "doi": "10.1162/NECO_a_00849", "report-no": null, "categories": "cs.NE cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility of approximating a continuous function on a compact subset of\nthe real line by a feedforward single hidden layer neural network with a\nsigmoidal activation function has been studied in many papers. Such networks\ncan approximate an arbitrary continuous function provided that an unlimited\nnumber of neurons in a hidden layer is permitted. In this paper, we consider\nconstructive approximation on any finite interval of $\\mathbb{R}$ by neural\nnetworks with only one neuron in the hidden layer. We construct algorithmically\na smooth, sigmoidal, almost monotone activation function $\\sigma$ providing\napproximation to an arbitrary continuous function within any degree of\naccuracy. This algorithm is implemented in a computer program, which computes\nthe value of $\\sigma$ at any reasonable point of the real axis.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 21:32:58 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 12:28:37 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Guliyev", "Namig J.", ""], ["Ismailov", "Vugar E.", ""]]}, {"id": "1601.00034", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Barnabas Poczos, Jeff Schneider, Dale Schuurmans,\n  Russell Greiner", "title": "Stochastic Neural Networks with Monotonic Activation Functions", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Laplace approximation that creates a stochastic unit from any\nsmooth monotonic activation function, using only Gaussian noise. This paper\ninvestigates the application of this stochastic approximation in training a\nfamily of Restricted Boltzmann Machines (RBM) that are closely linked to\nBregman divergences. This family, that we call exponential family RBM\n(Exp-RBM), is a subset of the exponential family Harmoniums that expresses\nfamily members through a choice of smooth monotonic non-linearity for each\nneuron. Using contrastive divergence along with our Gaussian approximation, we\nshow that Exp-RBM can learn useful representations using novel stochastic\nunits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 00:47:29 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 15:38:53 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 15:52:02 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 17:38:18 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""], ["Schuurmans", "Dale", ""], ["Greiner", "Russell", ""]]}, {"id": "1601.00191", "submitter": "Emmanuel Osegi", "authors": "N.E. Osegi", "title": "An Improved Intelligent Agent for Mining Real-Time Databases Using\n  Modified Cortical Learning Algorithms", "comments": "16 pages, 8 figures", "journal-ref": "Advances in Multidisciplinary Research Journal. Vol. 2. No. 2,\n  Issue 1 Pp 47-58", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTM\nhave been developed by Numenta Incorporation from which variations and\nmodifications are currently being investigated upon. HTM offers better promises\nas a future computational model of the neocortex the seat of intelligence in\nthe brain. Currently, intelligent agents are embedded in almost every modern\nday electronic system found in homes, offices and industries worldwide. In this\npaper, we present a first step in realising useful HTM like applications\nspecifically for mining a synthetic and real time dataset based on a novel\nintelligent agent framework, and demonstrate how a modified version of this\nvery important computational technique will lead to improved recognition.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 17:21:14 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Osegi", "N. E.", ""]]}, {"id": "1601.00909", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Johannes Schemmel\n  and Karlheinz Meier", "title": "The high-conductance state enables neural sampling in networks of LIF\n  neurons", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": "10.1186/1471-2202-16-S1-O2", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The apparent stochasticity of in-vivo neural circuits has long been\nhypothesized to represent a signature of ongoing stochastic inference in the\nbrain. More recently, a theoretical framework for neural sampling has been\nproposed, which explains how sample-based inference can be performed by\nnetworks of spiking neurons. One particular requirement of this approach is\nthat the neural response function closely follows a logistic curve.\n  Analytical approaches to calculating neural response functions have been the\nsubject of many theoretical studies. In order to make the problem tractable,\nparticular assumptions regarding the neural or synaptic parameters are usually\nmade. However, biologically significant activity regimes exist which are not\ncovered by these approaches: Under strong synaptic bombardment, as is often the\ncase in cortex, the neuron is shifted into a high-conductance state (HCS)\ncharacterized by a small membrane time constant. In this regime, synaptic time\nconstants and refractory periods dominate membrane dynamics.\n  The core idea of our approach is to separately consider two different \"modes\"\nof spiking dynamics: burst spiking and transient quiescence, in which the\nneuron does not spike for longer periods. We treat the former by propagating\nthe PDF of the effective membrane potential from spike to spike within a burst,\nwhile using a diffusion approximation for the latter. We find that our\nprediction of the neural response function closely matches simulation data.\nMoreover, in the HCS scenario, we show that the neural response function\nbecomes symmetric and can be well approximated by a logistic function, thereby\nproviding the correct dynamics in order to perform neural sampling. We hereby\nprovide not only a normative framework for Bayesian inference in cortex, but\nalso powerful applications of low-power, accelerated neuromorphic systems to\nrelevant machine learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 17:15:37 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Bytschok", "Ilja", ""], ["Bill", "Johannes", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1601.00917", "submitter": "Jie Fu", "authors": "Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, Tat-Seng Chua", "title": "DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing\n  Hyperparameters of Deep Neural Networks", "comments": "International Joint Conference on Artificial Intelligence, IJCAI,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep neural networks is well-known to be sensitive to the\nsetting of their hyperparameters. Recent advances in reverse-mode automatic\ndifferentiation allow for optimizing hyperparameters with gradients. The\nstandard way of computing these gradients involves a forward and backward pass\nof computations. However, the backward pass usually needs to consume\nunaffordable memory to store all the intermediate variables to exactly reverse\nthe forward training procedure. In this work we propose a simple but effective\nmethod, DrMAD, to distill the knowledge of the forward pass into a shortcut\npath, through which we approximately reverse the training trajectory.\nExperiments on several image benchmark datasets show that DrMAD is at least 45\ntimes faster and consumes 100 times less memory compared to state-of-the-art\nmethods for optimizing hyperparameters with minimal compromise to its\neffectiveness. To the best of our knowledge, DrMAD is the first research\nattempt to make it practical to automatically tune thousands of hyperparameters\nof deep neural networks. The code can be downloaded from\nhttps://github.com/bigaidream-projects/drmad\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 17:43:15 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 05:57:51 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 11:43:31 GMT"}, {"version": "v4", "created": "Fri, 5 Feb 2016 05:45:35 GMT"}, {"version": "v5", "created": "Wed, 6 Apr 2016 15:55:19 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Fu", "Jie", ""], ["Luo", "Hongyin", ""], ["Feng", "Jiashi", ""], ["Low", "Kian Hsiang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1601.01607", "submitter": "Juan Juli\\'an Merelo-Guerv\\'os Pr.", "authors": "Juan-J. Merelo, Mario Garc\\'ia-Valdez, Pedro A. Castillo, Pablo\n  Garc\\'ia-S\\'anchez, P. de las Cuevas, Nuria Rico", "title": "NodIO, a JavaScript framework for volunteer-based evolutionary\n  algorithms : first results", "comments": "GeNeura 2006-01", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  JavaScript is an interpreted language mainly known for its inclusion in web\nbrowsers, making them a container for rich Internet based applications. This\nhas inspired its use, for a long time, as a tool for evolutionary algorithms,\nmainly so in browser-based volunteer computing environments. Several libraries\nhave also been published so far and are in use. However, the last years have\nseen a resurgence of interest in the language, becoming one of the most popular\nand thus spawning the improvement of its implementations, which are now the\nfoundation of many new client-server applications. We present such an\napplication for running distributed volunteer-based evolutionary algorithm\nexperiments, and we make a series of measurements to establish the speed of\nJavaScript in evolutionary algorithms that can serve as a baseline for\ncomparison with other distributed computing experiments. These experiments use\ndifferent integer and floating point problems, and prove that the speed of\nJavaScript is actually competitive with other languages commonly used by the\nevolutionary algorithm practitioner.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 17:21:38 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Merelo", "Juan-J.", ""], ["Garc\u00eda-Valdez", "Mario", ""], ["Castillo", "Pedro A.", ""], ["Garc\u00eda-S\u00e1nchez", "Pablo", ""], ["Cuevas", "P. de las", ""], ["Rico", "Nuria", ""]]}, {"id": "1601.01705", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein", "title": "Learning to Compose Neural Networks for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 21:21:59 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 18:20:37 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 01:44:25 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 23:25:51 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Klein", "Dan", ""]]}, {"id": "1601.02539", "submitter": "Zhizheng Wu", "authors": "Zhizheng Wu, Simon King", "title": "Investigating gated recurrent neural networks for speech synthesis", "comments": "Accepted by ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, recurrent neural networks (RNNs) as powerful sequence models have\nre-emerged as a potential acoustic model for statistical parametric speech\nsynthesis (SPSS). The long short-term memory (LSTM) architecture is\nparticularly attractive because it addresses the vanishing gradient problem in\nstandard RNNs, making them easier to train. Although recent studies have\ndemonstrated that LSTMs can achieve significantly better performance on SPSS\nthan deep feed-forward neural networks, little is known about why. Here we\nattempt to answer two questions: a) why do LSTMs work well as a sequence model\nfor SPSS; b) which component (e.g., input gate, output gate, forget gate) is\nmost important. We present a visual analysis alongside a series of experiments,\nresulting in a proposal for a simplified architecture. The simplified\narchitecture has significantly fewer parameters than an LSTM, thus reducing\ngeneration complexity considerably without degrading quality.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 17:54:53 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Wu", "Zhizheng", ""], ["King", "Simon", ""]]}, {"id": "1601.02919", "submitter": "Vincent Andrearczyk", "authors": "Vincent Andrearczyk and Paul F. Whelan", "title": "Using Filter Banks in Convolutional Neural Networks for Texture\n  Classification", "comments": "12 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has established many new state of the art solutions in the last\ndecade in areas such as object, scene and speech recognition. In particular\nConvolutional Neural Network (CNN) is a category of deep learning which obtains\nexcellent results in object detection and recognition tasks. Its architecture\nis indeed well suited to object analysis by learning and classifying complex\n(deep) features that represent parts of an object or the object itself.\nHowever, some of its features are very similar to texture analysis methods. CNN\nlayers can be thought of as filter banks of complexity increasing with the\ndepth. Filter banks are powerful tools to extract texture features and have\nbeen widely used in texture analysis. In this paper we develop a simple network\narchitecture named Texture CNN (T-CNN) which explores this observation. It is\nbuilt on the idea that the overall shape information extracted by the fully\nconnected layers of a classic CNN is of minor importance in texture analysis.\nTherefore, we pool an energy measure from the last convolution layer which we\nconnect to a fully connected layer. We show that our approach can improve the\nperformance of a network while greatly reducing the memory usage and\ncomputation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:38:41 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 10:43:24 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 09:32:52 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 10:24:00 GMT"}, {"version": "v5", "created": "Fri, 23 Sep 2016 09:20:56 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Andrearczyk", "Vincent", ""], ["Whelan", "Paul F.", ""]]}, {"id": "1601.03277", "submitter": "Adenilton Jos\\'e Da Silva", "authors": "Adenilton J. da Silva, Wilson R. de Oliveira and Teresa B. Ludermir", "title": "Weightless neural network parameters and architecture selection in a\n  quantum computer", "comments": null, "journal-ref": "Neurocomputing, Volume 183, 26 March 2016, Pages 13-22", "doi": "10.1016/j.neucom.2015.05.139", "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training artificial neural networks requires a tedious empirical evaluation\nto determine a suitable neural network architecture. To avoid this empirical\nprocess several techniques have been proposed to automatise the architecture\nselection process. In this paper, we propose a method to perform parameter and\narchitecture selection for a quantum weightless neural network (qWNN). The\narchitecture selection is performed through the learning procedure of a qWNN\nwith a learning algorithm that uses the principle of quantum superposition and\na non-linear quantum operator. The main advantage of the proposed method is\nthat it performs a global search in the space of qWNN architecture and\nparameters rather than a local search.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 12:53:04 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["da Silva", "Adenilton J.", ""], ["de Oliveira", "Wilson R.", ""], ["Ludermir", "Teresa B.", ""]]}, {"id": "1601.03481", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash, H.S. Behera", "title": "A Fuzzy MLP Approach for Non-linear Pattern Classification", "comments": "The final version of this paper has been published in \"International\n  Conference on Communication and Computing (ICC-2014)\"\n  [http://www.elsevierst.com/conference_book_download_chapter.php?cbid=86#chapter41]", "journal-ref": "In Proc: K.R. Venugopal, S.C. Lingareddy (eds.) International\n  Conference on Communication and Computing (ICC- 2014), Bangalore, India (June\n  12-14, 2014), Computer Networks and Security, 314-323", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In case of decision making problems, classification of pattern is a complex\nand crucial task. Pattern classification using multilayer perceptron (MLP)\ntrained with back propagation learning becomes much complex with increase in\nnumber of layers, number of nodes and number of epochs and ultimate increases\ncomputational time [31]. In this paper, an attempt has been made to use fuzzy\nMLP and its learning algorithm for pattern classification. The time and space\ncomplexities of the algorithm have been analyzed. A training performance\ncomparison has been carried out between MLP and the proposed fuzzy-MLP model by\nconsidering six cases. Results are noted against different learning rates\nranging from 0 to 1. A new performance evaluation factor 'convergence gain' has\nbeen introduced. It is observed that the number of epochs drastically reduced\nand performance increased compared to MLP. The average and minimum gain has\nbeen found to be 93% and 75% respectively. The best gain is found to be 95% and\nis obtained by setting the learning rate to 0.55.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 12:45:19 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Behera", "H. S.", ""]]}, {"id": "1601.03649", "submitter": "Brian Gardner BG", "authors": "Brian Gardner and Andr\\'e Gr\\\"uning", "title": "Supervised Learning in Spiking Neural Networks for Precise Temporal\n  Encoding", "comments": "26 pages, 10 figures, this version is published in PLoS ONE and\n  incorporates reviewer comments", "journal-ref": null, "doi": "10.1371/journal.pone.0161335", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise spike timing as a means to encode information in neural networks is\nbiologically supported, and is advantageous over frequency-based codes by\nprocessing input features on a much shorter time-scale. For these reasons, much\nrecent attention has been focused on the development of supervised learning\nrules for spiking neural networks that utilise a temporal coding scheme.\nHowever, despite significant progress in this area, there still lack rules that\nhave a theoretical basis, and yet can be considered biologically relevant. Here\nwe examine the general conditions under which synaptic plasticity most\neffectively takes place to support the supervised learning of a precise\ntemporal code. As part of our analysis we examine two spike-based learning\nmethods: one of which relies on an instantaneous error signal to modify\nsynaptic weights in a network (INST rule), and the other one on a filtered\nerror signal for smoother synaptic weight modifications (FILT rule). We test\nthe accuracy of the solutions provided by each rule with respect to their\ntemporal encoding precision, and then measure the maximum number of input\npatterns they can learn to memorise using the precise timings of individual\nspikes as an indication of their storage capacity. Our results demonstrate the\nhigh performance of FILT in most cases, underpinned by the rule's\nerror-filtering mechanism, which is predicted to provide smooth convergence\ntowards a desired solution during learning. We also find FILT to be most\nefficient at performing input pattern memorisations, and most noticeably when\npatterns are identified using spikes with sub-millisecond temporal precision.\nIn comparison with existing work, we determine the performance of FILT to be\nconsistent with that of the highly efficient E-learning Chronotron, but with\nthe distinct advantage that FILT is also implementable as an online method for\nincreased biological realism.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 16:28:32 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 18:35:05 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Gardner", "Brian", ""], ["Gr\u00fcning", "Andr\u00e9", ""]]}, {"id": "1601.03809", "submitter": "Mostafa Sayyed", "authors": "Mostafa Sayyed", "title": "Artificial neural network approach for condition-based maintenance", "comments": "108 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, computerized maintenance management will be investigated.\nThe rise of maintenance cost forced the research community to look for more\neffective ways to schedule maintenance operations. Using computerized models to\ncome up with optimal maintenance policy has led to better equipment utilization\nand lower costs. This research adopts Condition-Based Maintenance model where\nthe maintenance decision is generated based on equipment conditions. Artificial\nNeural Network technique is proposed to capture and analyze equipment condition\nsignals which lead to higher level of knowledge gathering. This knowledge is\nused to accurately estimate equipment failure time. Based on these estimations,\nan optimal maintenance management policy can be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 07:21:44 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Sayyed", "Mostafa", ""]]}, {"id": "1601.04155", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, and\n  Thomas S. Huang", "title": "Brain-Inspired Deep Networks for Image Aesthetics Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetics assessment has been challenging due to its subjective\nnature. Inspired by the scientific advances in the human visual perception and\nneuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task.\nBDN first learns attributes through the parallel supervised pathways, on a\nvariety of selected feature dimensions. A high-level synthesis network is\ntrained to associate and transform those attributes into the overall aesthetics\nrating. We then extend BDN to predicting the distribution of human ratings,\nsince aesthetics ratings are often subjective. Another highlight is our\nfirst-of-its-kind study of label-preserving transformations in the context of\naesthetics assessment, which leads to an effective data augmentation approach.\nExperimental results on the AVA dataset show that our biological inspired and\ntask-specific BDN model gains significantly performance improvement, compared\nto other state-of-the-art models with the same or higher parameter capacity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:59:40 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 03:46:27 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Dolcos", "Florin", ""], ["Beck", "Diane", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04183", "submitter": "Peter Diehl Peter U. Diehl", "authors": "Peter U. Diehl, Bruno U. Pedroni, Andrew Cassidy, Paul Merolla, Emre\n  Neftci and Guido Zarrella", "title": "TrueHappiness: Neuromorphic Emotion Recognition on TrueNorth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to constructing a neuromorphic device that responds to\nlanguage input by producing neuron spikes in proportion to the strength of the\nappropriate positive or negative emotional response. Specifically, we perform a\nfine-grained sentiment analysis task with implementations on two different\nsystems: one using conventional spiking neural network (SNN) simulators and the\nother one using IBM's Neurosynaptic System TrueNorth. Input words are projected\ninto a high-dimensional semantic space and processed through a fully-connected\nneural network (FCNN) containing rectified linear units trained via\nbackpropagation. After training, this FCNN is converted to a SNN by\nsubstituting the ReLUs with integrate-and-fire neurons. We show that there is\npractically no performance loss due to conversion to a spiking network on a\nsentiment analysis test set, i.e. correlations between predictions and human\nannotations differ by less than 0.02 comparing the original DNN and its spiking\nequivalent. Additionally, we show that the SNN generated with this technique\ncan be mapped to existing neuromorphic hardware -- in our case, the TrueNorth\nchip. Mapping to the chip involves 4-bit synaptic weight discretization and\nadjustment of the neuron thresholds. The resulting end-to-end system can take a\nuser input, i.e. a word in a vocabulary of over 300,000 words, and estimate its\nsentiment on TrueNorth with a power consumption of approximately 50 uW.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 17:04:25 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Diehl", "Peter U.", ""], ["Pedroni", "Bruno U.", ""], ["Cassidy", "Andrew", ""], ["Merolla", "Paul", ""], ["Neftci", "Emre", ""], ["Zarrella", "Guido", ""]]}, {"id": "1601.04187", "submitter": "Peter Diehl Peter U. Diehl", "authors": "Peter U. Diehl, Guido Zarrella, Andrew Cassidy, Bruno U. Pedroni and\n  Emre Neftci", "title": "Conversion of Artificial Recurrent Neural Networks to Spiking Neural\n  Networks for Low-power Neuromorphic Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the field of neuromorphic low-power systems that consume\norders of magnitude less power gained significant momentum. However, their\nwider use is still hindered by the lack of algorithms that can harness the\nstrengths of such architectures. While neuromorphic adaptations of\nrepresentation learning algorithms are now emerging, efficient processing of\ntemporal sequences or variable length-inputs remain difficult. Recurrent neural\nnetworks (RNN) are widely used in machine learning to solve a variety of\nsequence learning tasks. In this work we present a train-and-constrain\nmethodology that enables the mapping of machine learned (Elman) RNNs on a\nsubstrate of spiking neurons, while being compatible with the capabilities of\ncurrent and near-future neuromorphic systems. This \"train-and-constrain\" method\nconsists of first training RNNs using backpropagation through time, then\ndiscretizing the weights and finally converting them to spiking RNNs by\nmatching the responses of artificial neurons with those of the spiking neurons.\nWe demonstrate our approach by mapping a natural language processing task\n(question classification), where we demonstrate the entire mapping process of\nthe recurrent layer of the network on IBM's Neurosynaptic System \"TrueNorth\", a\nspike-based digital neuromorphic hardware architecture. TrueNorth imposes\nspecific constraints on connectivity, neural and synaptic parameters. To\nsatisfy these constraints, it was necessary to discretize the synaptic weights\nand neural activities to 16 levels, and to limit fan-in to 64 inputs. We find\nthat short synaptic delays are sufficient to implement the dynamical (temporal)\naspect of the RNN in the question classification task. The hardware-constrained\nmodel achieved 74% accuracy in question classification while using less than\n0.025% of the cores on one TrueNorth chip, resulting in an estimated power\nconsumption of ~17 uW.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 17:48:34 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Diehl", "Peter U.", ""], ["Zarrella", "Guido", ""], ["Cassidy", "Andrew", ""], ["Pedroni", "Bruno U.", ""], ["Neftci", "Emre", ""]]}, {"id": "1601.04296", "submitter": "Adel Ammar", "authors": "Adel Ammar, Sylvie Labroue, Estelle Obligis, Michel Cr\\'epon, and\n  Sylvie Thiria", "title": "Building a Learning Database for the Neural Network Retrieval of Sea\n  Surface Salinity from SMOS Brightness Temperatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with an important aspect of the neural network retrieval\nof sea surface salinity (SSS) from SMOS brightness temperatures (TBs). The\nneural network retrieval method is an empirical approach that offers the\npossibility of being independent from any theoretical emissivity model, during\nthe in-flight phase. A Previous study [1] has proven that this approach is\napplicable to all pixels on ocean, by designing a set of neural networks with\ndifferent inputs. The present study focuses on the choice of the learning\ndatabase and demonstrates that a judicious distribution of the geophysical\nparameters allows to markedly reduce the systematic regional biases of the\nretrieved SSS, which are due to the high noise on the TBs. An equalization of\nthe distribution of the geophysical parameters, followed by a new technique for\nboosting the learning process, makes the regional biases almost disappear for\nlatitudes between 40{\\deg}S and 40{\\deg}N, while the global standard deviation\nremains between 0.6 psu (at the center of the of the swath) and 1 psu (at the\nedges).\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 13:56:38 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Ammar", "Adel", ""], ["Labroue", "Sylvie", ""], ["Obligis", "Estelle", ""], ["Cr\u00e9pon", "Michel", ""], ["Thiria", "Sylvie", ""]]}, {"id": "1601.04862", "submitter": "Christoph Richter", "authors": "Christoph Richter, S\\\"oren Jentzsch, Rafael Hostettler, Jes\\'us A.\n  Garrido, Eduardo Ros, Alois C. Knoll, Florian R\\\"ohrbein, Patrick van der\n  Smagt, J\\\"org Conradt", "title": "Scalability in Neural Control of Musculoskeletal Robots", "comments": "Accepted at IEEE Robotics and Automation Magazine on 2015-12-31", "journal-ref": null, "doi": "10.1109/MRA.2016.2535081", "report-no": null, "categories": "cs.RO cs.DC cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anthropomimetic robots are robots that sense, behave, interact and feel like\nhumans. By this definition, anthropomimetic robots require human-like physical\nhardware and actuation, but also brain-like control and sensing. The most\nself-evident realization to meet those requirements would be a human-like\nmusculoskeletal robot with a brain-like neural controller. While both\nmusculoskeletal robotic hardware and neural control software have existed for\ndecades, a scalable approach that could be used to build and control an\nanthropomimetic human-scale robot has not been demonstrated yet. Combining\nMyorobotics, a framework for musculoskeletal robot development, with SpiNNaker,\na neuromorphic computing platform, we present the proof-of-principle of a\nsystem that can scale to dozens of neurally-controlled, physically compliant\njoints. At its core, it implements a closed-loop cerebellar model which\nprovides real-time low-level neural control at minimal power consumption and\nmaximal extensibility: higher-order (e.g., cortical) neural networks and\nneuromorphic sensors like silicon-retinae or -cochleae can naturally be\nincorporated.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 10:29:12 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Richter", "Christoph", ""], ["Jentzsch", "S\u00f6ren", ""], ["Hostettler", "Rafael", ""], ["Garrido", "Jes\u00fas A.", ""], ["Ros", "Eduardo", ""], ["Knoll", "Alois C.", ""], ["R\u00f6hrbein", "Florian", ""], ["van der Smagt", "Patrick", ""], ["Conradt", "J\u00f6rg", ""]]}, {"id": "1601.05409", "submitter": "Mitra Montazeri", "authors": "Mitra Montazeri, Mahdieh Soleymani Baghshah, Aliakbar Niknafs", "title": "Selecting Efficient Features via a Hyper-Heuristic Approach", "comments": "The Fifth Iran Data Mining Conference (IDMC 2011), Amirkabir\n  University of Technology, Tehran, Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By Emerging huge databases and the need to efficient learning algorithms on\nthese datasets, new problems have appeared and some methods have been proposed\nto solve these problems by selecting efficient features. Feature selection is a\nproblem of finding efficient features among all features in which the final\nfeature set can improve accuracy and reduce complexity. One way to solve this\nproblem is to evaluate all possible feature subsets. However, evaluating all\npossible feature subsets is an exhaustive search and thus it has high\ncomputational complexity. Until now many heuristic algorithms have been studied\nfor solving this problem. Hyper-heuristic is a new heuristic approach which can\nsearch the solution space effectively by applying local searches appropriately.\nEach local search is a neighborhood searching algorithm. Since each region of\nthe solution space can have its own characteristics, it should be chosen an\nappropriate local search and apply it to current solution. This task is tackled\nto a supervisor. The supervisor chooses a local search based on the functional\nhistory of local searches. By doing this task, it can trade of between\nexploitation and exploration. Since the existing heuristic cannot trade of\nbetween exploration and exploitation appropriately, the solution space has not\nbeen searched appropriately in these methods and thus they have low convergence\nrate. For the first time, in this paper use a hyper-heuristic approach to find\nan efficient feature subset. In the proposed method, genetic algorithm is used\nas a supervisor and 16 heuristic algorithms are used as local searches.\nEmpirical study of the proposed method on several commonly used data sets from\nUCI data sets indicates that it outperforms recent existing methods in the\nliterature for feature selection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 20:59:55 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Montazeri", "Mitra", ""], ["Baghshah", "Mahdieh Soleymani", ""], ["Niknafs", "Aliakbar", ""]]}, {"id": "1601.05654", "submitter": "Nikolaos Gianniotis", "authors": "Nikolaos Gianniotis and Sven D. K\\\"ugler and Peter Ti\\v{n}o and Kai L.\n  Polsterer", "title": "Model-Coupled Autoencoder for Time Series Visualisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the visualisation of a set of time series that\ncombines an echo state network with an autoencoder. For each time series in the\ndataset we train an echo state network, using a common and fixed reservoir of\nhidden neurons, and use the optimised readout weights as the new\nrepresentation. Dimensionality reduction is then performed via an autoencoder\non the readout weight representations. The crux of the work is to equip the\nautoencoder with a loss function that correctly interprets the reconstructed\nreadout weights by associating them with a reconstruction error measured in the\ndata space of sequences. This essentially amounts to measuring the predictive\nperformance that the reconstructed readout weights exhibit on their\ncorresponding sequences when plugged back into the echo state network with the\nsame fixed reservoir. We demonstrate that the proposed visualisation framework\ncan deal both with real valued sequences as well as binary sequences. We derive\nmagnification factors in order to analyse distance preservations and\ndistortions in the visualisation space. The versatility and advantages of the\nproposed method are demonstrated on datasets of time series that originate from\ndiverse domains.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 14:26:21 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Gianniotis", "Nikolaos", ""], ["K\u00fcgler", "Sven D.", ""], ["Ti\u0148o", "Peter", ""], ["Polsterer", "Kai L.", ""]]}, {"id": "1601.05911", "submitter": "N. Michael Mayer", "authors": "Norbert Michael Mayer, Ying-Hao Yu", "title": "Orthogonal Echo State Networks and stochastic evaluations of likelihoods", "comments": null, "journal-ref": "Cogn Comput (2017) 9:379-390", "doi": "10.1007/s12559-017-9466-4", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report about probabilistic likelihood estimates that are performed on time\nseries using an echo state network with orthogonal recurrent connectivity. The\nresults from tests using synthetic stochastic input time series with temporal\ninference indicate that the capability of the network to infer depends on the\nbalance between input strength and recurrent activity. This balance has an\ninfluence on the network with regard to the quality of inference from the short\nterm input history versus inference that accounts for influences that date back\na long time. Sensitivity of such networks against noise and the finite accuracy\nof network states in the recurrent layer are investigated. In addition, a\nmeasure based on mutual information between the output time series and the\nreservoir is introduced. Finally, different types of recurrent connectivity are\nevaluated. Orthogonal matrices show the best results of all investigated\nconnectivity types overall, but also in the way how the network performance\nscales with the size of the recurrent layer.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 09:01:45 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 08:24:42 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 11:31:34 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 13:49:18 GMT"}, {"version": "v5", "created": "Tue, 13 Jun 2017 11:58:12 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Mayer", "Norbert Michael", ""], ["Yu", "Ying-Hao", ""]]}, {"id": "1601.06008", "submitter": "Mahmood Yousefi-Azar", "authors": "Mahmood Yousefi-Azar, Farbod Razzazi", "title": "A Robust Frame-based Nonlinear Prediction System for Automatic Speech\n  Coding", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a neural-based coding scheme in which an artificial\nneural network is exploited to automatically compress and decompress speech\nsignals by a trainable approach. Having a two-stage training phase, the system\ncan be fully specified to each speech frame and have robust performance across\ndifferent speakers and wide range of spoken utterances. Indeed, Frame-based\nnonlinear predictive coding (FNPC) would code a frame in the procedure of\ntraining to predict the frame samples. The motivating objective is to analyze\nthe system behavior in regenerating not only the envelope of spectra, but also\nthe spectra phase. This scheme has been evaluated in time and discrete cosine\ntransform (DCT) domains and the output of predicted phonemes show the\npotentiality of the FNPC to reconstruct complicated signals. The experiments\nwere conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains\nversus the number of neurons in the hidden layer. Experiments approve the FNPC\ncapability as an automatic coding system by which /b/d/g/ phonemes have been\nreproduced with a good accuracy. Evaluations revealed that the performance of\nFNPC system, trained to predict DCT coefficients is more desirable,\nparticularly for frames with the wider distribution of energy, compared to time\nsamples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 14:13:53 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Yousefi-Azar", "Mahmood", ""], ["Razzazi", "Farbod", ""]]}, {"id": "1601.06071", "submitter": "Minje Kim", "authors": "Minje Kim and Paris Smaragdis", "title": "Bitwise Neural Networks", "comments": "This paper was presented at the International Conference on Machine\n  Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille,\n  France, Jul. 6-11, 2015", "journal-ref": "International Conference on Machine Learning (ICML) Workshop on\n  Resource-Efficient Machine Learning, Lille, France, Jul. 6-11, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the assumption that there exists a neural network that efficiently\nrepresents a set of Boolean functions between all binary inputs and outputs, we\npropose a process for developing and deploying neural networks whose weight\nparameters, bias terms, input, and intermediate hidden layer output signals,\nare all binary-valued, and require only basic bit logic for the feedforward\npass. The proposed Bitwise Neural Network (BNN) is especially suitable for\nresource-constrained environments, since it replaces either floating or\nfixed-point arithmetic with significantly more efficient bitwise operations.\nHence, the BNN requires for less spatial complexity, less memory bandwidth, and\nless power consumption in hardware. In order to design such networks, we\npropose to add a few training schemes, such as weight compression and noisy\nbackpropagation, which result in a bitwise network that performs almost as well\nas its corresponding real-valued network. We test the proposed network on the\nMNIST dataset, represented using binary features, and show that BNNs result in\ncompetitive performance while offering dramatic computational savings.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:59:01 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Kim", "Minje", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1601.06580", "submitter": "Christian Napoli", "authors": "Dawid Polap, Marcin Wozniak, Christian Napoli, Emiliano Tramontana", "title": "Is swarm intelligence able to create mazes?", "comments": null, "journal-ref": "International Journal of Electronics and Telecommunications, Vol.\n  6, n. 4, pp. 305-310 (2015)", "doi": "10.1515/eletel-2015-0039", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the idea of applying Computational Intelligence in the process\nof creation board games, in particular mazes, is presented. For two different\nalgorithms the proposed idea has been examined. The results of the experiments\nare shown and discussed to present advantages and disadvantages.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 12:49:28 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Polap", "Dawid", ""], ["Wozniak", "Marcin", ""], ["Napoli", "Christian", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1601.06581", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Wonyong Sung", "title": "Character-Level Incremental Speech Recognition with Recurrent Neural\n  Networks", "comments": "To appear in ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472696", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-time speech recognition applications, the latency is an important\nissue. We have developed a character-level incremental speech recognition (ISR)\nsystem that responds quickly even during the speech, where the hypotheses are\ngradually improved while the speaking proceeds. The algorithm employs a\nspeech-to-character unidirectional recurrent neural network (RNN), which is\nend-to-end trained with connectionist temporal classification (CTC), and an\nRNN-based character-level language model (LM). The output values of the\nCTC-trained RNN are character-level probabilities, which are processed by beam\nsearch decoding. The RNN LM augments the decoding by providing long-term\ndependency information. We propose tree-based online beam search with\nadditional depth-pruning, which enables the system to process infinitely long\ninput speech with low latency. This system not only responds quickly on speech\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\ntraining set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 12:51:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 11:03:05 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1601.06733", "submitter": "Jianpeng Cheng J", "authors": "Jianpeng Cheng, Li Dong, Mirella Lapata", "title": "Long Short-Term Memory-Networks for Machine Reading", "comments": "Published as a conference paper at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 19:25:48 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 20:48:02 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2016 14:29:04 GMT"}, {"version": "v4", "created": "Thu, 17 Mar 2016 13:28:16 GMT"}, {"version": "v5", "created": "Thu, 7 Apr 2016 09:53:49 GMT"}, {"version": "v6", "created": "Wed, 1 Jun 2016 12:27:42 GMT"}, {"version": "v7", "created": "Tue, 20 Sep 2016 21:20:09 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Dong", "Li", ""], ["Lapata", "Mirella", ""]]}, {"id": "1601.06759", "submitter": "A\\\"aron van den Oord", "authors": "Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu", "title": "Pixel Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:34:24 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 15:32:16 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 14:10:16 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Oord", "Aaron van den", ""], ["Kalchbrenner", "Nal", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1601.06815", "submitter": "Tyler Highlander", "authors": "Tyler Highlander and Andres Rodriguez", "title": "Very Efficient Training of Convolutional Neural Networks using Fast\n  Fourier Transform and Overlap-and-Add", "comments": "British Machine Vision Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are currently state-of-the-art for\nvarious classification tasks, but are computationally expensive. Propagating\nthrough the convolutional layers is very slow, as each kernel in each layer\nmust sequentially calculate many dot products for a single forward and backward\npropagation which equates to $\\mathcal{O}(N^{2}n^{2})$ per kernel per layer\nwhere the inputs are $N \\times N$ arrays and the kernels are $n \\times n$\narrays. Convolution can be efficiently performed as a Hadamard product in the\nfrequency domain. The bottleneck is the transformation which has a cost of\n$\\mathcal{O}(N^{2}\\log_2 N)$ using the fast Fourier transform (FFT). However,\nthe increase in efficiency is less significant when $N\\gg n$ as is the case in\nCNNs. We mitigate this by using the \"overlap-and-add\" technique reducing the\ncomputational complexity to $\\mathcal{O}(N^2\\log_2 n)$ per kernel. This method\nincreases the algorithm's efficiency in both the forward and backward\npropagation, reducing the training and testing time for CNNs. Our empirical\nresults show our method reduces computational time by a factor of up to 16.3\ntimes the traditional convolution implementation for a 8 $\\times$ 8 kernel and\na 224 $\\times$ 224 image.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 21:29:11 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Highlander", "Tyler", ""], ["Rodriguez", "Andres", ""]]}, {"id": "1601.07213", "submitter": "Alexander Ororbia II", "authors": "Alexander G. Ororbia II, C. Lee Giles, and Daniel Kifer", "title": "Unifying Adversarial Training Algorithms with Flexible Deep Data\n  Gradient Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous proposals for adversarial training of deep neural nets have\nincluded di- rectly modifying the gradient, training on a mix of original and\nadversarial examples, using contractive penalties, and approximately optimizing\nconstrained adversarial ob- jective functions. In this paper, we show these\nproposals are actually all instances of optimizing a general, regularized\nobjective we call DataGrad. Our proposed DataGrad framework, which can be\nviewed as a deep extension of the layerwise contractive au- toencoder penalty,\ncleanly simplifies prior work and easily allows extensions such as adversarial\ntraining with multi-task cues. In our experiments, we find that the deep gra-\ndient regularization of DataGrad (which also has L1 and L2 flavors of\nregularization) outperforms alternative forms of regularization, including\nclassical L1, L2, and multi- task, both on the original dataset as well as on\nadversarial sets. Furthermore, we find that combining multi-task optimization\nwith DataGrad adversarial training results in the most robust performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 22:41:13 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 20:40:13 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 15:36:19 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Ororbia", "Alexander G.", "II"], ["Giles", "C. Lee", ""], ["Kifer", "Daniel", ""]]}, {"id": "1601.07227", "submitter": "Veit Elser", "authors": "Veit Elser", "title": "A network that learns Strassen multiplication", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study neural networks whose only non-linear components are multipliers, to\ntest a new training rule in a context where the precise representation of data\nis paramount. These networks are challenged to discover the rules of matrix\nmultiplication, given many examples. By limiting the number of multipliers, the\nnetwork is forced to discover the Strassen multiplication rules. This is the\nmathematical equivalent of finding low rank decompositions of the $n\\times n$\nmatrix multiplication tensor, $M_n$. We train these networks with the\nconservative learning rule, which makes minimal changes to the weights so as to\ngive the correct output for each input at the time the input-output pair is\nreceived. Conservative learning needs a few thousand examples to find the rank\n7 decomposition of $M_2$, and $10^5$ for the rank 23 decomposition of $M_3$\n(the lowest known). High precision is critical, especially for $M_3$, to\ndiscriminate between true decompositions and \"border approximations\".\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 23:45:56 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Elser", "Veit", ""]]}, {"id": "1601.07446", "submitter": "Christian Napoli", "authors": "Marcin Wozniak, Dawid Polap, Grzegorz Borowik, Christian Napoli", "title": "A First Attempt to Cloud-Based User Verification in Distributed System", "comments": "Final version published on: Asia-Pacific Conference on Computer Aided\n  System Engineering (APCASE), pp. 226-231 (2015)", "journal-ref": "Asia-Pacific Conference on Computer Aided System Engineering\n  (APCASE), pp. 226-231 (2015)", "doi": "10.1109/APCASE.2015.47", "report-no": null, "categories": "cs.NE cs.AI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the idea of client verification in distributed systems is\npresented. The proposed solution presents a sample system where client\nverification through cloud resources using input signature is discussed. For\ndifferent signatures the proposed method has been examined. Research results\nare presented and discussed to show potential advantages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 16:54:51 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Wozniak", "Marcin", ""], ["Polap", "Dawid", ""], ["Borowik", "Grzegorz", ""], ["Napoli", "Christian", ""]]}, {"id": "1601.07596", "submitter": "Francisco Chicano", "authors": "Francisco Chicano, Darrell Whitley and Renato Tinos", "title": "Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization", "comments": "Paper accepted for publication in the 16th European Conference on\n  Evolutionary Computation for Combinatorial Optimisation (EvoCOP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search algorithms and iterated local search algorithms are a basic\ntechnique. Local search can be a stand along search methods, but it can also be\nhybridized with evolutionary algorithms. Recently, it has been shown that it is\npossible to identify improving moves in Hamming neighborhoods for k-bounded\npseudo-Boolean optimization problems in constant time. This means that local\nsearch does not need to enumerate neighborhoods to find improving moves. It\nalso means that evolutionary algorithms do not need to use random mutation as a\noperator, except perhaps as a way to escape local optima. In this paper, we\nshow how improving moves can be identified in constant time for multiobjective\nproblems that are expressed as k-bounded pseudo-Boolean functions. In\nparticular, multiobjective forms of NK Landscapes and Mk Landscapes are\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 23:35:05 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Chicano", "Francisco", ""], ["Whitley", "Darrell", ""], ["Tinos", "Renato", ""]]}, {"id": "1601.07925", "submitter": "Randal Olson", "authors": "Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A.\n  Lavender, La Creis Kidd, Jason H. Moore", "title": "Automating biomedical data science through tree-based pipeline\n  optimization", "comments": "16 pages, 5 figures, to appear in EvoBIO 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, data science and machine learning has grown from a\nmysterious art form to a staple tool across a variety of fields in academia,\nbusiness, and government. In this paper, we introduce the concept of tree-based\npipeline optimization for automating one of the most tedious parts of machine\nlearning---pipeline design. We implement a Tree-based Pipeline Optimization\nTool (TPOT) and demonstrate its effectiveness on a series of simulated and\nreal-world genetic data sets. In particular, we show that TPOT can build\nmachine learning pipelines that achieve competitive classification accuracy and\ndiscover novel pipeline operators---such as synthetic feature\nconstructors---that significantly improve classification accuracy on these data\nsets. We also highlight the current challenges to pipeline optimization, such\nas the tendency to produce pipelines that overfit the data, and suggest future\nresearch paths to overcome these challenges. As such, this work represents an\nearly step toward fully automating machine learning pipeline design.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 21:45:55 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Olson", "Randal S.", ""], ["Urbanowicz", "Ryan J.", ""], ["Andrews", "Peter C.", ""], ["Lavender", "Nicole A.", ""], ["Kidd", "La Creis", ""], ["Moore", "Jason H.", ""]]}]