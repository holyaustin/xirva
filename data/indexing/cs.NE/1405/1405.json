[{"id": "1405.0549", "submitter": "Omar S. Soliman", "authors": "Omar S. Soliman, Eman AboElhamd", "title": "Classification of Diabetes Mellitus using Modified Particle Swarm\n  Optimization and Least Squares Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Diabetes Mellitus is a major health problem all over the world. Many\nclassification algorithms have been applied for its diagnoses and treatment. In\nthis paper, a hybrid algorithm of Modified-Particle Swarm Optimization and\nLeast Squares- Support Vector Machine is proposed for the classification of\ntype II DM patients. LS-SVM algorithm is used for classification by finding\noptimal hyper-plane which separates various classes. Since LS-SVM is so\nsensitive to the changes of its parameter values, Modified-PSO algorithm is\nused as an optimization technique for LS-SVM parameters. This will Guarantee\nthe robustness of the hybrid algorithm by searching for the optimal values for\nLS-SVM parameters. The pro-posed Algorithm is implemented and evaluated using\nPima Indians Diabetes Data set from UCI repository of machine learning\ndatabases. It is also compared with different classifier algorithms which were\napplied on the same database. The experimental results showed the superiority\nof the proposed algorithm which could achieve an average classification\naccuracy of 97.833%.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 02:31:07 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Soliman", "Omar S.", ""], ["AboElhamd", "Eman", ""]]}, {"id": "1405.0573", "submitter": "Lucas Antiqueira", "authors": "Lucas Antiqueira and Liang Zhao", "title": "Spatial Neural Networks and their Functional Samples: Similarities and\n  Differences", "comments": "10 pages, 6 figures (submitted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of neural networks have proven their utility in the development of\nlearning algorithms in computer science and in the theoretical study of brain\ndynamics in computational neuroscience. We propose in this paper a spatial\nneural network model to analyze the important class of functional networks,\nwhich are commonly employed in computational studies of clinical brain imaging\ntime series. We developed a simulation framework inspired by multichannel brain\nsurface recordings (more specifically, EEG -- electroencephalogram) in order to\nlink the mesoscopic network dynamics (represented by sampled functional\nnetworks) and the microscopic network structure (represented by an\nintegrate-and-fire neural network located in a 3D space -- hence the term\nspatial neural network). Functional networks are obtained by computing pairwise\ncorrelations between time-series of mesoscopic electric potential dynamics,\nwhich allows the construction of a graph where each node represents one\ntime-series. The spatial neural network model is central in this study in the\nsense that it allowed us to characterize sampled functional networks in terms\nof what features they are able to reproduce from the underlying spatial\nnetwork. Our modeling approach shows that, in specific conditions of sample\nsize and edge density, it is possible to precisely estimate several network\nmeasurements of spatial networks by just observing functional samples.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 11:58:27 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Antiqueira", "Lucas", ""], ["Zhao", "Liang", ""]]}, {"id": "1405.0931", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Fabio L. Traversa and Massimiliano Di Ventra", "title": "Universal Memcomputing Machines", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, vol.\n  26, is. 11, pgs. 2702 - 2715, year 2015", "doi": "10.1109/TNNLS.2015.2391182", "report-no": null, "categories": "cs.NE cond-mat.mes-hall cs.ET cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of universal memcomputing machines (UMMs): a class of\nbrain-inspired general-purpose computing machines based on systems with memory,\nwhereby processing and storing of information occur on the same physical\nlocation. We analytically prove that the memory properties of UMMs endow them\nwith universal computing power - they are Turing-complete -, intrinsic\nparallelism, functional polymorphism, and information overhead, namely their\ncollective states can support exponential data compression directly in memory.\nWe also demonstrate that a UMM has the same computational power as a\nnon-deterministic Turing machine, namely it can solve NP--complete problems in\npolynomial time. However, by virtue of its information overhead, a UMM needs\nonly an amount of memory cells (memprocessors) that grows polynomially with the\nproblem size. As an example we provide the polynomial-time solution of the\nsubset-sum problem and a simple hardware implementation of the same. Even\nthough these results do not prove the statement NP=P within the Turing\nparadigm, the practical realization of these UMMs would represent a paradigm\nshift from present von Neumann architectures bringing us closer to brain-like\nneural computation.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 15:34:29 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 23:01:53 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Traversa", "Fabio L.", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1405.1380", "submitter": "Yingbo Zhou", "authors": "Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju", "title": "Is Joint Training Better for Deep Auto-Encoders?", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, when generative models of data are developed via deep\narchitectures, greedy layer-wise pre-training is employed. In a well-trained\nmodel, the lower layer of the architecture models the data distribution\nconditional upon the hidden variables, while the higher layers model the hidden\ndistribution prior. But due to the greedy scheme of the layerwise training\ntechnique, the parameters of lower layers are fixed when training higher\nlayers. This makes it extremely challenging for the model to learn the hidden\ndistribution prior, which in turn leads to a suboptimal model for the data\ndistribution. We therefore investigate joint training of deep autoencoders,\nwhere the architecture is viewed as one stack of two or more single-layer\nautoencoders. A single global reconstruction objective is jointly optimized,\nsuch that the objective for the single autoencoders at each layer acts as a\nlocal, layer-level regularizer. We empirically evaluate the performance of this\njoint training scheme and observe that it not only learns a better data model,\nbut also learns better higher layer representations, which highlights its\npotential for unsupervised feature learning. In addition, we find that the\nusage of regularizations in the joint training scheme is crucial in achieving\ngood performance. In the supervised setting, joint training also shows superior\nperformance when training deeper models. The joint training framework can thus\nprovide a platform for investigating more efficient usage of different types of\nregularizers, especially in light of the growing volumes of available unlabeled\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 17:41:33 GMT"}, {"version": "v2", "created": "Sat, 14 Jun 2014 15:48:53 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 18:02:06 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2015 23:52:59 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhou", "Yingbo", ""], ["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1405.1404", "submitter": "Omar Soliman S.", "authors": "Omar S. Soliman, Aliaa Rassem", "title": "A Network Intrusions Detection System based on a Quantum Bio Inspired\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Network intrusion detection systems (NIDSs) have a role of identifying\nmalicious activities by monitoring the behavior of networks. Due to the\ncurrently high volume of networks trafic in addition to the increased number of\nattacks and their dynamic properties, NIDSs have the challenge of improving\ntheir classification performance. Bio-Inspired Optimization Algorithms (BIOs)\nare used to automatically extract the the discrimination rules of normal or\nabnormal behavior to improve the classification accuracy and the detection\nability of NIDS. A quantum vaccined immune clonal algorithm with the estimation\nof distribution algorithm (QVICA-with EDA) is proposed in this paper to build a\nnew NIDS. The proposed algorithm is used as classification algorithm of the new\nNIDS where it is trained and tested using the KDD data set. Also, the new NIDS\nis compared with another detection system based on particle swarm optimization\n(PSO). Results shows the ability of the proposed algorithm of achieving high\nintrusions classification accuracy where the highest obtained accuracy is 94.8\n%.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 02:19:17 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Soliman", "Omar S.", ""], ["Rassem", "Aliaa", ""]]}, {"id": "1405.1436", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Russell Greiner, Brendan Frey", "title": "Training Restricted Boltzmann Machine by Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to maximum likelihood learning of discrete graphical models\nand RBM in particular is introduced. Our method, Perturb and Descend (PD) is\ninspired by two ideas (I) perturb and MAP method for sampling (II) learning by\nContrastive Divergence minimization. In contrast to perturb and MAP, PD\nleverages training data to learn the models that do not allow efficient MAP\nestimation. During the learning, to produce a sample from the current model, we\nstart from a training data and descend in the energy landscape of the\n\"perturbed model\", for a fixed number of steps, or until a local optima is\nreached. For RBM, this involves linear calculations and thresholding which can\nbe very fast. Furthermore we show that the amount of perturbation is closely\nrelated to the temperature parameter and it can regularize the model by\nproducing robust features resulting in sparse hidden layer activation.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:02:46 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Greiner", "Russell", ""], ["Frey", "Brendan", ""]]}, {"id": "1405.1445", "submitter": "Yimin Yang", "authors": "Yimin Yang, Q.M.Jonathan Wu, Guangbin Huang and Yaonan Wang", "title": "Pulling back error to the hidden-node parameter technology:\n  Single-hidden-layer feedforward network without output weight", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to conventional neural network theories, the feature of\nsingle-hidden-layer feedforward neural networks(SLFNs) resorts to parameters of\nthe weighted connections and hidden nodes. SLFNs are universal approximators\nwhen at least the parameters of the networks including hidden-node parameter\nand output weight are exist. Unlike above neural network theories, this paper\nindicates that in order to let SLFNs work as universal approximators, one may\nsimply calculate the hidden node parameter only and the output weight is not\nneeded at all. In other words, this proposed neural network architecture can be\nconsidered as a standard SLFNs with fixing output weight equal to an unit\nvector. Further more, this paper presents experiments which show that the\nproposed learning method tends to extremely reduce network output error to a\nvery small number with only 1 hidden node. Simulation results demonstrate that\nthe proposed method can provide several to thousands of times faster than other\nlearning algorithm including BP, SVM/SVR and other ELM methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:18:49 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Yang", "Yimin", ""], ["Wu", "Q. M. Jonathan", ""], ["Huang", "Guangbin", ""], ["Wang", "Yaonan", ""]]}, {"id": "1405.1958", "submitter": "Mohamed Hassan", "authors": "Mohamed Hassan", "title": "A Self-Adaptive Network Protection System", "comments": "91. arXiv admin note: text overlap with arXiv:1204.1336 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this treatise we aim to build a hybrid network automated (self-adaptive)\nsecurity threats discovery and prevention system; by using unconventional\ntechniques and methods, including fuzzy logic and biological inspired\nalgorithms under the context of soft computing.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 15:05:28 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 00:51:00 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Hassan", "Mohamed", ""]]}, {"id": "1405.2168", "submitter": "Elham Shadkam", "authors": "Elham Shadkam and Mehdi Bijari", "title": "Evaluation The Efficiency Of Cuckoo Optimization Algorithm", "comments": "9 pages", "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.4, No.2, April 2014", "doi": null, "report-no": null, "categories": "cs.NE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new evolutionary algorithm, for continuous nonlinear\noptimization problems, is surveyed. This method is inspired by the life of a\nbird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated by\nusing the Rastrigin function. The problem is a non-linear continuous function\nwhich is used for evaluating optimization algorithms. The efficiency of the COA\nhas been studied by obtaining optimal solution of various dimensions Rastrigin\nfunction in this paper. The mentioned function also was solved by FA and ABC\nalgorithms. Comparing the results shows the COA has better performance than\nother algorithms. Application of algorithm to test function has proven its\ncapability to deal with difficult optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 08:11:59 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Shadkam", "Elham", ""], ["Bijari", "Mehdi", ""]]}, {"id": "1405.2262", "submitter": "Michael S. Gashler Ph.D.", "authors": "Michael S. Gashler and Stephen C. Ashmore", "title": "Training Deep Fourier Neural Networks To Fit Time-Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training a deep neural network containing sinusoidal\nactivation functions to fit to time-series data. Weights are initialized using\na fast Fourier transform, then trained with regularization to improve\ngeneralization. A simple dynamic parameter tuning method is employed to adjust\nboth the learning rate and regularization term, such that stability and\nefficient training are both achieved. We show how deeper layers can be utilized\nto model the observed sequence using a sparser set of sinusoid units, and how\nnon-uniform regularization can improve generalization by promoting the shifting\nof weight toward simpler units. The method is demonstrated with time-series\nproblems to show that it leads to effective extrapolation of nonlinear trends.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 15:23:06 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Gashler", "Michael S.", ""], ["Ashmore", "Stephen C.", ""]]}, {"id": "1405.4322", "submitter": "David B. Knoester", "authors": "David B. Knoester, Heather J. Goldsby, and Christoph Adami", "title": "Leveraging Evolutionary Search to Discover Self-Adaptive and\n  Self-Organizing Cellular Automata", "comments": "10 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building self-adaptive and self-organizing (SASO) systems is a challenging\nproblem, in part because SASO principles are not yet well understood and few\nplatforms exist for exploring them. Cellular automata (CA) are a well-studied\napproach to exploring the principles underlying self-organization. A CA\ncomprises a lattice of cells whose states change over time based on a discrete\nupdate function. One challenge to developing CA is that the relationship of an\nupdate function, which describes the local behavior of each cell, to the global\nbehavior of the entire CA is often unclear. As a result, many researchers have\nused stochastic search techniques, such as evolutionary algorithms, to\nautomatically discover update functions that produce a desired global behavior.\nHowever, these update functions are typically defined in a way that does not\nprovide for self-adaptation. Here we describe an approach to discovering CA\nupdate functions that are both self-adaptive and self-organizing. Specifically,\nwe use a novel evolutionary algorithm-based approach to discover finite state\nmachines (FSMs) that implement update functions for CA. We show how this\napproach is able to evolve FSM-based update functions that perform well on the\ndensity classification task for 1-, 2-, and 3-dimensional CA. Moreover, we show\nthat these FSMs are self-adaptive, self-organizing, and highly scalable, often\nperforming well on CA that are orders of magnitude larger than those used to\nevaluate performance during the evolutionary search. These results demonstrate\nthat CA are a viable platform for studying the integration of self-adaptation\nand self-organization, and strengthen the case for using evolutionary\nalgorithms as a component of SASO systems.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 22:12:48 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Knoester", "David B.", ""], ["Goldsby", "Heather J.", ""], ["Adami", "Christoph", ""]]}, {"id": "1405.4507", "submitter": "Tao  Ye", "authors": "Tao Ye and Tao Wang and Zhipeng Lu and Jin-Kao Hao", "title": "A Multi-parent Memetic Algorithm for the Linear Ordering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-parent memetic algorithm (denoted by MPM)\nfor solving the classic Linear Ordering Problem (LOP). The MPM algorithm\nintegrates in particular a multi-parent recombination operator for generating\noffspring solutions and a distance-and-quality based criterion for pool\nupdating. Our MPM algorithm is assessed on 8 sets of 484 widely used LOP\ninstances and compared with several state-of-the-art algorithms in the\nliterature, showing the efficacy of the MPM algorithm. Specifically, for the\n255 instances whose optimal solutions are unknown, the MPM is able to detect\nbetter solutions than the previous best-known ones for 66 instances, while\nmatching the previous best-known results for 163 instances. Furthermore, some\nadditional experiments are carried out to analyze the key elements and\nimportant parameters of MPM.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 14:02:15 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Ye", "Tao", ""], ["Wang", "Tao", ""], ["Lu", "Zhipeng", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1405.4510", "submitter": "Tao  Ye", "authors": "Tao Ye and Kan Zhou and Zhipeng Lu and Jin-Kao Hao", "title": "A Memetic Algorithm for the Linear Ordering Problem with Cumulative\n  Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an effective memetic algorithm for the linear ordering\nproblem with cumulative costs. The proposed algorithm combines an order-based\nrecombination operator with an improved forward-backward local search procedure\nand employs a solution quality based replacement criterion for pool updating.\nExtensive experiments on 118 well-known benchmark instances show that the\nproposed algorithm achieves competitive results by identifying 46 new upper\nbounds. Furthermore, some critical ingredients of our algorithm are analyzed to\nunderstand the source of its performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 14:25:56 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Ye", "Tao", ""], ["Zhou", "Kan", ""], ["Lu", "Zhipeng", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1405.4589", "submitter": "Chao Zhang", "authors": "Chao Zhang, Hong-cen Mei, Hao Yang", "title": "A Parallel Way to Select the Parameters of SVM Based on the Ant\n  Optimization Algorithm", "comments": "3 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of experimental data shows that Support Vector Machine (SVM)\nalgorithm has obvious advantages in text classification, handwriting\nrecognition, image classification, bioinformatics, and some other fields. To\nsome degree, the optimization of SVM depends on its kernel function and Slack\nvariable, the determinant of which is its parameters $\\delta$ and c in the\nclassification function. That is to say,to optimize the SVM algorithm, the\noptimization of the two parameters play a huge role. Ant Colony Optimization\n(ACO) is optimization algorithm which simulate ants to find the optimal path.In\nthe available literature, we mix the ACO algorithm and Parallel algorithm\ntogether to find a well parameters.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 03:50:21 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 11:53:39 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Zhang", "Chao", ""], ["Mei", "Hong-cen", ""], ["Yang", "Hao", ""]]}, {"id": "1405.4604", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu, Yann N. Dauphin, Surya Ganguli and Yoshua Bengio", "title": "On the saddle point problem for non-convex optimization", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge to many fields of science and engineering involves\nminimizing non-convex error functions over continuous, high dimensional spaces.\nGradient descent or quasi-Newton methods are almost ubiquitously used to\nperform such minimizations, and it is often thought that a main source of\ndifficulty for the ability of these local methods to find the global minimum is\nthe proliferation of local minima with much higher error than the global\nminimum. Here we argue, based on results from statistical physics, random\nmatrix theory, and neural network theory, that a deeper and more profound\ndifficulty originates from the proliferation of saddle points, not local\nminima, especially in high dimensional problems of practical interest. Such\nsaddle points are surrounded by high error plateaus that can dramatically slow\ndown learning, and give the illusory impression of the existence of a local\nminimum. Motivated by these arguments, we propose a new algorithm, the\nsaddle-free Newton method, that can rapidly escape high dimensional saddle\npoints, unlike gradient descent and quasi-Newton methods. We apply this\nalgorithm to deep neural network training, and provide preliminary numerical\nevidence for its superior performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 04:56:30 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 03:05:00 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Pascanu", "Razvan", ""], ["Dauphin", "Yann N.", ""], ["Ganguli", "Surya", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1405.4894", "submitter": "Gabriel Lellouch", "authors": "Gabriel Lellouch and Amit Kumar Mishra", "title": "Optimization of OFDM radar waveforms using genetic algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our investigations on the use of single objective\nand multiobjective genetic algorithms based optimisation algorithms to improve\nthe design of OFDM pulses for radar. We discuss these optimization procedures\nin the scope of a waveform design intended for two different radar processing\nsolutions. Lastly, we show how the encoding solution is suited to permit the\noptimizations of waveform for OFDM radar related challenges such as enhanced\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 12:59:40 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Lellouch", "Gabriel", ""], ["Mishra", "Amit Kumar", ""]]}, {"id": "1405.5050", "submitter": "Hosein Azarbonyad", "authors": "Hosein Azarbonyad, Reza Babazadeh", "title": "A Genetic Algorithm for solving Quadratic Assignment Problem(QAP)", "comments": "arXiv admin note: text overlap with arXiv:1305.2684 by other authors", "journal-ref": "In Proceeding of 5th International Conference of Iranian\n  Operations Research Society (ICIORS), Tabriz, Iran, 2012", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadratic Assignment Problem (QAP) is one of the models used for the\nmulti-row layout problem with facilities of equal area. There are a set of n\nfacilities and a set of n locations. For each pair of locations, a distance is\nspecified and for each pair of facilities a weight or flow is specified (e.g.,\nthe amount of supplies transported between the two facilities). The problem is\nto assign all facilities to different locations with the aim of minimizing the\nsum of the distances multiplied by the corresponding flows. The QAP is among\nthe most difficult NP-hard combinatorial optimization problems. Because of\nthis, this paper presents an efficient Genetic algorithm (GA) to solve this\nproblem in reasonable time. For validation the proposed GA some examples are\nselected from QAP library. The obtained results in reasonable time show the\nefficiency of proposed GA.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 11:59:45 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Azarbonyad", "Hosein", ""], ["Babazadeh", "Reza", ""]]}, {"id": "1405.6136", "submitter": "Arun P V", "authors": "P.V. Arun and S.K. Katiyar", "title": "An evolutionary computational based approach towards automatic image\n  registration", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.6711", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a key component of various image processing operations\nwhich involve the analysis of different image data sets. Automatic image\nregistration domains have witnessed the application of many intelligent\nmethodologies over the past decade; however inability to properly model object\nshape as well as contextual information had limited the attainable accuracy. In\nthis paper, we propose a framework for accurate feature shape modeling and\nadaptive resampling using advanced techniques such as Vector Machines, Cellular\nNeural Network (CNN), SIFT, coreset, and Cellular Automata. CNN has found to be\neffective in improving feature matching as well as resampling stages of\nregistration and complexity of the approach has been considerably reduced using\ncorset optimization The salient features of this work are cellular neural\nnetwork approach based SIFT feature point optimisation, adaptive resampling and\nintelligent object modelling. Developed methodology has been compared with\ncontemporary methods using different statistical measures. Investigations over\nvarious satellite images revealed that considerable success was achieved with\nthe approach. System has dynamically used spectral and spatial information for\nrepresenting contextual knowledge using CNN-prolog approach. Methodology also\nillustrated to be effective in providing intelligent interpretation and\nadaptive resampling.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 17:09:45 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Arun", "P. V.", ""], ["Katiyar", "S. K.", ""]]}, {"id": "1405.6137", "submitter": "Arun P V", "authors": "S.K. Katiyar and P.V. Arun", "title": "An enhanced neural network based approach towards object extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvements in spectral and spatial resolution of the satellite images\nhave facilitated the automatic extraction and identification of the features\nfrom satellite images and aerial photographs. An automatic object extraction\nmethod is presented for extracting and identifying the various objects from\nsatellite images and the accuracy of the system is verified with regard to IRS\nsatellite images. The system is based on neural network and simulates the\nprocess of visual interpretation from remote sensing images and hence increases\nthe efficiency of image analysis. This approach obtains the basic\ncharacteristics of the various features and the performance is enhanced by the\nautomatic learning approach, intelligent interpretation, and intelligent\ninterpolation. The major advantage of the method is its simplicity and that the\nsystem identifies the features not only based on pixel value but also based on\nthe shape, haralick features etc of the objects. Further the system allows\nflexibility for identifying the features within the same category based on size\nand shape. The successful application of the system verified its effectiveness\nand the accuracy of the system were assessed by ground truth verification.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 20:05:34 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Katiyar", "S. K.", ""], ["Arun", "P. V.", ""]]}, {"id": "1405.6173", "submitter": "Ahmed Ibrahim Taloba", "authors": "M. H. Marghny, Rasha M. Abd El-Aziz, Ahmed I. Taloba", "title": "An Effective Evolutionary Clustering Algorithm: Hepatitis C Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Clustering analysis plays an important role in scientific research and\ncommercial application. K-means algorithm is a widely used partition method in\nclustering. However, it is known that the K-means algorithm may get stuck at\nsuboptimal solutions, depending on the choice of the initial cluster centers.\nIn this article, we propose a technique to handle large scale data, which can\nselect initial clustering center purposefully using Genetic algorithms (GAs),\nreduce the sensitivity to isolated point, avoid dissevering big cluster, and\novercome deflexion of data in some degree that caused by the disproportion in\ndata partitioning owing to adoption of multi-sampling. We applied our method to\nsome public datasets these show the advantages of the proposed approach for\nexample Hepatitis C dataset that has been taken from the machine learning\nwarehouse of University of California. Our aim is to evaluate hepatitis\ndataset. In order to evaluate this dataset we did some preprocessing operation,\nthe reason to preprocessing is to summarize the data in the best and suitable\nway for our algorithm. Missing values of the instances are adjusted using local\nmean method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 11:03:28 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Marghny", "M. H.", ""], ["El-Aziz", "Rasha M. Abd", ""], ["Taloba", "Ahmed I.", ""]]}, {"id": "1405.6285", "submitter": "David Rodrigues", "authors": "David M.S. Rodrigues and Vitorino Ramos", "title": "Traversing News with Ant Colony Optimisation and Negative Pheromones", "comments": "accepted as preprint for oral presentation at ECCS'14 in Lucca, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen the rapid development of the online newsroom. News\npublished online are the main outlet of news surpassing traditional printed\nnewspapers. This poses challenges to the production and to the consumption of\nthose news. With those many sources of information available it is important to\nfind ways to cluster and organise the documents if one wants to understand this\nnew system. A novel bio inspired approach to the problem of traversing the news\nis presented. It finds Hamiltonian cycles over documents published by the\nnewspaper The Guardian. A Second Order Swarm Intelligence algorithm based on\nAnt Colony Optimisation was developed that uses a negative pheromone to mark\nunrewarding paths with a \"no-entry\" signal. This approach follows recent\nfindings of negative pheromone usage in real ants.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 09:08:49 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Rodrigues", "David M. S.", ""], ["Ramos", "Vitorino", ""]]}, {"id": "1405.6296", "submitter": "Hiroki Sayama", "authors": "Hiroki Sayama", "title": "Four Classes of Morphogenetic Collective Systems", "comments": "8 pages, 2 figures, 1 table; accepted for publication in ALIFE 14\n  proceedings", "journal-ref": "Artificial Life 14: Proceedings of the Fourteenth International\n  Conference on the Synthesis and Simulation of Living Systems, 2014, MIT\n  Press, pp. 320-327", "doi": null, "report-no": null, "categories": "nlin.AO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We studied the roles of morphogenetic principles---heterogeneity of\ncomponents, dynamic differentiation/re-differentiation of components, and local\ninformation sharing among components---in the self-organization of\nmorphogenetic collective systems. By incrementally introducing these principles\nto collectives, we defined four distinct classes of morphogenetic collective\nsystems. Monte Carlo simulations were conducted using an extended version of\nthe Swarm Chemistry model that was equipped with dynamic\ndifferentiation/re-differentiation and local information sharing capabilities.\nSelf-organization of swarms was characterized by several kinetic and\ntopological measurements, the latter of which were facilitated by a newly\ndeveloped network-based method. Results of simulations revealed that, while\nheterogeneity of components had a strong impact on the structure and behavior\nof the swarms, dynamic differentiation/re-differentiation of components and\nlocal information sharing helped the swarms maintain spatially adjacent,\ncoherent organization.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 12:30:53 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Sayama", "Hiroki", ""]]}, {"id": "1405.7229", "submitter": "Erik Cuevas E", "authors": "Erik Cuevas, Felipe Sencion, Daniel Zaldivar, Marco Perez, Humberto\n  Sossa", "title": "A Multi-threshold Segmentation Approach Based on Artificial Bee Colony\n  Optimization", "comments": "16 Pages", "journal-ref": "Applied Intelligence 37 (3), (2012), pp. 321-336", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of the Artificial Bee Colony (ABC) algorithm to\ncompute threshold selection for image segmentation. ABC is a heuristic\nalgorithm motivated by the intelligent behavior of honey-bees which has been\nsuccessfully employed to solve complex optimization problems. In this approach,\nan image 1D histogram is approximated through a Gaussian mixture model whose\nparameters are calculated by the ABC algorithm. For the approximation scheme,\neach Gaussian function represents a pixel class and therefore a threshold.\nUnlike the Expectation Maximization (EM) algorithm, the ABC based method shows\nfast convergence and low sensitivity to initial conditions. Remarkably, it also\nimproves complex time consuming computations commonly required by\ngradient-based methods. Experimental results demonstrate the algorithms ability\nto perform automatic multi threshold selection yet showing interesting\nadvantages by comparison to other well known algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 13:39:20 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Cuevas", "Erik", ""], ["Sencion", "Felipe", ""], ["Zaldivar", "Daniel", ""], ["Perez", "Marco", ""], ["Sossa", "Humberto", ""]]}, {"id": "1405.7349", "submitter": "Bing Wang", "authors": "Bing Wang, Yao-hua Meng, Xiao-hong Yu", "title": "Radial basis function process neural network training based on\n  generalized frechet distance and GA-SA hybrid strategy", "comments": "9 pages, 4 figures,14 references", "journal-ref": "Computer Science & Engineering: An International Journal (CSEIJ),\n  Vol. 3, No. 6, December 2013:1-9", "doi": "10.5121/cseij.2013.3601", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For learning problem of Radial Basis Function Process Neural Network\n(RBF-PNN), an optimization training method based on GA combined with SA is\nproposed in this paper. Through building generalized Fr\\'echet distance to\nmeasure similarity between time-varying function samples, the learning problem\nof radial basis centre functions and connection weights is converted into the\ntraining on corresponding discrete sequence coefficients. Network training\nobjective function is constructed according to the least square error\ncriterion, and global optimization solving of network parameters is implemented\nin feasible solution space by use of global optimization feature of GA and\nprobabilistic jumping property of SA . The experiment results illustrate that\nthe training algorithm improves the network training efficiency and stability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 10:24:00 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Wang", "Bing", ""], ["Meng", "Yao-hua", ""], ["Yu", "Xiao-hong", ""]]}, {"id": "1405.7406", "submitter": "Valent\\'in Osuna-Enciso Mr", "authors": "Valent\\'in Osuna-Enciso, Erik Cuevas, Humberto Sossa", "title": "A Comparison of Nature Inspired Algorithms for Multi-threshold Image\n  Segmentation", "comments": "16 pages, this is a draft of the final version of the article sent to\n  the Journal", "journal-ref": "Expert Systems with Applications, Volume 40, Issue 4, March 2013,\n  Pages 1213-1219", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of image analysis, segmentation is one of the most important\npreprocessing steps. One way to achieve segmentation is by mean of threshold\nselection, where each pixel that belongs to a determined class islabeled\naccording to the selected threshold, giving as a result pixel groups that share\nvisual characteristics in the image. Several methods have been proposed in\norder to solve threshold selectionproblems; in this work, it is used the method\nbased on the mixture of Gaussian functions to approximate the 1D histogram of a\ngray level image and whose parameters are calculated using three nature\ninspired algorithms (Particle Swarm Optimization, Artificial Bee Colony\nOptimization and Differential Evolution). Each Gaussian function approximates\nthehistogram, representing a pixel class and therefore a threshold point.\nExperimental results are shown, comparing in quantitative and qualitative\nfashion as well as the main advantages and drawbacks of each algorithm, applied\nto multi-threshold problem.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 21:40:31 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Osuna-Enciso", "Valent\u00edn", ""], ["Cuevas", "Erik", ""], ["Sossa", "Humberto", ""]]}, {"id": "1405.7716", "submitter": "Sukru Burc Eryilmaz", "authors": "S. Burc Eryilmaz, Duygu Kuzum, Rakesh G. D. Jeyasingh, SangBum Kim,\n  Matthew BrightSky, Chung Lam and H.-S. Philip Wong", "title": "Experimental Demonstration of Array-level Learning with Phase Change\n  Synaptic Devices", "comments": "IEDM 2013", "journal-ref": "Electron Devices Meeting (IEDM), 2013 IEEE International,\n  pp.25.5.1,25.5.4, 9-11 Dec. 2013", "doi": "10.1109/IEDM.2013.6724691", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational performance of the biological brain has long attracted\nsignificant interest and has led to inspirations in operating principles,\nalgorithms, and architectures for computing and signal processing. In this\nwork, we focus on hardware implementation of brain-like learning in a\nbrain-inspired architecture. We demonstrate, in hardware, that 2-D crossbar\narrays of phase change synaptic devices can achieve associative learning and\nperform pattern recognition. Device and array-level studies using an\nexperimental 10x10 array of phase change synaptic devices have shown that\npattern recognition is robust against synaptic resistance variations and large\nvariations can be tolerated by increasing the number of training iterations.\nOur measurements show that increase in initial variation from 9 % to 60 %\ncauses required training iterations to increase from 1 to 11.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 20:22:00 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 05:45:54 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Eryilmaz", "S. Burc", ""], ["Kuzum", "Duygu", ""], ["Jeyasingh", "Rakesh G. D.", ""], ["Kim", "SangBum", ""], ["BrightSky", "Matthew", ""], ["Lam", "Chung", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1405.7777", "submitter": "Jonathan Tapson", "authors": "Andr\\'e van Schaik and Jonathan Tapson", "title": "Online and Adaptive Pseudoinverse Solutions for ELM Weights", "comments": "Accepted for publication in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ELM method has become widely used for classification and regressions\nproblems as a result of its accuracy, simplicity and ease of use. The solution\nof the hidden layer weights by means of a matrix pseudoinverse operation is a\nsignificant contributor to the utility of the method; however, the conventional\ncalculation of the pseudoinverse by means of a singular value decomposition\n(SVD) is not always practical for large data sets or for online updates to the\nsolution. In this paper we discuss incremental methods for solving the\npseudoinverse which are suitable for ELM. We show that careful choice of\nmethods allows us to optimize for accuracy, ease of computation, or\nadaptability of the solution.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 05:24:22 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["van Schaik", "Andr\u00e9", ""], ["Tapson", "Jonathan", ""]]}, {"id": "1405.7780", "submitter": "Jonathan Tapson", "authors": "Jonathan Tapson and Andr\\'e van Schaik", "title": "ELM Solutions for Event-Based Systems", "comments": "Accepted for publication in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst most engineered systems use signals that are continuous in time, there\nis a domain of systems in which signals consist of events. Events, like Dirac\ndelta functions, have no meaningful time duration. Many important real-world\nsystems are intrinsically event-based, including the mammalian brain, in which\nthe primary packets of data are spike events, or action potentials. In this\ndomain, signal processing requires responses to spatio-temporal patterns of\nevents. We show that some straightforward modifications to the standard ELM\ntopology produce networks that are able to perform spatio-temporal event\nprocessing online with a high degree of accuracy. The modifications involve the\nre-definition of hidden layer units as synaptic kernels, in which the input\ndelta functions are transformed into continuous-valued signals using a variety\nof impulse-response functions. This permits the use of linear solution methods\nin the output layer, which can produce events as output, if modeled as a\nclassifier; the output classes are 'event' or 'no event'. We illustrate the\nmethod in application to a spike-processing problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 05:29:39 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}]