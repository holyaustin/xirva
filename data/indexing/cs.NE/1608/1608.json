[{"id": "1608.00138", "submitter": "Wen Ying", "authors": "Wen-Bo Du, Wen Ying, Gang Yan, Yan-Bo Zhu, and Xian-Bin Cao", "title": "Heterogeneous Strategy Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PSO is a widely recognized optimization algorithm inspired by social swarm.\nIn this brief we present a heterogeneous strategy particle swarm optimization\n(HSPSO), in which a proportion of particles adopt a fully informed strategy to\nenhance the converging speed while the rest are singly informed to maintain the\ndiversity. Our extensive numerical experiments show that HSPSO algorithm is\nable to obtain satisfactory solutions, outperforming both PSO and the fully\ninformed PSO. The evolution process is examined from both structural and\nmicroscopic points of view. We find that the cooperation between two types of\nparticles can facilitate a good balance between exploration and exploitation,\nyielding better performance. We demonstrate the applicability of HSPSO on the\nfilter design problem.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 16:06:18 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Du", "Wen-Bo", ""], ["Ying", "Wen", ""], ["Yan", "Gang", ""], ["Zhu", "Yan-Bo", ""], ["Cao", "Xian-Bin", ""]]}, {"id": "1608.00218", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Jiashi Feng", "title": "Hyperparameter Transfer Learning through Surrogate Alignment for\n  Efficient Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several optimization methods have been successfully applied to the\nhyperparameter optimization of deep neural networks (DNNs). The methods work by\nmodeling the joint distribution of hyperparameter values and corresponding\nerror. Those methods become less practical when applied to modern DNNs whose\ntraining may take a few days and thus one cannot collect sufficient\nobservations to accurately model the distribution. To address this challenging\nissue, we propose a method that learns to transfer optimal hyperparameter\nvalues for a small source dataset to hyperparameter values with comparable\nperformance on a dataset of interest. As opposed to existing transfer learning\nmethods, our proposed method does not use hand-designed features. Instead, it\nuses surrogates to model the hyperparameter-error distributions of the two\ndatasets and trains a neural network to learn the transfer function. Extensive\nexperiments on three CV benchmark datasets clearly demonstrate the efficiency\nof our method.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 14:09:17 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ilievski", "Ilija", ""], ["Feng", "Jiashi", ""]]}, {"id": "1608.00466", "submitter": "Madhusudan Lakshmana", "authors": "Madhusudan Lakshmana, Sundararajan Sellamanickam, Shirish Shevade,\n  Keerthi Selvaraj", "title": "Learning Semantically Coherent and Reusable Kernels in Convolution\n  Neural Nets for Sentence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art CNN models give good performance on sentence\nclassification tasks. The purpose of this work is to empirically study\ndesirable properties such as semantic coherence, attention mechanism and\nreusability of CNNs in these tasks. Semantically coherent kernels are\npreferable as they are a lot more interpretable for explaining the decision of\nthe learned CNN model. We observe that the learned kernels do not have semantic\ncoherence. Motivated by this observation, we propose to learn kernels with\nsemantic coherence using clustering scheme combined with Word2Vec\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\ntechnique to visualize attention mechanism of CNNs for decision explanation\npurpose. Reusable property enables kernels learned on one problem to be used in\nanother problem. This helps in efficient learning as only a few additional\ndomain specific filters may have to be learned. We demonstrate the efficacy of\nour core ideas of learning semantically coherent kernels and leveraging\nreusable kernels for efficient learning on several benchmark datasets.\nExperimental results show the usefulness of our approach by achieving\nperformance close to the state-of-the-art methods but with semantic and\nreusable properties.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:14:08 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 03:57:26 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Lakshmana", "Madhusudan", ""], ["Sellamanickam", "Sundararajan", ""], ["Shevade", "Shirish", ""], ["Selvaraj", "Keerthi", ""]]}, {"id": "1608.00530", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Kevin Gimpel", "title": "Early Methods for Detecting Adversarial Images", "comments": "ICLR 2017 Workshop Contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:13:58 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:03:47 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1608.00611", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, and Kaushik Roy", "title": "Attention Tree: Learning Hierarchies of Visual Features for Large-Scale\n  Image Recognition", "comments": "11 pages, 8 figures, Under review in IEEE Transactions on Neural\n  Networks and Learning systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in machine learning is to design a computationally\nefficient multi-class classifier while maintaining the output accuracy and\nperformance. In this paper, we present a tree-based classifier: Attention Tree\n(ATree) for large-scale image classification that uses recursive Adaboost\ntraining to construct a visual attention hierarchy. The proposed attention\nmodel is inspired from the biological 'selective tuning mechanism for cortical\nvisual processing'. We exploit the inherent feature similarity across images in\ndatasets to identify the input variability and use recursive optimization\nprocedure, to determine data partitioning at each node, thereby, learning the\nattention hierarchy. A set of binary classifiers is organized on top of the\nlearnt hierarchy to minimize the overall test-time complexity. The attention\nmodel maximizes the margins for the binary classifiers for optimal decision\nboundary modelling, leading to better performance at minimal complexity. The\nproposed framework has been evaluated on both Caltech-256 and SUN datasets and\nachieves accuracy improvement over state-of-the-art tree-based methods at\nsignificantly lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 20:51:29 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1608.00781", "submitter": "Edward J. Yoon", "authors": "Edward J. Yoon", "title": "Horn: A System for Parallel Training and Regularizing of Large-Scale\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "EP-909420F9A6E94B3691E5EE413DAD353E", "categories": "cs.DC cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  I introduce a new distributed system for effective training and regularizing\nof Large-Scale Neural Networks on distributed computing architectures. The\nexperiments demonstrate the effectiveness of flexible model partitioning and\nparallelization strategies based on neuron-centric computation model, with an\nimplementation of the collective and parallel dropout neural networks training.\nExperiments are performed on MNIST handwritten digits classification including\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 11:57:09 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 22:01:12 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Yoon", "Edward J.", ""]]}, {"id": "1608.00895", "submitter": "Patrick Doetsch", "authors": "Patrick Doetsch, Albert Zeyer, Paul Voigtlaender, Ilya Kulikov, Ralf\n  Schl\\\"uter, Hermann Ney", "title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we release our extensible and easily configurable neural network\ntraining software. It provides a rich set of functional layers with a\nparticular focus on efficient training of recurrent neural network topologies\non multiple GPUs. The source of the software package is public and freely\navailable for academic research purposes and can be used as a framework or as a\nstandalone tool which supports a flexible configuration. The software allows to\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\non both one dimensional data like speech or two dimensional data like\nhandwritten text and was used to develop successful submission systems in\nseveral evaluation campaigns.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 16:43:27 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 14:25:28 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Doetsch", "Patrick", ""], ["Zeyer", "Albert", ""], ["Voigtlaender", "Paul", ""], ["Kulikov", "Ilya", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1608.01298", "submitter": "Peter Wittek", "authors": "S\\'andor Dar\\'anyi, Peter Wittek, Konstantinos Konstantinidis, Symeon\n  Papadopoulos, Efstratios Kontopoulos", "title": "A Physical Metaphor to Study Semantic Drift", "comments": "8 pages, 4 figures, to appear in Proceedings of SuCCESS-16, 1st\n  International Workshop on Semantic Change & Evolving Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In accessibility tests for digital preservation, over time we experience\ndrifts of localized and labelled content in statistical models of evolving\nsemantics represented as a vector field. This articulates the need to detect,\nmeasure, interpret and model outcomes of knowledge dynamics. To this end we\nemploy a high-performance machine learning algorithm for the training of\nextremely large emergent self-organizing maps for exploratory data analysis.\nThe working hypothesis we present here is that the dynamics of semantic drifts\ncan be modeled on a relaxed version of Newtonian mechanics called social\nmechanics. By using term distances as a measure of semantic relatedness vs.\ntheir PageRank values indicating social importance and applied as variable\n`term mass', gravitation as a metaphor to express changes in the semantic\ncontent of a vector field lends a new perspective for experimentation. From\n`term gravitation' over time, one can compute its generating potential whose\nfluctuations manifest modifications in pairwise term similarity vs. social\nimportance, thereby updating Osgood's semantic differential. The dataset\nexamined is the public catalog metadata of Tate Galleries, London.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 19:34:13 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Wittek", "Peter", ""], ["Konstantinidis", "Konstantinos", ""], ["Papadopoulos", "Symeon", ""], ["Kontopoulos", "Efstratios", ""]]}, {"id": "1608.01783", "submitter": "Aneta Neumann", "authors": "Aneta Neumann, Bradley Alexander, Frank Neumann", "title": "The Evolutionary Process of Image Transition in Conjunction with Box and\n  Strip Mutation", "comments": "Conference version appears at ICONIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms have been used in many ways to generate digital art.\nWe study how evolutionary processes are used for evolutionary art and present a\nnew approach to the transition of images. Our main idea is to define\nevolutionary processes for digital image transition, combining different\nvariants of mutation and evolutionary mechanisms. We introduce box and strip\nmutation operators which are specifically designed for image transition. Our\nexperimental results show that the process of an evolutionary algorithm in\ncombination with these mutation operators can be used as a valuable way to\nproduce unique generative art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 07:15:38 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Neumann", "Aneta", ""], ["Alexander", "Bradley", ""], ["Neumann", "Frank", ""]]}, {"id": "1608.01818", "submitter": "Manuel Mazzara", "authors": "Leonard Johard, Lukas Breitwieser, Alberto Di Meglio, Marco Manca,\n  Manuel Mazzara, Max Talanov", "title": "The BioDynaMo Project: a platform for computer simulations of biological\n  dynamics", "comments": "The paper contains inaccurate content and claims that need to be\n  verified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a brief update on developments in the BioDynaMo project, a new\nplatform for computer simulations for biological research. We will discuss the\nnew capabilities of the simulator, important new concepts simulation\nmethodology as well as its numerous applications to the computational biology\nand nanoscience communities.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 09:55:59 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 12:48:57 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Johard", "Leonard", ""], ["Breitwieser", "Lukas", ""], ["Di Meglio", "Alberto", ""], ["Manca", "Marco", ""], ["Mazzara", "Manuel", ""], ["Talanov", "Max", ""]]}, {"id": "1608.02164", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths", "title": "Adapting Deep Network Features to Capture Psychological Representations", "comments": "6 pages, 4 figures, To appear in the Proceedings of the 38th Annual\n  Conference of the Cognitive Science Society, Winner of the Computational\n  Modeling Prize in Perception/Action", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become increasingly successful at solving classic\nperception problems such as object recognition, semantic segmentation, and\nscene understanding, often reaching or surpassing human-level accuracy. This\nsuccess is due in part to the ability of DNNs to learn useful representations\nof high-dimensional inputs, a problem that humans must also solve. We examine\nthe relationship between the representations learned by these networks and\nhuman psychological representations recovered from similarity judgments. We\nfind that deep features learned in service of object classification account for\na significant amount of the variance in human similarity judgments for a set of\nanimal images. However, these features do not capture some qualitative\ndistinctions that are a key part of human representations. To remedy this, we\ndevelop a method for adapting deep features to align with human similarity\njudgments, resulting in image representations that can potentially be used to\nextend the scope of psychological experiments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 23:49:48 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Abbott", "Joshua T.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1608.02229", "submitter": "Fernando Corbacho", "authors": "Fernando Corbacho", "title": "Towards the Self-constructive Brain: emergence of adaptive behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive behavior is mainly the result of adaptive brains. We go a step\nbeyond and claim that the brain does not only adapt to its surrounding reality\nbut rather, it builds itself up to constructs its own reality. That is, rather\nthan just trying to passively understand its environment, the brain is the\narchitect of its own reality in an active process where its internal models of\nthe external world frame how its new interactions with the environment are\nassimilated. These internal models represent relevant predictive patterns of\ninteraction all over the different brain structures: perceptual, sensorimotor,\nmotor, etc. The emergence of adaptive behavior arises from this\nself-constructive nature of the brain, based on the following principles of\norganization: self-experimental, self- growing, and self-repairing.\nSelf-experimental, since to ensure survival, the self-constructive brain (SCB)\nis an active machine capable of performing experiments of its own interactions\nwith the environment by mental simulation. Self-growing, since it dynamically\nand incrementally constructs internal structures in order to build a model of\nthe world as it gathers statistics from its interactions with the environment.\nSelf-repairing, since to survive the SCB must also be robust and capable of\nfinding ways to repair parts of previously working structures and hence\nre-construct a previous relevant pattern of activity.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 15:52:28 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Corbacho", "Fernando", ""]]}, {"id": "1608.02292", "submitter": "Thushan Ganegedara", "authors": "Thushan Ganegedara, Lionel Ott and Fabio Ramos", "title": "Online Adaptation of Deep Architectures with Reinforcement Learning", "comments": "ECAI 2016, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has become crucial to many problems in machine learning. As\nmore data is collected sequentially, quickly adapting to changes in the data\ndistribution can offer several competitive advantages such as avoiding loss of\nprior knowledge and more efficient learning. However, adaptation to changes in\nthe data distribution (also known as covariate shift) needs to be performed\nwithout compromising past knowledge already built in into the model to cope\nwith voluminous and dynamic data. In this paper, we propose an online stacked\nDenoising Autoencoder whose structure is adapted through reinforcement\nlearning. Our algorithm forces the network to exploit and explore favourable\narchitectures employing an estimated utility function that maximises the\naccuracy of an unseen validation sequence. Different actions, such as Pool,\nIncrement and Merge are available to modify the structure of the network. As we\nobserve through a series of experiments, our approach is more responsive,\nrobust, and principled than its counterparts for non-stationary as well as\nstationary data distributions. Experimental results indicate that our algorithm\nperforms better at preserving gained prior knowledge and responding to changes\nin the data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 01:10:51 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Ganegedara", "Thushan", ""], ["Ott", "Lionel", ""], ["Ramos", "Fabio", ""]]}, {"id": "1608.02728", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky and Nikos Komodakis", "title": "OnionNet: Sharing Features in Cascaded Deep Classifiers", "comments": "Accepted to BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of our work is speeding up evaluation of deep neural networks in\nretrieval scenarios, where conventional architectures may spend too much time\non negative examples. We propose to replace a monolithic network with our novel\ncascade of feature-sharing deep classifiers, called OnionNet, where subsequent\nstages may add both new layers as well as new feature channels to the previous\nones. Importantly, intermediate feature maps are shared among classifiers,\npreventing them from the necessity of being recomputed. To accomplish this, the\nmodel is trained end-to-end in a principled way under a joint loss. We validate\nour approach in theory and on a synthetic benchmark. As a result demonstrated\nin three applications (patch matching, object detection, and image retrieval),\nour cascade can operate significantly faster than both monolithic networks and\ntraditional cascades without sharing at the cost of marginal decrease in\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:59:47 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1608.02971", "submitter": "Karan Budhraja", "authors": "Karan K. Budhraja and Tim Oates", "title": "Neuroevolution-Based Inverse Reinforcement Learning", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Learning from Demonstration is targeted at learning to perform\ntasks based on observed examples. One approach to Learning from Demonstration\nis Inverse Reinforcement Learning, in which actions are observed to infer\nrewards. This work combines a feature based state evaluation approach to\nInverse Reinforcement Learning with neuroevolution, a paradigm for modifying\nneural networks based on their performance on a given task. Neural networks are\nused to learn from a demonstrated expert policy and are evolved to generate a\npolicy similar to the demonstration. The algorithm is discussed and evaluated\nagainst competitive feature-based Inverse Reinforcement Learning approaches. At\nthe cost of execution time, neural networks allow for non-linear combinations\nof features in state evaluations. These valuations may correspond to state\nvalue or state reward. This results in better correspondence to observed\nexamples as opposed to using linear combinations. This work also extends\nexisting work on Bayesian Non-Parametric Feature Construction for Inverse\nReinforcement Learning by using non-linear combinations of intermediate data to\nimprove performance. The algorithm is observed to be specifically suitable for\na linearly solvable non-deterministic Markov Decision Processes in which\nmultiple rewards are sparsely scattered in state space. A conclusive\nperformance hierarchy between evaluated algorithms is presented.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 20:04:40 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Budhraja", "Karan K.", ""], ["Oates", "Tim", ""]]}, {"id": "1608.02996", "submitter": "Antonio Valerio Miceli Barone", "authors": "Antonio Valerio Miceli Barone", "title": "Towards cross-lingual distributed representations without parallel text\n  trained with adversarial autoencoders", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to learning vector representations of text that are\ncompatible between different languages usually require some amount of parallel\ntext, aligned at word, sentence or at least document level. We hypothesize\nhowever, that different natural languages share enough semantic structure that\nit should be possible, in principle, to learn compatible vector representations\njust by analyzing the monolingual distribution of words.\n  In order to evaluate this hypothesis, we propose a scheme to map word vectors\ntrained on a source language to vectors semantically compatible with word\nvectors trained on a target language using an adversarial autoencoder.\n  We present preliminary qualitative results and discuss possible future\ndevelopments of this technique, such as applications to cross-lingual sentence\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:24:16 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""]]}, {"id": "1608.03123", "submitter": "Per Kristian Lehre", "authors": "Duc-Cuong Dang and Tobias Friedrich and Timo K\\\"otzing and Martin S.\n  Krejca and Per Kristian Lehre and Pietro S. Oliveto and Dirk Sudholt and\n  Andrew M. Sutton", "title": "Escaping Local Optima using Crossover with Emergent or Reinforced\n  Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population diversity is essential for avoiding premature convergence in\nGenetic Algorithms (GAs) and for the effective use of crossover. Yet the\ndynamics of how diversity emerges in populations are not well understood. We\nuse rigorous runtime analysis to gain insight into population dynamics and GA\nperformance for the ($\\mu$+1) GA and the $\\text{Jump}_k$ test function. We show\nthat the interplay of crossover and mutation may serve as a catalyst leading to\na sudden burst of diversity. This leads to improvements of the expected\noptimisation time of order $\\Omega(n/\\log n)$ compared to mutation-only\nalgorithms like (1+1) EA. Moreover, increasing the mutation rate by an\narbitrarily small constant factor can facilitate the generation of diversity,\nleading to speedups of order $\\Omega(n)$. We also compare seven commonly used\ndiversity mechanisms and evaluate their impact on runtime bounds for the\n($\\mu$+1) GA. All previous results in this context only hold for\nunrealistically low crossover probability $p_c=O(k/n)$, while we give analyses\nfor the setting of constant $p_c < 1$ in all but one case.\n  For the typical case of constant $k > 2$ and constant $p_c$, we can compare\nthe resulting expected runtimes for different diversity mechanisms assuming an\noptimal choice of $\\mu$: $O(n^{k-1})$ for duplicate elimination/minim.,\n$O(n^2\\log n)$ for maximising the convex hull, $O(n\\log n)$ for deterministic\ncrowding (assuming $p_c = k/n$), $O(n\\log n)$ for maximising Hamming distance,\n$O(n\\log n)$ for fitness sharing, $O(n\\log n)$ for single-receiver island\nmodel.\n  This proves a sizeable advantage of all variants of the ($\\mu$+1) GA compared\nto (1+1) EA, which requires time $\\Theta(n^k)$. Experiments complement our\ntheoretical findings and further highlight the benefits of crossover and\ndiversity on $\\text{Jump}_k$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:00:15 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Dang", "Duc-Cuong", ""], ["Friedrich", "Tobias", ""], ["K\u00f6tzing", "Timo", ""], ["Krejca", "Martin S.", ""], ["Lehre", "Per Kristian", ""], ["Oliveto", "Pietro S.", ""], ["Sudholt", "Dirk", ""], ["Sutton", "Andrew M.", ""]]}, {"id": "1608.03226", "submitter": "Johannes Lengler", "authors": "Johannes Lengler, Angelika Steger", "title": "Drift Analysis and Evolutionary Algorithms Revisited", "comments": "minor changes to improve readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the easiest randomized greedy optimization algorithms is the following\nevolutionary algorithm which aims at maximizing a boolean function $f:\\{0,1\\}^n\n\\to {\\mathbb R}$. The algorithm starts with a random search point $\\xi \\in\n\\{0,1\\}^n$, and in each round it flips each bit of $\\xi$ with probability $c/n$\nindependently at random, where $c>0$ is a fixed constant. The thus created\noffspring $\\xi'$ replaces $\\xi$ if and only if $f(\\xi') \\ge f(\\xi)$. The\nanalysis of the runtime of this simple algorithm on monotone and on linear\nfunctions turned out to be highly non-trivial. In this paper we review known\nresults and provide new and self-contained proofs of partly stronger results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 16:14:57 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 14:01:42 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 14:20:58 GMT"}, {"version": "v4", "created": "Wed, 15 Nov 2017 08:40:41 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Lengler", "Johannes", ""], ["Steger", "Angelika", ""]]}, {"id": "1608.03639", "submitter": "Truyen Tran", "authors": "Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Faster Training of Very Deep Networks Via p-Norm Gates", "comments": "To appear in ICPR'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major contributing factor to the recent advances in deep neural networks is\nstructural units that let sensory information and gradients to propagate\neasily. Gating is one such structure that acts as a flow control. Gates are\nemployed in many recent state-of-the-art recurrent models such as LSTM and GRU,\nand feedforward models such as Residual Nets and Highway Networks. This enables\nlearning in very deep networks with hundred layers and helps achieve\nrecord-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP\n(e.g., machine translation with GRU). However, there is limited work in\nanalysing the role of gating in the learning process. In this paper, we propose\na flexible $p$-norm gating scheme, which allows user-controllable flow and as a\nconsequence, improve the learning speed. This scheme subsumes other existing\ngating schemes, including those in GRU, Highway Networks and Residual Nets as\nspecial cases. Experiments on large sequence and vector datasets demonstrate\nthat the proposed gating scheme helps improve the learning speed significantly\nwithout extra overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 23:48:44 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1608.03644", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Ritambhara Singh, Beilun Wang, and Yanjun Qi", "title": "Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences\n  Using Deep Neural Networks", "comments": "11 pages, 2 figures. Updated for PSB submission, Pacific Symposium on\n  Biocomputing (PSB) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) models have recently obtained state-of-the-art\nprediction accuracy for the transcription factor binding (TFBS) site\nclassification task. However, it remains unclear how these approaches identify\nmeaningful DNA sequence signals and give insights as to why TFs bind to certain\nlocations. In this paper, we propose a toolkit called the Deep Motif Dashboard\n(DeMo Dashboard) which provides a suite of visualization strategies to extract\nmotifs, or sequence patterns from deep neural network models for TFBS\nclassification. We demonstrate how to visualize and understand three important\nDNN models: convolutional, recurrent, and convolutional-recurrent networks. Our\nfirst visualization method is finding a test sequence's saliency map which uses\nfirst-order derivatives to describe the importance of each nucleotide in making\nthe final prediction. Second, considering recurrent models make predictions in\na temporal manner (from one end of a TFBS sequence to the other), we introduce\ntemporal output scores, indicating the prediction score of a model over time\nfor a sequential input. Lastly, a class-specific visualization strategy finds\nthe optimal input sequence for a given TFBS positive class via stochastic\ngradient optimization. Our experimental results indicate that a\nconvolutional-recurrent architecture performs the best among the three\narchitectures. The visualization techniques indicate that CNN-RNN makes\npredictions by modeling both motifs as well as dependencies among them.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 00:43:59 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 14:00:44 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 16:37:45 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 20:20:22 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Lanchantin", "Jack", ""], ["Singh", "Ritambhara", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1608.03665", "submitter": "Wei Wen", "authors": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li", "title": "Learning Structured Sparsity in Deep Neural Networks", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High demand for computation resources severely hinders deployment of\nlarge-scale Deep Neural Networks (DNN) in resource constrained devices. In this\nwork, we propose a Structured Sparsity Learning (SSL) method to regularize the\nstructures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.\nSSL can: (1) learn a compact structure from a bigger DNN to reduce computation\ncost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently\naccelerate the DNNs evaluation. Experimental results show that SSL achieves on\naverage 5.1x and 3.1x speedups of convolutional layer computation of AlexNet\nagainst CPU and GPU, respectively, with off-the-shelf libraries. These speedups\nare about twice speedups of non-structured sparsity; (3) regularize the DNN\nstructure to improve classification accuracy. The results show that for\nCIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual\nNetwork (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,\nwhich is still slightly higher than that of original ResNet with 32 layers. For\nAlexNet, structure regularization by SSL also reduces the error by around ~1%.\nOpen source code is in https://github.com/wenwei202/caffe/tree/scnn\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 03:20:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 20:46:15 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 05:58:48 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 04:03:41 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Wen", "Wei", ""], ["Wu", "Chunpeng", ""], ["Wang", "Yandan", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1608.03757", "submitter": "Bin Liu", "authors": "Bin Liu, Shi Cheng, Yuhui Shi", "title": "Student's t Distribution based Estimation of Distribution Algorithms for\n  Derivative-free Global Optimization", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with a branch of evolutionary algorithms\ntermed estimation of distribution (EDA), which has been successfully used to\ntackle derivative-free global optimization problems. For existent EDA\nalgorithms, it is a common practice to use a Gaussian distribution or a mixture\nof Gaussian components to represent the statistical property of available\npromising solutions found so far. Observing that the Student's t distribution\nhas heavier and longer tails than the Gaussian, which may be beneficial for\nexploring the solution space, we propose a novel EDA algorithm termed ESTDA, in\nwhich the Student's t distribution, rather than Gaussian, is employed. To\naddress hard multimodal and deceptive problems, we extend ESTDA further by\nsubstituting a single Student's t distribution with a mixture of Student's t\ndistributions. The resulting algorithm is named as estimation of mixture of\nStudent's t distribution algorithm (EMSTDA). Both ESTDA and EMSTDA are\nevaluated through extensive and in-depth numerical experiments using over a\ndozen of benchmark objective functions. Empirical results demonstrate that the\nproposed algorithms provide remarkably better performance than their Gaussian\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 11:36:18 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 22:58:40 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Liu", "Bin", ""], ["Cheng", "Shi", ""], ["Shi", "Yuhui", ""]]}, {"id": "1608.03793", "submitter": "Rajiv Shah", "authors": "Rajiv Shah and Rob Romijnders", "title": "Applying Deep Learning to Basketball Trajectories", "comments": "KDD 2016, Large Scale Sports Analytic Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the emerging trends for sports analytics is the growing use of player\nand ball tracking data. A parallel development is deep learning predictive\napproaches that use vast quantities of data with less reliance on feature\nengineering. This paper applies recurrent neural networks in the form of\nsequence modeling to predict whether a three-point shot is successful. The\nmodels are capable of learning the trajectory of a basketball without any\nknowledge of physics. For comparison, a baseline static machine learning model\nwith a full set of features, such as angle and velocity, in addition to the\npositional data is also tested. Using a dataset of over 20,000 three pointers\nfrom NBA SportVu data, the models based simply on sequential positional data\noutperform a static feature rich machine learning model in predicting whether a\nthree-point shot is successful. This suggests deep learning models may offer an\nimprovement to traditional feature based machine learning methods for tracking\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:50:24 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 18:36:44 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Shah", "Rajiv", ""], ["Romijnders", "Rob", ""]]}, {"id": "1608.03983", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov and Frank Hutter", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "comments": "ICLR 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restart techniques are common in gradient-free optimization to deal with\nmultimodal functions. Partial warm restarts are also gaining popularity in\ngradient-based optimization to improve the rate of convergence in accelerated\ngradient schemes to deal with ill-conditioned functions. In this paper, we\npropose a simple warm restart technique for stochastic gradient descent to\nimprove its anytime performance when training deep neural networks. We\nempirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where\nwe demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively.\nWe also demonstrate its advantages on a dataset of EEG recordings and on a\ndownsampled version of the ImageNet dataset. Our source code is available at\nhttps://github.com/loshchil/SGDR\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 13:46:05 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 13:05:07 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 14:33:00 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 13:06:59 GMT"}, {"version": "v5", "created": "Wed, 3 May 2017 16:28:09 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1608.04105", "submitter": "Timothy Molter", "authors": "Timothy W. Molter and M. Alexander Nugent", "title": "Machine Learning with Memristors via Thermodynamic RAM", "comments": null, "journal-ref": "CNNA 2016, 15th International Workshop on Cellular Nanoscale\n  Networks and their Applications, Dresden, Germany, 2016, pp. 1-2", "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermodynamic RAM (kT-RAM) is a neuromemristive co-processor design based on\nthe theory of AHaH Computing and implemented via CMOS and memristors. The\nco-processor is a 2-D array of differential memristor pairs (synapses) that can\nbe selectively coupled together (neurons) via the digital bit addressing of the\nunderlying CMOS RAM circuitry. The chip is designed to plug into existing\ndigital computers and be interacted with via a simple instruction set.\nAnti-Hebbian and Hebbian (AHaH) computing forms the theoretical framework from\nwhich a nature-inspired type of computing architecture is built where, unlike\nvon Neumann architectures, memory and processor are physically combined for\nsynaptic operations. Through exploitation of AHaH attractor states,\nmemristor-based circuits converge to attractor basins that represents machine\nlearning solutions such as unsupervised feature learning, supervised\nclassification and anomaly detection. Because kT-RAM eliminates the need to\nshuttle bits back and forth between memory and processor and can operate at\nvery low voltage levels, it can significantly surpass CPU, GPU, and FPGA\nperformance for synaptic integration and learning operations. Here, we present\na memristor technology developed for use in kT-RAM, in particular\nbi-directional incremental adaptation of conductance via short low-voltage 1.0\nV, 1.0 microsecond pulses.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 14:01:10 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Molter", "Timothy W.", ""], ["Nugent", "M. Alexander", ""]]}, {"id": "1608.04147", "submitter": "Georgios Spithourakis", "authors": "Georgios P. Spithourakis, Isabelle Augenstein, Sebastian Riedel", "title": "Numerically Grounded Language Models for Semantic Error Correction", "comments": "accepted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic error detection and correction is an important task for applications\nsuch as fact checking, speech-to-text or grammatical error correction. Current\napproaches generally focus on relatively shallow semantics and do not account\nfor numeric quantities. Our approach uses language models grounded in numbers\nwithin the text. Such groundings are easily achieved for recurrent neural\nlanguage model architectures, which can be further conditioned on incomplete\nbackground knowledge bases. Our evaluation on clinical reports shows that\nnumerical grounding improves perplexity by 33% and F1 for semantic error\ncorrection by 5 points when compared to ungrounded approaches. Conditioning on\na knowledge base yields further improvements.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 22:34:22 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Spithourakis", "Georgios P.", ""], ["Augenstein", "Isabelle", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1608.04171", "submitter": "Yuanlong Li", "authors": "Yuanlong Li, Han Hu, Yonggang Wen, Jun Zhang", "title": "Power Data Classification: A Hybrid of a Novel Local Time Warping and\n  LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the purpose of data centre energy consumption monitoring\nand analysis, we propose to detect the running programs in a server by\nclassifying the observed power consumption series. Time series classification\nproblem has been extensively studied with various distance measurements\ndeveloped; also recently the deep learning based sequence models have been\nproved to be promising. In this paper, we propose a novel distance measurement\nand build a time series classification algorithm hybridizing nearest neighbour\nand long short term memory (LSTM) neural network. More specifically, first we\npropose a new distance measurement termed as Local Time Warping (LTW), which\nutilizes a user-specified set for local warping, and is designed to be\nnon-commutative and non-dynamic programming. Second we hybridize the 1NN-LTW\nand LSTM together. In particular, we combine the prediction probability vector\nof 1NN-LTW and LSTM to determine the label of the test cases. Finally, using\nthe power consumption data from a real data center, we show that the proposed\nLTW can improve the classification accuracy of DTW from about 84% to 90%. Our\nexperimental results prove that the proposed LTW is competitive on our data set\ncompared with existed DTW variants and its non-commutative feature is indeed\nbeneficial. We also test a linear version of LTW and it can significantly\noutperform existed linear runtime lower bound methods like LB_Keogh.\nFurthermore, with the hybrid algorithm, for the power series classification\ntask we achieve an accuracy up to about 93%. Our research can inspire more\nstudies on time series distance measurement and the hybrid of the deep learning\nmodels with other traditional models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 02:49:17 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 12:51:46 GMT"}, {"version": "v3", "created": "Sat, 8 Oct 2016 04:33:02 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 04:34:06 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Li", "Yuanlong", ""], ["Hu", "Han", ""], ["Wen", "Yonggang", ""], ["Zhang", "Jun", ""]]}, {"id": "1608.04363", "submitter": "Justin Salamon", "authors": "Justin Salamon and Juan Pablo Bello", "title": "Deep Convolutional Neural Networks and Data Augmentation for\n  Environmental Sound Classification", "comments": "Accepted November 2016, IEEE Signal Processing Letters. Copyright\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses, in any current or future media, including\n  reprinting/republishing this material, creating new collective works, for\n  resale or redistribution, or reuse of any copyrighted component of this work\n  in other works", "journal-ref": null, "doi": "10.1109/LSP.2017.2657381", "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of deep convolutional neural networks (CNN) to learn\ndiscriminative spectro-temporal patterns makes them well suited to\nenvironmental sound classification. However, the relative scarcity of labeled\ndata has impeded the exploitation of this family of high-capacity models. This\nstudy has two primary contributions: first, we propose a deep convolutional\nneural network architecture for environmental sound classification. Second, we\npropose the use of audio data augmentation for overcoming the problem of data\nscarcity and explore the influence of different augmentations on the\nperformance of the proposed CNN architecture. Combined with data augmentation,\nthe proposed model produces state-of-the-art results for environmental sound\nclassification. We show that the improved performance stems from the\ncombination of a deep, high-capacity model and an augmented training set: this\ncombination outperforms both the proposed CNN without augmentation and a\n\"shallow\" dictionary learning model with augmentation. Finally, we examine the\ninfluence of each augmentation on the model's classification accuracy for each\nclass, and observe that the accuracy for each class is influenced differently\nby each augmentation, suggesting that the performance of the model could be\nimproved further by applying class-conditional data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:57:10 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 17:48:04 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Salamon", "Justin", ""], ["Bello", "Juan Pablo", ""]]}, {"id": "1608.04374", "submitter": "Anthony Caterini", "authors": "Anthony L. Caterini, Dong Eui Chang", "title": "A Geometric Framework for Convolutional Neural Networks", "comments": "Added proofs and algorithms that were missing from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a geometric framework for neural networks is proposed. This\nframework uses the inner product space structure underlying the parameter set\nto perform gradient descent not in a component-based form, but in a\ncoordinate-free manner. Convolutional neural networks are described in this\nframework in a compact form, with the gradients of standard --- and\nhigher-order --- loss functions calculated for each layer of the network. This\napproach can be applied to other network structures and provides a basis on\nwhich to create new networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 19:38:35 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 17:45:06 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Caterini", "Anthony L.", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1608.04426", "submitter": "Baiyang Wang", "authors": "Baiyang Wang, Diego Klabjan", "title": "Regularization for Unsupervised Deep Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural networks, such as restricted Boltzmann machines (RBMs)\nand deep belief networks (DBNs), are powerful tools for feature selection and\npattern recognition tasks. We demonstrate that overfitting occurs in such\nmodels just as in deep feedforward neural networks, and discuss possible\nregularization methods to reduce overfitting. We also propose a \"partial\"\napproach to improve the efficiency of Dropout/DropConnect in this scenario, and\ndiscuss the theoretical justification of these methods from model convergence\nand likelihood bounds. Finally, we compare the performance of these methods\nbased on their likelihood and classification error rates for various pattern\nrecognition data sets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:28:05 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 23:41:53 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 16:53:55 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2017 02:49:12 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Wang", "Baiyang", ""], ["Klabjan", "Diego", ""]]}, {"id": "1608.04428", "submitter": "Alexander Gaunt", "authors": "Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman,\n  Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow", "title": "TerpreT: A Probabilistic Programming Language for Program Induction", "comments": "50 pages, 20 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study machine learning formulations of inductive program synthesis; given\ninput-output examples, we try to synthesize source code that maps inputs to\ncorresponding outputs. Our aims are to develop new machine learning approaches\nbased on neural networks and graphical models, and to understand the\ncapabilities of machine learning techniques relative to traditional\nalternatives, such as those based on constraint solving from the programming\nlanguages community.\n  Our key contribution is the proposal of TerpreT, a domain-specific language\nfor expressing program synthesis problems. TerpreT is similar to a\nprobabilistic programming language: a model is composed of a specification of a\nprogram representation (declarations of random variables) and an interpreter\ndescribing how programs map inputs to outputs (a model connecting unknowns to\nobservations). The inference task is to observe a set of input-output examples\nand infer the underlying program. TerpreT has two main benefits. First, it\nenables rapid exploration of a range of domains, program representations, and\ninterpreter models. Second, it separates the model specification from the\ninference algorithm, allowing like-to-like comparisons between different\napproaches to inference. From a single TerpreT specification we automatically\nperform inference using four different back-ends. These are based on gradient\ndescent, linear program (LP) relaxations for graphical models, discrete\nsatisfiability solving, and the Sketch program synthesis system.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an empirical comparison between alternative inference\nalgorithms. Our key empirical finding is that constraint solvers dominate the\ngradient descent and LP-based formulations. We conclude with suggestions for\nthe machine learning community to make progress on program synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:34:50 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Brockschmidt", "Marc", ""], ["Singh", "Rishabh", ""], ["Kushman", "Nate", ""], ["Kohli", "Pushmeet", ""], ["Taylor", "Jonathan", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1608.04493", "submitter": "Anbang Yao", "authors": "Yiwen Guo, Anbang Yao, Yurong Chen", "title": "Dynamic Network Surgery for Efficient DNNs", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become a ubiquitous technology to improve machine\nintelligence. However, most of the existing deep models are structurally very\ncomplex, making them difficult to be deployed on the mobile platforms with\nlimited computational power. In this paper, we propose a novel network\ncompression method called dynamic network surgery, which can remarkably reduce\nthe network complexity by making on-the-fly connection pruning. Unlike the\nprevious methods which accomplish this task in a greedy way, we properly\nincorporate connection splicing into the whole process to avoid incorrect\npruning and make it as a continual network maintenance. The effectiveness of\nour method is proved with experiments. Without any accuracy loss, our method\ncan efficiently compress the number of parameters in LeNet-5 and AlexNet by a\nfactor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it\noutperforms the recent pruning method by considerable margins. Code and some\nmodels are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 06:23:05 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 00:17:25 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Guo", "Yiwen", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""]]}, {"id": "1608.04540", "submitter": "Gabriele Scheler", "authors": "Gabriele Scheler, Jean-Marc Fellous", "title": "Dopamine modulation of prefrontal delay activity-reverberatory activity\n  and sharpness of tuning curves", "comments": "CNS Conference 2001; 2 figures", "journal-ref": "Neurocomputing 38-40:1549-1556; June 2001", "doi": "10.1016/S0925-2312(01)00559-8", "report-no": null, "categories": "q-bio.NC cs.NE q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent electrophysiological experiments have shown that dopamine (D1)\nmodulation of pyramidal cells in prefrontal cortex reduces spike frequency\nadaptation and enhances NMDA transmission. Using four models, from\nmulticompartmental to integrate and fire, we examine the effects of these\nmodulations on sustained (delay) activity in a reverberatory network. We find\nthat D1 modulation may enable robust network bistability yielding selective\nreverberation among cells that code for a particular item or location. We\nfurther show that the tuning curve of such cells is sharpened, and that\nsignal-to-noise ratio is increased. We postulate that D1 modulation affects the\ntuning of \"memory fields\" and yield efficient distributed dynamic\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 10:14:55 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Scheler", "Gabriele", ""], ["Fellous", "Jean-Marc", ""]]}, {"id": "1608.04622", "submitter": "Filippo Maria Bianchi", "authors": "Sigurd L{\\o}kse, Filippo Maria Bianchi and Robert Jenssen", "title": "Training Echo State Networks with Regularization through Dimensionality\n  Reduction", "comments": null, "journal-ref": null, "doi": "10.1007/s12559-017-9450-z", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new framework to train an Echo State Network to\npredict real valued time-series. The method consists in projecting the output\nof the internal layer of the network on a space with lower dimensionality,\nbefore training the output layer to learn the target task. Notably, we enforce\na regularization constraint that leads to better generalization capabilities.\nWe evaluate the performances of our approach on several benchmark tests, using\ndifferent techniques to train the readout of the network, achieving superior\npredictive performance when using the proposed framework. Finally, we provide\nan insight on the effectiveness of the implemented mechanics through a\nvisualization of the trajectory in the phase space and relying on the\nmethodologies of nonlinear time-series analysis. By applying our method on well\nknown chaotic systems, we provide evidence that the lower dimensional embedding\nretains the dynamical properties of the underlying system better than the\nfull-dimensional internal states of the network.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:41:12 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["L\u00f8kse", "Sigurd", ""], ["Bianchi", "Filippo Maria", ""], ["Jenssen", "Robert", ""]]}, {"id": "1608.04980", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski, Francesco Visin, Yoshua Bengio", "title": "Mollifying Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of deep neural networks can be more challenging than\ntraditional convex optimization problems due to the highly non-convex nature of\nthe loss function, e.g. it can involve pathological landscapes such as\nsaddle-surfaces that can be difficult to escape for algorithms based on simple\ngradient descent. In this paper, we attack the problem of optimization of\nhighly non-convex neural networks by starting with a smoothed -- or\n\\textit{mollified} -- objective function that gradually has a more non-convex\nenergy landscape during the training. Our proposition is inspired by the recent\nstudies in continuation methods: similar to curriculum methods, we begin\nlearning an easier (possibly convex) objective function and let it evolve\nduring the training, until it eventually goes back to being the original,\ndifficult to optimize, objective function. The complexity of the mollified\nnetworks is controlled by a single hyperparameter which is annealed during the\ntraining. We show improvements on various difficult optimization tasks and\nestablish a relationship with recent works on continuation methods for neural\nnetworks and mollifiers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 14:37:34 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Visin", "Francesco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1608.05044", "submitter": "Marcos Cardinot", "authors": "Marcos Cardinot, Maud Gibbons, Colm O'Riordan, Josephine Griffith", "title": "Simulation of an Optional Strategy in the Prisoner's Dilemma in Spatial\n  and Non-spatial Environments", "comments": "12 pages, 8 figures. International Conference on the Simulation of\n  Adaptive Behavior", "journal-ref": "From Animals to Animats 14 (pp. 145-156). Springer International\n  Publishing, 2016", "doi": "10.1007/978-3-319-43488-9_14", "report-no": null, "categories": "cs.GT cs.MA cs.NE nlin.AO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents research comparing the effects of different environments\non the outcome of an extended Prisoner's Dilemma, in which agents have the\noption to abstain from playing the game. We consider three different pure\nstrategies: cooperation, defection and abstinence. We adopt an evolutionary\ngame theoretic approach and consider two different environments: the first\nwhich imposes no spatial constraints and the second in which agents are placed\non a lattice grid. We analyse the performance of the three strategies as we\nvary the loner's payoff in both structured and unstructured environments.\nFurthermore we also present the results of simulations which identify scenarios\nin which cooperative clusters of agents emerge and persist in both\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 18:50:51 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Cardinot", "Marcos", ""], ["Gibbons", "Maud", ""], ["O'Riordan", "Colm", ""], ["Griffith", "Josephine", ""]]}, {"id": "1608.05081", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed,\n  Li Deng", "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 20:00:04 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 18:20:55 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 18:01:04 GMT"}, {"version": "v4", "created": "Thu, 23 Nov 2017 10:24:17 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Ahmed", "Faisal", ""], ["Deng", "Li", ""]]}, {"id": "1608.05105", "submitter": "Helmut Katzgraber", "authors": "Roberto Santana, Zheng Zhu, and Helmut G. Katzgraber", "title": "Evolutionary Approaches to Optimization Problems in Chimera Topologies", "comments": "8 pages, 5 figures, 3 tables", "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO-2016), ACM Press, 397-404 (2016)", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chimera graphs define the topology of one of the first commercially available\nquantum computers. A variety of optimization problems have been mapped to this\ntopology to evaluate the behavior of quantum enhanced optimization heuristics\nin relation to other optimizers, being able to efficiently solve problems\nclassically to use them as benchmarks for quantum machines. In this paper we\ninvestigate for the first time the use of Evolutionary Algorithms (EAs) on\nIsing spin glass instances defined on the Chimera topology. Three genetic\nalgorithms (GAs) and three estimation of distribution algorithms (EDAs) are\nevaluated over $1000$ hard instances of the Ising spin glass constructed from\nSidon sets. We focus on determining whether the information about the topology\nof the graph can be used to improve the results of EAs and on identifying the\ncharacteristics of the Ising instances that influence the success rate of GAs\nand EDAs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 21:21:46 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Santana", "Roberto", ""], ["Zhu", "Zheng", ""], ["Katzgraber", "Helmut G.", ""]]}, {"id": "1608.05109", "submitter": "Ankur Sinha PhD", "authors": "Ankur Sinha, Janne R\\\"am\\\"o, Pekka Malo, Markku Kallio, Olli Tahvonen", "title": "Optimal Management of Naturally Regenerating Uneven-aged Forests", "comments": "29 pages, 11 tables and 13 figures", "journal-ref": null, "doi": "10.1016/j.ejor.2016.06.071", "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A shift from even-aged forest management to uneven-aged management practices\nleads to a problem rather different from the existing straightforward practice\nthat follows a rotation cycle of artificial regeneration, thinning of inferior\ntrees and a clearcut. A lack of realistic models and methods suggesting how to\nmanage uneven-aged stands in a way that is economically viable and ecologically\nsustainable creates difficulties in adopting this new management practice. To\ntackle this problem, we make a two-fold contribution in this paper. The first\ncontribution is the proposal of an algorithm that is able to handle a realistic\nuneven-aged stand management model that is otherwise computationally tedious\nand intractable. The model considered in this paper is an empirically estimated\nsize-structured ecological model for uneven-aged spruce forests. The second\ncontribution is on the sensitivity analysis of the forest model with respect to\na number of important parameters. The analysis provides us an insight into the\nbehavior of the uneven-aged forest model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 21:39:16 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Sinha", "Ankur", ""], ["R\u00e4m\u00f6", "Janne", ""], ["Malo", "Pekka", ""], ["Kallio", "Markku", ""], ["Tahvonen", "Olli", ""]]}, {"id": "1608.05513", "submitter": "Sagar Gandhi", "authors": "Shraddha Deshmukh, Sagar Gandhi, Pratap Sanap and Vivek Kulkarni", "title": "Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network", "comments": "This paper has been withdrawn by the author due to crucial evidence\n  that the similar work has already been published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a multi-level fuzzy min max neural network (MLF) was proposed,\nwhich improves the classification accuracy by handling an overlapped region\n(area of confusion) with the help of a tree structure. In this brief, an\nextension of MLF is proposed which defines a new boundary region, where the\npreviously proposed methods mark decisions with less confidence and hence\nmisclassification is more frequent. A methodology to classify patterns more\naccurately is presented. Our work enhances the testing procedure by means of\ndata centroids. We exhibit an illustrative example, clearly highlighting the\nadvantage of our approach. Results on standard datasets are also presented to\nevidentially prove a consistent improvement in the classification rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 07:05:33 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 08:09:40 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Deshmukh", "Shraddha", ""], ["Gandhi", "Sagar", ""], ["Sanap", "Pratap", ""], ["Kulkarni", "Vivek", ""]]}, {"id": "1608.05578", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Haploid-Diploid Evolutionary Algorithms", "comments": "arXiv admin note: text overlap with arXiv:1607.00318", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses the recent idea that the fundamental haploid-diploid\nlifecycle of eukaryotic organisms implements a rudimentary form of learning\nwithin evolution. A general approach for evolutionary computation is here\nderived that differs from all previous known work using diploid\nrepresentations. The primary role of recombination is also changed from that\npreviously considered in both natural and artificial evolution under the new\nview. Using well-known abstract tuneable models it is shown that varying\nfitness landscape ruggedness varies the benefit of the new approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 12:17:59 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 15:02:32 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 15:48:37 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1608.05745", "submitter": "Edward Choi", "authors": "Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz,\n  Walter F. Stewart, Jimeng Sun", "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse\n  Time Attention Mechanism", "comments": "Accepted at Neural Information Processing Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and interpretability are two dominant features of successful\npredictive models. Typically, a choice must be made in favor of complex black\nbox models such as recurrent neural networks (RNN) for accuracy versus less\naccurate but more interpretable traditional models such as logistic regression.\nThis tradeoff poses challenges in medicine where both accuracy and\ninterpretability are important. We addressed this challenge by developing the\nREverse Time AttentIoN model (RETAIN) for application to Electronic Health\nRecords (EHR) data. RETAIN achieves high accuracy while remaining clinically\ninterpretable and is based on a two-level neural attention model that detects\ninfluential past visits and significant clinical variables within those visits\n(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR\ndata in a reverse time order so that recent clinical visits are likely to\nreceive higher attention. RETAIN was tested on a large health system EHR\ndataset with 14 million visits completed by 263K patients over an 8 year period\nand demonstrated predictive accuracy and computational scalability comparable\nto state-of-the-art methods such as RNN, and ease of interpretability\ncomparable to traditional models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 21:54:46 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:03:43 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 19:45:03 GMT"}, {"version": "v4", "created": "Sun, 26 Feb 2017 15:13:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Choi", "Edward", ""], ["Bahadori", "Mohammad Taha", ""], ["Kulas", "Joshua A.", ""], ["Schuetz", "Andy", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1608.05916", "submitter": "Christophe Guyeux", "authors": "Jacques M. Bahi, Jean-Fran\\c{c}ois Couchot, Christophe Guyeux, and\n  Michel Salomon", "title": "Neural Networks and Chaos: Construction, Evaluation of Chaotic Networks,\n  and Prediction of Chaos with Multilayer Feedforward Networks", "comments": null, "journal-ref": "AIP Chaos, An Interdisciplinary Journal of Nonlinear Science.\n  22(1), 013122 (2012)", "doi": null, "report-no": null, "categories": "cs.NE math.DS nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research works deal with chaotic neural networks for various fields of\napplication. Unfortunately, up to now these networks are usually claimed to be\nchaotic without any mathematical proof. The purpose of this paper is to\nestablish, based on a rigorous theoretical framework, an equivalence between\nchaotic iterations according to Devaney and a particular class of neural\nnetworks. On the one hand we show how to build such a network, on the other\nhand we provide a method to check if a neural network is a chaotic one.\nFinally, the ability of classical feedforward multilayer perceptrons to learn\nsets of data obtained from a dynamical system is regarded. Various Boolean\nfunctions are iterated on finite states. Iterations of some of them are proven\nto be chaotic as it is defined by Devaney. In that context, important\ndifferences occur in the training process, establishing with various neural\nnetworks that chaotic behaviors are far more difficult to learn.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 10:48:36 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bahi", "Jacques M.", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Guyeux", "Christophe", ""], ["Salomon", "Michel", ""]]}, {"id": "1608.06027", "submitter": "Kamil Rocki", "authors": "Kamil M Rocki", "title": "Surprisal-Driven Feedback in Recurrent Networks", "comments": "ICLR 2017 submission, fixed some equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 01:42:45 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:26:02 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 04:42:02 GMT"}, {"version": "v4", "created": "Wed, 19 Oct 2016 04:32:46 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Rocki", "Kamil M", ""]]}, {"id": "1608.06037", "submitter": "SeyyedHossein Hasanpour Matikolaee", "authors": "Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad\n  Sabokrou", "title": "Lets keep it simple, Using simple architectures to outperform deeper and\n  more complex architectures", "comments": "replaced low-res images with high-res versions, minor corrections in\n  the appendix, switched to LaTex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet,\nResNet, GoogleNet, include tens to hundreds of millions of parameters, which\nimpose considerable computation and memory overhead. This limits their\npractical use for training, optimization and memory efficiency. On the\ncontrary, light-weight architectures, being proposed to address this issue,\nmainly suffer from low accuracy. These inefficiencies mostly stem from\nfollowing an ad hoc procedure. We propose a simple architecture, called\nSimpleNet, based on a set of designing principles, with which we empirically\nshow, a well-crafted yet simple and reasonably deep architecture can perform on\npar with deeper and more complex architectures. SimpleNet provides a good\ntradeoff between the computation/memory efficiency and the accuracy. Our simple\n13-layer architecture outperforms most of the deeper and complex architectures\nto date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks\nwhile having 2 to 25 times fewer number of parameters and operations. This\nmakes it very handy for embedded system or system with computational and memory\nlimitations. We achieved state-of-the-art result on CIFAR10 outperforming\nseveral heavier architectures, near state of the art on MNIST and competitive\nresults on CIFAR100 and SVHN. Models are made available at:\nhttps://github.com/Coderx7/SimpleNet\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 02:50:57 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 18:36:14 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 14:08:46 GMT"}, {"version": "v4", "created": "Mon, 15 May 2017 17:30:05 GMT"}, {"version": "v5", "created": "Sat, 6 Jan 2018 19:08:55 GMT"}, {"version": "v6", "created": "Tue, 13 Feb 2018 18:09:24 GMT"}, {"version": "v7", "created": "Wed, 14 Feb 2018 09:19:10 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Hasanpour", "Seyyed Hossein", ""], ["Rouhani", "Mohammad", ""], ["Fayyaz", "Mohsen", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "1608.06132", "submitter": "J\\\"orn Fischer", "authors": "J. Fischer, P. Manoonpong, S. Lackner", "title": "Reconstructing Neural Parameters and Synapses of arbitrary\n  interconnected Neurons from their Simulated Spiking Activity", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the behavior of a neural circuit it is a presupposition that we\nhave a model of the dynamical system describing this circuit. This model is\ndetermined by several parameters, including not only the synaptic weights, but\nalso the parameters of each neuron. Existing works mainly concentrate on either\nthe synaptic weights or the neural parameters. In this paper we present an\nalgorithm to reconstruct all parameters including the synaptic weights of a\nspiking neuron model. The model based on works of Eugene M. Izhikevich\n(Izhikevich 2007) consists of two differential equations and covers different\ntypes of cortical neurons. It combines the dynamical properties of\nHodgkin-Huxley-type dynamics with a high computational efficiency. The\npresented algorithm uses the recordings of the corresponding membrane\npotentials of the model for the reconstruction and consists of two main\ncomponents. The first component is a rank based Genetic Algorithm (GA) which is\nused to find the neural parameters of the model. The second one is a Least Mean\nSquares approach which computes the synaptic weights of all interconnected\nneurons by minimizing the squared error between the calculated and the measured\nmembrane potentials for each time step. In preparation for the reconstruction\nof the neural parameters and of the synaptic weights from real measured\nmembrane potentials, promising results based on simulated data generated with a\nrandomly parametrized Izhikevich model are presented. The reconstruction does\nnot only converge to a global minimum of neural parameters, but also\napproximates the synaptic weights with high precision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 11:47:18 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Fischer", "J.", ""], ["Manoonpong", "P.", ""], ["Lackner", "S.", ""]]}, {"id": "1608.06514", "submitter": "Ke Li Kl", "authors": "Renzhi Chen, Ke Li and Xin Yao", "title": "Dynamic Multi-Objectives Optimization with a Changing Number of\n  Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies on dynamic multi-objective optimization focus on problems\nwith time-dependent objective functions, while the ones with a changing number\nof objectives have rarely been considered in the literature. Instead of\nchanging the shape or position of the Pareto-optimal front/set when having\ntime-dependent objective functions, increasing or decreasing the number of\nobjectives usually leads to the expansion or contraction of the dimension of\nthe Pareto-optimal front/set manifold. Unfortunately, most existing dynamic\nhandling techniques can hardly be adapted to this type of dynamics. In this\npaper, we report our attempt toward tackling the dynamic multi-objective\noptimization problems with a changing number of objectives. We implement a new\ntwo-archive evolutionary algorithm which maintains two co-evolving populations\nsimultaneously. In particular, these two populations are complementary to each\nother: one concerns more about the convergence while the other concerns more\nabout the diversity. The compositions of these two populations are adaptively\nreconstructed once the environment changes. In addition, these two populations\ninteract with each other via a mating selection mechanism. Comprehensive\nexperiments are conducted on various benchmark problems with a time-dependent\nnumber of objectives. Empirical results fully demonstrate the effectiveness of\nour proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 13:57:54 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 10:08:22 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Chen", "Renzhi", ""], ["Li", "Ke", ""], ["Yao", "Xin", ""]]}, {"id": "1608.06627", "submitter": "Purnima Pandit", "authors": "Purnima Pandit, A. Anand", "title": "Artificial Neural Networks for Detection of Malaria in RBCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is one of the most common diseases caused by mosquitoes and is a\ngreat public health problem worldwide. Currently, for malaria diagnosis the\nstandard technique is microscopic examination of a stained blood film. We\npropose use of Artificial Neural Networks (ANN) for the diagnosis of the\ndisease in the red blood cell. For this purpose features / parameters are\ncomputed from the data obtained by the digital holographic images of the blood\ncells and is given as input to ANN which classifies the cell as the infected\none or otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 06:01:19 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Pandit", "Purnima", ""], ["Anand", "A.", ""]]}, {"id": "1608.06884", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "Towards Bayesian Deep Learning: A Framework and Some Existing Methods", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 2016. This is a slightly shorter version of the survey\n  arXiv:1604.01662", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While perception tasks such as visual object recognition and text\nunderstanding play an important role in human intelligence, the subsequent\ntasks that involve inference, reasoning and planning require an even higher\nlevel of intelligence. The past few years have seen major advances in many\nperception tasks using deep learning models. For higher-level inference,\nhowever, probabilistic graphical models with their Bayesian nature are still\nmore powerful and flexible. To achieve integrated intelligence that involves\nboth perception and inference, it is naturally desirable to tightly integrate\ndeep learning and Bayesian models within a principled probabilistic framework,\nwhich we call Bayesian deep learning. In this unified framework, the perception\nof text or images using deep learning can boost the performance of higher-level\ninference and in return, the feedback from the inference process is able to\nenhance the perception of text or images. This paper proposes a general\nframework for Bayesian deep learning and reviews its recent applications on\nrecommender systems, topic models, and control. In this paper, we also discuss\nthe relationship and differences between Bayesian deep learning and other\nrelated topics like Bayesian treatment of neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:15:22 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 15:32:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1608.06902", "submitter": "Joachim Ott", "authors": "Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, Yoshua Bengio", "title": "Recurrent Neural Networks With Limited Numerical Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) produce state-of-art performance on many\nmachine learning tasks but their demand on resources in terms of memory and\ncomputational power are often high. Therefore, there is a great interest in\noptimizing the computations performed with these models especially when\nconsidering development of specialized low-power hardware for deep networks.\nOne way of reducing the computational needs is to limit the numerical precision\nof the network weights and biases. This has led to different proposed rounding\nmethods which have been applied so far to only Convolutional Neural Networks\nand Fully-Connected Networks. This paper addresses the question of how to best\nreduce weight precision during training in the case of RNNs. We present results\nfrom the use of different stochastic and deterministic reduced precision\ntraining methods applied to three major RNN types which are then tested on\nseveral datasets. The results show that the weight binarization methods do not\nwork with the RNNs. However, the stochastic and deterministic ternarization,\nand pow2-ternarization methods gave rise to low-precision RNNs that produce\nsimilar and even higher accuracy on certain datasets therefore providing a path\ntowards training more efficient implementations of RNNs in specialized\nhardware.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 17:15:29 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 14:01:40 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Ott", "Joachim", ""], ["Lin", "Zhouhan", ""], ["Zhang", "Ying", ""], ["Liu", "Shih-Chii", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1608.07373", "submitter": "Jen-Yu Liu", "authors": "Jen-Yu Liu, Shyh-Kang Jeng, Yi-Hsuan Yang", "title": "Applying Topological Persistence in Convolutional Neural Network for\n  Music Audio Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an increased interest in the application of\npersistent homology, a topological tool for data analysis, to machine learning\nproblems. Persistent homology is known for its ability to numerically\ncharacterize the shapes of spaces induced by features or functions. On the\nother hand, deep neural networks have been shown effective in various tasks. To\nour best knowledge, however, existing neural network models seldom exploit\nshape information. In this paper, we investigate a way to use persistent\nhomology in the framework of deep neural networks. Specifically, we propose to\nembed the so-called \"persistence landscape,\" a rather new topological summary\nfor data, into a convolutional neural network (CNN) for dealing with audio\nsignals. Our evaluation on automatic music tagging, a multi-label\nclassification task, shows that the resulting persistent convolutional neural\nnetwork (PCNN) model can perform significantly better than state-of-the-art\nmodels in prediction accuracy. We also discuss the intuition behind the design\nof the proposed model, and offer insights into the features that it learns.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 07:14:37 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Liu", "Jen-Yu", ""], ["Jeng", "Shyh-Kang", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1608.07955", "submitter": "Yangqi Huang", "authors": "Yangqi Huang, Wang Kang, Xichao Zhang, Yan Zhou and Weisheng Zhao", "title": "Magnetic skyrmion-based synaptic devices", "comments": null, "journal-ref": "Nanotechnology 28, 08LT02 (2017)", "doi": "10.1088/1361-6528/aa5838", "report-no": null, "categories": "cs.ET cond-mat.str-el cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic skyrmions are promising candidates for next-generation information\ncarriers, owing to their small size, topological stability, and ultralow\ndepinning current density. A wide variety of skyrmionic device concepts and\nprototypes have been proposed, highlighting their potential applications. Here,\nwe report on a bioinspired skyrmionic device with synaptic plasticity. The\nsynaptic weight of the proposed device can be strengthened/weakened by\npositive/negative stimuli, mimicking the potentiation/depression process of a\nbiological synapse. Both short-term plasticity(STP) and long-term\npotentiation(LTP) functionalities have been demonstrated for a spiking\ntime-dependent plasticity(STDP) scheme. This proposal suggests new\npossibilities for synaptic devices for use in spiking neuromorphic computing\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 09:05:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Huang", "Yangqi", ""], ["Kang", "Wang", ""], ["Zhang", "Xichao", ""], ["Zhou", "Yan", ""], ["Zhao", "Weisheng", ""]]}, {"id": "1608.08225", "submitter": "Max Tegmark", "authors": "Henry W. Lin (Harvard), Max Tegmark (MIT), David Rolnick (MIT)", "title": "Why does deep and cheap learning work so well?", "comments": "Replaced to match version published in Journal of Statistical\n  Physics: https://link.springer.com/article/10.1007/s10955-017-1836-5 Improved\n  refs & discussion, typos fixed. 16 pages, 3 figs", "journal-ref": null, "doi": "10.1007/s10955-017-1836-5", "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the success of deep learning could depend not only on mathematics\nbut also on physics: although well-known mathematical theorems guarantee that\nneural networks can approximate arbitrary functions well, the class of\nfunctions of practical interest can frequently be approximated through \"cheap\nlearning\" with exponentially fewer parameters than generic ones. We explore how\nproperties frequently encountered in physics such as symmetry, locality,\ncompositionality, and polynomial log-probability translate into exceptionally\nsimple neural networks. We further argue that when the statistical process\ngenerating the data is of a certain hierarchical form prevalent in physics and\nmachine-learning, a deep neural network can be more efficient than a shallow\none. We formalize these claims using information theory and discuss the\nrelation to the renormalization group. We prove various \"no-flattening\ntheorems\" showing when efficient linear deep networks cannot be accurately\napproximated by shallow ones without efficiency loss, for example, we show that\n$n$ variables cannot be multiplied using fewer than 2^n neurons in a single\nhidden layer.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 20:00:14 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 00:38:45 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 02:15:43 GMT"}, {"version": "v4", "created": "Thu, 3 Aug 2017 18:32:53 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lin", "Henry W.", "", "Harvard"], ["Tegmark", "Max", "", "MIT"], ["Rolnick", "David", "", "MIT"]]}, {"id": "1608.08265", "submitter": "J\\\"orn Fischer", "authors": "J. Fischer, S. Lackner", "title": "About Learning in Recurrent Bistable Gradient Networks", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Bistable Gradient Networks are attractor based neural networks\ncharacterized by bistable dynamics of each single neuron. Coupled together\nusing linear interaction determined by the interconnection weights, these\nnetworks do not suffer from spurious states or very limited capacity anymore.\nVladimir Chinarov and Michael Menzinger, who invented these networks, trained\nthem using Hebb's learning rule. We show, that this way of computing the\nweights leads to unwanted behaviour and limitations of the networks\ncapabilities. Furthermore we evince, that using the first order of Hintons\nContrastive Divergence algorithm leads to a quite promising recurrent neural\nnetwork. These findings are tested by learning images of the MNIST database for\nhandwritten numbers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 22:02:39 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Fischer", "J.", ""], ["Lackner", "S.", ""]]}, {"id": "1608.08267", "submitter": "Peter Diehl Peter U. Diehl", "authors": "Peter U. Diehl and Matthew Cook", "title": "Learning and Inferring Relations in Cortical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pressing scientific challenge is to understand how brains work. Of\nparticular interest is the neocortex,the part of the brain that is especially\nlarge in humans, capable of handling a wide variety of tasks including visual,\nauditory, language, motor, and abstract processing. These functionalities are\nprocessed in different self-organized regions of the neocortical sheet, and yet\nthe anatomical structure carrying out the processing is relatively uniform\nacross the sheet. We are at a loss to explain, simulate, or understand such a\nmulti-functional homogeneous sheet-like computational structure - we do not\nhave computational models which work in this way. Here we present an important\nstep towards developing such models: we show how uniform modules of excitatory\nand inhibitory neurons can be connected bidirectionally in a network that, when\nexposed to input in the form of population codes, learns the input encodings as\nwell as the relationships between the inputs. STDP learning rules lead the\nmodules to self-organize into a relational network, which is able to infer\nmissing inputs,restore noisy signals, decide between conflicting inputs, and\ncombine cues to improve estimates. These networks show that it is possible for\na homogeneous network of spiking units to self-organize so as to provide\nmeaningful processing of its inputs. If such networks can be scaled up, they\ncould provide an initial computational model relevant to the large scale\nanatomy of the neocortex.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 22:11:01 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["Diehl", "Peter U.", ""], ["Cook", "Matthew", ""]]}, {"id": "1608.08435", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "comments": "6 pages, 7 figures, 7 tables, ICARCV", "journal-ref": null, "doi": "10.1109/ICARCV.2014.7064375", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an Extreme Learning Machine (ELM) based technique for\nMulti-label classification problems is proposed and discussed. In multi-label\nclassification, each of the input data samples belongs to one or more than one\nclass labels. The traditional binary and multi-class classification problems\nare the subset of the multi-label problem with the number of labels\ncorresponding to each sample limited to one. The proposed ELM based multi-label\nclassification technique is evaluated with six different benchmark multi-label\ndatasets from different domains such as multimedia, text and biology. A\ndetailed comparison of the results is made by comparing the proposed method\nwith the results from nine state of the arts techniques for five different\nevaluation metrics. The nine methods are chosen from different categories of\nmulti-label methods. The comparative results shows that the proposed Extreme\nLearning Machine based multi-label classification technique is a better\nalternative than the existing state of the art methods for multi-label\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 13:08:06 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""]]}, {"id": "1608.08607", "submitter": "Ke Li Kl", "authors": "Mengyuan Wu, Ke Li, Sam Kwong, Yu Zhou and Qingfu Zhang", "title": "Matching-Based Selection with Incomplete Lists for Decomposition\n  Multi-Objective Optimization", "comments": "27 pages, 3 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balance between convergence and diversity is a key issue of evolutionary\nmulti-objective optimization. The recently proposed stable matching-based\nselection provides a new perspective to handle this balance under the framework\nof decomposition multi-objective optimization. In particular, the stable\nmatching between subproblems and solutions, which achieves an equilibrium\nbetween their mutual preferences, implicitly strikes a balance between the\nconvergence and diversity. Nevertheless, the original stable matching model has\na high risk of matching a solution with a unfavorable subproblem which finally\nleads to an imbalanced selection result. In this paper, we propose an adaptive\ntwo-level stable matching-based selection for decomposition multi-objective\noptimization. Specifically, borrowing the idea of stable matching with\nincomplete lists, we match each solution with one of its favorite subproblems\nby restricting the length of its preference list during the first-level stable\nmatching. During the second-level stable matching, the remaining subproblems\nare thereafter matched with their favorite solutions according to the classic\nstable matching model. In particular, we develop an adaptive mechanism to\nautomatically set the length of preference list for each solution according to\nits local competitiveness. The performance of our proposed method is validated\nand compared with several state-of-the-art evolutionary multi-objective\noptimization algorithms on 62 benchmark problem instances. Empirical results\nfully demonstrate the competitive performance of our proposed method on\nproblems with complicated Pareto sets and those with more than three\nobjectives.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 19:30:25 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 11:03:57 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Wu", "Mengyuan", ""], ["Li", "Ke", ""], ["Kwong", "Sam", ""], ["Zhou", "Yu", ""], ["Zhang", "Qingfu", ""]]}, {"id": "1608.08782", "submitter": "Jun Haeng Lee", "authors": "Jun Haeng Lee, Tobi Delbruck and Michael Pfeiffer", "title": "Training Deep Spiking Neural Networks using Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep spiking neural networks (SNNs) hold great potential for improving the\nlatency and energy efficiency of deep neural networks through event-based\ncomputation. However, training such networks is difficult due to the\nnon-differentiable nature of asynchronous spike events. In this paper, we\nintroduce a novel technique, which treats the membrane potentials of spiking\nneurons as differentiable signals, where discontinuities at spike times are\nonly considered as noise. This enables an error backpropagation mechanism for\ndeep SNNs, which works directly on spike signals and membrane potentials. Thus,\ncompared with previous methods relying on indirect training and conversion, our\ntechnique has the potential to capture the statics of spikes more precisely.\nOur novel framework outperforms all previously reported results for SNNs on the\npermutation invariant MNIST benchmark, as well as the N-MNIST benchmark\nrecorded with event-based vision sensors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 09:21:17 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Lee", "Jun Haeng", ""], ["Delbruck", "Tobi", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1608.08898", "submitter": "Rajasekar Venkatesan", "authors": "Meng Joo Er, Rajasekar Venkatesan and Ning Wang", "title": "A High Speed Multi-label Classifier based on Extreme Learning Machines", "comments": "12 pages, 2 figures, 10 tables", "journal-ref": null, "doi": "10.1007/978-3-319-28373-9_37", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a high speed neural network classifier based on extreme\nlearning machines for multi-label classification problem is proposed and\ndis-cussed. Multi-label classification is a superset of traditional binary and\nmulti-class classification problems. The proposed work extends the extreme\nlearning machine technique to adapt to the multi-label problems. As opposed to\nthe single-label problem, both the number of labels the sample belongs to, and\neach of those target labels are to be identified for multi-label classification\nresulting in in-creased complexity. The proposed high speed multi-label\nclassifier is applied to six benchmark datasets comprising of different\napplication areas such as multi-media, text and biology. The training time and\ntesting time of the classifier are compared with those of the state-of-the-arts\nmethods. Experimental studies show that for all the six datasets, our proposed\ntechnique have faster execution speed and better performance, thereby\noutperforming all the existing multi-label clas-sification methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 14:56:12 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""], ["Wang", "Ning", ""]]}, {"id": "1608.08905", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er, Shiqian Wu, Mahardhika Pratama", "title": "A Novel Online Real-time Classifier for Multi-label Data Streams", "comments": "8 pages, 7 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:1609.00086", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel extreme learning machine based online multi-label\nclassifier for real-time data streams is proposed. Multi-label classification\nis one of the actively researched machine learning paradigm that has gained\nmuch attention in the recent years due to its rapidly increasing real world\napplications. In contrast to traditional binary and multi-class classification,\nmulti-label classification involves association of each of the input samples\nwith a set of target labels simultaneously. There are no real-time online\nneural network based multi-label classifier available in the literature. In\nthis paper, we exploit the inherent nature of high speed exhibited by the\nextreme learning machines to develop a novel online real-time classifier for\nmulti-label data streams. The developed classifier is experimented with\ndatasets from different application domains for consistency, performance and\nspeed. The experimental studies show that the proposed method outperforms the\nexisting state-of-the-art techniques in terms of speed and accuracy and can\nclassify multi-label data streams in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 15:14:06 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""], ["Wu", "Shiqian", ""], ["Pratama", "Mahardhika", ""]]}]