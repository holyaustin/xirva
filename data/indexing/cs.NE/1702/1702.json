[{"id": "1702.00071", "submitter": "Eugene Vorontsov", "authors": "Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, Chris Pal", "title": "On orthogonality and learning recurrent networks with long term\n  dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that it is challenging to train deep neural networks and\nrecurrent neural networks for tasks that exhibit long term dependencies. The\nvanishing or exploding gradient problem is a well known issue associated with\nthese challenges. One approach to addressing vanishing and exploding gradients\nis to use either soft or hard constraints on weight matrices so as to encourage\nor enforce orthogonality. Orthogonal matrices preserve gradient norm during\nbackpropagation and may therefore be a desirable property. This paper explores\nissues with optimization convergence, speed and gradient stability when\nencouraging or enforcing orthogonality. To perform this analysis, we propose a\nweight matrix factorization and parameterization strategy through which we can\nbound matrix norms and therein control the degree of expansivity induced during\nbackpropagation. We find that hard constraints on orthogonality can negatively\naffect the speed of convergence and model performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 22:14:59 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 11:09:28 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 23:12:14 GMT"}, {"version": "v4", "created": "Thu, 12 Oct 2017 17:18:51 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Vorontsov", "Eugene", ""], ["Trabelsi", "Chiheb", ""], ["Kadoury", "Samuel", ""], ["Pal", "Chris", ""]]}, {"id": "1702.00159", "submitter": "Wei Du", "authors": "Wei Du, Yang Tang, Sunney Yung Sun Leung, Le Tong, Athanasios V.\n  Vasilakos, Feng Qian", "title": "Robust Order Scheduling in the Fashion Industry: A Multi-Objective\n  Optimization Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fashion industry, order scheduling focuses on the assignment of\nproduction orders to appropriate production lines. In reality, before a new\norder can be put into production, a series of activities known as\npre-production events need to be completed. In addition, in real production\nprocess, owing to various uncertainties, the daily production quantity of each\norder is not always as expected. In this research, by considering the\npre-production events and the uncertainties in the daily production quantity,\nrobust order scheduling problems in the fashion industry are investigated with\nthe aid of a multi-objective evolutionary algorithm (MOEA) called nondominated\nsorting adaptive differential evolution (NSJADE). The experimental results\nillustrate that it is of paramount importance to consider pre-production events\nin order scheduling problems in the fashion industry. We also unveil that the\nexistence of the uncertainties in the daily production quantity heavily affects\nthe order scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 08:33:21 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Du", "Wei", ""], ["Tang", "Yang", ""], ["Leung", "Sunney Yung Sun", ""], ["Tong", "Le", ""], ["Vasilakos", "Athanasios V.", ""], ["Qian", "Feng", ""]]}, {"id": "1702.00288", "submitter": "Yi Zhang", "authors": "Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin, Yang Chen, Peixi\n  Liao, Jiliu Zhou, and Ge Wang", "title": "Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network\n  (RED-CNN)", "comments": "Accepted by IEEE TMI", "journal-ref": null, "doi": "10.1109/TMI.2017.2715284", "report-no": null, "categories": "physics.med-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the potential X-ray radiation risk to the patient, low-dose CT has\nattracted a considerable interest in the medical imaging field. The current\nmain stream low-dose CT methods include vendor-specific sinogram domain\nfiltration and iterative reconstruction, but they need to access original raw\ndata whose formats are not transparent to most users. Due to the difficulty of\nmodeling the statistical characteristics in the image domain, the existing\nmethods for directly processing reconstructed images cannot eliminate image\nnoise very well while keeping structural details. Inspired by the idea of deep\nlearning, here we combine the autoencoder, the deconvolution network, and\nshortcut connections into the residual encoder-decoder convolutional neural\nnetwork (RED-CNN) for low-dose CT imaging. After patch-based training, the\nproposed RED-CNN achieves a competitive performance relative to\nthe-state-of-art methods in both simulated and clinical cases. Especially, our\nmethod has been favorably evaluated in terms of noise suppression, structural\npreservation and lesion detection.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 14:44:17 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 07:17:03 GMT"}, {"version": "v3", "created": "Sun, 11 Jun 2017 16:53:43 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chen", "Hu", ""], ["Zhang", "Yi", ""], ["Kalra", "Mannudeep K.", ""], ["Lin", "Feng", ""], ["Chen", "Yang", ""], ["Liao", "Peixi", ""], ["Zhou", "Jiliu", ""], ["Wang", "Ge", ""]]}, {"id": "1702.00477", "submitter": "Miqing Li", "authors": "Miqing Li, Xin Yao", "title": "Dominance Move: A Measure of Comparing Solution Sets in Multiobjective\n  Optimization", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common approaches for multiobjective optimization is to\ngenerate a solution set that well approximates the whole Pareto-optimal\nfrontier to facilitate the later decision-making process. However, how to\nevaluate and compare the quality of different solution sets remains\nchallenging. Existing measures typically require additional problem knowledge\nand information, such as a reference point or a substituted set of the\nPareto-optimal frontier. In this paper, we propose a quality measure, called\ndominance move (DoM), to compare solution sets generated by multiobjective\noptimizers. Given two solution sets, DoM measures the minimum sum of move\ndistances for one set to weakly Pareto dominate the other set. DoM can be seen\nas a natural reflection of the difference between two solutions, capturing all\naspects of solution sets' quality, being compliant with Pareto dominance, and\ndoes not need any additional problem knowledge and parameters. We present an\nexact method to calculate the DoM in the biobjective case. We show the\nnecessary condition of constructing the optimal partition for a solution set's\nminimum move, and accordingly propose an efficient algorithm to recursively\ncalculate the DoM. Finally, DoM is evaluated on several groups of artificial\nand real test cases as well as by a comparison with two well-established\nquality measures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 22:14:15 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Li", "Miqing", ""], ["Yao", "Xin", ""]]}, {"id": "1702.00493", "submitter": "Wentao Huang", "authors": "Wentao Huang, Xin Huang and Kechen Zhang", "title": "Information-theoretic interpretation of tuning curves for multiple\n  motion directions", "comments": "The 51st Annual Conference on Information Sciences and Systems\n  (CISS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NE math.IT q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an efficient information-maximization method for computing\nthe optimal shapes of tuning curves of sensory neurons by optimizing the\nparameters of the underlying feedforward network model. When applied to the\nproblem of population coding of visual motion with multiple directions, our\nmethod yields several types of tuning curves with both symmetric and asymmetric\nshapes that resemble what have been found in the visual cortex. Our result\nsuggests that the diversity or heterogeneity of tuning curve shapes as observed\nin neurophysiological experiment might actually constitute an optimal\npopulation representation of visual motions with multiple components.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 23:03:18 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Huang", "Wentao", ""], ["Huang", "Xin", ""], ["Zhang", "Kechen", ""]]}, {"id": "1702.00614", "submitter": "Miguel Aguilera", "authors": "Miguel Aguilera, Manuel G. Bedia", "title": "Learning Criticality in an Embodied Boltzmann Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological and cognitive systems do not operate deep into one or other\nregime of activity. Instead, they exploit critical surfaces poised at\ntransitions in their parameter space. The pervasiveness of criticality in\nnatural systems suggests that there may be general principles inducing this\nbehaviour. However, there is a lack of conceptual models explaining how\nembodied agents propel themselves towards these critical points. In this paper,\nwe present a learning model driving an embodied Boltzmann Machine towards\ncritical behaviour by maximizing the heat capacity of the network. We test and\ncorroborate the model implementing an embodied agent in the mountain car\nbenchmark, controlled by a Boltzmann Machine that adjust its weights according\nto the model. We find that the neural controller reaches a point of\ncriticality, which coincides with a transition point of the behaviour of the\nagent between two regimes of behaviour, maximizing the synergistic information\nbetween its sensors and the hidden and motor neurons. Finally, we discuss the\npotential of our learning model to study the contribution of criticality to the\nbehaviour of embodied living systems in scenarios not necessarily constrained\nby biological restrictions of the examples of criticality we find in nature.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 11:04:48 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Aguilera", "Miguel", ""], ["Bedia", "Manuel G.", ""]]}, {"id": "1702.00768", "submitter": "Xerxes D. Arsiwalla", "authors": "Riccardo Zucca, Xerxes D. Arsiwalla, Hoang Le, Mikail Rubinov, Paul\n  Verschure", "title": "Scaling Properties of Human Brain Functional Networks", "comments": "International Conference on Artificial Neural Networks - ICANN 2016", "journal-ref": "Artificial Neural Networks and Machine Learning, Lecture Notes in\n  Computer Science, vol 9886, 2016", "doi": "10.1007/978-3-319-44778-0_13", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate scaling properties of human brain functional networks in the\nresting-state. Analyzing network degree distributions, we statistically test\nwhether their tails scale as power-law or not. Initial studies, based on\nleast-squares fitting, were shown to be inadequate for precise estimation of\npower-law distributions. Subsequently, methods based on maximum-likelihood\nestimators have been proposed and applied to address this question.\nNevertheless, no clear consensus has emerged, mainly because results have shown\nsubstantial variability depending on the data-set used or its resolution. In\nthis study, we work with high-resolution data (10K nodes) from the Human\nConnectome Project and take into account network weights. We test for the\npower-law, exponential, log-normal and generalized Pareto distributions. Our\nresults show that the statistics generally do not support a power-law, but\ninstead these degree distributions tend towards the thin-tail limit of the\ngeneralized Pareto model. This may have implications for the number of hubs in\nhuman brain functional networks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 18:01:07 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Zucca", "Riccardo", ""], ["Arsiwalla", "Xerxes D.", ""], ["Le", "Hoang", ""], ["Rubinov", "Mikail", ""], ["Verschure", "Paul", ""]]}, {"id": "1702.00815", "submitter": "Nicolas Heslot", "authors": "Vitaliy Feoktistov, Stephane Pietravalle, Nicolas Heslot", "title": "Optimal Experimental Design of Field Trials using Differential Evolution", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When setting up field experiments, to test and compare a range of genotypes\n(e.g. maize hybrids), it is important to account for any possible field effect\nthat may otherwise bias performance estimates of genotypes. To do so, we\npropose a model-based method aimed at optimizing the allocation of the tested\ngenotypes and checks between fields and placement within field, according to\ntheir kinship. This task can be formulated as a combinatorial permutation-based\nproblem. We used Differential Evolution concept to solve this problem. We then\npresent results of optimal strategies for between-field and within-field\nplacements of genotypes and compare them to existing optimization strategies,\nboth in terms of convergence time and result quality. The new algorithm gives\npromising results in terms of convergence and search space exploration.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 17:25:41 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 08:06:02 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Feoktistov", "Vitaliy", ""], ["Pietravalle", "Stephane", ""], ["Heslot", "Nicolas", ""]]}, {"id": "1702.00837", "submitter": "Juan Biondi", "authors": "Juan Biondi, Gerardo Fernandez, Silvia Castro, Osvaldo Agamennoni", "title": "Eye-Movement behavior identification for AD diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we develop a deep-learning approach for differentiating\nthe eye-movement behavior of people with neurodegenerative diseases over\nhealthy control subjects during reading well-defined sentences. We define an\ninformation compaction of the eye-tracking data of subjects without and with\nprobable Alzheimer's disease when reading a set of well-defined, previously\nvalidated, sentences including high-, low-predictable sentences, and proverbs.\nUsing this information we train a set of denoising sparse-autoencoders and\nbuild a deep neural network with these and a softmax classifier. Our results\nare very promising and show that these models may help to understand the\ndynamics of eye movement behavior and its relationship with underlying\nneuropsychological correlates.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 21:47:02 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 15:08:34 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 13:14:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Biondi", "Juan", ""], ["Fernandez", "Gerardo", ""], ["Castro", "Silvia", ""], ["Agamennoni", "Osvaldo", ""]]}, {"id": "1702.00887", "submitter": "Yoon Kim", "authors": "Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush", "title": "Structured Attention Networks", "comments": "ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 01:40:45 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 16:37:44 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 17:52:03 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Kim", "Yoon", ""], ["Denton", "Carl", ""], ["Hoang", "Luong", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1702.00993", "submitter": "Casey Kneale", "authors": "Casey Kneale, Karl S. Booksh", "title": "Robust Particle Swarm Optimizer based on Chemomimicry", "comments": "Revision # 2. Included pseudo code, more references, changed\n  description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particle swarm optimizer (PSO) loosely based on the phenomena of\ncrystallization and a chaos factor which follows the complimentary error\nfunction is described. The method features three phases: diffusion, directed\nmotion, and nucleation. During the diffusion phase random walk is the only\ncontributor to particle motion. As the algorithm progresses the contribution\nfrom chaos decreases and movement toward global best locations is pursued until\nconvergence has occurred. The algorithm was found to be more robust to local\nminima in multimodal test functions than a standard PSO algorithm and is\ndesigned for problems which feature experimental precision.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 13:07:35 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:15:35 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kneale", "Casey", ""], ["Booksh", "Karl S.", ""]]}, {"id": "1702.01692", "submitter": "Christian Schulz", "authors": "Peter Sanders, Christian Schulz, Darren Strash and Robert Williger", "title": "Distributed Evolutionary k-way Node Separators", "comments": "arXiv admin note: text overlap with arXiv:1509.01190, arXiv:1110.0477", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing high quality node separators in large graphs is necessary for a\nvariety of applications, ranging from divide-and-conquer algorithms to VLSI\ndesign. In this work, we present a novel distributed evolutionary algorithm\ntackling the k-way node separator problem. A key component of our contribution\nincludes new k-way local search algorithms based on maximum flows. We combine\nour local search with a multilevel approach to compute an initial population\nfor our evolutionary algorithm, and further show how to modify the coarsening\nstage of our multilevel algorithm to create effective combine and mutation\noperations. Lastly, we combine these techniques with a scalable communication\nprotocol, producing a system that is able to compute high quality solutions in\na short amount of time. Our experiments against competing algorithms show that\nour advanced evolutionary algorithm computes the best result on 94% of the\nchosen benchmark instances.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 16:34:27 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["Williger", "Robert", ""]]}, {"id": "1702.01780", "submitter": "Randal Olson", "authors": "Andrew Sohn and Randal S. Olson and Jason H. Moore", "title": "Toward the automated analysis of complex diseases in genome-wide\n  association studies using genetic programming", "comments": "9 pages, 4 figures, submitted to GECCO 2017 conference and currently\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been gaining traction in recent years to meet the demand\nfor tools that can efficiently analyze and make sense of the ever-growing\ndatabases of biomedical data in health care systems around the world. However,\neffectively using machine learning methods requires considerable domain\nexpertise, which can be a barrier of entry for bioinformaticians new to\ncomputational data science methods. Therefore, off-the-shelf tools that make\nmachine learning more accessible can prove invaluable for bioinformaticians. To\nthis end, we have developed an open source pipeline optimization tool\n(TPOT-MDR) that uses genetic programming to automatically design machine\nlearning pipelines for bioinformatics studies. In TPOT-MDR, we implement\nMultifactor Dimensionality Reduction (MDR) as a feature construction method for\nmodeling higher-order feature interactions, and combine it with a new expert\nknowledge-guided feature selector for large biomedical data sets. We\ndemonstrate TPOT-MDR's capabilities using a combination of simulated and real\nworld data sets from human genetics and find that TPOT-MDR significantly\noutperforms modern machine learning methods such as logistic regression and\neXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline\ndiscovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's\nability to produce a high-accuracy solution that is also easily interpretable.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 20:10:10 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Sohn", "Andrew", ""], ["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1702.02092", "submitter": "Tom Anderson", "authors": "Tom A. F. Anderson, David M. W. Powers", "title": "Characterisation of speech diversity using self-organising maps", "comments": "16th Speech Science and Technology Conference (SST2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report investigations into speaker classification of larger quantities of\nunlabelled speech data using small sets of manually phonemically annotated\nspeech. The Kohonen speech typewriter is a semi-supervised method comprised of\nself-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a\n2D array of cells that learn vector representations of the data based on\nneighbourhoods. In this paper, we report a method to evaluate pronunciation\nusing multilevel SOMs with /hVd/ single syllable utterances for the study of\nvowels, for Australian pronunciation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 11:18:06 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Anderson", "Tom A. F.", ""], ["Powers", "David M. W.", ""]]}, {"id": "1702.02125", "submitter": "Eug\\'enio Rodrigues", "authors": "Eug\\'enio Rodrigues and Lu\\'isa Dias Pereira and Ad\\'elio Rodrigues\n  Gaspar and \\'Alvaro Gomes and Manuel Carlos Gameiro da Silva", "title": "Estimation of classrooms occupancy using a multi-layer perceptron", "comments": "6 pages, 2 figures, conference article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-layer perceptron model for the estimation of\nclassrooms number of occupants from sensed indoor environmental data-relative\nhumidity, air temperature, and carbon dioxide concentration. The modelling\ndatasets were collected from two classrooms in the Secondary School of Pombal,\nPortugal. The number of occupants and occupation periods were obtained from\nclass attendance reports. However, post-class occupancy was unknown and the\ndeveloped model is used to reconstruct the classrooms occupancy by filling the\nunreported periods. Different model structure and environment variables\ncombination were tested. The model with best accuracy had as input vector 10\nvariables of five averaged time intervals of relative humidity and carbon\ndioxide concentration. The model presented a mean square error of 1.99,\ncoefficient of determination of 0.96 with a significance of p-value < 0.001,\nand a mean absolute error of 1 occupant. These results show promising\nestimation capabilities in uncertain indoor environment conditions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 18:17:25 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Rodrigues", "Eug\u00e9nio", ""], ["Pereira", "Lu\u00edsa Dias", ""], ["Gaspar", "Ad\u00e9lio Rodrigues", ""], ["Gomes", "\u00c1lvaro", ""], ["da Silva", "Manuel Carlos Gameiro", ""]]}, {"id": "1702.02181", "submitter": "Moshe Looks", "authors": "Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, Peter Norvig", "title": "Deep Learning with Dynamic Computation Graphs", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference. They are also difficult to implement in popular\ndeep learning libraries, which are based on static data-flow graphs. We\nintroduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size. We further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:59:43 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 04:43:02 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Looks", "Moshe", ""], ["Herreshoff", "Marcello", ""], ["Hutchins", "DeLesley", ""], ["Norvig", "Peter", ""]]}, {"id": "1702.02217", "submitter": "Eric O. Scott", "authors": "Eric O. Scott, Kenneth A. De Jong", "title": "Multitask Evolution with Cartesian Genetic Programming", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a genetic programming method for solving multiple Boolean\ncircuit synthesis tasks simultaneously. This allows us to solve a set of\nelementary logic functions twice as easily as with a direct, single-task\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 22:20:43 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 16:33:34 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Scott", "Eric O.", ""], ["De Jong", "Kenneth A.", ""]]}, {"id": "1702.02277", "submitter": "Frank Z. Xing", "authors": "Frank Z. Xing", "title": "A Historical Review of Forty Years of Research on CMAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Cerebellar Model Articulation Controller (CMAC) is an influential\nbrain-inspired computing model in many relevant fields. Since its inception in\nthe 1970s, the model has been intensively studied and many variants of the\nprototype, such as Kernel-CMAC, Self-Organizing Map CMAC, and Linguistic CMAC,\nhave been proposed. This review article focus on how the CMAC model is\ngradually developed and refined to meet the demand of fast, adaptive, and\nrobust control. Two perspective, CMAC as a neural network and CMAC as a table\nlook-up technique are presented. Three aspects of the model: the architecture,\nlearning algorithms and applications are discussed. In the end, some potential\nfuture research directions on this model are suggested.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:27:11 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Xing", "Frank Z.", ""]]}, {"id": "1702.02526", "submitter": "Michael Kampffmeyer", "authors": "Michael Kampffmeyer, Sigurd L{\\o}kse, Filippo Maria Bianchi, Robert\n  Jenssen and Lorenzo Livi", "title": "Deep Kernelized Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the deep kernelized autoencoder, a neural network\nmodel that allows an explicit approximation of (i) the mapping from an input\nspace to an arbitrary, user-specified kernel space and (ii) the back-projection\nfrom such a kernel space to input space. The proposed method is based on\ntraditional autoencoders and is trained through a new unsupervised loss\nfunction. During training, we optimize both the reconstruction accuracy of\ninput samples and the alignment between a kernel matrix given as prior and the\ninner products of the hidden representations computed by the autoencoder.\nKernel alignment provides control over the hidden representation learned by the\nautoencoder. Experiments have been performed to evaluate both reconstruction\nand kernel alignment performance. Additionally, we applied our method to\nemulate kPCA on a denoising task obtaining promising results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:11:34 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Kampffmeyer", "Michael", ""], ["L\u00f8kse", "Sigurd", ""], ["Bianchi", "Filippo Maria", ""], ["Jenssen", "Robert", ""], ["Livi", "Lorenzo", ""]]}, {"id": "1702.02540", "submitter": "William Murdoch", "authors": "W. James Murdoch and Arthur Szlam", "title": "Automatic Rule Extraction from Long Short Term Memory Networks", "comments": "ICLR 2017 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:46:37 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 22:20:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Murdoch", "W. James", ""], ["Szlam", "Arthur", ""]]}, {"id": "1702.02604", "submitter": "Mohammad Taha Bahadori", "authors": "Mohammad Taha Bahadori, Krzysztof Chalupka, Edward Choi, Robert Chen,\n  Walter F. Stewart, Jimeng Sun", "title": "Causal Regularization", "comments": "Adding theoretical analysis, revising the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application domains such as healthcare, we want accurate predictive models\nthat are also causally interpretable. In pursuit of such models, we propose a\ncausal regularizer to steer predictive models towards causally-interpretable\nsolutions and theoretically study its properties. In a large-scale analysis of\nElectronic Health Records (EHR), our causally-regularized model outperforms its\nL1-regularized counterpart in causal accuracy and is competitive in predictive\nperformance. We perform non-linear causality analysis by causally regularizing\na special neural network architecture. We also show that the proposed causal\nregularizer can be used together with neural representation learning algorithms\nto yield up to 20% improvement over multilayer perceptron in detecting\nmultivariate causation, a situation common in healthcare, where many causal\nfactors should occur simultaneously to have an effect on the target variable.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 20:23:59 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 18:52:58 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Chalupka", "Krzysztof", ""], ["Choi", "Edward", ""], ["Chen", "Robert", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1702.02676", "submitter": "Arman Afrasiyabi", "authors": "Arman Afrasiyabi, Ozan Yildiz, Baris Nasir, Fatos T. Yarman Vural and\n  A. Enis Cetin", "title": "Energy Saving Additive Neural Network", "comments": "8 pages (double column), 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, machine learning techniques based on neural networks for\nmobile computing become increasingly popular. Classical multi-layer neural\nnetworks require matrix multiplications at each stage. Multiplication operation\nis not an energy efficient operation and consequently it drains the battery of\nthe mobile device. In this paper, we propose a new energy efficient neural\nnetwork with the universal approximation property over space of Lebesgue\nintegrable functions. This network, called, additive neural network, is very\nsuitable for mobile computing. The neural structure is based on a novel vector\nproduct definition, called ef-operator, that permits a multiplier-free\nimplementation. In ef-operation, the \"product\" of two real numbers is defined\nas the sum of their absolute values, with the sign determined by the sign of\nthe product of the numbers. This \"product\" is used to construct a vector\nproduct in $R^N$. The vector product induces the $l_1$ norm. The proposed\nadditive neural network successfully solves the XOR problem. The experiments on\nMNIST dataset show that the classification performances of the proposed\nadditive neural networks are very similar to the corresponding multi-layer\nperceptron and convolutional neural networks (LeNet).\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 02:02:27 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Afrasiyabi", "Arman", ""], ["Yildiz", "Ozan", ""], ["Nasir", "Baris", ""], ["Vural", "Fatos T. Yarman", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1702.03044", "submitter": "Anbang Yao", "authors": "Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen", "title": "Incremental Network Quantization: Towards Lossless CNNs with\n  Low-Precision Weights", "comments": "Published by ICLR 2017, and the code is available at\n  https://github.com/Zhouaojun/Incremental-Network-Quantization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents incremental network quantization (INQ), a novel method,\ntargeting to efficiently convert any pre-trained full-precision convolutional\nneural network (CNN) model into a low-precision version whose weights are\nconstrained to be either powers of two or zero. Unlike existing methods which\nare struggled in noticeable accuracy loss, our INQ has the potential to resolve\nthis issue, as benefiting from two innovations. On one hand, we introduce three\ninterdependent operations, namely weight partition, group-wise quantization and\nre-training. A well-proven measure is employed to divide the weights in each\nlayer of a pre-trained CNN model into two disjoint groups. The weights in the\nfirst group are responsible to form a low-precision base, thus they are\nquantized by a variable-length encoding method. The weights in the other group\nare responsible to compensate for the accuracy loss from the quantization, thus\nthey are the ones to be re-trained. On the other hand, these three operations\nare repeated on the latest re-trained group in an iterative manner until all\nthe weights are converted into low-precision ones, acting as an incremental\nnetwork quantization and accuracy enhancement procedure. Extensive experiments\non the ImageNet classification task using almost all known deep CNN\narchitectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the\nefficacy of the proposed method. Specifically, at 5-bit quantization, our\nmodels have improved accuracy than the 32-bit floating-point references. Taking\nResNet-18 as an example, we further show that our quantized models with 4-bit,\n3-bit and 2-bit ternary weights have improved or very similar accuracy against\nits 32-bit floating-point baseline. Besides, impressive results with the\ncombination of network pruning and INQ are also reported. The code is available\nat https://github.com/Zhouaojun/Incremental-Network-Quantization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 02:30:22 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 13:21:18 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Zhou", "Aojun", ""], ["Yao", "Anbang", ""], ["Guo", "Yiwen", ""], ["Xu", "Lin", ""], ["Chen", "Yurong", ""]]}, {"id": "1702.03180", "submitter": "Dianhui Wang", "authors": "Dianhui Wang, Ming Li", "title": "Stochastic Configuration Networks: Fundamentals and Algorithms", "comments": "Manuscript submitted to IEEE TCYB on Feb. 12, 2017; 18 pages, 4\n  figures and 28 references", "journal-ref": "IEEE Transactions on Cybernetics 47(10):3466-3479, 2017", "doi": "10.1109/TCYB.2017.2734043", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to a development of randomized methods for neural\nnetworks. The proposed learner model is generated incrementally by stochastic\nconfiguration (SC) algorithms, termed as Stochastic Configuration Networks\n(SCNs). In contrast to the existing randomised learning algorithms for single\nlayer feed-forward neural networks (SLFNNs), we randomly assign the input\nweights and biases of the hidden nodes in the light of a supervisory mechanism,\nand the output weights are analytically evaluated in either constructive or\nselective manner. As fundamentals of SCN-based data modelling techniques, we\nestablish some theoretical results on the universal approximation property.\nThree versions of SC algorithms are presented for regression problems\n(applicable for classification problems as well) in this work. Simulation\nresults concerning both function approximation and real world data regression\nindicate some remarkable merits of our proposed SCNs in terms of less human\nintervention on the network size setting, the scope adaptation of random\nparameters, fast learning and sound generalization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 14:24:16 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 13:24:59 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 12:09:19 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 14:49:35 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Wang", "Dianhui", ""], ["Li", "Ming", ""]]}, {"id": "1702.03197", "submitter": "Abdulaziz Alayba", "authors": "Abdulaziz M. Alayba, Vasile Palade, Matthew England and Rahat Iqbal", "title": "Arabic Language Sentiment Analysis on Health Services", "comments": "Authors accepted version of submission for ASAR 2017", "journal-ref": "Proc. 1st International Workshop on Arabic Script Analysis and\n  Recognition (ASAR '17), pp. 114-118. IEEE, 2017", "doi": "10.1109/ASAR.2017.8067771", "report-no": null, "categories": "cs.CL cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media network phenomenon leads to a massive amount of valuable\ndata that is available online and easy to access. Many users share images,\nvideos, comments, reviews, news and opinions on different social networks\nsites, with Twitter being one of the most popular ones. Data collected from\nTwitter is highly unstructured, and extracting useful information from tweets\nis a challenging task. Twitter has a huge number of Arabic users who mostly\npost and write their tweets using the Arabic language. While there has been a\nlot of research on sentiment analysis in English, the amount of researches and\ndatasets in Arabic language is limited. This paper introduces an Arabic\nlanguage dataset which is about opinions on health services and has been\ncollected from Twitter. The paper will first detail the process of collecting\nthe data from Twitter and also the process of filtering, pre-processing and\nannotating the Arabic text in order to build a big sentiment analysis dataset\nin Arabic. Several Machine Learning algorithms (Naive Bayes, Support Vector\nMachine and Logistic Regression) alongside Deep and Convolutional Neural\nNetworks were utilized in our experiments of sentiment analysis on our health\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 14:59:14 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Alayba", "Abdulaziz M.", ""], ["Palade", "Vasile", ""], ["England", "Matthew", ""], ["Iqbal", "Rahat", ""]]}, {"id": "1702.03260", "submitter": "Eric Tramel", "authors": "Eric W. Tramel and Marylou Gabri\\'e and Andre Manoel and Francesco\n  Caltagirone and Florent Krzakala", "title": "A Deterministic and Generalized Framework for Unsupervised Learning with\n  Restricted Boltzmann Machines", "comments": null, "journal-ref": "Phys. Rev. X 8, 041006 (2018)", "doi": "10.1103/PhysRevX.8.041006", "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) are energy-based neural-networks which\nare commonly used as the building blocks for deep architectures neural\narchitectures. In this work, we derive a deterministic framework for the\ntraining, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer\n(TAP) mean-field approximation of widely-connected systems with weak\ninteractions coming from spin-glass theory. While the TAP approach has been\nextensively studied for fully-visible binary spin systems, our construction is\ngeneralized to latent-variable models, as well as to arbitrarily distributed\nreal-valued spin systems with bounded support. In our numerical experiments, we\ndemonstrate the effective deterministic training of our proposed models and are\nable to show interesting features of unsupervised learning which could not be\ndirectly observed with sampling. Additionally, we demonstrate how to utilize\nour TAP-based framework for leveraging trained RBMs as joint priors in\ndenoising problems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 17:29:51 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 13:55:46 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 09:07:13 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Tramel", "Eric W.", ""], ["Gabri\u00e9", "Marylou", ""], ["Manoel", "Andre", ""], ["Caltagirone", "Francesco", ""], ["Krzakala", "Florent", ""]]}, {"id": "1702.03389", "submitter": "Bing Zeng", "authors": "Bing Zeng, Liang Gao, Xinyu Li", "title": "Whale swarm algorithm for function optimization", "comments": "8 pages, 5 figures", "journal-ref": "LNCS. volume 10361. ICIC 2017: pp 624-639", "doi": "10.1007/978-3-319-63309-1_55", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing nature-inspired metaheuristic algorithms are applied to solving\nthe real-world optimization problems, as they have some advantages over the\nclassical methods of numerical optimization. This paper has proposed a new\nnature-inspired metaheuristic called Whale Swarm Algorithm for function\noptimization, which is inspired by the whales behavior of communicating with\neach other via ultrasound for hunting. The proposed Whale Swarm Algorithm has\nbeen compared with several popular metaheuristic algorithms on comprehensive\nperformance metrics. According to the experimental results, Whale Swarm\nAlgorithm has a quite competitive performance when compared with other\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 06:39:38 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 12:53:54 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Zeng", "Bing", ""], ["Gao", "Liang", ""], ["Li", "Xinyu", ""]]}, {"id": "1702.03443", "submitter": "Wei Wen", "authors": "Yandan Wang, Wei Wen, Beiye Liu, Donald Chiarulli, Hai Li", "title": "Group Scissor: Scaling Neuromorphic Computing Design to Large Neural\n  Networks", "comments": "Accepted in DAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synapse crossbar is an elementary structure in Neuromorphic Computing Systems\n(NCS). However, the limited size of crossbars and heavy routing congestion\nimpedes the NCS implementations of big neural networks. In this paper, we\npropose a two-step framework (namely, group scissor) to scale NCS designs to\nbig neural networks. The first step is rank clipping, which integrates low-rank\napproximation into the training to reduce total crossbar area. The second step\nis group connection deletion, which structurally prunes connections to reduce\nrouting congestion between crossbars. Tested on convolutional neural networks\nof LeNet on MNIST database and ConvNet on CIFAR-10 database, our experiments\nshow significant reduction of crossbar area and routing area in NCS designs.\nWithout accuracy loss, rank clipping reduces total crossbar area to 13.62\\% and\n51.81\\% in the NCS designs of LeNet and ConvNet, respectively. Following rank\nclipping, group connection deletion further reduces the routing area of LeNet\nand ConvNet to 8.1\\% and 52.06\\%, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 17:34:34 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 18:29:37 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wang", "Yandan", ""], ["Wen", "Wei", ""], ["Liu", "Beiye", ""], ["Chiarulli", "Donald", ""], ["Li", "Hai", ""]]}, {"id": "1702.03713", "submitter": "Adam Gaier", "authors": "Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret", "title": "Data-Efficient Exploration, Optimization, and Modeling of Diverse\n  Designs through Surrogate-Assisted Illumination", "comments": "Genetic and Evolutionary Computation Conference 2017", "journal-ref": null, "doi": "10.1145/3071178.3071282", "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MAP-Elites algorithm produces a set of high-performing solutions that\nvary according to features defined by the user. This technique has the\npotential to be a powerful tool for design space exploration, but is limited by\nthe need for numerous evaluations. The Surrogate-Assisted Illumination\nalgorithm (SAIL), introduced here, integrates approximative models and\nintelligent sampling of the objective function to minimize the number of\nevaluations required by MAP-Elites.\n  The ability of SAIL to efficiently produce both accurate models and diverse\nhigh performing solutions is illustrated on a 2D airfoil design problem. The\nsearch space is divided into bins, each holding a design with a different\ncombination of features. In each bin SAIL produces a better performing solution\nthan MAP-Elites, and requires several orders of magnitude fewer evaluations.\nThe CMA-ES algorithm was used to produce an optimal design in each bin: with\nthe same number of evaluations required by CMA-ES to find a near-optimal\nsolution in a single bin, SAIL finds solutions of similar quality in every bin.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 10:48:56 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 09:44:28 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Gaier", "Adam", ""], ["Asteroth", "Alexander", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1702.03946", "submitter": "Daoyi Dong", "authors": "Daoyi Dong, Xi Xing, Hailan Ma, Chunlin Chen, Zhixin Liu, Herschel\n  Rabitz", "title": "Learning-based Quantum Robust Control: Algorithm, Applications and\n  Experiments", "comments": "13 pages, 10 figures and 1 table", "journal-ref": "IEEE Transactions on Cybernetics, VOL. 50, NO. 8, pp. 3581-3593,\n  AUGUST 2020", "doi": "10.1109/TCYB.2019.2921424", "report-no": null, "categories": "quant-ph cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust control design for quantum systems has been recognized as a key task\nin quantum information technology, molecular chemistry and atomic physics. In\nthis paper, an improved differential evolution algorithm, referred to as\n\\emph{msMS}\\_DE, is proposed to search robust fields for various quantum\ncontrol problems. In \\emph{msMS}\\_DE, multiple samples are used for fitness\nevaluation and a mixed strategy is employed for the mutation operation. In\nparticular, the \\emph{msMS}\\_DE algorithm is applied to the control problems of\n(i) open inhomogeneous quantum ensembles and (ii) the consensus goal of a\nquantum network with uncertainties. Numerical results are presented to\ndemonstrate the excellent performance of the improved machine learning\nalgorithm for these two classes of quantum robust control problems.\nFurthermore, \\emph{msMS}\\_DE is experimentally implemented on femtosecond laser\ncontrol applications to optimize two-photon absorption and control\nfragmentation of the molecule $\\text{CH}_2\\text{BrI}$. Experimental results\ndemonstrate excellent performance of \\emph{msMS}\\_DE in searching for effective\nfemtosecond laser pulses for various tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 19:02:30 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 09:03:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dong", "Daoyi", ""], ["Xing", "Xi", ""], ["Ma", "Hailan", ""], ["Chen", "Chunlin", ""], ["Liu", "Zhixin", ""], ["Rabitz", "Herschel", ""]]}, {"id": "1702.03993", "submitter": "Konstantinos Michmizos", "authors": "Leo Kozachkov and Konstantinos P. Michmizos", "title": "The Causal Role of Astrocytes in Slow-Wave Rhythmogenesis: A\n  Computational Modelling Study", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the origin of slow and infra-slow oscillations could reveal or\nexplain brain mechanisms in health and disease. Here, we present a\nbiophysically constrained computational model of a neural network where the\ninclusion of astrocytes introduced slow and infra-slow-oscillations, through\ntwo distinct mechanisms. Specifically, we show how astrocytes can modulate the\nfast network activity through their slow inter-cellular calcium wave speed and\namplitude and possibly cause the oscillatory imbalances observed in diseases\ncommonly known for such abnormalities, namely Alzheimer's disease, Parkinson's\ndisease, epilepsy, depression and ischemic stroke. This work aims to increase\nour knowledge on how astrocytes and neurons synergize to affect brain function\nand dysfunction.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 21:33:06 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Kozachkov", "Leo", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "1702.04283", "submitter": "Leslie Smith", "authors": "Leslie N. Smith and Nicholay Topin", "title": "Exploring loss function topology with cyclical learning rates", "comments": "Submitted as an ICLR 2017 Workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present observations and discussion of previously unreported phenomena\ndiscovered while training residual networks. The goal of this work is to better\nunderstand the nature of neural networks through the examination of these new\nempirical results. These behaviors were identified through the application of\nCyclical Learning Rates (CLR) and linear network interpolation. Among these\nbehaviors are counterintuitive increases and decreases in training loss and\ninstances of rapid training. For example, we demonstrate how CLR can produce\ngreater testing accuracy than traditional training despite using large learning\nrates. Files to replicate these results are available at\nhttps://github.com/lnsmith54/exploring-loss\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:46:19 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Smith", "Leslie N.", ""], ["Topin", "Nicholay", ""]]}, {"id": "1702.04459", "submitter": "Dianhui Wang", "authors": "Dianhui Wang, Ming Li", "title": "Robust Stochastic Configuration Networks with Kernel Density Estimation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been widely used as predictive models to fit data\ndistribution, and they could be implemented through learning a collection of\nsamples. In many applications, however, the given dataset may contain noisy\nsamples or outliers which may result in a poor learner model in terms of\ngeneralization. This paper contributes to a development of robust stochastic\nconfiguration networks (RSCNs) for resolving uncertain data regression\nproblems. RSCNs are built on original stochastic configuration networks with\nweighted least squares method for evaluating the output weights, and the input\nweights and biases are incrementally and randomly generated by satisfying with\na set of inequality constrains. The kernel density estimation (KDE) method is\nemployed to set the penalty weights for each training samples, so that some\nnegative impacts, caused by noisy data or outliers, on the resulting learner\nmodel can be reduced. The alternating optimization technique is applied for\nupdating a RSCN model with improved penalty weights computed from the kernel\ndensity estimation function. Performance evaluation is carried out by a\nfunction approximation, four benchmark datasets and a case study on engineering\napplication. Comparisons to other robust randomised neural modelling\ntechniques, including the probabilistic robust learning algorithm for neural\nnetworks with random weights and improved RVFL networks, indicate that the\nproposed RSCNs with KDE perform favourably and demonstrate good potential for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 03:54:29 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 15:29:47 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wang", "Dianhui", ""], ["Li", "Ming", ""]]}, {"id": "1702.04521", "submitter": "Tim Rockt\\\"aschel", "authors": "Micha{\\l} Daniluk, Tim Rockt\\\"aschel, Johannes Welbl, Sebastian Riedel", "title": "Frustratingly Short Attention Spans in Neural Language Modeling", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 09:45:23 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Daniluk", "Micha\u0142", ""], ["Rockt\u00e4schel", "Tim", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1702.04649", "submitter": "Greg Wayne", "authors": "Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir\n  Mohamed, Danilo J. Rezende, David Amos, Timothy Lillicrap", "title": "Generative Temporal Models with Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of modeling temporal data with long-range\ndependencies, wherein new observations are fully or partially predictable based\non temporally-distant, past observations. A sufficiently powerful temporal\nmodel should separate predictable elements of the sequence from unpredictable\nelements, express uncertainty about those unpredictable elements, and rapidly\nidentify novel elements that may help to predict the future. To create such\nmodels, we introduce Generative Temporal Models augmented with external memory\nsystems. They are developed within the variational inference framework, which\nprovides both a practical training methodology and methods to gain insight into\nthe models' operation. We show, on a range of problems with sparse, long-term\ntemporal dependencies, that these models store information from early in a\nsequence, and reuse this stored information efficiently. This allows them to\nperform substantially better than existing models based on well-known recurrent\nneural networks, like LSTMs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 15:19:02 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 10:14:52 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Gemici", "Mevlana", ""], ["Hung", "Chia-Chun", ""], ["Santoro", "Adam", ""], ["Wayne", "Greg", ""], ["Mohamed", "Shakir", ""], ["Rezende", "Danilo J.", ""], ["Amos", "David", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1702.04770", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Sumit Chopra, Marc'Aurelio Ranzato, Arthur Szlam, Ruoyu\n  Sun, Soumith Chintala, Nicolas Vasilache", "title": "Training Language Models Using Target-Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Truncated Back-Propagation through Time (BPTT) is the most popular\napproach to training Recurrent Neural Networks (RNNs), it suffers from being\ninherently sequential (making parallelization difficult) and from truncating\ngradient flow between distant time-steps. We investigate whether Target\nPropagation (TPROP) style approaches can address these shortcomings.\nUnfortunately, extensive experiments suggest that TPROP generally underperforms\nBPTT, and we end with an analysis of this phenomenon, and suggestions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 20:56:30 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Wiseman", "Sam", ""], ["Chopra", "Sumit", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""], ["Sun", "Ruoyu", ""], ["Chintala", "Soumith", ""], ["Vasilache", "Nicolas", ""]]}, {"id": "1702.04782", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Subarna Tripathi", "title": "Precise Recovery of Latent Vectors from Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) transform latent vectors into visually\nplausible images. It is generally thought that the original GAN formulation\ngives no out-of-the-box method to reverse the mapping, projecting images back\ninto latent space. We introduce a simple, gradient-based technique called\nstochastic clipping. In experiments, for images generated by the GAN, we\nprecisely recover their latent vector pre-images 100% of the time. Additional\nexperiments demonstrate that this method is robust to noise. Finally, we show\nthat even for unseen images, our method appears to recover unique encodings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 21:26:21 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 01:56:36 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Tripathi", "Subarna", ""]]}, {"id": "1702.05043", "submitter": "Corentin Tallec", "authors": "Corentin Tallec and Yann Ollivier", "title": "Unbiased Online Recurrent Optimization", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for\nonline learning of general recurrent computational graphs such as recurrent\nnetwork models. It works in a streaming fashion and avoids backtracking through\npast activations and inputs. UORO is computationally as costly as Truncated\nBackpropagation Through Time (truncated BPTT), a widespread algorithm for\nonline learning of recurrent networks. UORO is a modification of NoBackTrack\nthat bypasses the need for model sparsity and makes implementation easy in\ncurrent deep learning frameworks, even for complex models.\n  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is\nthe core hypothesis in stochastic gradient descent theory, without which\nconvergence to a local optimum is not guaranteed. On the contrary, truncated\nBPTT does not provide this property, leading to possible divergence.\n  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges.\nFor instance, when a parameter has a positive short-term but negative long-term\ninfluence, truncated BPTT diverges unless the truncation span is very\nsignificantly longer than the intrinsic temporal range of the interactions,\nwhile UORO performs well thanks to the unbiasedness of its gradients.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 16:38:08 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 15:29:33 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 11:42:03 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tallec", "Corentin", ""], ["Ollivier", "Yann", ""]]}, {"id": "1702.05308", "submitter": "Shubham Dokania", "authors": "Shubham Dokania, Ayush Chopra, Feroz Ahmad, Anil Singh Parihar", "title": "Hierarchy Influenced Differential Evolution: A Motor Operation Inspired\n  Approach", "comments": "8 pages, accepted at IJCCI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operational maturity of biological control systems have fuelled the\ninspiration for a large number of mathematical and logical models for control,\nautomation and optimisation. The human brain represents the most sophisticated\ncontrol architecture known to us and is a central motivation for several\nresearch attempts across various domains. In the present work, we introduce an\nalgorithm for mathematical optimisation that derives its intuition from the\nhierarchical and distributed operations of the human motor system. The system\ncomprises global leaders, local leaders and an effector population that adapt\ndynamically to attain global optimisation via a feedback mechanism coupled with\nthe structural hierarchy. The hierarchical system operation is distributed into\nlocal control for movement and global controllers that facilitate gross motion\nand decision making. We present our algorithm as a variant of the classical\nDifferential Evolution algorithm, introducing a hierarchical crossover\noperation. The discussed approach is tested exhaustively on standard test\nfunctions as well as the CEC 2017 benchmark. Our algorithm significantly\noutperforms various standard algorithms as well as their popular variants as\ndiscussed in the results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 11:47:50 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 11:46:59 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Dokania", "Shubham", ""], ["Chopra", "Ayush", ""], ["Ahmad", "Feroz", ""], ["Parihar", "Anil Singh", ""]]}, {"id": "1702.05386", "submitter": "Zachary Lipton", "authors": "Nathan Ng, Rodney A Gabriel, Julian McAuley, Charles Elkan, Zachary C\n  Lipton", "title": "Predicting Surgery Duration with Neural Heteroscedastic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling surgeries is a challenging task due to the fundamental uncertainty\nof the clinical environment, as well as the risks and costs associated with\nunder- and over-booking. We investigate neural regression algorithms to\nestimate the parameters of surgery case durations, focusing on the issue of\nheteroscedasticity. We seek to simultaneously estimate the duration of each\nsurgery, as well as a surgery-specific notion of our uncertainty about its\nduration. Estimating this uncertainty can lead to more nuanced and effective\nscheduling strategies, as we are able to schedule surgeries more efficiently\nwhile allowing an informed and case-specific margin of error. Using surgery\nrecords %from the UC San Diego Health System, from a large United States health\nsystem we demonstrate potential improvements on the order of 20% (in terms of\nminutes overbooked) compared to current scheduling techniques. Moreover, we\ndemonstrate that surgery durations are indeed heteroscedastic. We show that\nmodels that estimate case-specific uncertainty better fit the data (log\nlikelihood). Additionally, we show that the heteroscedastic predictions can\nmore optimally trade off between over and under-booking minutes, especially\nwhen idle minutes and scheduling collisions confer disparate costs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:28:28 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 22:26:47 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 03:48:14 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ng", "Nathan", ""], ["Gabriel", "Rodney A", ""], ["McAuley", "Julian", ""], ["Elkan", "Charles", ""], ["Lipton", "Zachary C", ""]]}, {"id": "1702.05441", "submitter": "Junpei Zhong", "authors": "Junpei Zhong and Angelo Cangelosi and Tetsuya Ogata", "title": "Toward Abstraction from Multi-modal Data: Empirical Studies on Multiple\n  Time-scale Recurrent Models", "comments": "Accepted by IJCNN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abstraction tasks are challenging for multi- modal sequences as they\nrequire a deeper semantic understanding and a novel text generation for the\ndata. Although the recurrent neural networks (RNN) can be used to model the\ncontext of the time-sequences, in most cases the long-term dependencies of\nmulti-modal data make the back-propagation through time training of RNN tend to\nvanish in the time domain. Recently, inspired from Multiple Time-scale\nRecurrent Neural Network (MTRNN), an extension of Gated Recurrent Unit (GRU),\ncalled Multiple Time-scale Gated Recurrent Unit (MTGRU), has been proposed to\nlearn the long-term dependencies in natural language processing. Particularly\nit is also able to accomplish the abstraction task for paragraphs given that\nthe time constants are well defined. In this paper, we compare the MTRNN and\nMTGRU in terms of its learning performances as well as their abstraction\nrepresentation on higher level (with a slower neural activation). This was done\nby conducting two studies based on a smaller data- set (two-dimension time\nsequences from non-linear functions) and a relatively large data-set\n(43-dimension time sequences from iCub manipulation tasks with multi-modal\ndata). We conclude that gated recurrent mechanisms may be necessary for\nlearning long-term dependencies in large dimension multi-modal data-sets (e.g.\nlearning of robot manipulation), even when natural language commands was not\ninvolved. But for smaller learning tasks with simple time-sequences, generic\nversion of recurrent models, such as MTRNN, were sufficient to accomplish the\nabstraction task.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 09:31:41 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Zhong", "Junpei", ""], ["Cangelosi", "Angelo", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "1702.05552", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan and Clinton Fookes", "title": "Soft + Hardwired Attention: An LSTM Framework for Human Trajectory\n  Prediction and Abnormal Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans we possess an intuitive ability for navigation which we master\nthrough years of practice; however existing approaches to model this trait for\ndiverse tasks including monitoring pedestrian flow and detecting abnormal\nevents have been limited by using a variety of hand-crafted features. Recent\nresearch in the area of deep-learning has demonstrated the power of learning\nfeatures directly from the data; and related research in recurrent neural\nnetworks has shown exemplary results in sequence-to-sequence problems such as\nneural machine translation and neural image caption generation. Motivated by\nthese approaches, we propose a novel method to predict the future motion of a\npedestrian given a short history of their, and their neighbours, past\nbehaviour. The novelty of the proposed method is the combined attention model\nwhich utilises both \"soft attention\" as well as \"hard-wired\" attention in order\nto map the trajectory information from the local neighbourhood to the future\npositions of the pedestrian of interest. We illustrate how a simple\napproximation of attention weights (i.e hard-wired) can be merged together with\nsoft attention weights in order to make our model applicable for challenging\nreal world scenarios with hundreds of neighbours. The navigational capability\nof the proposed method is tested on two challenging publicly available\nsurveillance databases where our model outperforms the current-state-of-the-art\nmethods. Additionally, we illustrate how the proposed architecture can be\ndirectly applied for the task of abnormal event detection without handcrafting\nthe features.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 01:08:18 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1702.05639", "submitter": "Dianhui Wang", "authors": "Dianhui Wang, Ming Li", "title": "Deep Stochastic Configuration Networks with Universal Approximation\n  Property", "comments": "To appear in the Proceedings of IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a randomized approach for incrementally building deep\nneural networks, where a supervisory mechanism is proposed to constrain the\nrandom assignment of the weights and biases, and all the hidden layers have\ndirect links to the output layer. A fundamental result on the universal\napproximation property is established for such a class of randomized leaner\nmodels, namely deep stochastic configuration networks (DeepSCNs). A learning\nalgorithm is presented to implement DeepSCNs with either specific architecture\nor self-organization. The read-out weights attached with all direct links from\neach hidden layer to the output layer are evaluated by the least squares\nmethod. Given a set of training examples, DeepSCNs can speedily produce a\nlearning representation, that is, a collection of random basis functions with\nthe cascaded inputs together with the read-out weights. An empirical study on a\nfunction approximation is carried out to demonstrate some properties of the\nproposed deep learner model.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 18:18:32 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 02:38:56 GMT"}, {"version": "v3", "created": "Sat, 10 Mar 2018 10:42:47 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 08:08:57 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Wang", "Dianhui", ""], ["Li", "Ming", ""]]}, {"id": "1702.05939", "submitter": "Andr\\'e Gr\\\"uning", "authors": "Joseph Chrol-Cannon and Yaochu Jin and Andr\\'e Gr\\\"uning", "title": "An Efficient Method for online Detection of Polychronous Patterns in\n  Spiking Neural Network", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.neucom.2017.06.025", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polychronous neural groups are effective structures for the recognition of\nprecise spike-timing patterns but the detection method is an inefficient\nmulti-stage brute force process that works off-line on pre-recorded simulation\ndata. This work presents a new model of polychronous patterns that can capture\nprecise sequences of spikes directly in the neural simulation. In this scheme,\neach neuron is assigned a randomized code that is used to tag the post-synaptic\nneurons whenever a spike is transmitted. This creates a polychronous code that\npreserves the order of pre-synaptic activity and can be registered in a hash\ntable when the post-synaptic neuron spikes. A polychronous code is a\nsub-component of a polychronous group that will occur, along with others, when\nthe group is active. We demonstrate the representational and pattern\nrecognition ability of polychronous codes on a direction selective visual task\ninvolving moving bars that is typical of a computation performed by simple\ncells in the cortex. The computational efficiency of the proposed algorithm far\nexceeds existing polychronous group detection methods and is well suited for\nonline detection.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 12:02:50 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Chrol-Cannon", "Joseph", ""], ["Jin", "Yaochu", ""], ["Gr\u00fcning", "Andr\u00e9", ""]]}, {"id": "1702.06053", "submitter": "Sahil Sharma", "authors": "Sahil Sharma, Ashutosh Jha, Parikshit Hegde, Balaraman Ravindran", "title": "Learning to Multi-Task by Active Sampling", "comments": "11 pages + 30 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the long-standing challenges in Artificial Intelligence for learning\ngoal-directed behavior is to build a single agent which can solve multiple\ntasks. Recent progress in multi-task learning for goal-directed sequential\nproblems has been in the form of distillation based learning wherein a student\nnetwork learns from multiple task-specific expert networks by mimicking the\ntask-specific policies of the expert networks. While such approaches offer a\npromising solution to the multi-task learning problem, they require supervision\nfrom large expert networks which require extensive data and computation time\nfor training. In this work, we propose an efficient multi-task learning\nframework which solves multiple goal-directed tasks in an on-line setup without\nthe need for expert supervision. Our work uses active learning principles to\nachieve multi-task learning by sampling the harder tasks more than the easier\nones. We propose three distinct models under our active sampling framework. An\nadaptive method with extremely competitive multi-tasking performance. A\nUCB-based meta-learner which casts the problem of picking the next task to\ntrain on as a multi-armed bandit problem. A meta-learning method that casts the\nnext-task picking problem as a full Reinforcement Learning problem and uses\nactor critic methods for optimizing the multi-tasking performance directly. We\ndemonstrate results in the Atari 2600 domain on seven multi-tasking instances:\nthree 6-task instances, one 8-task instance, two 12-task instances and one\n21-task instance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 16:31:56 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 04:38:43 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 08:49:45 GMT"}, {"version": "v4", "created": "Sun, 21 May 2017 12:47:34 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Sharma", "Sahil", ""], ["Jha", "Ashutosh", ""], ["Hegde", "Parikshit", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1702.06054", "submitter": "Aravind Srinivas", "authors": "Sahil Sharma, Aravind Srinivas, Balaraman Ravindran", "title": "Learning to Repeat: Fine Grained Action Repetition for Deep\n  Reinforcement Learning", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning algorithms can learn complex behavioral patterns for\nsequential decision making tasks wherein an agent interacts with an environment\nand acquires feedback in the form of rewards sampled from it. Traditionally,\nsuch algorithms make decisions, i.e., select actions to execute, at every\nsingle time step of the agent-environment interactions. In this paper, we\npropose a novel framework, Fine Grained Action Repetition (FiGAR), which\nenables the agent to decide the action as well as the time scale of repeating\nit. FiGAR can be used for improving any Deep Reinforcement Learning algorithm\nwhich maintains an explicit policy estimate by enabling temporal abstractions\nin the action space. We empirically demonstrate the efficacy of our framework\nby showing performance improvements on top of three policy search algorithms in\ndifferent domains: Asynchronous Advantage Actor Critic in the Atari 2600\ndomain, Trust Region Policy Optimization in Mujoco domain and Deep\nDeterministic Policy Gradients in the TORCS car racing domain.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 16:32:07 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:22:25 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Sharma", "Sahil", ""], ["Srinivas", "Aravind", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1702.06064", "submitter": "Aayush Ankit", "authors": "Aayush Ankit, Abhronil Sengupta, Priyadarshini Panda, Kaushik Roy", "title": "RESPARC: A Reconfigurable and Energy-Efficient Architecture with\n  Memristive Crossbars for Deep Spiking Neural Networks", "comments": "6 pages, 14 figures, Accepted in Design Automation Conference, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing using post-CMOS technologies is gaining immense\npopularity due to its promising abilities to address the memory and power\nbottlenecks in von-Neumann computing systems. In this paper, we propose RESPARC\n- a reconfigurable and energy efficient architecture built-on Memristive\nCrossbar Arrays (MCA) for deep Spiking Neural Networks (SNNs). Prior works were\nprimarily focused on device and circuit implementations of SNNs on crossbars.\nRESPARC advances this by proposing a complete system for SNN acceleration and\nits subsequent analysis. RESPARC utilizes the energy-efficiency of MCAs for\ninner-product computation and realizes a hierarchical reconfigurable design to\nincorporate the data-flow patterns in an SNN in a scalable fashion. We evaluate\nthe proposed architecture on different SNNs ranging in complexity from 2k-230k\nneurons and 1.2M-5.5M synapses. Simulation results on these networks show that\ncompared to the baseline digital CMOS architecture, RESPARC achieves 500X (15X)\nefficiency in energy benefits at 300X (60X) higher throughput for multi-layer\nperceptrons (deep convolutional networks). Furthermore, RESPARC is a\ntechnology-aware architecture that maps a given SNN topology to the most\noptimized MCA size for the given crossbar technology.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 17:04:08 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Ankit", "Aayush", ""], ["Sengupta", "Abhronil", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1702.06186", "submitter": "Amit Sahu", "authors": "Amit Sahu", "title": "Survey of reasoning using Neural networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reason and inference require process as well as memory skills by humans.\nNeural networks are able to process tasks like image recognition (better than\nhumans) but in memory aspects are still limited (by attention mechanism, size).\nRecurrent Neural Network (RNN) and it's modified version LSTM are able to solve\nsmall memory contexts, but as context becomes larger than a threshold, it is\ndifficult to use them. The Solution is to use large external memory. Still, it\nposes many challenges like, how to train neural networks for discrete memory\nrepresentation, how to describe long term dependencies in sequential data etc.\nMost prominent neural architectures for such tasks are Memory networks:\ninference components combined with long term memory and Neural Turing Machines:\nneural networks using external memory resources. Also, additional techniques\nlike attention mechanism, end to end gradient descent on discrete memory\nrepresentation are needed to support these solutions. Preliminary results of\nabove neural architectures on simple algorithms (sorting, copying) and Question\nAnswering (based on story, dialogs) application are comparable with the state\nof the art. In this paper, I explain these architectures (in general), the\nadditional techniques used and the results of their application.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 17:24:04 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:36:24 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Sahu", "Amit", ""]]}, {"id": "1702.06456", "submitter": "Yanis Bahroun", "authors": "Yanis Bahroun, Andrea Soltoggio", "title": "Online Representation Learning with Single and Multi-layer Hebbian\n  Networks for Image Classification", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning permits the development of algorithms that are able to\nadapt to a variety of different data sets using the same underlying rules\nthanks to the autonomous discovery of discriminating features during training.\nRecently, a new class of Hebbian-like and local unsupervised learning rules for\nneural networks have been developed that minimise a similarity matching\ncost-function. These have been shown to perform sparse representation learning.\nThis study tests the effectiveness of one such learning rule for learning\nfeatures from images. The rule implemented is derived from a nonnegative\nclassical multidimensional scaling cost-function, and is applied to both single\nand multi-layer architectures. The features learned by the algorithm are then\nused as input to an SVM to test their effectiveness in classification on the\nestablished CIFAR-10 image dataset. The algorithm performs well in comparison\nto other unsupervised learning algorithms and multi-layer networks, thus\nsuggesting its validity in the design of a new class of compact, online\nlearning networks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:01:28 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 12:51:08 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 12:03:51 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Bahroun", "Yanis", ""], ["Soltoggio", "Andrea", ""]]}, {"id": "1702.06463", "submitter": "Aditya Gilra", "authors": "Aditya Gilra, Wulfram Gerstner", "title": "Predicting non-linear dynamics by stable local learning in a recurrent\n  spiking neural network", "comments": null, "journal-ref": "eLife 2017;6:e28295", "doi": "10.7554/eLife.28295", "report-no": null, "categories": "q-bio.NC cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brains need to predict how the body reacts to motor commands. It is an open\nquestion how networks of spiking neurons can learn to reproduce the non-linear\nbody dynamics caused by motor commands, using local, online and stable learning\nrules. Here, we present a supervised learning scheme for the feedforward and\nrecurrent connections in a network of heterogeneous spiking neurons. The error\nin the output is fed back through fixed random connections with a negative\ngain, causing the network to follow the desired dynamics, while an online and\nlocal rule changes the weights. The rule for Feedback-based Online Local\nLearning Of Weights (FOLLOW) is local in the sense that weight changes depend\non the presynaptic activity and the error signal projected onto the\npostsynaptic neuron. We provide examples of learning linear, non-linear and\nchaotic dynamics, as well as the dynamics of a two-link arm. Using the Lyapunov\nmethod, and under reasonable assumptions and approximations, we show that\nFOLLOW learning is stable uniformly, with the error going to zero\nasymptotically.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:15:34 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 17:58:00 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Gilra", "Aditya", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1702.06700", "submitter": "Yuetan Lin", "authors": "Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang", "title": "Task-driven Visual Saliency and Attention-based Visual Question\n  Answering", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) has witnessed great progress since May, 2015\nas a classic problem unifying visual and textual data into a system. Many\nenlightening VQA works explore deep into the image and question encodings and\nfusing methods, of which attention is the most effective and infusive\nmechanism. Current attention based methods focus on adequate fusion of visual\nand textual features, but lack the attention to where people focus to ask\nquestions about the image. Traditional attention based methods attach a single\nvalue to the feature at each spatial location, which losses many useful\ninformation. To remedy these problems, we propose a general method to perform\nsaliency-like pre-selection on overlapped region features by the interrelation\nof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication\nbased attention method to capture more competent correlation information\nbetween visual and textual features. We conduct experiments on the large-scale\nCOCO-VQA dataset and analyze the effectiveness of our model demonstrated by\nstrong empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 08:19:38 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lin", "Yuetan", ""], ["Pang", "Zhangyang", ""], ["Wang", "Donghui", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1702.06856", "submitter": "Christian Gagn\\'e", "authors": "Mahdieh Abbasi and Christian Gagn\\'e", "title": "Robustness to Adversarial Examples through an Ensemble of Specialists", "comments": "Submitted to ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proposing to use an ensemble of diverse specialists, where speciality\nis defined according to the confusion matrix. Indeed, we observed that for\nadversarial instances originating from a given class, labeling tend to be done\ninto a small subset of (incorrect) classes. Therefore, we argue that an\nensemble of specialists should be better able to identify and reject fooling\ninstances, with a high entropy (i.e., disagreement) over the decisions in the\npresence of adversaries. Experimental results obtained confirm that\ninterpretation, opening a way to make the system more robust to adversarial\nexamples through a rejection mechanism, rather than trying to classify them\nproperly at any cost.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 15:37:50 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 05:14:14 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 04:10:18 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Abbasi", "Mahdieh", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1702.07097", "submitter": "Hongyin Luo", "authors": "Hongyin Luo, Jie Fu, James Glass", "title": "Adaptive Bidirectional Backpropagation: Towards Biologically Plausible\n  Error Signal Transmission in Neural Networks", "comments": "[v2]Extended the paper to the length of a long paper; added\n  references in introduction; corrected the experiments of BFA. [v3] Added link\n  to source code [v4] Added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The back-propagation (BP) algorithm has been considered the de-facto method\nfor training deep neural networks. It back-propagates errors from the output\nlayer to the hidden layers in an exact manner using the transpose of the\nfeedforward weights. However, it has been argued that this is not biologically\nplausible because back-propagating error signals with the exact incoming\nweights are not considered possible in biological neural systems. In this work,\nwe propose a biologically plausible paradigm of neural architecture based on\nrelated literature in neuroscience and asymmetric BP-like methods.\nSpecifically, we propose two bidirectional learning algorithms with trainable\nfeedforward and feedback weights. The feedforward weights are used to relay\nactivations from the inputs to target outputs. The feedback weights pass the\nerror signals from the output layer to the hidden layers. Different from other\nasymmetric BP-like methods, the feedback weights are also plastic in our\nframework and are trained to approximate the forward activations. Preliminary\nresults show that our models outperform other asymmetric BP-like methods on the\nMNIST and the CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 05:00:54 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 16:41:35 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 01:16:07 GMT"}, {"version": "v4", "created": "Sun, 29 Apr 2018 23:53:18 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Luo", "Hongyin", ""], ["Fu", "Jie", ""], ["Glass", "James", ""]]}, {"id": "1702.07426", "submitter": "Ramin M. Hasani", "authors": "Ramin M. Hasani, Giorgio Ferrari, Hideaki Yamamoto, Sho Kono, Koji\n  Ishihara, Soya Fujimori, Takashi Tanii and Enrico Prati", "title": "Control of the Correlation of Spontaneous Neuron Activity in Biological\n  and Noise-activated CMOS Artificial Neural Microcircuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several indications that brain is organized not on a basis of\nindividual unreliable neurons, but on a micro-circuital scale providing Lego\nblocks employed to create complex architectures. At such an intermediate scale,\nthe firing activity in the microcircuits is governed by collective effects\nemerging by the background noise soliciting spontaneous firing, the degree of\nmutual connections between the neurons, and the topology of the connections. We\ncompare spontaneous firing activity of small populations of neurons adhering to\nan engineered scaffold with simulations of biologically plausible CMOS\nartificial neuron populations whose spontaneous activity is ignited by tailored\nbackground noise. We provide a full set of flexible and low-power consuming\nsilicon blocks including neurons, excitatory and inhibitory synapses, and both\nwhite and pink noise generators for spontaneous firing activation. We achieve a\ncomparable degree of correlation of the firing activity of the biological\nneurons by controlling the kind and the number of connection among the silicon\nneurons. The correlation between groups of neurons, organized as a ring of four\ndistinct populations connected by the equivalent of interneurons, is triggered\nmore effectively by adding multiple synapses to the connections than increasing\nthe number of independent point-to-point connections. The comparison between\nthe biological and the artificial systems suggests that a considerable number\nof synapses is active also in biological populations adhering to engineered\nscaffolds.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 00:09:40 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Hasani", "Ramin M.", ""], ["Ferrari", "Giorgio", ""], ["Yamamoto", "Hideaki", ""], ["Kono", "Sho", ""], ["Ishihara", "Koji", ""], ["Fujimori", "Soya", ""], ["Tanii", "Takashi", ""], ["Prati", "Enrico", ""]]}, {"id": "1702.07560", "submitter": "Eliya Nachmani", "authors": "Eliya Nachmani, Elad Marciano, David Burshtein and Yair Be'ery", "title": "RNN Decoding of Linear Block Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a practical, low complexity, close to optimal, channel decoder for\npowerful algebraic codes with short to moderate block length is an open\nresearch problem. Recently it has been shown that a feed-forward neural network\narchitecture can improve on standard belief propagation decoding, despite the\nlarge example space. In this paper we introduce a recurrent neural network\narchitecture for decoding linear block codes. Our method shows comparable bit\nerror rate results compared to the feed-forward neural network with\nsignificantly less parameters. We also demonstrate improved performance over\nbelief propagation on sparser Tanner graph representations of the codes.\nFurthermore, we demonstrate that the RNN decoder can be used to improve the\nperformance or alternatively reduce the computational complexity of the mRRD\nalgorithm for low complexity, close to optimal, decoding of short BCH codes.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 12:49:29 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Nachmani", "Eliya", ""], ["Marciano", "Elad", ""], ["Burshtein", "David", ""], ["Be'ery", "Yair", ""]]}, {"id": "1702.07787", "submitter": "Yong Xu", "authors": "Yong Xu and Qiuqiang Kong and Qiang Huang and Wenwu Wang and Mark D.\n  Plumbley", "title": "Convolutional Gated Recurrent Neural Network Incorporating Spatial\n  Features for Audio Tagging", "comments": "Accepted to IJCNN2017, Anchorage, Alaska, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental audio tagging is a newly proposed task to predict the presence\nor absence of a specific audio event in a chunk. Deep neural network (DNN)\nbased methods have been successfully adopted for predicting the audio tags in\nthe domestic audio scene. In this paper, we propose to use a convolutional\nneural network (CNN) to extract robust features from mel-filter banks (MFBs),\nspectrograms or even raw waveforms for audio tagging. Gated recurrent unit\n(GRU) based recurrent neural networks (RNNs) are then cascaded to model the\nlong-term temporal structure of the audio signal. To complement the input\ninformation, an auxiliary CNN is designed to learn on the spatial features of\nstereo recordings. We evaluate our proposed methods on Task 4 (audio tagging)\nof the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE\n2016) challenge. Compared with our recent DNN-based method, the proposed\nstructure can reduce the equal error rate (EER) from 0.13 to 0.11 on the\ndevelopment set. The spatial features can further reduce the EER to 0.10. The\nperformance of the end-to-end learning on raw waveforms is also comparable.\nFinally, on the evaluation set, we get the state-of-the-art performance with\n0.12 EER while the performance of the best existing system is 0.15 EER.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 22:27:29 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Xu", "Yong", ""], ["Kong", "Qiuqiang", ""], ["Huang", "Qiang", ""], ["Wang", "Wenwu", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1702.07800", "submitter": "Haohan Wang", "authors": "Haohan Wang and Bhiksha Raj", "title": "On the Origin of Deep Learning", "comments": "70 pages, 200 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a review of the evolutionary history of deep learning models.\nIt covers from the genesis of neural networks when associationism modeling of\nthe brain is studied, to the models that dominate the last decade of research\nin deep learning like convolutional neural networks, deep belief networks, and\nrecurrent neural networks. In addition to a review of these models, this paper\nprimarily focuses on the precedents of the models above, examining how the\ninitial ideas are assembled to construct the early models and how these\npreliminary models are developed into their current forms. Many of these\nevolutionary paths last more than half a century and have a diversity of\ndirections. For example, CNN is built on prior knowledge of biological vision\nsystem; DBN is evolved from a trade-off of modeling power and computation\ncomplexity of graphical models and many nowadays models are neural counterparts\nof ancient linear models. This paper reviews these evolutionary paths and\noffers a concise thought flow of how these models are developed, and aims to\nprovide a thorough background for deep learning. More importantly, along with\nthe path, this paper summarizes the gist behind these milestones and proposes\nmany directions to guide the future research of deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 23:30:08 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 08:30:41 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 01:41:09 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 03:03:32 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wang", "Haohan", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1702.07805", "submitter": "Robert DiPietro", "authors": "Robert DiPietro, Christian Rupprecht, Nassir Navab, Gregory D. Hager", "title": "Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term\n  Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have achieved state-of-the-art performance\non many diverse tasks, from machine translation to surgical activity\nrecognition, yet training RNNs to capture long-term dependencies remains\ndifficult. To date, the vast majority of successful RNN architectures alleviate\nthis problem using nearly-additive connections between states, as introduced by\nlong short-term memory (LSTM). We take an orthogonal approach and introduce\nMIST RNNs, a NARX RNN architecture that allows direct connections from the very\ndistant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient\nproperties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far\nmore efficient than previously-proposed NARX RNN architectures, requiring even\nfewer computations than LSTM; and 3) improve performance substantially over\nLSTM and Clockwork RNNs on tasks requiring very long-term dependencies.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 23:48:11 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 15:53:56 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 12:37:36 GMT"}, {"version": "v4", "created": "Fri, 20 Apr 2018 18:32:09 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["DiPietro", "Robert", ""], ["Rupprecht", "Christian", ""], ["Navab", "Nassir", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1702.07811", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama", "title": "Adaptive Neural Networks for Efficient Inference", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:527-536, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 00:22:51 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 18:14:49 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Wang", "Joseph", ""], ["Dekel", "Ofer", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1702.07825", "submitter": "Andrew Gibiansky", "authors": "Sercan O. Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew\n  Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman,\n  Shubho Sengupta, Mohammad Shoeybi", "title": "Deep Voice: Real-time Neural Text-to-Speech", "comments": "Submitted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 03:11:04 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 23:09:23 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Arik", "Sercan O.", ""], ["Chrzanowski", "Mike", ""], ["Coates", "Adam", ""], ["Diamos", "Gregory", ""], ["Gibiansky", "Andrew", ""], ["Kang", "Yongguo", ""], ["Li", "Xian", ""], ["Miller", "John", ""], ["Ng", "Andrew", ""], ["Raiman", "Jonathan", ""], ["Sengupta", "Shubho", ""], ["Shoeybi", "Mohammad", ""]]}, {"id": "1702.08139", "submitter": "Zichao Yang", "authors": "Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, Taylor Berg-Kirkpatrick", "title": "Improved Variational Autoencoders for Text Modeling using Dilated\n  Convolutions", "comments": "camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:16:01 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:31:34 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Yang", "Zichao", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1702.08231", "submitter": "Benjamin Graham", "authors": "Benjamin Graham", "title": "Low-Precision Batch-Normalized Activations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks can be trained with relatively low-precision\nfloating-point and fixed-point arithmetic, using between one and 16 bits.\nPrevious works have focused on relatively wide-but-shallow, feed-forward\nnetworks. We introduce a quantization scheme that is compatible with training\nvery deep neural networks. Quantizing the network activations in the middle of\neach batch-normalization module can greatly reduce the amount of memory and\ncomputational power needed, with little loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 11:10:54 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Graham", "Benjamin", ""]]}, {"id": "1702.08389", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Jeff Schneider, Barnabas Poczos", "title": "Equivariance Through Parameter-Sharing", "comments": "icml'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study equivariance in deep neural networks through parameter\nsymmetries. In particular, given a group $\\mathcal{G}$ that acts discretely on\nthe input and output of a standard neural network layer $\\phi_{W}: \\Re^{M} \\to\n\\Re^{N}$, we show that $\\phi_{W}$ is equivariant with respect to\n$\\mathcal{G}$-action iff $\\mathcal{G}$ explains the symmetries of the network\nparameters $W$. Inspired by this observation, we then propose two\nparameter-sharing schemes to induce the desirable symmetry on $W$. Our\nprocedures for tying the parameters achieve $\\mathcal{G}$-equivariance and,\nunder some conditions on the action of $\\mathcal{G}$, they guarantee\nsensitivity to all other permutation groups outside $\\mathcal{G}$.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:22:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 19:37:28 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1702.08481", "submitter": "Nenad Marku\\v{s}", "authors": "Nenad Marku\\v{s}, Ivan Gogi\\'c, Igor S. Pand\\v{z}i\\'c, J\\\"orgen\n  Ahlberg", "title": "Memory-Efficient Global Refinement of Decision-Tree Ensembles and its\n  Application to Face Alignment", "comments": "BMVC Newcastle 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ren et al. recently introduced a method for aggregating multiple decision\ntrees into a strong predictor by interpreting a path taken by a sample down\neach tree as a binary vector and performing linear regression on top of these\nvectors stacked together. They provided experimental evidence that the method\noffers advantages over the usual approaches for combining decision trees\n(random forests and boosting). The method truly shines when the regression\ntarget is a large vector with correlated dimensions, such as a 2D face shape\nrepresented with the positions of several facial landmarks. However, we argue\nthat their basic method is not applicable in many practical scenarios due to\nlarge memory requirements. This paper shows how this issue can be solved\nthrough the use of quantization and architectural changes of the predictor that\nmaps decision tree-derived encodings to the desired output.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:25:53 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 08:49:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Marku\u0161", "Nenad", ""], ["Gogi\u0107", "Ivan", ""], ["Pand\u017ei\u0107", "Igor S.", ""], ["Ahlberg", "J\u00f6rgen", ""]]}, {"id": "1702.08580", "submitter": "Haihao Lu", "authors": "Haihao Lu and Kenji Kawaguchi", "title": "Depth Creates No Bad Local Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, \\textit{depth}, as well as \\textit{nonlinearity}, create\nnon-convex loss surfaces. Then, does depth alone create bad local minima? In\nthis paper, we prove that without nonlinearity, depth alone does not create bad\nlocal minima, although it induces non-convex loss surface. Using this insight,\nwe greatly simplify a recently proposed proof to show that all of the local\nminima of feedforward deep linear neural networks are global minima. Our\ntheoretical results generalize previous results with fewer assumptions, and\nthis analysis provides a method to show similar results beyond square loss in\ndeep linear models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 23:27:36 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 03:42:41 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lu", "Haihao", ""], ["Kawaguchi", "Kenji", ""]]}, {"id": "1702.08591", "submitter": "David Balduzzi", "authors": "David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma,\n  Brian McWilliams", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is\n  the question?", "comments": "ICML 2017, final version", "journal-ref": "PMLR volume 70 (2017)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing obstacle to progress in deep learning is the problem of\nvanishing and exploding gradients. Although, the problem has largely been\novercome via carefully constructed initializations and batch normalization,\narchitectures incorporating skip-connections such as highway and resnets\nperform much better than standard feedforward architectures despite well-chosen\ninitialization and batch normalization. In this paper, we identify the\nshattered gradients problem. Specifically, we show that the correlation between\ngradients in standard feedforward networks decays exponentially with depth\nresulting in gradients that resemble white noise whereas, in contrast, the\ngradients in architectures with skip-connections are far more resistant to\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\nsupport of the analysis, on both fully-connected networks and convnets.\nFinally, we present a new \"looks linear\" (LL) initialization that prevents\nshattering, with preliminary experiments showing the new initialization allows\nto train very deep networks without the addition of skip-connections.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 01:06:13 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 10:08:21 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Balduzzi", "David", ""], ["Frean", "Marcus", ""], ["Leary", "Lennox", ""], ["Lewis", "JP", ""], ["Ma", "Kurt Wan-Duo", ""], ["McWilliams", "Brian", ""]]}, {"id": "1702.08690", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Yizhou Yu", "title": "Borrowing Treasures from the Wealthy: Deep Transfer Learning through\n  Selective Joint Fine-tuning", "comments": "To appear in 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks require a large amount of labeled training data during\nsupervised learning. However, collecting and labeling so much data might be\ninfeasible in many cases. In this paper, we introduce a source-target selective\njoint fine-tuning scheme for improving the performance of deep learning tasks\nwith insufficient training data. In this scheme, a target learning task with\ninsufficient training data is carried out simultaneously with another source\nlearning task with abundant training data. However, the source learning task\ndoes not use all existing training data. Our core idea is to identify and use a\nsubset of training images from the original source learning task whose\nlow-level characteristics are similar to those from the target learning task,\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\nwe compute descriptors from linear or nonlinear filter bank responses on\ntraining images from both tasks, and use such descriptors to search for a\ndesired subset of training samples for the source learning task.\n  Experiments demonstrate that our selective joint fine-tuning scheme achieves\nstate-of-the-art performance on multiple visual classification tasks with\ninsufficient training data for deep learning. Such tasks include Caltech 256,\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\nfine-tuning without a source domain, the proposed method can improve the\nclassification accuracy by 2% - 10% using a single model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:40:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 11:51:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ge", "Weifeng", ""], ["Yu", "Yizhou", ""]]}, {"id": "1702.08727", "submitter": "Renars Liepins", "authors": "Karlis Freivalds, Renars Liepins", "title": "Improving the Neural GPU Architecture for Algorithm Learning", "comments": "Minor edits", "journal-ref": "NAMPI v2 - Neural Abstract Machines & Program Induction v2, 2018", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm learning is a core problem in artificial intelligence with\nsignificant implications on automation level that can be achieved by machines.\nRecently deep learning methods are emerging for synthesizing an algorithm from\nits input-output examples, the most successful being the Neural GPU, capable of\nlearning multiplication. We present several improvements to the Neural GPU that\nsubstantially reduces training time and improves generalization. We introduce a\nnew technique - hard nonlinearities with saturation costs- that has general\napplicability. We also introduce a technique of diagonal gates that can be\napplied to active-memory models. The proposed architecture is the first capable\nof learning decimal multiplication end-to-end.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:19:51 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 09:24:22 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Freivalds", "Karlis", ""], ["Liepins", "Renars", ""]]}, {"id": "1702.08882", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Bo Xie, Vikas Verma, Le Song", "title": "Deep Semi-Random Features for Nonlinear Function Approximation", "comments": "AAAI 2018 - Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose semi-random features for nonlinear function approximation. The\nflexibility of semi-random feature lies between the fully adjustable units in\ndeep learning and the random features used in kernel methods. For one hidden\nlayer models with semi-random features, we prove with no unrealistic\nassumptions that the model classes contain an arbitrarily good function as the\nwidth increases (universality), and despite non-convexity, we can find such a\ngood function (optimization theory) that generalizes to unseen new data\n(generalization bound). For deep models, with no unrealistic assumptions, we\nprove universal approximation ability, a lower bound on approximation error, a\npartial optimization guarantee, and a generalization bound. Depending on the\nproblems, the generalization bound of deep semi-random features can be\nexponentially better than the known bounds of deep ReLU nets; our\ngeneralization error bound can be independent of the depth, the number of\ntrainable weights as well as the input dimensionality. In experiments, we show\nthat semi-random features can match the performance of neural networks by using\nslightly more units, and it outperforms random features by using significantly\nfewer units. Moreover, we introduce a new implicit ensemble method by using\nsemi-random features.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 17:47:34 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 22:51:06 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 22:31:17 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 02:39:31 GMT"}, {"version": "v5", "created": "Fri, 2 Jun 2017 04:05:15 GMT"}, {"version": "v6", "created": "Sat, 10 Jun 2017 17:11:27 GMT"}, {"version": "v7", "created": "Tue, 21 Nov 2017 03:44:50 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Xie", "Bo", ""], ["Verma", "Vikas", ""], ["Song", "Le", ""]]}]