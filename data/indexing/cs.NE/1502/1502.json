[{"id": "1502.00130", "submitter": "Joseph Corneli", "authors": "Joseph Corneli and Ewen Maclean", "title": "The Search for Computational Intelligence", "comments": "8 pages. Submitted to Social Aspects of Cognition and Computing\n  symposium at AISB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We define and explore in simulation several rules for the local evolution of\ngenerative rules for 1D and 2D cellular automata. Our implementation uses\nstrategies from conceptual blending. We discuss potential applications to\nmodelling social dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 16:10:29 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Corneli", "Joseph", ""], ["Maclean", "Ewen", ""]]}, {"id": "1502.00193", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Albert Y.S. Lam, Victor O.K. Li", "title": "Evolutionary Artificial Neural Network Based on Chemical Reaction\n  Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2011.5949872", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) are very popular tools to design and evolve\nartificial neural networks (ANNs), especially to train them. These methods have\nadvantages over the conventional backpropagation (BP) method because of their\nlow computational requirement when searching in a large solution space. In this\npaper, we employ Chemical Reaction Optimization (CRO), a newly developed global\noptimization method, to replace BP in training neural networks. CRO is a\npopulation-based metaheuristics mimicking the transition of molecules and their\ninteractions in a chemical reaction. Simulation results show that CRO\noutperforms many EA strategies commonly used to train neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:39:30 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Yu", "James J. Q.", ""], ["Lam", "Albert Y. S.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1502.00194", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Albert Y.S. Lam, Victor O.K. Li", "title": "Real-Coded Chemical Reaction Optimization with Different Perturbation\n  Functions", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2012.6252925", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimics\nthe interactions of molecules in chemical reactions to search for the global\noptimum. The perturbation function greatly influences the performance of CRO on\nsolving different continuous problems. In this paper, we study four different\nprobability distributions, namely, the Gaussian distribution, the Cauchy\ndistribution, the exponential distribution, and a modified Rayleigh\ndistribution, for the perturbation function of CRO. Different distributions\nhave different impacts on the solutions. The distributions are tested by a set\nof well-known benchmark functions and simulation results show that problems\nwith different characteristics have different preference on the distribution\nfunction. Our study gives guidelines to design CRO for different types of\noptimization problems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:45:43 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Yu", "James J. Q.", ""], ["Lam", "Albert Y. S.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1502.00195", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu and Victor O.K. Li and Albert Y.S. Lam", "title": "Sensor Deployment for Air Pollution Monitoring Using Public\n  Transportation System", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2012.6256495", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution monitoring is a very popular research topic and many monitoring\nsystems have been developed. In this paper, we formulate the Bus Sensor\nDeployment Problem (BSDP) to select the bus routes on which sensors are\ndeployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is\na recently proposed metaheuristic designed to solve a wide range of\noptimization problems. Using the real world data, namely Hong Kong Island bus\nroute data, we perform a series of simulations and the results show that CRO is\ncapable of solving this optimization problem efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:48:18 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Yu", "James J. Q.", ""], ["Li", "Victor O. K.", ""], ["Lam", "Albert Y. S.", ""]]}, {"id": "1502.00196", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Victor O.K. Li, Albert Y.S. Lam", "title": "Optimal V2G Scheduling of Electric Vehicles and Unit Commitment using\n  Chemical Reaction Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2013.6557596", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electric vehicle (EV) may be used as energy storage which allows the\nbi-directional electricity flow between the vehicle's battery and the electric\npower grid. In order to flatten the load profile of the electricity system, EV\nscheduling has become a hot research topic in recent years. In this paper, we\npropose a new formulation of the joint scheduling of EV and Unit Commitment\n(UC), called EVUC. Our formulation considers the characteristics of EVs while\noptimizing the system total running cost. We employ Chemical Reaction\nOptimization (CRO), a general-purpose optimization algorithm to solve this\nproblem and the simulation results on a widely used set of instances indicate\nthat CRO can effectively optimize this problem.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:51:13 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Yu", "James J. Q.", ""], ["Li", "Victor O. K.", ""], ["Lam", "Albert Y. S.", ""]]}, {"id": "1502.00197", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Victor O.K. Li, Albert Y.S. Lam", "title": "An Inter-molecular Adaptive Collision Scheme for Chemical Reaction\n  Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2014.6900234", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization techniques are frequently applied in science and engineering\nresearch and development. Evolutionary algorithms, as a kind of general-purpose\nmetaheuristic, have been shown to be very effective in solving a wide range of\noptimization problems. A recently proposed chemical-reaction-inspired\nmetaheuristic, Chemical Reaction Optimization (CRO), has been applied to solve\nmany global optimization problems. However, the functionality of the\ninter-molecular ineffective collision operator in the canonical CRO design\noverlaps that of the on-wall ineffective collision operator, which can\npotential impair the overall performance. In this paper we propose a new\ninter-molecular ineffective collision operator for CRO for global optimization.\nTo fully utilize our newly proposed operator, we also design a scheme to adapt\nthe algorithm to optimization problems with different search space\ncharacteristics. We analyze the performance of our proposed algorithm with a\nnumber of widely used benchmark functions. The simulation results indicate that\nthe new algorithm has superior performance over the canonical CRO.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:53:48 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Yu", "James J. Q.", ""], ["Li", "Victor O. K.", ""], ["Lam", "Albert Y. S.", ""]]}, {"id": "1502.00199", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Albert Y.S. Lam, Victor O.K. Li", "title": "Chemical Reaction Optimization for the Set Covering Problem", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2014.6900233", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set covering problem (SCP) is one of the representative combinatorial\noptimization problems, having many practical applications. This paper\ninvestigates the development of an algorithm to solve SCP by employing chemical\nreaction optimization (CRO), a general-purpose metaheuristic. It is tested on a\nwide range of benchmark instances of SCP. The simulation results indicate that\nthis algorithm gives outstanding performance compared with other heuristics and\nmetaheuristics in solving SCP.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:56:13 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Yu", "James J. Q.", ""], ["Lam", "Albert Y. S.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1502.00702", "submitter": "Hui Jiang", "authors": "Shiliang Zhang and Hui Jiang", "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to\n  Probe and Learn Neural Networks", "comments": "31 pages, 5 Figures, technical report", "journal-ref": "Journal of Machine Learning Research (JMLR), 17(37):1-33, 2016.\n  (http://jmlr.org/papers/v17/15-335.html)", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel model for high-dimensional data, called the\nHybrid Orthogonal Projection and Estimation (HOPE) model, which combines a\nlinear orthogonal projection and a finite mixture model under a unified\ngenerative modeling framework. The HOPE model itself can be learned\nunsupervised from unlabelled data based on the maximum likelihood estimation as\nwell as discriminatively from labelled data. More interestingly, we have shown\nthe proposed HOPE models are closely related to neural networks (NNs) in a\nsense that each hidden layer can be reformulated as a HOPE model. As a result,\nthe HOPE framework can be used as a novel tool to probe why and how NNs work,\nmore importantly, to learn NNs in either supervised or unsupervised ways. In\nthis work, we have investigated the HOPE framework to learn NNs for several\nstandard tasks, including image recognition on MNIST and speech recognition on\nTIMIT. Experimental results have shown that the HOPE framework yields\nsignificant performance gains over the current state-of-the-art methods in\nvarious types of NN learning problems, including unsupervised feature learning,\nsupervised or semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 01:38:19 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 01:57:42 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Zhang", "Shiliang", ""], ["Jiang", "Hui", ""]]}, {"id": "1502.00718", "submitter": "Alireza Goudarzi", "authors": "Alireza Goudarzi and Alireza Shabani and Darko Stefanovic", "title": "Product Reservoir Computing: Time-Series Computation with Multiplicative\n  Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state networks (ESN), a type of reservoir computing (RC) architecture,\nare efficient and accurate artificial neural systems for time series processing\nand learning. An ESN consists of a core of recurrent neural networks, called a\nreservoir, with a small number of tunable parameters to generate a\nhigh-dimensional representation of an input, and a readout layer which is\neasily trained using regression to produce a desired output from the reservoir\nstates. Certain computational tasks involve real-time calculation of high-order\ntime correlations, which requires nonlinear transformation either in the\nreservoir or the readout layer. Traditional ESN employs a reservoir with\nsigmoid or tanh function neurons. In contrast, some types of biological neurons\nobey response curves that can be described as a product unit rather than a sum\nand threshold. Inspired by this class of neurons, we introduce a RC\narchitecture with a reservoir of product nodes for time series computation. We\nfind that the product RC shows many properties of standard ESN such as\nshort-term memory and nonlinear capacity. On standard benchmarks for chaotic\nprediction tasks, the product RC maintains the performance of a standard\nnonlinear ESN while being more amenable to mathematical analysis. Our study\nprovides evidence that such networks are powerful in highly nonlinear tasks\nowing to high-order statistics generated by the recurrent product node\nreservoir.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:04:33 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 00:13:03 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Goudarzi", "Alireza", ""], ["Shabani", "Alireza", ""], ["Stefanovic", "Darko", ""]]}, {"id": "1502.00839", "submitter": "Ant\\'onio Manso", "authors": "Luis Correia, Antonio Manso", "title": "A multiset model of multi-species evolution to solve big deceptive\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This chapter presents SMuGA, an integration of symbiogenesis with the\nMultiset Genetic Algorithm (MuGA). The symbiogenetic approach used here is\nbased on the host-parasite model with the novelty of varying the length of\nparasites along the evolutionary process. Additionally, it models\ncollaborations between multiple parasites and a single host. To improve\nefficiency, we introduced proxy evaluation of parasites, which saves fitness\nfunction calls and exponentially reduces the symbiotic collaborations produced.\nAnother novel feature consists of breaking the evolutionary cycle into two\nphases: a symbiotic phase and a phase of independent evolution of both hosts\nand parasites. SMuGA was tested in optimization of a variety of deceptive\nfunctions, with results one order of magnitude better than state of the art\nsymbiotic algorithms. This allowed to optimize deceptive problems with large\nsizes, and showed a linear scaling in the number of iterations to attain the\noptimum.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 12:27:40 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Correia", "Luis", ""], ["Manso", "Antonio", ""]]}, {"id": "1502.01380", "submitter": "Anna Kucerova", "authors": "Tom\\'a\\v{s} Mare\\v{s}, Eli\\v{s}ka Janouchov\\'a, and Anna\n  Ku\\v{c}erov\\'a", "title": "Artificial neural networks in calibration of nonlinear mechanical models", "comments": "26 pages, 8 figures, 11 tables, accepted for publication in Advances\n  in Engineering Software", "journal-ref": "Advances in Engineering Software, 95:68-81, 2016", "doi": "10.1016/j.advengsoft.2016.01.017", "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid development in numerical modelling of materials and the complexity of\nnew models increases quickly together with their computational demands. Despite\nthe growing performance of modern computers and clusters, calibration of such\nmodels from noisy experimental data remains a nontrivial and often\ncomputationally exhaustive task. The layered neural networks thus represent a\nrobust and efficient technique to overcome the time-consuming simulations of a\ncalibrated model. The potential of neural networks consists in simple\nimplementation and high versatility in approximating nonlinear relationships.\nTherefore, there were several approaches proposed to accelerate the calibration\nof nonlinear models by neural networks. This contribution reviews and compares\nthree possible strategies based on approximating (i) model response, (ii)\ninverse relationship between the model response and its parameters and (iii)\nerror function quantifying how well the model fits the data. The advantages and\ndrawbacks of particular strategies are demonstrated on the calibration of four\nparameters of the affinity hydration model from simulated data as well as from\nexperimental measurements. This model is highly nonlinear, but computationally\ncheap thus allowing its calibration without any approximation and better\nquantification of results obtained by the examined calibration strategies. The\npaper can be thus viewed as a guide intended for the engineers to help them\nselect an appropriate strategy in their particular calibration problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 22:24:35 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 20:24:04 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Mare\u0161", "Tom\u00e1\u0161", ""], ["Janouchov\u00e1", "Eli\u0161ka", ""], ["Ku\u010derov\u00e1", "Anna", ""]]}, {"id": "1502.01687", "submitter": "Christian Schulz", "authors": "Sebastian Lamm and Peter Sanders and Christian Schulz", "title": "Graph Partitioning for Independent Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing maximum independent sets in graphs is an important problem in\ncomputer science. In this paper, we develop an evolutionary algorithm to tackle\nthe problem. The core innovations of the algorithm are very natural combine\noperations based on graph partitioning and local search algorithms. More\nprecisely, we employ a state-of-the-art graph partitioner to derive operations\nthat enable us to quickly exchange whole blocks of given independent sets. To\nenhance newly computed offsprings we combine our operators with a local search\nalgorithm. Our experimental evaluation indicates that we are able to outperform\nstate-of-the-art algorithms on a variety of instances.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 19:35:24 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Lamm", "Sebastian", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1502.01753", "submitter": "Peter Wittek", "authors": "Peter Wittek, S\\'andor Dar\\'anyi, Efstratios Kontopoulos, Theodoros\n  Moysiadis, Ioannis Kompatsiaris", "title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving\n  Vector Field", "comments": "8 pages, 1 figure. Code used to conduct the experiments is available\n  at https://github.com/peterwittek/concept_drifts", "journal-ref": "Proceedings of IJCNN-15, International Joint Conference on Neural\n  Networks, pages 1--8, 2015", "doi": "10.1109/IJCNN.2015.7280766", "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the Aristotelian concept of potentiality vs. actuality allowing for\nthe study of energy and dynamics in language, we propose a field approach to\nlexical analysis. Falling back on the distributional hypothesis to\nstatistically model word meaning, we used evolving fields as a metaphor to\nexpress time-dependent changes in a vector space model by a combination of\nrandom indexing and evolving self-organizing maps (ESOM). To monitor semantic\ndrifts within the observation period, an experiment was carried out on the term\nspace of a collection of 12.8 million Amazon book reviews. For evaluation, the\nsemantic consistency of ESOM term clusters was compared with their respective\nneighbourhoods in WordNet, and contrasted with distances among term vectors by\nrandom indexing. We found that at 0.05 level of significance, the terms in the\nclusters showed a high level of semantic consistency. Tracking the drift of\ndistributional patterns in the term space across time periods, we found that\nconsistency decreased, but not at a statistically significant level. Our method\nis highly scalable, with interpretations in philosophy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 22:51:45 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wittek", "Peter", ""], ["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Kontopoulos", "Efstratios", ""], ["Moysiadis", "Theodoros", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1502.02072", "submitter": "Bharath Ramsundar", "authors": "Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David\n  Konerding, Vijay Pande", "title": "Massively Multitask Networks for Drug Discovery", "comments": "Preliminary work. Under review by the International Conference on\n  Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively multitask neural architectures provide a learning framework for\ndrug discovery that synthesizes information from many distinct biological\nsources. To train these architectures at scale, we gather large amounts of data\nfrom public sources to create a dataset of nearly 40 million measurements\nacross more than 200 biological targets. We investigate several aspects of the\nmultitask framework by performing a series of empirical studies and obtain some\ninteresting results: (1) massively multitask networks obtain predictive\naccuracies significantly better than single-task methods, (2) the predictive\npower of multitask networks improves as additional tasks and data are added,\n(3) the total amount of data and the total number of tasks both contribute\nsignificantly to multitask improvement, and (4) multitask networks afford\nlimited transferability to tasks not in the training set. Our results\nunderscore the need for greater data sharing and further algorithmic innovation\nto accelerate the drug discovery process.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 23:04:01 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ramsundar", "Bharath", ""], ["Kearnes", "Steven", ""], ["Riley", "Patrick", ""], ["Webster", "Dale", ""], ["Konerding", "David", ""], ["Pande", "Vijay", ""]]}, {"id": "1502.02367", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho and Yoshua Bengio", "title": "Gated Feedback Recurrent Neural Networks", "comments": "9 pages, removed appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 05:25:54 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 19:18:07 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 11:34:38 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 06:26:21 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chung", "Junyoung", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1502.02407", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu and Victor O.K. Li", "title": "A Social Spider Algorithm for Global Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing complexity of real-world problems has motivated computer\nscientists to search for efficient problem-solving methods. Metaheuristics\nbased on evolutionary computation and swarm intelligence are outstanding\nexamples of nature-inspired solution techniques. Inspired by the social\nspiders, we propose a novel Social Spider Algorithm to solve global\noptimization problems. This algorithm is mainly based on the foraging strategy\nof social spiders, utilizing the vibrations on the spider web to determine the\npositions of preys. Different from the previously proposed swarm intelligence\nalgorithms, we introduce a new social animal foraging strategy model to solve\noptimization problems. In addition, we perform preliminary parameter\nsensitivity analysis for our proposed algorithm, developing guidelines for\nchoosing the parameter values. The Social Spider Algorithm is evaluated by a\nseries of widely-used benchmark functions, and our proposed algorithm has\nsuperior performance compared with other state-of-the-art metaheuristics.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 09:46:40 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Yu", "James J. Q.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1502.02444", "submitter": "Berkay Kicanaoglu", "authors": "Rama Garimella, Berkay Kicanaoglu, Moncef Gabbouj", "title": "On the Dynamics of a Recurrent Hopfield Network", "comments": "6 pages, 6 figures, 1 table, submitted to IJCNN-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research paper novel real/complex valued recurrent Hopfield Neural\nNetwork (RHNN) is proposed. The method of synthesizing the energy landscape of\nsuch a network and the experimental investigation of dynamics of Recurrent\nHopfield Network is discussed. Parallel modes of operation (other than fully\nparallel mode) in layered RHNN is proposed. Also, certain potential\napplications are proposed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:45:02 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Garimella", "Rama", ""], ["Kicanaoglu", "Berkay", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1502.02478", "submitter": "Benjamin Graham", "authors": "Ben Graham, Jeremy Reizenstein, Leigh Robinson", "title": "Efficient batchwise dropout training using submatrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a popular technique for regularizing artificial neural networks.\nDropout networks are generally trained by minibatch gradient descent with a\ndropout mask turning off some of the units---a different pattern of dropout is\napplied to every sample in the minibatch. We explore a very simple alternative\nto the dropout mask. Instead of masking dropped out units by setting them to\nzero, we perform matrix multiplication using a submatrix of the weight\nmatrix---unneeded hidden units are never calculated. Performing dropout\nbatchwise, so that one pattern of dropout is used for each sample in a\nminibatch, we can substantially reduce training times. Batchwise dropout can be\nused with fully-connected and convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 13:29:48 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Graham", "Ben", ""], ["Reizenstein", "Jeremy", ""], ["Robinson", "Leigh", ""]]}, {"id": "1502.02551", "submitter": "Suyog Gupta", "authors": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan", "title": "Deep Learning with Limited Numerical Precision", "comments": "10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of large-scale deep neural networks is often constrained by the\navailable computational resources. We study the effect of limited precision\ndata representation and computation on neural network training. Within the\ncontext of low-precision fixed-point computations, we observe the rounding\nscheme to play a crucial role in determining the network's behavior during\ntraining. Our results show that deep networks can be trained using only 16-bit\nwide fixed-point number representation when using stochastic rounding, and\nincur little to no degradation in the classification accuracy. We also\ndemonstrate an energy-efficient hardware accelerator that implements\nlow-precision fixed-point arithmetic with stochastic rounding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:37:29 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Gupta", "Suyog", ""], ["Agrawal", "Ankur", ""], ["Gopalakrishnan", "Kailash", ""], ["Narayanan", "Pritish", ""]]}, {"id": "1502.02793", "submitter": "Andrew M. Sutton", "authors": "Tobias Friedrich and Timo K\\\"otzing and Martin Krejca and Andrew M.\n  Sutton", "title": "The Benefit of Sex in Noisy Evolutionary Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefit of sexual recombination is one of the most fundamental questions\nboth in population genetics and evolutionary computation. It is widely believed\nthat recombination helps solving difficult optimization problems. We present\nthe first result, which rigorously proves that it is beneficial to use sexual\nrecombination in an uncertain environment with a noisy fitness function. For\nthis, we model sexual recombination with a simple estimation of distribution\nalgorithm called the Compact Genetic Algorithm (cGA), which we compare with the\nclassical $\\mu+1$ EA. For a simple noisy fitness function with additive\nGaussian posterior noise $\\mathcal{N}(0,\\sigma^2)$, we prove that the\nmutation-only $\\mu+1$ EA typically cannot handle noise in polynomial time for\n$\\sigma^2$ large enough while the cGA runs in polynomial time as long as the\npopulation size is not too small. This shows that in this uncertain environment\nsexual recombination is provably beneficial. We observe the same behavior in a\nsmall empirical study.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 06:26:15 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Friedrich", "Tobias", ""], ["K\u00f6tzing", "Timo", ""], ["Krejca", "Martin", ""], ["Sutton", "Andrew M.", ""]]}, {"id": "1502.03509", "submitter": "Iain Murray", "authors": "Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle", "title": "MADE: Masked Autoencoder for Distribution Estimation", "comments": "9 pages and 1 page of supplementary material. Updated to match\n  published version", "journal-ref": "Proceedings of the 32nd International Conference on Machine\n  Learning, JMLR W&CP 37:881-889, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of recent interest in designing neural network models to\nestimate a distribution from a set of examples. We introduce a simple\nmodification for autoencoder neural networks that yields powerful generative\nmodels. Our method masks the autoencoder's parameters to respect autoregressive\nconstraints: each input is reconstructed only from previous inputs in a given\nordering. Constrained this way, the autoencoder outputs can be interpreted as a\nset of conditional probabilities, and their product, the full joint\nprobability. We can also train a single network that can decompose the joint\nprobability in multiple different orderings. Our simple framework can be\napplied to multiple architectures, including deep ones. Vectorized\nimplementations, such as on GPUs, are simple and fast. Experiments demonstrate\nthat this approach is competitive with state-of-the-art tractable distribution\nestimators. At test time, the method is significantly faster and scales better\nthan other autoregressive estimators.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:06:07 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 14:37:32 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Germain", "Mathieu", ""], ["Gregor", "Karol", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1502.03581", "submitter": "Ashish Chandra", "authors": "Ashish Chandra, Mohammad Suaib, and Dr. Rizwan Beg", "title": "Web spam classification using supervised artificial neural network\n  algorithms", "comments": "10 Pages in Advanced Computational Intelligence: An International\n  Journal (ACII), Vol.2, No.1, January 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid growth in technology employed by the spammers, there is a\nneed of classifiers that are more efficient, generic and highly adaptive.\nNeural Network based technologies have high ability of adaption as well as\ngeneralization. As per our knowledge, very little work has been done in this\nfield using neural network. We present this paper to fill this gap. This paper\nevaluates performance of three supervised learning algorithms of artificial\nneural network by creating classifiers for the complex problem of latest web\nspam pattern classification. These algorithms are Conjugate Gradient algorithm,\nResilient Backpropagation learning, and Levenberg-Marquardt algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 09:58:23 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Chandra", "Ashish", ""], ["Suaib", "Mohammad", ""], ["Beg", "Dr. Rizwan", ""]]}, {"id": "1502.03648", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Over-Sampling in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) are the state of the art on many engineering\nproblems such as computer vision and audition. A key factor in the success of\nthe DNN is scalability - bigger networks work better. However, the reason for\nthis scalability is not yet well understood. Here, we interpret the DNN as a\ndiscrete system, of linear filters followed by nonlinear activations, that is\nsubject to the laws of sampling theory. In this context, we demonstrate that\nover-sampled networks are more selective, learn faster and learn more robustly.\nOur findings may ultimately generalize to the human brain.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:29:03 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1502.03682", "submitter": "Matthias Samwald", "authors": "Jose Antonio Mi\\~narro-Gim\\'enez, Oscar Mar\\'in-Alonso, Matthias\n  Samwald", "title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:44:15 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Mi\u00f1arro-Gim\u00e9nez", "Jose Antonio", ""], ["Mar\u00edn-Alonso", "Oscar", ""], ["Samwald", "Matthias", ""]]}, {"id": "1502.03699", "submitter": "Jun He", "authors": "Jun He, Yong Wang and Yuren Zhou", "title": "Analysis of Solution Quality of a Multiobjective Optimization-based\n  Evolutionary Algorithm for Knapsack Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective optimisation is regarded as one of the most promising ways\nfor dealing with constrained optimisation problems in evolutionary\noptimisation. This paper presents a theoretical investigation of a\nmulti-objective optimisation evolutionary algorithm for solving the 0-1\nknapsack problem. Two initialisation methods are considered in the algorithm:\nlocal search initialisation and greedy search initialisation. Then the solution\nquality of the algorithm is analysed in terms of the approximation ratio.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 15:24:19 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["He", "Jun", ""], ["Wang", "Yong", ""], ["Zhou", "Yuren", ""]]}, {"id": "1502.04042", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Abstract Learning via Demodulation in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the brain, deep neural networks (DNN) are thought to learn\nabstract representations through their hierarchical architecture. However, at\npresent, how this happens is not well understood. Here, we demonstrate that DNN\nlearn abstract representations by a process of demodulation. We introduce a\nbiased sigmoid activation function and use it to show that DNN learn and\nperform better when optimized for demodulation. Our findings constitute the\nfirst unambiguous evidence that DNN perform abstract learning in practical use.\nOur findings may also explain abstract learning in the human brain.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 16:09:41 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1502.04163", "submitter": "Zhang Junlin", "authors": "Zhang Junlin, Cai Heng, Huang Tongwen, Xue Huiping", "title": "A Distributional Representation Model For Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a very concise deep learning approach for\ncollaborative filtering that jointly models distributional representation for\nusers and items. The proposed framework obtains better performance when\ncompared against current state-of-art algorithms and that made the\ndistributional representation model a promising direction for further research\nin the collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 03:23:53 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Junlin", "Zhang", ""], ["Heng", "Cai", ""], ["Tongwen", "Huang", ""], ["Huiping", "Xue", ""]]}, {"id": "1502.04423", "submitter": "Alireza Goudarzi", "authors": "Alireza Goudarzi and Alireza Shabani and Darko Stefanovic", "title": "Exploring Transfer Function Nonlinearity in Echo State Networks", "comments": "arXiv admin note: text overlap with arXiv:1502.00718", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supralinear and sublinear pre-synaptic and dendritic integration is\nconsidered to be responsible for nonlinear computation power of biological\nneurons, emphasizing the role of nonlinear integration as opposed to nonlinear\noutput thresholding. How, why, and to what degree the transfer function\nnonlinearity helps biologically inspired neural network models is not fully\nunderstood. Here, we study these questions in the context of echo state\nnetworks (ESN). ESN is a simple neural network architecture in which a fixed\nrecurrent network is driven with an input signal, and the output is generated\nby a readout layer from the measurements of the network states. ESN\narchitecture enjoys efficient training and good performance on certain\nsignal-processing tasks, such as system identification and time series\nprediction. ESN performance has been analyzed with respect to the connectivity\npattern in the network structure and the input bias. However, the effects of\nthe transfer function in the network have not been studied systematically.\nHere, we use an approach tanh on the Taylor expansion of a frequently used\ntransfer function, the hyperbolic tangent function, to systematically study the\neffect of increasing nonlinearity of the transfer function on the memory,\nnonlinear capacity, and signal processing performance of ESN. Interestingly, we\nfind that a quadratic approximation is enough to capture the computational\npower of ESN with tanh function. The results of this study apply to both\nsoftware and hardware implementation of ESN.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 05:52:23 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 02:01:13 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Goudarzi", "Alireza", ""], ["Shabani", "Alireza", ""], ["Stefanovic", "Darko", ""]]}, {"id": "1502.04434", "submitter": "Sergey Demyanov", "authors": "Sergey Demyanov, James Bailey, Ramamohanarao Kotagiri, Christopher\n  Leckie", "title": "Invariant backpropagation: how to train a transformation-invariant\n  neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification problems a classifier should be robust to small\nvariations in the input vector. This is a desired property not only for\nparticular transformations, such as translation and rotation in image\nclassification problems, but also for all others for which the change is small\nenough to retain the object perceptually indistinguishable. We propose two\nextensions of the backpropagation algorithm that train a neural network to be\nrobust to variations in the feature vector. While the first of them enforces\nrobustness of the loss function to all variations, the second method trains the\npredictions to be robust to a particular variation which changes the loss\nfunction the most. The second methods demonstrates better results, but is\nslightly slower. We analytically compare the proposed algorithm with two the\nmost similar approaches (Tangent BP and Adversarial Training), and propose\ntheir fast versions. In the experimental part we perform comparison of all\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\nproposed algorithm depends on the dataset size and data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 06:28:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 11:44:59 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 04:49:00 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Demyanov", "Sergey", ""], ["Bailey", "James", ""], ["Kotagiri", "Ramamohanarao", ""], ["Leckie", "Christopher", ""]]}, {"id": "1502.04623", "submitter": "Ivo Danihelka", "authors": "Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan\n  Wierstra", "title": "DRAW: A Recurrent Neural Network For Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:48:56 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 15:29:42 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Gregor", "Karol", ""], ["Danihelka", "Ivo", ""], ["Graves", "Alex", ""], ["Rezende", "Danilo Jimenez", ""], ["Wierstra", "Daan", ""]]}, {"id": "1502.04681", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Elman Mansimov and Ruslan Salakhutdinov", "title": "Unsupervised Learning of Video Representations using LSTMs", "comments": "Added link to code on github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use multilayer Long Short Term Memory (LSTM) networks to learn\nrepresentations of video sequences. Our model uses an encoder LSTM to map an\ninput sequence into a fixed length representation. This representation is\ndecoded using single or multiple decoder LSTMs to perform different tasks, such\nas reconstructing the input sequence, or predicting the future sequence. We\nexperiment with two kinds of input sequences - patches of image pixels and\nhigh-level representations (\"percepts\") of video frames extracted using a\npretrained convolutional net. We explore different design choices such as\nwhether the decoder LSTMs should condition on the generated output. We analyze\nthe outputs of the model qualitatively to see how well the model can\nextrapolate the learned video representation into the future and into the past.\nWe try to visualize and interpret the learned features. We stress test the\nmodel by running it on longer time scales and on out-of-domain data. We further\nevaluate the representations by finetuning them for a supervised learning\nproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We show\nthat the representations help improve classification accuracy, especially when\nthere are only a few training examples. Even models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help action recognition performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 20:00:07 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 23:45:59 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 00:42:07 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Srivastava", "Nitish", ""], ["Mansimov", "Elman", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1502.04972", "submitter": "Chuan-Yung Tsai", "authors": "Chuan-Yung Tsai and David D. Cox", "title": "Measuring and Understanding Sensory Representations within Deep Networks\n  Using a Numerical Optimization Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in sensory neuroscience is describing how the activity of\npopulations of neurons can represent useful features of the external\nenvironment. However, while neurophysiologists have long been able to record\nthe responses of neurons in awake, behaving animals, it is another matter\nentirely to say what a given neuron does. A key problem is that in many sensory\ndomains, the space of all possible stimuli that one might encounter is\neffectively infinite; in vision, for instance, natural scenes are\ncombinatorially complex, and an organism will only encounter a tiny fraction of\npossible stimuli. As a result, even describing the response properties of\nsensory neurons is difficult, and investigations of neuronal functions are\nalmost always critically limited by the number of stimuli that can be\nconsidered. In this paper, we propose a closed-loop, optimization-based\nexperimental framework for characterizing the response properties of sensory\nneurons, building on past efforts in closed-loop experimental methods, and\nleveraging recent advances in artificial neural networks to serve as as a\nproving ground for our techniques. Specifically, using deep convolutional\nneural networks, we asked whether modern black-box optimization techniques can\nbe used to interrogate the \"tuning landscape\" of an artificial neuron in a\ndeep, nonlinear system, without imposing significant constraints on the space\nof stimuli under consideration. We introduce a series of measures to quantify\nthe tuning landscapes, and show how these relate to the performances of the\nnetworks in an object recognition task. To the extent that deep convolutional\nneural networks increasingly serve as de facto working hypotheses for\nbiological vision, we argue that developing a unified approach for studying\nboth artificial and biological systems holds great potential to advance both\nfields together.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 17:39:04 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Tsai", "Chuan-Yung", ""], ["Cox", "David D.", ""]]}, {"id": "1502.05113", "submitter": "Jiajun Liu", "authors": "Jiajun Liu, Kun Zhao, Brano Kusy, Ji-rong Wen, Raja Jurdak", "title": "Temporal Embedding in Convolutional Neural Networks for Robust Learning\n  of Abstract Snippets", "comments": "a submission to kdd 15'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of periodical time-series remains challenging due to various\ntypes of data distortions and misalignments. Here, we propose a novel model\ncalled Temporal embedding-enhanced convolutional neural Network (TeNet) to\nlearn repeatedly-occurring-yet-hidden structural elements in periodical\ntime-series, called abstract snippets, for predicting future changes. Our model\nuses convolutional neural networks and embeds a time-series with its potential\nneighbors in the temporal domain for aligning it to the dominant patterns in\nthe dataset. The model is robust to distortions and misalignments in the\ntemporal domain and demonstrates strong prediction power for periodical\ntime-series.\n  We conduct extensive experiments and discover that the proposed model shows\nsignificant and consistent advantages over existing methods on a variety of\ndata modalities ranging from human mobility to household power consumption\nrecords. Empirical results indicate that the model is robust to various factors\nsuch as number of samples, variance of data, numerical ranges of data etc. The\nexperiments also verify that the intuition behind the model can be generalized\nto multiple data types and applications and promises significant improvement in\nprediction performances across the datasets studied.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 04:25:23 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Liu", "Jiajun", ""], ["Zhao", "Kun", ""], ["Kusy", "Brano", ""], ["Wen", "Ji-rong", ""], ["Jurdak", "Raja", ""]]}, {"id": "1502.05213", "submitter": "Sankar Mukherjee", "authors": "Sankar Mukherjee, Shyamal Kumar Das Mandal", "title": "F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief\n  Network", "comments": "OCOCOSDA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years multilayer perceptrons (MLPs) with many hid- den layers Deep\nNeural Network (DNN) has performed sur- prisingly well in many speech tasks,\ni.e. speech recognition, speaker verification, speech synthesis etc. Although\nin the context of F0 modeling these techniques has not been ex- ploited\nproperly. In this paper, Deep Belief Network (DBN), a class of DNN family has\nbeen employed and applied to model the F0 contour of synthesized speech which\nwas generated by HMM-based speech synthesis system. The experiment was done on\nBengali language. Several DBN-DNN architectures ranging from four to seven\nhidden layers and up to 200 hid- den units per hidden layer was presented and\nevaluated. The results were compared against clustering tree techniques pop-\nularly found in statistical parametric speech synthesis. We show that from\ntextual inputs DBN-DNN learns a high level structure which in turn improves F0\ncontour in terms of ob- jective and subjective tests.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 13:15:13 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Mukherjee", "Sankar", ""], ["Mandal", "Shyamal Kumar Das", ""]]}, {"id": "1502.05777", "submitter": "James Henderson", "authors": "James A. Henderson, TingTing A. Gibson, Janet Wiles", "title": "Spike Event Based Learning in Neural Networks", "comments": "Figure 4 can be viewed as a movie in a separate file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scheme is derived for learning connectivity in spiking neural networks. The\nscheme learns instantaneous firing rates that are conditional on the activity\nin other parts of the network. The scheme is independent of the choice of\nneuron dynamics or activation function, and network architecture. It involves\ntwo simple, online, local learning rules that are applied only in response to\noccurrences of spike events. This scheme provides a direct method for\ntransferring ideas between the fields of deep learning and computational\nneuroscience. This learning scheme is demonstrated using a layered feedforward\nspiking neural network trained self-supervised on a prediction and\nclassification task for moving MNIST images collected using a Dynamic Vision\nSensor.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 05:26:09 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Henderson", "James A.", ""], ["Gibson", "TingTing A.", ""], ["Wiles", "Janet", ""]]}, {"id": "1502.06094", "submitter": "Tom Ameloot", "authors": "Tom J. Ameloot and Jan Van den Bussche", "title": "Positive Neural Networks in Discrete Time Implement Monotone-Regular\n  Behaviors", "comments": null, "journal-ref": "Neural Computation, December 2015, Vol. 27, No. 12 , Pages\n  2623-2660", "doi": "10.1162/NECO_a_00789", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the expressive power of positive neural networks. The model uses\npositive connection weights and multiple input neurons. Different behaviors can\nbe expressed by varying the connection weights. We show that in discrete time,\nand in absence of noise, the class of positive neural networks captures the\nso-called monotone-regular behaviors, that are based on regular languages. A\nfiner picture emerges if one takes into account the delay by which a\nmonotone-regular behavior is implemented. Each monotone-regular behavior can be\nimplemented by a positive neural network with a delay of one time unit. Some\nmonotone-regular behaviors can be implemented with zero delay. And,\ninterestingly, some simple monotone-regular behaviors can not be implemented\nwith zero delay.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 11:17:08 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 08:35:42 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Ameloot", "Tom J.", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1502.06096", "submitter": "Richard Evans", "authors": "Richard Evans", "title": "Reinforcement Learning in a Neurally Controlled Robot Using Dopamine\n  Modulated STDP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that dopamine-modulated STDP can solve many of the\nissues associated with reinforcement learning, such as the distal reward\nproblem. Spiking neural networks provide a useful technique in implementing\nreinforcement learning in an embodied context as they can deal with continuous\nparameter spaces and as such are better at generalizing the correct behaviour\nto perform in a given context.\n  In this project we implement a version of DA-modulated STDP in an embodied\nrobot on a food foraging task. Through simulated dopaminergic neurons we show\nhow the robot is able to learn a sequence of behaviours in order to achieve a\nfood reward. In tests the robot was able to learn food-attraction behaviour,\nand subsequently unlearn this behaviour when the environment changed, in all 50\ntrials. Moreover we show that the robot is able to operate in an environment\nwhereby the optimal behaviour changes rapidly and so the agent must constantly\nrelearn. In a more complex environment, consisting of food-containers, the\nrobot was able to learn food-container attraction in 95% of trials, despite the\nlarge temporal distance between the correct behaviour and the reward. This is\nachieved by shifting the dopamine response from the primary stimulus (food) to\nthe secondary stimulus (food-container).\n  Our work provides insights into the reasons behind some observed biological\nphenomena, such as the bursting behaviour observed in dopaminergic neurons. As\nwell as demonstrating how spiking neural network controlled robots are able to\nsolve a range of reinforcement learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 11:38:38 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Evans", "Richard", ""]]}, {"id": "1502.06434", "submitter": "Barack Wanjawa Mr.", "authors": "B. W. Wanjawa and L. Muchemi", "title": "ANN Model to Predict Stock Prices at Stock Exchange Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock exchanges are considered major players in financial sectors of many\ncountries. Most Stockbrokers, who execute stock trade, use technical,\nfundamental or time series analysis in trying to predict stock prices, so as to\nadvise clients. However, these strategies do not usually guarantee good returns\nbecause they guide on trends and not the most likely price. It is therefore\nnecessary to explore improved methods of prediction.\n  The research proposes the use of Artificial Neural Network that is\nfeedforward multi-layer perceptron with error backpropagation and develops a\nmodel of configuration 5:21:21:1 with 80% training data in 130,000 cycles. The\nresearch develops a prototype and tests it on 2008-2012 data from stock markets\ne.g. Nairobi Securities Exchange and New York Stock Exchange, where prediction\nresults show MAPE of between 0.71% and 2.77%. Validation done with Encog and\nNeuroph realized comparable results. The model is thus capable of prediction on\ntypical stock markets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 06:59:18 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Wanjawa", "B. W.", ""], ["Muchemi", "L.", ""]]}, {"id": "1502.06464", "submitter": "Djork-Arn\\'e Clevert", "authors": "Djork-Arn\\'e Clevert, Andreas Mayr, Thomas Unterthiner, Sepp\n  Hochreiter", "title": "Rectified Factor Networks", "comments": "9 pages + 49 pages supplement", "journal-ref": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose rectified factor networks (RFNs) to efficiently construct very\nsparse, non-linear, high-dimensional representations of the input. RFN models\nidentify rare and small events in the input, have a low interference between\ncode units, have a small reconstruction error, and explain the data covariance\nstructure. RFN learning is a generalized alternating minimization algorithm\nderived from the posterior regularization method which enforces non-negative\nand normalized posterior means. We proof convergence and correctness of the RFN\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\ncovariance structure more precisely, and have a significantly smaller\nreconstruction error. We test RFNs as pretraining technique for deep networks\non different vision datasets, where RFNs were superior to RBMs and\nautoencoders. On gene expression data from two pharmaceutical drug discovery\nstudies, RFNs detected small and rare gene modules that revealed highly\nrelevant new biological insights which were so far missed by other unsupervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:44:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 21:27:53 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Clevert", "Djork-Arn\u00e9", ""], ["Mayr", "Andreas", ""], ["Unterthiner", "Thomas", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1502.06922", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He,\n  Jianshu Chen, Xinying Song, Rabab Ward", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval", "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": "10.1109/TASLP.2016.2520371", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 19:39:27 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2015 06:11:19 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 06:35:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Palangi", "Hamid", ""], ["Deng", "Li", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Song", "Xinying", ""], ["Ward", "Rabab", ""]]}, {"id": "1502.07058", "submitter": "Adam Harley", "authors": "Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 05:58:43 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Harley", "Adam W.", ""], ["Ufkes", "Alex", ""], ["Derpanis", "Konstantinos G.", ""]]}]