[{"id": "1012.0490", "submitter": "Alexander K. Vidybida", "authors": "Alexander K. Vidybida", "title": "Testing of information condensation in a model reverberating spiking\n  neural network", "comments": "12 pages, 9 figures, 40 references. Content of this work was\n  partially published in an abstract form in the abstract book of the 2nd\n  International Biophysics Congress and Biotechnology at GAP & 21th National\n  Biophysics Congress, (5-9 Oct. 2009) Diyarbakir, Turkey,\n  http://www.ibc2009.org/. In v2 the ancillary file movie.pdf is added, which\n  offers examples of neuronal network dynamics", "journal-ref": "International Journal of Neural Systems (IJNS), Volume: 21, Issue:\n  3 (June 2011), Page: 187-198", "doi": "10.1142/S0129065711002742", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about external world is delivered to the brain in the form of\nstructured in time spike trains. During further processing in higher areas,\ninformation is subjected to a certain condensation process, which results in\nformation of abstract conceptual images of external world, apparently,\nrepresented as certain uniform spiking activity partially independent on the\ninput spike trains details. Possible physical mechanism of condensation at the\nlevel of individual neuron was discussed recently. In a reverberating spiking\nneural network, due to this mechanism the dynamics should settle down to the\nsame uniform/periodic activity in response to a set of various inputs. Since\nthe same periodic activity may correspond to different input spike trains, we\ninterpret this as possible candidate for information condensation mechanism in\na network. Our purpose is to test this possibility in a network model\nconsisting of five fully connected neurons, particularly, the influence of\ngeometric size of the network, on its ability to condense information. Dynamics\nof 20 spiking neural networks of different geometric sizes are modelled by\nmeans of computer simulation. Each network was propelled into reverberating\ndynamics by applying various initial input spike trains. We run the dynamics\nuntil it becomes periodic. The Shannon's formula is used to calculate the\namount of information in any input spike train and in any periodic state found.\nAs a result, we obtain explicit estimate of the degree of information\ncondensation in the networks, and conclude that it depends strongly on the\nnet's geometric size.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 16:52:04 GMT"}, {"version": "v2", "created": "Mon, 10 Jan 2011 16:42:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Vidybida", "Alexander K.", ""]]}, {"id": "1012.0841", "submitter": "Pekka Malo", "authors": "Pekka Malo and Pyry Siitari and Ankur Sinha", "title": "Automated Query Learning with Wikipedia and Genetic Programming", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing information retrieval systems are based on bag of words\nmodel and are not equipped with common world knowledge. Work has been done\ntowards improving the efficiency of such systems by using intelligent\nalgorithms to generate search queries, however, not much research has been done\nin the direction of incorporating human-and-society level knowledge in the\nqueries. This paper is one of the first attempts where such information is\nincorporated into the search queries using Wikipedia semantics. The paper\npresents an essential shift from conventional token based queries to concept\nbased queries, leading to an enhanced efficiency of information retrieval\nsystems. To efficiently handle the automated query learning problem, we propose\nWikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based\nqueries are learnt using a co-evolving evolutionary procedure. Learning concept\nbased queries using an intelligent evolutionary procedure yields significant\nimprovement in performance which is shown through an extensive study using\nReuters newswire documents. Comparison of the proposed framework is performed\nwith other information retrieval systems. Concept based approach has also been\nimplemented on other information retrieval systems to justify the effectiveness\nof a transition from token based queries to concept based queries.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 20:53:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Malo", "Pekka", ""], ["Siitari", "Pyry", ""], ["Sinha", "Ankur", ""]]}, {"id": "1012.0952", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr, Daniel Johannsen, Timo K\\\"otzing, Per Kristian Lehre,\n  Markus Wagner, Carola Winzen", "title": "Faster Black-Box Algorithms Through Higher Arity Operators", "comments": "To appear at FOGA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the work of Lehre and Witt (GECCO 2010) on the unbiased black-box\nmodel by considering higher arity variation operators. In particular, we show\nthat already for binary operators the black-box complexity of \\leadingones\ndrops from $\\Theta(n^2)$ for unary operators to $O(n \\log n)$. For \\onemax, the\n$\\Omega(n \\log n)$ unary black-box complexity drops to O(n) in the binary case.\nFor $k$-ary operators, $k \\leq n$, the \\onemax-complexity further decreases to\n$O(n/\\log k)$.\n", "versions": [{"version": "v1", "created": "Sat, 4 Dec 2010 22:11:48 GMT"}], "update_date": "2010-12-07", "authors_parsed": [["Doerr", "Benjamin", ""], ["Johannsen", "Daniel", ""], ["K\u00f6tzing", "Timo", ""], ["Lehre", "Per Kristian", ""], ["Wagner", "Markus", ""], ["Winzen", "Carola", ""]]}, {"id": "1012.3098", "submitter": "Per Kristian Lehre", "authors": "Per Kristian Lehre and Xin Yao", "title": "On the Impact of Mutation-Selection Balance on the Runtime of\n  Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.AO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between mutation and selection plays a fundamental role in the\nbehaviour of evolutionary algorithms (EAs). However, this interplay is still\nnot completely understood. This paper presents a rigorous runtime analysis of a\nnon-elitist population-based EA that uses the linear ranking selection\nmechanism. The analysis focuses on how the balance between parameter $\\eta$,\ncontrolling the selection pressure in linear ranking, and parameter $\\chi$\ncontrolling the bit-wise mutation rate, impacts the runtime of the algorithm.\nThe results point out situations where a correct balance between selection\npressure and mutation rate is essential for finding the optimal solution in\npolynomial time. In particular, it is shown that there exist fitness functions\nwhich can only be solved in polynomial time if the ratio between parameters\n$\\eta$ and $\\chi$ is within a narrow critical interval, and where a small\nchange in this ratio can increase the runtime exponentially. Furthermore, it is\nshown quantitatively how the appropriate parameter choice depends on the\ncharacteristics of the fitness function. In addition to the original results on\nthe runtime of EAs, this paper also introduces a very useful analytical tool,\ni.e., multi-type branching processes, to the runtime analysis of non-elitist\npopulation-based EAs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 17:30:47 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Lehre", "Per Kristian", ""], ["Yao", "Xin", ""]]}, {"id": "1012.3476", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Aaron Courville, Yoshua Bengio", "title": "Adaptive Parallel Tempering for Stochastic Maximum Likelihood Learning\n  of RBMs", "comments": "Presented at the \"NIPS 2010 Workshop on Deep Learning and\n  Unsupervised Feature Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBM) have attracted a lot of attention of\nlate, as one the principle building blocks of deep networks. Training RBMs\nremains problematic however, because of the intractibility of their partition\nfunction. The maximum likelihood gradient requires a very robust sampler which\ncan accurately sample from the model despite the loss of ergodicity often\nincurred during learning. While using Parallel Tempering in the negative phase\nof Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a\ntrade-off between computational complexity and high ergodicity, and requires\ncareful hand-tuning of the temperatures. In this paper, we show that this\ntrade-off is unnecessary. The choice of optimal temperatures can be automated\nby minimizing average return time (a concept first proposed by [Katzgraber et\nal., 2006]) while chains can be spawned dynamically, as needed, thus minimizing\nthe computational overhead. We show on a synthetic dataset, that this results\nin better likelihood scores.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 21:23:09 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1012.3656", "submitter": "Stephen Luttrell", "authors": "Stephen Luttrell", "title": "Adaptive Cluster Expansion (ACE): A Multilayer Network for Estimating\n  Probability Density Functions", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an adaptive hierarchical method of estimating high dimensional\nprobability density functions. We call this method of density estimation the\n\"adaptive cluster expansion\" or ACE for short. We present an application of\nthis approach, based on a multilayer topographic mapping network, that\nadaptively estimates the joint probability density function of the pixel values\nof an image, and presents this result as a \"probability image\". We apply this\nto the problem of identifying statistically anomalous regions in otherwise\nstatistically homogeneous images.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 16:21:42 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Luttrell", "Stephen", ""]]}, {"id": "1012.3705", "submitter": "Stephen Luttrell", "authors": "Stephen Luttrell", "title": "Stochastic Vector Quantisers", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a stochastic generalisation of the standard Linde-Buzo-Gray\n(LBG) approach to vector quantiser (VQ) design is presented, in which the\nencoder is implemented as the sampling of a vector of code indices from a\nprobability distribution derived from the input vector, and the decoder is\nimplemented as a superposition of reconstruction vectors, and the stochastic VQ\nis optimised using a minimum mean Euclidean reconstruction distortion\ncriterion, as in the LBG case. Numerical simulations are used to demonstrate\nhow this leads to self-organisation of the stochastic VQ, where different\nstochastically sampled code indices become associated with different input\nsubspaces. This property may be used to automate the process of splitting\nhigh-dimensional input vectors into low-dimensional blocks before encoding\nthem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 18:10:46 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Luttrell", "Stephen", ""]]}, {"id": "1012.3724", "submitter": "Stephen Luttrell", "authors": "Stephen Luttrell", "title": "The Development of Dominance Stripes and Orientation Maps in a\n  Self-Organising Visual Cortex Network (VICON)", "comments": "33 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A self-organising neural network is presented that is based on a rigorous\nBayesian analysis of the information contained in individual neural firing\nevents. This leads to a visual cortex network (VICON) that has many of the\nproperties emerge when a mammalian visual cortex is exposed to data arriving\nfrom two imaging sensors (i.e. the two retinae), such as dominance stripes and\norientation maps.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 19:30:20 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Luttrell", "Stephen", ""]]}, {"id": "1012.4126", "submitter": "Stephen Luttrell", "authors": "Stephen Luttrell", "title": "Self-Organising Stochastic Encoders", "comments": "23 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processing of mega-dimensional data, such as images, scales linearly with\nimage size only if fixed size processing windows are used. It would be very\nuseful to be able to automate the process of sizing and interconnecting the\nprocessing windows. A stochastic encoder that is an extension of the standard\nLinde-Buzo-Gray vector quantiser, called a stochastic vector quantiser (SVQ),\nincludes this required behaviour amongst its emergent properties, because it\nautomatically splits the input space into statistically independent subspaces,\nwhich it then separately encodes. Various optimal SVQs have been obtained, both\nanalytically and numerically. Analytic solutions which demonstrate how the\ninput space is split into independent subspaces may be obtained when an SVQ is\nused to encode data that lives on a 2-torus (e.g. the superposition of a pair\nof uncorrelated sinusoids). Many numerical solutions have also been obtained,\nusing both SVQs and chains of linked SVQs: (1) images of multiple independent\ntargets (encoders for single targets emerge), (2) images of multiple correlated\ntargets (various types of encoder for single and multiple targets emerge), (3)\nsuperpositions of various waveforms (encoders for the separate waveforms emerge\n- this is a type of independent component analysis (ICA)), (4) maternal and\nfoetal ECGs (another example of ICA), (5) images of textures (orientation maps\nand dominance stripes emerge). Overall, SVQs exhibit a rich variety of\nself-organising behaviour, which effectively discovers the internal structure\nof the training data. This should have an immediate impact on \"intelligent\"\ncomputation, because it reduces the need for expert human intervention in the\ndesign of data processing algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 22:34:21 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Luttrell", "Stephen", ""]]}, {"id": "1012.4173", "submitter": "Stephen Luttrell", "authors": "S P Luttrell", "title": "A Self-Organising Neural Network for Processing Data from Multiple\n  Sensors", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how a folded Markov chain network can be applied to the\nproblem of processing data from multiple sensors, with an emphasis on the\nspecial case of 2 sensors. It is necessary to design the network so that it can\ntransform a high dimensional input vector into a posterior probability, for\nwhich purpose the partitioned mixture distribution network is ideally suited.\nThe underlying theory is presented in detail, and a simple numerical simulation\nis given that shows the emergence of ocular dominance stripes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Dec 2010 14:48:55 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Luttrell", "S P", ""]]}, {"id": "1012.4981", "submitter": "Leonid Litinskii", "authors": "Yakov Karandashev, Boris Kryzhanovsky and Leonid Litinskii", "title": "Local Minima of a Quadratic Binary Functional with a Quasi-Hebbian\n  Connection Matrix", "comments": "13 pages, 7 figures. Slightly extended version of the reports\n  presented to IJCNN-2010 and ICANN-2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local minima of a quadratic functional depending on binary variables are\ndiscussed. An arbitrary connection matrix can be presented in the form of\nquasi-Hebbian expansion where each pattern is supplied with its own individual\nweight. For such matrices statistical physics methods allow one to derive an\nequation describing local minima of the functional. A model where only one\nweight differs from other ones is discussed in detail. In this case the\nequation can be solved analytically. The critical values of the weight, for\nwhich the energy landscape is reconstructed, are obtained. Obtained results are\nconfirmed by computer simulations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 14:11:08 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Karandashev", "Yakov", ""], ["Kryzhanovsky", "Boris", ""], ["Litinskii", "Leonid", ""]]}]