[{"id": "2105.00053", "submitter": "Goncalo Raposo", "authors": "Gon\\c{c}alo Raposo and Pedro Tom\\'as and Nuno Roma", "title": "PositNN: Training Deep Neural Networks with Mixed Low-Precision Posit", "comments": null, "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), pp. 7908-7912", "doi": "10.1109/ICASSP39728.2021.9413919", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision formats have proven to be an efficient way to reduce not only\nthe memory footprint but also the hardware resources and power consumption of\ndeep learning computations. Under this premise, the posit numerical format\nappears to be a highly viable substitute for the IEEE floating-point, but its\napplication to neural networks training still requires further research. Some\npreliminary results have shown that 8-bit (and even smaller) posits may be used\nfor inference and 16-bit for training, while maintaining the model accuracy.\nThe presented research aims to evaluate the feasibility to train deep\nconvolutional neural networks using posits. For such purpose, a software\nframework was developed to use simulated posits and quires in end-to-end\ntraining and inference. This implementation allows using any bit size,\nconfiguration, and even mixed precision, suitable for different precision\nrequirements in various stages. The obtained results suggest that 8-bit posits\ncan substitute 32-bit floats during training with no negative impact on the\nresulting loss and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:30:37 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:26:38 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 17:15:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Raposo", "Gon\u00e7alo", ""], ["Tom\u00e1s", "Pedro", ""], ["Roma", "Nuno", ""]]}, {"id": "2105.00162", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, S. M. Ali Eslami, Jean-Baptiste Alayrac, Piotr\n  Mirowski, Dylan Banarse, Simon Osindero", "title": "Generative Art Using Neural Visual Grammars and Dual Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are perhaps only a few scientific methods, there seem to be\nalmost as many artistic methods as there are artists. Artistic processes appear\nto inhabit the highest order of open-endedness. To begin to understand some of\nthe processes of art making it is helpful to try to automate them even\npartially. In this paper, a novel algorithm for producing generative art is\ndescribed which allows a user to input a text string, and which in a creative\nresponse to this string, outputs an image which interprets that string. It does\nso by evolving images using a hierarchical neural Lindenmeyer system, and\nevaluating these images along the way using an image text dual encoder trained\non billions of images and their associated text from the internet. In doing so\nwe have access to and control over an instance of an artistic process, allowing\nanalysis of which aspects of the artistic process become the task of the\nalgorithm, and which elements remain the responsibility of the artist.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 04:21:52 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 01:34:46 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Eslami", "S. M. Ali", ""], ["Alayrac", "Jean-Baptiste", ""], ["Mirowski", "Piotr", ""], ["Banarse", "Dylan", ""], ["Osindero", "Simon", ""]]}, {"id": "2105.00173", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\n  Tool for Singers", "comments": "26 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CY cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current computational-emotion research has focused on applying acoustic\nproperties to analyze how emotions are perceived mathematically or used in\nnatural language processing machine learning models. While recent interest has\nfocused on analyzing emotions from the spoken voice, little experimentation has\nbeen performed to discover how emotions are recognized in the singing voice --\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\ndifficult to interpret, has corrupted/distorted/nonsense information like\nactual noise sounds in this case, or has a low ratio of usable/unusable\ninformation). Not only does this ignore the challenges of training machine\nlearning models on more subjective data and testing them with much noisier\ndata, but there is also a clear disconnect in progress between advancing the\ndevelopment of convolutional neural networks and the goal of emotionally\ncognizant artificial intelligence. By training a new model to include this type\nof information with a rich comprehension of psycho-acoustic properties, not\nonly can models be trained to recognize information within extremely noisy\ndata, but advancement can be made toward more complex biofeedback applications\n-- including creating a model which could recognize emotions given any human\ninformation (language, breath, voice, body, posture) and be used in any\nperformance medium (music, speech, acting) or psychological assistance for\npatients with disorders such as BPD, alexithymia, autism, among others. This\npaper seeks to reflect and expand upon the findings of related research and\npresent a stepping-stone toward this end goal.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 05:47:15 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 07:34:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2105.00293", "submitter": "Mohammad Samar Ansari", "authors": "Mohammad Samar Ansari", "title": "A Single-Layer Asymmetric RNN: Potential Low Hardware Complexity Linear\n  Equation Solver", "comments": "Preprint submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A single layer neural network for the solution of linear equations is\npresented. The proposed circuit is based on the standard Hopfield model albeit\nwith the added flexibility that the interconnection weight matrix need not be\nsymmetric. This results in an asymmetric Hopfield neural network capable of\nsolving linear equations. PSPICE simulation results are given which verify the\ntheoretical predictions. Experimental results for circuits set up to solve\nsmall problems further confirm the operation of the proposed circuit.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 16:05:14 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 08:10:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ansari", "Mohammad Samar", ""]]}, {"id": "2105.00324", "submitter": "Fangfang Xia", "authors": "Zixuan Zhao, Nathan Wycoff, Neil Getty, Rick Stevens, Fangfang Xia", "title": "Neko: a Library for Exploring Neuromorphic Learning Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neuromorphic computing is in a period of active exploration.\nWhile many tools have been developed to simulate neuronal dynamics or convert\ndeep networks to spiking models, general software libraries for learning rules\nremain underexplored. This is partly due to the diverse, challenging nature of\nefforts to design new learning rules, which range from encoding methods to\ngradient approximations, from population approaches that mimic the Bayesian\nbrain to constrained learning algorithms deployed on memristor crossbars. To\naddress this gap, we present Neko, a modular, extensible library with a focus\non aiding the design of new learning algorithms. We demonstrate the utility of\nNeko in three exemplar cases: online local learning, probabilistic learning,\nand analog on-device learning. Our results show that Neko can replicate the\nstate-of-the-art algorithms and, in one case, lead to significant\noutperformance in accuracy and speed. Further, it offers tools including\ngradient comparison that can help develop new algorithmic variants. Neko is an\nopen source Python library that supports PyTorch and TensorFlow backends.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 18:50:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhao", "Zixuan", ""], ["Wycoff", "Nathan", ""], ["Getty", "Neil", ""], ["Stevens", "Rick", ""], ["Xia", "Fangfang", ""]]}, {"id": "2105.00349", "submitter": "Andrea Castellani", "authors": "Andrea Castellani, Sebastian Schmitt, and Barbara Hammer", "title": "Estimating the electrical power output of industrial devices with\n  end-to-end time-series classification in the presence of label noise", "comments": "Accepted in Applied Data Science track at ECML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In complex industrial settings, it is common practice to monitor the\noperation of machines in order to detect undesired states, adjust maintenance\nschedules, optimize system performance or collect usage statistics of\nindividual machines. In this work, we focus on estimating the power output of a\nCombined Heat and Power (CHP) machine of a medium-sized company facility by\nanalyzing the total facility power consumption. We formulate the problem as a\ntime-series classification problem where the class label represents the CHP\npower output. As the facility is fully instrumented and sensor measurements\nfrom the CHP are available, we generate the training labels in an automated\nfashion from the CHP sensor readings. However, sensor failures result in\nmislabeled training data samples which are hard to detect and remove from the\ndataset. Therefore, we propose a novel multi-task deep learning approach that\njointly trains a classifier and an autoencoder with a shared embedding\nrepresentation. The proposed approach targets to gradually correct the\nmislabelled data samples during training in a self-supervised fashion, without\nany prior assumption on the amount of label noise. We benchmark our approach on\nseveral time-series classification datasets and find it to be comparable and\nsometimes better than state-of-the-art methods. On the real-world use-case of\npredicting the CHP power output, we thoroughly evaluate the architectural\ndesign choices and show that the final architecture considerably increases the\nrobustness of the learning process and consistently beats other recent\nstate-of-the-art algorithms in the presence of unstructured as well as\nstructured label noise.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 21:45:42 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:21:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Castellani", "Andrea", ""], ["Schmitt", "Sebastian", ""], ["Hammer", "Barbara", ""]]}, {"id": "2105.00420", "submitter": "Johann Dreo", "authors": "Johann Dreo (Systems Biology Group, Department of Computational\n  Biology, USR 3756, Institut Pasteur and CNRS, Paris, France), Arnaud\n  Liefooghe (Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, Lille,\n  France), S\\'ebastien Verel (Univ. Littoral C\\^ote d'Opale, Calais, France),\n  Marc Schoenauer (TAU, Inria, CNRS and UPSaclay, LISN, Saclay, France), Juan\n  J. Merelo (University of Granada, Granada, Spain), Alexandre Quemy (Poznan\n  University of Technology, Poznan, Poland), Benjamin Bouvier, Jan Gmys (Inria,\n  Lille, France)", "title": "Paradiseo: From a Modular Framework for Evolutionary Computation to the\n  Automated Design of Metaheuristics ---22 Years of Paradiseo---", "comments": "12 pages, 6 figures, 3 listings, 1 table. To appear in 2021 Genetic\n  and Evolutionary Computation Conference Companion (GECCO'21 Companion), July\n  10--14, 2021, Lille, France. ACM, New York, NY, USA", "journal-ref": null, "doi": "10.1145/3449726.3463276", "report-no": null, "categories": "cs.NE cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of metaheuristic optimization methods has led to the development\nof a large variety of algorithm paradigms. However, no algorithm clearly\ndominates all its competitors on all problems. Instead, the underlying variety\nof landscapes of optimization problems calls for a variety of algorithms to\nsolve them efficiently. It is thus of prior importance to have access to mature\nand flexible software frameworks which allow for an efficient exploration of\nthe algorithm design space. Such frameworks should be flexible enough to\naccommodate any kind of metaheuristics, and open enough to connect with\nhigher-level optimization, monitoring and evaluation softwares. This article\nsummarizes the features of the ParadisEO framework, a comprehensive C++ free\nsoftware which targets the development of modular metaheuristics. ParadisEO\nprovides a highly modular architecture, a large set of components, speed of\nexecution and automated algorithm design features, which are key to modern\napproaches to metaheuristics development.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:45:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dreo", "Johann", "", "Systems Biology Group, Department of Computational\n  Biology, USR 3756, Institut Pasteur and CNRS, Paris, France"], ["Liefooghe", "Arnaud", "", "Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, Lille,\n  France"], ["Verel", "S\u00e9bastien", "", "Univ. Littoral C\u00f4te d'Opale, Calais, France"], ["Schoenauer", "Marc", "", "TAU, Inria, CNRS and UPSaclay, LISN, Saclay, France"], ["Merelo", "Juan J.", "", "University of Granada, Granada, Spain"], ["Quemy", "Alexandre", "", "Poznan\n  University of Technology, Poznan, Poland"], ["Bouvier", "Benjamin", "", "Inria,\n  Lille, France"], ["Gmys", "Jan", "", "Inria,\n  Lille, France"]]}, {"id": "2105.00507", "submitter": "Dmitry Yarotsky", "authors": "Maksim Velikanov and Dmitry Yarotsky", "title": "Universal scaling laws in the gradient descent training of neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current theoretical results on optimization trajectories of neural networks\ntrained by gradient descent typically have the form of rigorous but potentially\nloose bounds on the loss values. In the present work we take a different\napproach and show that the learning trajectory can be characterized by an\nexplicit asymptotic at large training times. Specifically, the leading term in\nthe asymptotic expansion of the loss behaves as a power law $L(t) \\sim\nt^{-\\xi}$ with exponent $\\xi$ expressed only through the data dimension, the\nsmoothness of the activation function, and the class of function being\napproximated. Our results are based on spectral analysis of the integral\noperator representing the linearized evolution of a large network trained on\nthe expected loss. Importantly, the techniques we employ do not require\nspecific form of a data distribution, for example Gaussian, thus making our\nfindings sufficiently universal.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:46:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Velikanov", "Maksim", ""], ["Yarotsky", "Dmitry", ""]]}, {"id": "2105.00682", "submitter": "Leo Cazenille", "authors": "Leo Cazenille", "title": "Ensemble Feature Extraction for Multi-Container Quality-Diversity\n  Algorithms", "comments": "Draft version. 10 pages, 4 figures, 4 tables, Accepted at the\n  GECCO2021 Conference", "journal-ref": null, "doi": "10.1145/3449639.3459392", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality-Diversity algorithms search for large collections of diverse and\nhigh-performing solutions, rather than just for a single solution like typical\noptimisation methods. They are specially adapted for multi-modal problems that\ncan be solved in many different ways, such as complex reinforcement learning or\nrobotics tasks. However, these approaches are highly dependent on the choice of\nfeature descriptors (FDs) quantifying the similarity in behaviour of the\nsolutions. While FDs usually needs to be hand-designed, recent studies have\nproposed ways to define them automatically by using feature extraction\ntechniques, such as PCA or Auto-Encoders, to learn a representation of the\nproblem from previously explored solutions. Here, we extend these approaches to\nmore complex problems which cannot be efficiently explored by relying only on a\nsingle representation but require instead a set of diverse and complementary\nrepresentations. We describe MC-AURORA, a Quality-Diversity approach that\noptimises simultaneously several collections of solutions, each with a\ndifferent set of FDs, which are, in turn, defined automatically by an ensemble\nof modular auto-encoders. We show that this approach produces solutions that\nare more diverse than those produced by single-representation approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:35:00 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cazenille", "Leo", ""]]}, {"id": "2105.00843", "submitter": "Leila Ismail Prof.", "authors": "Huned Materwala and Leila Ismail", "title": "Performance and Energy-Aware Bi-objective Tasks Scheduling for Cloud\n  Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud computing enables remote execution of users tasks. The pervasive\nadoption of cloud computing in smart cities services and applications requires\ntimely execution of tasks adhering to Quality of Services (QoS).\n  However, the increasing use of computing servers exacerbates the issues of\nhigh energy consumption, operating costs, and environmental pollution.\nMaximizing the performance and minimizing the energy in a cloud data center is\nchallenging. In this paper, we propose a performance and energy optimization\nbi-objective algorithm to tradeoff the contradicting performance and energy\nobjectives. An evolutionary algorithm-based multi-objective optimization is for\nthe first time proposed using system performance counters. The performance of\nthe proposed model is evaluated using a realistic cloud dataset in a cloud\ncomputing environment. Our experimental results achieve higher performance and\nlower energy consumption compared to a state of the art algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:55:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Materwala", "Huned", ""], ["Ismail", "Leila", ""]]}, {"id": "2105.01196", "submitter": "Patryk Orzechowski", "authors": "Pawe{\\l} Renc, Patryk Orzechowski, Aleksander Byrski, Jaros{\\l}aw\n  W\\k{a}s, and Jason H. Moore", "title": "EBIC.JL -- an Efficient Implementation of Evolutionary Biclustering\n  Algorithm in Julia", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": "10.1145/3449726.3463197", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is a data mining technique which searches for local patterns in\nnumeric tabular data with main application in bioinformatics. This technique\nhas shown promise in multiple areas, including development of biomarkers for\ncancer, disease subtype identification, or gene-drug interactions among others.\nIn this paper we introduce EBIC.JL - an implementation of one of the most\naccurate biclustering algorithms in Julia, a modern highly parallelizable\nprogramming language for data science. We show that the new version maintains\ncomparable accuracy to its predecessor EBIC while converging faster for the\nmajority of the problems. We hope that this open source software in a\nhigh-level programming language will foster research in this promising field of\nbioinformatics and expedite development of new biclustering methods for big\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:30:38 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Renc", "Pawe\u0142", ""], ["Orzechowski", "Patryk", ""], ["Byrski", "Aleksander", ""], ["W\u0105s", "Jaros\u0142aw", ""], ["Moore", "Jason H.", ""]]}, {"id": "2105.01256", "submitter": "Muhannad Alkaddour", "authors": "Muhannad Alkaddour, Usman Tariq, Abhinav Dhall", "title": "Self-Supervised Approach for Facial Movement Based Optical Flow", "comments": "14 pages, 4 figures, 5 tables The supplemental material (error\n  histograms) can be found on\n  https://www.dropbox.com/s/o7158gi46tppvb1/SupplementalMaterial_OpticalFlow.docx?dl=0\n  Manuscript submitted to: IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing optical flow is a fundamental problem in computer vision. However,\ndeep learning-based optical flow techniques do not perform well for non-rigid\nmovements such as those found in faces, primarily due to lack of the training\ndata representing the fine facial motion. We hypothesize that learning optical\nflow on face motion data will improve the quality of predicted flow on faces.\nThe aim of this work is threefold: (1) exploring self-supervised techniques to\ngenerate optical flow ground truth for face images; (2) computing baseline\nresults on the effects of using face data to train Convolutional Neural\nNetworks (CNN) for predicting optical flow; and (3) using the learned optical\nflow in micro-expression recognition to demonstrate its effectiveness. We\ngenerate optical flow ground truth using facial key-points in the\nBP4D-Spontaneous dataset. The generated optical flow is used to train the\nFlowNetS architecture to test its performance on the generated dataset. The\nperformance of FlowNetS trained on face images surpassed that of other optical\nflow CNN architectures, demonstrating its usefulness. Our optical flow features\nare further compared with other methods using the STSTNet micro-expression\nclassifier, and the results indicate that the optical flow obtained using this\nwork has promising applications in facial expression analysis.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 02:38:11 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Alkaddour", "Muhannad", ""], ["Tariq", "Usman", ""], ["Dhall", "Abhinav", ""]]}, {"id": "2105.01358", "submitter": "Apoorv Kishore", "authors": "Apoorv Kishore, Vivek Saraswat, Udayan Ganguly", "title": "Simplified Klinokinesis using Spiking Neural Networks for\n  Resource-Constrained Navigation on the Neuromorphic Processor Loihi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  C. elegans shows chemotaxis using klinokinesis where the worm senses the\nconcentration based on a single concentration sensor to compute the\nconcentration gradient to perform foraging through gradient ascent/descent\ntowards the target concentration followed by contour tracking. The biomimetic\nimplementation requires complex neurons with multiple ion channel dynamics as\nwell as interneurons for control. While this is a key capability of autonomous\nrobots, its implementation on energy-efficient neuromorphic hardware like\nIntel's Loihi requires adaptation of the network to hardware-specific\nconstraints, which has not been achieved. In this paper, we demonstrate the\nadaptation of chemotaxis based on klinokinesis to Loihi by implementing\nnecessary neuronal dynamics with only LIF neurons as well as a complete\nspike-based implementation of all functions e.g. Heaviside function and\nsubtractions. Our results show that Loihi implementation is equivalent to the\nsoftware counterpart on Python in terms of performance - both during foraging\nand contour tracking. The Loihi results are also resilient in noisy\nenvironments. Thus, we demonstrate a successful adaptation of chemotaxis on\nLoihi - which can now be combined with the rich array of SNN blocks for SNN\nbased complex robotic control.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 08:26:46 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kishore", "Apoorv", ""], ["Saraswat", "Vivek", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2105.01616", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en and Alexander Schulz and Barbara Hammer", "title": "Reservoir Stack Machines", "comments": "in print at the Journal Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2021.05.106", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Memory-augmented neural networks equip a recurrent neural network with an\nexplicit memory to support tasks that require information storage without\ninterference over long times. A key motivation for such research is to perform\nclassic computation tasks, such as parsing. However, memory-augmented neural\nnetworks are notoriously hard to train, requiring many backpropagation epochs\nand a lot of data. In this paper, we introduce the reservoir stack machine, a\nmodel which can provably recognize all deterministic context-free languages and\ncircumvents the training problem by training only the output layer of a\nrecurrent net and employing auxiliary information during training about the\ndesired interaction with a stack. In our experiments, we validate the reservoir\nstack machine against deep and shallow networks from the literature on three\nbenchmark tasks for Neural Turing machines and six deterministic context-free\nlanguages. Our results show that the reservoir stack machine achieves zero\nerror, even on test sequences longer than the training data, requiring only a\nfew seconds of training time and 100 training sequences.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:50:40 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 08:08:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""], ["Schulz", "Alexander", ""], ["Hammer", "Barbara", ""]]}, {"id": "2105.01795", "submitter": "Anup Das", "authors": "Adarsha Balaji and Shihao Song and Twisha Titirsha and Anup Das and\n  Jeffrey Krichmar and Nikil Dutt and James Shackleford and Nagarajan Kandasamy\n  and Francky Catthoor", "title": "NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration\n  with Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, both industry and academia have proposed many different\nneuromorphic architectures to execute applications that are designed with\nSpiking Neural Network (SNN). Consequently, there is a growing need for an\nextensible simulation framework that can perform architectural explorations\nwith SNNs, including both platform-based design of today's hardware, and\nhardware-software co-design and design-technology co-optimization of the\nfuture. We present NeuroXplorer, a fast and extensible framework that is based\non a generalized template for modeling a neuromorphic architecture that can be\ninfused with the specific details of a given hardware and/or technology.\nNeuroXplorer can perform both low-level cycle-accurate architectural\nsimulations and high-level analysis with data-flow abstractions. NeuroXplorer's\noptimization engine can incorporate hardware-oriented metrics such as energy,\nthroughput, and latency, as well as SNN-oriented metrics such as inter-spike\ninterval distortion and spike disorder, which directly impact SNN performance.\nWe demonstrate the architectural exploration capabilities of NeuroXplorer\nthrough case studies with many state-of-the-art machine learning models.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:31:11 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Balaji", "Adarsha", ""], ["Song", "Shihao", ""], ["Titirsha", "Twisha", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey", ""], ["Dutt", "Nikil", ""], ["Shackleford", "James", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2105.02038", "submitter": "Anup Das", "authors": "Shihao Song, Jui Hanamshet, Adarsha Balaji, Anup Das, Jeffrey L.\n  Krichmar, Nikil D. Dutt, Nagarajan Kandasamy, Francky Catthoor", "title": "Dynamic Reliability Management in Neuromorphic Computing", "comments": "Accepted in ACM JETC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing systems uses non-volatile memory (NVM) to implement\nhigh-density and low-energy synaptic storage. Elevated voltages and currents\nneeded to operate NVMs cause aging of CMOS-based transistors in each neuron and\nsynapse circuit in the hardware, drifting the transistor's parameters from\ntheir nominal values. Aggressive device scaling increases power density and\ntemperature, which accelerates the aging, challenging the reliable operation of\nneuromorphic systems. Existing reliability-oriented techniques periodically\nde-stress all neuron and synapse circuits in the hardware at fixed intervals,\nassuming worst-case operating conditions, without actually tracking their aging\nat run time. To de-stress these circuits, normal operation must be interrupted,\nwhich introduces latency in spike generation and propagation, impacting the\ninter-spike interval and hence, performance, e.g., accuracy. We propose a new\narchitectural technique to mitigate the aging-related reliability problems in\nneuromorphic systems, by designing an intelligent run-time manager (NCRTM),\nwhich dynamically destresses neuron and synapse circuits in response to the\nshort-term aging in their CMOS transistors during the execution of machine\nlearning workloads, with the objective of meeting a reliability target. NCRTM\nde-stresses these circuits only when it is absolutely necessary to do so,\notherwise reducing the performance impact by scheduling de-stress operations\noff the critical path. We evaluate NCRTM with state-of-the-art machine learning\nworkloads on a neuromorphic hardware. Our results demonstrate that NCRTM\nsignificantly improves the reliability of neuromorphic hardware, with marginal\nimpact on performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:17:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Song", "Shihao", ""], ["Hanamshet", "Jui", ""], ["Balaji", "Adarsha", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey L.", ""], ["Dutt", "Nikil D.", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2105.02322", "submitter": "Zsigmond Benk\\H{o}", "authors": "Zsigmond Benk\\H{o}, Zolt\\'an Somogyv\\'ari", "title": "Reconstructing common latent input from time series with the\n  mapper-coach network and error backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A two-module, feedforward neural network architecture called mapper-coach\nnetwork has been introduced to reconstruct an unobserved, continuous latent\nvariable input, driving two observed dynamical systems. The method has been\ndemonstrated on time series generated by two chaotic logistic maps driven by a\nhidden third one. The network has been trained to predict one of the observed\ntime series based on its own past and on the other observed time series by\nerror-back propagation. It was shown, that after this prediction have been\nlearned successfully, the activity of the bottleneck neuron, connecting the\nmapper and the coach module, correlates strongly with the latent common input\nvariable. The method has the potential to reveal hidden components of dynamical\nsystems, where experimental intervention is not possible.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:55:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Benk\u0151", "Zsigmond", ""], ["Somogyv\u00e1ri", "Zolt\u00e1n", ""]]}, {"id": "2105.02590", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty, Kathy Baxter, Araz Taeihagh, Gregory A.\n  Bennett, Min-Yen Kan", "title": "Reliability Testing for Natural Language Processing Systems", "comments": "Accepted to ACL-IJCNLP 2021 (main conference). Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questions of fairness, robustness, and transparency are paramount to address\nbefore deploying NLP systems. Central to these concerns is the question of\nreliability: Can NLP systems reliably treat different demographics fairly and\nfunction correctly in diverse and noisy environments? To address this, we argue\nfor the need for reliability testing and contextualize it among existing work\non improving accountability. We show how adversarial attacks can be reframed\nfor this goal, via a framework for developing reliability tests. We argue that\nreliability testing -- with an emphasis on interdisciplinary collaboration --\nwill enable rigorous and targeted testing, and aid in the enactment and\nenforcement of industry standards.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 11:24:58 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 04:17:44 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 03:55:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""], ["Baxter", "Kathy", ""], ["Taeihagh", "Araz", ""], ["Bennett", "Gregory A.", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2105.02752", "submitter": "Arlindo Oliveira L", "authors": "M\\'ario Cardoso, Andr\\'e Cavalheiro, Alexandre Borges, Ana F. Duarte,\n  Am\\'ilcar Soares, Maria Jo\\~ao Pereira, Nuno J. Nunes, Leonardo Azevedo,\n  Arlindo L. Oliveira", "title": "Modeling the geospatial evolution of COVID-19 using spatio-temporal\n  convolutional sequence-to-sequence neural networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Europe was hit hard by the COVID-19 pandemic and Portugal was one of the most\naffected countries, having suffered three waves in the first twelve months.\nApproximately between Jan 19th and Feb 5th 2021 Portugal was the country in the\nworld with the largest incidence rate, with 14-days incidence rates per 100,000\ninhabitants in excess of 1000. Despite its importance, accurate prediction of\nthe geospatial evolution of COVID-19 remains a challenge, since existing\nanalytical methods fail to capture the complex dynamics that result from both\nthe contagion within a region and the spreading of the infection from infected\nneighboring regions.\n  We use a previously developed methodology and official municipality level\ndata from the Portuguese Directorate-General for Health (DGS), relative to the\nfirst twelve months of the pandemic, to compute an estimate of the incidence\nrate in each location of mainland Portugal. The resulting sequence of incidence\nrate maps was then used as a gold standard to test the effectiveness of\ndifferent approaches in the prediction of the spatial-temporal evolution of the\nincidence rate. Four different methods were tested: a simple cell level\nautoregressive moving average (ARMA) model, a cell level vector autoregressive\n(VAR) model, a municipality-by-municipality compartmental SIRD model followed\nby direct block sequential simulation and a convolutional sequence-to-sequence\nneural network model based on the STConvS2S architecture. We conclude that the\nconvolutional sequence-to-sequence neural network is the best performing\nmethod, when predicting the medium-term future incidence rate, using the\navailable information.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:24:00 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Cardoso", "M\u00e1rio", ""], ["Cavalheiro", "Andr\u00e9", ""], ["Borges", "Alexandre", ""], ["Duarte", "Ana F.", ""], ["Soares", "Am\u00edlcar", ""], ["Pereira", "Maria Jo\u00e3o", ""], ["Nunes", "Nuno J.", ""], ["Azevedo", "Leonardo", ""], ["Oliveira", "Arlindo L.", ""]]}, {"id": "2105.02786", "submitter": "Yi Ding", "authors": "Yi Ding, Neethu Robinson, Qiuhao Zeng, Cuntai Guan", "title": "LGGNet: Learning from Local-Global-Graph Representations for\n  Brain-Computer Interface", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose LGG, a neurologically inspired graph neural\nnetwork, to learn local-global-graph representations from\nElectroencephalography (EEG) for a Brain-Computer Interface (BCI). A temporal\nconvolutional layer with multi-scale 1D convolutional kernels and kernel-level\nattention fusion is proposed to learn the temporal dynamics of EEG. Inspired by\nneurological knowledge of cognitive processes in the brain, we propose local\nand global graph-filtering layers to learn the brain activities within and\nbetween different functional areas of the brain to model the complex relations\namong them during the cognitive processes. Under the robust nested\ncross-validation settings, the proposed method is evaluated on the publicly\navailable dataset DEAP, and the classification performance is compared with\nstate-of-the-art methods, such as FBFgMDM, FBTSC, Unsupervised learning,\nDeepConvNet, ShallowConvNet, EEGNet, and TSception. The results show that the\nproposed method outperforms all these state-of-the-art methods, and the\nimprovements are statistically significant (p<0.05) in most cases. The source\ncode can be found at: https://github.com/yi-ding-cs/LGG\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:06:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ding", "Yi", ""], ["Robinson", "Neethu", ""], ["Zeng", "Qiuhao", ""], ["Guan", "Cuntai", ""]]}, {"id": "2105.02809", "submitter": "S.J.Ben Yoo", "authors": "Yun-jhu Lee, Mehmet Berkay On, Xian Xiao, Roberto Proietti, S. J. Ben\n  Yoo", "title": "Izhikevich-Inspired Optoelectronic Neurons with Excitatory and\n  Inhibitory Inputs for Energy-Efficient Photonic Spiking Neural Networks", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.optics", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We designed, prototyped, and experimentally demonstrated, for the first time\nto our knowledge, an optoelectronic spiking neuron inspired by the Izhikevich\nmodel incorporating both excitatory and inhibitory optical spiking inputs and\nproducing optical spiking outputs accordingly. The optoelectronic neurons\nconsist of three transistors acting as electrical spiking circuits, a\nvertical-cavity surface-emitting laser (VCSEL) for optical spiking outputs, and\ntwo photodetectors for excitatory and inhibitory optical spiking inputs.\nAdditional inclusion of capacitors and resistors complete the\nIzhikevich-inspired optoelectronic neurons, which receive excitatory and\ninhibitory optical spikes as inputs from other optoelectronic neurons. We\ndeveloped a detailed optoelectronic neuron model in Verilog-A and simulated the\ncircuit-level operation of various cases with excitatory input and inhibitory\ninput signals. The experimental results closely resemble the simulated results\nand demonstrate how the excitatory inputs trigger the optical spiking outputs\nwhile the inhibitory inputs suppress the outputs. Utilizing the simulated\nneuron model, we conducted simulations using fully connected (FC) and\nconvolutional neural networks (CNN). The simulation results using MNIST\nhandwritten digits recognition show 90% accuracy on unsupervised learning and\n97% accuracy on a supervised modified FC neural network. We further designed a\nnanoscale optoelectronic neuron utilizing quantum impedance conversion where a\n200 aJ/spike input can trigger the output from on-chip nanolasers with 10\nfJ/spike. The nanoscale neuron can support a fanout of ~80 or overcome 19 dB\nexcess optical loss while running at 10 GSpikes/second in the neural network,\nwhich corresponds to 100x throughput and 1000x energy-efficiency improvement\ncompared to state-of-art electrical neuromorphic hardware such as Loihi and\nNeuroGrid.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 03:58:03 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lee", "Yun-jhu", ""], ["On", "Mehmet Berkay", ""], ["Xiao", "Xian", ""], ["Proietti", "Roberto", ""], ["Yoo", "S. J. Ben", ""]]}, {"id": "2105.02810", "submitter": "Bryar Hassan Dr.", "authors": "Bryar A. Hassan, TarikA. Rashid, Seyedali Mirjalili", "title": "Performance evaluation results of evolutionary clustering algorithm star\n  for clustering heterogeneous datasets", "comments": null, "journal-ref": null, "doi": "10.1016/j.dib.2021.107044", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article presents the data used to evaluate the performance of\nevolutionary clustering algorithm star (ECA*) compared to five traditional and\nmodern clustering algorithms. Two experimental methods are employed to examine\nthe performance of ECA* against genetic algorithm for clustering++\n(GENCLUST++), learning vector quantisation (LVQ) , expectation maximisation\n(EM) , K-means++ (KM++) and K-means (KM). These algorithms are applied to 32\nheterogenous and multi-featured datasets to determine which one performs well\non the three tests. For one, ther paper examines the efficiency of ECA* in\ncontradiction of its corresponding algorithms using clustering evaluation\nmeasures. These validation criteria are objective function and cluster quality\nmeasures. For another, it suggests a performance rating framework to measurethe\nthe performance sensitivity of these algorithms on varos dataset features\n(cluster dimensionality, number of clusters, cluster overlap, cluster shape and\ncluster structure). The contributions of these experiments are two-folds: (i)\nECA* exceeds its counterpart aloriths in ability to find out the right cluster\nnumber; (ii) ECA* is less sensitive towards dataset features compared to its\ncompetitive techniques. Nonetheless, the results of the experiments performed\ndemonstrate some limitations in the ECA*: (i) ECA* is not fully applied based\non the premise that no prior knowledge exists; (ii) Adapting and utilising ECA*\non several real applications has not been achieved yet.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:17:19 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hassan", "Bryar A.", ""], ["Rashid", "TarikA.", ""], ["Mirjalili", "Seyedali", ""]]}, {"id": "2105.02944", "submitter": "Edgar Galvan", "authors": "Edgar Galv\\'an, Leonardo Trujillo and Fergal Stapleton", "title": "Semantics in Multi-objective Genetic Programming", "comments": "30 pages, 4 figures, 10 tables, journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantics has become a key topic of research in Genetic Programming (GP).\nSemantics refers to the outputs (behaviour) of a GP individual when this is run\non a data set. The majority of works that focus on semantic diversity in\nsingle-objective GP indicates that it is highly beneficial in evolutionary\nsearch. Surprisingly, there is minuscule research conducted in semantics in\nMulti-objective GP (MOGP). In this work we make a leap beyond our understanding\nof semantics in MOGP and propose SDO: Semantic-based Distance as an additional\ncriteriOn. This naturally encourages semantic diversity in MOGP. To do so, we\nfind a pivot in the less dense region of the first Pareto front (most promising\nfront). This is then used to compute a distance between the pivot and every\nindividual in the population. The resulting distance is then used as an\nadditional criterion to be optimised to favour semantic diversity. We also use\ntwo other semantic-based methods as baselines, called Semantic Similarity-based\nCrossover and Semantic-based Crowding Distance. Furthermore, we also use the\nNSGA-II and the SPEA2 for comparison too. We use highly unbalanced binary\nclassification problems and consistently show how our proposed SDO approach\nproduces more non-dominated solutions and better diversity, leading to better\nstatistically significant results, using the hypervolume results as evaluation\nmeasure, compared to the rest of the other four methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:43:30 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Galv\u00e1n", "Edgar", ""], ["Trujillo", "Leonardo", ""], ["Stapleton", "Fergal", ""]]}, {"id": "2105.02953", "submitter": "Andrei Velichko", "authors": "Y. A. Izotov, A. A. Velichko, A. A. Ivshin and R. E. Novitskiy", "title": "Recognition of handwritten MNIST digits on low-memory 2 Kb RAM Arduino\n  board using LogNNet reservoir neural network", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1088/1757-899X/1155/1/012056", "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented compact algorithm for recognizing handwritten digits of the\nMNIST database, created on the LogNNet reservoir neural network, reaches the\nrecognition accuracy of 82%. The algorithm was tested on a low-memory Arduino\nboard with 2 Kb static RAM low-power microcontroller. The dependences of the\naccuracy and time of image recognition on the number of neurons in the\nreservoir have been investigated. The memory allocation demonstrates that the\nalgorithm stores all the necessary information in RAM without using additional\ndata storage, and operates with original images without preliminary processing.\nThe simple structure of the algorithm, with appropriate training, can be\nadapted for wide practical application, for example, for creating mobile\nbiosensors for early diagnosis of adverse events in medicine. The study results\nare important for the implementation of artificial intelligence on peripheral\nconstrained IoT devices and for edge computing.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:16:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Izotov", "Y. A.", ""], ["Velichko", "A. A.", ""], ["Ivshin", "A. A.", ""], ["Novitskiy", "R. E.", ""]]}, {"id": "2105.03090", "submitter": "Benjamin Doerr", "authors": "Henry Bambury, Antoine Bultel, Benjamin Doerr", "title": "An Extended Jump Function Benchmark for the Analysis of Randomized\n  Search Heuristics", "comments": "Extended version of a paper appearing in the proceedings of GECCO\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jump functions are the most studied non-unimodal benchmark in the theory of\nrandomized search heuristics, in particular, evolutionary algorithms (EAs).\nThey have significantly improved our understanding of how EAs escape from local\noptima. However, their particular structure -- to leave the local optimum one\ncan only jump directly to the global optimum -- raises the question of how\nrepresentative such results are.\n  For this reason, we propose an extended class $\\textsc{Jump}_{k,\\delta}$ of\njump functions that contain a valley of low fitness of width $\\delta$ starting\nat distance $k$ from the global optimum. We prove that several previous results\nextend to this more general class: for all $k = o(n^{1/3})$ and $\\delta < k$,\nthe optimal mutation rate for the $(1+1)$~EA is $\\frac{\\delta}{n}$, and the\nfast $(1+1)$~EA runs faster than the classical $(1+1)$~EA by a factor\nsuper-exponential in $\\delta$. However, we also observe that some known results\ndo not generalize: the randomized local search algorithm with stagnation\ndetection, which is faster than the fast $(1+1)$~EA by a factor polynomial in\n$k$ on $\\textsc{Jump}_k$, is slower by a factor polynomial in $n$ on some\n$\\textsc{Jump}_{k,\\delta}$ instances.\n  Computationally, the new class allows experiments with wider fitness valleys,\nespecially when they lie further away from the global optimum.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:21:10 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bambury", "Henry", ""], ["Bultel", "Antoine", ""], ["Doerr", "Benjamin", ""]]}, {"id": "2105.03649", "submitter": "Amar Shrestha", "authors": "Amar Shrestha, Haowen Fang, Daniel Patrick Rider, Zaidao Mei and Qinru\n  Qiu", "title": "In-Hardware Learning of Multilayer Spiking Neural Networks on a\n  Neuromorphic Processor", "comments": "6 pages, 5 figures, accepted for Design Automation Conference (DAC)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although widely used in machine learning, backpropagation cannot directly be\napplied to SNN training and is not feasible on a neuromorphic processor that\nemulates biological neuron and synapses. This work presents a spike-based\nbackpropagation algorithm with biological plausible local update rules and\nadapts it to fit the constraint in a neuromorphic hardware. The algorithm is\nimplemented on Intel Loihi chip enabling low power in-hardware supervised\nonline learning of multilayered SNNs for mobile applications. We test this\nimplementation on MNIST, Fashion-MNIST, CIFAR-10 and MSTAR datasets with\npromising performance and energy-efficiency, and demonstrate a possibility of\nincremental online learning with the implementation.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:22:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shrestha", "Amar", ""], ["Fang", "Haowen", ""], ["Rider", "Daniel Patrick", ""], ["Mei", "Zaidao", ""], ["Qiu", "Qinru", ""]]}, {"id": "2105.03680", "submitter": "Maciej Swiechowski PhD", "authors": "Maciej \\'Swiechowski", "title": "A Crossover That Matches Diverse Parents Together in Evolutionary\n  Algorithms", "comments": "Accepted to GECCO 2021", "journal-ref": null, "doi": "10.1145/3449726.3459431", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Crossover and mutation are the two main operators that lead to new solutions\nin evolutionary approaches. In this article, a new method of performing the\ncrossover phase is presented. The problem of choice is evolutionary decision\ntree construction. The method aims at finding such individuals that together\ncomplement each other. Hence we say that they are diversely specialized. We\npropose the way of calculating the so-called complementary fitness. In several\nempirical experiments, we evaluate the efficacy of the method proposed in four\nvariants and compare it to a fitness-rank-based approach. One variant emerges\nclearly as the best approach, whereas the remaining ones are below the\nbaseline.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 11:43:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["\u015awiechowski", "Maciej", ""]]}, {"id": "2105.03687", "submitter": "Yangjie Mei", "authors": "Yangjie Mei, Hao Wang", "title": "Covariance Matrix Adaptation Evolution Strategy Assisted by Principal\n  Component Analysis", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decades, more and more methods gain a giant development due to\nthe development of technology. Evolutionary Algorithms are widely used as a\nheuristic method. However, the budget of computation increases exponentially\nwhen the dimensions increase. In this paper, we will use the dimensionality\nreduction method Principal component analysis (PCA) to reduce the dimension\nduring the iteration of Covariance Matrix Adaptation Evolution Strategy\n(CMA-ES), which is a good Evolutionary Algorithm that is presented as the\nnumeric type and useful for different kinds of problems. We assess the\nperformance of our new methods in terms of convergence rate on multi-modal\nproblems from the Black-Box Optimization Benchmarking (BBOB) problem set and we\nalso use the framework COmparing Continuous Optimizers (COCO) to see how the\nnew method going and compare it to the other algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 12:43:38 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 11:13:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Mei", "Yangjie", ""], ["Wang", "Hao", ""]]}, {"id": "2105.03703", "submitter": "Greg Yang", "authors": "Greg Yang, Etai Littwin", "title": "Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel\n  Training Dynamics", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at\ninitialization has an infinite-width limit for a large class of architectures\nincluding modern staples such as ResNet and Transformers. However, their\nanalysis does not apply to training. Here, we show the same neural networks (in\nthe so-called NTK parametrization) during training follow a kernel gradient\ndescent dynamics in function space, where the kernel is the infinite-width NTK.\nThis completes the proof of the *architectural universality* of NTK behavior.\nTo achieve this result, we apply the Tensor Programs technique: Write the\nentire SGD dynamics inside a Tensor Program and analyze it via the Master\nTheorem. To facilitate this proof, we develop a graphical notation for Tensor\nPrograms.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 14:05:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Greg", ""], ["Littwin", "Etai", ""]]}, {"id": "2105.04045", "submitter": "Yingbo Li", "authors": "Yingbo Li, Yucong Duan, Zakaria Maama, Haoyang Che, Anamaria-Beatrice\n  Spulber, Stelios Fuentes", "title": "Swarm Differential Privacy for Purpose Driven\n  Data-Information-Knowledge-Wisdom Architecture", "comments": null, "journal-ref": "Mobile Information Systems. Volume 2021, Article ID 6671628. 28\n  Jun 2021", "doi": "10.1155/2021/6671628", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy protection has recently been in the spotlight of attention to both\nacademia and industry. Society protects individual data privacy through complex\nlegal frameworks. The increasing number of applications of data science and\nartificial intelligence has resulted in a higher demand for the ubiquitous\napplication of the data. The privacy protection of the broad\nData-Information-Knowledge-Wisdom (DIKW) landscape, the next generation of\ninformation organization, has taken a secondary role. In this paper, we will\nexplore DIKW architecture through the applications of the popular swarm\nintelligence and differential privacy. As differential privacy proved to be an\neffective data privacy approach, we will look at it from a DIKW domain\nperspective. Swarm Intelligence can effectively optimize and reduce the number\nof items in DIKW used in differential privacy, thus accelerating both the\neffectiveness and the efficiency of differential privacy for crossing multiple\nmodals of conceptual DIKW. The proposed approach is demonstrated through the\napplication of personalized data that is based on the open-sourse IRIS dataset.\nThis experiment demonstrates the efficiency of Swarm Intelligence in reducing\ncomputing complexity.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 23:09:07 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:27:40 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 18:50:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Yingbo", ""], ["Duan", "Yucong", ""], ["Maama", "Zakaria", ""], ["Che", "Haoyang", ""], ["Spulber", "Anamaria-Beatrice", ""], ["Fuentes", "Stelios", ""]]}, {"id": "2105.04097", "submitter": "Stephen MacDonell", "authors": "Nidhi Gowdra, Roopak Sinha and Stephen MacDonell", "title": "Examining convolutional feature extraction using Maximum Entropy (ME)\n  and Signal-to-Noise Ratio (SNR) for image classification", "comments": "Conference paper, 6 pages, 1 table", "journal-ref": "Proceedings of the 46th Annual Conference of the IEEE Industrial\n  Electronics Society (IECON2020). IEEE Computer Society Press, pp.471-476", "doi": "10.1109/IECON43393.2020.9254346", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) specialize in feature extraction rather\nthan function mapping. In doing so they form complex internal hierarchical\nfeature representations, the complexity of which gradually increases with a\ncorresponding increment in neural network depth. In this paper, we examine the\nfeature extraction capabilities of CNNs using Maximum Entropy (ME) and\nSignal-to-Noise Ratio (SNR) to validate the idea that, CNN models should be\ntailored for a given task and complexity of the input data. SNR and ME measures\nare used as they can accurately determine in the input dataset, the relative\namount of signal information to the random noise and the maximum amount of\ninformation respectively. We use two well known benchmarking datasets, MNIST\nand CIFAR-10 to examine the information extraction and abstraction capabilities\nof CNNs. Through our experiments, we examine convolutional feature extraction\nand abstraction capabilities in CNNs and show that the classification accuracy\nor performance of CNNs is greatly dependent on the amount, complexity and\nquality of the signal information present in the input data. Furthermore, we\nshow the effect of information overflow and underflow on CNN classification\naccuracies. Our hypothesis is that the feature extraction and abstraction\ncapabilities of convolutional layers are limited and therefore, CNN models\nshould be tailored to the input data by using appropriately sized CNNs based on\nthe SNR and ME measures of the input dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:58:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gowdra", "Nidhi", ""], ["Sinha", "Roopak", ""], ["MacDonell", "Stephen", ""]]}, {"id": "2105.04128", "submitter": "Stephen MacDonell", "authors": "Nidhi Gowdra, Roopak Sinha and Stephen MacDonell", "title": "Examining and Mitigating Kernel Saturation in Convolutional Neural\n  Networks using Negative Images", "comments": "Conference paper, 6 pages, 3 figures, 1 table", "journal-ref": "Proceedings of the 46th Annual Conference of the IEEE Industrial\n  Electronics Society (IECON2020). IEEE Computer Society Press, pp.465-470", "doi": "10.1109/IECON43393.2020.9255147", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural saturation in Deep Neural Networks (DNNs) has been studied\nextensively, but remains relatively unexplored in Convolutional Neural Networks\n(CNNs). Understanding and alleviating the effects of convolutional kernel\nsaturation is critical for enhancing CNN models classification accuracies. In\nthis paper, we analyze the effect of convolutional kernel saturation in CNNs\nand propose a simple data augmentation technique to mitigate saturation and\nincrease classification accuracy, by supplementing negative images to the\ntraining dataset. We hypothesize that greater semantic feature information can\nbe extracted using negative images since they have the same structural\ninformation as standard images but differ in their data representations. Varied\ndata representations decrease the probability of kernel saturation and thus\nincrease the effectiveness of kernel weight updates. The two datasets selected\nto evaluate our hypothesis were CIFAR- 10 and STL-10 as they have similar image\nclasses but differ in image resolutions thus making for a better understanding\nof the saturation phenomenon. MNIST dataset was used to highlight the\nineffectiveness of the technique for linearly separable data. The ResNet CNN\narchitecture was chosen since the skip connections in the network ensure the\nmost important features contributing the most to classification accuracy are\nretained. Our results show that CNNs are indeed susceptible to convolutional\nkernel saturation and that supplementing negative images to the training\ndataset can offer a statistically significant increase in classification\naccuracies when compared against models trained on the original datasets. Our\nresults present accuracy increases of 6.98% and 3.16% on the STL-10 and\nCIFAR-10 datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:06:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gowdra", "Nidhi", ""], ["Sinha", "Roopak", ""], ["MacDonell", "Stephen", ""]]}, {"id": "2105.04247", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Sebastian Berns, Alexander Asteroth, Simon Colton,\n  Thomas B\\\"ack", "title": "Expressivity of Parameterized and Data-driven Representations in Quality\n  Diversity Search", "comments": "For code for reproducing experiments, see\n  https://github.com/alexander-hagg/ExpressivityGECCO2021", "journal-ref": null, "doi": "10.1145/3449639.3459287", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider multi-solution optimization and generative models for the\ngeneration of diverse artifacts and the discovery of novel solutions. In cases\nwhere the domain's factors of variation are unknown or too complex to encode\nmanually, generative models can provide a learned latent space to approximate\nthese factors. When used as a search space, however, the range and diversity of\npossible outputs are limited to the expressivity and generative capabilities of\nthe learned model. We compare the output diversity of a quality diversity\nevolutionary search performed in two different search spaces: 1) a predefined\nparameterized space and 2) the latent space of a variational autoencoder model.\nWe find that the search on an explicit parametric encoding creates more diverse\nartifact sets than searching the latent space. A learned model is better at\ninterpolating between known data points than at extrapolating or expanding\ntowards unseen examples. We recommend using a generative model's latent space\nprimarily to measure similarity between artifacts rather than for search and\ngeneration. Whenever a parametric encoding is obtainable, it should be\npreferred over a learned representation as it produces a higher diversity of\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:27:43 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hagg", "Alexander", ""], ["Berns", "Sebastian", ""], ["Asteroth", "Alexander", ""], ["Colton", "Simon", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04252", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Mike Preuss, Alexander Asteroth, Thomas B\\\"ack", "title": "An Analysis of Phenotypic Diversity in Multi-Solution Optimization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-63710-1_4", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  More and more, optimization methods are used to find diverse solution sets.\nWe compare solution diversity in multi-objective optimization, multimodal\noptimization, and quality diversity in a simple domain. We show that\nmultiobjective optimization does not always produce much diversity, multimodal\noptimization produces higher fitness solutions, and quality diversity is not\nsensitive to genetic neutrality and creates the most diverse set of solutions.\nAn autoencoder is used to discover phenotypic features automatically, producing\nan even more diverse solution set with quality diversity. Finally, we make\nrecommendations about when to use which approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:39:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hagg", "Alexander", ""], ["Preuss", "Mike", ""], ["Asteroth", "Alexander", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04256", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Dominik Wilde, Alexander Asteroth, Thomas B\\\"ack", "title": "Designing Air Flow with Surrogate-assisted Phenotypic Niching", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58112-1_10", "report-no": null, "categories": "cs.NE cs.CE cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In complex, expensive optimization domains we often narrowly focus on finding\nhigh performing solutions, instead of expanding our understanding of the domain\nitself. But what if we could quickly understand the complex behaviors that can\nemerge in said domains instead? We introduce surrogate-assisted phenotypic\nniching, a quality diversity algorithm which allows to discover a large,\ndiverse set of behaviors by using computationally expensive phenotypic\nfeatures. In this work we discover the types of air flow in a 2D fluid dynamics\noptimization problem. A fast GPU-based fluid dynamics solver is used in\nconjunction with surrogate models to accurately predict fluid characteristics\nfrom the shapes that produce the air flow. We show that these features can be\nmodeled in a data-driven way while sampling to improve performance, rather than\nexplicitly sampling to improve feature models. Our method can reduce the need\nto run an infeasibly large set of simulations while still being able to design\na large diversity of air flows and the shapes that cause them. Discovering\ndiversity of behaviors helps engineers to better understand expensive domains\nand their solutions.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:45:28 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hagg", "Alexander", ""], ["Wilde", "Dominik", ""], ["Asteroth", "Alexander", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04311", "submitter": "Sasanka Sekhar Chanda", "authors": "Sasanka Sekhar Chanda and Sai Yayavaram", "title": "Overcoming Complexity Catastrophe: An Algorithm for Beneficial\n  Far-Reaching Adaptation under High Complexity", "comments": "10 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his seminal work with NK algorithms, Kauffman noted that fitness outcomes\nfrom algorithms navigating an NK landscape show a sharp decline at high\ncomplexity arising from pervasive interdependence among problem dimensions.\nThis phenomenon - where complexity effects dominate (Darwinian) adaptation\nefforts - is called complexity catastrophe. We present an algorithm -\nincremental change taking turns (ICTT) - that finds distant configurations\nhaving fitness superior to that reported in extant research, under high\ncomplexity. Thus, complexity catastrophe is not inevitable: a series of\nincremental changes can lead to excellent outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:46:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chanda", "Sasanka Sekhar", ""], ["Yayavaram", "Sai", ""]]}, {"id": "2105.04480", "submitter": "Diederick Vermetten", "authors": "Diederick Vermetten, Anna V. Kononova, Fabio Caraffini, Hao Wang,\n  Thomas B\\\"ack", "title": "Is there Anisotropy in Structural Bias?", "comments": null, "journal-ref": null, "doi": "10.1145/3449726.3463218", "report-no": null, "categories": "stat.ME cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural Bias (SB) is an important type of algorithmic deficiency within\niterative optimisation heuristics. However, methods for detecting structural\nbias have not yet fully matured, and recent studies have uncovered many\ninteresting questions. One of these is the question of how structural bias can\nbe related to anisotropy. Intuitively, an algorithm that is not isotropic would\nbe considered structurally biased. However, there have been cases where\nalgorithms appear to only show SB in some dimensions. As such, we investigate\nwhether these algorithms actually exhibit anisotropy, and how this impacts the\ndetection of SB. We find that anisotropy is very rare, and even in cases where\nit is present, there are clear tests for SB which do not rely on any\nassumptions of isotropy, so we can safely expand the suite of SB tests to\nencompass these kinds of deficiencies not found by the original tests.\n  We propose several additional testing procedures for SB detection and aim to\nmotivate further research into the creation of a robust portfolio of tests.\nThis is crucial since no single test will be able to work effectively with all\ntypes of SB we identify.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:20:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vermetten", "Diederick", ""], ["Kononova", "Anna V.", ""], ["Caraffini", "Fabio", ""], ["Wang", "Hao", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04693", "submitter": "Fabio Caraffini PhD", "authors": "Bas van Stein, Fabio Caraffini and Anna V. Kononova", "title": "Emergence of Structural Bias in Differential Evolution", "comments": null, "journal-ref": null, "doi": "10.1145/3449726.3463223", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heuristic optimisation algorithms are in high demand due to the overwhelming\namount of complex optimisation problems that need to be solved. The complexity\nof these problems is well beyond the boundaries of applicability of exact\noptimisation algorithms and therefore require modern heuristics to find\nfeasible solutions quickly. These heuristics and their effects are almost\nalways evaluated and explained by particular problem instances. In previous\nworks, it has been shown that many such algorithms show structural bias, by\neither being attracted to a certain region of the search space or by\nconsistently avoiding regions of the search space, on a special test function\ndesigned to ensure uniform 'exploration' of the domain. In this paper, we\nanalyse the emergence of such structural bias for Differential Evolution (DE)\nconfigurations and, specifically, the effect of different mutation, crossover\nand correction strategies. We also analyse the emergence of the structural bias\nduring the run-time of each algorithm. We conclude with recommendations of\nwhich configurations should be avoided in order to run DE unbiased.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 22:22:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["van Stein", "Bas", ""], ["Caraffini", "Fabio", ""], ["Kononova", "Anna V.", ""]]}, {"id": "2105.04916", "submitter": "Yanqi Chen", "authors": "Yanqi Chen, Zhaofei Yu, Wei Fang, Tiejun Huang and Yonghong Tian", "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring", "comments": "9 pages, 7 figures, 4 tables. To appear in the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:05:53 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:35:21 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 08:38:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chen", "Yanqi", ""], ["Yu", "Zhaofei", ""], ["Fang", "Wei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "2105.04934", "submitter": "Guanqiang Gao", "authors": "Guanqiang Gao, Bin Xin, Yi Mei, Shuxin Ding, and Juan Li", "title": "A Hybrid Decomposition-based Multi-objective Evolutionary Algorithm for\n  the Multi-Point Dynamic Aggregation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An emerging optimisation problem from the real-world applications, named the\nmulti-point dynamic aggregation (MPDA) problem, has become one of the active\nresearch topics of the multi-robot system. This paper focuses on a\nmulti-objective MPDA problem which is to design an execution plan of the robots\nto minimise the number of robots and the maximal completion time of all the\ntasks. The strongly-coupled relationships among robots and tasks, the\nredundancy of the MPDA encoding, and the variable-size decision space of the\nMO-MPDA problem posed extra challenges for addressing the problem effectively.\nTo address the above issues, we develop a hybrid decomposition-based\nmulti-objective evolutionary algorithm (HDMOEA) using $ \\varepsilon\n$-constraint method. It selects the maximal completion time of all tasks as the\nmain objective, and converted the other objective into constraints. HDMOEA\ndecomposes a MO-MPDA problem into a series of scalar constrained optimization\nsubproblems by assigning each subproblem with an upper bound robot number. All\nthe subproblems are optimized simultaneously with the transferring knowledge\nfrom other subproblems. Besides, we develop a hybrid population initialisation\nmechanism to enhance the quality of initial solutions, and a reproduction\nmechanism to transmit effective information and tackle the encoding redundancy.\nExperimental results show that the proposed HDMOEA method significantly\noutperforms the state-of-the-art methods in terms of several most-used metrics.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 10:53:16 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Gao", "Guanqiang", ""], ["Xin", "Bin", ""], ["Mei", "Yi", ""], ["Ding", "Shuxin", ""], ["Li", "Juan", ""]]}, {"id": "2105.05212", "submitter": "Abdesslem Layeb", "authors": "Abdesslem Layeb", "title": "Two novel feature selection algorithms based on crowding distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two novel algorithms for features selection are proposed. The\nfirst one is a filter method while the second is wrapper method. Both the\nproposed algorithms use the crowding distance used in the multiobjective\noptimization as a metric in order to sort the features. The less crowded\nfeatures have great effects on the target attribute (class). The experimental\nresults have shown the effectiveness and the robustness of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:27:56 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:19:50 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 15:16:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Layeb", "Abdesslem", ""]]}, {"id": "2105.05530", "submitter": "Wenshuo Li", "authors": "Wenshuo Li, Hanting Chen, Mingqiang Huang, Xinghao Chen, Chunjing Xu,\n  Yunhe Wang", "title": "Winograd Algorithm for AdderNet", "comments": "9 pages, accepted by ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adder neural network (AdderNet) is a new kind of deep model that replaces the\noriginal massive multiplications in convolutions by additions while preserving\nthe high performance. Since the hardware complexity of additions is much lower\nthan that of multiplications, the overall energy consumption is thus reduced\nsignificantly. To further optimize the hardware overhead of using AdderNet,\nthis paper studies the winograd algorithm, which is a widely used fast\nalgorithm for accelerating convolution and saving the computational costs.\nUnfortunately, the conventional Winograd algorithm cannot be directly applied\nto AdderNets since the distributive law in multiplication is not valid for the\nl1-norm. Therefore, we replace the element-wise multiplication in the Winograd\nequation by additions and then develop a new set of transform matrixes that can\nenhance the representation ability of output features to maintain the\nperformance. Moreover, we propose the l2-to-l1 training strategy to mitigate\nthe negative impacts caused by formal inconsistency. Experimental results on\nboth FPGA and benchmarks show that the new method can further reduce the energy\nconsumption without affecting the accuracy of the original AdderNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:13:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Wenshuo", ""], ["Chen", "Hanting", ""], ["Huang", "Mingqiang", ""], ["Chen", "Xinghao", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2105.05712", "submitter": "Shradha Agrawal", "authors": "Shradha Agrawal, Shankar Venkitachalam, Dhanya Raghu, Deepak Pai", "title": "Directional GAN: A Novel Conditioning Strategy for Generative Networks", "comments": "Accepted to AICC workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image content is a predominant factor in marketing campaigns, websites and\nbanners. Today, marketers and designers spend considerable time and money in\ngenerating such professional quality content. We take a step towards\nsimplifying this process using Generative Adversarial Networks (GANs). We\npropose a simple and novel conditioning strategy which allows generation of\nimages conditioned on given semantic attributes using a generator trained for\nan unconditional image generation task. Our approach is based on modifying\nlatent vectors, using directional vectors of relevant semantic attributes in\nlatent space. Our method is designed to work with both discrete (binary and\nmulti-class) and continuous image attributes. We show the applicability of our\nproposed approach, named Directional GAN, on multiple public datasets, with an\naverage accuracy of 86.4% across different attributes.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:02:41 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 22:04:31 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Agrawal", "Shradha", ""], ["Venkitachalam", "Shankar", ""], ["Raghu", "Dhanya", ""], ["Pai", "Deepak", ""]]}, {"id": "2105.05757", "submitter": "Thomas Goerttler", "authors": "Thomas Goerttler and Klaus Obermayer", "title": "Exploring the Similarity of Representations in Model-Agnostic\n  Meta-Learning", "comments": "Learning to Learn workshop at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In past years model-agnostic meta-learning (MAML) has been one of the most\npromising approaches in meta-learning. It can be applied to different kinds of\nproblems, e.g., reinforcement learning, but also shows good results on few-shot\nlearning tasks. Besides their tremendous success in these tasks, it has still\nnot been fully revealed yet, why it works so well. Recent work proposes that\nMAML rather reuses features than rapidly learns. In this paper, we want to\ninspire a deeper understanding of this question by analyzing MAML's\nrepresentation. We apply representation similarity analysis (RSA), a\nwell-established method in neuroscience, to the few-shot learning instantiation\nof MAML. Although some part of our analysis supports their general results that\nfeature reuse is predominant, we also reveal arguments against their\nconclusion. The similarity-increase of layers closer to the input layers arises\nfrom the learning task itself and not from the model. In addition, the\nrepresentations after inner gradient steps make a broader change to the\nrepresentation than the changes during meta-training.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 16:20:40 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Goerttler", "Thomas", ""], ["Obermayer", "Klaus", ""]]}, {"id": "2105.05911", "submitter": "Christopher Morris", "authors": "Christopher Morris, Matthias Fey, Nils M. Kriege", "title": "The Power of the Weisfeiler-Leman Algorithm for Machine Learning with\n  Graphs", "comments": "Accepted at IJCAI 2021 (survey track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, algorithms and neural architectures based on the\nWeisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism\nproblem, emerged as a powerful tool for (supervised) machine learning with\ngraphs and relational data. Here, we give a comprehensive overview of the\nalgorithm's use in a machine learning setting. We discuss the theoretical\nbackground, show how to use it for supervised graph- and node classification,\ndiscuss recent extensions, and its connection to neural architectures.\nMoreover, we give an overview of current applications and future directions to\nstimulate research.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:05:18 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:04:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Morris", "Christopher", ""], ["Fey", "Matthias", ""], ["Kriege", "Nils M.", ""]]}, {"id": "2105.05916", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yue Bai, Yun Fu", "title": "Dynamical Isometry: The Missing Ingredient for Neural Network Pruning", "comments": "8 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works [40, 24] observed an interesting phenomenon in neural\nnetwork pruning: A larger finetuning learning rate can improve the final\nperformance significantly. Unfortunately, the reason behind it remains elusive\nup to date. This paper is meant to explain it through the lens of dynamical\nisometry [42]. Specifically, we examine neural network pruning from an unusual\nperspective: pruning as initialization for finetuning, and ask whether the\ninherited weights serve as a good initialization for the finetuning? The\ninsights from dynamical isometry suggest a negative answer. Despite its\ncritical role, this issue has not been well-recognized by the community so far.\nIn this paper, we will show the understanding of this problem is very important\n-- on top of explaining the aforementioned mystery about the larger finetuning\nrate, it also unveils the mystery about the value of pruning [5, 30]. Besides a\nclearer theoretical understanding of pruning, resolving the problem can also\nbring us considerable performance benefits in practice.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 19:20:09 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Bai", "Yue", ""], ["Fu", "Yun", ""]]}, {"id": "2105.06109", "submitter": "Kishor Datta Gupta", "authors": "Kishor Datta Gupta and Dipankar Dasgupta", "title": "Negative Selection Algorithm Research and Applications in the last\n  decade: A Review", "comments": "Dataset and code: https://github.com/kishordgupta/OCC_DATASET and\n  https://github.com/kishordgupta/pyod", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Negative selection Algorithm (NSA) is one of the important methods in the\nfield of Immunological Computation (or Artificial Immune Systems). Over the\nyears, some progress was made which turns this algorithm (NSA) into an\nefficient approach to solve problems in different domain. This review takes\ninto account these signs of progress during the last decade and categorizes\nthose based on different characteristics and performances. Our study shows that\nNSA's evolution can be labeled in four ways highlighting the most notable NSA\nvariations and their limitations in different application domains. We also\npresent alternative approaches to NSA for comparison and analysis. It is\nevident that NSA performs better for nonlinear representation than most of the\nother methods, and it can outperform neural-based models in computation time.\nWe summarize NSA's development and highlight challenges in NSA research in\ncomparison with other similar models.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 06:58:03 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Gupta", "Kishor Datta", ""], ["Dasgupta", "Dipankar", ""]]}, {"id": "2105.06168", "submitter": "Mansura Habiba Miss", "authors": "Mehrdad Maleki and Mansura Habiba and Barak A. Pearlmutter", "title": "HeunNet: Extending ResNet using Heun's Methods", "comments": "Irish Signals & Systems Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an analogy between the ResNet (Residual Network) architecture for\ndeep neural networks and an Euler solver for an ODE. The transformation\nperformed by each layer resembles an Euler step in solving an ODE. We consider\nthe Heun Method, which involves a single predictor-corrector cycle, and\ncomplete the analogy, building a predictor-corrector variant of ResNet, which\nwe call a HeunNet. Just as Heun's method is more accurate than Euler's,\nexperiments show that HeunNet achieves high accuracy with low computational\n(both training and test) time compared to both vanilla recurrent neural\nnetworks and other ResNet variants.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 09:55:26 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 14:50:53 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Maleki", "Mehrdad", ""], ["Habiba", "Mansura", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "2105.06381", "submitter": "Yongxin Liu", "authors": "Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song", "title": "Class-Incremental Learning for Wireless Device Identification in IoT", "comments": "Accepted for publication by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) has been utilized pervasively in the Internet of Things\n(IoT). One typical application of DL in IoT is device identification from\nwireless signals, namely Non-cryptographic Device Identification (NDI).\nHowever, learning components in NDI systems have to evolve to adapt to\noperational variations, such a paradigm is termed as Incremental Learning (IL).\nVarious IL algorithms have been proposed and many of them require dedicated\nspace to store the increasing amount of historical data, and therefore, they\nare not suitable for IoT or mobile applications. However, conventional IL\nschemes can not provide satisfying performance when historical data are not\navailable. In this paper, we address the IL problem in NDI from a new\nperspective, firstly, we provide a new metric to measure the degree of\ntopological maturity of DNN models from the degree of conflict of\nclass-specific fingerprints. We discover that an important cause for\nperformance degradation in IL enabled NDI is owing to the conflict of devices'\nfingerprints. Second, we also show that the conventional IL schemes can lead to\nlow topological maturity of DNN models in NDI systems. Thirdly, we propose a\nnew Channel Separation Enabled Incremental Learning (CSIL) scheme without using\nhistorical data, in which our strategy can automatically separate devices'\nfingerprints in different learning stages and avoid potential conflict.\nFinally, We evaluated the effectiveness of the proposed framework using real\ndata from ADS-B (Automatic Dependent Surveillance-Broadcast), an application of\nIoT in aviation. The proposed framework has the potential to be applied to\naccurate identification of IoT devices in a variety of IoT applications and\nservices. Data and code available at IEEE Dataport (DOI: 10.21227/1bxc-ke87)\nand \\url{https://github.com/pcwhy/CSIL}}\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:11:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Liu", "Yongxin", ""], ["Wang", "Jian", ""], ["Li", "Jianqiang", ""], ["Niu", "Shuteng", ""], ["Song", "Houbing", ""]]}, {"id": "2105.06386", "submitter": "Brian Hutchinson", "authors": "Alexis Ayala, Christopher Drazic, Brian Hutchinson, Ben Kravitz,\n  Claudia Tebaldi", "title": "Loosely Conditioned Emulation of Global Climate Models With Generative\n  Adversarial Networks", "comments": "Presented at NeurIPS 2020 Workshop Tackling Climate Change with\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG cs.NE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate models encapsulate our best understanding of the Earth system,\nallowing research to be conducted on its future under alternative assumptions\nof how human-driven climate forces are going to evolve. An important\napplication of climate models is to provide metrics of mean and extreme climate\nchanges, particularly under these alternative future scenarios, as these\nquantities drive the impacts of climate on society and natural systems. Because\nof the need to explore a wide range of alternative scenarios and other sources\nof uncertainties in a computationally efficient manner, climate models can only\ntake us so far, as they require significant computational resources, especially\nwhen attempting to characterize extreme events, which are rare and thus demand\nlong and numerous simulations in order to accurately represent their changing\nstatistics. Here we use deep learning in a proof of concept that lays the\nfoundation for emulating global climate model output for different scenarios.\nWe train two \"loosely conditioned\" Generative Adversarial Networks (GANs) that\nemulate daily precipitation output from a fully coupled Earth system model: one\nGAN modeling Fall-Winter behavior and the other Spring-Summer. Our GANs are\ntrained to produce spatiotemporal samples: 32 days of precipitation over a\n64x128 regular grid discretizing the globe. We evaluate the generator with a\nset of related performance metrics based upon KL divergence, and find the\ngenerated samples to be nearly as well matched to the test data as the\nvalidation data is to test. We also find the generated samples to accurately\nestimate the mean number of dry days and mean longest dry spell in the 32 day\nsamples. Our trained GANs can rapidly generate numerous realizations at a\nvastly reduced computational expense, compared to large ensembles of climate\nmodels, which greatly aids in estimating the statistics of extreme events.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 02:10:08 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ayala", "Alexis", ""], ["Drazic", "Christopher", ""], ["Hutchinson", "Brian", ""], ["Kravitz", "Ben", ""], ["Tebaldi", "Claudia", ""]]}, {"id": "2105.06562", "submitter": "Chethan M Parameshwara", "authors": "Chethan M. Parameshwara, Simin Li, Cornelia Ferm\\\"uller, Nitin J.\n  Sanket, Matthew S. Evanusa, Yiannis Aloimonos", "title": "SpikeMS: Deep Spiking Neural Network for Motion Segmentation", "comments": "7 pages, 6 figures, 3 tables, Under review IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking Neural Networks (SNN) are the so-called third generation of neural\nnetworks which attempt to more closely match the functioning of the biological\nbrain. They inherently encode temporal data, allowing for training with less\nenergy usage and can be extremely energy efficient when coded on neuromorphic\nhardware. In addition, they are well suited for tasks involving event-based\nsensors, which match the event-based nature of the SNN. However, SNNs have not\nbeen as effectively applied to real-world, large-scale tasks as standard\nArtificial Neural Networks (ANNs) due to the algorithmic and training\ncomplexity. To exacerbate the situation further, the input representation is\nunconventional and requires careful analysis and deep understanding. In this\npaper, we propose \\textit{SpikeMS}, the first deep encoder-decoder SNN\narchitecture for the real-world large-scale problem of motion segmentation\nusing the event-based DVS camera as input. To accomplish this, we introduce a\nnovel spatio-temporal loss formulation that includes both spike counts and\nclassification labels in conjunction with the use of new techniques for SNN\nbackpropagation. In addition, we show that \\textit{SpikeMS} is capable of\n\\textit{incremental predictions}, or predictions from smaller amounts of test\ndata than it is trained on. This is invaluable for providing outputs even with\npartial input data for low-latency applications and those requiring fast\npredictions. We evaluated \\textit{SpikeMS} on challenging synthetic and\nreal-world sequences from EV-IMO, EED and MOD datasets and achieving results on\na par with a comparable ANN method, but using potentially 50 times less power.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 21:34:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Parameshwara", "Chethan M.", ""], ["Li", "Simin", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Sanket", "Nitin J.", ""], ["Evanusa", "Matthew S.", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2105.06727", "submitter": "Gesina Schwalbe", "authors": "Gesina Schwalbe", "title": "Verification of Size Invariance in DNN Activations using Concept\n  Embeddings", "comments": "12 pages, 7 figures; Camera-ready version for AIAI2021", "journal-ref": null, "doi": "10.1007/978-3-030-79150-6_30", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The benefits of deep neural networks (DNNs) have become of interest for\nsafety critical applications like medical ones or automated driving. Here,\nhowever, quantitative insights into the DNN inner representations are\nmandatory. One approach to this is concept analysis, which aims to establish a\nmapping between the internal representation of a DNN and intuitive semantic\nconcepts. Such can be sub-objects like human body parts that are valuable for\nvalidation of pedestrian detection. To our knowledge, concept analysis has not\nyet been applied to large object detectors, specifically not for sub-parts.\nTherefore, this work first suggests a substantially improved version of the\nNet2Vec approach (arXiv:1801.03454) for post-hoc segmentation of sub-objects.\nIts practical applicability is then demonstrated on a new concept dataset by\ntwo exemplary assessments of three standard networks, including the larger Mask\nR-CNN model (arXiv:1703.06870): (1) the consistency of body part similarity,\nand (2) the invariance of internal representations of body parts with respect\nto the size in pixels of the depicted person. The findings show that the\nrepresentation of body parts is mostly size invariant, which may suggest an\nearly intelligent fusion of information in different size categories.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:25:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Schwalbe", "Gesina", ""]]}, {"id": "2105.06757", "submitter": "Rick Boks", "authors": "Rick Boks, Anna V. Kononova, Hao Wang", "title": "Quantifying the Impact of Boundary Constraint Handling Methods on\n  Differential Evolution", "comments": "9 pages, 2 figures. To be published in the 2021 Genetic and\n  Evolutionary Computation Conference Companion", "journal-ref": null, "doi": "10.1145/3449726.3463214", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint handling is one of the most influential aspects of applying\nmetaheuristics to real-world applications, which can hamper the search progress\nif treated improperly. In this work, we focus on a particular case - the box\nconstraints, for which many boundary constraint handling methods (BCHMs) have\nbeen proposed. We call for the necessity of studying the impact of BCHMs on\nmetaheuristics' performance and behavior, which receives seemingly little\nattention in the field. We target quantifying such impacts through systematic\nbenchmarking by investigating 28 major variants of Differential Evolution (DE)\ntaken from the modular DE framework (by combining different mutation and\ncrossover operators) and $13$ commonly applied BCHMs, resulting in $28 \\times\n13 = 364$ algorithm instances after pairing DE variants with BCHMs. After\nexecuting the algorithm instances on the well-known BBOB/COCO problem set, we\nanalyze the best-reached objective function value (performance-wise) and the\npercentage of repaired solutions (behavioral) using statistical ranking methods\nfor each combination of mutation, crossover, and BBOB function group. Our\nresults clearly show that the choice of BCHMs substantially affects the\nempirical performance as well as the number of generated infeasible solutions,\nwhich allows us to provide general guidelines for selecting an appropriate BCHM\nfor a given scenario.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 10:36:10 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Boks", "Rick", ""], ["Kononova", "Anna V.", ""], ["Wang", "Hao", ""]]}, {"id": "2105.06824", "submitter": "KongFatt Wong-Lin", "authors": "James Fitzgerald and KongFatt Wong-Lin", "title": "Multi-Objective Optimisation of Cortical Spiking Neural Networks With\n  Genetic Algorithms", "comments": "In: 32nd Irish Signals and Systems Conference (ISSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking neural networks (SNNs) communicate through the all-or-none spiking\nactivity of neurons. However, fitting the large number of SNN model parameters\nto observed neural activity patterns, for example, in biological experiments,\nremains a challenge. Previous work using genetic algorithm (GA) optimisation on\na specific efficient SNN model, using the Izhikevich neuronal model, was\nlimited to a single parameter and objective. This work applied a version of GA,\ncalled non-dominated sorting GA (NSGA-III), to demonstrate the feasibility of\nperforming multi-objective optimisation on the same SNN, focusing on searching\nnetwork connectivity parameters to achieve target firing rates of excitatory\nand inhibitory neuronal types, including across different network connectivity\nsparsity. We showed that NSGA-III could readily optimise for various firing\nrates. Notably, when the excitatory neural firing rates were higher than or\nequal to that of inhibitory neurons, the errors were small. Moreover, when\nconnectivity sparsity was considered as a parameter to be optimised, the\noptimal solutions required sparse network connectivity. We also found that for\nexcitatory neural firing rates lower than that of inhibitory neurons, the\nerrors were generally larger. Overall, we have successfully demonstrated the\nfeasibility of implementing multi-objective GA optimisation on network\nparameters of recurrent and sparse SNN.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:35:39 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Fitzgerald", "James", ""], ["Wong-Lin", "KongFatt", ""]]}, {"id": "2105.06943", "submitter": "Zhehui Wang", "authors": "Zhehui Wang, Xiaozhe Gu, Rick Goh, Joey Tianyi Zhou, Tao Luo", "title": "Efficient Spiking Neural Networks with Radix Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) have advantages in latency and energy\nefficiency over traditional artificial neural networks (ANNs) due to its\nevent-driven computation mechanism and replacement of energy-consuming weight\nmultiplications with additions. However, in order to reach accuracy of its ANN\ncounterpart, it usually requires long spike trains to ensure the accuracy.\nTraditionally, a spike train needs around one thousand time steps to approach\nsimilar accuracy as its ANN counterpart. This offsets the computation\nefficiency brought by SNNs because longer spike trains mean a larger number of\noperations and longer latency. In this paper, we propose a radix encoded SNN\nwith ultra-short spike trains. In the new model, the spike train takes less\nthan ten time steps. Experiments show that our method demonstrates 25X speedup\nand 1.1% increment on accuracy, compared with the state-of-the-art work on\nVGG-16 network architecture and CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:35:53 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Zhehui", ""], ["Gu", "Xiaozhe", ""], ["Goh", "Rick", ""], ["Zhou", "Joey Tianyi", ""], ["Luo", "Tao", ""]]}, {"id": "2105.07202", "submitter": "Berkan H\\\"oke", "authors": "Burak Ta\\u{g}tekin, Berkan H\\\"oke, Mert Kutay Sezer, Mahiye\n  Uluya\\u{g}mur \\\"Ozt\\\"urk", "title": "FOGA: Flag Optimization with Genetic Algorithm", "comments": "6 pages, 7 figures, to be published in IEEE INISTA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, program autotuning has become very popular especially in embedded\nsystems, when we have limited resources such as computing power and memory\nwhere these systems run generally time-critical applications. Compiler\noptimization space gradually expands with the renewed compiler options and\ninclusion of new architectures. These advancements bring autotuning even more\nimportant position. In this paper, we introduced Flag Optimization with Genetic\nAlgorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has\ntwo main advantages over the other autotuning approaches: the first one is the\nhyperparameter tuning of the genetic algorithm (GA), the second one is the\nmaximum iteration parameter to stop when no further improvement occurs. We\ndemonstrated remarkable speedup in the execution time of C++ source codes with\nthe help of optimization flags provided by FOGA when compared to the state of\nthe art framework OpenTuner.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 11:29:12 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ta\u011ftekin", "Burak", ""], ["H\u00f6ke", "Berkan", ""], ["Sezer", "Mert Kutay", ""], ["\u00d6zt\u00fcrk", "Mahiye Uluya\u011fmur", ""]]}, {"id": "2105.07743", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios", "title": "Universal Regular Conditional Distributions", "comments": "Keywords: Universal Regular Conditional Distributions, Geometric Deep\n  Learning, Measure-Valued Neural Networks, Conditional Expectation,\n  Uncertainty Quantification. Additional Information: 27 Pages + 22 Page\n  Appendix, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.MG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for approximating regular conditional\ndistributions (RCDs). Our approximations of these RCDs are implemented by a new\nclass of geometric deep learning models with inputs in $\\mathbb{R}^d$ and\noutputs in the Wasserstein-$1$ space $\\mathcal{P}_1(\\mathbb{R}^D)$. We find\nthat the models built using our framework can approximate any continuous\nfunctions from $\\mathbb{R}^d$ to $\\mathcal{P}_1(\\mathbb{R}^D)$ uniformly on\ncompacts, and quantitative rates are obtained. We identify two methods for\navoiding the \"curse of dimensionality\"; i.e.: the number of parameters\ndetermining the approximating neural network depends only polynomially on the\ninvolved dimension and the approximation error. The first solution describes\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated on any compact subset of $\\mathbb{R}^d$. Conversely,\nthe second approach describes sets in $\\mathbb{R}^d$, on which any function in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ can be efficiently approximated.\nOur framework is used to obtain an affirmative answer to the open conjecture of\nBishop (1994); namely: mixture density networks are universal regular\nconditional distributions. The predictive performance of the proposed models is\nevaluated against comparable learning models on various probabilistic\npredictions tasks in the context of ELMs, model uncertainty, and\nheteroscedastic regression. All the results are obtained for more general input\nand output spaces and thus apply to geometric deep learning contexts.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:34:09 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 17:51:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kratsios", "Anastasis", ""]]}, {"id": "2105.07776", "submitter": "Julien Girard-Satabin", "authors": "Julien Girard-Satabin (LIST, TAU), Aymeric Varasse (LIST), Marc\n  Schoenauer (TAU), Guillaume Charpiat (TAU), Zakaria Chihani (LIST)", "title": "DISCO Verification: Division of Input Space into COnvex polytopes for\n  neural network verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive results of modern neural networks partly come from their non\nlinear behaviour. Unfortunately, this property makes it very difficult to apply\nformal verification tools, even if we restrict ourselves to networks with a\npiecewise linear structure. However, such networks yields subregions that are\nlinear and thus simpler to analyse independently. In this paper, we propose a\nmethod to simplify the verification problem by operating a partitionning into\nmultiple linear subproblems. To evaluate the feasibility of such an approach,\nwe perform an empirical analysis of neural networks to estimate the number of\nlinear regions, and compare them to the bounds currently known. We also present\nthe impact of a technique aiming at reducing the number of linear regions\nduring training.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:40:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Girard-Satabin", "Julien", "", "LIST, TAU"], ["Varasse", "Aymeric", "", "LIST"], ["Schoenauer", "Marc", "", "TAU"], ["Charpiat", "Guillaume", "", "TAU"], ["Chihani", "Zakaria", "", "LIST"]]}, {"id": "2105.07789", "submitter": "Lukas Drees", "authors": "Lukas Drees, Laura Verena Junker-Frohn, Jana Kierdorf, Ribana Roscher", "title": "Temporal Prediction and Evaluation of Brassica Growth in the Field using\n  Conditional Generative Adversarial Networks", "comments": "38 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Farmers frequently assess plant growth and performance as basis for making\ndecisions when to take action in the field, such as fertilization, weed\ncontrol, or harvesting. The prediction of plant growth is a major challenge, as\nit is affected by numerous and highly variable environmental factors. This\npaper proposes a novel monitoring approach that comprises high-throughput\nimaging sensor measurements and their automatic analysis to predict future\nplant growth. Our approach's core is a novel machine learning-based growth\nmodel based on conditional generative adversarial networks, which is able to\npredict the future appearance of individual plants. In experiments with RGB\ntime-series images of laboratory-grown Arabidopsis thaliana and field-grown\ncauliflower plants, we show that our approach produces realistic, reliable, and\nreasonable images of future growth stages. The automatic interpretation of the\ngenerated images through neural network-based instance segmentation allows the\nderivation of various phenotypic traits that describe plant growth.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:00:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Drees", "Lukas", ""], ["Junker-Frohn", "Laura Verena", ""], ["Kierdorf", "Jana", ""], ["Roscher", "Ribana", ""]]}, {"id": "2105.07917", "submitter": "Giulia Cisotto", "authors": "Alberto Zancanaro, Giulia Cisotto, Jo\\~ao Ruivo Paulo, Gabriel Pires,\n  and Urbano J. Nunes", "title": "CNN-based Approaches For Cross-Subject Classification in Motor Imagery:\n  From The State-of-The-Art to DynamicNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery (MI)-based brain-computer interface (BCI) systems are being\nincreasingly employed to provide alternative means of communication and control\nfor people suffering from neuro-motor impairments, with a special effort to\nbring these systems out of the controlled lab environments. Hence, accurately\nclassifying MI from brain signals, e.g., from electroencephalography (EEG), is\nessential to obtain reliable BCI systems. However, MI classification is still a\nchallenging task, because the signals are characterized by poor SNR, high\nintra-subject and cross-subject variability. Deep learning approaches have\nstarted to emerge as valid alternatives to standard machine learning\ntechniques, e.g., filter bank common spatial pattern (FBCSP), to extract\nsubject-independent features and to increase the cross-subject classification\nperformance of MI BCI systems. In this paper, we first present a review of the\nmost recent studies using deep learning for MI classification, with particular\nattention to their cross-subject performance. Second, we propose DynamicNet, a\nPython-based tool for quick and flexible implementations of deep learning\nmodels based on convolutional neural networks. We show-case the potentiality of\nDynamicNet by implementing EEGNet, a well-established architecture for\neffective EEG classification. Finally, we compare its performance with FBCSP in\na 4-class MI classification over public datasets. To explore its cross-subject\nclassification ability, we applied three different cross-validation schemes.\nFrom our results, we demonstrate that DynamicNet-implemented EEGNet outperforms\nFBCSP by about 25%, with a statistically significant difference when\ncross-subject validation schemes are applied.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:57:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zancanaro", "Alberto", ""], ["Cisotto", "Giulia", ""], ["Paulo", "Jo\u00e3o Ruivo", ""], ["Pires", "Gabriel", ""], ["Nunes", "Urbano J.", ""]]}, {"id": "2105.07957", "submitter": "Daniel Tanneberg", "authors": "Daniel Tanneberg, Elmar Rueckert, Jan Peters", "title": "Evolutionary Training and Abstraction Yields Algorithmic Generalization\n  of Neural Computers", "comments": "Nature Machine Intelligence", "journal-ref": "Nature Machine Intelligence, Vol. 2, December 2020, 753-763", "doi": "10.1038/s42256-020-00255-1", "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key feature of intelligent behaviour is the ability to learn abstract\nstrategies that scale and transfer to unfamiliar problems. An abstract strategy\nsolves every sample from a problem class, no matter its representation or\ncomplexity -- like algorithms in computer science. Neural networks are powerful\nmodels for processing sensory data, discovering hidden patterns, and learning\ncomplex functions, but they struggle to learn such iterative, sequential or\nhierarchical algorithmic strategies. Extending neural networks with external\nmemories has increased their capacities in learning such strategies, but they\nare still prone to data variations, struggle to learn scalable and transferable\nsolutions, and require massive training data. We present the Neural Harvard\nComputer (NHC), a memory-augmented network based architecture, that employs\nabstraction by decoupling algorithmic operations from data manipulations,\nrealized by splitting the information flow and separated modules. This\nabstraction mechanism and evolutionary training enable the learning of robust\nand scalable algorithmic solutions. On a diverse set of 11 algorithms with\nvarying complexities, we show that the NHC reliably learns algorithmic\nsolutions with strong generalization and abstraction: perfect generalization\nand scaling to arbitrary task configurations and complexities far beyond seen\nduring training, and being independent of the data representation and the task\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:37:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tanneberg", "Daniel", ""], ["Rueckert", "Elmar", ""], ["Peters", "Jan", ""]]}, {"id": "2105.07960", "submitter": "Martin Zaefferer", "authors": "J\\\"org Stork, Martin Zaefferer, Nils Eisler, Patrick Tichelmann,\n  Thomas Bartz-Beielstein, A. E. Eiben", "title": "Behavior-based Neuroevolutionary Training in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": "10.1145/3449726.3463171", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to their undisputed success in solving classical optimization\nproblems, neuroevolutionary and population-based algorithms have become an\nalternative to standard reinforcement learning methods. However, evolutionary\nmethods often lack the sample efficiency of standard value-based methods that\nleverage gathered state and value experience. If reinforcement learning for\nreal-world problems with significant resource cost is considered, sample\nefficiency is essential. The enhancement of evolutionary algorithms with\nexperience exploiting methods is thus desired and promises valuable insights.\nThis work presents a hybrid algorithm that combines topology-changing\nneuroevolutionary optimization with value-based reinforcement learning. We\nillustrate how the behavior of policies can be used to create distance and loss\nfunctions, which benefit from stored experiences and calculated state values.\nThey allow us to model behavior and perform a directed search in the behavior\nspace by gradient-free evolutionary algorithms and surrogate-based\noptimization. For this purpose, we consolidate different methods to generate\nand optimize agent policies, creating a diverse population. We exemplify the\nperformance of our algorithm on standard benchmarks and a purpose-built\nreal-world problem. Our results indicate that combining methods can enhance the\nsample efficiency and learning speed for evolutionary approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:40:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Stork", "J\u00f6rg", ""], ["Zaefferer", "Martin", ""], ["Eisler", "Nils", ""], ["Tichelmann", "Patrick", ""], ["Bartz-Beielstein", "Thomas", ""], ["Eiben", "A. E.", ""]]}, {"id": "2105.08111", "submitter": "Thomas Schumacher", "authors": "Thomas Schumacher", "title": "Livewired Neural Networks: Making Neurons That Fire Together Wire\n  Together", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Until recently, artificial neural networks were typically designed with a\nfixed network structure. Here, I argue that network structure is highly\nrelevant to function, and therefore neural networks should be livewired\n(Eagleman 2020): dynamically rewired to reflect relationships between higher\norder representations of the external environment identified by coincident\nactivations in individual neurons. I discuss how this approach may enable such\nnetworks to build compositional world models that operate on symbols and that\nachieve few-shot learning, capabilities thought by many to be critical to\nhuman-level cognition. Here, I also 1) discuss how such livewired neural\nnetworks maximize the information the environment provides to a model, 2)\nexplore evidence indicating that livewiring is implemented in the brain, guided\nby glial cells, 3) discuss how livewiring may give rise to the associative\nemergent behaviors of brains, and 4) suggest paths for future research using\nlivewired networks to understand and create human-like reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:45:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Schumacher", "Thomas", ""]]}, {"id": "2105.08470", "submitter": "Lorenz K. Muller", "authors": "Lorenz K. Muller", "title": "Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast\n  Neural Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks can enhance images taken with small mobile\ncamera sensors and excel at tasks like demoisaicing, denoising and\nsuper-resolution. However, for practical use on mobile devices these networks\noften require too many FLOPs and reducing the FLOPs of a convolution layer,\nalso reduces its parameter count. This is problematic in view of the recent\nfinding that heavily over-parameterized neural networks are often the ones that\ngeneralize best. In this paper we propose to use HyperNetworks to break the\nfixed ratio of FLOPs to parameters of standard convolutions. This allows us to\nexceed previous state-of-the-art architectures in SSIM and MS-SSIM on the\nZurich RAW- to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we\nfurther observe generalization curves consistent with 'double-descent' behavior\nat fixed FLOP-count, in the large image limit. Finally we demonstrate the same\ntechnique can be applied to an existing network (VDN) to reduce its\ncomputational cost while maintaining fidelity on the Smartphone Image Denoising\nDataset (SIDD). Code for key functions is given in the appendix.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:27:05 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Muller", "Lorenz K.", ""]]}, {"id": "2105.08675", "submitter": "Christoph Hertrich", "authors": "Vincent Froese, Christoph Hertrich, Rolf Niedermeier", "title": "The Computational Complexity of ReLU Network Training Parameterized by\n  Data Dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the computational complexity of training simple neural networks\nwith rectified linear units (ReLUs) has recently been a subject of intensive\nresearch. Closing gaps and complementing results from the literature, we\npresent several results on the parameterized complexity of training two-layer\nReLU networks with respect to various loss functions. After a brief discussion\nof other parameters, we focus on analyzing the influence of the dimension $d$\nof the training data on the computational complexity. We provide running time\nlower bounds in terms of W[1]-hardness for parameter $d$ and prove that known\nbrute-force strategies are essentially optimal (assuming the Exponential Time\nHypothesis). In comparison with previous work, our results hold for a broad(er)\nrange of loss functions, including $\\ell^p$-loss for all $p\\in[0,\\infty]$. In\nparticular, we extend a known polynomial-time algorithm for constant $d$ and\nconvex loss functions to a more general class of loss functions, matching our\nrunning time lower bounds also in these cases.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:05:26 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:32:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Froese", "Vincent", ""], ["Hertrich", "Christoph", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2105.08810", "submitter": "Nicolas Perez-Nieves", "authors": "Nicolas Perez-Nieves and Dan F.M. Goodman", "title": "Sparse Spiking Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in emulating Spiking Neural Networks (SNNs)\non neuromorphic computing devices due to their low energy consumption. Recent\nadvances have allowed training SNNs to a point where they start to compete with\ntraditional Artificial Neural Networks (ANNs) in terms of accuracy, while at\nthe same time being energy efficient when run on neuromorphic hardware.\nHowever, the process of training SNNs is still based on dense tensor operations\noriginally developed for ANNs which do not leverage the spatiotemporally sparse\nnature of SNNs. We present here the first sparse SNN backpropagation algorithm\nwhich achieves the same or better accuracy as current state of the art methods\nwhile being significantly faster and more memory efficient. We show the\neffectiveness of our method on real datasets of varying complexity\n(Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a\nspeedup in the backward pass of up to 70x, and 40% more memory efficient,\nwithout losing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:00:55 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Perez-Nieves", "Nicolas", ""], ["Goodman", "Dan F. M.", ""]]}, {"id": "2105.09821", "submitter": "Noor Awad", "authors": "Noor Awad, Neeratyoy Mallik, Frank Hutter", "title": "DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient\n  Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern machine learning algorithms crucially rely on several design decisions\nto achieve strong performance, making the problem of Hyperparameter\nOptimization (HPO) more important than ever. Here, we combine the advantages of\nthe popular bandit-based HPO method Hyperband (HB) and the evolutionary search\napproach of Differential Evolution (DE) to yield a new HPO method which we call\nDEHB. Comprehensive results on a very broad range of HPO problems, as well as a\nwide range of tabular benchmarks from neural architecture search, demonstrate\nthat DEHB achieves strong performance far more robustly than all previous HPO\nmethods we are aware of, especially for high-dimensional problems with discrete\ninput dimensions. For example, DEHB is up to 1000x faster than random search.\nIt is also efficient in computational time, conceptually simple and easy to\nimplement, positioning it well to become a new default HPO method.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:13:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Awad", "Noor", ""], ["Mallik", "Neeratyoy", ""], ["Hutter", "Frank", ""]]}, {"id": "2105.09909", "submitter": "Dipayan Das", "authors": "Dipayan Das, Saumik Bhattacharya, Umapada Pal, and Sukalpa Chanda", "title": "PLSM: A Parallelized Liquid State Machine for Unintentional Action\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reservoir Computing (RC) offers a viable option to deploy AI algorithms on\nlow-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired\nRC model that mimics the cortical microcircuits and uses spiking neural\nnetworks (SNN) that can be directly realized on neuromorphic hardware. In this\npaper, we present a novel Parallelized LSM (PLSM) architecture that\nincorporates spatio-temporal read-out layer and semantic constraints on model\noutput. To the best of our knowledge, such a formulation has been done for the\nfirst time in literature, and it offers a computationally lighter alternative\nto traditional deep-learning models. Additionally, we also present a\ncomprehensive algorithm for the implementation of parallelizable SNNs and LSMs\nthat are GPU-compatible. We implement the PLSM model to classify\nunintentional/accidental video clips, using the Oops dataset. From the\nexperimental results on detecting unintentional action in video, it can be\nobserved that our proposed model outperforms a self-supervised model and a\nfully supervised traditional deep learning model. All the implemented codes can\nbe found at our repository\nhttps://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:10:35 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Das", "Dipayan", ""], ["Bhattacharya", "Saumik", ""], ["Pal", "Umapada", ""], ["Chanda", "Sukalpa", ""]]}, {"id": "2105.10005", "submitter": "C.-H. Huck Yang", "authors": "C.-H. Huck Yang, Mohit Chhabra, Y.-C. Liu, Quan Kong, Tomoaki\n  Yoshinaga, Tomokazu Murakami", "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments", "comments": "Accepted to IEEE ICIP 2021", "journal-ref": "2021 IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:38:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:29:32 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 01:36:22 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 06:52:21 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Chhabra", "Mohit", ""], ["Liu", "Y. -C.", ""], ["Kong", "Quan", ""], ["Yoshinaga", "Tomoaki", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "2105.10190", "submitter": "Juan M Haut", "authors": "S.K. Roy, M.E. Paoletti, J.M. Haut, S.R. Dubey, P. Kar, A. Plaza, B.B.\n  Chaudhuri", "title": "AngularGrad: A New Optimization Technique for Angular Convergence of\n  Convolutional Neural Networks", "comments": "Submitted in IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are trained using stochastic gradient\ndescent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam)\noptimizer has become very popular due to its adaptive momentum, which tackles\nthe dying gradient problem of SGD. Nevertheless, existing optimizers are still\nunable to exploit the optimization curvature information efficiently. This\npaper proposes a new AngularGrad optimizer that considers the behavior of the\ndirection/angle of consecutive gradients. This is the first attempt in the\nliterature to exploit the gradient angular information apart from its\nmagnitude. The proposed AngularGrad generates a score to control the step size\nbased on the gradient angular information of previous iterations. Thus, the\noptimization steps become smoother as a more accurate step size of immediate\npast gradients is captured through the angular information. Two variants of\nAngularGrad are developed based on the use of Tangent or Cosine functions for\ncomputing the gradient angular information. Theoretically, AngularGrad exhibits\nthe same regret bound as Adam for convergence purposes. Nevertheless, extensive\nexperiments conducted on benchmark data sets against state-of-the-art methods\nreveal a superior performance of AngularGrad. The source code will be made\npublicly available at: https://github.com/mhaut/AngularGrad.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 08:00:53 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Roy", "S. K.", ""], ["Paoletti", "M. E.", ""], ["Haut", "J. M.", ""], ["Dubey", "S. R.", ""], ["Kar", "P.", ""], ["Plaza", "A.", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "2105.10317", "submitter": "David Mark Bossens", "authors": "David M. Bossens and Danesh Tarapore", "title": "On the use of feature-maps and parameter control for improved\n  quality-diversity meta-evolution", "comments": "extended version of GECCO'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Quality-Diversity (QD) algorithms, which evolve a behaviourally diverse\narchive of high-performing solutions, the behaviour space is a difficult design\nchoice that should be tailored to the target application. In QD meta-evolution,\none evolves a population of QD algorithms to optimise the behaviour space based\non an archive-level objective, the meta-fitness. This paper proposes an\nimproved meta-evolution system such that (i) the database used to rapidly\npopulate new archives is reformulated to prevent loss of quality-diversity;\n(ii) the linear transformation of base-features is generalised to a\nfeature-map, a function of the base-features parametrised by the meta-genotype;\nand (iii) the mutation rate of the QD algorithm and the number of generations\nper meta-generation are controlled dynamically. Experiments on an 8-joint\nplanar robot arm compare feature-maps (linear, non-linear, and\nfeature-selection), parameter control strategies (static, endogenous,\nreinforcement learning, and annealing), and traditional MAP-Elites variants,\nfor a total of 49 experimental conditions. Results reveal that non-linear and\nfeature-selection feature-maps yield a 15-fold and 3-fold improvement in\nmeta-fitness, respectively, over linear feature-maps. Reinforcement learning\nranks among top parameter control methods. Finally, our approach allows the\nrobot arm to recover a reach of over 80% for most damages and at least 60% for\nsevere damages.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:43:27 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Bossens", "David M.", ""], ["Tarapore", "Danesh", ""]]}, {"id": "2105.10325", "submitter": "Jana Kierdorf", "authors": "Jana Kierdorf, Immanuel Weber, Anna Kicherer, Laura Zabawa, Lukas\n  Drees, Ribana Roscher", "title": "Behind the leaves -- Estimation of occluded grapevine berries with\n  conditional generative adversarial networks", "comments": "45 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for accurate yield estimates for viticulture is becoming more\nimportant due to increasing competition in the wine market worldwide. One of\nthe most promising methods to estimate the harvest is berry counting, as it can\nbe approached non-destructively, and its process can be automated. In this\narticle, we present a method that addresses the challenge of occluded berries\nwith leaves to obtain a more accurate estimate of the number of berries that\nwill enable a better estimate of the harvest. We use generative adversarial\nnetworks, a deep learning-based approach that generates a likely scenario\nbehind the leaves exploiting learned patterns from images with non-occluded\nberries. Our experiments show that the estimate of the number of berries after\napplying our method is closer to the manually counted reference. In contrast to\napplying a factor to the berry count, our approach better adapts to local\nconditions by directly involving the appearance of the visible berries.\nFurthermore, we show that our approach can identify which areas in the image\nshould be changed by adding new berries without explicitly requiring\ninformation about hidden areas.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:57:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kierdorf", "Jana", ""], ["Weber", "Immanuel", ""], ["Kicherer", "Anna", ""], ["Zabawa", "Laura", ""], ["Drees", "Lukas", ""], ["Roscher", "Ribana", ""]]}, {"id": "2105.10335", "submitter": "Debasmit Das", "authors": "Debasmit Das, Yash Bhalgat and Fatih Porikli", "title": "Data-driven Weight Initialization with Sylvester Solvers", "comments": "Practical Machine Learning for Developing Countries Workshop,\n  International Conference on Learning Representations, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a data-driven scheme to initialize the parameters of\na deep neural network. This is in contrast to traditional approaches which\nrandomly initialize parameters by sampling from transformed standard\ndistributions. Such methods do not use the training data to produce a more\ninformed initialization. Our method uses a sequential layer-wise approach where\neach layer is initialized using its input activations. The initialization is\ncast as an optimization problem where we minimize a combination of encoding and\ndecoding losses of the input activations, which is further constrained by a\nuser-defined latent code. The optimization problem is then restructured into\nthe well-known Sylvester equation, which has fast and efficient gradient-free\nsolutions. Our data-driven method achieves a boost in performance compared to\nrandom initialization methods, both before start of training and after training\nis over. We show that our proposed method is especially effective in few-shot\nand fine-tuning settings. We conclude this paper with analyses on time\ncomplexity and the effect of different latent codes on the recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 07:33:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Das", "Debasmit", ""], ["Bhalgat", "Yash", ""], ["Porikli", "Fatih", ""]]}, {"id": "2105.10410", "submitter": "Martin Trefzer", "authors": "Linan Cao, Simon J. Bale, Martin A. Trefzer", "title": "Multi-objective Optimisation of Digital Circuits based on Cell Mapping\n  in an Industrial EDA Flow", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern electronic design automation (EDA) tools can handle the complexity of\nstate-of-the-art electronic systems by decomposing them into smaller blocks or\ncells, introducing different levels of abstraction and staged design flows.\nHowever, throughout each independent-optimised design step, overhead and\ninefficiency can accumulate in the resulting overall design. Performing\ndesign-specific optimisation from a more global viewpoint requires more time\ndue to the larger search space, but has the potential to provide solutions with\nimproved performance. In this work, a fully-automated, multi-objective (MO) EDA\nflow is introduced to address this issue. It specifically tunes drive strength\nmapping, preceding physical implementation, through multi-objective\npopulation-based search algorithms. Designs are evaluated with respect to their\npower, performance and area (PPA). The proposed approach is capable of\nexpanding the design space, offering a set of Pareto-optimised trade-off\nsolutions for different case-specific utilisation. We have applied the proposed\nMOEDA framework to ISCAS-85 benchmark circuits using a commercial 65nm standard\ncell library. The experimental results demonstrate how the MOEDA flow enhances\nthe solutions initially generated by the standard digital flow, and how\nsimultaneously a significant improvement in PPA metrics is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:29:58 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cao", "Linan", ""], ["Bale", "Simon J.", ""], ["Trefzer", "Martin A.", ""]]}, {"id": "2105.10430", "submitter": "Zihao Zhang", "authors": "Zihao Zhang, Stefan Zohren", "title": "Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning\n  Approaches and Hardware Acceleration using Intelligent Processing Units", "comments": "12 pages, 6 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design multi-horizon forecasting models for limit order book (LOB) data by\nusing deep learning techniques. Unlike standard structures where a single\nprediction is made, we adopt encoder-decoder models with sequence-to-sequence\nand Attention mechanisms, to generate a forecasting path. Our methods achieve\ncomparable performance to state-of-art algorithms at short prediction horizons.\nImportantly, they outperform when generating predictions over long horizons by\nleveraging the multi-horizon setup. Given that encoder-decoder models rely on\nrecurrent neural layers, they generally suffer from a slow training process. To\nremedy this, we experiment with utilising novel hardware, so-called Intelligent\nProcessing Units (IPUs) produced by Graphcore. IPUs are specifically designed\nfor machine intelligence workload with the aim to speed up the computation\nprocess. We show that in our setup this leads to significantly faster training\ntimes when compared to training models with GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:06:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zhang", "Zihao", ""], ["Zohren", "Stefan", ""]]}, {"id": "2105.10577", "submitter": "Zack Dulberg", "authors": "Zack Dulberg, Taylor Webb, Jonathan Cohen", "title": "Modelling the development of counting with memory-augmented neural\n  networks", "comments": "Accepted talk at Proceedings of the 42nd Annual Meeting of the\n  Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to count is an important example of the broader human capacity for\nsystematic generalization, and the development of counting is often\ncharacterized by an inflection point when children rapidly acquire proficiency\nwith the procedures that support this ability. We aimed to model this process\nby training a reinforcement learning agent to select N items from a binary\nvector when instructed (known as the give-$N$ task). We found that a\nmemory-augmented modular network architecture based on the recently proposed\nEmergent Symbol Binding Network (ESBN) exhibited an inflection during learning\nthat resembled human development. This model was also capable of systematic\nextrapolation outside the range of its training set - for example, trained only\nto select between 1 and 10 items, it could succeed at selecting 11 to 15 items\nas long as it could make use of an arbitrary count sequence of at least that\nlength. The close parallels to child development and the capacity for\nextrapolation suggest that our model could shed light on the emergence of\nsystematicity in humans.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 21:17:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Dulberg", "Zack", ""], ["Webb", "Taylor", ""], ["Cohen", "Jonathan", ""]]}, {"id": "2105.10585", "submitter": "Phil Long", "authors": "Philip M. Long", "title": "Properties of the After Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Neural Tangent Kernel (NTK) is the wide-network limit of a kernel defined\nusing neural networks at initialization, whose embedding is the gradient of the\noutput of the network with respect to its parameters. We study the \"after\nkernel\", which is defined using the same embedding, except after training, for\nneural networks with standard architectures, on binary classification problems\nextracted from MNIST and CIFAR-10, trained using SGD in a standard way. For\nsome dataset-architecture pairs, after a few epochs of neural network training,\na hard-margin SVM using the network's after kernel is much more accurate than\nwhen the network's initial kernel is used. For networks with an architecture\nsimilar to VGG, the after kernel is more \"global\", in the sense that it is less\ninvariant to transformations of input images that disrupt the global structure\nof the image while leaving the local statistics largely intact. For fully\nconnected networks, the after kernel is less global in this sense. The after\nkernel tends to be more invariant to small shifts, rotations and zooms; data\naugmentation does not improve these invariances. The (finite approximation to\nthe) conjugate kernel, obtained using the last layer of hidden nodes,\nsometimes, but not always, provides a good approximation to the NTK and the\nafter kernel.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 21:50:18 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:39:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Long", "Philip M.", ""]]}, {"id": "2105.10657", "submitter": "Ye Tian", "authors": "Ye Tian, Xingyi Zhang, Cheng He, Kay Chen Tan, Yaochu Jin", "title": "Principled Design of Translation, Scale, and Rotation Invariant\n  Variation Operators for Metaheuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past three decades, a large number of metaheuristics have been\nproposed and shown high performance in solving complex optimization problems.\nWhile most variation operators in existing metaheuristics are empirically\ndesigned, this paper aims to design new operators automatically, which are\nexpected to be search space independent and thus exhibit robust performance on\ndifferent problems. For this purpose, this work first investigates the\ninfluence of translation invariance, scale invariance, and rotation invariance\non the search behavior and performance of some representative operators. Then,\nwe deduce the generic form of translation, scale, and rotation invariant\noperators. Afterwards, a principled approach is proposed for the automated\ndesign of operators, which searches for high-performance operators based on the\ndeduced generic form. The experimental results demonstrate that the operators\ngenerated by the proposed approach outperform state-of-the-art ones on a\nvariety of problems with complex landscapes and up to 1000 decision variables.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 07:27:14 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tian", "Ye", ""], ["Zhang", "Xingyi", ""], ["He", "Cheng", ""], ["Tan", "Kay Chen", ""], ["Jin", "Yaochu", ""]]}, {"id": "2105.10672", "submitter": "Satoshi Sunada", "authors": "Satoshi Sunada and Atsushi Uchida", "title": "Photonic neural field on a silicon chip: large-scale, high-speed\n  neuro-inspired computing and sensing", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photonic neural networks have significant potential for high-speed neural\nprocessing with low latency and ultralow energy consumption. However, the\non-chip implementation of a large-scale neural network is still challenging\nowing to its low scalability. Herein, we propose the concept of a photonic\nneural field and implement it experimentally on a silicon chip to realize\nhighly scalable neuro-inspired computing. In contrast to existing photonic\nneural networks, the photonic neural field is a spatially continuous field that\nnonlinearly responds to optical inputs, and its high spatial degrees of freedom\nallow for large-scale and high-density neural processing on a millimeter-scale\nchip. In this study, we use the on-chip photonic neural field as a reservoir of\ninformation and demonstrate a high-speed chaotic time-series prediction with\nlow errors using a training approach similar to reservoir computing. We discuss\nthat the photonic neural field is potentially capable of executing more than\none peta multiply-accumulate operations per second for a single input\nwavelength on a footprint as small as a few square millimeters. In addition to\nprocessing, the photonic neural field can be used for rapidly sensing the\ntemporal variation of an optical phase, facilitated by its high sensitivity to\noptical inputs. The merging of optical processing with optical sensing paves\nthe way for an end-to-end data-driven optical sensing scheme.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 09:28:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sunada", "Satoshi", ""], ["Uchida", "Atsushi", ""]]}, {"id": "2105.10709", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash, Ashwin Srinivasan, A Baskar", "title": "Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse\n  Entailment", "comments": "submitted to Machine Learning Journal (MLJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general technique for constructing Graph Neural Networks (GNNs)\ncapable of using multi-relational domain knowledge. The technique is based on\nmode-directed inverse entailment (MDIE) developed in Inductive Logic\nProgramming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE\nidentifies a most-specific logical formula $\\bot_B(e)$ that contains all the\nrelational information in $B$ that is related to $e$. We transform $\\bot_B(e)$\ninto a corresponding \"bottom-graph\" that can be processed for use by standard\nGNN implementations. This transformation allows a principled way of\nincorporating generic background knowledge into GNNs: we use the term `BotGNN'\nfor this form of graph neural networks. For several GNN variants, using\nreal-world datasets with substantial background knowledge, we show that BotGNNs\nperform significantly better than both GNNs without background knowledge and a\nrecently proposed simplified technique for including domain knowledge into\nGNNs. We also provide experimental evidence comparing BotGNNs favourably to\nmulti-layer perceptrons (MLPs) that use features representing a\n\"propositionalised\" form of the background knowledge; and BotGNNs to a standard\nILP based on the use of most-specific clauses. Taken together, these results\npoint to BotGNNs as capable of combining the computational efficacy of GNNs\nwith the representational versatility of ILP.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 12:25:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Srinivasan", "Ashwin", ""], ["Baskar", "A", ""]]}, {"id": "2105.10907", "submitter": "Unnikrishnan Menon", "authors": "Unnikrishnan Rajendran Menon and Anirudh Rajiv Menon", "title": "An Efficient Application of Neuroevolution for Competitive Multiagent\n  Learning", "comments": "13 pages, 7 figures, 2 tables", "journal-ref": "Transactions on Machine Learning and Artificial Intelligence,\n  9(3), 1-13 (2021)", "doi": "10.14738/tmlai.93.10149", "report-no": "TMLAI-10149", "categories": "cs.AI cs.MA cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiagent systems provide an ideal environment for the evaluation and\nanalysis of real-world problems using reinforcement learning algorithms. Most\ntraditional approaches to multiagent learning are affected by long training\nperiods as well as high computational complexity. NEAT (NeuroEvolution of\nAugmenting Topologies) is a popular evolutionary strategy used to obtain the\nbest performing neural network architecture often used to tackle optimization\nproblems in the field of artificial intelligence. This paper utilizes the NEAT\nalgorithm to achieve competitive multiagent learning on a modified pong game\nenvironment in an efficient manner. The competing agents abide by different\nrules while having similar observation space parameters. The proposed algorithm\nutilizes this property of the environment to define a singular\nneuroevolutionary procedure that obtains the optimal policy for all the agents.\nThe compiled results indicate that the proposed implementation achieves ideal\nbehaviour in a very short training period when compared to existing multiagent\nreinforcement learning models.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 10:34:48 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Menon", "Unnikrishnan Rajendran", ""], ["Menon", "Anirudh Rajiv", ""]]}, {"id": "2105.11028", "submitter": "Milad Khademinori", "authors": "Milad Khademi Nori, Sangseok Yun, and Il-Min Kim", "title": "Fast Federated Learning by Balancing Communication Trade-Offs", "comments": "14 pages, 24 figures, accepted for publication in IEEE Transactions\n  on Communications", "journal-ref": null, "doi": "10.1109/TCOMM.2021.3083316", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has recently received a lot of attention for\nlarge-scale privacy-preserving machine learning. However, high communication\noverheads due to frequent gradient transmissions decelerate FL. To mitigate the\ncommunication overheads, two main techniques have been studied: (i) local\nupdate of weights characterizing the trade-off between communication and\ncomputation and (ii) gradient compression characterizing the trade-off between\ncommunication and precision. To the best of our knowledge, studying and\nbalancing those two trade-offs jointly and dynamically while considering their\nimpacts on convergence has remained unresolved even though it promises\nsignificantly faster FL. In this paper, we first formulate our problem to\nminimize learning error with respect to two variables: local update\ncoefficients and sparsity budgets of gradient compression who characterize\ntrade-offs between communication and computation/precision, respectively. We\nthen derive an upper bound of the learning error in a given wall-clock time\nconsidering the interdependency between the two variables. Based on this\ntheoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),\nthat jointly and dynamically adjusts the two variables to minimize the learning\nerror. We demonstrate that FFL consistently achieves higher accuracies faster\nthan similar schemes existing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:55:14 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Nori", "Milad Khademi", ""], ["Yun", "Sangseok", ""], ["Kim", "Il-Min", ""]]}, {"id": "2105.11233", "submitter": "Hans-Christian Ruiz-Euler Dr.", "authors": "Marcus N. Boon, Hans-Christian Ruiz Euler, Tao Chen, Bram van de Ven,\n  Unai Alegre Ibarra, Peter A. Bobbert, Wilfred G. van der Wiel", "title": "Gradient Descent in Materio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning, a multi-layered neural network approach inspired by the brain,\nhas revolutionized machine learning. One of its key enablers has been\nbackpropagation, an algorithm that computes the gradient of a loss function\nwith respect to the weights in the neural network model, in combination with\nits use in gradient descent. However, the implementation of deep learning in\ndigital computers is intrinsically wasteful, with energy consumption becoming\nprohibitively high for many applications. This has stimulated the development\nof specialized hardware, ranging from neuromorphic CMOS integrated circuits and\nintegrated photonic tensor cores to unconventional, material-based computing\nsystems. The learning process in these material systems, taking place, e.g., by\nartificial evolution or surrogate neural network modelling, is still a\ncomplicated and time-consuming process. Here, we demonstrate an efficient and\naccurate homodyne gradient extraction method for performing gradient descent on\nthe loss function directly in the material system. We demonstrate the method in\nour recently developed dopant network processing units, where we readily\nrealize all Boolean gates. This shows that gradient descent can in principle be\nfully implemented in materio using simple electronics, opening up the way to\nautonomously learning material systems.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 12:18:31 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Boon", "Marcus N.", ""], ["Euler", "Hans-Christian Ruiz", ""], ["Chen", "Tao", ""], ["van de Ven", "Bram", ""], ["Ibarra", "Unai Alegre", ""], ["Bobbert", "Peter A.", ""], ["van der Wiel", "Wilfred G.", ""]]}, {"id": "2105.11502", "submitter": "Luca Mariot", "authors": "Lucija Planinic, Marko Djurasevic, Luca Mariot, Domagoj Jakobovic,\n  Stjepan Picek, Carlos Coello Coello", "title": "On the Genotype Compression and Expansion for Evolutionary Algorithms in\n  the Continuous Domain", "comments": "17 pages, 3 figures, 4 tables, pre-print accepted at the AABOH\n  workshop co-located with GECCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the influence of genotype size on evolutionary\nalgorithms' performance. We consider genotype compression (where genotype is\nsmaller than phenotype) and expansion (genotype is larger than phenotype) and\ndefine different strategies to reconstruct the original variables of the\nphenotype from both the compressed and expanded genotypes. We test our approach\nwith several evolutionary algorithms over three sets of optimization problems:\nCOCO benchmark functions, modeling of Physical Unclonable Functions, and neural\nnetwork weight optimization. Our results show that genotype expansion works\nsignificantly better than compression, and in many scenarios, outperforms the\noriginal genotype encoding. This could be attributed to the change in the\ngenotype-phenotype mapping introduced with the expansion methods: this\nmodification beneficially transforms the domain landscape and alleviates the\nsearch space traversal.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:56:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Planinic", "Lucija", ""], ["Djurasevic", "Marko", ""], ["Mariot", "Luca", ""], ["Jakobovic", "Domagoj", ""], ["Picek", "Stjepan", ""], ["Coello", "Carlos Coello", ""]]}, {"id": "2105.11521", "submitter": "Adil Rasheed Professor", "authors": "Sindre Stenen Blakseth and Adil Rasheed and Trond Kvamsdal and Omer\n  San", "title": "Deep neural network enabled corrective source term approach to hybrid\n  analysis and modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Analysis and Modeling (HAM) is an emerging modeling paradigm which\naims to combine physics-based modeling (PBM) and data-driven modeling (DDM) to\ncreate generalizable, trustworthy, accurate, computationally efficient and\nself-evolving models. Here, we introduce, justify and demonstrate a novel\napproach to HAM -- the Corrective Source Term Approach (CoSTA) -- which\naugments the governing equation of a PBM model with a corrective source term\ngenerated by a deep neural network (DNN). In a series of numerical experiments\non one-dimensional heat diffusion, CoSTA is generally found to outperform\ncomparable DDM and PBM models in terms of accuracy -- often reducing predictive\nerrors by several orders of magnitude -- while also generalizing better than\npure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides\na modular framework for leveraging novel developments within both PBM and DDM,\nand due to the interpretability of the DNN-generated source term within the PBM\nparadigm, CoSTA can be a potential door-opener for data-driven techniques to\nenter high-stakes applications previously reserved for pure PBM.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 20:17:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Blakseth", "Sindre Stenen", ""], ["Rasheed", "Adil", ""], ["Kvamsdal", "Trond", ""], ["San", "Omer", ""]]}, {"id": "2105.11586", "submitter": "Youhei Akimoto", "authors": "Youhei Akimoto, Yoshiki Miyauchi, Atsuo Maki", "title": "Saddle Point Optimization with Approximate Minimization Oracle and its\n  Application to Robust Berthing Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an approach to saddle point optimization relying only on an oracle\nthat solves a minimization problem approximately. We analyze its convergence\nproperty on a strongly convex--concave problem and show its linear convergence\ntoward the global min--max saddle point. Based on the convergence analysis, we\npropose a heuristic approach to adapt the learning rate for the proposed saddle\npoint optimization approach. The implementation of the proposed approach using\nthe (1+1)-CMA-ES as the minimization oracle, namely Adversarial-CMA-ES, is\nevaluated on test problems. Numerical evaluation reveals the tightness of the\ntheoretical convergence rate bound as well as the efficiency of the learning\nrate adaptation mechanism. As an example of real-world applications, it is\napplied to automatic berthing control problems under model uncertainties,\nshowing its usefulness in obtaining solutions robust under model uncertainties.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:08:47 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 04:00:22 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Akimoto", "Youhei", ""], ["Miyauchi", "Yoshiki", ""], ["Maki", "Atsuo", ""]]}, {"id": "2105.11602", "submitter": "Saeedreza Shehnepoor", "authors": "Saeedreza Shehnepoor, Roberto Togneri, Wei Liu, Mohammed Bennamoun", "title": "HIN-RNN: A Graph Representation Learning Neural Network for Fraudster\n  Group Detection With No Handcrafted Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Social reviews are indispensable resources for modern consumers' decision\nmaking. For financial gain, companies pay fraudsters preferably in groups to\ndemote or promote products and services since consumers are more likely to be\nmisled by a large number of similar reviews from groups. Recent approaches on\nfraudster group detection employed handcrafted features of group behaviors\nwithout considering the semantic relation between reviews from the reviewers in\na group. In this paper, we propose the first neural approach, HIN-RNN, a\nHeterogeneous Information Network (HIN) Compatible RNN for fraudster group\ndetection that requires no handcrafted features. HIN-RNN provides a unifying\narchitecture for representation learning of each reviewer, with the initial\nvector as the sum of word embeddings of all review text written by the same\nreviewer, concatenated by the ratio of negative reviews. Given a co-review\nnetwork representing reviewers who have reviewed the same items with the same\nratings and the reviewers' vector representation, a collaboration matrix is\nacquired through HIN-RNN training. The proposed approach is confirmed to be\neffective with marked improvement over state-of-the-art approaches on both the\nYelp (22% and 12% in terms of recall and F1-value, respectively) and Amazon (4%\nand 2% in terms of recall and F1-value, respectively) datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:48:28 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shehnepoor", "Saeedreza", ""], ["Togneri", "Roberto", ""], ["Liu", "Wei", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2105.11654", "submitter": "Jianhao Ding", "authors": "Jianhao Ding, Zhaofei Yu, Yonghong Tian and Tiejun Huang", "title": "Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep\n  Spiking Neural Networks", "comments": "9 pages, 7 figures, 2 tables. To appear in the 30th International\n  Joint Conference on Artificial Intelligence (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural\nnetworks, have attracted great attentions from researchers and industry. The\nmost efficient way to train deep SNNs is through ANN-SNN conversion. However,\nthe conversion usually suffers from accuracy loss and long inference time,\nwhich impede the practical application of SNN. In this paper, we theoretically\nanalyze ANN-SNN conversion and derive sufficient conditions of the optimal\nconversion. To better correlate ANN-SNN and get greater accuracy, we propose\nRate Norm Layer to replace the ReLU activation function in source ANN training,\nenabling direct conversion from a trained ANN to an SNN. Moreover, we propose\nan optimal fit curve to quantify the fit between the activation value of source\nANN and the actual firing rate of target SNN. We show that the inference time\ncan be reduced by optimizing the upper bound of the fit curve in the revised\nANN to achieve fast inference. Our theory can explain the existing work on fast\nreasoning and get better results. The experimental results show that the\nproposed method achieves near loss less conversion with VGG-16,\nPreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster\nreasoning performance under 0.265x energy consumption of the typical method.\nThe code is available at\nhttps://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:15:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ding", "Jianhao", ""], ["Yu", "Zhaofei", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""]]}, {"id": "2105.11848", "submitter": "Tao Luo", "authors": "Tao Luo, Wai Teng Tang, Matthew Kay Fei Lee, Chuping Qu, Weng-Fai\n  Wong, Rick Goh", "title": "DTNN: Energy-efficient Inference with Dendrite Tree Inspired Neural\n  Networks for Edge Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have achieved remarkable success in computer\nvision (CV). However, training and inference of DNN models are both memory and\ncomputation intensive, incurring significant overhead in terms of energy\nconsumption and silicon area. In particular, inference is much more\ncost-sensitive than training because training can be done offline with powerful\nplatforms, while inference may have to be done on battery powered devices with\nconstrained form factors, especially for mobile or edge vision applications. In\norder to accelerate DNN inference, model quantization was proposed. However\nprevious works only focus on the quantization rate without considering the\nefficiency of operations. In this paper, we propose Dendrite-Tree based Neural\nNetwork (DTNN) for energy-efficient inference with table lookup operations\nenabled by activation quantization. In DTNN both costly weight access and\narithmetic computations are eliminated for inference. We conducted experiments\non various kinds of DNN models such as LeNet-5, MobileNet, VGG, and ResNet with\ndifferent datasets, including MNIST, Cifar10/Cifar100, SVHN, and ImageNet. DTNN\nachieved significant energy saving (19.4X and 64.9X improvement on ResNet-18\nand VGG-11 with ImageNet, respectively) with negligible loss of accuracy. To\nfurther validate the effectiveness of DTNN and compare with state-of-the-art\nlow energy implementation for edge vision, we design and implement DTNN based\nMLP image classifiers using off-the-shelf FPGAs. The results show that DTNN on\nthe FPGA, with higher accuracy, could achieve orders of magnitude better energy\nconsumption and latency compared with the state-of-the-art low energy\napproaches reported that use ASIC chips.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 11:44:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Luo", "Tao", ""], ["Tang", "Wai Teng", ""], ["Lee", "Matthew Kay Fei", ""], ["Qu", "Chuping", ""], ["Wong", "Weng-Fai", ""], ["Goh", "Rick", ""]]}, {"id": "2105.11989", "submitter": "Rushabh Patel", "authors": "Rushabh Patel, Yanhui Guo", "title": "Graph Based Link Prediction between Human Phenotypes and Genes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The learning of genotype-phenotype associations and history of\nhuman disease by doing detailed and precise analysis of phenotypic\nabnormalities can be defined as deep phenotyping. To understand and detect this\ninteraction between phenotype and genotype is a fundamental step when\ntranslating precision medicine to clinical practice. The recent advances in the\nfield of machine learning is efficient to predict these interactions between\nabnormal human phenotypes and genes.\n  Methods: In this study, we developed a framework to predict links between\nhuman phenotype ontology (HPO) and genes. The annotation data from the\nheterogeneous knowledge resources i.e., orphanet, is used to parse human\nphenotype-gene associations. To generate the embeddings for the nodes (HPO &\ngenes), an algorithm called node2vec was used. It performs node sampling on\nthis graph based on random walks, then learns features over these sampled nodes\nto generate embeddings. These embeddings were used to perform the downstream\ntask to predict the presence of the link between these nodes using 5 different\nsupervised machine learning algorithms.\n  Results: The downstream link prediction task shows that the Gradient Boosting\nDecision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR\n0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87.\nCompared to the other 4 methods LightGBM is able to find more accurate\ninteraction/link between human phenotype & gene pairs.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:47:07 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 18:28:30 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Patel", "Rushabh", ""], ["Guo", "Yanhui", ""]]}, {"id": "2105.12039", "submitter": "Luca Mariot", "authors": "Luca Mariot, Stjepan Picek, Domagoj Jakobovic, Alberto Leporati", "title": "Evolutionary Algorithms for Designing Reversible Cellular Automata", "comments": "39 pages, 12 figures, 2 tables, pre-print of an extension of a paper\n  published in EuroGP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible Cellular Automata (RCA) are a particular kind of shift-invariant\ntransformations characterized by a dynamics composed only of disjoint cycles.\nThey have many applications in the simulation of physical systems, cryptography\nand reversible computing. In this work, we formulate the search of a specific\nclass of RCA -- namely, those whose local update rules are defined by conserved\nlandscapes -- as an optimization problem to be tackled with Genetic Algorithms\n(GA) and Genetic Programming (GP). In particular, our experimental\ninvestigation revolves around three different research questions, which we\naddress through a single-objective, a multi-objective, and a lexicographic\napproach. The results obtained from our experiments corroborate the previous\nfindings and shed new light on 1) the difficulty of the associated optimization\nproblem for GA and GP, 2) the relevance of conserved landscape CA in the domain\nof cryptography and reversible computing, and 3) the relationship between the\nreversibility property and the Hamming weight.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:19:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mariot", "Luca", ""], ["Picek", "Stjepan", ""], ["Jakobovic", "Domagoj", ""], ["Leporati", "Alberto", ""]]}, {"id": "2105.12196", "submitter": "Siqi Liu", "authors": "Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, S. M. Ali Eslami, Daniel\n  Hennes, Wojciech M. Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas\n  Abdolmaleki, Noah Y. Siegel, Leonard Hasenclever, Luke Marris, Saran\n  Tunyasuvunakool, H. Francis Song, Markus Wulfmeier, Paul Muller, Tuomas\n  Haarnoja, Brendan D. Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess", "title": "From Motor Control to Team Play in Simulated Humanoid Football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent behaviour in the physical world exhibits structure at multiple\nspatial and temporal scales. Although movements are ultimately executed at the\nlevel of instantaneous muscle tensions or joint torques, they must be selected\nto serve goals defined on much longer timescales, and in terms of relations\nthat extend far beyond the body itself, ultimately involving coordination with\nother agents. Recent research in artificial intelligence has shown the promise\nof learning-based approaches to the respective problems of complex movement,\nlonger-term planning and multi-agent coordination. However, there is limited\nresearch aimed at their integration. We study this problem by training teams of\nphysically simulated humanoid avatars to play football in a realistic virtual\nenvironment. We develop a method that combines imitation learning, single- and\nmulti-agent reinforcement learning and population-based training, and makes use\nof transferable representations of behaviour for decision making at different\nlevels of abstraction. In a sequence of stages, players first learn to control\na fully articulated body to perform realistic, human-like movements such as\nrunning and turning; they then acquire mid-level football skills such as\ndribbling and shooting; finally, they develop awareness of others and play as a\nteam, bridging the gap between low-level motor control at a timescale of\nmilliseconds, and coordinated goal-directed behaviour as a team at the\ntimescale of tens of seconds. We investigate the emergence of behaviours at\ndifferent levels of abstraction, as well as the representations that underlie\nthese behaviours using several analysis techniques, including statistics from\nreal-world sports analytics. Our work constitutes a complete demonstration of\nintegrated decision-making at multiple scales in a physically embodied\nmulti-agent setting. See project video at https://youtu.be/KHMwq9pv7mg.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:17:10 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Liu", "Siqi", ""], ["Lever", "Guy", ""], ["Wang", "Zhe", ""], ["Merel", "Josh", ""], ["Eslami", "S. M. Ali", ""], ["Hennes", "Daniel", ""], ["Czarnecki", "Wojciech M.", ""], ["Tassa", "Yuval", ""], ["Omidshafiei", "Shayegan", ""], ["Abdolmaleki", "Abbas", ""], ["Siegel", "Noah Y.", ""], ["Hasenclever", "Leonard", ""], ["Marris", "Luke", ""], ["Tunyasuvunakool", "Saran", ""], ["Song", "H. Francis", ""], ["Wulfmeier", "Markus", ""], ["Muller", "Paul", ""], ["Haarnoja", "Tuomas", ""], ["Tracey", "Brendan D.", ""], ["Tuyls", "Karl", ""], ["Graepel", "Thore", ""], ["Heess", "Nicolas", ""]]}, {"id": "2105.12210", "submitter": "George Philipp", "authors": "George Philipp", "title": "The Nonlinearity Coefficient - A Practical Guide to Neural Architecture\n  Design", "comments": "This work is based on the PhD thesis with the same name, author, year\n  and institution. Both works may be cited interchangeably", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In essence, a neural network is an arbitrary differentiable, parametrized\nfunction. Choosing a neural network architecture for any task is as complex as\nsearching the space of those functions. For the last few years, 'neural\narchitecture design' has been largely synonymous with 'neural architecture\nsearch' (NAS), i.e. brute-force, large-scale search. NAS has yielded\nsignificant gains on practical tasks. However, NAS methods end up searching for\na local optimum in architecture space in a small neighborhood around\narchitectures that often go back decades, based on CNN or LSTM.\n  In this work, we present a different and complementary approach to\narchitecture design, which we term 'zero-shot architecture design' (ZSAD). We\ndevelop methods that can predict, without any training, whether an architecture\nwill achieve a relatively high test or training error on a task after training.\nWe then go on to explain the error in terms of the architecture definition\nitself and develop tools for modifying the architecture based on this\nexplanation. This confers an unprecedented level of control on the deep\nlearning practitioner. They can make informed design decisions before the first\nline of code is written, even for tasks for which no prior art exists.\n  Our first major contribution is to show that the 'degree of nonlinearity' of\na neural architecture is a key causal driver behind its performance, and a\nprimary aspect of the architecture's model complexity. We introduce the\n'nonlinearity coefficient' (NLC), a scalar metric for measuring nonlinearity.\nVia extensive empirical study, we show that the value of the NLC in the\narchitecture's randomly initialized state before training is a powerful\npredictor of test error after training and that attaining a right-sized NLC is\nessential for attaining an optimal test error. The NLC is also conceptually\nsimple, well-defined for any feedforward network, easy and cheap to compute,\nhas extensive theoretical, empirical and conceptual grounding, follows\ninstructively from the architecture definition, and can be easily controlled\nvia our 'nonlinearity normalization' algorithm. We argue that the NLC is the\nmost powerful scalar statistic for architecture design specifically and neural\nnetwork analysis in general. Our analysis is fueled by mean field theory, which\nwe use to uncover the 'meta-distribution' of layers.\n  Beyond the NLC, we uncover and flesh out a range of metrics and properties\nthat have a significant explanatory influence on test and training error. We go\non to explain the majority of the error variation across a wide range of\nrandomly generated architectures with these metrics and properties. We compile\nour insights into a practical guide for architecture designers, which we argue\ncan significantly shorten the trial-and-error phase of deep learning\ndeployment.\n  Our results are grounded in an experimental protocol that exceeds that of the\nvast majority of other deep learning studies in terms of carefulness and rigor.\nWe study the impact of e.g. dataset, learning rate, floating-point precision,\nloss function, statistical estimation error and batch inter-dependency on\nperformance and other key properties. We promote research practices that we\nbelieve can significantly accelerate progress in architecture design research.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:47:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Philipp", "George", ""]]}, {"id": "2105.12245", "submitter": "Alain Rossier", "authors": "Alain-Sam Cohen, Rama Cont, Alain Rossier, Renyuan Xu", "title": "Scaling Properties of Deep Residual Networks", "comments": "Published at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) have displayed impressive results in pattern\nrecognition and, recently, have garnered considerable theoretical interest due\nto a perceived link with neural ordinary differential equations (neural ODEs).\nThis link relies on the convergence of network weights to a smooth function as\nthe number of layers increases. We investigate the properties of weights\ntrained by stochastic gradient descent and their scaling with network depth\nthrough detailed numerical experiments. We observe the existence of scaling\nregimes markedly different from those assumed in neural ODE literature.\nDepending on certain features of the network architecture, such as the\nsmoothness of the activation function, one may obtain an alternative ODE limit,\na stochastic differential equation or neither of these. These findings cast\ndoubts on the validity of the neural ODE model as an adequate asymptotic\ndescription of deep ResNets and point to an alternative class of differential\nequations as a better description of the deep network limit.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:31:30 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:46:14 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Cohen", "Alain-Sam", ""], ["Cont", "Rama", ""], ["Rossier", "Alain", ""], ["Xu", "Renyuan", ""]]}, {"id": "2105.12307", "submitter": "Venkata Vaishnav Tadiparthi", "authors": "Vaishnav Tadiparthi and Raktim Bhattacharya", "title": "Optimal Transport Based Refinement of Physics-Informed Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a refinement strategy to the well-known\nPhysics-Informed Neural Networks (PINNs) for solving partial differential\nequations (PDEs) based on the concept of Optimal Transport (OT).\n  Conventional black-box PINNs solvers have been found to suffer from a host of\nissues: spectral bias in fully-connected architectures, unstable gradient\npathologies, as well as difficulties with convergence and accuracy.\n  Current network training strategies are agnostic to dimension sizes and rely\non the availability of powerful computing resources to optimize through a large\nnumber of collocation points.\n  This is particularly challenging when studying stochastic dynamical systems\nwith the Fokker-Planck-Kolmogorov Equation (FPKE), a second-order PDE which is\ntypically solved in high-dimensional state space.\n  While we focus exclusively on the stationary form of the FPKE, positivity and\nnormalization constraints on its solution make it all the more unfavorable to\nsolve directly using standard PINNs approaches.\n  To mitigate the above challenges, we present a novel training strategy for\nsolving the FPKE using OT-based sampling to supplement the existing PINNs\nframework.\n  It is an iterative approach that induces a network trained on a small dataset\nto add samples to its training dataset from regions where it nominally makes\nthe most error.\n  The new samples are found by solving a linear programming problem at every\niteration.\n  The paper is complemented by an experimental evaluation of the proposed\nmethod showing its applicability on a variety of stochastic systems with\nnonlinear dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:51:20 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:26:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tadiparthi", "Vaishnav", ""], ["Bhattacharya", "Raktim", ""]]}, {"id": "2105.12395", "submitter": "Khaled Koutini", "authors": "Khaled Koutini, Hamid Eghbal-zadeh, Gerhard Widmer", "title": "Receptive Field Regularization Techniques for Audio Classification and\n  Tagging with Deep Convolutional Neural Networks", "comments": "Accepted in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. Code available: https://github.com/kkoutini/cpjku_dcase20", "journal-ref": null, "doi": "10.1109/TASLP.2021.3082307", "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of variants of well-known\nConvolutional Neural Network (CNN) architectures on different audio tasks. We\nshow that tuning the Receptive Field (RF) of CNNs is crucial to their\ngeneralization. An insufficient RF limits the CNN's ability to fit the training\ndata. In contrast, CNNs with an excessive RF tend to over-fit the training data\nand fail to generalize to unseen testing data. As state-of-the-art CNN\narchitectures-in computer vision and other domains-tend to go deeper in terms\nof number of layers, their RF size increases and therefore they degrade in\nperformance in several audio classification and tagging tasks. We study\nwell-known CNN architectures and how their building blocks affect their\nreceptive field. We propose several systematic approaches to control the RF of\nCNNs and systematically test the resulting architectures on different audio\nclassification and tagging tasks and datasets. The experiments show that\nregularizing the RF of CNNs using our proposed approaches can drastically\nimprove the generalization of models, out-performing complex architectures and\npre-trained models on larger datasets. The proposed CNNs achieve\nstate-of-the-art results in multiple tasks, from acoustic scene classification\nto emotion and theme detection in music to instrument recognition, as\ndemonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 08:36:29 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Koutini", "Khaled", ""], ["Eghbal-zadeh", "Hamid", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2105.12525", "submitter": "Jakob Bossek", "authors": "Jakob Bossek, Frank Neumann, Pan Peng, Dirk Sudholt", "title": "Time Complexity Analysis of Randomized Search Heuristics for the Dynamic\n  Graph Coloring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute to the theoretical understanding of randomized search\nheuristics for dynamic problems. We consider the classical vertex coloring\nproblem on graphs and investigate the dynamic setting where edges are added to\nthe current graph. We then analyze the expected time for randomized search\nheuristics to recompute high quality solutions. The (1+1)~Evolutionary\nAlgorithm and RLS operate in a setting where the number of colors is bounded\nand we are minimizing the number of conflicts. Iterated local search algorithms\nuse an unbounded color palette and aim to use the smallest colors and,\nconsequently, the smallest number of colors.\n  We identify classes of bipartite graphs where reoptimization is as hard as or\neven harder than optimization from scratch, i.e., starting with a random\ninitialization. Even adding a single edge can lead to hard symmetry problems.\nHowever, graph classes that are hard for one algorithm turn out to be easy for\nothers. In most cases our bounds show that reoptimization is faster than\noptimizing from scratch. We further show that tailoring mutation operators to\nparts of the graph where changes have occurred can significantly reduce the\nexpected reoptimization time. In most settings the expected reoptimization time\nfor such tailored algorithms is linear in the number of added edges. However,\ntailored algorithms cannot prevent exponential times in settings where the\noriginal algorithm is inefficient.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:00:31 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bossek", "Jakob", ""], ["Neumann", "Frank", ""], ["Peng", "Pan", ""], ["Sudholt", "Dirk", ""]]}, {"id": "2105.12770", "submitter": "Xun Jiao", "authors": "Rahul Thapa, Dongning Ma, Xun Jiao", "title": "HDXplore: Automated Blackbox Testing of Brain-Inspired Hyperdimensional\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the way human brain works, the emerging hyperdimensional\ncomputing (HDC) is getting more and more attention. HDC is an emerging\ncomputing scheme based on the working mechanism of brain that computes with\ndeep and abstract patterns of neural activity instead of actual numbers.\nCompared with traditional ML algorithms such as DNN, HDC is more\nmemory-centric, granting it advantages such as relatively smaller model size,\nless computation cost, and one-shot learning, making it a promising candidate\nin low-cost computing platforms. However, the robustness of HDC models have not\nbeen systematically studied. In this paper, we systematically expose the\nunexpected or incorrect behaviors of HDC models by developing HDXplore, a\nblackbox differential testing-based framework. We leverage multiple HDC models\nwith similar functionality as cross-referencing oracles to avoid manual\nchecking or labeling the original input. We also propose different perturbation\nmechanisms in HDXplore. HDXplore automatically finds thousands of incorrect\ncorner case behaviors of the HDC model. We propose two retraining mechanisms\nand using the corner cases generated by HDXplore to retrain the HDC model, we\ncan improve the model accuracy by up to 9%.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:08:52 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Thapa", "Rahul", ""], ["Ma", "Dongning", ""], ["Jiao", "Xun", ""]]}, {"id": "2105.12781", "submitter": "Supreeth Mysore Shivanandamurthy", "authors": "Supreeth Mysore Shivanandamurthy, Ishan. G. Thakkar, Sayed Ahmad\n  Salehi", "title": "ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for\n  In-DRAM CNN Processing", "comments": "Preprint accepted in ISVLSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the rapidly growing use of Convolutional Neural Networks (CNNs) in\nreal-world applications related to machine learning and Artificial Intelligence\n(AI), several hardware accelerator designs for CNN inference and training have\nbeen proposed recently. In this paper, we present ATRIA, a novel bit-pArallel\nsTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and\nhigh-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM\ncell arrays to implement bit-parallel stochastic arithmetic based acceleration\nof multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly\nimproves the latency, throughput, and efficiency of processing CNN inferences\nby performing 16 MAC operations in only five consecutive memory operation\ncycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to\ncompare its performance with five state-of-the-art in-DRAM CNN accelerators\nfrom prior work. The results of our analysis show that ATRIA exhibits only 3.5%\ndrop in CNN inference accuracy and still achieves improvements of up to 3.2x in\nframes-per-second (FPS) and up to 10x in efficiency (FPS/W/mm2), compared to\nthe best-performing in-DRAM accelerator from prior work.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:36:01 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shivanandamurthy", "Supreeth Mysore", ""], ["Thakkar", "Ishan. G.", ""], ["Salehi", "Sayed Ahmad", ""]]}, {"id": "2105.12836", "submitter": "Unai Garciarena", "authors": "Unai Garciarena, Nuno Louren\\c{c}o, Penousal Machado, Roberto Santana,\n  Alexander Mendiburu", "title": "On the Exploitation of Neuroevolutionary Information: Analyzing the Past\n  for a More Efficient Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuroevolutionary algorithms, automatic searches of neural network structures\nby means of evolutionary techniques, are computationally costly procedures. In\nspite of this, due to the great performance provided by the architectures which\nare found, these methods are widely applied. The final outcome of\nneuroevolutionary processes is the best structure found during the search, and\nthe rest of the procedure is commonly omitted in the literature. However, a\ngood amount of residual information consisting of valuable knowledge that can\nbe extracted is also produced during these searches. In this paper, we propose\nan approach that extracts this information from neuroevolutionary runs, and use\nit to build a metamodel that could positively impact future neural architecture\nsearches. More specifically, by inspecting the best structures found during\nneuroevolutionary searches of generative adversarial networks with varying\ncharacteristics (e.g., based on dense or convolutional layers), we propose a\nBayesian network-based model which can be used to either find strong neural\nstructures right away, conveniently initialize different structural searches\nfor different problems, or help future optimization of structures of any type\nto keep finding increasingly better structures where uninformed methods get\nstuck into local optima.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:55:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Garciarena", "Unai", ""], ["Louren\u00e7o", "Nuno", ""], ["Machado", "Penousal", ""], ["Santana", "Roberto", ""], ["Mendiburu", "Alexander", ""]]}, {"id": "2105.12917", "submitter": "Yang Li", "authors": "Yang Li, Yi Zeng, Dongcheng Zhao", "title": "BSNN: Towards Faster and Better Conversion of Artificial Neural Networks\n  to Spiking Neural Networks with Bistable Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spiking neural network (SNN) computes and communicates information\nthrough discrete binary events. It is considered more biologically plausible\nand more energy-efficient than artificial neural networks (ANN) in emerging\nneuromorphic hardware. However, due to the discontinuous and non-differentiable\ncharacteristics, training SNN is a relatively challenging task. Recent work has\nachieved essential progress on an excellent performance by converting ANN to\nSNN. Due to the difference in information processing, the converted deep SNN\nusually suffers serious performance loss and large time delay. In this paper,\nwe analyze the reasons for the performance loss and propose a novel bistable\nspiking neural network (BSNN) that addresses the problem of spikes of\ninactivated neurons (SIN) caused by the phase lead and phase lag. Also, when\nResNet structure-based ANNs are converted, the information of output neurons is\nincomplete due to the rapid transmission of the shortcut path. We design\nsynchronous neurons (SN) to help efficiently improve performance. Experimental\nresults show that the proposed method only needs 1/4-1/10 of the time steps\ncompared to previous work to achieve nearly lossless conversion. We demonstrate\nstate-of-the-art ANN-SNN conversion for VGG16, ResNet20, and ResNet34 on\nchallenging datasets including CIFAR-10 (95.16% top-1), CIFAR-100 (78.12%\ntop-1), and ImageNet (72.64% top-1).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:38:02 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Yang", ""], ["Zeng", "Yi", ""], ["Zhao", "Dongcheng", ""]]}, {"id": "2105.12960", "submitter": "Jacob Schrum", "authors": "Jacob Schrum, Benjamin Capps, Kirby Steckel, Vanessa Volz, Sebastian\n  Risi", "title": "Hybrid Encoding For Generating Large Scale Game Level Patterns With\n  Local Variations Using a GAN", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.01703", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are a powerful indirect\ngenotype-to-phenotype mapping for evolutionary search, but they have\nlimitations. In particular, GAN output does not scale to arbitrary dimensions,\nand there is no obvious way to combine GAN outputs into a cohesive whole, which\nwould be useful in many areas, such as video game level generation. Game levels\noften consist of several segments, sometimes repeated directly or with\nvariation, organized into an engaging pattern. Such patterns can be produced\nwith Compositional Pattern Producing Networks (CPPNs). Specifically, a CPPN can\ndefine latent vector GAN inputs as a function of geometry, which provides a way\nto organize level segments output by a GAN into a complete level. However, a\ncollection of latent vectors can also be evolved directly, to produce more\nchaotic levels. Here, we propose a new hybrid approach that evolves CPPNs\nfirst, but allows the latent vectors to evolve later, and combines the benefits\nof both approaches. These approaches are evaluated in Super Mario Bros. and The\nLegend of Zelda. We previously demonstrated via divergent search (MAP-Elites)\nthat CPPNs better cover the space of possible levels than directly evolved\nlevels. Here, we show that the hybrid approach can cover areas that neither of\nthe other methods can and achieves comparable or superior QD scores.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 06:27:19 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Schrum", "Jacob", ""], ["Capps", "Benjamin", ""], ["Steckel", "Kirby", ""], ["Volz", "Vanessa", ""], ["Risi", "Sebastian", ""]]}, {"id": "2105.13004", "submitter": "Dongcheng Zhao", "authors": "Dongcheng Zhao, Yi Zeng, Yang Li", "title": "BackEISNN: A Deep Spiking Neural Network with Adaptive Self-Feedback and\n  Balanced Excitatory-Inhibitory Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) transmit information through discrete spikes,\nwhich performs well in processing spatial-temporal information. Due to the\nnon-differentiable characteristic, there still exist difficulties in designing\nwell-performed SNNs. Recently, SNNs trained with backpropagation have shown\nsuperior performance due to the proposal of the gradient approximation.\nHowever, the performance on complex tasks is still far away from the deep\nneural networks. Taking inspiration from the autapse in the brain which\nconnects the spiking neurons with a self-feedback connection, we apply an\nadaptive time-delayed self-feedback on the membrane potential to regulate the\nspike precisions. As well as, we apply the balanced excitatory and inhibitory\nneurons mechanism to control the spiking neurons' output dynamically. With the\ncombination of the two mechanisms, we propose a deep spiking neural network\nwith adaptive self-feedback and balanced excitatory and inhibitory neurons\n(BackEISNN). The experimental results on several standard datasets have shown\nthat the two modules not only accelerate the convergence of the network but\nalso improve the accuracy. For the MNIST, FashionMNIST, and N-MNIST datasets,\nour model has achieved state-of-the-art performance. For the CIFAR10 dataset,\nour BackEISNN also gets remarkable performance on a relatively light structure\nthat competes against state-of-the-art SNNs.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:38:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhao", "Dongcheng", ""], ["Zeng", "Yi", ""], ["Li", "Yang", ""]]}, {"id": "2105.13095", "submitter": "Jian Yang", "authors": "Jian Yang and Yuhui Shi", "title": "Attention-oriented Brain Storm Optimization for Multimodal Optimization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Population-based methods are often used to solve multimodal optimization\nproblems. By combining niching or clustering strategy, the state-of-the-art\napproaches generally divide the population into several subpopulations to find\nmultiple solutions for a problem at hand. However, these methods only guided by\nthe fitness value during iterations, which are suffering from determining the\nnumber of subpopulations, i.e., the number of niche areas or clusters. To\ncompensate for this drawback, this paper presents an Attention-oriented Brain\nStorm Optimization (ABSO) method that introduces the attention mechanism into a\nrelatively new swarm intelligence algorithm, i.e., Brain Storm Optimization\n(BSO). By converting the objective space from the fitness space into\n\"attention\" space, the individuals are clustered and updated iteratively\naccording to their salient values. Rather than converge to a single global\noptimum, the proposed method can guide the search procedure to converge to\nmultiple \"salient\" solutions. The preliminary results show that the proposed\nmethod can locate multiple global and local optimal solutions of several\nmultimodal benchmark functions. The proposed method needs less prior knowledge\nof the problem and can automatically converge to multiple optimums guided by\nthe attention mechanism, which has excellent potential for further development.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:47:57 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Jian", ""], ["Shi", "Yuhui", ""]]}, {"id": "2105.13108", "submitter": "Jian Yang", "authors": "Jian Yang and Yuhui Shi", "title": "Robotic Brain Storm Optimization: A Multi-target Collaborative Searching\n  Paradigm for Swarm Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Swarm intelligence optimization algorithms can be adopted in swarm robotics\nfor target searching tasks in a 2-D or 3-D space by treating the target signal\nstrength as fitness values. Many current works in the literature have achieved\ngood performance in single-target search problems. However, when there are\nmultiple targets in an environment to be searched, many swarm\nintelligence-based methods may converge to specific locations prematurely,\nmaking it impossible to explore the environment further. The Brain Storm\nOptimization (BSO) algorithm imitates a group of humans in solving problems\ncollectively. A series of guided searches can finally obtain a relatively\noptimal solution for particular optimization problems. Furthermore, with a\nsuitable clustering operation, it has better multi-modal optimization\nperformance, i.e., it can find multiple optima in the objective space. By\nmatching the members in a robotic swarm to the individuals in the algorithm\nunder both environments and robots constraints, this paper proposes a BSO-based\ncollaborative searching framework for swarm robotics called Robotic BSO. The\nsimulation results show that the proposed method can simulate the BSO's guided\nsearch characteristics and has an excellent prospect for multi-target searching\nproblems for swarm robotics.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:05:48 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yang", "Jian", ""], ["Shi", "Yuhui", ""]]}, {"id": "2105.13228", "submitter": "Zenan Ling", "authors": "Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Yisen Wang, Guangcan Liu,\n  Zhouchen Lin", "title": "Optimization Induced Equilibrium Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by\nimplicit equations, have been becoming more and more attractive recently. In\nthis paper, we investigate an emerging question: can an implicit equilibrium\nmodel's equilibrium point be regarded as the solution of an optimization\nproblem? To this end, we first decompose DNNs into a new class of unit layer\nthat is the proximal operator of an implicit convex function while keeping its\noutput unchanged. Then, the equilibrium model of the unit layer can be derived,\nnamed Optimization Induced Equilibrium Networks (OptEq), which can be easily\nextended to deep layers. The equilibrium point of OptEq can be theoretically\nconnected to the solution of its corresponding convex optimization problem with\nexplicit objectives. Based on this, we can flexibly introduce prior properties\nto the equilibrium points: 1) modifying the underlying convex problems\nexplicitly so as to change the architectures of OptEq; and 2) merging the\ninformation into the fixed point iteration, which guarantees to choose the\ndesired equilibrium point when the fixed point set is non-singleton. We show\nthat deep OptEq outperforms previous implicit models even with fewer\nparameters. This work establishes the first step towards the\noptimization-guided design of deep models.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:17:41 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:48:54 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 07:56:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Xingyu", ""], ["Wang", "Qiuhao", ""], ["Ling", "Zenan", ""], ["Li", "Xia", ""], ["Wang", "Yisen", ""], ["Liu", "Guangcan", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2105.13262", "submitter": "Harideep Nair", "authors": "Harideep Nair, John Paul Shen and James E. Smith", "title": "A Microarchitecture Implementation Framework for Online Learning with\n  Temporal Neural Networks", "comments": "To be published in ISVLSI 2021. arXiv admin note: substantial text\n  overlap with arXiv:2009.00457", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Neural Networks (TNNs) are spiking neural networks that use time as\na resource to represent and process information, similar to the mammalian\nneocortex. In contrast to compute-intensive deep neural networks that employ\nseparate training and inference phases, TNNs are capable of extremely efficient\nonline incremental/continual learning and are excellent candidates for building\nedge-native sensory processing units. This work proposes a microarchitecture\nframework for implementing TNNs using standard CMOS. Gate-level implementations\nof three key building blocks are presented: 1) multi-synapse neurons, 2)\nmulti-neuron columns, and 3) unsupervised and supervised online learning\nalgorithms based on Spike Timing Dependent Plasticity (STDP). The proposed\nmicroarchitecture is embodied in a set of characteristic scaling equations for\nassessing the gate count, area, delay and power for any TNN design.\nPost-synthesis results (in 45nm CMOS) for the proposed designs are presented,\nand their online incremental learning capability is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:59:54 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 21:51:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Nair", "Harideep", ""], ["Shen", "John Paul", ""], ["Smith", "James E.", ""]]}, {"id": "2105.13336", "submitter": "Kaixin Zhang", "authors": "Kaixin Zhang, Hongzhi Wang, Tongxin Li, Han Hu, Jiye Qiu, Songling Zou", "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduler method\n  towards multiple dynamic workloads system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been an area of intense researching. However, as\na kind of computing intensive task, deep learning highly relies on the the\nscale of the GPU memory, which is usually expensive and scarce. Although there\nare some extensive works have been proposed for dynamic GPU memory management,\nthey are hard to be applied to systems with multitasking dynamic workloads,\nsuch as in-database machine learning system.\n  In this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, with taking the multitasking\ndynamic workloads into consideration. As far as we know, TENSILE is the first\nmethod which is designed to manage multiple workloads' GPU memory using. We\nimplement TENSILE on our own deep learning framework, and evaluated its\nperformance. The experiment results shows that our method can achieve less time\noverhead than prior works with more GPU memory saved.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:46:16 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:31:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Kaixin", ""], ["Wang", "Hongzhi", ""], ["Li", "Tongxin", ""], ["Hu", "Han", ""], ["Qiu", "Jiye", ""], ["Zou", "Songling", ""]]}, {"id": "2105.13585", "submitter": "Clifford Bohm", "authors": "Douglas Kirkpatrick, Victoria Cao, Clifford Bohm", "title": "Fragmentation; a Tool for Finding Information, Encryption and Data Flow\n  in Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.NE math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new information-theoretic measure, fragmentation (F) which can\nbe used to determine how fragmented predictive information is in a system. The\nconcept can be extended to generate fragmentation matrices that can illustrate\ninformation flows through digital brains, in the form of directed graphs.\nFragmentation and fragmentation matrices can provide new insights into digital\nbrains structure and function, in other words, how causal digital networks\n\"think\" and process information. In addition to describing F we demonstrate how\nit can be used to examine how complex processing arises in neural networks,\nincluding differences in lifetime processing and incidents of incidental\nencryption.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 04:49:14 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kirkpatrick", "Douglas", ""], ["Cao", "Victoria", ""], ["Bohm", "Clifford", ""]]}, {"id": "2105.13971", "submitter": "Carlos Gershenson", "authors": "Carlos Gershenson and Jitka Cejkova", "title": "Artificial life: sustainable self-replicating systems", "comments": "Short review chapter, to be included in EPS Grand Challenges 2050\n  book", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nature has found one method of organizing living matter, but maybe there are\nalso other options -- not yet discovered -- on how to create life. To study the\nlife as it could be is the objective of an interdisciplinary field called\nArtificial Life (commonly abbreviated as ALife). The word \"artificial\" refers\nto the fact that humans are involved in the creation process. The results might\nbe completely unlike natural forms of life, not only because of their chemical\ncomposition, but even some computer programs exhibiting life-like behaviours\ninterest ALife researchers.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:54:23 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Gershenson", "Carlos", ""], ["Cejkova", "Jitka", ""]]}, {"id": "2105.14039", "submitter": "Andrew Lampinen", "authors": "Andrew Kyle Lampinen, Stephanie C.Y. Chan, Andrea Banino, Felix Hill", "title": "Towards mental time travel: a hierarchical memory for reinforcement\n  learning agents", "comments": "10 pages main text; 22 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning agents often forget details of the past, especially\nafter delays or distractor tasks. Agents with common memory architectures\nstruggle to recall and integrate across multiple timesteps of a past event, or\neven to recall the details of a single timestep that is followed by distractor\ntasks. To address these limitations, we propose a Hierarchical Transformer\nMemory (HTM), which helps agents to remember the past in detail. HTM stores\nmemories by dividing the past into chunks, and recalls by first performing\nhigh-level attention over coarse summaries of the chunks, and then performing\ndetailed attention within only the most relevant chunks. An agent with HTM can\ntherefore \"mentally time-travel\" -- remember past events in detail without\nattending to all intervening events. We show that agents with HTM substantially\noutperform agents with other memory architectures at tasks requiring long-term\nrecall, retention, or reasoning over memory. These include recalling where an\nobject is hidden in a 3D environment, rapidly learning to navigate efficiently\nin a new neighborhood, and rapidly learning and retaining new object names.\nAgents with HTM can extrapolate to task sequences an order of magnitude longer\nthan they were trained on, and can even generalize zero-shot from a\nmeta-learning setting to maintaining knowledge across episodes. HTM improves\nagent sample efficiency, generalization, and generality (by solving tasks that\npreviously required specialized architectures). Our work is a step towards\nagents that can learn, interact, and adapt in complex and temporally-extended\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:12:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lampinen", "Andrew Kyle", ""], ["Chan", "Stephanie C. Y.", ""], ["Banino", "Andrea", ""], ["Hill", "Felix", ""]]}, {"id": "2105.14092", "submitter": "Pawe{\\l} Wawrzy\\'nski", "authors": "{\\L}ukasz Neumann, Pawe{\\l} Wawrzy\\'nski", "title": "Deep Memory Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are key tools for sequential data processing.\nExisting architectures support only a limited class of operations that these\nnetworks can apply to their memory state. In this paper, we address this issue\nand introduce a recurrent neural module called Deep Memory Update (DMU). This\nmodule is an alternative to well-established LSTM and GRU. However, it uses a\nuniversal function approximator to process its lagged memory state. In\naddition, the module normalizes the lagged memory to avoid gradient exploding\nor vanishing in backpropagation through time. The subnetwork that transforms\nthe memory state of DMU can be arbitrary. Experimental results presented here\nconfirm that the previously mentioned properties of the network allow it to\ncompete with and often outperform state-of-the-art architectures such as LSTM\nand GRU.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:24:00 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 13:28:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Neumann", "\u0141ukasz", ""], ["Wawrzy\u0144ski", "Pawe\u0142", ""]]}, {"id": "2105.14192", "submitter": "Tarik A. Rashid", "authors": "Wu Chao, Mohammad Khishe, Mokhtar Mohammadi, Sarkhel H. Taher Karim,\n  Tarik A. Rashid", "title": "Evolving Deep Convolutional Neural Network by Hybrid Sine-Cosine and\n  Extreme Learning Machine for Real-time COVID19 Diagnosis from X-Ray Images", "comments": "28 pages, Soft Computing, 2021", "journal-ref": null, "doi": "10.1007/s00500-021-05839-6", "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID19 pandemic globally and significantly has affected the life and\nhealth of many communities. The early detection of infected patients is\neffective in fighting COVID19. Using radiology (X-Ray) images is perhaps the\nfastest way to diagnose the patients. Thereby, deep Convolutional Neural\nNetworks (CNNs) can be considered as applicable tools to diagnose COVID19\npositive cases. Due to the complicated architecture of a deep CNN, its\nreal-time training and testing become a challenging problem. This paper\nproposes using the Extreme Learning Machine (ELM) instead of the last fully\nconnected layer to address this deficiency. However, the parameters' stochastic\ntuning of ELM's supervised section causes the final model unreliability.\nTherefore, to cope with this problem and maintain network reliability, the\nsine-cosine algorithm was utilized to tune the ELM's parameters. The designed\nnetwork is then benchmarked on the COVID-Xray-5k dataset, and the results are\nverified by a comparative study with canonical deep CNN, ELM optimized by\ncuckoo search, ELM optimized by genetic algorithm, and ELM optimized by whale\noptimization algorithm. The proposed approach outperforms comparative\nbenchmarks with a final accuracy of 98.83% on the COVID-Xray-5k dataset,\nleading to a relative error reduction of 2.33% compared to a canonical deep\nCNN. Even more critical, the designed network's training time is only 0.9421\nmilliseconds and the overall detection test time for 3100 images is 2.721\nseconds.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 19:40:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chao", "Wu", ""], ["Khishe", "Mohammad", ""], ["Mohammadi", "Mokhtar", ""], ["Karim", "Sarkhel H. Taher", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "2105.14383", "submitter": "Aman Bhargava", "authors": "Aman Bhargava, Mohammad R. Rezaei, Milad Lankarany", "title": "Gradient-Free Neural Network Training via Synaptic-Level Reinforcement\n  Learning", "comments": "10 pages, 3 figures, submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An ongoing challenge in neural information processing is: how do neurons\nadjust their connectivity to improve task performance over time (i.e.,\nactualize learning)? It is widely believed that there is a consistent,\nsynaptic-level learning mechanism in specific brain regions that actualizes\nlearning. However, the exact nature of this mechanism remains unclear. Here we\npropose an algorithm based on reinforcement learning (RL) to generate and apply\na simple synaptic-level learning policy for multi-layer perceptron (MLP)\nmodels. In this algorithm, the action space for each MLP synapse consists of a\nsmall increase, decrease, or null action on the synapse weight, and the state\nfor each synapse consists of the last two actions and reward signals. A binary\nreward signal indicates improvement or deterioration in task performance. The\nstatic policy produces superior training relative to the adaptive policy and is\nagnostic to activation function, network shape, and task. Trained MLPs yield\ncharacter recognition performance comparable to identically shaped networks\ntrained with gradient descent. 0 hidden unit character recognition tests\nyielded an average validation accuracy of 88.28%, 1.86$\\pm$0.47% higher than\nthe same MLP trained with gradient descent. 32 hidden unit character\nrecognition tests yielded an average validation accuracy of 88.45%,\n1.11$\\pm$0.79% lower than the same MLP trained with gradient descent. The\nrobustness and lack of reliance on gradient computations opens the door for new\ntechniques for training difficult-to-differentiate artificial neural networks\nsuch as spiking neural networks (SNNs) and recurrent neural networks (RNNs).\nFurther, the method's simplicity provides a unique opportunity for further\ndevelopment of local rule-driven multi-agent connectionist models for machine\nintelligence analogous to cellular automata.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 22:26:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bhargava", "Aman", ""], ["Rezaei", "Mohammad R.", ""], ["Lankarany", "Milad", ""]]}, {"id": "2105.14399", "submitter": "David Mac\\^edo", "authors": "David Mac\\^edo, Teresa Ludermir", "title": "Improving Entropic Out-of-Distribution Detection using Isometric\n  Distances and the Minimum Distance Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:55:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:18:07 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 04:02:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mac\u00eado", "David", ""], ["Ludermir", "Teresa", ""]]}, {"id": "2105.14412", "submitter": "Andrei Velichko", "authors": "Hanif Heidari and Andrei Velichko", "title": "An improved LogNNet classifier for IoT application", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet of things devices suffer of low memory while good accuracy is\nneeded. Designing suitable algorithms is vital in this subject. This paper\nproposes a feed forward LogNNet neural network which uses a semi-linear Henon\ntype discrete chaotic map to classify MNIST-10 dataset. The model is composed\nof reservoir part and trainable classifier. The aim of reservoir part is\ntransforming the inputs to maximize the classification accuracy using a special\nmatrix filing method and a time series generated by the chaotic map. The\nparameters of the chaotic map are optimized using particle swarm optimization\nwith random immigrants. The results show that the proposed LogNNet/Henon\nclassifier has higher accuracy and same RAM saving comparable to the original\nversion of LogNNet and has broad prospects for implementation in IoT devices.\nIn addition, the relation between the entropy and accuracy of the\nclassification is investigated. It is shown that there exists a direct relation\nbetween the value of entropy and accuracy of the classification.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:12:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Heidari", "Hanif", ""], ["Velichko", "Andrei", ""]]}, {"id": "2105.14506", "submitter": "Jivitesh Sharma", "authors": "Jivitesh Sharma, Rohan Yadav, Ole-Christoffer Granmo and Lei Jiao", "title": "Human Interpretable AI: Enhancing Tsetlin Machine Stochasticity with\n  Drop Clause", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we introduce a novel variant of the Tsetlin machine (TM)\nthat randomly drops clauses, the key learning elements of a TM. In effect, TM\nwith drop clause ignores a random selection of the clauses in each epoch,\nselected according to a predefined probability. In this way, additional\nstochasticity is introduced in the learning phase of TM. Along with producing\nmore distinct and well-structured patterns that improve the performance, we\nalso show that dropping clauses increases learning robustness. To explore the\neffects clause dropping has on accuracy, training time, and interpretability,\nwe conduct extensive experiments on various benchmark datasets in natural\nlanguage processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and\nCIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2x to\n4x faster learning. We further employ the Convolutional TM to document\ninterpretable results on the CIFAR10 dataset. To the best of our knowledge,\nthis is the first time an interpretable machine learning algorithm has been\nused to produce pixel-level human-interpretable results on CIFAR10. Also,\nunlike previous interpretable methods that focus on attention visualisation or\ngradient interpretability, we show that the TM is a more general interpretable\nmethod. That is, by producing rule-based propositional logic expressions that\nare \\emph{human}-interpretable, the TM can explain how it classifies a\nparticular instance at the pixel level for computer vision and at the word\nlevel for NLP.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:29:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sharma", "Jivitesh", ""], ["Yadav", "Rohan", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2105.14614", "submitter": "Danielle Azar", "authors": "Andrew Nader and Danielle Azar", "title": "Evolution of Activation Functions: An Empirical Investigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hyper-parameters of a neural network are traditionally designed through a\ntime consuming process of trial and error that requires substantial expert\nknowledge. Neural Architecture Search (NAS) algorithms aim to take the human\nout of the loop by automatically finding a good set of hyper-parameters for the\nproblem at hand. These algorithms have mostly focused on hyper-parameters such\nas the architectural configurations of the hidden layers and the connectivity\nof the hidden neurons, but there has been relatively little work on automating\nthe search for completely new activation functions, which are one of the most\ncrucial hyper-parameters to choose. There are some widely used activation\nfunctions nowadays which are simple and work well, but nonetheless, there has\nbeen some interest in finding better activation functions. The work in the\nliterature has mostly focused on designing new activation functions by hand, or\nchoosing from a set of predefined functions while this work presents an\nevolutionary algorithm to automate the search for completely new activation\nfunctions. We compare these new evolved activation functions to other existing\nand commonly used activation functions. The results are favorable and are\nobtained from averaging the performance of the activation functions found over\n30 runs, with experiments being conducted on 10 different datasets and\narchitectures to ensure the statistical robustness of the study.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:08:20 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Nader", "Andrew", ""], ["Azar", "Danielle", ""]]}, {"id": "2105.14639", "submitter": "Kiran Lekkala", "authors": "Kiran Lekkala, Laurent Itti", "title": "Shaped Policy Search for Evolutionary Strategies using Waypoints", "comments": "Presented at the International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we try to improve exploration in Blackbox methods,\nparticularly Evolution strategies (ES), when applied to Reinforcement Learning\n(RL) problems where intermediate waypoints/subgoals are available. Since\nEvolutionary strategies are highly parallelizable, instead of extracting just a\nscalar cumulative reward, we use the state-action pairs from the trajectories\nobtained during rollouts/evaluations, to learn the dynamics of the agent. The\nlearnt dynamics are then used in the optimization procedure to speed-up\ntraining. Lastly, we show how our proposed approach is universally applicable\nby presenting results from experiments conducted on Carla driving and UR5\nrobotic arm simulators.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 22:15:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lekkala", "Kiran", ""], ["Itti", "Laurent", ""]]}, {"id": "2105.14677", "submitter": "Biswadeep Chakraborty", "authors": "Biswadeep Chakraborty, Saibal Mukhopadhyay", "title": "Characterization of Generalizability of Spike Time Dependent Plasticity\n  trained Spiking Neural Networks", "comments": "15 pages, submitted to Frontiers in Neuroscience. arXiv admin note:\n  text overlap with arXiv:2010.08195, arXiv:2006.09313 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Spiking Neural Network (SNN) trained with Spike Time Dependent Plasticity\n(STDP) is a neuro-inspired unsupervised learning method for various machine\nlearning applications. This paper studies the generalizability properties of\nthe STDP learning processes using the Hausdorff dimension of the trajectories\nof the learning algorithm. The paper analyzes the effects of STDP learning\nmodels and associated hyper-parameters on the generalizability properties of an\nSNN and characterizes the generalizability vs learnability trade-off in an SNN.\nThe analysis is used to develop a Bayesian optimization approach to optimize\nthe hyper-parameters for an STDP model to improve the generalizability\nproperties of an SNN.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:19:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chakraborty", "Biswadeep", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "2105.14753", "submitter": "Am\\'elie Gruel", "authors": "Am\\'elie Gruel and Jean Martinet", "title": "Bio-inspired visual attention for silicon retinas based on spiking\n  neural networks applied to pattern classification", "comments": "6 pages, 3 figures. To be published in Content-Based Multimedia\n  Indexing (CBMI) 2021, Lille, France. This work was supported by the European\n  Union's ERA-NET CHIST-ERA 2018 research and innovation programme under grant\n  agreement ANR-19-CHR3-0008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual attention can be defined as the behavioral and cognitive process of\nselectively focusing on a discrete aspect of sensory cues while disregarding\nother perceivable information. This biological mechanism, more specifically\nsaliency detection, has long been used in multimedia indexing to drive the\nanalysis only on relevant parts of images or videos for further processing.\n  The recent advent of silicon retinas (or event cameras -- sensors that\nmeasure pixel-wise changes in brightness and output asynchronous events\naccordingly) raises the question of how to adapt attention and saliency to the\nunconventional type of such sensors' output. Silicon retina aims to reproduce\nthe biological retina behaviour. In that respect, they produce punctual events\nin time that can be construed as neural spikes and interpreted as such by a\nneural network.\n  In particular, Spiking Neural Networks (SNNs) represent an asynchronous type\nof artificial neural network closer to biology than traditional artificial\nnetworks, mainly because they seek to mimic the dynamics of neural membrane and\naction potentials over time. SNNs receive and process information in the form\nof spike trains. Therefore, they make for a suitable candidate for the\nefficient processing and classification of incoming event patterns measured by\nsilicon retinas. In this paper, we review the biological background behind the\nattentional mechanism, and introduce a case study of event videos\nclassification with SNNs, using a biology-grounded low-level computational\nattention mechanism, with interesting preliminary results.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:34:13 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gruel", "Am\u00e9lie", ""], ["Martinet", "Jean", ""]]}, {"id": "2105.14835", "submitter": "Christoph Hertrich", "authors": "Christoph Hertrich, Amitabh Basu, Marco Di Summa, Martin Skutella", "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.NE math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute to a better understanding of the class of functions that is\nrepresented by a neural network with ReLU activations and a given architecture.\nUsing techniques from mixed-integer optimization, polyhedral theory, and\ntropical geometry, we provide a mathematical counterbalance to the universal\napproximation theorems which suggest that a single hidden layer is sufficient\nfor learning tasks. In particular, we investigate whether the class of exactly\nrepresentable functions strictly increases by adding more layers (with no\nrestrictions on size). This problem has potential impact on algorithmic and\nstatistical aspects because of the insight it provides into the class of\nfunctions represented by neural hypothesis classes. However, to the best of our\nknowledge, this question has not been investigated in the neural network\nliterature. We also present upper bounds on the sizes of neural networks\nrequired to represent functions in these neural hypothesis classes.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:49:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hertrich", "Christoph", ""], ["Basu", "Amitabh", ""], ["Di Summa", "Marco", ""], ["Skutella", "Martin", ""]]}, {"id": "2105.14849", "submitter": "Albert Zeyer", "authors": "Albert Zeyer and Ralf Schl\\\"uter and Hermann Ney", "title": "Why does CTC result in peaky behavior?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE cs.SD eess.AS math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The peaky behavior of CTC models is well known experimentally. However, an\nunderstanding about why peaky behavior occurs is missing, and whether this is a\ngood property. We provide a formal analysis of the peaky behavior and gradient\ndescent convergence properties of the CTC loss and related training criteria.\nOur analysis provides a deep understanding why peaky behavior occurs and when\nit is suboptimal. On a simple example which should be trivial to learn for any\nmodel, we prove that a feed-forward neural network trained with CTC from\nuniform initialization converges towards peaky behavior with a 100% error rate.\nOur analysis further explains why CTC only works well together with the blank\nlabel. We further demonstrate that peaky behavior does not occur on other\nrelated losses including a label prior model, and that this improves\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:03:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:44:23 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2105.15074", "submitter": "Sergio Contreras", "authors": "Vannessa de J. Duarte, Paul Leger, Sergio Contreras and Hiroaki Fukuda", "title": "Detecting Fetal Alcohol Spectrum Disorder in children using Artificial\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fetal alcohol spectrum disorder (FASD) is a syndrome whose only difference\ncompared to other children's conditions is the mother's alcohol consumption\nduring pregnancy. An earlier diagnosis of FASD improving the quality of life of\nchildren and adolescents. For this reason, this study focus on evaluating the\nuse of the artificial neural network (ANN) to classify children with FASD and\nexplore how accurate it is. ANN has been used to diagnose cancer, diabetes, and\nother diseases in the medical area, being a tool that presents good results.\nThe data used is from a battery of tests from children for 5-18 years old\n(include tests of psychometric, saccade eye movement, and diffusion tensor\nimaging (DTI)). We study the different configurations of ANN with dense layers.\nThe first one predicts 75\\% of the outcome correctly for psychometric data. The\nothers models include a feature layer, and we used it to predict FASD using\nevery test individually. The models accurately predict over 70\\% of the cases,\nand psychometric and memory guides predict over 88\\% accuracy. The results\nsuggest that the ANN approach is a competitive and efficient methodology to\ndetect FASD. However, we could be careful in used as a diagnostic technique.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:02:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Duarte", "Vannessa de J.", ""], ["Leger", "Paul", ""], ["Contreras", "Sergio", ""], ["Fukuda", "Hiroaki", ""]]}]