[{"id": "1708.00111", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Graham Neubig, Chris Dyer and Taylor Berg-Kirkpatrick", "title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural\n  Sequence Models", "comments": "Updated for clarity and notational consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beam search is a desirable choice of test-time decoding algorithm for neural\nsequence models because it potentially avoids search errors made by simpler\ngreedy methods. However, typical cross entropy training procedures for these\nmodels do not directly consider the behaviour of the final decoding method. As\na result, for cross-entropy trained models, beam decoding can sometimes yield\nreduced test performance when compared with greedy decoding. In order to train\nmodels that can more effectively make use of beam search, we propose a new\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\napproach, we form a sub-differentiable surrogate objective by introducing a\nnovel continuous approximation of the beam search decoding procedure. In\nexperiments, we show that optimizing this new training objective yields\nsubstantially better results on two sequence tasks (Named Entity Recognition\nand CCG Supertagging) when compared with both cross entropy trained greedy\ndecoding and cross entropy trained beam decoding baselines.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 00:16:08 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 19:29:30 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Goyal", "Kartik", ""], ["Neubig", "Graham", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1708.00214", "submitter": "Jan Botha", "authors": "Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David\n  Weiss, Ryan McDonald, Slav Petrov", "title": "Natural Language Processing with Small Feed-Forward Networks", "comments": "EMNLP 2017 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that small and shallow feed-forward neural networks can achieve near\nstate-of-the-art results on a range of unstructured and structured language\nprocessing tasks while being considerably cheaper in memory and computational\nrequirements than deep recurrent models. Motivated by resource-constrained\nenvironments like mobile phones, we showcase simple techniques for obtaining\nsuch small neural network models, and investigate different tradeoffs when\ndeciding how to allocate a small memory budget.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:13:44 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Botha", "Jan A.", ""], ["Pitler", "Emily", ""], ["Ma", "Ji", ""], ["Bakalov", "Anton", ""], ["Salcianu", "Alex", ""], ["Weiss", "David", ""], ["McDonald", "Ryan", ""], ["Petrov", "Slav", ""]]}, {"id": "1708.00331", "submitter": "Junhua Wu", "authors": "Junhua Wu, Markus Wagner, Sergey Polyakovskiy and Frank Neumann", "title": "Exact Approaches for the Travelling Thief Problem", "comments": "13 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many evolutionary and constructive heuristic approaches have been introduced\nin order to solve the Traveling Thief Problem (TTP). However, the accuracy of\nsuch approaches is unknown due to their inability to find global optima. In\nthis paper, we propose three exact algorithms and a hybrid approach to the TTP.\nWe compare these with state-of-the-art approaches to gather a comprehensive\noverview on the accuracy of heuristic methods for solving small TTP instances.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 13:56:27 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Wu", "Junhua", ""], ["Wagner", "Markus", ""], ["Polyakovskiy", "Sergey", ""], ["Neumann", "Frank", ""]]}, {"id": "1708.00339", "submitter": "Yanjun  Qi Dr.", "authors": "Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi", "title": "Attend and Predict: Understanding Gene Regulation by Selective Attention\n  on Chromatin", "comments": "12 pages; At NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen a revolution in genomic technologies that enable a\nflood of genome-wide profiling of chromatin marks. Recent literature tried to\nunderstand gene regulation by predicting gene expression from large-scale\nchromatin measurements. Two fundamental challenges exist for such learning\ntasks: (1) genome-wide chromatin signals are spatially structured,\nhigh-dimensional and highly modular; and (2) the core aim is to understand what\nare the relevant factors and how they work together? Previous studies either\nfailed to model complex dependencies among input signals or relied on separate\nfeature analysis to explain the decisions. This paper presents an\nattention-based deep learning approach; we call AttentiveChrome, that uses a\nunified architecture to model and to interpret dependencies among chromatin\nfactors for controlling gene regulation. AttentiveChrome uses a hierarchy of\nmultiple Long short-term memory (LSTM) modules to encode the input signals and\nto model how various chromatin marks cooperate automatically. AttentiveChrome\ntrains two levels of attention jointly with the target prediction, enabling it\nto attend differentially to relevant marks and to locate important positions\nper mark. We evaluate the model across 56 different cell types (tasks) in\nhuman. Not only is the proposed architecture more accurate, but its attention\nscores also provide a better interpretation than state-of-the-art feature\nvisualization methods such as saliency map.\n  Code and data are shared at www.deepchrome.org\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:06:12 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 17:20:13 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 16:40:55 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Singh", "Ritambhara", ""], ["Lanchantin", "Jack", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""]]}, {"id": "1708.00580", "submitter": "Gang Wang", "authors": "Gang Wang", "title": "A Novel Neural Network Model Specified for Representing Logical\n  Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With computers to handle more and more complicated things in variable\nenvironments, it becomes an urgent requirement that the artificial intelligence\nhas the ability of automatic judging and deciding according to numerous\nspecific conditions so as to deal with the complicated and variable cases. ANNs\ninspired by brain is a good candidate. However, most of current numeric ANNs\nare not good at representing logical relations because these models still try\nto represent logical relations in the form of ratio based on functional\napproximation. On the other hand, researchers have been trying to design novel\nneural network models to make neural network model represent logical relations.\nIn this work, a novel neural network model specified for representing logical\nrelations is proposed and applied. New neurons and multiple kinds of links are\ndefined. Inhibitory links are introduced besides exciting links. Different from\ncurrent numeric ANNs, one end of an inhibitory link connects an exciting link\nrather than a neuron. Inhibitory links inhibit the connected exciting links\nconditionally to make this neural network model represent logical relations\ncorrectly. This model can simulate the operations of Boolean logic gates, and\nconstruct complex logical relations with the advantages of simpler neural\nnetwork structures than recent works in this area. This work provides some\nideas to make neural networks represent logical relations more directly and\nefficiently, and the model could be used as the complement to current numeric\nANN to deal with logical issues and expand the application areas of ANN.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 02:35:20 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Wang", "Gang", ""]]}, {"id": "1708.00587", "submitter": "Sibaek Seong", "authors": "Si-Baek Seong, Chongwon Pae, and Hae-Jeong Park", "title": "Geometric Convolutional Neural Network for Analyzing Surface-Based\n  Neuroimaging Data", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional CNN, widely used for two-dimensional images, however, is not\ndirectly applicable to non-regular geometric surface, such as a cortical\nthickness. We propose Geometric CNN (gCNN) that deals with data representation\nover a spherical surface and renders pattern recognition in a multi-shell mesh\nstructure. The classification accuracy for sex was significantly higher than\nthat of SVM and image based CNN. It only uses MRI thickness data to classify\ngender but this method can expand to classify disease from other MRI or fMRI\ndata\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 03:23:17 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Seong", "Si-Baek", ""], ["Pae", "Chongwon", ""], ["Park", "Hae-Jeong", ""]]}, {"id": "1708.00630", "submitter": "Sujith Ravi", "authors": "Sujith Ravi", "title": "ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become ubiquitous for applications related to\nvisual recognition and language understanding tasks. However, it is often\nprohibitive to use typical neural networks on devices like mobile phones or\nsmart watches since the model sizes are huge and cannot fit in the limited\nmemory available on such devices. While these devices could make use of machine\nlearning models running on high-performance data centers with CPUs or GPUs,\nthis is not feasible for many applications because data can be privacy\nsensitive and inference needs to be performed directly \"on\" device.\n  We introduce a new architecture for training compact neural networks using a\njoint optimization framework. At its core lies a novel objective that jointly\ntrains using two different types of networks--a full trainer neural network\n(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with\na simpler \"projection\" network that leverages random projections to transform\ninputs or intermediate representations into bits. The simpler network encodes\nlightweight and efficient-to-compute operations in bit space with a low memory\nfootprint. The two networks are trained jointly using backpropagation, where\nthe projection network learns from the full network similar to apprenticeship\nlearning. Once trained, the smaller network can be used directly for inference\nat low memory and computation cost. We demonstrate the effectiveness of the new\napproach at significantly shrinking the memory requirements of different types\nof neural networks while preserving good accuracy on visual recognition and\ntext classification tasks. We also study the question \"how many neural bits are\nrequired to solve a given task?\" using the new framework and show empirical\nresults contrasting model predictive capacity (in bits) versus accuracy on\nseveral datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 07:58:45 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 10:05:09 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Ravi", "Sujith", ""]]}, {"id": "1708.00631", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos", "title": "On the Importance of Consistency in Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain that the difficulties of training deep neural networks come from a\nsyndrome of three consistency issues. This paper describes our efforts in their\nanalysis and treatment. The first issue is the training speed inconsistency in\ndifferent layers. We propose to address it with an intuitive,\nsimple-to-implement, low footprint second-order method. The second issue is the\nscale inconsistency between the layer inputs and the layer residuals. We\nexplain how second-order information provides favorable convenience in removing\nthis roadblock. The third and most challenging issue is the inconsistency in\nresidual propagation. Based on the fundamental theorem of linear algebra, we\nprovide a mathematical characterization of the famous vanishing gradient\nproblem. Thus, an important design principle for future optimization and neural\nnetwork design is derived. We conclude this paper with the construction of a\nnovel contractive neural network.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:05:09 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ye", "Chengxi", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1708.01009", "submitter": "Stephen Merity", "authors": "Stephen Merity, Bryan McCann, Richard Socher", "title": "Revisiting Activation Regularization for Language RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) serve as a fundamental building block for\nmany sequence tasks across natural language processing. Recent research has\nfocused on recurrent dropout techniques or custom RNN cells in order to improve\nperformance. Both of these can require substantial modifications to the machine\nlearning model or to the underlying RNN configurations. We revisit traditional\nregularization techniques, specifically L2 regularization on RNN activations\nand slowness regularization over successive hidden states, to improve the\nperformance of RNNs on the task of language modeling. Both of these techniques\nrequire minimal modification to existing RNN architectures and result in\nperformance improvements comparable or superior to more complicated\nregularization techniques or custom cell architectures. These regularization\ntechniques can be used without any modification on optimized LSTM\nimplementations such as the NVIDIA cuDNN LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 05:53:53 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Merity", "Stephen", ""], ["McCann", "Bryan", ""], ["Socher", "Richard", ""]]}, {"id": "1708.01146", "submitter": "Aimin Zhou", "authors": "Jinyuan Zhang, Aimin Zhou, Ke Tang, and Guixu Zhang", "title": "Preselection via Classification: A Case Study on Evolutionary\n  Multiobjective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary algorithms, a preselection operator aims to select the\npromising offspring solutions from a candidate offspring set. It is usually\nbased on the estimated or real objective values of the candidate offspring\nsolutions. In a sense, the preselection can be treated as a classification\nprocedure, which classifies the candidate offspring solutions into promising\nones and unpromising ones. Following this idea, we propose a classification\nbased preselection (CPS) strategy for evolutionary multiobjective optimization.\nWhen applying classification based preselection, an evolutionary algorithm\nmaintains two external populations (training data set) that consist of some\nselected good and bad solutions found so far; then it trains a classifier based\non the training data set in each generation. Finally it uses the classifier to\nfilter the unpromising candidate offspring solutions and choose a promising one\nfrom the generated candidate offspring set for each parent solution. In such\ncases, it is not necessary to estimate or evaluate the objective values of the\ncandidate offspring solutions. The classification based preselection is applied\nto three state-of-the-art multiobjective evolutionary algorithms (MOEAs) and is\nempirically studied on two sets of test instances. The experimental results\nsuggest that classification based preselection can successfully improve the\nperformance of these MOEAs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 14:02:21 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Zhang", "Jinyuan", ""], ["Zhou", "Aimin", ""], ["Tang", "Ke", ""], ["Zhang", "Guixu", ""]]}, {"id": "1708.01368", "submitter": "Vin\\'icius Veloso de Melo", "authors": "Vin\\'icius Veloso de Melo", "title": "A novel metaheuristic method for solving constrained engineering\n  optimization problems: Drone Squadron Optimization", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several constrained optimization problems have been adequately solved over\nthe years thanks to advances in the metaheuristics area. In this paper, we\nevaluate a novel self-adaptive and auto-constructive metaheuristic called Drone\nSquadron Optimization (DSO) in solving constrained engineering design problems.\nThis paper evaluates DSO with death penalty on three widely tested engineering\ndesign problems. Results show that the proposed approach is competitive with\nsome very popular metaheuristics.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 03:21:23 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["de Melo", "Vin\u00edcius Veloso", ""]]}, {"id": "1708.01571", "submitter": "Dogan Corus", "authors": "Dogan Corus and Pietro S. Oliveto", "title": "Standard Steady State Genetic Algorithms Can Hillclimb Faster than\n  Mutation-only Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining to what extent the real power of genetic algorithms lies in the\nability of crossover to recombine individuals into higher quality solutions is\nan important problem in evolutionary computation. In this paper we show how the\ninterplay between mutation and crossover can make genetic algorithms hillclimb\nfaster than their mutation-only counterparts. We devise a Markov Chain\nframework that allows to rigorously prove an upper bound on the runtime of\nstandard steady state genetic algorithms to hillclimb the OneMax function. The\nbound establishes that the steady-state genetic algorithms are 25% faster than\nall standard bit mutation-only evolutionary algorithms with static mutation\nrate up to lower order terms for moderate population sizes. The analysis also\nsuggests that larger populations may be faster than populations of size 2. We\npresent a lower bound for a greedy (2+1) GA that matches the upper bound for\npopulations larger than 2, rigorously proving that 2 individuals cannot\noutperform larger population sizes under greedy selection and greedy crossover\nup to lower order terms. In complementary experiments the best population size\nis greater than 2 and the greedy genetic algorithms are faster than standard\nones, further suggesting that the derived lower bound also holds for the\nstandard steady state (2+1) GA.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 16:20:23 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 09:41:11 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Corus", "Dogan", ""], ["Oliveto", "Pietro S.", ""]]}, {"id": "1708.01659", "submitter": "Emmanuel Osegi", "authors": "V.I. Anireh and EN Osegi", "title": "HTM-MAT: An online prediction software toolbox based on cortical machine\n  learning algorithm", "comments": "This research is currently under review in a Journal. Contents might\n  vary from final published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  HTM-MAT is a MATLAB based toolbox for implementing cortical learning\nalgorithms (CLA) including related cortical-like algorithms that possesses\nspatiotemporal properties. CLA is a suite of predictive machine learning\nalgorithms developed by Numenta Inc. and is based on the hierarchical temporal\nmemory (HTM). This paper presents an implementation of HTM-MAT with several\nillustrative examples including several toy datasets and compared with two\nsequence learning applications employing state-of-the-art algorithms - the\nrecurrentjs based on the Long Short-Term Memory (LSTM) algorithm and OS-ELM\nwhich is based on an online sequential version of the Extreme Learning Machine.\nThe performance of HTM-MAT using two historical benchmark datasets and one real\nworld dataset is also compared with one of the existing sequence learning\napplications, the OS-ELM. The results indicate that HTM-MAT predictions are\nindeed competitive and can outperform OS-ELM in sequential prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 01:45:25 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Anireh", "V. I.", ""], ["Osegi", "EN", ""]]}, {"id": "1708.01897", "submitter": "Ali Teimouri", "authors": "Mikhail Goykhman, Ali Teimouri", "title": "Machine learning in sentiment reconstruction of the simulated stock\n  market", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.physa.2017.11.093", "report-no": null, "categories": "q-fin.TR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we continue the study of the simulated stock market framework\ndefined by the driving sentiment processes. We focus on the market environment\ndriven by the buy/sell trading sentiment process of the Markov chain type. We\napply the methodology of the Hidden Markov Models and the Recurrent Neural\nNetworks to reconstruct the transition probabilities matrix of the Markov\nsentiment process and recover the underlying sentiment states from the observed\nstock price behavior.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 14:30:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Goykhman", "Mikhail", ""], ["Teimouri", "Ali", ""]]}, {"id": "1708.02043", "submitter": "Albert Gatt", "authors": "Marc Tanti, Albert Gatt and Kenneth P. Camilleri", "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\n  Generator?", "comments": "Appears in: Proceedings of the 10th International Conference on\n  Natural Language Generation (INLG'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural image captioning systems, a recurrent neural network (RNN) is\ntypically viewed as the primary `generation' component. This view suggests that\nthe image features should be `injected' into the RNN. This is in fact the\ndominant view in the literature. Alternatively, the RNN can instead be viewed\nas only encoding the previously generated words. This view suggests that the\nRNN should only be used to encode linguistic features and that only the final\nrepresentation should be `merged' with the image features at a later stage.\nThis paper compares these two architectures. We find that, in general, late\nmerging outperforms injection, suggesting that RNNs are better viewed as\nencoders, rather than generators.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 09:01:35 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 15:40:00 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Camilleri", "Kenneth P.", ""]]}, {"id": "1708.02068", "submitter": "Jialin Liu Ph.D", "authors": "Simon M. Lucas, Jialin Liu, Diego P\\'erez-Li\\'ebana", "title": "Efficient Noisy Optimisation with the Sliding Window Compact Genetic\n  Algorithm", "comments": "11 pages, 2 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compact genetic algorithm is an Estimation of Distribution Algorithm for\nbinary optimisation problems. Unlike the standard Genetic Algorithm, no\ncross-over or mutation is involved. Instead, the compact Genetic Algorithm uses\na virtual population represented as a probability distribution over the set of\nbinary strings. At each optimisation iteration, exactly two individuals are\ngenerated by sampling from the distribution, and compared exactly once to\ndetermine a winner and a loser. The probability distribution is then adjusted\nto increase the likelihood of generating individuals similar to the winner.\n  This paper introduces two straightforward variations of the compact Genetic\nAlgorithm, each of which lead to a significant improvement in performance. The\nmain idea is to make better use of each fitness evaluation, by ensuring that\neach evaluated individual is used in multiple win/loss comparisons. The first\nvariation is to sample $n>2$ individuals at each iteration to make $n(n-1)/2$\ncomparisons. The second variation only samples one individual at each iteration\nbut keeps a sliding history window of previous individuals to compare with. We\nevaluate methods on two noisy test problems and show that in each case they\nsignificantly outperform the compact Genetic Algorithm, while maintaining the\nsimplicity of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 11:07:57 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Lucas", "Simon M.", ""], ["Liu", "Jialin", ""], ["P\u00e9rez-Li\u00e9bana", "Diego", ""]]}, {"id": "1708.02182", "submitter": "Stephen Merity", "authors": "Stephen Merity, Nitish Shirish Keskar, Richard Socher", "title": "Regularizing and Optimizing LSTM Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:03:44 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Merity", "Stephen", ""], ["Keskar", "Nitish Shirish", ""], ["Socher", "Richard", ""]]}, {"id": "1708.02238", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Joseph Barfett, Parham Aarabi, Shahrokh Valaee,\n  Errol Colak, Bruce Gray and Tim Dowdell", "title": "A Convolutional Neural Network for Search Term Detection", "comments": "This paper is accepted for presentation at 2017 IEEE 28th Annual\n  International Symposium on Personal, Indoor, and Mobile Radio Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathfinding in hospitals is challenging for patients, visitors, and even\nemployees. Many people have experienced getting lost due to lack of clear\nguidance, large footprint of hospitals, and confusing array of hospital wings.\nIn this paper, we propose Halo; An indoor navigation application based on\nvoice-user interaction to help provide directions for users without assistance\nof a localization system. The main challenge is accurate detection of origin\nand destination search terms. A custom convolutional neural network (CNN) is\nproposed to detect origin and destination search terms from transcription of a\nsubmitted speech query. The CNN is trained based on a set of queries tailored\nspecifically for hospital and clinic environments. Performance of the proposed\nmodel is studied and compared with Levenshtein distance-based word matching.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 03:37:33 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 05:50:51 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 15:54:20 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Barfett", "Joseph", ""], ["Aarabi", "Parham", ""], ["Valaee", "Shahrokh", ""], ["Colak", "Errol", ""], ["Gray", "Bruce", ""], ["Dowdell", "Tim", ""]]}, {"id": "1708.02603", "submitter": "Tae Seung Kang", "authors": "Tae Seung Kang and Arunava Banerjee", "title": "Learning Feedforward and Recurrent Deterministic Spiking Neuron Network\n  Feedback Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning feedback control where the controller is a\nnetwork constructed solely of deterministic spiking neurons. In contrast to\nprevious investigations that were based on a spike rate model of the neuron,\nthe control signal here is determined by the precise temporal positions of\nspikes generated by the output neurons of the network. We model the problem\nformally as a hybrid dynamical system comprised of a closed loop between a\nplant and a spiking neuron network. We derive a novel synaptic weight update\nrule via which the spiking neuron network controller learns to hold process\nvariables at desired set points. The controller achieves its learning objective\nbased solely on access to the plant's process variables and their derivatives\nwith respect to changing control signals; in particular, it requires no\ninternal model of the plant. We demonstrate the efficacy of the rule by\napplying it to the classical control problem of the cart-pole (inverted\npendulum) and a model of fish locomotion. Experiments show that the proposed\ncontroller has a stability region comparable to a traditional PID controller,\nits trajectories differ qualitatively from those of a PID controller, and in\nmany instances the controller achieves its objective using very sparse spike\ntrain outputs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 18:42:17 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 22:54:29 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Kang", "Tae Seung", ""], ["Banerjee", "Arunava", ""]]}, {"id": "1708.02735", "submitter": "Stanislav Fort", "authors": "Stanislav Fort", "title": "Gaussian Prototypical Networks for Few-Shot Learning on Omniglot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture for $k$-shot classification on the Omniglot\ndataset. Building on prototypical networks, we extend their architecture to\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\nbetween images and embedding vectors, and use their clustering for\nclassification. In our model, a part of the encoder output is interpreted as a\nconfidence region estimate about the embedding point, and expressed as a\nGaussian covariance matrix. Our network then constructs a direction and class\ndependent distance metric on the embedding space, using uncertainties of\nindividual data points as weights. We show that Gaussian prototypical networks\nare a preferred architecture over vanilla prototypical networks with an\nequivalent number of parameters. We report state-of-the-art performance in\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\nWe explore artificially down-sampling a fraction of images in the training set,\nwhich improves our performance even further. We therefore hypothesize that\nGaussian prototypical networks might perform better in less homogeneous,\nnoisier datasets, which are commonplace in real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:53:31 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Fort", "Stanislav", ""]]}, {"id": "1708.02840", "submitter": "Pawel Cyrta", "authors": "Pawel Cyrta, Tomasz Trzci\\'nski, Wojciech Stokowiec", "title": "Speaker Diarization using Deep Recurrent Convolutional Neural Networks\n  for Speaker Embeddings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-67220-5_10", "report-no": null, "categories": "cs.SD cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method of speaker diarization that employs a\ndeep learning architecture to learn speaker embeddings. In contrast to the\ntraditional approaches that build their speaker embeddings using manually\nhand-crafted spectral features, we propose to train for this purpose a\nrecurrent convolutional neural network applied directly on magnitude\nspectrograms. To compare our approach with the state of the art, we collect and\nrelease for the public an additional dataset of over 6 hours of fully annotated\nbroadcast material. The results of our evaluation on the new dataset and three\nother benchmark datasets show that our proposed method significantly\noutperforms the competitors and reduces diarization error rate by a large\nmargin of over 30% with respect to the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:53:01 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 13:49:45 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Cyrta", "Pawel", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Stokowiec", "Wojciech", ""]]}, {"id": "1708.02975", "submitter": "Daniel Hsu", "authors": "Daniel Hsu", "title": "Anomaly Detection on Graph Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we use variational recurrent neural network to investigate the\nanomaly detection problem on graph time series. The temporal correlation is\nmodeled by the combination of recurrent neural network (RNN) and variational\ninference (VI), while the spatial information is captured by the graph\nconvolutional network. In order to incorporate external factors, we use feature\nextractor to augment the transition of latent variables, which can learn the\ninfluence of external factors. With the target function as accumulative ELBO,\nit is easy to extend this model to on-line method. The experimental study on\ntraffic flow data shows the detection capability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:15:56 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 23:50:17 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Hsu", "Daniel", ""]]}, {"id": "1708.02979", "submitter": "Andrei Turkin", "authors": "Andrei Turkin", "title": "Tikhonov Regularization for Long Short-Term Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a well-known fact that adding noise to the input data often improves\nnetwork performance. While the dropout technique may be a cause of memory loss,\nwhen it is applied to recurrent connections, Tikhonov regularization, which can\nbe regarded as the training with additive noise, avoids this issue naturally,\nthough it implies regularizer derivation for different architectures. In case\nof feedforward neural networks this is straightforward, while for networks with\nrecurrent connections and complicated layers it leads to some difficulties. In\nthis paper, a Tikhonov regularizer is derived for Long-Short Term Memory (LSTM)\nnetworks. Although it is independent of time for simplicity, it considers\ninteraction between weights of the LSTM unit, which in theory makes it possible\nto regularize the unit with complicated dependences by using only one parameter\nthat measures the input data perturbation. The regularizer that is proposed in\nthis paper has three parameters: one to control the regularization process, and\nother two to maintain computation stability while the network is being trained.\nThe theory developed in this paper can be applied to get such regularizers for\ndifferent recurrent neural networks with Hadamard products and Lipschitz\ncontinuous functions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:34:26 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Turkin", "Andrei", ""]]}, {"id": "1708.03417", "submitter": "Seungkyun Hong", "authors": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from\n  Remote Sensing Imagery", "comments": "Under review as a workshop paper at CI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in remote sensing technologies have made it possible to use\nhigh-resolution visual data for weather observation and forecasting tasks. We\npropose the use of multi-layer neural networks for understanding complex\natmospheric dynamics based on multichannel satellite images. The capability of\nour model was evaluated by using a linear regression task for single typhoon\ncoordinates prediction. A specific combination of models and different\nactivation policies enabled us to obtain an interesting prediction result in\nthe northeastern hemisphere (ENH).\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:41:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Hong", "Seungkyun", ""], ["Kim", "Seongchan", ""], ["Joh", "Minsu", ""], ["Song", "Sa-kwang", ""]]}, {"id": "1708.03498", "submitter": "Sjoerd van Steenkiste", "authors": "Klaus Greff, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Neural Expectation Maximization", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real world tasks such as reasoning and physical interaction require\nidentification and manipulation of conceptual entities. A first step towards\nsolving these tasks is the automated discovery of distributed symbol-like\nrepresentations. In this paper, we explicitly formalize this problem as\ninference in a spatial mixture model where each component is parametrized by a\nneural network. Based on the Expectation Maximization framework we then derive\na differentiable clustering method that simultaneously learns how to group and\nrepresent individual entities. We evaluate our method on the (sequential)\nperceptual grouping task and find that it is able to accurately recover the\nconstituent objects. We demonstrate that the learned representations are useful\nfor next-step prediction.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 10:17:23 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 15:14:47 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Greff", "Klaus", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1708.03535", "submitter": "Iman Malik", "authors": "Iman Malik, Carl Henrik Ek", "title": "Neural Translation of Musical Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music is an expressive form of communication often used to convey emotion in\nscenarios where \"words are not enough\". Part of this information lies in the\nmusical composition where well-defined language exists. However, a significant\namount of information is added during a performance as the musician interprets\nthe composition. The performer injects expressiveness into the written score\nthrough variations of different musical properties such as dynamics and tempo.\nIn this paper, we describe a model that can learn to perform sheet music. Our\nresearch concludes that the generated performances are indistinguishable from a\nhuman performance, thereby passing a test in the spirit of a \"musical Turing\ntest\".\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 13:24:32 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Malik", "Iman", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1708.03910", "submitter": "Mario Giulianelli", "authors": "Mario Giulianelli", "title": "Semi-supervised emotion lexicon expansion with label propagation and\n  specialized word embeddings", "comments": null, "journal-ref": "Computational Linguistics in the Netherlands Journal, 8, 99-121\n  (2018). Retrieved from https://clinjournal.org/clinj/article/view/82", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist two main approaches to automatically extract affective\norientation: lexicon-based and corpus-based. In this work, we argue that these\ntwo methods are compatible and show that combining them can improve the\naccuracy of emotion classifiers. In particular, we introduce a novel variant of\nthe Label Propagation algorithm that is tailored to distributed word\nrepresentations, we apply batch gradient descent to accelerate the optimization\nof label propagation and to make the optimization feasible for large graphs,\nand we propose a reproducible method for emotion lexicon expansion. We conclude\nthat label propagation can expand an emotion lexicon in a meaningful way and\nthat the expanded emotion lexicon can be leveraged to improve the accuracy of\nan emotion classifier.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 14:09:22 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Giulianelli", "Mario", ""]]}, {"id": "1708.03989", "submitter": "Nikolaos Mitianoudis", "authors": "Dimitrios Mallis and Thomas Sgouros and Nikolaos Mitianoudis", "title": "Convolutive Audio Source Separation using Robust ICA and an intelligent\n  evolving permutation ambiguity solution", "comments": null, "journal-ref": "Evolving Systems, Volume 9, Issue 4, pp 315,329, December 2018", "doi": "10.1007/s12530-017-9199-3", "report-no": null, "categories": "cs.SD cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio source separation is the task of isolating sound sources that are\nactive simultaneously in a room captured by a set of microphones. Convolutive\naudio source separation of equal number of sources and microphones has a number\nof shortcomings including the complexity of frequency-domain ICA, the\npermutation ambiguity and the problem's scalabity with increasing number of\nsensors. In this paper, the authors propose a multiple-microphone audio source\nseparation algorithm based on a previous work of Mitianoudis and Davies (2003).\nComplex FastICA is substituted by Robust ICA increasing robustness and\nperformance. Permutation ambiguity is solved using two methodologies. The first\nis using the Likelihood Ration Jump solution, which is now modified to decrease\ncomputational complexity in the case of multiple microphones. The application\nof the MuSIC algorithm, as a preprocessing step to the previous solution, forms\na second methodology with promising results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 01:59:34 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mallis", "Dimitrios", ""], ["Sgouros", "Thomas", ""], ["Mitianoudis", "Nikolaos", ""]]}, {"id": "1708.04251", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa and Gert Cauwenberghs", "title": "A learning framework for winner-take-all networks with stochastic\n  synapses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent generative models make use of neural networks to transform the\nprobability distribution of a simple low-dimensional noise process into the\ncomplex distribution of the data. This raises the question of whether\nbiological networks operate along similar principles to implement a\nprobabilistic model of the environment through transformations of intrinsic\nnoise processes. The intrinsic neural and synaptic noise processes in\nbiological networks, however, are quite different from the noise processes used\nin current abstract generative networks. This, together with the discrete\nnature of spikes and local circuit interactions among the neurons, raises\nseveral difficulties when using recent generative modeling frameworks to train\nbiologically motivated models. In this paper, we show that a biologically\nmotivated model based on multi-layer winner-take-all (WTA) circuits and\nstochastic synapses admits an approximate analytical description. This allows\nus to use the proposed networks in a variational learning setting where\nstochastic backpropagation is used to optimize a lower bound on the data log\nlikelihood, thereby learning a generative model of the data. We illustrate the\ngenerality of the proposed networks and learning technique by using them in a\nstructured output prediction task, and in a semi-supervised learning task. Our\nresults extend the domain of application of modern stochastic network\narchitectures to networks where synaptic transmission failure is the principal\nnoise mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 18:03:21 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 19:15:31 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Mostafa", "Hesham", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1708.04485", "submitter": "Stephen Keckler", "authors": "Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli,\n  Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and\n  William J. Dally", "title": "SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have emerged as a fundamental technology\nfor machine learning. High performance and extreme energy efficiency are\ncritical for deployments of CNNs in a wide range of situations, especially\nmobile platforms such as autonomous vehicles, cameras, and electronic personal\nassistants. This paper introduces the Sparse CNN (SCNN) accelerator\narchitecture, which improves performance and energy efficiency by exploiting\nthe zero-valued weights that stem from network pruning during training and\nzero-valued activations that arise from the common ReLU operator applied during\ninference. Specifically, SCNN employs a novel dataflow that enables maintaining\nthe sparse weights and activations in a compressed encoding, which eliminates\nunnecessary data transfers and reduces storage requirements. Furthermore, the\nSCNN dataflow facilitates efficient delivery of those weights and activations\nto the multiplier array, where they are extensively reused. In addition, the\naccumulation of multiplication products are performed in a novel accumulator\narray. Our results show that on contemporary neural networks, SCNN can improve\nboth performance and energy by a factor of 2.7x and 2.3x, respectively, over a\ncomparably provisioned dense CNN accelerator.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:11:11 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Parashar", "Angshuman", ""], ["Rhu", "Minsoo", ""], ["Mukkara", "Anurag", ""], ["Puglielli", "Antonio", ""], ["Venkatesan", "Rangharajan", ""], ["Khailany", "Brucek", ""], ["Emer", "Joel", ""], ["Keckler", "Stephen W.", ""], ["Dally", "William J.", ""]]}, {"id": "1708.04498", "submitter": "Victor Rivera", "authors": "Leonard Johard, Victor Rivera, Manuel Mazzara, and JooYoung Lee", "title": "Self-adaptive node-based PCA encodings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it\nis able to calculate the principal component analysis (PCA) in a distributed\nfashion across nodes. It simplifies existing network structures by removing\nintralayer weights, essentially cutting the number of weights that need to be\ntrained in half.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 12:29:41 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Johard", "Leonard", ""], ["Rivera", "Victor", ""], ["Mazzara", "Manuel", ""], ["Lee", "JooYoung", ""]]}, {"id": "1708.04583", "submitter": "Chen Chen", "authors": "Chen Chen, Changtong Luo, Zonglin Jiang", "title": "Fast Modeling Methods for Complex System with Separable Features", "comments": "arXiv admin note: substantial text overlap with arXiv:1706.02281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven modeling plays an increasingly important role in different areas\nof engineering. For most of existing methods, such as genetic programming (GP),\nthe convergence speed might be too slow for large scale problems with a large\nnumber of variables. Fortunately, in many applications, the target models are\nseparable in some sense. In this paper, we analyze different types of\nseparability of some real-world engineering equations and establish a\nmathematical model of generalized separable system (GS system). In order to get\nthe structure of the GS system, two concepts, namely block and factor are\nintroduced, and a special method, block and factor detection is also proposed,\nin which the target model is decomposed into a number of blocks, further into\nminimal blocks and factors. Compare to the conventional GP, the new method can\nmake large reductions to the search space. The minimal blocks and factors are\noptimized and assembled with a global optimization search engine, low\ndimensional simplex evolution (LDSE). An extensive study between the proposed\nmethod and a state-of-the-art data-driven fitting tool, Eureqa, has been\npresented with several man-made problems. Test results indicate that the\nproposed method is more effective and efficient under all the investigated\ncases.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 13:48:18 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Chen", "Chen", ""], ["Luo", "Changtong", ""], ["Jiang", "Zonglin", ""]]}, {"id": "1708.04745", "submitter": "Isabela Maria Carneiro de Albuquerque", "authors": "F. B. Lima Neto, I. M. C. Albuquerque and J. B. Monteiro Filho", "title": "Weight-based Fish School Search algorithm for Many-Objective\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with more than one objective consist in a very\nattractive topic for researchers due to its applicability in real-world\nsituations. Over the years, the research effort in the Computational\nIntelligence field resulted in algorithms able to achieve good results by\nsolving problems with more than one conflicting objective. However, these\ntechniques do not exhibit the same performance as the number of objectives\nincreases and become greater than 3. This paper proposes an adaptation of the\nmetaheuristic Fish School Search to solve optimization problems with many\nobjectives. This adaptation is based on the division of the candidate solutions\nin clusters that are specialized in solving a single-objective problem\ngenerated by the decomposition of the original problem. For this, we used\nconcepts and ideas often employed by state-of-the-art algorithms, namely: (i)\nreference points and lines in the objectives space; (ii) clustering process;\nand (iii) the decomposition technique Penalty-based Boundary Intersection. The\nproposed algorithm was compared with two state-of-the-art bio-inspired\nalgorithms. Moreover, a version of the proposed technique tailored to solve\nmulti-modal problems was also presented. The experiments executed have shown\nthat the performance obtained by both versions is competitive with\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 02:14:11 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 14:14:31 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 22:46:17 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Neto", "F. B. Lima", ""], ["Albuquerque", "I. M. C.", ""], ["Filho", "J. B. Monteiro", ""]]}, {"id": "1708.05356", "submitter": "Anup Das", "authors": "Anup Das, Paruthi Pradhapan, Willemijn Groenendaal, Prathyusha\n  Adiraju, Raj Thilak Rajan, Francky Catthoor, Siebren Schaafsma, Jeffrey L.\n  Krichmar, Nikil Dutt and Chris Van Hoof", "title": "Unsupervised Heart-rate Estimation in Wearables With Liquid States and A\n  Probabilistic Readout", "comments": "51 pages, 12 figures, 6 tables, 95 references. Under submission at\n  Elsevier Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2017.12.015", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart-rate estimation is a fundamental feature of modern wearable devices. In\nthis paper we propose a machine intelligent approach for heart-rate estimation\nfrom electrocardiogram (ECG) data collected using wearable devices. The novelty\nof our approach lies in (1) encoding spatio-temporal properties of ECG signals\ndirectly into spike train and using this to excite recurrently connected\nspiking neurons in a Liquid State Machine computation model; (2) a novel\nlearning algorithm; and (3) an intelligently designed unsupervised readout\nbased on Fuzzy c-Means clustering of spike responses from a subset of neurons\n(Liquid states), selected using particle swarm optimization. Our approach\ndiffers from existing works by learning directly from ECG signals (allowing\npersonalization), without requiring costly data annotations. Additionally, our\napproach can be easily implemented on state-of-the-art spiking-based\nneuromorphic systems, offering high accuracy, yet significantly low energy\nfootprint, leading to an extended battery life of wearable devices. We\nvalidated our approach with CARLsim, a GPU accelerated spiking neural network\nsimulator modeling Izhikevich spiking neurons with Spike Timing Dependent\nPlasticity (STDP) and homeostatic scaling. A range of subjects are considered\nfrom in-house clinical trials and public ECG databases. Results show high\naccuracy and low energy footprint in heart-rate estimation across subjects with\nand without cardiac irregularities, signifying the strong potential of this\napproach to be integrated in future wearable devices.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 13:53:55 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Das", "Anup", ""], ["Pradhapan", "Paruthi", ""], ["Groenendaal", "Willemijn", ""], ["Adiraju", "Prathyusha", ""], ["Rajan", "Raj Thilak", ""], ["Catthoor", "Francky", ""], ["Schaafsma", "Siebren", ""], ["Krichmar", "Jeffrey L.", ""], ["Dutt", "Nikil", ""], ["Van Hoof", "Chris", ""]]}, {"id": "1708.05376", "submitter": "Andre Pacheco", "authors": "Andre Pacheco, Renato Krohling, Carlos da Silva", "title": "Restricted Boltzmann machine to determine the input weights for extreme\n  learning machines", "comments": "14 pages, 7 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Extreme Learning Machine (ELM) is a single-hidden layer feedforward\nneural network (SLFN) learning algorithm that can learn effectively and\nquickly. The ELM training phase assigns the input weights and bias randomly and\ndoes not change them in the whole process. Although the network works well, the\nrandom weights in the input layer can make the algorithm less effective and\nimpact on its performance. Therefore, we propose a new approach to determine\nthe input weights and bias for the ELM using the restricted Boltzmann machine\n(RBM), which we call RBM-ELM. We compare our new approach with a well-known\napproach to improve the ELM and a state of the art algorithm to select the\nweights for the ELM. The results show that the RBM-ELM outperforms both\nmethodologies and achieve a better performance than the ELM.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 17:45:08 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Pacheco", "Andre", ""], ["Krohling", "Renato", ""], ["da Silva", "Carlos", ""]]}, {"id": "1708.05604", "submitter": "Viacheslav Khomenko", "authors": "Viacheslav Khomenko (1), Oleg Shyshkov (1), Olga Radyvonenko (1),\n  Kostiantyn Bokhan (1) ((1) Samsung R&D Institute Ukraine SRK)", "title": "Accelerating recurrent neural network training using sequence bucketing\n  and multi-GPU data parallelization", "comments": "4 pages, 5 figures, Comments, 2016 IEEE First International\n  Conference on Data Stream Mining & Processing (DSMP), Lviv, 2016", "journal-ref": "IEEE DSMP Lviv (2016) 100-103", "doi": "10.1109/DSMP.2016.7583516", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm for recurrent neural network training is presented.\nThe approach increases the training speed for tasks where a length of the input\nsequence may vary significantly. The proposed approach is based on the optimal\nbatch bucketing by input sequence length and data parallelization on multiple\ngraphical processing units. The baseline training performance without sequence\nbucketing is compared with the proposed solution for a different number of\nbuckets. An example is given for the online handwriting recognition task using\nan LSTM recurrent neural network. The evaluation is performed in terms of the\nwall clock time, number of epochs, and validation loss value.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 13:36:30 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Khomenko", "Viacheslav", "", "Samsung R&D Institute Ukraine SRK"], ["Shyshkov", "Oleg", "", "Samsung R&D Institute Ukraine SRK"], ["Radyvonenko", "Olga", "", "Samsung R&D Institute Ukraine SRK"], ["Bokhan", "Kostiantyn", "", "Samsung R&D Institute Ukraine SRK"]]}, {"id": "1708.05732", "submitter": "Raja Naeem Akram", "authors": "Raja Naeem Akram, Konstantinos Markantonakis, Keith Mayes, Oussama\n  Habachi, Damien Sauveron, Andreas Steyven and Serge Chaumette", "title": "Security, Privacy and Safety Evaluation of Dynamic and Static Fleets of\n  Drones", "comments": "12 Pages, 7 Figures, Conference, The 36th IEEE/AIAA Digital Avionics\n  Systems Conference (DASC'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-connected objects, either via public or private networks are the near\nfuture of modern societies. Such inter-connected objects are referred to as\nInternet-of-Things (IoT) and/or Cyber-Physical Systems (CPS). One example of\nsuch a system is based on Unmanned Aerial Vehicles (UAVs). The fleet of such\nvehicles are prophesied to take on multiple roles involving mundane to\nhigh-sensitive, such as, prompt pizza or shopping deliveries to your homes to\nbattlefield deployment for reconnaissance and combat missions. Drones, as we\nrefer to UAVs in this paper, either can operate individually (solo missions) or\npart of a fleet (group missions), with and without constant connection with the\nbase station. The base station acts as the command centre to manage the\nactivities of the drones. However, an independent, localised and effective\nfleet control is required, potentially based on swarm intelligence, for the\nreasons: 1) increase in the number of drone fleets, 2) number of drones in a\nfleet might be multiple of tens, 3) time-criticality in making decisions by\nsuch fleets in the wild, 4) potential communication congestions/lag, and 5) in\nsome cases working in challenging terrains that hinders or mandates-limited\ncommunication with control centre (i.e., operations spanning long period of\ntimes or military usage of such fleets in enemy territory). This self-ware,\nmission-focused and independent fleet of drones that potential utilises swarm\nintelligence for a) air-traffic and/or flight control management, b) obstacle\navoidance, c) self-preservation while maintaining the mission criteria, d)\ncollaboration with other fleets in the wild (autonomously) and e) assuring the\nsecurity, privacy and safety of physical (drones itself) and virtual (data,\nsoftware) assets. In this paper, we investigate the challenges faced by fleet\nof drones and propose a potential course of action on how to overcome them.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 18:41:38 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Akram", "Raja Naeem", ""], ["Markantonakis", "Konstantinos", ""], ["Mayes", "Keith", ""], ["Habachi", "Oussama", ""], ["Sauveron", "Damien", ""], ["Steyven", "Andreas", ""], ["Chaumette", "Serge", ""]]}, {"id": "1708.05963", "submitter": "Dmitry Ignatov", "authors": "Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko", "title": "Neural Networks Compression for Language Modeling", "comments": "Keywords: LSTM, RNN, language modeling, low-rank factorization,\n  pruning, quantization. Published by Springer in the LNCS series, 7th\n  International Conference on Pattern Recognition and Machine Intelligence,\n  2017", "journal-ref": null, "doi": "10.1007/978-3-319-69900-4_44", "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider several compression techniques for the language\nmodeling problem based on recurrent neural networks (RNNs). It is known that\nconventional RNNs, e.g, LSTM-based networks in language modeling, are\ncharacterized with either high space complexity or substantial inference time.\nThis problem is especially crucial for mobile applications, in which the\nconstant interaction with the remote server is inappropriate. By using the Penn\nTreebank (PTB) dataset we compare pruning, quantization, low-rank\nfactorization, tensor train decomposition for LSTM networks in terms of model\nsize and suitability for fast inference.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 13:37:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Grachev", "Artem M.", ""], ["Ignatov", "Dmitry I.", ""], ["Savchenko", "Andrey V.", ""]]}, {"id": "1708.06004", "submitter": "Takayuki Osogami Ph.D.", "authors": "Takayuki Osogami", "title": "Boltzmann machines for time-series", "comments": "Version 1.0.1. 33 pages. The topics covered in this paper are\n  presented in Part III of IJCAI-17 tutorial on energy-based machine learning.\n  https://researcher.watson.ibm.com/researcher/view_group.php?id=7834", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review Boltzmann machines extended for time-series. These models often\nhave recurrent structure, and back propagration through time (BPTT) is used to\nlearn their parameters. The per-step computational complexity of BPTT in online\nlearning, however, grows linearly with respect to the length of preceding\ntime-series (i.e., learning rule is not local in time), which limits the\napplicability of BPTT in online learning. We then review dynamic Boltzmann\nmachines (DyBMs), whose learning rule is local in time. DyBM's learning rule\nrelates to spike-timing dependent plasticity (STDP), which has been postulated\nand experimentally confirmed for biological neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 19:09:34 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 12:18:26 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 10:43:54 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Osogami", "Takayuki", ""]]}, {"id": "1708.06008", "submitter": "Takayuki Osogami Ph.D.", "authors": "Takayuki Osogami", "title": "Boltzmann machines and energy-based models", "comments": "36 pages. The topics covered in this paper are presented in Part I of\n  IJCAI-17 tutorial on energy-based machine learning.\n  https://researcher.watson.ibm.com/researcher/view_group.php?id=7834", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review Boltzmann machines and energy-based models. A Boltzmann machine\ndefines a probability distribution over binary-valued patterns. One can learn\nparameters of a Boltzmann machine via gradient based approaches in a way that\nlog likelihood of data is increased. The gradient and Hessian of a Boltzmann\nmachine admit beautiful mathematical representations, although computing them\nis in general intractable. This intractability motivates approximate methods,\nincluding Gibbs sampler and contrastive divergence, and tractable alternatives,\nnamely energy-based models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 19:29:44 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 10:35:56 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Osogami", "Takayuki", ""]]}, {"id": "1708.06019", "submitter": "Gerald Friedland", "authors": "Gerald Friedland and Mario Krell", "title": "A Capacity Scaling Law for Artificial Neural Networks", "comments": "13 pages, 4 figures, 2 listings of source code", "journal-ref": null, "doi": null, "report-no": "LLNL-TR-736950", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the calculation of two critical numbers predicting the behavior of\nperceptron networks. First, we derive the calculation of what we call the\nlossless memory (LM) dimension. The LM dimension is a generalization of the\nVapnik--Chervonenkis (VC) dimension that avoids structured data and therefore\nprovides an upper bound for perfectly fitting almost any training data. Second,\nwe derive what we call the MacKay (MK) dimension. This limit indicates a 50%\nchance of not being able to train a given function. Our derivations are\nperformed by embedding a neural network into Shannon's communication model\nwhich allows to interpret the two points as capacities measured in bits. We\npresent a proof and practical experiments that validate our upper bounds with\nrepeatable experiments using different network configurations, diverse\nimplementations, varying activation functions, and several learning algorithms.\nThe bottom line is that the two capacity points scale strictly linear with the\nnumber of weights. Among other practical applications, our result allows to\ncompare and benchmark different neural network implementations independent of a\nconcrete learning task. Our results provide insight into the capabilities and\nlimits of neural networks and generate valuable know how for experimental\ndesign decisions.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 21:10:42 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 05:02:07 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 01:30:30 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Friedland", "Gerald", ""], ["Krell", "Mario", ""]]}, {"id": "1708.06219", "submitter": "Namig Guliyev", "authors": "Namig J. Guliyev and Vugar E. Ismailov", "title": "On the approximation by single hidden layer feedforward neural networks\n  with fixed weights", "comments": "17 pages, 5 figures, submitted; for associated SageMath worksheet,\n  see https://sites.google.com/site/njguliyev/papers/monic-sigmoidal", "journal-ref": "Neural Networks, 98 (2018), 296-304", "doi": "10.1016/j.neunet.2017.12.007", "report-no": null, "categories": "cs.NE cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedforward neural networks have wide applicability in various disciplines of\nscience due to their universal approximation property. Some authors have shown\nthat single hidden layer feedforward neural networks (SLFNs) with fixed weights\nstill possess the universal approximation property provided that approximated\nfunctions are univariate. But this phenomenon does not lay any restrictions on\nthe number of neurons in the hidden layer. The more this number, the more the\nprobability of the considered network to give precise results. In this note, we\nconstructively prove that SLFNs with the fixed weight $1$ and two neurons in\nthe hidden layer can approximate any continuous function on a compact subset of\nthe real line. The applicability of this result is demonstrated in various\nnumerical examples. Finally, we show that SLFNs with fixed weights cannot\napproximate all continuous multivariate functions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 13:46:15 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Guliyev", "Namig J.", ""], ["Ismailov", "Vugar E.", ""]]}, {"id": "1708.06238", "submitter": "Abhinav Parihar", "authors": "Abhinav Parihar, Matthew Jerry, Suman Datta, Arijit Raychowdhury", "title": "Stochastic IMT (insulator-metal-transition) neurons: An interplay of\n  thermal and threshold noise at bifurcation", "comments": "Added sectioning, Figure 6, Table 1, and Section II.E Updated\n  abstract, discussion and corrected typos", "journal-ref": null, "doi": "10.3389/fnins.2018.00210", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks can harness stochasticity in multiple ways to\nenable a vast class of computationally powerful models. Electronic\nimplementation of such stochastic networks is currently limited to addition of\nalgorithmic noise to digital machines which is inherently inefficient; albeit\nrecent efforts to harness physical noise in devices for stochasticity have\nshown promise. To succeed in fabricating electronic neuromorphic networks we\nneed experimental evidence of devices with measurable and controllable\nstochasticity which is complemented with the development of reliable\nstatistical models of such observed stochasticity. Current research literature\nhas sparse evidence of the former and a complete lack of the latter. This\nmotivates the current article where we demonstrate a stochastic neuron using an\ninsulator-metal-transition (IMT) device, based on electrically induced\nphase-transition, in series with a tunable resistance. We show that an IMT\nneuron has dynamics similar to a piecewise linear FitzHugh-Nagumo (FHN) neuron\nand incorporates all characteristics of a spiking neuron in the device\nphenomena. We experimentally demonstrate spontaneous stochastic spiking along\nwith electrically controllable firing probabilities using Vanadium Dioxide\n(VO$_2$) based IMT neurons which show a sigmoid-like transfer function. The\nstochastic spiking is explained by two noise sources - thermal noise and\nthreshold fluctuations, which act as precursors of bifurcation. As such, the\nIMT neuron is modeled as an Ornstein-Uhlenbeck (OU) process with a fluctuating\nboundary resulting in transfer curves that closely match experiments. As one of\nthe first comprehensive studies of a stochastic neuron hardware and its\nstatistical properties, this article would enable efficient implementation of a\nlarge class of neuro-mimetic networks and algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 19:35:28 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 18:10:16 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 21:43:15 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 18:02:43 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Parihar", "Abhinav", ""], ["Jerry", "Matthew", ""], ["Datta", "Suman", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1708.06250", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Yu Qian", "title": "Pillar Networks++: Distributed non-parametric deep and wide networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.06923", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, it was shown that combining multi-kernel based support vector\nmachines (SVMs) can lead to near state-of-the-art performance on an action\nrecognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks\nthat used hand-crafted features in addition to the deep convolutional feature\nextractors. In the present work, we show that combining distributed Gaussian\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\nthe need to augment a neural network with hand-crafted features. In contrast to\nprior work, we treat each deep neural convolutional network as an expert\nwherein the individual predictions (and their respective uncertainties) are\ncombined into a Product of Experts (PoE) framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 07:51:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sengupta", "Biswa", ""], ["Qian", "Yu", ""]]}, {"id": "1708.06257", "submitter": "Zhen Li", "authors": "Zhen Li, Zuoqiang Shi", "title": "A Flow Model of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a natural connection between ResNet and transport equation or its\ncharacteristic equation, we propose a continuous flow model for both ResNet and\nplain net. Through this continuous model, a ResNet can be explicitly\nconstructed as a refinement of a plain net. The flow model provides an\nalternative perspective to understand phenomena in deep neural networks, such\nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why\ndeeper is better, and why ResNets are even deeper, and so on. It also opens a\ngate to bring in more tools from the huge area of differential equations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:30:49 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 11:26:00 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Li", "Zhen", ""], ["Shi", "Zuoqiang", ""]]}, {"id": "1708.06525", "submitter": "Xiaojun Xu", "authors": "Xiaojun Xu and Chang Liu and Qian Feng and Heng Yin and Le Song and\n  Dawn Song", "title": "Neural Network-based Graph Embedding for Cross-Platform Binary Code\n  Similarity Detection", "comments": "ACM CCS 17", "journal-ref": null, "doi": "10.1145/3133956.3134018", "report-no": null, "categories": "cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of cross-platform binary code similarity detection aims at\ndetecting whether two binary functions coming from different platforms are\nsimilar or not. It has many security applications, including plagiarism\ndetection, malware detection, vulnerability search, etc. Existing approaches\nrely on approximate graph matching algorithms, which are inevitably slow and\nsometimes inaccurate, and hard to adapt to a new task. To address these issues,\nin this work, we propose a novel neural network-based approach to compute the\nembedding, i.e., a numeric vector, based on the control flow graph of each\nbinary function, then the similarity detection can be done efficiently by\nmeasuring the distance between the embeddings for two functions. We implement a\nprototype called Gemini. Our extensive evaluation shows that Gemini outperforms\nthe state-of-the-art approaches by large margins with respect to similarity\ndetection accuracy. Further, Gemini can speed up prior art's embedding\ngeneration time by 3 to 4 orders of magnitude and reduce the required training\ntime from more than 1 week down to 30 minutes to 10 hours. Our real world case\nstudies demonstrate that Gemini can identify significantly more vulnerable\nfirmware images than the state-of-the-art, i.e., Genius. Our research showcases\na successful application of deep learning on computer security problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 07:53:51 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 10:44:46 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 06:25:31 GMT"}, {"version": "v4", "created": "Fri, 27 Jul 2018 05:52:22 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Xu", "Xiaojun", ""], ["Liu", "Chang", ""], ["Feng", "Qian", ""], ["Yin", "Heng", ""], ["Song", "Le", ""], ["Song", "Dawn", ""]]}, {"id": "1708.07040", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Adaptive Plant Propagation Algorithm for Solving Economic Load Dispatch\n  Problem", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems in design engineering are complex by nature, often\nbecause of the involvement of critical objective functions accompanied by a\nnumber of rigid constraints associated with the products involved. One such\nproblem is Economic Load Dispatch (ED) problem which focuses on the\noptimization of the fuel cost while satisfying some system constraints.\nClassical optimization algorithms are not sufficient and also inefficient for\nthe ED problem involving highly nonlinear, and non-convex functions both in the\nobjective and in the constraints. This led to the development of metaheuristic\noptimization approaches which can solve the ED problem almost efficiently. This\npaper presents a novel robust plant intelligence based Adaptive Plant\nPropagation Algorithm (APPA) which is used to solve the classical ED problem.\nThe application of the proposed method to the 3-generator and 6-generator\nsystems shows the efficiency and robustness of the proposed algorithm. A\ncomparative study with another state-of-the-art algorithm (APSO) demonstrates\nthe quality of the solution achieved by the proposed method along with the\nconvergence characteristics of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:09:36 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1708.07061", "submitter": "Jesus Lago", "authors": "Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter", "title": "Forecasting day-ahead electricity prices in Europe: the importance of\n  considering market integration", "comments": null, "journal-ref": "Applied Energy, Volume 211, 1 February 2018, Pages 890-903", "doi": "10.1016/j.apenergy.2017.11.098", "report-no": null, "categories": "q-fin.ST cs.CE cs.LG cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing integration among electricity markets, in this\npaper we propose two different methods to incorporate market integration in\nelectricity price forecasting and to improve the predictive performance. First,\nwe propose a deep neural network that considers features from connected markets\nto improve the predictive accuracy in a local market. To measure the importance\nof these features, we propose a novel feature selection algorithm that, by\nusing Bayesian optimization and functional analysis of variance, evaluates the\neffect of the features on the algorithm performance. In addition, using market\nintegration, we propose a second model that, by simultaneously predicting\nprices from two markets, improves the forecasting accuracy even further. As a\ncase study, we consider the electricity market in Belgium and the improvements\nin forecasting accuracy when using various French electricity features. We show\nthat the two proposed models lead to improvements that are statistically\nsignificant. Particularly, due to market integration, the predictive accuracy\nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage\nerror). In addition, we show that the proposed feature selection algorithm is\nable to perform a correct assessment, i.e. to discard the irrelevant features.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 15:34:48 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 17:12:16 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 15:34:43 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Lago", "Jesus", ""], ["De Ridder", "Fjo", ""], ["Vrancx", "Peter", ""], ["De Schutter", "Bart", ""]]}, {"id": "1708.07120", "submitter": "Leslie Smith", "authors": "Leslie N. Smith and Nicholay Topin", "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large\n  Learning Rates", "comments": "This paper was significantly revised to show super-convergence as a\n  general fast training methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/).\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 17:51:57 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 18:24:34 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 17:40:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Smith", "Leslie N.", ""], ["Topin", "Nicholay", ""]]}, {"id": "1708.07147", "submitter": "Ashley Prater", "authors": "Ashley Prater", "title": "Classification via Tensor Decompositions of Echo State Networks", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a tensor-based method to perform supervised\nclassification on spatiotemporal data processed in an echo state network.\nTypically when performing supervised classification tasks on data processed in\nan echo state network, the entire collection of hidden layer node states from\nthe training dataset is shaped into a matrix, allowing one to use standard\nlinear algebra techniques to train the output layer. However, the collection of\nhidden layer states is multidimensional in nature, and representing it as a\nmatrix may lead to undesirable numerical conditions or loss of spatial and\ntemporal correlations in the data.\n  This work proposes a tensor-based supervised classification method on echo\nstate network data that preserves and exploits the multidimensional nature of\nthe hidden layer states. The method, which is based on orthogonal Tucker\ndecompositions of tensors, is compared with the standard linear output weight\napproach in several numerical experiments on both synthetic and natural data.\nThe results show that the tensor-based approach tends to outperform the\nstandard approach in terms of classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 18:51:08 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Prater", "Ashley", ""]]}, {"id": "1708.07524", "submitter": "Jitong Chen", "authors": "DeLiang Wang and Jitong Chen", "title": "Supervised Speech Separation Based on Deep Learning: An Overview", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech separation is the task of separating target speech from background\ninterference. Traditionally, speech separation is studied as a signal\nprocessing problem. A more recent approach formulates speech separation as a\nsupervised learning problem, where the discriminative patterns of speech,\nspeakers, and background noise are learned from training data. Over the past\ndecade, many supervised separation algorithms have been put forward. In\nparticular, the recent introduction of deep learning to supervised speech\nseparation has dramatically accelerated progress and boosted separation\nperformance. This article provides a comprehensive overview of the research on\ndeep learning based supervised speech separation in the last several years. We\nfirst introduce the background of speech separation and the formulation of\nsupervised separation. Then we discuss three main components of supervised\nseparation: learning machines, training targets, and acoustic features. Much of\nthe overview is on separation algorithms where we review monaural methods,\nincluding speech enhancement (speech-nonspeech separation), speaker separation\n(multi-talker separation), and speech dereverberation, as well as\nmulti-microphone techniques. The important issue of generalization, unique to\nsupervised learning, is discussed. This overview provides a historical\nperspective on how advances are made. In addition, we discuss a number of\nconceptual issues, including what constitutes the target source.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 18:51:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 03:28:26 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Wang", "DeLiang", ""], ["Chen", "Jitong", ""]]}, {"id": "1708.07576", "submitter": "Sam Greydanus", "authors": "Sam Greydanus", "title": "Learning the Enigma with Recurrent Neural Networks", "comments": "7 pages, 11 figures, intended submission for AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) represent the state of the art in\ntranslation, image captioning, and speech recognition. They are also capable of\nlearning algorithmic tasks such as long addition, copying, and sorting from a\nset of training examples. We demonstrate that RNNs can learn decryption\nalgorithms -- the mappings from plaintext to ciphertext -- for three\npolyalphabetic ciphers (Vigen\\`ere, Autokey, and Enigma). Most notably, we\ndemonstrate that an RNN with a 3000-unit Long Short-Term Memory (LSTM) cell can\nlearn the decryption function of the Enigma machine. We argue that our model\nlearns efficient internal representations of these ciphers 1) by exploring\nactivations of individual memory neurons and 2) by comparing memory usage\nacross the three ciphers. To be clear, our work is not aimed at 'cracking' the\nEnigma cipher. However, we do show that our model can perform elementary\ncryptanalysis by running known-plaintext attacks on the Vigen\\`ere and Autokey\nciphers. Our results indicate that RNNs can learn algorithmic representations\nof black box polyalphabetic ciphers and that these representations are useful\nfor cryptanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 23:35:19 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 19:05:23 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Greydanus", "Sam", ""]]}, {"id": "1708.07881", "submitter": "Guohai Situ", "authors": "Meng Lyu, Hao Wang, Guowei Li and Guohai Situ", "title": "Exploit imaging through opaque wall via deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging through scattering media is encountered in many disciplines or\nsciences, ranging from biology, mesescopic physics and astronomy. But it is\nstill a big challenge because light suffers from multiple scattering is such\nmedia and can be totally decorrelated. Here, we propose a deep-learning-based\nmethod that can retrieve the image of a target behind a thick scattering\nmedium. The method uses a trained deep neural network to fit the way of mapping\nof objects at one side of a thick scattering medium to the corresponding\nspeckle patterns observed at the other side. For demonstration, we retrieve the\nimages of a set of objects hidden behind a 3mm thick white polystyrene slab,\nthe optical depth of which is 13.4 times of the scattering mean free path. Our\nwork opens up a new way to tackle the longstanding challenge by using the\ntechnique of deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:38:32 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Lyu", "Meng", ""], ["Wang", "Hao", ""], ["Li", "Guowei", ""], ["Situ", "Guohai", ""]]}, {"id": "1708.07949", "submitter": "Aayush Ankit", "authors": "Aayush Ankit, Abhronil Sengupta, Kaushik Roy", "title": "TraNNsformer: Neural network transformation for memristive crossbar\n  based neuromorphic system design", "comments": "(8 pages, 9 figures) Published in Computer-Aided Design (ICCAD), 2017\n  IEEE/ACM International Conference on", "journal-ref": null, "doi": "10.1109/ICCAD.2017.8203823", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementation of Neuromorphic Systems using post Complementary\nMetal-Oxide-Semiconductor (CMOS) technology based Memristive Crossbar Array\n(MCA) has emerged as a promising solution to enable low-power acceleration of\nneural networks. However, the recent trend to design Deep Neural Networks\n(DNNs) for achieving human-like cognitive abilities poses significant\nchallenges towards the scalable design of neuromorphic systems (due to the\nincrease in computation/storage demands). Network pruning [7] is a powerful\ntechnique to remove redundant connections for designing optimally connected\n(maximally sparse) DNNs. However, such pruning techniques induce irregular\nconnections that are incoherent to the crossbar structure. Eventually they\nproduce DNNs with highly inefficient hardware realizations (in terms of area\nand energy). In this work, we propose TraNNsformer - an integrated training\nframework that transforms DNNs to enable their efficient realization on\nMCA-based systems. TraNNsformer first prunes the connectivity matrix while\nforming clusters with the remaining connections. Subsequently, it retrains the\nnetwork to fine tune the connections and reinforce the clusters. This is done\niteratively to transform the original connectivity into an optimally pruned and\nmaximally clustered mapping. Without accuracy loss, TraNNsformer reduces the\narea (energy) consumption by 28% - 55% (49% - 67%) with respect to the original\nnetwork. Compared to network pruning, TraNNsformer achieves 28% - 49% (15% -\n29%) area (energy) savings. Furthermore, TraNNsformer is a technology-aware\nframework that allows mapping a given DNN to any MCA size permissible by the\nmemristive technology for reliable operations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 08:07:57 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 18:10:00 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ankit", "Aayush", ""], ["Sengupta", "Abhronil", ""], ["Roy", "Kaushik", ""]]}, {"id": "1708.08012", "submitter": "Robin Tibor Schirrmeister", "authors": "Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank\n  Hutter, Tonio Ball", "title": "Deep learning with convolutional neural networks for decoding and\n  visualization of EEG pathology", "comments": "Published at IEEE SPMB 2017 https://www.ieeespmb.org/2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply convolutional neural networks (ConvNets) to the task of\ndistinguishing pathological from normal EEG recordings in the Temple University\nHospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet\narchitectures recently shown to decode task-related information from EEG at\nleast as well as established algorithms designed for this purpose. In decoding\nEEG pathology, both ConvNets reached substantially better accuracies (about 6%\nbetter, ~85% vs. ~79%) than the only published result for this dataset, and\nwere still better when using only 1 minute of each recording for training and\nonly six seconds of each recording for testing. We used automated methods to\noptimize architectural hyperparameters and found intriguingly different ConvNet\narchitectures, e.g., with max pooling as the only nonlinearity. Visualizations\nof the ConvNet decoding behavior showed that they used spectral power changes\nin the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside\nother features, consistent with expectations derived from spectral analysis of\nthe EEG data and from the textual medical reports. Analysis of the textual\nmedical reports also highlighted the potential for accuracy increases by\nintegrating contextual information, such as the age of subjects. In summary,\nthe ConvNets and visualization techniques used in this study constitute a next\nstep towards clinically useful automated EEG diagnosis and establish a new\nbaseline for future work on this topic.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 19:14:47 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:56:40 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 20:11:04 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Schirrmeister", "Robin Tibor", ""], ["Gemein", "Lukas", ""], ["Eggensperger", "Katharina", ""], ["Hutter", "Frank", ""], ["Ball", "Tonio", ""]]}, {"id": "1708.08127", "submitter": "Jianfeng Chen", "authors": "Jianfeng Chen, Tim Menzies", "title": "RIOT: a Stochastic-based Method for Workflow Scheduling in the Cloud", "comments": "8 pages, 4 figures, 3 tables. In Proceedings of IEEE international\n  conference on Cloud Computing'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing provides engineers or scientists a place to run complex\ncomputing tasks. Finding a workflow's deployment configuration in a cloud\nenvironment is not easy. Traditional workflow scheduling algorithms were based\non some heuristics, e.g. reliability greedy, cost greedy, cost-time balancing,\netc., or more recently, the meta-heuristic methods, such as genetic algorithms.\nThese methods are very slow and not suitable for rescheduling in the dynamic\ncloud environment. This paper introduces RIOT (Randomized Instance Order\nTypes), a stochastic based method for workflow scheduling. RIOT groups the\ntasks in the workflow into virtual machines via a probability model and then\nuses an effective surrogate-based method to assess a large amount of potential\nscheduling. Experiments in dozens of study cases showed that RIOT executes tens\nof times faster than traditional methods while generating comparable results to\nother methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 19:45:02 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 21:35:01 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Chen", "Jianfeng", ""], ["Menzies", "Tim", ""]]}, {"id": "1708.08282", "submitter": "Peng-Bo Zhang", "authors": "Peng-Bo Zhang and Zhi-Xin Yang", "title": "A New Learning Paradigm for Random Vector Functional-Link Network: RVFL+", "comments": "We have updated the previous work", "journal-ref": "Neural Networks 122(2019) 94-105", "doi": "10.1016/j.neunet.2019.09.039", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In school, a teacher plays an important role in various classroom teaching\npatterns. Likewise to this human learning activity, the learning using\nprivileged information (LUPI) paradigm provides additional information\ngenerated by the teacher to 'teach' learning models during the training stage.\nTherefore, this novel learning paradigm is a typical Teacher-Student\nInteraction mechanism. This paper is the first to present a random vector\nfunctional link network based on the LUPI paradigm, called RVFL+. Rather than\nsimply combining two existing approaches, the newly-derived RVFL+ fills the gap\nbetween classical randomized neural networks and the newfashioned LUPI\nparadigm, which offers an alternative way to train RVFL networks. Moreover, the\nproposed RVFL+ can perform in conjunction with the kernel trick for highly\ncomplicated nonlinear feature learning, which is termed KRVFL+. Furthermore,\nthe statistical property of the proposed RVFL+ is investigated, and we present\na sharp and high-quality generalization error bound based on the Rademacher\ncomplexity. Competitive experimental results on 14 real-world datasets\nillustrate the great effectiveness and efficiency of the novel RVFL+ and\nKRVFL+, which can achieve better generalization performance than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 11:55:00 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 10:55:16 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 14:31:36 GMT"}, {"version": "v4", "created": "Sun, 17 Mar 2019 03:40:19 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Peng-Bo", ""], ["Yang", "Zhi-Xin", ""]]}, {"id": "1708.08296", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Thomas Wiegand, Klaus-Robert M\\\"uller", "title": "Explainable Artificial Intelligence: Understanding, Visualizing and\n  Interpreting Deep Learning Models", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:53:49 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Samek", "Wojciech", ""], ["Wiegand", "Thomas", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1708.08557", "submitter": "Luke Godfrey", "authors": "Luke B. Godfrey and Michael S. Gashler", "title": "A parameterized activation function for learning fuzzy logic operations\n  in deep neural networks", "comments": "6 pages, 3 figures, IEEE SMC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning architecture for learning fuzzy logic expressions.\nOur model uses an innovative, parameterized, differentiable activation function\nthat can learn a number of logical operations by gradient descent. This\nactivation function allows a neural network to determine the relationships\nbetween its input variables and provides insight into the logical significance\nof learned network parameters. We provide a theoretical basis for this\nparameterization and demonstrate its effectiveness and utility by successfully\napplying our model to five classification problems from the UCI Machine\nLearning Repository.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 23:08:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 19:04:57 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Godfrey", "Luke B.", ""], ["Gashler", "Michael S.", ""]]}, {"id": "1708.08694", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Natasha 2: Faster Non-Convex Optimization Than SGD", "comments": "V2 and V3 polished writing; V4 was a deep revision and simplified\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a stochastic algorithm to train any smooth neural network to\n$\\varepsilon$-approximate local minima, using $O(\\varepsilon^{-3.25})$\nbackpropagations. The best result was essentially $O(\\varepsilon^{-4})$ by SGD.\n  More broadly, it finds $\\varepsilon$-approximate local minima of any smooth\nnonconvex function in rate $O(\\varepsilon^{-3.25})$, with only oracle access to\nstochastic gradients.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:56:28 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 11:23:34 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 09:40:29 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 10:25:50 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1708.09040", "submitter": "Elahe Rahimtoroghi", "authors": "Elahe Rahimtoroghi, Jiaqi Wu, Ruimin Wang, Pranav Anand, Marilyn A\n  Walker", "title": "Modelling Protagonist Goals and Desires in First-Person Narrative", "comments": "10 pages, 18th Annual SIGdial Meeting on Discourse and Dialogue\n  (SIGDIAL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many genres of natural language text are narratively structured, a testament\nto our predilection for organizing our experiences as narratives. There is\nbroad consensus that understanding a narrative requires identifying and\ntracking the goals and desires of the characters and their narrative outcomes.\nHowever, to date, there has been limited work on computational models for this\nproblem. We introduce a new dataset, DesireDB, which includes gold-standard\nlabels for identifying statements of desire, textual evidence for desire\nfulfillment, and annotations for whether the stated desire is fulfilled given\nthe evidence in the narrative context. We report experiments on tracking desire\nfulfillment using different methods, and show that LSTM Skip-Thought model\nachieves F-measure of 0.7 on our corpus.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 21:40:22 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Rahimtoroghi", "Elahe", ""], ["Wu", "Jiaqi", ""], ["Wang", "Ruimin", ""], ["Anand", "Pranav", ""], ["Walker", "Marilyn A", ""]]}, {"id": "1708.09097", "submitter": "Kazunori Yamada", "authors": "Kazunori D Yamada", "title": "Optimizing scoring function of dynamic programming of pairwise profile\n  alignment using derivative free neural network", "comments": null, "journal-ref": null, "doi": "10.1186/s13015-018-0123-6", "report-no": null, "categories": "q-bio.QM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A profile comparison method with position-specific scoring matrix (PSSM) is\none of the most accurate alignment methods. Currently, cosine similarity and\ncorrelation coefficient are used as scoring functions of dynamic programming to\ncalculate similarity between PSSMs. However, it is unclear that these functions\nare optimal for profile alignment methods. At least, by definition, these\nfunctions cannot capture non-linear relationships between profiles. Therefore,\nin this study, we attempted to discover a novel scoring function, which was\nmore suitable for the profile comparison method than the existing ones. Firstly\nwe implemented a new derivative free neural network by combining the\nconventional neural network with evolutionary strategy optimization method.\nNext, using the framework, the scoring function was optimized for aligning\nremote sequence pairs. Nepal, the pairwise profile aligner with the novel\nscoring function significantly improved both alignment sensitivity and\nprecision, compared to aligners with the existing functions. Nepal improved\nalignment quality because of adaptation to remote sequence alignment and\nincreasing the expressive power of similarity score. The novel scoring function\ncan be realized using a simple matrix operation and easily incorporated into\nother aligners. With our scoring function, the performance of homology\ndetection and/or multiple sequence alignment for remote homologous sequences\nwould be further improved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 03:28:13 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 05:10:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Yamada", "Kazunori D", ""]]}, {"id": "1708.09116", "submitter": "Xin Xie", "authors": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, and Zhengyu Yang", "title": "Slope Stability Analysis with Geometric Semantic Genetic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic programming has been widely used in the engineering field. Compared\nwith the conventional genetic programming and artificial neural network,\ngeometric semantic genetic programming (GSGP) is superior in astringency and\ncomputing efficiency. In this paper, GSGP is adopted for the classification and\nregression analysis of a sample dataset. Furthermore, a model for slope\nstability analysis is established on the basis of geometric semantics.\nAccording to the results of the study based on GSGP, the method can analyze\nslope stability objectively and is highly precise in predicting slope stability\nand safety factors. Hence, the predicted results can be used as a reference for\nslope safety design.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 04:48:33 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 18:03:19 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Xu", "Juncai", ""], ["Shen", "Zhenzhong", ""], ["Ren", "Qingwen", ""], ["Xie", "Xin", ""], ["Yang", "Zhengyu", ""]]}, {"id": "1708.09175", "submitter": "Saverio De Vito", "authors": "S. De Vito, E. Esposito, M. Salvato, O. Popoola, F. Formisano, R.\n  Jones, G. Di Francia", "title": "Calibrating chemical multisensory devices for real world applications:\n  An in-depth comparison of quantitative Machine Learning approaches", "comments": null, "journal-ref": "Sensors and Actuators B: Chemical, Volume 255, Part 2, 2018, Pages\n  1191-1210, ISSN 0925-4005", "doi": "10.1016/j.snb.2017.07.155", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical multisensor devices need calibration algorithms to estimate gas\nconcentrations. Their possible adoption as indicative air quality measurements\ndevices poses new challenges due to the need to operate in continuous\nmonitoring modes in uncontrolled environments. Several issues, including slow\ndynamics, continue to affect their real world performances. At the same time,\nthe need for estimating pollutant concentrations on board the devices, espe-\ncially for wearables and IoT deployments, is becoming highly desirable. In this\nframework, several calibration approaches have been proposed and tested on a\nvariety of proprietary devices and datasets; still, no thorough comparison is\navailable to researchers. This work attempts a benchmarking of the most\npromising calibration algorithms according to recent literature with a focus on\nmachine learning approaches. We test the techniques against absolute and\ndynamic performances, generalization capabilities and computational/storage\nneeds using three different datasets sharing continuous monitoring operation\nmethodology. Our results can guide researchers and engineers in the choice of\noptimal strategy. They show that non-linear multivariate techniques yield\nreproducible results, outperforming lin- ear approaches. Specifically, the\nSupport Vector Regression method consistently shows good performances in all\nthe considered scenarios. We highlight the enhanced suitability of shallow\nneural networks in a trade-off between performance and computational/storage\nneeds. We confirm, on a much wider basis, the advantages of dynamic approaches\nwith respect to static ones that only rely on instantaneous sensor array\nresponse. The latter have been shown to be best choice whenever prompt and\nprecise response is needed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 08:53:15 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["De Vito", "S.", ""], ["Esposito", "E.", ""], ["Salvato", "M.", ""], ["Popoola", "O.", ""], ["Formisano", "F.", ""], ["Jones", "R.", ""], ["Di Francia", "G.", ""]]}, {"id": "1708.09251", "submitter": "Antoine Cully", "authors": "Antoine Cully and Yiannis Demiris", "title": "Quality and Diversity Optimization: A Unifying Modular Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimization of functions to find the best solution according to one or\nseveral objectives has a central role in many engineering and research fields.\nRecently, a new family of optimization algorithms, named Quality-Diversity\noptimization, has been introduced, and contrasts with classic algorithms.\nInstead of searching for a single solution, Quality-Diversity algorithms are\nsearching for a large collection of both diverse and high-performing solutions.\nThe role of this collection is to cover the range of possible solution types as\nmuch as possible, and to contain the best solution for each type. The\ncontribution of this paper is threefold. Firstly, we present a unifying\nframework of Quality-Diversity optimization algorithms that covers the two main\nalgorithms of this family (Multi-dimensional Archive of Phenotypic Elites and\nthe Novelty Search with Local Competition), and that highlights the large\nvariety of variants that can be investigated within this family. Secondly, we\npropose algorithms with a new selection mechanism for Quality-Diversity\nalgorithms that outperforms all the algorithms tested in this paper. Lastly, we\npresent a new collection management that overcomes the erosion issues observed\nwhen using unstructured collections. These three contributions are supported by\nextensive experimental comparisons of Quality-Diversity algorithms on three\ndifferent experimental scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 13:38:08 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Cully", "Antoine", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1708.09832", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann, Felix Lucka, Marta Betcke, Nam Huynh, Jonas Adler,\n  Ben Cox, Paul Beard, Sebastien Ourselin, and Simon Arridge", "title": "Model based learning for accelerated, limited-view 3D photoacoustic\n  tomography", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2018.2820382", "report-no": null, "categories": "cs.CV cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning for tomographic reconstructions have shown\ngreat potential to create accurate and high quality images with a considerable\nspeed-up. In this work we present a deep neural network that is specifically\ndesigned to provide high resolution 3D images from restricted photoacoustic\nmeasurements. The network is designed to represent an iterative scheme and\nincorporates gradient information of the data fit to compensate for limited\nview artefacts. Due to the high complexity of the photoacoustic forward\noperator, we separate training and computation of the gradient information. A\nsuitable prior for the desired image structures is learned as part of the\ntraining. The resulting network is trained and tested on a set of segmented\nvessels from lung CT scans and then applied to in-vivo photoacoustic\nmeasurement data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:32:29 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 14:44:48 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 14:12:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Lucka", "Felix", ""], ["Betcke", "Marta", ""], ["Huynh", "Nam", ""], ["Adler", "Jonas", ""], ["Cox", "Ben", ""], ["Beard", "Paul", ""], ["Ourselin", "Sebastien", ""], ["Arridge", "Simon", ""]]}]