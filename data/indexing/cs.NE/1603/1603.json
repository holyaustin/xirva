[{"id": "1603.00162", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Amnon Shashua", "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions", "comments": null, "journal-ref": "Proceedings of The 33rd International Conference on Machine\n  Learning, pp. 955-963, 2016", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional rectifier networks, i.e. convolutional neural networks with\nrectified linear activation and max or average pooling, are the cornerstone of\nmodern deep learning. However, despite their wide use and success, our\ntheoretical understanding of the expressive properties that drive these\nnetworks is partial at best. On the other hand, we have a much firmer grasp of\nthese issues in the world of arithmetic circuits. Specifically, it is known\nthat convolutional arithmetic circuits possess the property of \"complete depth\nefficiency\", meaning that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be implemented (or even approximated) by a shallow network. In this\npaper we describe a construction based on generalized tensor decompositions,\nthat transforms convolutional arithmetic circuits into convolutional rectifier\nnetworks. We then use mathematical tools available from the world of arithmetic\ncircuits to prove new results. First, we show that convolutional rectifier\nnetworks are universal with max pooling but not with average pooling. Second,\nand more importantly, we show that depth efficiency is weaker with\nconvolutional rectifier networks than it is with convolutional arithmetic\ncircuits. This leads us to believe that developing effective methods for\ntraining convolutional arithmetic circuits, thereby fulfilling their expressive\npotential, may give rise to a deep learning architecture that is provably\nsuperior to convolutional rectifier networks but has so far been overlooked by\npractitioners.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 06:44:34 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 12:52:16 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1603.00223", "submitter": "Liang Lu", "authors": "Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith and Steve Renals", "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", "comments": "5 pages, 2 figures, accepted by Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field (CRF)\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\nmost previous CRF-based acoustic models, it does not rely on an external system\nto provide features or segmentation boundaries. Instead, this model\nmarginalises out all the possible segmentations, and features are extracted\nfrom the RNN trained together with the segmental CRF. In essence, this model is\nself-contained and can be trained end-to-end. In this paper, we discuss\npractical training and decoding issues as well as the method to speed up the\ntraining in the context of speech recognition. We performed experiments on the\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\ndecoding --- the best reported result using CRFs, despite the fact that we only\nused a zeroth-order CRF and without using any language model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 10:43:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 10:29:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lu", "Liang", ""], ["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""], ["Renals", "Steve", ""]]}, {"id": "1603.00391", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski, Misha Denil and Yoshua Bengio", "title": "Noisy Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Common nonlinear activation functions used in neural networks can cause\ntraining difficulties due to the saturation behavior of the activation\nfunction, which may hide dependencies that are not visible to vanilla-SGD\n(using first order gradients only). Gating mechanisms that use softly\nsaturating activation functions to emulate the discrete switching of digital\nlogic circuits are good examples of this. We propose to exploit the injection\nof appropriate noise so that the gradients may flow easily, even if the\nnoiseless application of the activation function would yield zero gradient.\nLarge noise will dominate the noise-free gradient and allow stochastic gradient\ndescent toexplore more. By adding noise only to the problematic parts of the\nactivation function, we allow the optimization procedure to explore the\nboundary between the degenerate (saturating) and the well-behaved parts of the\nactivation function. We also establish connections to simulated annealing, when\nthe amount of noise is annealed down, making it easier to optimize hard\nobjective functions. We find experimentally that replacing such saturating\nactivation functions by noisy variants helps training in many contexts,\nyielding state-of-the-art or competitive results on different datasets and\ntask, especially when training seems to be the most difficult, e.g., when\ncurriculum learning is necessary to obtain good results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 18:30:15 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 20:51:57 GMT"}, {"version": "v3", "created": "Sun, 3 Apr 2016 21:41:47 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.00423", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Quantifying the vanishing gradient and long distance dependency problem\n  in recursive neural networks and recursive LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural networks (RNN) and their recently proposed extension\nrecursive long short term memory networks (RLSTM) are models that compute\nrepresentations for sentences, by recursively combining word embeddings\naccording to an externally provided parse tree. Both models thus, unlike\nrecurrent networks, explicitly make use of the hierarchical structure of a\nsentence. In this paper, we demonstrate that RNNs nevertheless suffer from the\nvanishing gradient and long distance dependency problem, and that RLSTMs\ngreatly improve over RNN's on these problems. We present an artificial learning\ntask that allows us to quantify the severity of these problems for both models.\nWe further show that a ratio of gradients (at the root node and a focal leaf\nnode) is highly indicative of the success of backpropagation at optimizing the\nrelevant weights low in the tree. This paper thus provides an explanation for\nexisting, superior results of RLSTMs on tasks such as sentiment analysis, and\nsuggests that the benefits of including hierarchical structure and of including\nLSTM-style gating are complementary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 19:45:25 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1603.00806", "submitter": "Florian Strub", "authors": "Florian Strub (SEQUEL, CRIStAL), Jeremie Mary (CRIStAL, SEQUEL),\n  Romaric Gaudel (LIFL)", "title": "Hybrid Collaborative Filtering with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering aims at exploiting the feedback of users to provide\npersonalised recommendations. Such algorithms look for latent variables in a\nlarge sparse matrix of ratings. They can be enhanced by adding side information\nto tackle the well-known cold start problem. While Neu-ral Networks have\ntremendous success in image and speech recognition, they have received less\nattention in Collaborative Filtering. This is all the more surprising that\nNeural Networks are able to discover latent variables in large and\nheterogeneous datasets. In this paper, we introduce a Collaborative Filtering\nNeural network architecture aka CFN which computes a non-linear Matrix\nFactorization from sparse rating inputs and side information. We show\nexperimentally on the MovieLens and Douban dataset that CFN outper-forms the\nstate of the art and benefits from side information. We provide an\nimplementation of the algorithm as a reusable plugin for Torch, a popular\nNeural Network framework.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 17:48:25 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 19:18:09 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 08:10:08 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Strub", "Florian", "", "SEQUEL, CRIStAL"], ["Mary", "Jeremie", "", "CRIStAL, SEQUEL"], ["Gaudel", "Romaric", "", "LIFL"]]}, {"id": "1603.00810", "submitter": "Marta R. Costa-Juss\\`a", "authors": "Marta R. Costa-Juss\\`a and Jos\\'e A. R. Fonollosa", "title": "Character-based Neural Machine Translation", "comments": "Accepted for publication at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (MT) has reached state-of-the-art results.\nHowever, one of the main challenges that neural MT still faces is dealing with\nvery large vocabularies and morphologically rich languages. In this paper, we\npropose a neural MT system using character-based embeddings in combination with\nconvolutional and highway layers to replace the standard lookup-based word\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\nembeddings are tested in a state-of-the-art neural MT based on an\nattention-based bidirectional recurrent neural network. The proposed MT scheme\nprovides improved results even when the source language is not morphologically\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 18:01:57 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 14:02:48 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 10:28:36 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Costa-Juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1603.00930", "submitter": "Adam Summerville", "authors": "Adam Summerville and Michael Mateas", "title": "Super Mario as a String: Platformer Level Generation Via LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The procedural generation of video game levels has existed for at least 30\nyears, but only recently have machine learning approaches been used to generate\nlevels without specifying the rules for generation. A number of these have\nlooked at platformer levels as a sequence of characters and performed\ngeneration using Markov chains. In this paper we examine the use of Long\nShort-Term Memory recurrent neural networks (LSTMs) for the purpose of\ngenerating levels trained from a corpus of Super Mario Brothers levels. We\nanalyze a number of different data representations and how the generated levels\nfit into the space of human authored Super Mario Brothers levels.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 23:44:03 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 21:26:58 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Summerville", "Adam", ""], ["Mateas", "Michael", ""]]}, {"id": "1603.00954", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar", "title": "Training Input-Output Recurrent Neural Networks through Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training input-output recurrent neural networks\n(RNN) for sequence labeling tasks. We propose a novel spectral approach for\nlearning the network parameters. It is based on decomposition of the\ncross-moment tensor between the output and a non-linear transformation of the\ninput, based on score functions. We guarantee consistent learning with\npolynomial sample and computational complexity under transparent conditions\nsuch as non-degeneracy of model parameters, polynomial activations for the\nneurons, and a Markovian evolution of the input sequence. We also extend our\nresults to Bidirectional RNN which uses both previous and future information to\noutput the label at each time point, and is employed in many NLP tasks such as\nPOS tagging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:14:47 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 03:46:12 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 07:03:46 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 19:52:21 GMT"}, {"version": "v5", "created": "Mon, 31 Oct 2016 18:30:51 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1603.01025", "submitter": "Edward Lee", "authors": "Daisuke Miyashita and Edward H. Lee and Boris Murmann", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in convolutional neural networks have considered model\ncomplexity and hardware efficiency to enable deployment onto embedded systems\nand mobile devices. For example, it is now well-known that the arithmetic\noperations of deep networks can be encoded down to 8-bit fixed-point without\nsignificant deterioration in performance. However, further reduction in\nprecision down to as low as 3-bit fixed-point results in significant losses in\nperformance. In this paper we propose a new data representation that enables\nstate-of-the-art networks to be encoded to 3 bits with negligible loss in\nclassification performance. To perform this, we take advantage of the fact that\nthe weights and activations in a trained network naturally have non-uniform\ndistributions. Using non-uniform, base-2 logarithmic representation to encode\nweights, communicate activations, and perform dot-products enables networks to\n1) achieve higher classification accuracies than fixed-point at the same\nresolution and 2) eliminate bulky digital multipliers. Finally, we propose an\nend-to-end training procedure that uses log representation at 5-bits, which\nachieves higher final test accuracy than linear at 5-bits.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 08:51:52 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 03:32:30 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Miyashita", "Daisuke", ""], ["Lee", "Edward H.", ""], ["Murmann", "Boris", ""]]}, {"id": "1603.01185", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Evolving Boolean Regulatory Networks with Variable Gene Expression Times", "comments": "arXiv admin note: text overlap with arXiv:1505.01980,\n  arXiv:1306.4793, arXiv:1303.7220, arXiv:1310.5568", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.NE q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time taken for gene expression varies not least because proteins vary in\nlength considerably. This paper uses an abstract, tuneable Boolean regulatory\nnetwork model to explore gene expression time variation. In particular, it is\nshown how non-uniform expression times can emerge under certain conditions\nthrough simulated evolution. That is, gene expression time variance appears\nbeneficial in the shaping of the dynamical behaviour of the regulatory network\nwithout explicit consideration of protein function.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 14:31:15 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 14:48:37 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1603.01417", "submitter": "Richard Socher", "authors": "Caiming Xiong, Stephen Merity, Richard Socher", "title": "Dynamic Memory Networks for Visual and Textual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 10:40:28 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Xiong", "Caiming", ""], ["Merity", "Stephen", ""], ["Socher", "Richard", ""]]}, {"id": "1603.01489", "submitter": "Brendan Cody-Kenny", "authors": "Brendan Cody-Kenny, Michael O'Neill, Stephen Barrett", "title": "Performance Localisation", "comments": "Major revision including extended analysis of previous results.\n  Submitted for publication, currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance becomes an issue particularly when execution cost hinders the\nfunctionality of a program. Typically a profiler can be used to find program\ncode execution which represents a large portion of the overall execution cost\nof a program. Pinpointing where a performance issue exists provides a starting\npoint for tracing cause back through a program.\n  While profiling shows where a performance issue manifests, we use mutation\nanalysis to show where a performance improvement is likely to exist. We find\nthat mutation analysis can indicate locations within a program which are highly\nimpactful to the overall execution cost of a program yet are executed\nrelatively infrequently. By better locating potential performance improvements\nin programs we hope to make performance improvement more amenable to\nautomation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 15:05:54 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 12:58:24 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Cody-Kenny", "Brendan", ""], ["O'Neill", "Michael", ""], ["Barrett", "Stephen", ""]]}, {"id": "1603.01670", "submitter": "Tao Wei", "authors": "Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen", "title": "Network Morphism", "comments": "Under review for ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 02:06:43 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 16:36:00 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Wei", "Tao", ""], ["Wang", "Changhu", ""], ["Rui", "Yong", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1603.01913", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Gholamreza Haffari and Jacob Eisenstein", "title": "A Latent Variable Recurrent Neural Network for Discourse Relation\n  Language Models", "comments": "NAACL 2016 camera ready, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel latent variable recurrent neural network\narchitecture for jointly modeling sequences of words and (possibly latent)\ndiscourse relations between adjacent sentences. A recurrent neural network\ngenerates individual words, thus reaping the benefits of\ndiscriminatively-trained vector representations. The discourse relations are\nrepresented with a latent variable, which can be predicted or marginalized,\ndepending on the task. The resulting model can therefore employ a training\nobjective that includes not only discourse relation classification, but also\nword prediction. As a result, it outperforms state-of-the-art alternatives for\ntwo tasks: implicit discourse relation classification in the Penn Discourse\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\nby marginalizing over latent discourse relations at test time, we obtain a\ndiscourse informed language model, which improves over a strong LSTM baseline.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 01:54:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:58:10 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Ji", "Yangfeng", ""], ["Haffari", "Gholamreza", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1603.02238", "submitter": "Stanis{\\l}aw  Ambroszkiewicz", "authors": "Stanislaw Ambroszkiewicz", "title": "On higher order computations, rewiring the connectome, and non-von\n  Neumann computer architecture", "comments": "This version: a substantial extension and revision of the paper\n  published in Proc. ICANN2016. Keywords: computations in human brain, higher\n  order functions and functionals, synaptic meta-plasticity, glia and\n  atrocytes, non-von Neumann computer architecture, Backus' function-level\n  programming language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural plasticity in the brain (i.e. rewiring the connectome) may be\nviewed as mechanisms for dynamic reconfiguration of neural circuits. First\norder computations in the brain are done by static neural circuits, whereas\nhigher order computations are done by dynamic reconfigurations of the links\n(synapses) between the neural circuits. Static neural circuits correspond to\nfirst order computable functions. Synapse creation (activation) between them\ncorrespond to the mathematical notion of function composition. Functionals are\nhigher order functions that take functions as their arguments. The construction\nof functionals is based on dynamic reconfigurations of function compositions.\nPerhaps the functionals correspond to rewiring mechanisms of the connectome.\nThe architecture of human mind is different than the von Neumann computer\narchitecture. Higher order computations in the human brain (based on\nfunctionals) may suggest a non-von Neumann computer architecture, a challenge\nposed by John Backus in 1977 \\cite{Backus}. The presented work is a substantial\nextension and revision of the paper published in Proc. ICANN2016.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:10:16 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 13:04:57 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 12:26:15 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2020 21:31:08 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ambroszkiewicz", "Stanislaw", ""]]}, {"id": "1603.02636", "submitter": "Lucas Beyer", "authors": "Lucas Beyer and Alexander Hermans and Bastian Leibe", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range\n  Data", "comments": "Lucas Beyer and Alexander Hermans contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the DROW detector, a deep learning based detector for 2D range\ndata. Laser scanners are lighting invariant, provide accurate range data, and\ntypically cover a large field of view, making them interesting sensors for\nrobotics applications. So far, research on detection in laser range data has\nbeen dominated by hand-crafted features and boosted classifiers, potentially\nlosing performance due to suboptimal design choices. We propose a Convolutional\nNeural Network (CNN) based detector for this task. We show how to effectively\napply CNNs for detection in 2D range data, and propose a depth preprocessing\nstep and voting scheme that significantly improve CNN performance. We\ndemonstrate our approach on wheelchairs and walkers, obtaining state of the art\ndetection results. Apart from the training data, none of our design choices\nlimits the detector to these two classes, though. We provide a ROS node for our\ndetector and release our dataset containing 464k laser scans, out of which 24k\nwere annotated.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:39:19 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 18:06:28 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Beyer", "Lucas", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "1603.02776", "submitter": "Yang Liu", "authors": "Yang Liu, Sujian Li, Xiaodong Zhang and Zhifang Sui", "title": "Implicit Discourse Relation Classification via Multi-Task Neural\n  Networks", "comments": "This is the pre-print version of a paper accepted by AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without discourse connectives, classifying implicit discourse relations is a\nchallenging task and a bottleneck for building a practical discourse parser.\nPrevious research usually makes use of one kind of discourse framework such as\nPDTB or RST to improve the classification performance on discourse relations.\nActually, under different discourse annotation frameworks, there exist multiple\ncorpora which have internal connections. To exploit the combination of\ndifferent discourse corpora, we design related discourse classification tasks\nspecific to a corpus, and propose a novel Convolutional Neural Network embedded\nmulti-task learning system to synthesize these tasks by learning both unique\nand shared representations for each task. The experimental results on the PDTB\nimplicit discourse relation classification task demonstrate that our model\nachieves significant gains over baseline systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 03:13:37 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Liu", "Yang", ""], ["Li", "Sujian", ""], ["Zhang", "Xiaodong", ""], ["Sui", "Zhifang", ""]]}, {"id": "1603.03116", "submitter": "Antonio Valerio Miceli Barone", "authors": "Antonio Valerio Miceli Barone", "title": "Low-rank passthrough neural networks", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various common deep learning architectures, such as LSTMs, GRUs, Resnets and\nHighway Networks, employ state passthrough connections that support training\nwith high feed-forward depth or recurrence over many time steps. These\n\"Passthrough Networks\" architectures also enable the decoupling of the network\nstate size from the number of parameters of the network, a possibility has been\nstudied by \\newcite{Sak2014} with their low-rank parametrization of the LSTM.\nIn this work we extend this line of research, proposing effective, low-rank and\nlow-rank plus diagonal matrix parametrizations for Passthrough Networks which\nexploit this decoupling property, reducing the data complexity and memory\nrequirements of the network while preserving its memory capacity. This is\nparticularly beneficial in low-resource settings as it supports expressive\nmodels with a compact parametrization less susceptible to overfitting. We\npresent competitive experimental results on several tasks, including language\nmodeling and a near state of the art result on sequential randomly-permuted\nMNIST classification, a hard task on natural data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 01:04:07 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 19:38:30 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 16:19:29 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""]]}, {"id": "1603.03149", "submitter": "Prashant Sharma Mr.", "authors": "Prashant Sharma, Shaju K. Albert, S. Rajeswari", "title": "Real time error detection in metal arc welding process using Artificial\n  Neural Netwroks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assurance in production line demands reliable weld joints. Human made\nerrors is a major cause of faulty production. Promptly Identifying errors in\nthe weld while welding is in progress will decrease the post inspection cost\nspent on the welding process. Electrical parameters generated during welding,\ncould able to characterize the process efficiently. Parameter values are\ncollected using high speed data acquisition system. Time series analysis tasks\nsuch as filtering, pattern recognition etc. are performed over the collected\ndata. Filtering removes the unwanted noisy signal components and pattern\nrecognition task segregate error patterns in the time series based upon\nsimilarity, which is performed by Self Organized mapping clustering algorithm.\nWelder quality is thus compared by detecting and counting number of error\npatterns appeared in his parametric time series. Moreover, Self Organized\nmapping algorithm provides the database in which patterns are segregated into\ntwo classes either desirable or undesirable. Database thus generated is used to\ntrain the classification algorithms, and thereby automating the real time error\ndetection task. Multi Layer Perceptron and Radial basis function are the two\nclassification algorithms used, and their performance has been compared based\non metrics such as specificity, sensitivity, accuracy and time required in\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 05:00:26 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Sharma", "Prashant", ""], ["Albert", "Shaju K.", ""], ["Rajeswari", "S.", ""]]}, {"id": "1603.03657", "submitter": "Koen Groenland", "authors": "Koen Groenland, Sander Bohte", "title": "Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 15:16:09 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Groenland", "Koen", ""], ["Bohte", "Sander", ""]]}, {"id": "1603.03685", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Filippo Maria Bianchi, Cesare Alippi", "title": "Determination of the edge of criticality in echo state networks through\n  Fisher information maximization", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2016.2644268", "report-no": null, "categories": "physics.data-an cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a widely accepted fact that the computational capability of recurrent\nneural networks is maximized on the so-called \"edge of criticality\". Once the\nnetwork operates in this configuration, it performs efficiently on a specific\napplication both in terms of (i) low prediction error and (ii) high short-term\nmemory capacity. Since the behavior of recurrent networks is strongly\ninfluenced by the particular input signal driving the dynamics, a universal,\napplication-independent method for determining the edge of criticality is still\nmissing. In this paper, we aim at addressing this issue by proposing a\ntheoretically motivated, unsupervised method based on Fisher information for\ndetermining the edge of criticality in recurrent neural networks. It is proven\nthat Fisher information is maximized for (finite-size) systems operating in\nsuch critical regions. However, Fisher information is notoriously difficult to\ncompute and either requires the probability density function or the conditional\ndependence of the system states with respect to the model parameters. The paper\ntakes advantage of a recently-developed non-parametric estimator of the Fisher\ninformation matrix and provides a method to determine the critical region of\necho state networks, a particular class of recurrent networks. The considered\ncontrol parameters, which indirectly affect the echo state network performance,\nare explored to identify those configurations lying on the edge of criticality\nand, as such, maximizing Fisher information and computational performance.\nExperimental results on benchmarks and real-world data demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 16:32:23 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 20:21:29 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Livi", "Lorenzo", ""], ["Bianchi", "Filippo Maria", ""], ["Alippi", "Cesare", ""]]}, {"id": "1603.03795", "submitter": "Vanessa Volz", "authors": "Vanessa Volz, G\\\"unter Rudolph, Boris Naujoks", "title": "Demonstrating the Feasibility of Automatic Game Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game balancing is an important part of the (computer) game design process, in\nwhich designers adapt a game prototype so that the resulting gameplay is as\nentertaining as possible. In industry, the evaluation of a game is often based\non costly playtests with human players. It suggests itself to automate this\nprocess using surrogate models for the prediction of gameplay and outcome. In\nthis paper, the feasibility of automatic balancing using simulation- and\ndeck-based objectives is investigated for the card game top trumps.\nAdditionally, the necessity of a multi-objective approach is asserted by a\ncomparison with the only known (single-objective) method. We apply a\nmulti-objective evolutionary algorithm to obtain decks that optimise\nobjectives, e.g. win rate and average number of tricks, developed to express\nthe fairness and the excitement of a game of top trumps. The results are\ncompared with decks from published top trumps decks using simulation-based\nobjectives. The possibility to generate decks better or at least as good as\ndecks from published top trumps decks in terms of these objectives is\ndemonstrated. Our results indicate that automatic balancing with the presented\napproach is feasible even for more complex games such as real-time strategy\ngames.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:36:27 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Volz", "Vanessa", ""], ["Rudolph", "G\u00fcnter", ""], ["Naujoks", "Boris", ""]]}, {"id": "1603.03827", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt", "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "comments": "Accepted as a conference paper at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:02:51 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""]]}, {"id": "1603.04000", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Learning Typographic Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typography is a ubiquitous art form that affects our understanding,\nperception, and trust in what we read. Thousands of different font-faces have\nbeen created with enormous variations in the characters. In this paper, we\nlearn the style of a font by analyzing a small subset of only four letters.\nFrom these four letters, we learn two tasks. The first is a discrimination\ntask: given the four letters and a new candidate letter, does the new letter\nbelong to the same font? Second, given the four basis letters, can we generate\nall of the other letters with the same characteristics as those in the basis\nset? We use deep neural networks to address both tasks, quantitatively and\nqualitatively measure the results in a variety of novel manners, and present a\nthorough investigation of the weaknesses and strengths of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 05:44:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "1603.04080", "submitter": "Chetan Singh Thakur", "authors": "Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan\n  Tapson, Andr\\'e van Schaik", "title": "A Stochastic Approach to STDP", "comments": "IEEE-International Symposium on Circuits and Systems (ISCAS)-2016", "journal-ref": null, "doi": "10.1109/ISCAS.2016.7538989", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a digital implementation of the Spike Timing Dependent Plasticity\n(STDP) learning rule. The proposed digital implementation consists of an\nexponential decay generator array and a STDP adaptor array. On the arrival of a\npre- and post-synaptic spike, the STDP adaptor will send a digital spike to the\ndecay generator. The decay generator will then generate an exponential decay,\nwhich will be used by the STDP adaptor to perform the weight adaption. The\nexponential decay, which is computational expensive, is efficiently implemented\nby using a novel stochastic approach, which we analyse and characterise here.\nWe use a time multiplexing approach to achieve 8192 (8k) virtual STDP adaptors\nand decay generators with only one physical implementation of each. We have\nvalidated our stochastic STDP approach with measurement results of a balanced\nexcitation/inhibition experiment. Our stochastic approach is ideal for\nimplementing the STDP learning rule in large-scale spiking neural networks\nrunning in real time.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 21:44:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wang", "Runchun", ""], ["Thakur", "Chetan Singh", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1603.04223", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Gregory Cohen, Tara Julia Hamilton, Jonathan Tapson,\n  Andre van Schaik", "title": "Investigation of event-based memory surfaces for high-speed tracking,\n  unsupervised feature extraction and object recognition", "comments": "This is an updated version of a previously submitted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we compare event-based decaying and time based-decaying memory\nsurfaces for high-speed eventbased tracking, feature extraction, and object\nclassification using an event-based camera. The high-speed recognition task\ninvolves detecting and classifying model airplanes that are dropped free-hand\nclose to the camera lens so as to generate a challenging dataset exhibiting\nsignificant variance in target velocity. This variance motivated the\ninvestigation of event-based decaying memory surfaces in comparison to\ntime-based decaying memory surfaces to capture the temporal aspect of the\nevent-based data. These surfaces are then used to perform unsupervised feature\nextraction, tracking and recognition. In order to generate the memory surfaces,\nevent binning, linearly decaying kernels, and exponentially decaying kernels\nwere investigated with exponentially decaying kernels found to perform best.\nEvent-based decaying memory surfaces were found to outperform time-based\ndecaying memory surfaces in recognition especially when invariance to target\nvelocity was made a requirement. A range of network and receptive field sizes\nwere investigated. The system achieves 98.75% recognition accuracy within 156\nmilliseconds of an airplane entering the field of view, using only twenty-five\nevent-based feature extracting neurons in series with a linear classifier. By\ncomparing the linear classifier results to an ELM classifier, we find that a\nsmall number of event-based feature extractors can effectively project the\ncomplex spatio-temporal event patterns of the dataset to an almost linearly\nseparable representation in feature space.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 11:54:15 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 02:11:48 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 10:39:46 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Afshar", "Saeed", ""], ["Cohen", "Gregory", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""]]}, {"id": "1603.04904", "submitter": "Roderich Gross", "authors": "Wei Li, Melvin Gauci and Roderich Gross", "title": "Turing learning: a metric-free approach to inferring behavior and its\n  application to swarms", "comments": "camera-ready version", "journal-ref": "Swarm Intelligence, 10(3):211-243, 2016", "doi": "10.1007/s11721-016-0126-1", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Turing Learning, a novel system identification method for\ninferring the behavior of natural or artificial systems. Turing Learning\nsimultaneously optimizes two populations of computer programs, one representing\nmodels of the behavior of the system under investigation, and the other\nrepresenting classifiers. By observing the behavior of the system as well as\nthe behaviors produced by the models, two sets of data samples are obtained.\nThe classifiers are rewarded for discriminating between these two sets, that\nis, for correctly categorizing data samples as either genuine or counterfeit.\nConversely, the models are rewarded for 'tricking' the classifiers into\ncategorizing their data samples as genuine. Unlike other methods for system\nidentification, Turing Learning does not require predefined metrics to quantify\nthe difference between the system and its models. We present two case studies\nwith swarms of simulated robots and prove that the underlying behaviors cannot\nbe inferred by a metric-based system identification method. By contrast, Turing\nLearning infers the behaviors with high accuracy. It also produces a useful\nby-product - the classifiers - that can be used to detect abnormal behavior in\nthe swarm. Moreover, we show that Turing Learning also successfully infers the\nbehavior of physical robot swarms. The results show that collective behaviors\ncan be directly inferred from motion trajectories of individuals in the swarm,\nwhich may have significant implications for the study of animal collectives.\nFurthermore, Turing Learning could prove useful whenever a behavior is not\neasily characterizable using metrics, making it suitable for a wide range of\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 22:20:52 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 07:37:00 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Li", "Wei", ""], ["Gauci", "Melvin", ""], ["Gross", "Roderich", ""]]}, {"id": "1603.05189", "submitter": "Adam Last", "authors": "Adam J. Last", "title": "Applying Artifical Neural Networks To Predict Nominal Vehicle\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of artificial neural networks (ANNs) to\nreplace traditional algorithms and manual review for identifying anomalies in\nvehicle run data. The specific data used for this study is from undersea\nvehicle qualification tests. Such data is highly non-linear, therefore\ntraditional algorithms are not adequate and manual review is time consuming. By\nusing ANNs to predict nominal vehicle performance based solely on information\navailable pre-run, vehicle deviation from expected performance can be\nautomatically identified in the post-run data. Such capability is only now\nbecoming available due to the rapid increase in understanding of ANN framework\nand available computing power in the past decade. The ANN trained for the\npurpose of this investigation is relatively simple, to keep the computing\nrequirements within the parameters of a modern desktop PC. This ANN showed\npotential in predicting vehicle performance, particularly during transient\nevents within the run data. However, there were also several performance cases,\nsuch as steady state operation and cases which did not have sufficient training\ndata, where the ANN showed deficiencies. It is expected that as computational\npower becomes more readily available, ANN understanding matures, and more\ntraining data is acquired from real world tests, the performance predictions of\nthe ANN will surpass traditional algorithms and manual human review.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 17:47:57 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Last", "Adam J.", ""]]}, {"id": "1603.05594", "submitter": "Enmei Tu", "authors": "Enmei Tu, Nikola Kasabov and Jie Yang", "title": "Mapping Temporal Variables into the NeuCube for Improved Pattern\n  Recognition, Predictive Modelling and Understanding of Stream Data", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for an optimized mapping of temporal\nvariables, describing a temporal stream data, into the recently proposed\nNeuCube spiking neural network architecture. This optimized mapping extends the\nuse of the NeuCube, which was initially designed for spatiotemporal brain data,\nto work on arbitrary stream data and to achieve a better accuracy of temporal\npattern recognition, a better and earlier event prediction and a better\nunderstanding of complex temporal stream data through visualization of the\nNeuCube connectivity. The effect of the new mapping is demonstrated on three\nbench mark problems. The first one is early prediction of patient sleep stage\nevent from temporal physiological data. The second one is pattern recognition\nof dynamic temporal patterns of traffic in the Bay Area of California and the\nlast one is the Challenge 2012 contest data set. In all cases the use of the\nproposed mapping leads to an improved accuracy of pattern recognition and event\nprediction and a better understanding of the data when compared to traditional\nmachine learning techniques or spiking neural network reservoirs with arbitrary\nmapping of the variables.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 17:58:48 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Tu", "Enmei", ""], ["Kasabov", "Nikola", ""], ["Yang", "Jie", ""]]}, {"id": "1603.05643", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Variance Reduction for Faster Non-Convex Optimization", "comments": "polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem in non-convex optimization of efficiently\nreaching a stationary point. In contrast to the convex case, in the long\nhistory of this basic problem, the only known theoretical results on\nfirst-order non-convex optimization remain to be full gradient descent that\nconverges in $O(1/\\varepsilon)$ iterations for smooth objectives, and\nstochastic gradient descent that converges in $O(1/\\varepsilon^2)$ iterations\nfor objectives that are sum of smooth functions.\n  We provide the first improvement in this line of research. Our result is\nbased on the variance reduction trick recently introduced to convex\noptimization, as well as a brand new analysis of variance reduction that is\nsuitable for non-convex optimization. For objectives that are sum of smooth\nfunctions, our first-order minibatch stochastic method converges with an\n$O(1/\\varepsilon)$ rate, and is faster than full gradient descent by\n$\\Omega(n^{1/3})$.\n  We demonstrate the effectiveness of our methods on empirical risk\nminimizations with non-convex loss functions and training neural nets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:55:12 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 02:34:00 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05670", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Peter Sarlin", "title": "Bank distress in the news: Describing events through deep learning", "comments": "Forthcoming in Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:1507.07870 [in version 1]", "journal-ref": "Neurocomputing, 264, 2017", "doi": "10.1016/j.neucom.2016.12.11", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.NE q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many models are purposed for detecting the occurrence of significant\nevents in financial systems, the task of providing qualitative detail on the\ndevelopments is not usually as well automated. We present a deep learning\napproach for detecting relevant discussion in text and extracting natural\nlanguage descriptions of events. Supervised by only a small set of event\ninformation, comprising entity names and dates, the model is leveraged by\nunsupervised learning of semantic vector representations on extensive text\ndata. We demonstrate applicability to the study of financial risk based on news\n(6.6M articles), particularly bank distress and government interventions (243\nevents), where indices can signal the level of bank-stress-related reporting at\nthe entity level, or aggregated at national or European level, while being\ncoupled with explanations. Thus, we exemplify how text, as timely, widely\navailable and descriptive data, can serve as a useful complementary source of\ninformation for financial and systemic risk analytics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 20:06:27 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 23:24:49 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1603.05824", "submitter": "Lars Hertel", "authors": "Lars Hertel, Huy Phan, Alfred Mertins", "title": "Comparing Time and Frequency Domain for Audio Event Recognition Using\n  Deep Learning", "comments": "5 pages, accepted version for publication in Proceedings of the IEEE\n  International Joint Conference on Neural Networks (IJCNN), July 2016,\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing acoustic events is an intricate problem for a machine and an\nemerging field of research. Deep neural networks achieve convincing results and\nare currently the state-of-the-art approach for many tasks. One advantage is\ntheir implicit feature learning, opposite to an explicit feature extraction of\nthe input signal. In this work, we analyzed whether more discriminative\nfeatures can be learned from either the time-domain or the frequency-domain\nrepresentation of the audio signal. For this purpose, we trained multiple deep\nnetworks with different architectures on the Freiburg-106 and ESC-10 datasets.\nOur results show that feature learning from the frequency domain is superior to\nthe time domain. Moreover, additionally using convolution and pooling layers,\nto explore local structures of the audio signal, significantly improves the\nrecognition performance and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 10:38:23 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Hertel", "Lars", ""], ["Phan", "Huy", ""], ["Mertins", "Alfred", ""]]}, {"id": "1603.06042", "submitter": "Daniel Andor", "authors": "Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro\n  Presta, Kuzman Ganchev, Slav Petrov and Michael Collins", "title": "Globally Normalized Transition-Based Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a globally normalized transition-based neural network model that\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\nsentence compression results. Our model is a simple feed-forward neural network\nthat operates on a task-specific transition system, yet achieves comparable or\nbetter accuracies than recurrent models. We discuss the importance of global as\nopposed to local normalization: a key insight is that the label bias problem\nimplies that globally normalized models can be strictly more expressive than\nlocally normalized models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:56:03 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 13:43:30 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Andor", "Daniel", ""], ["Alberti", "Chris", ""], ["Weiss", "David", ""], ["Severyn", "Aliaksei", ""], ["Presta", "Alessandro", ""], ["Ganchev", "Kuzman", ""], ["Petrov", "Slav", ""], ["Collins", "Michael", ""]]}, {"id": "1603.06111", "submitter": "Lili Mou", "authors": "Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin", "title": "How Transferable are Neural Networks in NLP Applications?", "comments": "Accepted by EMNLP-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is aimed to make use of valuable knowledge in a source\ndomain to help model performance in a target domain. It is particularly\nimportant to neural networks, which are very likely to be overfitting. In some\nfields like image processing, many studies have shown the effectiveness of\nneural network-based transfer learning. For neural NLP, however, existing\nstudies have only casually applied transfer learning, and conclusions are\ninconsistent. In this paper, we conduct systematic case studies and provide an\nilluminating picture on the transferability of neural networks in NLP.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 16:38:31 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:45:31 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mou", "Lili", ""], ["Meng", "Zhao", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1603.06127", "submitter": "Petr Baudi\\v{s}", "authors": "Petr Baudi\\v{s}, Jan Pichl, Tom\\'a\\v{s} Vysko\\v{c}il, Jan \\v{S}ediv\\'y", "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "comments": "submitted as paper to CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:35:26 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 03:10:26 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 22:17:36 GMT"}, {"version": "v4", "created": "Tue, 17 May 2016 14:08:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Baudi\u0161", "Petr", ""], ["Pichl", "Jan", ""], ["Vysko\u010dil", "Tom\u00e1\u0161", ""], ["\u0160ediv\u00fd", "Jan", ""]]}, {"id": "1603.06141", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e, Kevin Engel, Nick Fung, Isaac Julien", "title": "Evolving Shepherding Behavior with Genetic Programming Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply genetic programming techniques to the `shepherding' problem, in\nwhich a group of one type of animal (sheep dogs) attempts to control the\nmovements of a second group of animals (sheep) obeying flocking behavior. Our\ngenetic programming algorithm evolves an expression tree that governs the\nmovements of each dog. The operands of the tree are hand-selected features of\nthe simulation environment that may allow the dogs to herd the sheep\neffectively. The algorithm uses tournament-style selection, crossover\nreproduction, and a point mutation. We find that the evolved solutions\ngeneralize well and outperform a (naive) human-designed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 20:36:44 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Brul\u00e9", "Joshua", ""], ["Engel", "Kevin", ""], ["Fung", "Nick", ""], ["Julien", "Isaac", ""]]}, {"id": "1603.06160", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Stochastic Variance Reduction for Nonconvex Optimization", "comments": "Minor feedback changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonconvex finite-sum problems and analyze stochastic variance\nreduced gradient (SVRG) methods for them. SVRG and related methods have\nrecently surged into prominence for convex optimization given their edge over\nstochastic gradient descent (SGD); but their theoretical analysis almost\nexclusively assumes convexity. In contrast, we prove non-asymptotic rates of\nconvergence (to stationary points) of SVRG for nonconvex optimization, and show\nthat it is provably faster than SGD and gradient descent. We also analyze a\nsubclass of nonconvex problems on which SVRG attains linear convergence to the\nglobal optimum. We extend our analysis to mini-batch variants of SVRG, showing\n(theoretical) linear speedup due to mini-batching in parallel settings.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 23:37:38 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 23:08:20 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Hefny", "Ahmed", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1603.06212", "submitter": "Randal Olson", "authors": "Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore", "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating\n  Data Science", "comments": "8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet\n  made from reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the field of data science continues to grow, there will be an\never-increasing demand for tools that make machine learning accessible to\nnon-experts. In this paper, we introduce the concept of tree-based pipeline\noptimization for automating one of the most tedious parts of machine\nlearning---pipeline design. We implement an open source Tree-based Pipeline\nOptimization Tool (TPOT) in Python and demonstrate its effectiveness on a\nseries of simulated and real-world benchmark data sets. In particular, we show\nthat TPOT can design machine learning pipelines that provide a significant\nimprovement over a basic machine learning analysis while requiring little to no\ninput nor prior knowledge from the user. We also address the tendency for TPOT\nto design overly complex pipelines by integrating Pareto optimization, which\nproduces compact pipelines without sacrificing classification accuracy. As\nsuch, this work represents an important step toward fully automating machine\nlearning pipeline design.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 13:32:27 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Olson", "Randal S.", ""], ["Bartley", "Nathan", ""], ["Urbanowicz", "Ryan J.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1603.06353", "submitter": "Martijn Arts", "authors": "Martijn Arts, Marius Cordts, Monika Gorin, Marc Spehr and Rudolf\n  Mathar", "title": "A Discontinuous Neural Network for Non-Negative Sparse Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a discontinuous neural network which is used as a\nmodel of the mammalian olfactory system and can more generally be applied to\nsolve non-negative sparse approximation problems. By inherently limiting the\nsystems integrators to having non-negative outputs, the system function becomes\ndiscontinuous since the integrators switch between being inactive and being\nactive. It is shown that the presented network converges to equilibrium points\nwhich are solutions to general non-negative least squares optimization\nproblems. We specify a Caratheodory solution and prove that the network is\nstable, provided that the system matrix has full column-rank. Under a mild\ncondition on the equilibrium point, we show that the network converges to its\nequilibrium within a finite number of switches. Two applications of the neural\nnetwork are shown. Firstly, we apply the network as a model of the olfactory\nsystem and show that in principle it may be capable of performing complex\nsparse signal recovery tasks. Secondly, we generalize the application to\ninclude non-negative sparse approximation problems and compare the recovery\nperformance to a classical non-negative basis pursuit denoising algorithm. We\nconclude that the recovery performance differs only marginally from the\nclassical algorithm, while the neural network has the advantage that no\nperformance critical regularization parameter has to be chosen prior to\nrecovery.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 08:30:43 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Arts", "Martijn", ""], ["Cordts", "Marius", ""], ["Gorin", "Monika", ""], ["Spehr", "Marc", ""], ["Mathar", "Rudolf", ""]]}, {"id": "1603.06374", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Analyzing coevolutionary games with dynamic fitness landscapes", "comments": null, "journal-ref": "Proc. IEEE Congress on Evolutionary Computation, IEEE CEC 2016,\n  (Ed.: Y. S. Ong), IEEE Press, Piscataway, NJ, 2016, 610-616", "doi": null, "report-no": null, "categories": "q-bio.PE cs.GT cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coevolutionary games cast players that may change their strategies as well as\ntheir networks of interaction. In this paper a framework is introduced for\ndescribing coevolutionary game dynamics by landscape models. It is shown that\ncoevolutionary games invoke dynamic landscapes. Numerical experiments are shown\nfor a prisoner's dilemma (PD) and a snow drift (SD) game that both use either\nbirth-death (BD) or death-birth (DB) strategy updating. The resulting\nlandscapes are analyzed with respect to modality and ruggedness\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 10:13:12 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "1603.06393", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Zhengdong Lu, Hang Li and Victor O.K. Li", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "comments": "10 pages, 5 figures, accepted by ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 11:35:08 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 03:33:58 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 13:53:21 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1603.06624", "submitter": "R Devon Hjelm", "authors": "R. Devon Hjelm and Sergey M. Plis and Vince C. Calhoun", "title": "Variational Autoencoders for Feature Detection of Magnetic Resonance\n  Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA), as an approach to the blind\nsource-separation (BSS) problem, has become the de-facto standard in many\nmedical imaging settings. Despite successes and a large ongoing research\neffort, the limitation of ICA to square linear transformations have not been\novercome, so that general INFOMAX is still far from being realized. As an\nalternative, we present feature analysis in medical imaging as a problem solved\nby Helmholtz machines, which include dimensionality reduction and\nreconstruction of the raw data under the same objective, and which recently\nhave overcome major difficulties in inference and learning with deep and\nnonlinear configurations. We demonstrate one approach to training Helmholtz\nmachines, variational auto-encoders (VAE), as a viable approach toward feature\nextraction with magnetic resonance imaging (MRI) data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 21:31:36 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Hjelm", "R. Devon", ""], ["Plis", "Sergey M.", ""], ["Calhoun", "Vince C.", ""]]}, {"id": "1603.06665", "submitter": "Richard Kiehl", "authors": "Richard A. Kiehl", "title": "Information Processing by Nonlinear Phase Dynamics in Locally Connected\n  Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research toward powerful information processing systems that circumvent the\ninterconnect bottleneck by exploiting the nonlinear evolution of multiple phase\ndynamics in locally connected arrays is discussed. We focus on a scheme in\nwhich logic states are defined by the electrical phase of a dynamic process and\ninformation processing is realized through interactions between the elements in\nthe array. Simulation results are given for networks comprised of neuron-like\nintegrate-and-fire elements, which could potentially be implemented by\nultra-small tunnel junctions, molecules and other types of nanoscale elements.\nThis approach could lead to powerful information processing systems due to\nmassive parallelism in simple, highly scalable nano-architectures. The rational\nfor this approach, its advantages, simulation results, critical issues, and\nfuture research directions are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 03:14:00 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Kiehl", "Richard A.", ""]]}, {"id": "1603.06744", "submitter": "Wang Ling", "authors": "Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tom\\'a\\v{s}\n  Ko\\v{c}isk\\'y, Andrew Senior, Fumin Wang, Phil Blunsom", "title": "Latent Predictor Networks for Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many language generation tasks require the production of text conditioned on\nboth structured and unstructured inputs. We present a novel neural network\narchitecture which generates an output sequence conditioned on an arbitrary\nnumber of input functions. Crucially, our approach allows both the choice of\nconditioning context and the granularity of generation, for example characters\nor tokens, to be marginalised, thus permitting scalable and effective training.\nUsing this framework, we address the problem of generating programming code\nfrom a mixed natural language and structured specification. We create two new\ndata sets for this paradigm derived from the collectible trading card games\nMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,\nwe demonstrate that marginalising multiple predictors allows our model to\noutperform strong benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 11:41:51 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 14:46:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Ling", "Wang", ""], ["Grefenstette", "Edward", ""], ["Hermann", "Karl Moritz", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Senior", "Andrew", ""], ["Wang", "Fumin", ""], ["Blunsom", "Phil", ""]]}, {"id": "1603.06788", "submitter": "Irina Petrova", "authors": "Arkady Rost and Irina Petrova and Arina Buzdalova", "title": "Adaptive Parameter Selection in Evolutionary Algorithms by Reinforcement\n  Learning with Dynamic Discretization of Parameter Range", "comments": "this is a full version of a paper which has been accepted as a poster\n  to GECCO conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online parameter controllers for evolutionary algorithms adjust values of\nparameters during the run of an evolutionary algorithm. Recently a new\nefficient parameter controller based on reinforcement learning was proposed by\nKarafotias et al. In this method ranges of parameters are discretized into\nseveral intervals before the run. However, performing adaptive discretization\nduring the run may increase efficiency of an evolutionary algorithm. Aleti et\nal. proposed another efficient controller with adaptive discretization.\n  In the present paper we propose a parameter controller based on reinforcement\nlearning with adaptive discretization. The proposed controller is compared with\nthe existing parameter adjusting methods on several test problems using\ndifferent configurations of an evolutionary algorithm. For the test problems,\nwe consider four continuous functions, namely the sphere function, the\nRosenbrock function, the Levi function and the Rastrigin function. Results show\nthat the new controller outperforms the other controllers on most of the\nconsidered test problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 13:40:05 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Rost", "Arkady", ""], ["Petrova", "Irina", ""], ["Buzdalova", "Arina", ""]]}, {"id": "1603.06807", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Alberto Garc\\'ia-Dur\\'an, Caglar Gulcehre, Sungjin\n  Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M\n  Factoid Question-Answer Corpus", "comments": "13 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:25:16 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 20:00:20 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Chandar", "Sarath", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.06859", "submitter": "Fabricio de Franca Olivetti", "authors": "Andr\\'e L. V. Coelho and Fabr\\'icio O. de Fran\\c{c}a", "title": "Enhanced perceptrons using contrastive biclusters", "comments": "article under review by Neural Computing and Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptrons are neuronal devices capable of fully discriminating linearly\nseparable classes. Although straightforward to implement and train, their\napplicability is usually hindered by non-trivial requirements imposed by\nreal-world classification problems. Therefore, several approaches, such as\nkernel perceptrons, have been conceived to counteract such difficulties. In\nthis paper, we investigate an enhanced perceptron model based on the notion of\ncontrastive biclusters. From this perspective, a good discriminative bicluster\ncomprises a subset of data instances belonging to one class that show high\ncoherence across a subset of features and high differentiation from nearest\ninstances of the other class under the same features (referred to as its\ncontrastive bicluster). Upon each local subspace associated with a pair of\ncontrastive biclusters a perceptron is trained and the model with highest area\nunder the receiver operating characteristic curve (AUC) value is selected as\nthe final classifier. Experiments conducted on a range of data sets, including\nthose related to a difficult biosignal classification problem, show that the\nproposed variant can be indeed very useful, prevailing in most of the cases\nupon standard and kernel perceptrons in terms of accuracy and AUC measures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 16:32:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Coelho", "Andr\u00e9 L. V.", ""], ["de Fran\u00e7a", "Fabr\u00edcio O.", ""]]}, {"id": "1603.07044", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang and James Glass", "title": "Recurrent Neural Network Encoder with Attention for Community Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a general recurrent neural network (RNN) encoder framework to\ncommunity question answering (cQA) tasks. Our approach does not rely on any\nlinguistic processing, and can be applied to different languages or domains.\nFurther improvements are observed when we extend the RNN encoders with a neural\nattention mechanism that encourages reasoning over entire sequences. To deal\nwith practical issues such as data sparsity and imbalanced labels, we apply\nvarious techniques such as transfer learning and multitask learning. Our\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\ncompared to an information retrieval-based approach, and achieve comparable\nperformance to a strong handcrafted feature-based method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 01:52:54 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1603.07051", "submitter": "Mohamed El Yafrani", "authors": "Mohamed El Yafrani and Bela\\\"id Ahiod", "title": "Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief\n  Problem", "comments": "12th ACS/IEEE International Conference on Computer Systems and\n  Applications (AICCSA) 2015. November 17-20, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world problems are very difficult to optimize. However, many researchers\nhave been solving benchmark problems that have been extensively investigated\nfor the last decades even if they have very few direct applications. The\nTraveling Thief Problem (TTP) is a NP-hard optimization problem that aims to\nprovide a more realistic model. TTP targets particularly routing problem under\npacking/loading constraints which can be found in supply chain management and\ntransportation. In this paper, TTP is presented and formulated mathematically.\nA combined local search algorithm is proposed and compared with Random Local\nSearch (RLS) and Evolutionary Algorithm (EA). The obtained results are quite\npromising since new better solutions were found.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 02:30:35 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Yafrani", "Mohamed El", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1603.07249", "submitter": "Juan C. Cuevas-Tello", "authors": "Juan C. Cuevas-Tello and Manuel Valenzuela-Rendon and Juan A.\n  Nolazco-Flores", "title": "A Tutorial on Deep Neural Networks for Intelligent Systems", "comments": "30 pages, 19 figures, unpublished technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing Intelligent Systems involves artificial intelligence approaches\nincluding artificial neural networks. Here, we present a tutorial of Deep\nNeural Networks (DNNs), and some insights about the origin of the term \"deep\";\nreferences to deep learning are also given. Restricted Boltzmann Machines,\nwhich are the core of DNNs, are discussed in detail. An example of a simple\ntwo-layer network, performing unsupervised learning for unlabeled data, is\nshown. Deep Belief Networks (DBNs), which are used to build networks with more\nthan two layers, are also described. Moreover, examples for supervised learning\nwith DNNs performing simple prediction and classification tasks, are presented\nand explained. This tutorial includes two intelligent pattern recognition\napplications: hand- written digits (benchmark known as MNIST) and speech\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:55:20 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Cuevas-Tello", "Juan C.", ""], ["Valenzuela-Rendon", "Manuel", ""], ["Nolazco-Flores", "Juan A.", ""]]}, {"id": "1603.07285", "submitter": "Francesco Visin", "authors": "Vincent Dumoulin, Francesco Visin", "title": "A guide to convolution arithmetic for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 17:52:21 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:54:25 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Visin", "Francesco", ""]]}, {"id": "1603.07341", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, Yurii Vlasov", "title": "Acceleration of Deep Neural Network Training with Resistive Cross-Point\n  Devices", "comments": "19 pages, 5 figures, 2 tables", "journal-ref": "Front. Neurosci 10, 333 (2016)", "doi": "10.3389/fnins.2016.00333", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNN) have demonstrated significant\nbusiness impact in large scale analysis and classification tasks such as speech\nrecognition, visual object detection, pattern extraction, etc. Training of\nlarge DNNs, however, is universally considered as time consuming and\ncomputationally intensive task that demands datacenter-scale computational\nresources recruited for many days. Here we propose a concept of resistive\nprocessing unit (RPU) devices that can potentially accelerate DNN training by\norders of magnitude while using much less power. The proposed RPU device can\nstore and update the weight values locally thus minimizing data movement during\ntraining and allowing to fully exploit the locality and the parallelism of the\ntraining algorithm. We identify the RPU device and system specifications for\nimplementation of an accelerator chip for DNN training in a realistic\nCMOS-compatible technology. For large DNNs with about 1 billion weights this\nmassively parallel RPU architecture can achieve acceleration factors of 30,000X\ncompared to state-of-the-art microprocessors while providing power efficiency\nof 84,000 GigaOps/s/W. Problems that currently require days of training on a\ndatacenter-size cluster with thousands of machines can be addressed within\nhours on a single RPU accelerator. A system consisted of a cluster of RPU\naccelerators will be able to tackle Big Data problems with trillions of\nparameters that is impossible to address today like, for example, natural\nspeech recognition and translation between all world languages, real-time\nanalytics on large streams of business and scientific data, integration and\nanalysis of multimodal sensory data flows from massive number of IoT (Internet\nof Things) sensors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 20:13:11 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Vlasov", "Yurii", ""]]}, {"id": "1603.07454", "submitter": "Chao Ma", "authors": "Chao Ma, Tianchenghou, Bin Lan, Jinhui Xu, Zhenhua Zhang", "title": "Deep Extreme Feature Extraction: New MVA Method for Searching Particles\n  in High Energy Physics", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Deep Extreme Feature Extraction (DEFE), a new\nensemble MVA method for searching $\\tau^{+}\\tau^{-}$ channel of Higgs bosons in\nhigh energy physics. DEFE can be viewed as a deep ensemble learning scheme that\ntrains a strongly diverse set of neural feature learners without explicitly\nencouraging diversity and penalizing correlations. This is achieved by adopting\nan implicit neural controller (not involved in feedforward compuation) that\ndirectly controls and distributes gradient flows from higher level deep\nprediction network. Such model-independent controller results in that every\nsingle local feature learned are used in the feature-to-output mapping stage,\navoiding the blind averaging of features. DEFE makes the ensembles 'deep' in\nthe sense that it allows deep post-process of these features that tries to\nlearn to select and abstract the ensemble of neural feature learners. With the\napplication of this model, a selection regions full of signal process can be\nobtained through the training of a miniature collision events set. In\ncomparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art\nperformance: the error rate has decreased by about 37\\%, the accuracy has\nbroken through 90\\% for the first time, along with the discovery significance\nhas reached a standard deviation of 6.0 $\\sigma$. Experimental data shows that,\nDEFE is able to train an ensemble of discriminative feature learners that\nboosts the overperformance of final prediction.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 07:12:20 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Ma", "Chao", ""], ["Tianchenghou", "", ""], ["Lan", "Bin", ""], ["Xu", "Jinhui", ""], ["Zhang", "Zhenhua", ""]]}, {"id": "1603.07646", "submitter": "Saurabh Kataria", "authors": "Saurabh Kataria", "title": "Recursive Neural Language Architecture for Tag Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 16:39:37 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kataria", "Saurabh", ""]]}, {"id": "1603.07704", "submitter": "Quan Liu", "authors": "Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si\n  Wei, Yu Hu", "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models", "comments": "Probabilistic reasoning, Winograd Schema Challenge, Deep learning,\n  Neural Networks, Distributed Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep learning approach, called neural\nassociation model (NAM), for probabilistic reasoning in artificial\nintelligence. We propose to use neural networks to model association between\nany two events in a domain. Neural networks take one event as input and compute\na conditional probability of the other event to model how likely these two\nevents are to be associated. The actual meaning of the conditional\nprobabilities varies between applications and depends on how the models are\ntrained. In this work, as two case studies, we have investigated two NAM\nstructures, namely deep neural networks (DNN) and relation-modulated neural\nnets (RMNN), on several probabilistic reasoning tasks in AI, including\nrecognizing textual entailment, triple classification in multi-relational\nknowledge bases and commonsense reasoning. Experimental results on several\npopular datasets derived from WordNet, FreeBase and ConceptNet have all\ndemonstrated that both DNNs and RMNNs perform equally well and they can\nsignificantly outperform the conventional methods available for these reasoning\ntasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,\nwhere a pre-trained model can be quickly extended to an unseen relation after\nobserving only a few training samples. To further prove the effectiveness of\nthe proposed models, in this work, we have applied NAMs to solving challenging\nWinograd Schema (WS) problems. Experiments conducted on a set of WS problems\nprove that the proposed models have the potential for commonsense reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 18:54:18 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 14:31:17 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Liu", "Quan", ""], ["Jiang", "Hui", ""], ["Evdokimov", "Andrew", ""], ["Ling", "Zhen-Hua", ""], ["Zhu", "Xiaodan", ""], ["Wei", "Si", ""], ["Hu", "Yu", ""]]}, {"id": "1603.07839", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar", "title": "Early Detection of Combustion Instabilities using Deep Convolutional\n  Selective Autoencoders on Hi-speed Flame Video", "comments": "A 10 pages, 10 figures submission for Applied Data Science Track of\n  KDD16", "journal-ref": null, "doi": "10.1145/1235", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end convolutional selective autoencoder\napproach for early detection of combustion instabilities using rapidly arriving\nflame image frames. The instabilities arising in combustion processes cause\nsignificant deterioration and safety issues in various human-engineered systems\nsuch as land and air based gas turbine engines. These properties are described\nas self-sustaining, large amplitude pressure oscillations and show varying\nspatial scales periodic coherent vortex structure shedding. However, such\ninstability is extremely difficult to detect before a combustion process\nbecomes completely unstable due to its sudden (bifurcation-type) nature. In\nthis context, an autoencoder is trained to selectively mask stable flame and\nallow unstable flame image frames. In that process, the model learns to\nidentify and extract rich descriptive and explanatory flame shape features.\nWith such a training scheme, the selective autoencoder is shown to be able to\ndetect subtle instability features as a combustion process makes transition\nfrom stable to unstable region. As a consequence, the deep learning tool-chain\ncan perform as an early detection framework for combustion instabilities that\nwill have a transformative impact on the safety and performance of modern\nengines.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:02:41 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Lore", "Kin Gwn", ""], ["Sarkar", "Soumalya", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1603.07866", "submitter": "Romain Couillet", "authors": "Romain Couillet, Gilles Wainrib, Harry Sevi, Hafiz Tiomoko Ali", "title": "The Asymptotic Performance of Linear Echo State Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a study of the mean-square error (MSE) performance of linear\necho-state neural networks is performed, both for training and testing tasks.\nConsidering the realistic setting of noise present at the network nodes, we\nderive deterministic equivalents for the aforementioned MSE in the limit where\nthe number of input data $T$ and network size $n$ both grow large. Specializing\nthen the network connectivity matrix to specific random settings, we further\nobtain simple formulas that provide new insights on the performance of such\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 10:27:00 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Couillet", "Romain", ""], ["Wainrib", "Gilles", ""], ["Sevi", "Harry", ""], ["Ali", "Hafiz Tiomoko", ""]]}, {"id": "1603.07893", "submitter": "Hengjian Jia", "authors": "Hengjian Jia", "title": "Investigation Into The Effectiveness Of Long Short Term Memory Networks\n  For Stock Price Prediction", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The effectiveness of long short term memory networks trained by\nbackpropagation through time for stock price prediction is explored in this\npaper. A range of different architecture LSTM networks are constructed trained\nand tested.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 12:28:02 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 11:28:45 GMT"}, {"version": "v3", "created": "Sun, 28 Aug 2016 09:56:23 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Jia", "Hengjian", ""]]}, {"id": "1603.08023", "submitter": "Ryan Lowe T.", "authors": "Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent\n  Charlin, Joelle Pineau", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation", "comments": "First 4 authors had equal contribution. 13 pages, 5 tables, 6\n  figures. EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:32:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 18:28:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Liu", "Chia-Wei", ""], ["Lowe", "Ryan", ""], ["Serban", "Iulian V.", ""], ["Noseworthy", "Michael", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1603.08029", "submitter": "Diogo Almeida", "authors": "Sasha Targ, Diogo Almeida, Kevin Lyman", "title": "Resnet in Resnet: Generalizing Residual Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) have recently achieved state-of-the-art on\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\ndual-stream architecture that generalizes ResNets and standard CNNs and is\neasily implemented with no computational overhead. RiR consistently improves\nperformance over ResNets, outperforms architectures with similar amounts of\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:55:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Targ", "Sasha", ""], ["Almeida", "Diogo", ""], ["Lyman", "Kevin", ""]]}, {"id": "1603.08042", "submitter": "Ouais Alsharif", "authors": "Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw", "title": "On the Compression of Recurrent Neural Networks with an Application to\n  LVCSR acoustic modeling for Embedded Speech Recognition", "comments": "Accepted in ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of compressing recurrent neural networks (RNNs). In\nparticular, we focus on the compression of RNN acoustic models, which are\nmotivated by the goal of building compact and accurate speech recognition\nsystems which can be run efficiently on mobile devices. In this work, we\npresent a technique for general recurrent model compression that jointly\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\nfind that the proposed technique allows us to reduce the size of our Long\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\nnegligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:43:28 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 15:19:30 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Prabhavalkar", "Rohit", ""], ["Alsharif", "Ouais", ""], ["Bruguier", "Antoine", ""], ["McGraw", "Ian", ""]]}, {"id": "1603.08146", "submitter": "Jo\\~ao Ranhel", "authors": "Jo\\~ao Ranhel, Jo\\~ao H. Albuquerque, Bruno P. M. Azevedo, Nathalia M.\n  Cunha, Pedro J. Ishimaru", "title": "A Draft Memory Model on Spiking Neural Assemblies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A draft memory model (DM) for neural networks with spike propagation delay\n(SNNwD) is described. Novelty in this approach are that the DM learns\nimmediately, with stimuli presented once, without synaptic weight changes, and\nwithout external learning algorithm. Basal on this model is to trap spikes\nwithin neural loops. In order to construct the DM we developed two functional\nblocks, also described herein. The decoder block receives input from a single\nspikes source and connect it to one among many outputs. The selector block\noperates in the opposite direction, receiving many spikes sources and\nconnecting one of them to a single output. We realized conceptual proofs by\ntesting the DM in the prime numbers classifying task. This activation-based\nmemory can be used as immediate and short-term memory.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 21:41:04 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ranhel", "Jo\u00e3o", ""], ["Albuquerque", "Jo\u00e3o H.", ""], ["Azevedo", "Bruno P. M.", ""], ["Cunha", "Nathalia M.", ""], ["Ishimaru", "Pedro J.", ""]]}, {"id": "1603.08148", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou and Yoshua\n  Bengio", "title": "Pointing the Unknown Words", "comments": "ACL 2016 Oral Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of rare and unknown words is an important issue that can\npotentially influence the performance of many NLP systems, including both the\ntraditional count-based and the deep learning models. We propose a novel way to\ndeal with the rare and unseen words for the neural network models using\nattention. Our model uses two softmax layers in order to predict the next word\nin conditional language models: one predicts the location of a word in the\nsource sentence, and the other predicts a word in the shortlist vocabulary. At\neach time-step, the decision of which softmax layer to use choose adaptively\nmade by an MLP which is conditioned on the context.~We motivate our work from a\npsychological evidence that humans naturally have a tendency to point towards\nobjects in the context or the environment when the name of an object is not\nknown.~We observe improvements on two tasks, neural machine translation on the\nEuroparl English to French parallel corpora and text summarization on the\nGigaword dataset using our proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:31:57 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 21:12:57 GMT"}, {"version": "v3", "created": "Sun, 21 Aug 2016 20:03:39 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Nallapati", "Ramesh", ""], ["Zhou", "Bowen", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.08150", "submitter": "John J Nay", "authors": "John J. Nay, Jonathan M. Gilligan", "title": "Data-Driven Dynamic Decision Models", "comments": "Published in the Proceedings of the 2015 Winter Simulation Conference", "journal-ref": "Proceedings of the 2015 Winter Simulation Conference, Pages\n  2752-2763, IEEE Press", "doi": "10.1109/WSC.2015.7408381", "report-no": null, "categories": "stat.ML cs.GT cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article outlines a method for automatically generating models of dynamic\ndecision-making that both have strong predictive power and are interpretable in\nhuman terms. This is useful for designing empirically grounded agent-based\nsimulations and for gaining direct insight into observed dynamic processes. We\nuse an efficient model representation and a genetic algorithm-based estimation\nprocess to generate simple approximations that explain most of the structure of\ncomplex stochastic processes. This method, implemented in C++ and R, scales\nwell to large data sets. We apply our methods to empirical data from human\nsubjects game experiments and international relations. We also demonstrate the\nmethod's ability to recover known data-generating processes by simulating data\nwith agent-based models and correctly deriving the underlying decision models\nfor multiple agent models and degrees of stochasticity.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:45:13 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nay", "John J.", ""], ["Gilligan", "Jonathan M.", ""]]}, {"id": "1603.08233", "submitter": "Randal Olson", "authors": "Randal S. Olson, Jason H. Moore, Christoph Adami", "title": "Evolution of active categorical image classification via saccadic eye\n  movement", "comments": "10 pages, 5 figures, to appear in PPSN 2016 conference proceedings", "journal-ref": "Lecture Notes in Computer Science 9921 (2016) 581-590", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition and classification is a central concern for modern\ninformation processing systems. In particular, one key challenge to image and\nvideo classification has been that the computational cost of image processing\nscales linearly with the number of pixels in the image or video. Here we\npresent an intelligent machine (the \"active categorical classifier,\" or ACC)\nthat is inspired by the saccadic movements of the eye, and is capable of\nclassifying images by selectively scanning only a portion of the image. We\nharness evolutionary computation to optimize the ACC on the MNIST hand-written\ndigit classification task, and provide a proof-of-concept that the ACC works on\nnoisy multi-class data. We further analyze the ACC and demonstrate its ability\nto classify images after viewing only a fraction of the pixels, and provide\ninsight on future research paths to further improve upon the ACC presented\nhere.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 16:36:43 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 21:00:53 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""], ["Adami", "Christoph", ""]]}, {"id": "1603.08262", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Towards Machine Intelligence", "comments": "10 pages, submitted to AGI-16. arXiv admin note: substantial text\n  overlap with arXiv:1512.01926", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles of its operation. This theory assumes that the\nbrain has some initial rough architecture, a small library of simple innate\ncircuits which are prewired at birth and proposes that all significant mental\nalgorithms can be learned. Given current understanding and observations, this\npaper reviews and lists the ingredients of such an algorithm from both\narchitectural and functional perspectives.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 22:01:59 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1603.08270", "submitter": "Steven Esser", "authors": "Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy,\n  Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L.\n  McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta,\n  Arnon Amir, Brian Taba, Myron D. Flickner, and Dharmendra S. Modha", "title": "Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing", "comments": "7 pages, 6 figures", "journal-ref": "PNAS 113 (2016) 11441-11446", "doi": "10.1073/pnas.1604850113", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are now able to achieve human-level performance on a broad\nspectrum of recognition tasks. Independently, neuromorphic computing has now\ndemonstrated unprecedented energy-efficiency through a new chip architecture\nbased on spiking neurons, low precision synapses, and a scalable communication\nnetwork. Here, we demonstrate that neuromorphic computing, despite its novel\narchitectural primitives, can implement deep convolution networks that i)\napproach state-of-the-art classification accuracy across 8 standard datasets,\nencompassing vision and speech, ii) perform inference while preserving the\nhardware's underlying energy-efficiency and high throughput, running on the\naforementioned datasets at between 1200 and 2600 frames per second and using\nbetween 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be\nspecified and trained using backpropagation with the same ease-of-use as\ncontemporary deep learning. For the first time, the algorithmic power of deep\nlearning can be merged with the efficiency of neuromorphic processors, bringing\nthe promise of embedded, intelligent, brain-inspired computing one step closer.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 00:15:35 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 18:46:56 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Esser", "Steven K.", ""], ["Merolla", "Paul A.", ""], ["Arthur", "John V.", ""], ["Cassidy", "Andrew S.", ""], ["Appuswamy", "Rathinakumar", ""], ["Andreopoulos", "Alexander", ""], ["Berg", "David J.", ""], ["McKinstry", "Jeffrey L.", ""], ["Melano", "Timothy", ""], ["Barch", "Davis R.", ""], ["di Nolfo", "Carmelo", ""], ["Datta", "Pallab", ""], ["Amir", "Arnon", ""], ["Taba", "Brian", ""], ["Flickner", "Myron D.", ""], ["Modha", "Dharmendra S.", ""]]}, {"id": "1603.08296", "submitter": "Evgeny Nikulchev", "authors": "L. Demidova, E. Nikulchev, Yu. Sokolova", "title": "The SVM Classifier Based on the Modified Particle Swarm Optimization", "comments": "9 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications 7 (2016) 16-24", "doi": "10.14569/IJACSA.2016.070203", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of development of the SVM classifier based on the modified\nparticle swarm optimization has been considered. This algorithm carries out the\nsimultaneous search of the kernel function type, values of the kernel function\nparameters and value of the regularization parameter for the SVM classifier.\nSuch SVM classifier provides the high quality of data classification. The idea\nof particles' {\\guillemotleft}regeneration{\\guillemotright} is put on the basis\nof the modified particle swarm optimization algorithm. At the realization of\nthis idea, some particles change their kernel function type to the one which\ncorresponds to the particle with the best value of the classification accuracy.\nThe offered particle swarm optimization algorithm allows reducing the time\nexpenditures for development of the SVM classifier. The results of experimental\nstudies confirm the efficiency of this algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 20:12:44 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Demidova", "L.", ""], ["Nikulchev", "E.", ""], ["Sokolova", "Yu.", ""]]}, {"id": "1603.08367", "submitter": "Markus Thom", "authors": "Markus Thom and G\\\"unther Palm", "title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "comments": "See http://jmlr.org/papers/v14/thom13a.html for the authoritative\n  version", "journal-ref": "Journal of Machine Learning Research, vol. 14, pp. 1091-1143, 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparseness is a useful regularizer for learning in a wide range of\napplications, in particular in neural networks. This paper proposes a model\ntargeted at classification tasks, where sparse activity and sparse connectivity\nare used to enhance classification capabilities. The tool for achieving this is\na sparseness-enforcing projection operator which finds the closest vector with\na pre-defined sparseness for any given vector. In the theoretical part of this\npaper, a comprehensive theory for such a projection is developed. In\nconclusion, it is shown that the projection is differentiable almost everywhere\nand can thus be implemented as a smooth neuronal transfer function. The entire\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\non the MNIST database of handwritten digits show that classification\nperformance can be boosted by sparse activity or sparse connectivity. With a\ncombination of both, performance can be significantly better compared to\nclassical non-sparse approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 12:06:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thom", "Markus", ""], ["Palm", "G\u00fcnther", ""]]}, {"id": "1603.08474", "submitter": "Oswaldo Ludwig", "authors": "Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens", "title": "Deep Embedding for Spatial Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:38:46 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ludwig", "Oswaldo", ""], ["Liu", "Xiao", ""], ["Kordjamshidi", "Parisa", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1603.08538", "submitter": "{\\L}ukasz Olech Piotr", "authors": "Pawe{\\l} B. Myszkowski and Marek E. Skowro\\'nski and {\\L}ukasz P.\n  Olech and Krzysztof O\\'sliz{\\l}o", "title": "Hybrid Ant Colony Optimization in solving Multi-Skill\n  Resource-Constrained Project Scheduling Problem", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s00500-014-1455-x", "journal-ref": "Soft Computing 19(12), 3599-3619 (2014)", "doi": "10.1007/s00500-014-1455-x", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper Hybrid Ant Colony Optimization (HAntCO) approach in solving\nMulti--Skill Resource Constrained Project Scheduling Problem (MS--RCPSP) has\nbeen presented. We have proposed hybrid approach that links classical heuristic\npriority rules for project scheduling with Ant Colony Optimization (ACO).\nFurthermore, a novel approach for updating pheromone value has been proposed,\nbased on both the best and worst solutions stored by ants. The objective of\nthis paper is to research the usability and robustness of ACO and its hybrids\nwith priority rules in solving MS--RCPSP. Experiments have been performed using\nartificially created dataset instances, based on real--world ones. We published\nthose instances that can be used as a benchmark. Presented results show that\nACO--based hybrid method is an efficient approach. More directed search process\nby hybrids makes this approach more stable and provides mostly better results\nthan classical ACO.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 20:15:53 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 07:19:03 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Myszkowski", "Pawe\u0142 B.", ""], ["Skowro\u0144ski", "Marek E.", ""], ["Olech", "\u0141ukasz P.", ""], ["O\u015bliz\u0142o", "Krzysztof", ""]]}, {"id": "1603.08551", "submitter": "Hugo Martay Dr", "authors": "Hugo Martay", "title": "Genetic cellular neural networks for generating three-dimensional\n  geometry", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a number of ways to procedurally generate interesting\nthree-dimensional shapes, and a method where a cellular neural network is\ncombined with a mesh growth algorithm is presented here. The aim is to create a\nshape from a genetic code in such a way that a crude search can find\ninteresting shapes. Identical neural networks are placed at each vertex of a\nmesh which can communicate with neural networks on neighboring vertices. The\noutput of the neural networks determine how the mesh grows, allowing\ninteresting shapes to be produced emergently, mimicking some of the complexity\nof biological organism development. Since the neural networks' parameters can\nbe freely mutated, the approach is amenable for use in a genetic algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 20:28:09 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Martay", "Hugo", ""]]}, {"id": "1603.08776", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (Inria), Tea Tusar (Inria), Olaf Mersmann, Anne Auger\n  (Inria), Dimo Brockhoff (Inria)", "title": "COCO: The Experimental Procedure", "comments": "ArXiv e-prints, arXiv:1603.08776", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a budget-free experimental setup and procedure for benchmarking\nnumericaloptimization algorithms in a black-box scenario. This procedure can be\napplied with the COCO benchmarking platform. We describe initialization of and\ninput to the algorithm and touch upon therelevance of termination and restarts.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 14:10:14 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 11:58:22 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Hansen", "Nikolaus", "", "Inria"], ["Tusar", "Tea", "", "Inria"], ["Mersmann", "Olaf", "", "Inria"], ["Auger", "Anne", "", "Inria"], ["Brockhoff", "Dimo", "", "Inria"]]}, {"id": "1603.08983", "submitter": "Alex Graves", "authors": "Alex Graves", "title": "Adaptive Computation Time for Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 22:09:00 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 10:27:31 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 18:38:25 GMT"}, {"version": "v4", "created": "Mon, 18 Apr 2016 19:10:22 GMT"}, {"version": "v5", "created": "Thu, 2 Feb 2017 10:09:32 GMT"}, {"version": "v6", "created": "Tue, 21 Feb 2017 16:21:21 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Graves", "Alex", ""]]}, {"id": "1603.09002", "submitter": "Michael Bukatin", "authors": "Michael Bukatin and Steve Matthews and Andrey Radul", "title": "Dataflow Matrix Machines as a Generalization of Recurrent Neural\n  Networks", "comments": "4 pages position paper (v2 - update references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataflow matrix machines are a powerful generalization of recurrent neural\nnetworks. They work with multiple types of arbitrary linear streams, multiple\ntypes of powerful neurons, and allow to incorporate higher-order constructions.\nWe expect them to be useful in machine learning and probabilistic programming,\nand in the synthesis of dynamic systems and of deterministic and probabilistic\nprograms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 23:48:27 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 17:37:54 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Bukatin", "Michael", ""], ["Matthews", "Steve", ""], ["Radul", "Andrey", ""]]}, {"id": "1603.09051", "submitter": "Rahul Aralikatte", "authors": "Rahul Aralikatte and G Srinivasaraghavan", "title": "Phoenix: A Self-Optimizing Chess Engine", "comments": "Accepted in CICN 2015. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of computers, many tasks which required humans to spend a\nlot of time and energy have been trivialized by the computers' ability to\nperform repetitive tasks extremely quickly. Playing chess is one such task. It\nwas one of the first games which was `solved' using AI. With the advent of deep\nlearning, chess playing agents can surpass human ability with relative ease.\nHowever algorithms using deep learning must learn millions of parameters. This\nwork looks at the game of chess through the lens of genetic algorithms. We\ntrain a genetic player from scratch using only a handful of learnable\nparameters. We use Multi-Niche Crowding to optimize positional Value Tables\n(PVTs) which are used extensively in chess engines to evaluate the goodness of\na position. With a very simple setup and after only 1000 generations of\nevolution, the player reaches the level of an International Master.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 06:41:04 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 04:53:19 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 14:32:55 GMT"}, {"version": "v4", "created": "Sun, 20 Aug 2017 11:25:43 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Aralikatte", "Rahul", ""], ["Srinivasaraghavan", "G", ""]]}, {"id": "1603.09381", "submitter": "Peng Li", "authors": "Peng Li and Heng Huang", "title": "Clinical Information Extraction via Convolutional Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1408.5882 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report an implementation of a clinical information extraction tool that\nleverages deep neural network to annotate event spans and their attributes from\nraw clinical notes and pathology reports. Our approach uses context words and\ntheir part-of-speech tags and shape information as features. Then we hire\ntemporal (1D) convolutional neural network to learn hidden feature\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\nspans. The empirical evaluation demonstrates that our approach significantly\noutperforms baselines.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:57:07 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Peng", ""], ["Huang", "Heng", ""]]}, {"id": "1603.09382", "submitter": "Yu Sun", "authors": "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger", "title": "Deep Networks with Stochastic Depth", "comments": "first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:58:07 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 18:42:37 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 23:24:16 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Huang", "Gao", ""], ["Sun", "Yu", ""], ["Liu", "Zhuang", ""], ["Sedra", "Daniel", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1603.09405", "submitter": "Peng Li", "authors": "Peng Li and Heng Huang", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based approaches for sentence relation modeling automatically\ngenerate hidden matching features from raw sentence pairs. However, the quality\nof matching feature representation may not be satisfied due to complex semantic\nrelations such as entailment or contradiction. To address this challenge, we\npropose a new deep neural network architecture that jointly leverage\npre-trained word embedding and auxiliary character embedding to learn sentence\nmeanings. The two kinds of word sequence representations as inputs into\nmulti-layer bidirectional LSTM to learn enhanced sentence representation. After\nthat, we construct matching features followed by another temporal CNN to learn\nhigh-level hidden matching feature representations. Experimental results\ndemonstrate that our approach consistently outperforms the existing methods on\nstandard evaluation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 22:39:59 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Peng", ""], ["Huang", "Heng", ""]]}, {"id": "1603.09420", "submitter": "Jianxin Wu", "authors": "Guo-Bing Zhou and Jianxin Wu and Chen-Lin Zhang and Zhi-Hua Zhou", "title": "Minimal Gated Unit for Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently recurrent neural networks (RNN) has been very successful in handling\nsequence data. However, understanding RNN and finding the best practices for\nRNN is a difficult task, partly because there are many competing and complex\nhidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as\nMinimal Gated Unit (MGU), since it only contains one gate, which is a minimal\ndesign among all gated hidden units. The design of MGU benefits from evaluation\nresults on LSTM and GRU in the literature. Experiments on various sequence data\nshow that MGU has comparable accuracy with GRU, but has a simpler structure,\nfewer parameters, and faster training. Hence, MGU is suitable in RNN's\napplications. Its simple architecture also means that it is easier to evaluate\nand tune, and in principle it is easier to study MGU's properties theoretically\nand empirically.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 00:01:10 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Zhou", "Guo-Bing", ""], ["Wu", "Jianxin", ""], ["Zhang", "Chen-Lin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1603.09460", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Dong Wang, Xiaodong Zhang, Thomas Fang Zheng, Panshi Jin", "title": "System Combination for Short Utterance Speaker Recognition", "comments": "APSIPA ASC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text-independent short-utterance speaker recognition (SUSR), the\nperformance often degrades dramatically. This paper presents a combination\napproach to the SUSR tasks with two phonetic-aware systems: one is the\nDNN-based i-vector system and the other is our recently proposed\nsubregion-based GMM-UBM system. The former employs phone posteriors to\nconstruct an i-vector model in which the shared statistics offers stronger\nrobustness against limited test data, while the latter establishes a\nphone-dependent GMM-UBM system which represents speaker characteristics with\nmore details. A score-level fusion is implemented to integrate the respective\nadvantages from the two systems. Experimental results show that for the\ntext-independent SUSR task, both the DNN-based i-vector system and the\nsubregion-based GMM-UBM system outperform their respective baselines, and the\nscore-level system combination delivers performance improvement.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 05:47:03 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 13:49:05 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Zhang", "Xiaodong", ""], ["Zheng", "Thomas Fang", ""], ["Jin", "Panshi", ""]]}, {"id": "1603.09509", "submitter": "Zhenyao Zhu", "authors": "Zhenyao Zhu, Jesse H. Engel, Awni Hannun", "title": "Learning Multiscale Features Directly From Waveforms", "comments": "\"fix typo in the title\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the task at\nhand. However, true end-to-end learning, where features are learned directly\nfrom waveforms, has only recently reached the performance of hand-tailored\nrepresentations based on the Fourier transform. In this paper, we detail an\napproach to use convolutional filters to push past the inherent tradeoff of\ntemporal and frequency resolution that exists for spectral representations. At\nincreased computational cost, we show that increasing temporal resolution via\nreduced stride and increasing frequency resolution via additional filters\ndelivers significant performance improvements. Further, we find more efficient\nrepresentations by simultaneously learning at multiple scales, leading to an\noverall decrease in word error rate on a difficult internal speech test set by\n20.7% relative to networks with the same number of parameters trained on\nspectrograms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 09:54:44 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 14:17:09 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Zhu", "Zhenyao", ""], ["Engel", "Jesse H.", ""], ["Hannun", "Awni", ""]]}, {"id": "1603.09643", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Lantian Li and Dong Wang", "title": "Multi-task Recurrent Model for Speech and Speaker Recognition", "comments": "APSIPA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although highly correlated, speech and speaker recognition have been regarded\nas two independent tasks and studied by two communities. This is certainly not\nthe way that people behave: we decipher both speech content and speaker traits\nat the same time. This paper presents a unified model to perform speech and\nspeaker recognition simultaneously and altogether. The model is based on a\nunified neural network where the output of one task is fed to the input of the\nother, leading to a multi-task recurrent network. Experiments show that the\njoint model outperforms the task-specific models on both the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:37:29 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 05:54:30 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 06:25:01 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 12:27:17 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Li", "Lantian", ""], ["Wang", "Dong", ""]]}]