[{"id": "1402.0420", "submitter": "Luca Vassio Mr", "authors": "Francesco Bertini, Lorenzo Dal Mas, Luca Vassio and Enrico Ampellio", "title": "Multidiscipinary Optimization For Gas Turbines Design", "comments": "12 pages, 6 figures. Presented at the XXII Italian Association of\n  Aeronautics and Astronautics Conference (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art aeronautic Low Pressure gas Turbines (LPTs) are already\ncharacterized by high quality standards, thus they offer very narrow margins of\nimprovement. Typical design process starts with a Concept Design (CD) phase,\ndefined using mean-line 1D and other low-order tools, and evolves through a\nPreliminary Design (PD) phase, which allows the geometric definition in\ndetails. In this framework, multidisciplinary optimization is the only way to\nproperly handle the complicated peculiarities of the design. The authors\npresent different strategies and algorithms that have been implemented\nexploiting the PD phase as a real-like design benchmark to illustrate results.\nThe purpose of this work is to describe the optimization techniques, their\nsettings and how to implement them effectively in a multidisciplinary\nenvironment. Starting from a basic gradient method and a semi-random second\norder method, the authors have introduced an Artificial Bee Colony-like\noptimizer, a multi-objective Genetic Diversity Evolutionary Algorithm [1] and a\nmulti-objective response surface approach based on Artificial Neural Network,\nparallelizing and customizing them for the gas turbine study. Moreover, speedup\nand improvement arrangements are embedded in different hybrid strategies with\nthe aim at finding the best solutions for different kind of problems that arise\nin this field.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 16:38:51 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Bertini", "Francesco", ""], ["Mas", "Lorenzo Dal", ""], ["Vassio", "Luca", ""], ["Ampellio", "Enrico", ""]]}, {"id": "1402.0708", "submitter": "Sadik Ulker", "authors": "Ezgi Deniz Ulker and Sadik Ulker", "title": "Microstrip Coupler Design Using Bat Algorithm", "comments": "7 pages, 4 figures, 1 table", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), vol. 5, no. 1, January 2014, pp. 127-133", "doi": "10.5121/ijaia.2014.5110", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary and swarm algorithms have found many applications in design\nproblems since todays computing power enables these algorithms to find\nsolutions to complicated design problems very fast. Newly proposed hybrid\nalgorithm, bat algorithm, has been applied for the design of microwave\nmicrostrip couplers for the first time. Simulation results indicate that the\nbat algorithm is a very fast algorithm and it produces very reliable results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 12:25:31 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Ulker", "Ezgi Deniz", ""], ["Ulker", "Sadik", ""]]}, {"id": "1402.0710", "submitter": "Andrea Soltoggio", "authors": "Andrea Soltoggio", "title": "Short-term plasticity as cause-effect hypothesis testing in distal\n  reward learning", "comments": "Biological Cybernetics, September 2014", "journal-ref": null, "doi": "10.1007/s00422-014-0628-0", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchrony, overlaps and delays in sensory-motor signals introduce ambiguity\nas to which stimuli, actions, and rewards are causally related. Only the\nrepetition of reward episodes helps distinguish true cause-effect relationships\nfrom coincidental occurrences. In the model proposed here, a novel plasticity\nrule employs short and long-term changes to evaluate hypotheses on cause-effect\nrelationships. Transient weights represent hypotheses that are consolidated in\nlong-term memory only when they consistently predict or cause future rewards.\nThe main objective of the model is to preserve existing network topologies when\nlearning with ambiguous information flows. Learning is also improved by biasing\nthe exploration of the stimulus-response space towards actions that in the past\noccurred before rewards. The model indicates under which conditions beliefs can\nbe consolidated in long-term memory, it suggests a solution to the\nplasticity-stability dilemma, and proposes an interpretation of the role of\nshort-term plasticity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 12:37:20 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 14:22:29 GMT"}, {"version": "v3", "created": "Fri, 11 Jul 2014 13:06:49 GMT"}, {"version": "v4", "created": "Mon, 11 Aug 2014 13:37:45 GMT"}, {"version": "v5", "created": "Tue, 9 Sep 2014 15:42:23 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Soltoggio", "Andrea", ""]]}, {"id": "1402.0808", "submitter": "Hooman Jarollahi", "authors": "Hooman Jarollahi, Naoya Onizawa, Takahiro Hanyu, Warren J. Gross", "title": "Associative Memories Based on Multiple-Valued Sparse Clustered Networks", "comments": "6 pages, Accepted in IEEE ISMVL 2014 conference", "journal-ref": null, "doi": "10.1109/ISMVL.2014.44", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associative memories are structures that store data patterns and retrieve\nthem given partial inputs. Sparse Clustered Networks (SCNs) are\nrecently-introduced binary-weighted associative memories that significantly\nimprove the storage and retrieval capabilities over the prior state-of-the art.\nHowever, deleting or updating the data patterns result in a significant\nincrease in the data retrieval error probability. In this paper, we propose an\nalgorithm to address this problem by incorporating multiple-valued weights for\nthe interconnections used in the network. The proposed algorithm lowers the\nerror rate by an order of magnitude for our sample network with 60% deleted\ncontents. We then investigate the advantages of the proposed algorithm for\nhardware implementations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 16:23:33 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Jarollahi", "Hooman", ""], ["Onizawa", "Naoya", ""], ["Hanyu", "Takahiro", ""], ["Gross", "Warren J.", ""]]}, {"id": "1402.0836", "submitter": "Sakyasingha Dasgupta", "authors": "Sakyasingha Dasgupta", "title": "Cognitive Aging as Interplay between Hebbian Learning and Criticality", "comments": "Concise version of MSc thesis, Neural Models of the Ageing Brain,\n  University of Edinburgh, 2010. Supervisor Dr. J. Michael Herrmann. 64 pages,\n  20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive ageing seems to be a story of global degradation. As one ages there\nare a number of physical, chemical and biological changes that take place.\nTherefore it is logical to assume that the brain is no exception to this\nphenomenon. The principle purpose of this project is to use models of neural\ndynamics and learning based on the underlying principle of self-organised\ncriticality, to account for the age related cognitive effects. In this regard\nlearning in neural networks can serve as a model for the acquisition of skills\nand knowledge in early development stages i.e. the ageing process and\ncriticality in the network serves as the optimum state of cognitive abilities.\nPossible candidate mechanisms for ageing in a neural network are loss of\nconnectivity and neurons, increase in the level of noise, reduction in white\nmatter or more interestingly longer learning history and the competition among\nseveral optimization objectives. In this paper we are primarily interested in\nthe affect of the longer learning history on memory and thus the optimality in\nthe brain. Hence it is hypothesized that prolonged learning in the form of\nassociative memory patterns can destroy the state of criticality in the\nnetwork. We base our model on Tsodyks and Markrams [49] model of dynamic\nsynapses, in the process to explore the effect of combining standard Hebbian\nlearning with the phenomenon of Self-organised criticality. The project mainly\nconsists of evaluations and simulations of networks of integrate and\nfire-neurons that have been subjected to various combinations of neural-level\nageing effects, with the aim of establishing the primary hypothesis and\nunderstanding the decline of cognitive abilities due to ageing, using one of\nits important characteristics, a longer learning history.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 19:16:34 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Dasgupta", "Sakyasingha", ""]]}, {"id": "1402.1128", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Fran\\c{c}oise Beaufays", "title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for\n  Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)\narchitecture that has been designed to address the vanishing and exploding\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\nRNNs have cyclic connections making them powerful for modeling sequences. They\nhave been successfully used for sequence labeling and sequence prediction\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\nacoustic frames. However, in contrast to the deep neural networks, the use of\nRNNs in speech recognition has been limited to phone recognition in small scale\ntasks. In this paper, we present novel LSTM based RNN architectures which make\nmore effective use of model parameters to train acoustic models for large\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\nvarious numbers of parameters and configurations. We show that LSTM models\nconverge quickly and give state of the art speech recognition performance for\nrelatively small sized models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:01:51 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1402.1141", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Quantum Cybernetics and Complex Quantum Systems Science - A Quantum\n  Connectionist Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum cybernetics and its connections to complex quantum systems science is\naddressed from the perspective of complex quantum computing systems. In this\nway, the notion of an autonomous quantum computing system is introduced in\nregards to quantum artificial intelligence, and applied to quantum artificial\nneural networks, considered as autonomous quantum computing systems, which\nleads to a quantum connectionist framework within quantum cybernetics for\ncomplex quantum computing systems. Several examples of quantum feedforward\nneural networks are addressed in regards to Boolean functions' computation,\nmultilayer quantum computation dynamics, entanglement and quantum\ncomplementarity. The examples provide a framework for a reflection on the role\nof quantum artificial neural networks as a general framework for addressing\ncomplex quantum systems that perform network-based quantum computation,\npossible consequences are drawn regarding quantum technologies, as well as\nfundamental research in complex quantum systems science and quantum biology.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:48:24 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1402.1379", "submitter": "Jin-Kao Hao", "authors": "Zhang-Hua Fu and Jin-Kao Hao", "title": "A Three-Phase Search Approach for the Quadratic Minimum Spanning Tree\n  Problem", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph with costs associated with each edge as well as\neach pair of edges, the quadratic minimum spanning tree problem (QMSTP)\nconsists of determining a spanning tree of minimum total cost. This problem can\nbe used to model many real-life network design applications, in which both\nrouting and interference costs should be considered. For this problem, we\npropose a three-phase search approach named TPS, which integrates 1) a\ndescent-based neighborhood search phase using two different move operators to\nreach a local optimum from a given starting solution, 2) a local optima\nexploring phase to discover nearby local optima within a given regional search\narea, and 3) a perturbation-based diversification phase to jump out of the\ncurrent regional search area. Additionally, we introduce dedicated techniques\nto reduce the neighborhood to explore and streamline the neighborhood\nevaluations. Computational experiments based on hundreds of representative\nbenchmarks show that TPS produces highly competitive results with respect to\nthe best performing approaches in the literature by improving the best known\nresults for 31 instances and matching the best known results for the remaining\ninstances only except two cases. Critical elements of the proposed algorithms\nare analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 15:31:49 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Fu", "Zhang-Hua", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1402.1869", "submitter": "KyungHyun Cho", "authors": "Guido Mont\\'ufar, Razvan Pascanu, Kyunghyun Cho and Yoshua Bengio", "title": "On the Number of Linear Regions of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of functions computable by deep feedforward neural\nnetworks with piecewise linear activations in terms of the symmetries and the\nnumber of linear regions that they have. Deep networks are able to sequentially\nmap portions of each layer's input-space to the same output. In this way, deep\nmodels compute functions that react equally to complicated patterns of\ndifferent inputs. The compositional structure of these functions enables them\nto re-use pieces of computation exponentially often in terms of the network's\ndepth. This paper investigates the complexity of such compositional maps and\ncontributes new theoretical results regarding the advantage of depth for neural\nnetworks with piecewise linear activation functions. In particular, our\nanalysis is not specific to a single family of models, and as an example, we\nemploy it for rectifier and maxout networks. We improve complexity bounds from\npre-existing work and investigate the behavior of units in higher layers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 17:16:27 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 19:56:14 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Mont\u00fafar", "Guido", ""], ["Pascanu", "Razvan", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1402.1931", "submitter": "Ahmed Rashid", "authors": "Rashid Ahmed, John A. Avaritsiotis", "title": "MCA Learning Algorithm for Incident Signals Estimation: A Review", "comments": "5 pages,8 figures, 1 table. International Journal of Computer Trends\n  and Technology (IJCTT),Feb 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been many works on adaptive subspace filtering in the\nsignal processing literature. Most of them are concerned with tracking the\nsignal subspace spanned by the eigenvectors corresponding to the eigenvalues of\nthe covariance matrix of the signal plus noise data. Minor Component Analysis\n(MCA) is important tool and has a wide application in telecommunications,\nantenna array processing, statistical parametric estimation, etc. As an\nimportant feature extraction technique, MCA is a statistical method of\nextracting the eigenvector associated with the smallest eigenvalue of the\ncovariance matrix. In this paper, we will present a MCA learning algorithm to\nextract minor component from input signals, and the learning rate parameter is\nalso presented, which ensures fast convergence of the algorithm, because it has\ndirect effect on the convergence of the weight vector and the error level is\naffected by this value. MCA is performed to determine the estimated DOA.\nSimulation results will be furnished to illustrate the theoretical results\nachieved.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 09:55:13 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Ahmed", "Rashid", ""], ["Avaritsiotis", "John A.", ""]]}, {"id": "1402.2031", "submitter": "Hong Chang", "authors": "Wen Wang, Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen", "title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The comparison of heterogeneous samples extensively exists in many\napplications, especially in the task of image classification. In this paper, we\npropose a simple but effective coupled neural network, called Deeply Coupled\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\ncoupled with each other in every corresponding layers. In DCAN, each deep\nstructure is developed via stacking multiple discriminative coupled\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\nconsisting of intra-class compactness and inter-class penalty. This single\nlayer component makes our model simultaneously preserve the local consistency\nand enhance its discriminative capability. With increasing number of layers,\nthe coupled networks can gradually narrow the gap between the two views.\nExtensive experiments on cross-view image classification tasks demonstrate the\nsuperiority of our method over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 04:15:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Wang", "Wen", ""], ["Cui", "Zhen", ""], ["Chang", "Hong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1402.2704", "submitter": "Chris Watkins", "authors": "Chris Watkins, Yvonne Buttkewitz", "title": "Sex as Gibbs Sampling: a probability model of evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that evolutionary computation can be implemented as standard\nMarkov-chain Monte-Carlo (MCMC) sampling. With some care, `genetic algorithms'\ncan be constructed that are reversible Markov chains that satisfy detailed\nbalance; it follows that the stationary distribution of populations is a Gibbs\ndistribution in a simple factorised form. For some standard and popular\nnonparametric probability models, we exhibit Gibbs-sampling procedures that are\nplausible genetic algorithms. At mutation-selection equilibrium, a population\nof genomes is analogous to a sample from a Bayesian posterior, and the genomes\nare analogous to latent variables. We suggest this is a general, tractable, and\ninsightful formulation of evolutionary computation in terms of standard machine\nlearning concepts and techniques.\n  In addition, we show that evolutionary processes in which selection acts by\ndifferences in fecundity are not reversible, and also that it is not possible\nto construct reversible evolutionary models in which each child is produced by\nonly two parents.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 00:29:39 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Watkins", "Chris", ""], ["Buttkewitz", "Yvonne", ""]]}, {"id": "1402.2959", "submitter": "Sebastien Verel", "authors": "Gabriela Ochoa, S\\'ebastien Verel (LISIC), Fabio Daolio (ISI), Marco\n  Tomassini (ISI)", "title": "Local Optima Networks: A New Model of Combinatorial Fitness Landscapes", "comments": null, "journal-ref": "Recent Advances in the Theory and Application of Fitness\n  Landscapes, Hendrik Richter, Andries Engelbrecht (Ed.) (2014) 233-262", "doi": "10.1007/978-3-642-41888-4_9", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter overviews a recently introduced network-based model of\ncombinatorial landscapes: Local Optima Networks (LON). The model compresses the\ninformation given by the whole search space into a smaller mathematical object\nthat is a graph having as vertices the local optima and as edges the possible\nweighted transitions between them. Two definitions of edges have been proposed:\nbasin-transition and escape-edges, which capture relevant topological features\nof the underlying search spaces. This network model brings a new set of metrics\nto characterize the structure of combinatorial landscapes, those associated\nwith the science of complex networks. These metrics are described, and results\nare presented of local optima network extraction and analysis for two selected\ncombinatorial landscapes: NK landscapes and the quadratic assignment problem.\nNetwork features are found to correlate with and even predict the performance\nof heuristic search algorithms operating on these problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 20:21:54 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Ochoa", "Gabriela", "", "LISIC"], ["Verel", "S\u00e9bastien", "", "LISIC"], ["Daolio", "Fabio", "", "ISI"], ["Tomassini", "Marco", "", "ISI"]]}, {"id": "1402.3337", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic, David Krueger", "title": "Zero-bias autoencoders and the benefits of co-adapting features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized training of an autoencoder typically results in hidden unit\nbiases that take on large negative values. We show that negative biases are a\nnatural result of using a hidden layer whose responsibility is to both\nrepresent the input data and act as a selection mechanism that ensures sparsity\nof the representation. We then show that negative biases impede the learning of\ndata distributions whose intrinsic dimensionality is high. We also propose a\nnew activation function that decouples the two roles of the hidden layer and\nthat allows us to learn representations on data with very high intrinsic\ndimensionality, where standard autoencoders typically fail. Since the decoupled\nactivation function acts like an implicit regularizer, the model can be trained\nby minimizing the reconstruction error of training data, without requiring any\nadditional regularization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 23:37:39 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 21:39:48 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 02:07:47 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 01:15:33 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2015 14:51:11 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""], ["Krueger", "David", ""]]}, {"id": "1402.3346", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Nihat Ay, Keyan Ghazi-Zahedi", "title": "Geometry and Expressive Power of Conditional Restricted Boltzmann\n  Machines", "comments": "30 pages, 5 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional restricted Boltzmann machines are undirected stochastic neural\nnetworks with a layer of input and output units connected bipartitely to a\nlayer of hidden units. These networks define models of conditional probability\ndistributions on the states of the output units given the states of the input\nunits, parametrized by interaction weights and biases. We address the\nrepresentational power of these models, proving results their ability to\nrepresent conditional Markov random fields and conditional distributions with\nrestricted supports, the minimal size of universal approximators, the maximal\nmodel approximation errors, and on the dimension of the set of representable\nconditional distributions. We contribute new tools for investigating\nconditional probability models, which allow us to improve the results that can\nbe derived from existing work on restricted Boltzmann machine probability\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 02:15:09 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 17:00:35 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 15:20:04 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Montufar", "Guido", ""], ["Ay", "Nihat", ""], ["Ghazi-Zahedi", "Keyan", ""]]}, {"id": "1402.3511", "submitter": "Jan Koutn\\'ik", "authors": "Jan Koutn\\'ik, Klaus Greff, Faustino Gomez, J\\\"urgen Schmidhuber", "title": "A Clockwork RNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence prediction and classification are ubiquitous and challenging\nproblems in machine learning that can require identifying complex dependencies\nbetween temporally distant inputs. Recurrent Neural Networks (RNNs) have the\nability, in theory, to cope with these temporal dependencies by virtue of the\nshort-term memory implemented by their recurrent (feedback) connections.\nHowever, in practice they are difficult to train successfully when the\nlong-term memory is required. This paper introduces a simple, yet powerful\nmodification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in\nwhich the hidden layer is partitioned into separate modules, each processing\ninputs at its own temporal granularity, making computations only at its\nprescribed clock rate. Rather than making the standard RNN models more complex,\nCW-RNN reduces the number of RNN parameters, improves the performance\nsignificantly in the tasks tested, and speeds up the network evaluation. The\nnetwork is demonstrated in preliminary experiments involving two tasks: audio\nsignal generation and TIMIT spoken word classification, where it outperforms\nboth RNN and LSTM networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 16:05:12 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Koutn\u00edk", "Jan", ""], ["Greff", "Klaus", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1402.3811", "submitter": "Zhi-Hua Zhou", "authors": "Wei Gao and Zhi-Hua Zhou", "title": "Dropout Rademacher Complexity of Deep Neural Networks", "comments": "20 pagea", "journal-ref": "Science China Information Sciences, 2016, 59(7):\n  072104:1-072104:12", "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great successes of deep neural networks have been witnessed in various real\napplications. Many algorithmic and implementation techniques have been\ndeveloped, however, theoretical understanding of many aspects of deep neural\nnetworks is far from clear. A particular interesting issue is the usefulness of\ndropout, which was motivated from the intuition of preventing complex\nco-adaptation of feature detectors. In this paper, we study the Rademacher\ncomplexity of different types of dropout, and our theoretical results disclose\nthat for shallow neural networks (with one or none hidden layer) dropout is\nable to reduce the Rademacher complexity in polynomial, whereas for deep neural\nnetworks it can amazingly lead to an exponential reduction of the Rademacher\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 15:54:37 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 14:36:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gao", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1402.4007", "submitter": "Ella Gale", "authors": "Ella Gale, Ben de Lacy Costello and Andrew Adamatzky", "title": "Does the D.C. Response of Memristors Allow Robotic Short-Term Memory and\n  a Possible Route to Artificial Time Perception?", "comments": "3 page position paper", "journal-ref": "Workshop on Unconventional Approaches to Robotics, Automation and\n  Control (UARACIN), at International Conference on Robotics and Automation\n  (ICRA) 2013, Karlsruhe, Germany, Fr-Ws-09, pgs. 22-24", "doi": null, "report-no": null, "categories": "cs.RO cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time perception is essential for task switching, and in the mammalian brain\nappears alongside other processes. Memristors are electronic components used as\nsynapses and as models for neurons. The d.c. response of memristors can be\nconsidered as a type of short-term memory. Interactions of the memristor d.c.\nresponse within networks of memristors leads to the emergence of oscillatory\ndynamics and intermittent spike trains, which are similar to neural dynamics.\nBased on this data, the structure of a memristor network control for a robot as\nit undergoes task switching is discussed and it is suggested that these\nemergent network dynamics could improve the performance of role switching and\nlearning in an artificial intelligence and perhaps create artificial time\nperception.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 14:08:57 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Gale", "Ella", ""], ["Costello", "Ben de Lacy", ""], ["Adamatzky", "Andrew", ""]]}, {"id": "1402.4029", "submitter": "Ella Gale", "authors": "Deborah Gater, Attya Iqbal, Jeffrey Davey and Ella Gale", "title": "Connecting Spiking Neurons to a Spiking Memristor Network Changes the\n  Memristor Dynamics", "comments": "Conference paper, 4 pages", "journal-ref": "Proceedings of the International Conference on Electronics,\n  Circuits and Systems (ICECS) 2013, Abu Dhabi, UAE, December 8th-11th,\n  534--537", "doi": null, "report-no": null, "categories": "cs.ET cs.NE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristors have been suggested as neuromorphic computing elements. Spike-time\ndependent plasticity and the Hodgkin-Huxley model of the neuron have both been\nmodelled effectively by memristor theory. The d.c. response of the memristor is\na current spike. Based on these three facts we suggest that memristors are\nwell-placed to interface directly with neurons. In this paper we show that\nconnecting a spiking memristor network to spiking neuronal cells causes a\nchange in the memristor network dynamics by: removing the memristor spikes,\nwhich we show is due to the effects of connection to aqueous medium; causing a\nchange in current decay rate consistent with a change in memristor state;\npresenting more-linear $I-t$ dynamics; and increasing the memristor spiking\nrate, as a consequence of interaction with the spiking neurons. This\ndemonstrates that neurons are capable of communicating directly with\nmemristors, without the need for computer translation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 15:23:18 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Gater", "Deborah", ""], ["Iqbal", "Attya", ""], ["Davey", "Jeffrey", ""], ["Gale", "Ella", ""]]}, {"id": "1402.4036", "submitter": "Ella Gale", "authors": "Ella Gale, Ben de Lacy Costello and Andrew Adamatzky", "title": "Is Spiking Logic the Route to Memristor-Based Computers?", "comments": "Conference paper. Work also reported in US patent: `Logic device and\n  method of performing a logical operation', patent application no. 14/089,191\n  (November 25, 2013)", "journal-ref": "Proceedings of the International Conference on Electronics,\n  Circuits and Systems (ICECS) 2013, Abu Dhabi, UAE, December 8th-11th,\n  297--300", "doi": null, "report-no": null, "categories": "cs.ET cond-mat.mtrl-sci cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristors have been suggested as a novel route to neuromorphic computing\nbased on the similarity between neurons (synapses and ion pumps) and\nmemristors. The D.C. action of the memristor is a current spike, which we think\nwill be fruitful for building memristor computers. In this paper, we introduce\n4 different logical assignations to implement sequential logic in the memristor\nand introduce the physical rules, summation, `bounce-back', directionality and\n`diminishing returns', elucidated from our investigations. We then demonstrate\nhow memristor sequential logic works by instantiating a NOT gate, an AND gate\nand a Full Adder with a single memristor. The Full Adder makes use of the\nmemristor's memory to add three binary values together and outputs the value,\nthe carry digit and even the order they were input in.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 15:40:49 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Gale", "Ella", ""], ["Costello", "Ben de Lacy", ""], ["Adamatzky", "Andrew", ""]]}, {"id": "1402.4442", "submitter": "Donia El Kateb", "authors": "Donia El Kateb (SnT), Fran\\c{c}ois Fouquet (SnT), Johann Bourcier\n  (INRIA - IRISA), Yves Le Traon (SnT, CSC)", "title": "Artificial Mutation inspired Hyper-heuristic for Runtime Usage of\n  Multi-objective Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, multi-objective evolutionary algorithms (MOEA) have been\napplied to different software engineering problems where many conflicting\nobjectives have to be optimized simultaneously. In theory, evolutionary\nalgorithms feature a nice property for runtime optimization as they can provide\na solution in any execution time. In practice, based on a Darwinian inspired\nnatural selection, these evolutionary algorithms produce many deadborn\nsolutions whose computation results in a computational resources wastage:\nnatural selection is naturally slow. In this paper, we reconsider this founding\nanalogy to accelerate convergence of MOEA, by looking at modern biology\nstudies: artificial selection has been used to achieve an anticipated specific\npurpose instead of only relying on crossover and natural selection (i.e.,\nMuller et al [18] research on artificial mutation of fruits with X-Ray).\nPutting aside the analogy with natural selection , the present paper proposes\nan hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial\nselective mutation to improve the convergence speed of MOEA. Sputnik leverages\nthe past history of mutation efficiency to select the most relevant mutations\nto perform. We evaluate Sputnik on a cloud-reasoning engine, which drives\non-demand provisioning while considering conflicting performance and cost\nobjectives. We have conducted experiments to highlight the significant\nperformance improvement of Sputnik in terms of resolution time.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 19:09:23 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Kateb", "Donia El", "", "SnT"], ["Fouquet", "Fran\u00e7ois", "", "SnT"], ["Bourcier", "Johann", "", "INRIA - IRISA"], ["Traon", "Yves Le", "", "SnT, CSC"]]}, {"id": "1402.4699", "submitter": "Shujia Liu", "authors": "Shujia Liu", "title": "A Powerful Genetic Algorithm for Traveling Salesman Problem", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a powerful genetic algorithm(GA) to solve the traveling\nsalesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)\nwith a local search procedure to determine good combinations of building blocks\nof parent solutions for generating even better offspring solutions.\nExperimental results on well studied TSP benchmarks demonstrate that the\nproposed GA is competitive in finding very high quality solutions on instances\nwith up to 16,862 cities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 15:33:35 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Liu", "Shujia", ""]]}, {"id": "1402.5428", "submitter": "Khalid Jebari hassani", "authors": "Khalid jebari, Mohammed Madiafi and Abdelaziz Elmoujahid", "title": "An Evolutionary approach for solving Shr\\\"odinger Equation", "comments": "arXiv admin note: substantial text overlap with arXiv:1401.0523", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The purpose of this paper is to present a method of solving the Shr\\\"odinger\nEquation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms\ngenerations of trial solutions expressed in an analytical form. We illustrate\nthe effectiveness of this method providing, for example, the results of its\napplication to a quantum system minimal energy, and we compare these results\nwith those produced by traditional analytical methods\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 21:18:58 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["jebari", "Khalid", ""], ["Madiafi", "Mohammed", ""], ["Elmoujahid", "Abdelaziz", ""]]}, {"id": "1402.5708", "submitter": "Lavdim Kurtaj", "authors": "Lavdim Kurtaj, Ilir Limani, Vjosa Shatri and Avni Skeja", "title": "The Cerebellum: New Computational Model that Reveals its Primary\n  Function to Calculate Multibody Dynamics Conform to Lagrange-Euler\n  Formulation", "comments": "18 pages, 4 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 5, No 2, September 2013", "doi": null, "report-no": null, "categories": "cs.NE cs.CE cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cerebellum is part of the brain that occupies only 10% of the brain volume,\nbut it contains about 80% of total number of brain neurons. New cerebellar\nfunction model is developed that sets cerebellar circuits in context of\nmultibody dynamics model computations, as important step in controlling balance\nand movement coordination, functions performed by two oldest parts of the\ncerebellum. Model gives new functional interpretation for granule cells-Golgi\ncell circuit, including distinct function for upper and lower Golgi cell\ndendritc trees, and resolves issue of sharing Granule cells between Purkinje\ncells. Sets new function for basket cells, and for stellate cells according to\nposition in molecular layer. New model enables easily and direct integration of\nsensory information from vestibular system and cutaneous mechanoreceptors, for\nbalance, movement and interaction with environments. Model gives explanation of\nPurkinje cells convergence on deep-cerebellar nuclei.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 02:40:05 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Kurtaj", "Lavdim", ""], ["Limani", "Ilir", ""], ["Shatri", "Vjosa", ""], ["Skeja", "Avni", ""]]}, {"id": "1402.5830", "submitter": "Luca Vassio Mr", "authors": "Enrico Ampellio and Luca Vassio", "title": "A hybrid swarm-based algorithm for single-objective optimization\n  problems involving high-cost analyses", "comments": "19 pages, 4 figures, Springer Swarm Intelligence", "journal-ref": "Swarm Intelligence 10, 99-121 (2016)", "doi": "10.1007/s11721-016-0121-6", "report-no": null, "categories": "math.OC cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many technical fields, single-objective optimization procedures in\ncontinuous domains involve expensive numerical simulations. In this context, an\nimprovement of the Artificial Bee Colony (ABC) algorithm, called the Artificial\nsuper-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide\nfast convergence speed, high solution accuracy and robust performance over a\nwide range of problems. It implements enhancements of the ABC structure and\nhybridizations with interpolation strategies. The latter are inspired by the\nquadratic trust region approach for local investigation and by an efficient\nglobal optimizer for separable problems. Each modification and their combined\neffects are studied with appropriate metrics on a numerical benchmark, which is\nalso used for comparing AsBeC with some effective ABC variants and other\nderivative-free algorithms. In addition, the presented algorithm is validated\non two recent benchmarks adopted for competitions in international conferences.\nResults show remarkable competitiveness and robustness for AsBeC.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 14:06:52 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 14:55:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ampellio", "Enrico", ""], ["Vassio", "Luca", ""]]}, {"id": "1402.6366", "submitter": "Mustafa Abdul Salam", "authors": "Osman Hegazy, Omar S. Soliman and Mustafa Abdul Salam", "title": "LSSVM-ABC Algorithm for Stock Price prediction", "comments": "12 pages. International Journal of Computer Trends and Technology\n  (IJCTT)2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Artificial Bee Colony (ABC) algorithm which inspired from the\nbehavior of honey bees swarm is presented. ABC is a stochastic population-based\nevolutionary algorithm for problem solving. ABC algorithm, which is considered\none of the most recently swarm intelligent techniques, is proposed to optimize\nleast square support vector machine (LSSVM) to predict the daily stock prices.\nThe proposed model is based on the study of stocks historical data, technical\nindicators and optimizing LSSVM with ABC algorithm. ABC selects best free\nparameters combination for LSSVM to avoid over-fitting and local minima\nproblems and improve prediction accuracy. LSSVM optimized by Particle swarm\noptimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparison\nwith proposed model. Proposed model tested with twenty datasets representing\ndifferent sectors in S&P 500 stock market. Results presented in this paper show\nthat the proposed model has fast convergence speed, and it also achieves better\naccuracy than compared techniques in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 23:02:08 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Hegazy", "Osman", ""], ["Soliman", "Omar S.", ""], ["Salam", "Mustafa Abdul", ""]]}, {"id": "1402.6428", "submitter": "Vishakha Metre VAM", "authors": "Jayshree Ghorpade-Aher and Vishakha A. Metre", "title": "Clustering Multidimensional Data with PSO based Algorithm", "comments": "6 pages,6 figures,3 tables, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a recognized data analysis method in data mining whereas\nK-Means is the well known partitional clustering method, possessing pleasant\nfeatures. We observed that, K-Means and other partitional clustering techniques\nsuffer from several limitations such as initial cluster centre selection,\npreknowledge of number of clusters, dead unit problem, multiple cluster\nmembership and premature convergence to local optima. Several optimization\nmethods are proposed in the literature in order to solve clustering\nlimitations, but Swarm Intelligence (SI) has achieved its remarkable position\nin the concerned area. Particle Swarm Optimization (PSO) is the most popular SI\ntechnique and one of the favorite areas of researchers. In this paper, we\npresent a brief overview of PSO and applicability of its variants to solve\nclustering challenges. Also, we propose an advanced PSO algorithm named as\nSubtractive Clustering based Boundary Restricted Adaptive Particle Swarm\nOptimization (SC-BR-APSO) algorithm for clustering multidimensional data. For\ncomparison purpose, we have studied and analyzed various algorithms such as\nK-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposed\nalgorithm on nine different datasets. The motivation behind proposing\nSC-BR-APSO algorithm is to deal with multidimensional data clustering, with\nminimum error rate and maximum convergence rate.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 06:08:27 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Ghorpade-Aher", "Jayshree", ""], ["Metre", "Vishakha A.", ""]]}, {"id": "1402.6556", "submitter": "Csaba P\\u{a}tca\\c{s}", "authors": "Csaba Patcas and Attila Bartha", "title": "Evolutionary solving of the debts' clearing problem", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The debts' clearing problem is about clearing all the debts in a group of n\nentities (persons, companies etc.) using a minimal number of money transaction\noperations. The problem is known to be NP-hard in the strong sense. As for many\nintractable problems, techniques from the field of artificial intelligence are\nuseful in finding solutions close to optimum for large inputs. An evolutionary\nalgorithm for solving the debts' clearing problem is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 14:39:57 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Patcas", "Csaba", ""], ["Bartha", "Attila", ""]]}, {"id": "1402.6785", "submitter": "EPTCS", "authors": "Gal Katz (Bar Ilan University), Doron Peled (Bar Ilan University)", "title": "Synthesis of Parametric Programs using Genetic Programming and Model\n  Checking", "comments": "In Proceedings INFINITY 2013, arXiv:1402.6610", "journal-ref": "EPTCS 140, 2014, pp. 70-84", "doi": "10.4204/EPTCS.140.5", "report-no": null, "categories": "cs.SE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal methods apply algorithms based on mathematical principles to enhance\nthe reliability of systems. It would only be natural to try to progress from\nverification, model checking or testing a system against its formal\nspecification into constructing it automatically. Classical algorithmic\nsynthesis theory provides interesting algorithms but also alarming high\ncomplexity and undecidability results. The use of genetic programming, in\ncombination with model checking and testing, provides a powerful heuristic to\nsynthesize programs. The method is not completely automatic, as it is fine\ntuned by a user that sets up the specification and parameters. It also does not\nguarantee to always succeed and converge towards a solution that satisfies all\nthe required properties. However, we applied it successfully on quite\nnontrivial examples and managed to find solutions to hard programming\nchallenges, as well as to improve and to correct code. We describe here several\nversions of our method for synthesizing sequential and concurrent systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 03:45:20 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Katz", "Gal", "", "Bar Ilan University"], ["Peled", "Doron", "", "Bar Ilan University"]]}, {"id": "1402.6888", "submitter": "Adam Erskine", "authors": "Adam Erskine, J Michael Herrmann", "title": "CriPS: Critical Dynamics in Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Swarm Optimisation (PSO) makes use of a dynamical system for solving\na search task. Instead of adding search biases in order to improve performance\nin certain problems, we aim to remove algorithm-induced scales by controlling\nthe swarm with a mechanism that is scale-free except possibly for a suppression\nof scales beyond the system size. In this way a very promising performance is\nachieved due to the balance of large-scale exploration and local search. The\nresulting algorithm shows evidence for self-organised criticality, brought\nabout via the intrinsic dynamics of the swarm as it interacts with the\nobjective function, rather than being explicitly specified. The Critical\nParticle Swarm (CriPS) can be easily combined with many existing extensions\nsuch as chaotic exploration, additional force terms or non-trivial topologies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 12:35:27 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Erskine", "Adam", ""], ["Herrmann", "J Michael", ""]]}, {"id": "1402.6942", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa and Zbigniew J. Czech", "title": "A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with\n  Time Windows", "comments": "15 pages", "journal-ref": "Studia Informatica 33 (1), pp 91-106 (2012)", "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel memetic algorithm for solving the vehicle\nrouting problem with time windows (VRPTW). The VRPTW is a well-known NP-hard\ndiscrete optimization problem with two objectives. The main objective is to\nminimize the number of vehicles serving customers scattered on the map, and the\nsecond one is to minimize the total distance traveled by the vehicles. Here,\nthe fleet size is minimized in the first phase of the proposed method using the\nparallel heuristic algorithm (PHA), and the traveled distance is minimized in\nthe second phase by the parallel memetic algorithm (PMA). In both parallel\nalgorithms, the parallel components co-operate periodically in order to\nexchange the best solutions found so far. An extensive experimental study\nperformed on the Gehring and Homberger's benchmark proves the high convergence\ncapabilities and robustness of both PHA and PMA. Also, we present the speedup\nanalysis of the PMA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 15:41:05 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Nalepa", "Jakub", ""], ["Czech", "Zbigniew J.", ""]]}, {"id": "1402.7136", "submitter": "Peter Benes Ing.", "authors": "Peter Mark Benes, Ivo Bukovsky, Matous Cejnek and Jan Kalivoda", "title": "Neural Network Approach to Railway Stand Lateral Skew Control", "comments": "P. M. Benes et al., \"Neural Network Approach to Railway Stand Lateral\n  Skew Control\" in Computer Science & Information Technology (CS& IT), Sydney,\n  NSW, Australia, AIRCC, 2014, pp. 327-339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a study of an adaptive approach to lateral skew control\nfor an experimental railway stand. The preliminary experiments with the real\nexperimental railway stand and simulations with its 3-D mechanical model,\nindicates difficulties of model-based control of the device. Thus, use of\nneural networks for identification and control of lateral skew shall be\ninvestigated. This paper focuses on real-data based modeling of the railway\nstand by various neural network models, i.e; linear neural unit and quadratic\nneural unit architectures. Furthermore, training methods of these neural\narchitectures as such, real-time-recurrent-learning and a variation of\nback-propagation-through-time are examined, accompanied by a discussion of the\nproduced experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 05:34:36 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Benes", "Peter Mark", ""], ["Bukovsky", "Ivo", ""], ["Cejnek", "Matous", ""], ["Kalivoda", "Jan", ""]]}, {"id": "1402.7351", "submitter": "Mustafa Abdul Salam", "authors": "Osman Hegazy, Omar S. Soliman and Mustafa Abdul Salam", "title": "A Machine Learning Model for Stock Market Prediction", "comments": "7 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1402.6366", "journal-ref": "International Journal of Computer Science and Telecommunications\n  [Volume 4, Issue 12, December 2013]", "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock market prediction is the act of trying to determine the future value of\na company stock or other financial instrument traded on a financial exchange.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 19:12:50 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Hegazy", "Osman", ""], ["Soliman", "Omar S.", ""], ["Salam", "Mustafa Abdul", ""]]}]