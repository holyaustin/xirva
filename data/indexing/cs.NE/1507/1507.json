[{"id": "1507.00088", "submitter": "Guillaume Corriveau", "authors": "Guillaume Corriveau, Raynald Guilbault, Antoine Tahan, Robert Sabourin", "title": "Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous genotypic diversity measures (GDMs) are available in the literature\nto assess the convergence status of an evolutionary algorithm (EA) or describe\nits search behavior. In a recent study, the authors of this paper drew\nattention to the need for a GDM validation framework. In response, this study\nproposes three requirements (monotonicity in individual varieties, twinning,\nand monotonicity in distance) that can clearly portray any GDMs. These\ndiversity requirements are analysed by means of controlled population\narrangements. In this paper four GDMs are evaluated with the proposed\nvalidation framework. The results confirm that properly evaluating population\ndiversity is a rather difficult task, as none of the analysed GDMs complies\nwith all the diversity requirements.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 01:54:43 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Corriveau", "Guillaume", ""], ["Guilbault", "Raynald", ""], ["Tahan", "Antoine", ""], ["Sabourin", "Robert", ""]]}, {"id": "1507.00210", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray\n  Kavukcuoglu", "title": "Natural Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 12:42:01 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Simonyan", "Karen", ""], ["Pascanu", "Razvan", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1507.00235", "submitter": "Stefano Fusi", "authors": "Daniel Mart\\'i, Mattia Rigotti, Mingoo Seok, Stefano Fusi", "title": "Energy-efficient neuromorphic classifiers", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic engineering combines the architectural and computational\nprinciples of systems neuroscience with semiconductor electronics, with the aim\nof building efficient and compact devices that mimic the synaptic and neural\nmachinery of the brain. Neuromorphic engineering promises extremely low energy\nconsumptions, comparable to those of the nervous system. However, until now the\nneuromorphic approach has been restricted to relatively simple circuits and\nspecialized functions, rendering elusive a direct comparison of their energy\nconsumption to that used by conventional von Neumann digital machines solving\nreal-world tasks. Here we show that a recent technology developed by IBM can be\nleveraged to realize neuromorphic circuits that operate as classifiers of\ncomplex real-world stimuli. These circuits emulate enough neurons to compete\nwith state-of-the-art classifiers. We also show that the energy consumption of\nthe IBM chip is typically 2 or more orders of magnitude lower than that of\nconventional digital machines when implementing classifiers with comparable\nperformance. Moreover, the spike-based dynamics display a trade-off between\nintegration time and accuracy, which naturally translates into algorithms that\ncan be flexibly deployed for either fast and approximate classifications, or\nmore accurate classifications at the mere expense of longer running times and\nhigher energy costs. This work finally proves that the neuromorphic approach\ncan be efficiently used in real-world applications and it has significant\nadvantages over conventional digital devices when energy consumption is\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 13:52:07 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Mart\u00ed", "Daniel", ""], ["Rigotti", "Mattia", ""], ["Seok", "Mingoo", ""], ["Fusi", "Stefano", ""]]}, {"id": "1507.01053", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Aaron Courville, Yoshua Bengio", "title": "Describing Multimedia Content using Attention-based Encoder--Decoder\n  Networks", "comments": "Submitted to IEEE Transactions on Multimedia Special Issue on Deep\n  Learning for Multimedia Computing", "journal-ref": null, "doi": "10.1109/TMM.2015.2477044", "report-no": null, "categories": "cs.NE cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas deep neural networks were first mostly used for classification tasks,\nthey are rapidly expanding in the realm of structured output problems, where\nthe observed target is composed of multiple random variables that have a rich\njoint distribution, given the input. We focus in this paper on the case where\nthe input also has a rich structure and the input and output structures are\nsomehow related. We describe systems that learn to attend to different places\nin the input, for each element of the output, for a variety of tasks: machine\ntranslation, image caption generation, video clip description and speech\nrecognition. All these systems are based on a shared set of building blocks:\ngated recurrent neural networks and convolutional neural networks, along with\ntrained attention mechanisms. We report on experimental results with these\nsystems, showing impressively good performance and the advantage of the\nattention mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 01:06:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.01122", "submitter": "Gabriel Makdah", "authors": "Gabriel Makdah", "title": "Modeling the Mind: A brief review", "comments": "Part 1 in the Modeling the Mind review series. 52 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is a powerful tool used to achieve amazing feats. There have been\nseveral significant advances in neuroscience and artificial brain research in\nthe past two decades. This article is a review of such advances, ranging from\nthe concepts of connectionism, to neural network architectures and\nhigh-dimensional representations. There have also been advances in biologically\ninspired cognitive architectures of which we will cite a few. We will be\npositioning relatively specific models in a much broader perspective, while\ncomparing and contrasting their advantages and weaknesses. The projects\npresented are targeted to model the brain at different levels, utilizing\ndifferent methodologies.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 15:54:50 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 04:46:21 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 05:52:00 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Makdah", "Gabriel", ""]]}, {"id": "1507.01239", "submitter": "Hang Su", "authors": "Hang Su, Haoyu Chen", "title": "Experiments on Parallel Training of Deep Neural Network using Model\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we apply model averaging to parallel training of deep neural\nnetwork (DNN). Parallelization is done in a model averaging manner. Data is\npartitioned and distributed to different nodes for local model updates, and\nmodel averaging across nodes is done every few minibatches. We use multiple\nGPUs for data parallelization, and Message Passing Interface (MPI) for\ncommunication between nodes, which allows us to perform model averaging\nfrequently without losing much time on communication. We investigate the\neffectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and\nRestricted Boltzmann Machine (RBM) pretraining for parallel training in\nmodel-averaging framework, and explore the best setups in term of different\nlearning rate schedules, averaging frequencies and minibatch sizes. It is shown\nthat NG-SGD and RBM pretraining benefits parameter-averaging based model\ntraining. On the 300h Switchboard dataset, a 9.3 times speedup is achieved\nusing 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy\nloss.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 16:29:33 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 22:04:07 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 00:21:59 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Su", "Hang", ""], ["Chen", "Haoyu", ""]]}, {"id": "1507.01422", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Junting Pan and Xavier Gir\\'o-i-Nieto", "title": "End-to-end Convolutional Network for Saliency Prediction", "comments": "Winner of the saliency prediction challenge in the Large-scale Scene\n  Understanding (LSUN) Challenge in the associated workshop of the IEEE\n  Conference on Computer Vision and Pattern Recognition (CVPR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of saliency areas in images has been traditionally addressed\nwith hand crafted features based on neuroscience principles. This paper however\naddresses the problem with a completely data-driven approach by training a\nconvolutional network. The learning process is formulated as a minimization of\na loss function that measures the Euclidean distance of the predicted saliency\nmap with the provided ground truth. The recent publication of large datasets of\nsaliency prediction has provided enough data to train a not very deep\narchitecture which is both fast and accurate. The convolutional network in this\npaper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction\nwith a superior performance in all considered metrics.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 12:43:26 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Pan", "Junting", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1507.01526", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Ivo Danihelka, Alex Graves", "title": "Grid Long Short-Term Memory", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 16:30:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 17:40:17 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:39:48 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Danihelka", "Ivo", ""], ["Graves", "Alex", ""]]}, {"id": "1507.01687", "submitter": "Vipul Dabhi", "authors": "Vipul K. Dabhi and Sanjay Chaudhary", "title": "Developing Postfix-GP Framework for Symbolic Regression Problems", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": "10.1109/ACCT.2015.114", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Postfix-GP system, postfix notation based Genetic\nProgramming (GP), for solving symbolic regression problems. It presents an\nobject-oriented architecture of Postfix-GP framework. It assists the user in\nunderstanding of the implementation details of various components of\nPostfix-GP. Postfix-GP provides graphical user interface which allows user to\nconfigure the experiment, to visualize evolved solutions, to analyze GP run,\nand to perform out-of-sample predictions. The use of Postfix-GP is demonstrated\nby solving the benchmark symbolic regression problem. Finally, features of\nPostfix-GP framework are compared with that of other GP systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 06:54:47 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Dabhi", "Vipul K.", ""], ["Chaudhary", "Sanjay", ""]]}, {"id": "1507.01889", "submitter": "Gabriel Lellouch", "authors": "Gabriel Lellouch and Amit Kumar Mishra and Michael Inggs", "title": "Design of OFDM radar pulses using genetic algorithm based techniques", "comments": "IN PRESS with TAES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The merit of evolutionary algorithms (EA) to solve convex optimization\nproblems is widely acknowledged. In this paper, a genetic algorithm (GA)\noptimization based waveform design framework is used to improve the features of\nradar pulses relying on the orthogonal frequency division multiplexing (OFDM)\nstructure. Our optimization techniques focus on finding optimal phase code\nsequences for the OFDM signal. Several optimality criteria are used since we\nconsider two different radar processing solutions which call either for single\nor multiple-objective optimizations. When minimization of the so-called\npeak-to-mean envelope power ratio (PMEPR) single-objective is tackled, we\ncompare our findings with existing methods and emphasize on the merit of our\napproach. In the scope of the two-objective optimization, we first address\nPMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach based\non the non-dominated sorting genetic algorithm-II (NSGA-II) provides design\nsolutions with noticeable improvements as opposed to random sets of phase\ncodes. We then look at another case of interest where the objective functions\nare two measures of the sidelobe level, namely PSLR and the integrated-sidelobe\nlevel ratio (ISLR) and propose to modify the NSGA-II to include a constrain on\nthe PMEPR instead. In the last part, we illustrate via a case study how our\nencoding solution makes it possible to minimize the single objective PMEPR\nwhile enabling a target detection enhancement strategy, when the SNR metric\nwould be chosen for the detection framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 13:51:21 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Lellouch", "Gabriel", ""], ["Mishra", "Amit Kumar", ""], ["Inggs", "Michael", ""]]}, {"id": "1507.02221", "submitter": "Alessandro Sordoni", "authors": "Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma,\n  Jakob G. Simonsen and Jian-Yun Nie", "title": "A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware\n  Query Suggestion", "comments": "To appear in Conference of Information Knowledge and Management\n  (CIKM) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users may strive to formulate an adequate textual query for their information\nneed. Search engines assist the users by presenting query suggestions. To\npreserve the original search intent, suggestions should be context-aware and\naccount for the previous queries issued by the user. Achieving context\nawareness is challenging due to data sparsity. We present a probabilistic\nsuggestion model that is able to account for sequences of previous queries of\narbitrary lengths. Our novel hierarchical recurrent encoder-decoder\narchitecture allows the model to be sensitive to the order of queries in the\ncontext while avoiding data sparsity. Additionally, our model can suggest for\nrare, or long-tail, queries. The produced suggestions are synthetic and are\nsampled one word at a time, using computationally cheap decoding techniques.\nThis is in contrast to current synthetic suggestion models relying upon machine\nlearning pipelines and hand-engineered feature sets. Results show that it\noutperforms existing context-aware approaches in a next query prediction\nsetting. In addition to query suggestion, our model is general enough to be\nused in a variety of other applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 17:06:50 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Sordoni", "Alessandro", ""], ["Bengio", "Yoshua", ""], ["Vahabi", "Hossein", ""], ["Lioma", "Christina", ""], ["Simonsen", "Jakob G.", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "1507.02491", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Victor O.K. Li", "title": "Parameter Sensitivity Analysis of Social Spider Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Spider Algorithm (SSA) is a recently proposed general-purpose\nreal-parameter metaheuristic designed to solve global numerical optimization\nproblems. This work systematically benchmarks SSA on a suite of 11 functions\nwith different control parameters. We conduct parameter sensitivity analysis of\nSSA using advanced non-parametric statistical tests to generate statistically\nsignificant conclusion on the best performing parameter settings. The\nconclusion can be adopted in future work to reduce the effort in parameter\ntuning. In addition, we perform a success rate test to reveal the impact of the\ncontrol parameters on the convergence speed of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 13:10:38 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Yu", "James J. Q.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1507.02492", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Albert Y.S. Lam, Victor O.K. Li", "title": "Adaptive Chemical Reaction Optimization for Global Numerical\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction\nOptimization (CRO), has been applied to many optimization problems in both\ndiscrete and continuous domains. To alleviate the effort in tuning parameters,\nthis paper reduces the number of optimization parameters in canonical CRO and\ndevelops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)\nadapts better to different optimization problems. We perform simulations with\nACRO on a widely-used benchmark of continuous problems. The simulation results\nshow that ACRO has superior performance over canonical CRO.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 13:11:13 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Yu", "James J. Q.", ""], ["Lam", "Albert Y. S.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1507.02672", "submitter": "Mathias Berglund", "authors": "Antti Rasmus and Harri Valpola and Mikko Honkala and Mathias Berglund\n  and Tapani Raiko", "title": "Semi-Supervised Learning with Ladder Networks", "comments": "Revised denoising function, updated results, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine supervised learning with unsupervised learning in deep neural\nnetworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network\nproposed by Valpola (2015), which we extend by combining the model with\nsupervision. We show that the resulting model reaches state-of-the-art\nperformance in semi-supervised MNIST and CIFAR-10 classification, in addition\nto permutation-invariant MNIST classification with all labels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 19:52:19 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 09:22:23 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Rasmus", "Antti", ""], ["Valpola", "Harri", ""], ["Honkala", "Mikko", ""], ["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""]]}, {"id": "1507.02835", "submitter": "Chetan Singh Thakur", "authors": "Chetan Singh Thakur, Runchun Wang, Tara Julia Hamilton, Jonathan\n  Tapson and Andre van Schaik", "title": "A Trainable Neuromorphic Integrated Circuit that Exploits Device\n  Mismatch", "comments": "Submitted to TCAS-I", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Random device mismatch that arises as a result of scaling of the CMOS\n(complementary metal-oxide semi-conductor) technology into the deep submicron\nregime degrades the accuracy of analogue circuits. Methods to combat this\nincrease the complexity of design. We have developed a novel neuromorphic\nsystem called a Trainable Analogue Block (TAB), which exploits device mismatch\nas a means for random projections of the input to a higher dimensional space.\nThe TAB framework is inspired by the principles of neural population coding\noperating in the biological nervous system. Three neuronal layers, namely\ninput, hidden, and output, constitute the TAB framework, with the number of\nhidden layer neurons far exceeding the input layer neurons. Here, we present\nmeasurement results of the first prototype TAB chip built using a 65nm process\ntechnology and show its learning capability for various regression tasks. Our\nTAB chip exploits inherent randomness and variability arising due to the\nfabrication process to perform various learning tasks. Additionally, we\ncharacterise each neuron and discuss the statistical variability of its tuning\ncurve that arises due to random device mismatch, a desirable property for the\nlearning capability of the TAB. We also discuss the effect of the number of\nhidden neurons and the resolution of output weights on the accuracy of the\nlearning capability of the TAB.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 10:29:01 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Thakur", "Chetan Singh", ""], ["Wang", "Runchun", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""]]}, {"id": "1507.03641", "submitter": "Greg Durrett", "authors": "Greg Durrett and Dan Klein", "title": "Neural CRF Parsing", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a parsing model that combines the exact dynamic\nprogramming of CRF parsing with the rich nonlinear featurization of neural net\napproaches. Our model is structurally a CRF that factors over anchored rule\nproductions, but instead of linear potential functions based on sparse\nfeatures, we use nonlinear potentials computed via a feedforward neural\nnetwork. Because potentials are still local to anchored rules, structured\ninference (CKY) is unchanged from the sparse case. Computing gradients during\nlearning involves backpropagating an error signal formed from standard CRF\nsufficient statistics (expected rule counts). Using only dense features, our\nneural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In\ncombination with sparse features, our system achieves 91.1 F1 on section 23 of\nthe Penn Treebank, and more generally outperforms the best prior single parser\nresults on a range of languages.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 22:23:51 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Durrett", "Greg", ""], ["Klein", "Dan", ""]]}, {"id": "1507.03955", "submitter": "Behtash Babadi", "authors": "Abbas Kazemipour, Min Wu, and Behtash Babadi", "title": "Robust Estimation of Self-Exciting Generalized Linear Models with\n  Application to Neuronal Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT cs.SY math.IT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating self-exciting generalized linear models\nfrom limited binary observations, where the history of the process serves as\nthe covariate. We analyze the performance of two classes of estimators, namely\nthe $\\ell_1$-regularized maximum likelihood and greedy estimators, for a\ncanonical self-exciting process and characterize the sampling tradeoffs\nrequired for stable recovery in the non-asymptotic regime. Our results extend\nthose of compressed sensing for linear and generalized linear models with\ni.i.d. covariates to those with highly inter-dependent covariates. We further\nprovide simulation studies as well as application to real spiking data from the\nmouse's lateral geniculate nucleus and the ferret's retinal ganglion cells\nwhich agree with our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 18:07:31 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 21:12:12 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 23:13:49 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Kazemipour", "Abbas", ""], ["Wu", "Min", ""], ["Babadi", "Behtash", ""]]}, {"id": "1507.04116", "submitter": "Angelo Mariano", "authors": "Angelo Mariano, Giorgio Parisi, Saverio Pascazio", "title": "Language discrimination and clustering via a neural network approach", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.CL cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify twenty-one Indo-European languages starting from written text. We\nuse neural networks in order to define a distance among different languages,\nconstruct a dendrogram and analyze the ultrametric structure that emerges. Four\nor five subgroups of languages are identified, according to the \"cut\" of the\ndendrogram, drawn with an entropic criterion. The results and the method are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:19:14 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Mariano", "Angelo", ""], ["Parisi", "Giorgio", ""], ["Pascazio", "Saverio", ""]]}, {"id": "1507.04296", "submitter": "Arun Nair", "authors": "Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory\n  Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman,\n  Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray\n  Kavukcuoglu, David Silver", "title": "Massively Parallel Methods for Deep Reinforcement Learning", "comments": "Presented at the Deep Learning Workshop, International Conference on\n  Machine Learning, Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first massively distributed architecture for deep\nreinforcement learning. This architecture uses four main components: parallel\nactors that generate new behaviour; parallel learners that are trained from\nstored experience; a distributed neural network to represent the value function\nor behaviour policy; and a distributed store of experience. We used our\narchitecture to implement the Deep Q-Network algorithm (DQN). Our distributed\nalgorithm was applied to 49 games from Atari 2600 games from the Arcade\nLearning Environment, using identical hyperparameters. Our performance\nsurpassed non-distributed DQN in 41 of the 49 games and also reduced the\nwall-time required to achieve these results by an order of magnitude on most\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 16:56:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 09:27:06 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Nair", "Arun", ""], ["Srinivasan", "Praveen", ""], ["Blackwell", "Sam", ""], ["Alcicek", "Cagdas", ""], ["Fearon", "Rory", ""], ["De Maria", "Alessandro", ""], ["Panneershelvam", "Vedavyas", ""], ["Suleyman", "Mustafa", ""], ["Beattie", "Charles", ""], ["Petersen", "Stig", ""], ["Legg", "Shane", ""], ["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""], ["Silver", "David", ""]]}, {"id": "1507.04646", "submitter": "Yang Liu", "authors": "Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, Houfeng Wang", "title": "A Dependency-Based Neural Network for Relation Classification", "comments": "This preprint is the full version of a short paper accepted in the\n  annual meeting of the Association for Computational Linguistics (ACL) 2015\n  (Beijing, China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research on relation classification has verified the effectiveness\nof using dependency shortest paths or subtrees. In this paper, we further\nexplore how to make full use of the combination of these dependency\ninformation. We first propose a new structure, termed augmented dependency path\n(ADP), which is composed of the shortest dependency path between two entities\nand the subtrees attached to the shortest path. To exploit the semantic\nrepresentation behind the ADP structure, we develop dependency-based neural\nnetworks (DepNN): a recursive neural network designed to model the subtrees,\nand a convolutional neural network to capture the most important features on\nthe shortest path. Experiments on the SemEval-2010 dataset show that our\nproposed method achieves state-of-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:43:55 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Liu", "Yang", ""], ["Wei", "Furu", ""], ["Li", "Sujian", ""], ["Ji", "Heng", ""], ["Zhou", "Ming", ""], ["Wang", "Houfeng", ""]]}, {"id": "1507.04727", "submitter": "Behtash Babadi", "authors": "Alireza Sheikhattar, Jonathan B. Fritz, Shihab A. Shamma, and Behtash\n  Babadi", "title": "Recursive Sparse Point Process Regression with Application to\n  Spectrotemporal Receptive Field Plasticity Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2512560", "report-no": null, "categories": "cs.NE cs.SY math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the sparse time-varying parameter\nvectors of a point process model in an online fashion, where the observations\nand inputs respectively consist of binary and continuous time series. We\nconstruct a novel objective function by incorporating a forgetting factor\nmechanism into the point process log-likelihood to enforce adaptivity and\nemploy $\\ell_1$-regularization to capture the sparsity. We provide a rigorous\nanalysis of the maximizers of the objective function, which extends the\nguarantees of compressed sensing to our setting. We construct two recursive\nfilters for online estimation of the parameter vectors based on proximal\noptimization techniques, as well as a novel filter for recursive computation of\nstatistical confidence regions. Simulation studies reveal that our algorithms\noutperform several existing point process filters in terms of trackability,\ngoodness-of-fit and mean square error. We finally apply our filtering\nalgorithms to experimentally recorded spiking data from the ferret primary\nauditory cortex during attentive behavior in a click rate discrimination task.\nOur analysis provides new insights into the time-course of the spectrotemporal\nreceptive field plasticity of the auditory neurons.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:43:54 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Sheikhattar", "Alireza", ""], ["Fritz", "Jonathan B.", ""], ["Shamma", "Shihab A.", ""], ["Babadi", "Behtash", ""]]}, {"id": "1507.04761", "submitter": "Bob Sturm", "authors": "Corey Kereliuk and Bob L. Sturm and Jan Larsen", "title": "Deep Learning and Music Adversaries", "comments": "13 pages, 6 figures, 3 tables, 6 sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversary is essentially an algorithm intent on making a classification\nsystem perform in some particular way given an input, e.g., increase the\nprobability of a false negative. Recent work builds adversaries for deep\nlearning systems applied to image object recognition, which exploits the\nparameters of the system to find the minimal perturbation of the input image\nsuch that the network misclassifies it with high confidence. We adapt this\napproach to construct and deploy an adversary of deep learning systems applied\nto music content analysis. In our case, however, the input to the systems is\nmagnitude spectral frames, which requires special care in order to produce\nvalid input audio signals from network-derived perturbations. For two different\ntrain-test partitionings of two benchmark datasets, and two different deep\narchitectures, we find that this adversary is very effective in defeating the\nresulting systems. We find the convolutional networks are more robust, however,\ncompared with systems based on a majority vote over individually classified\naudio frames. Furthermore, we integrate the adversary into the training of new\ndeep systems, but do not find that this improves their resilience against the\nsame adversary.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 20:24:18 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Kereliuk", "Corey", ""], ["Sturm", "Bob L.", ""], ["Larsen", "Jan", ""]]}, {"id": "1507.04808", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville\n  and Joelle Pineau", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models", "comments": "8 pages with references; Published in AAAI 2016 (Special Track on\n  Cognitive Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 00:21:39 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 19:49:39 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 23:20:41 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sordoni", "Alessandro", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Pineau", "Joelle", ""]]}, {"id": "1507.05053", "submitter": "Keiron O'Shea Mr", "authors": "Keiron O'Shea", "title": "Massively Deep Artificial Neural Networks for Handwritten Digit\n  Recognition", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on\nthe famous MNIST database of handwritten digits. All that was required to\nachieve this result was a high number of hidden layers consisting of many\nneurons, and a graphics card to greatly speed up the rate of learning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 17:48:49 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["O'Shea", "Keiron", ""]]}, {"id": "1507.05695", "submitter": "Mark Wang", "authors": "Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan\n  Tapson, Andre van Schaik", "title": "A neuromorphic hardware architecture using the Neural Engineering\n  Framework for pattern recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hardware architecture that uses the Neural Engineering Framework\n(NEF) to implement large-scale neural networks on Field Programmable Gate\nArrays (FPGAs) for performing pattern recognition in real time. NEF is a\nframework that is capable of synthesising large-scale cognitive systems from\nsubnetworks. We will first present the architecture of the proposed neural\nnetwork implemented using fixed-point numbers and demonstrate a routine that\ncomputes the decoding weights by using the online pseudoinverse update method\n(OPIUM) in a parallel and distributed manner. The proposed system is\nefficiently implemented on a compact digital neural core. This neural core\nconsists of 64 neurons that are instantiated by a single physical neuron using\na time-multiplexing approach. As a proof of concept, we combined 128 identical\nneural cores together to build a handwritten digit recognition system using the\nMNIST database and achieved a recognition rate of 96.55%. The system is\nimplemented on a state-of-the-art FPGA and can process 5.12 million digits per\nsecond. The architecture is not limited to handwriting recognition, but is\ngenerally applicable as an extremely fast pattern recognition processor for\nvarious kinds of patterns such as speech and images.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 03:48:04 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Wang", "Runchun", ""], ["Thakur", "Chetan Singh", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""]]}, {"id": "1507.05775", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Jia-Nan Wu", "title": "Compression of Fully-Connected Layer in Neural Network by Kronecker\n  Product", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a technique to reduce the number of\nparameters and computation time in fully-connected layers of neural networks\nusing Kronecker product, at a mild cost of the prediction quality. The\ntechnique proceeds by replacing Fully-Connected layers with so-called Kronecker\nFully-Connected layers, where the weight matrices of the FC layers are\napproximated by linear combinations of multiple Kronecker products of smaller\nmatrices. In particular, given a model trained on SVHN dataset, we are able to\nconstruct a new KFC model with 73\\% reduction in total number of parameters,\nwhile the error only rises mildly. In contrast, using low-rank method can only\nachieve 35\\% reduction in total number of parameters given similar quality\ndegradation allowance. If we only compare the KFC layer with its counterpart\nfully-connected layer, the reduction in the number of parameters exceeds 99\\%.\nThe amount of computation is also reduced as we replace matrix product of the\nlarge matrices in FC layers with matrix products of a few smaller matrices in\nKFC layers. Further experiments on MNIST, SVHN and some Chinese Character\nrecognition models also demonstrate effectiveness of our technique.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:29:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 11:59:08 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wu", "Jia-Nan", ""]]}, {"id": "1507.06222", "submitter": "Ryad Benjamin Benosman", "authors": "Xavier Lagorce, Ryad Benosman", "title": "STICK: Spike Time Interval Computational Kernel, A Framework for General\n  Purpose Computation using Neurons, Precise Timing, Delays, and Synchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant research over the past two decades in developing\nnew platforms for spiking neural computation. Current neural computers are\nprimarily developed to mimick biology. They use neural networks which can be\ntrained to perform specific tasks to mainly solve pattern recognition problems.\nThese machines can do more than simulate biology, they allow us to re-think our\ncurrent paradigm of computation. The ultimate goal is to develop brain inspired\ngeneral purpose computation architectures that can breach the current\nbottleneck introduced by the Von Neumann architecture. This work proposes a new\nframework for such a machine. We show that the use of neuron like units with\nprecise timing representation, synaptic diversity, and temporal delays allows\nus to set a complete, scalable compact computation framework. The presented\nframework provides both linear and non linear operations, allowing us to\nrepresent and solve any function. We show usability in solving real use cases\nfrom simple differential equations to sets of non-linear differential equations\nleading to chaotic attractors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 15:09:07 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Lagorce", "Xavier", ""], ["Benosman", "Ryad", ""]]}, {"id": "1507.06228", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Klaus Greff, J\\\"urgen Schmidhuber", "title": "Training Very Deep Networks", "comments": "11 pages. Extends arXiv:1505.00387. Project webpage is at\n  http://people.idsia.ch/~rupesh/very_deep_learning/. in Advances in Neural\n  Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical and empirical evidence indicates that the depth of neural\nnetworks is crucial for their success. However, training becomes more difficult\nas depth increases, and training of very deep networks remains an open problem.\nHere we introduce a new architecture designed to overcome this. Our so-called\nhighway networks allow unimpeded information flow across many layers on\ninformation highways. They are inspired by Long Short-Term Memory recurrent\nnetworks and use adaptive gating units to regulate the information flow. Even\nwith hundreds of layers, highway networks can be trained directly through\nsimple gradient descent. This enables the study of extremely deep and efficient\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 15:29:14 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 16:25:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Greff", "Klaus", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1507.06550", "submitter": "Joao Carreira", "authors": "Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, Jitendra Malik", "title": "Human Pose Estimation with Iterative Error Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical feature extractors such as Convolutional Networks (ConvNets)\nhave achieved impressive performance on a variety of classification tasks using\npurely feedforward processing. Feedforward architectures can learn rich\nrepresentations of the input space but do not explicitly model dependencies in\nthe output spaces, that are quite structured for tasks such as articulated\nhuman pose estimation or object segmentation. Here we propose a framework that\nexpands the expressive power of hierarchical feature extractors to encompass\nboth input and output spaces, by introducing top-down feedback. Instead of\ndirectly predicting the outputs in one go, we use a self-correcting model that\nprogressively changes an initial solution by feeding back error predictions, in\na process we call Iterative Error Feedback (IEF). IEF shows excellent\nperformance on the task of articulated pose estimation in the challenging MPII\nand LSP benchmarks, matching the state-of-the-art without requiring ground\ntruth scale annotation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 16:20:57 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 12:37:48 GMT"}, {"version": "v3", "created": "Sun, 12 Jun 2016 19:10:55 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Carreira", "Joao", ""], ["Agrawal", "Pulkit", ""], ["Fragkiadaki", "Katerina", ""], ["Malik", "Jitendra", ""]]}, {"id": "1507.06594", "submitter": "Jack Kelly", "authors": "Jack Kelly and William Knottenbelt", "title": "Neural NILM: Deep Neural Networks Applied to Energy Disaggregation", "comments": "To appear in ACM BuildSys'15, November 4--5, 2015, Seoul", "journal-ref": null, "doi": "10.1145/2821650.2821672", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation estimates appliance-by-appliance electricity\nconsumption from a single meter that measures the whole home's electricity\ndemand. Recently, deep neural networks have driven remarkable improvements in\nclassification performance in neighbouring machine learning fields such as\nimage classification and automatic speech recognition. In this paper, we adapt\nthree deep neural network architectures to energy disaggregation: 1) a form of\nrecurrent neural network called `long short-term memory' (LSTM); 2) denoising\nautoencoders; and 3) a network which regresses the start time, end time and\naverage power demand of each appliance activation. We use seven metrics to test\nthe performance of these algorithms on real aggregate power data from five\nappliances. Tests are performed against a house not seen during training and\nagainst houses seen during training. We find that all three neural nets achieve\nbetter F1 scores (averaged over all five appliances) than either combinatorial\noptimisation or factorial hidden Markov models and that our neural net\nalgorithms generalise well to an unseen house.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 18:18:49 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 10:10:51 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 10:15:35 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Kelly", "Jack", ""], ["Knottenbelt", "William", ""]]}, {"id": "1507.06803", "submitter": "Ferran Mazzanti", "authors": "E. Romero, F. Mazzanti, J. Delgado", "title": "A Neighbourhood-Based Stopping Criterion for Contrastive Divergence\n  Learning", "comments": "7 pages. arXiv admin note: substantial text overlap with\n  arXiv:1312.6062", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are general unsupervised learning\ndevices to ascertain generative models of data distributions. RBMs are often\ntrained using the Contrastive Divergence learning algorithm (CD), an\napproximation to the gradient of the data log-likelihood. A simple\nreconstruction error is often used as a stopping criterion for CD, although\nseveral authors\n\\cite{schulz-et-al-Convergence-Contrastive-Divergence-2010-NIPSw,\nfischer-igel-Divergence-Contrastive-Divergence-2010-ICANN} have raised doubts\nconcerning the feasibility of this procedure. In many cases the evolution curve\nof the reconstruction error is monotonic while the log-likelihood is not, thus\nindicating that the former is not a good estimator of the optimal stopping\npoint for learning. However, not many alternatives to the reconstruction error\nhave been discussed in the literature. In this manuscript we investigate simple\nalternatives to the reconstruction error, based on the inclusion of information\ncontained in neighboring states to the training set, as a stopping criterion\nfor CD learning.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:45:19 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Romero", "E.", ""], ["Mazzanti", "F.", ""], ["Delgado", "J.", ""]]}, {"id": "1507.06821", "submitter": "Andreas Eitel", "authors": "Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello, Martin\n  Riedmiller, Wolfram Burgard", "title": "Multimodal Deep Learning for Robust RGB-D Object Recognition", "comments": "Final version submitted to IROS'2015, results unchanged,\n  reformulation of some text passages in abstract and introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust object recognition is a crucial ingredient of many, if not all,\nreal-world robotics applications. This paper leverages recent progress on\nConvolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture\nfor object recognition. Our architecture is composed of two separate CNN\nprocessing streams - one for each modality - which are consecutively combined\nwith a late fusion network. We focus on learning with imperfect sensor data, a\ntypical problem in real-world robotics tasks. For accurate learning, we\nintroduce a multi-stage training methodology and two crucial ingredients for\nhandling depth data with CNNs. The first, an effective encoding of depth\ninformation for CNNs that enables learning without the need for large depth\ndatasets. The second, a data augmentation scheme for robust learning with depth\nimages by corrupting them with realistic noise patterns. We present\nstate-of-the-art results on the RGB-D object dataset and show recognition in\nchallenging RGB-D real-world noisy settings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 12:20:19 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 13:04:29 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Eitel", "Andreas", ""], ["Springenberg", "Jost Tobias", ""], ["Spinello", "Luciano", ""], ["Riedmiller", "Martin", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1507.06877", "submitter": "Beno\\^it Girard", "authors": "St\\'ephane Doncieux, Jean Li\\'enard, Beno\\^it Girard, Mohamed\n  Hamdaoui, Jo\\\"el Chaskalovic", "title": "Multi-objective analysis of computational models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models are of increasing complexity and their behavior may in\nparticular emerge from the interaction of different parts. Studying such models\nbecomes then more and more difficult and there is a need for methods and tools\nsupporting this process. Multi-objective evolutionary algorithms generate a set\nof trade-off solutions instead of a single optimal solution. The availability\nof a set of solutions that have the specificity to be optimal relative to\ncarefully chosen objectives allows to perform data mining in order to better\nunderstand model features and regularities. We review the corresponding work,\npropose a unifying framework, and highlight its potential use. Typical\nquestions that such a methodology allows to address are the following: what are\nthe most critical parameters of the model? What are the relations between the\nparameters and the objectives? What are the typical behaviors of the model? Two\nexamples are provided to illustrate the capabilities of the methodology. The\nfeatures of a flapping-wing robot are thus evaluated to find out its\nspeed-energy relation, together with the criticality of its parameters. A\nneurocomputational model of the Basal Ganglia brain nuclei is then considered\nand its most salient features according to this methodology are presented and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 15:16:19 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Doncieux", "St\u00e9phane", ""], ["Li\u00e9nard", "Jean", ""], ["Girard", "Beno\u00eet", ""], ["Hamdaoui", "Mohamed", ""], ["Chaskalovic", "Jo\u00ebl", ""]]}, {"id": "1507.06947", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Kanishka Rao, Fran\\c{c}oise Beaufays", "title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech\n  Recognition", "comments": "To be published in the INTERSPEECH 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently shown that deep Long Short-Term Memory (LSTM) recurrent\nneural networks (RNNs) outperform feed forward deep neural networks (DNNs) as\nacoustic models for speech recognition. More recently, we have shown that the\nperformance of sequence trained context dependent (CD) hidden Markov model\n(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained\nphone models initialized with connectionist temporal classification (CTC). In\nthis paper, we present techniques that further improve performance of LSTM RNN\nacoustic models for large vocabulary speech recognition. We show that frame\nstacking and reduced frame rate lead to more accurate models and faster\ndecoding. CD phone modeling leads to further improvements. We also present\ninitial results for LSTM RNN models outputting words directly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 18:28:32 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Rao", "Kanishka", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1507.07200", "submitter": "Jaderick Pabico", "authors": "Jaderick P. Pabico, Jose Rene L. Micor and Elmer Rico E. Mojica", "title": "A Neural Prototype for a Virtual Chemical Spectrophotometer", "comments": "5 pages, 3 figures, appeared in Proceedings (CDROM) of the 6th\n  National Conference on IT in Education (NCITE 2008), University of the\n  Philippines Los Ba\\~nos, 23-24 October 2008", "journal-ref": "Philippine Computing Journal, 4(2):39-42, 2009", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A virtual chemical spectrophotometer for the simultaneous analysis of nickel\n(Ni) and cobalt (Co) was developed based on an artificial neural network (ANN).\nThe developed ANN correlates the respective concentrations of Co and Ni given\nthe absorbance profile of a Co-Ni mixture based on the Beer's Law. The virtual\nchemical spectrometer was trained using a 3-layer jump connection neural\nnetwork model (NNM) with 126 input nodes corresponding to the 126 absorbance\nreadings from 350 nm to 600 nm, 70 nodes in the hidden layer using a logistic\nactivation function, and 2 nodes in the output layer with a logistic function.\nTest result shows that the NNM has correlation coefficients of 0.9953 and\n0.9922 when predicting [Co] and [Ni], respectively. We observed, however, that\nthe NNM has a duality property and that there exists a real-world practical\napplication in solving the dual problem: Predict the Co-Ni mixture's absorbance\nprofile given [Co] and [Ni]. It turns out that the dual problem is much harder\nto solve because the intended output has a much bigger cardinality than that of\nthe input. Thus, we trained the dual ANN, a 3-layer jump connection nets with 2\ninput nodes corresponding to [Co] and [Ni], 70-logistic-activated nodes in the\nhidden layer, and 126 output nodes corresponding to the 126 absorbance readings\nfrom 250 nm to 600 nm. Test result shows that the dual NNM has correlation\ncoefficients that range from 0.9050 through 0.9980 at 356 nm through 578 nm\nwith the maximum coefficient observed at 480 nm. This means that the dual ANN\ncan be used to predict the absorbance profile given the respective Co-Ni\nconcentrations which can be of importance in creating academic models for a\nvirtual chemical spectrophotometer.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 14:13:29 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Pabico", "Jaderick P.", ""], ["Micor", "Jose Rene L.", ""], ["Mojica", "Elmer Rico E.", ""]]}, {"id": "1507.07204", "submitter": "Yasir Shoaib", "authors": "Yasir Shoaib, Olivia Das", "title": "Modeling Website Workload Using Neural Networks", "comments": "25 pages, 13 figures, 21 references, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, artificial neural networks (ANN) are used for modeling the\nnumber of requests received by 1998 FIFA World Cup website. Modeling is done by\nmeans of time-series forecasting. The log traces of the website, available\nthrough the Internet Traffic Archive (ITA), are processed to obtain two\ntime-series data sets that are used for finding the following measurements:\nrequests/day and requests/second. These are modeled by training and simulating\nANN. The method followed to collect and process the data, and perform the\nexperiments have been detailed in this article. In total, 13 cases have been\ntried and their results have been presented, discussed, compared and\nsummarized. Lastly, future works have also been mentioned.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 14:42:56 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Shoaib", "Yasir", ""], ["Das", "Olivia", ""]]}, {"id": "1507.07301", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu and Victor O.K. Li", "title": "A Social Spider Algorithm for Solving the Non-convex Economic Load\n  Dispatch Problem", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2015.07.037", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic Load Dispatch (ELD) is one of the essential components in power\nsystem control and operation. Although conventional ELD formulation can be\nsolved using mathematical programming techniques, modern power system\nintroduces new models of the power units which are non-convex,\nnon-differentiable, and sometimes non-continuous. In order to solve such\nnon-convex ELD problems, in this paper we propose a new approach based on the\nSocial Spider Algorithm (SSA). The classical SSA is modified and enhanced to\nadapt to the unique characteristics of ELD problems, e.g., valve-point effects,\nmulti-fuel operations, prohibited operating zones, and line losses. To\ndemonstrate the superiority of our proposed approach, five widely-adopted test\nsystems are employed and the simulation results are compared with the\nstate-of-the-art algorithms. In addition, the parameter sensitivity is\nillustrated by a series of simulations. The simulation results show that SSA\ncan solve ELD problems effectively and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 05:30:15 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Yu", "James J. Q.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1507.07374", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Andrey Ustyuzhanin", "title": "A genetic algorithm for autonomous navigation in partially observable\n  domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of autonomous navigation is one of the basic problems for\nrobotics. Although, in general, it may be challenging when an autonomous\nvehicle is placed into partially observable domain. In this paper we consider\nsimplistic environment model and introduce a navigation algorithm based on\nLearning Classifier System.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 11:50:44 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Borisyak", "Maxim", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1507.07403", "submitter": "Tim Taylor", "authors": "Tim Taylor", "title": "Requirements for Open-Ended Evolution in Natural and Artificial Systems", "comments": "Presented at the EvoEvo Workshop at the European Conference on\n  Artificial Life 2015 (ECAL 2015), University of York, UK, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended evolutionary dynamics remains an elusive goal for artificial\nevolutionary systems. Many ideas exist in the biological literature beyond the\nbasic Darwinian requirements of variation, differential reproduction and\ninheritance. I argue that these ideas can be seen as aspects of five\nfundamental requirements for open-ended evolution: (1) robustly reproductive\nindividuals, (2) a medium allowing the possible existence of a practically\nunlimited diversity of individuals and interactions, (3) individuals capable of\nproducing more complex offspring, (4) mutational pathways to other viable\nindividuals, and (5) drive for continued evolution. I briefly discuss\nimplications of this view for the design of artificial systems with greater\nevolutionary potential.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 13:47:41 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Taylor", "Tim", ""]]}, {"id": "1507.07680", "submitter": "Yann Ollivier", "authors": "Yann Ollivier, Corentin Tallec, Guillaume Charpiat", "title": "Training recurrent networks online without backtracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \"NoBackTrack\" algorithm to train the parameters of dynamical\nsystems such as recurrent neural networks. This algorithm works in an online,\nmemoryless setting, thus requiring no backpropagation through time, and is\nscalable, avoiding the large computational and memory cost of maintaining the\nfull gradient of the current state with respect to the parameters.\n  The algorithm essentially maintains, at each time, a single search direction\nin parameter space. The evolution of this search direction is partly stochastic\nand is constructed in such a way to provide, at every time, an unbiased random\nestimate of the gradient of the loss function with respect to the parameters.\nBecause the gradient estimate is unbiased, on average over time the parameter\nis updated as it should.\n  The resulting gradient estimate can then be fed to a lightweight Kalman-like\nfilter to yield an improved algorithm. For recurrent neural networks, the\nresulting algorithms scale linearly with the number of parameters.\n  Small-scale experiments confirm the suitability of the approach, showing that\nthe stochastic approximation of the gradient introduced in the algorithm is not\ndetrimental to learning. In particular, the Kalman-like version of NoBackTrack\nis superior to backpropagation through time (BPTT) when the time span of\ndependencies in the data is longer than the truncation span for BPTT.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 08:26:50 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 22:29:38 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ollivier", "Yann", ""], ["Tallec", "Corentin", ""], ["Charpiat", "Guillaume", ""]]}, {"id": "1507.07870", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist and Peter Sarlin", "title": "Detect & Describe: Deep learning of bank stress in the news", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.AI cs.LG cs.NE q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News is a pertinent source of information on financial risks and stress\nfactors, which nevertheless is challenging to harness due to the sparse and\nunstructured nature of natural text. We propose an approach based on\ndistributional semantics and deep learning with neural networks to model and\nlink text to a scarce set of bank distress events. Through unsupervised\ntraining, we learn semantic vector representations of news articles as\npredictors of distress events. The predictive model that we learn can signal\ncoinciding stress with an aggregated index at bank or European level, while\ncrucially allowing for automatic extraction of text descriptions of the events,\nbased on passages with high stress levels. The method offers insight that\nmodels based on other types of data cannot provide, while offering a general\nmeans for interpreting this type of semantic-predictive model. We model bank\ndistress with data on 243 events and 6.6M news articles for 101 large European\nbanks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 18:47:09 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1507.08007", "submitter": "Anton Eremeev", "authors": "Anton Eremeev", "title": "On Proportions of Fit Individuals in Population of Evolutionary\n  Algorithm with Tournament Selection", "comments": "Submited to Evolutionary Computation journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a fitness-level model of a non-elitist\nmutation-only evolutionary algorithm (EA) with tournament selection. The model\nprovides upper and lower bounds for the expected proportion of the individuals\nwith fitness above given thresholds. In the case of so-called monotone\nmutation, the obtained bounds imply that increasing the tournament size\nimproves the EA performance. As corollaries, we obtain an exponentially\nvanishing tail bound for the Randomized Local Search on unimodal functions and\npolynomial upper bounds on the runtime of EAs on 2-SAT problem and on a family\nof Set Cover problems proposed by E. Balas.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 02:24:55 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 06:59:33 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Eremeev", "Anton", ""]]}, {"id": "1507.08286", "submitter": "David Held", "authors": "David Held, Sebastian Thrun, Silvio Savarese", "title": "Deep Learning for Single-View Instance Recognition", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have typically been trained on large datasets in which\nmany training examples are available. However, many real-world product datasets\nhave only a small number of images available for each product. We explore the\nuse of deep learning methods for recognizing object instances when we have only\na single training example per class. We show that feedforward neural networks\noutperform state-of-the-art methods for recognizing objects from novel\nviewpoints even when trained from just a single image per object. To further\nimprove our performance on this task, we propose to take advantage of a\nsupplementary dataset in which we observe a separate set of objects from\nmultiple viewpoints. We introduce a new approach for training deep learning\nmethods for instance recognition with limited training data, in which we use an\nauxiliary multi-view dataset to train our network to be robust to viewpoint\nchanges. We find that this approach leads to a more robust classifier for\nrecognizing objects from novel viewpoints, outperforming previous\nstate-of-the-art approaches including keypoint-matching, template-based\ntechniques, and sparse coding.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 20:11:12 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Held", "David", ""], ["Thrun", "Sebastian", ""], ["Savarese", "Silvio", ""]]}, {"id": "1507.08467", "submitter": "David Sousa-Rodrigues", "authors": "Cristian Jimenez-Romero and David Sousa-Rodrigues and Jeffrey H.\n  Johnson and Vitorino Ramos", "title": "A Model for Foraging Ants, Controlled by Spiking Neural Networks and\n  Double Pheromones", "comments": "This work has been accepted for presentation at the UK Workshop on\n  Computational Intelligence --- University of Exeter, September 2015\n  http://www.ukci2015.ex.ac.uk/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of an Ant System where ants are controlled by a spiking neural\ncircuit and a second order pheromone mechanism in a foraging task is presented.\nA neural circuit is trained for individual ants and subsequently the ants are\nexposed to a virtual environment where a swarm of ants performed a resource\nforaging task. The model comprises an associative and unsupervised learning\nstrategy for the neural circuit of the ant. The neural circuit adapts to the\nenvironment by means of classical conditioning. The initially unknown\nenvironment includes different types of stimuli representing food and obstacles\nwhich, when they come in direct contact with the ant, elicit a reflex response\nin the motor neural system of the ant: moving towards or away from the source\nof the stimulus. The ants are released on a landscape with multiple food\nsources where one ant alone would have difficulty harvesting the landscape to\nmaximum efficiency. The introduction of a double pheromone mechanism yields\nbetter results than traditional ant colony optimization strategies. Traditional\nant systems include mainly a positive reinforcement pheromone. This approach\nuses a second pheromone that acts as a marker for forbidden paths (negative\nfeedback). This blockade is not permanent and is controlled by the evaporation\nrate of the pheromones. The combined action of both pheromones acts as a\ncollective stigmergic memory of the swarm, which reduces the search space of\nthe problem. This paper explores how the adaptation and learning abilities\nobserved in biologically inspired cognitive architectures is synergistically\nenhanced by swarm optimization strategies. The model portraits two forms of\nartificial intelligent behaviour: at the individual level the spiking neural\nnetwork is the main controller and at the collective level the pheromone\ndistribution is a map towards the solution emerged by the colony.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 11:57:54 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 09:25:03 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 14:17:39 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Jimenez-Romero", "Cristian", ""], ["Sousa-Rodrigues", "David", ""], ["Johnson", "Jeffrey H.", ""], ["Ramos", "Vitorino", ""]]}, {"id": "1507.08818", "submitter": "Dario Garcia-Gasulla", "authors": "D. Garcia-Gasulla, J. B\\'ejar, U. Cort\\'es, E. Ayguad\\'e, J. Labarta,\n  T. Suzumura and R. Chen", "title": "A Visual Embedding for the Unsupervised Extraction of Abstract Semantics", "comments": "14 pages, 5 figures, accepted at Cognitive Systems Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-space word representations obtained from neural network models have\nbeen shown to enable semantic operations based on vector arithmetic. In this\npaper, we explore the existence of similar information on vector\nrepresentations of images. For that purpose we define a methodology to obtain\nlarge, sparse vector representations of image classes, and generate vectors\nthrough the state-of-the-art deep learning architecture GoogLeNet for 20K\nimages obtained from ImageNet. We first evaluate the resultant vector-space\nsemantics through its correlation with WordNet distances, and find vector\ndistances to be strongly correlated with linguistic semantics. We then explore\nthe location of images within the vector space, finding elements close in\nWordNet to be clustered together, regardless of significant visual variances\n(e.g. 118 dog types). More surprisingly, we find that the space unsupervisedly\nseparates complex classes without prior knowledge (e.g. living things).\nAfterwards, we consider vector arithmetics. Although we are unable to obtain\nmeaningful results on this regard, we discuss the various problem we\nencountered, and how we consider to solve them. Finally, we discuss the impact\nof our research for cognitive systems, focusing on the role of the architecture\nbeing used.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 10:16:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 17:27:56 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 17:03:54 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 14:37:13 GMT"}, {"version": "v5", "created": "Fri, 25 Nov 2016 09:05:50 GMT"}, {"version": "v6", "created": "Fri, 16 Dec 2016 13:58:59 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Garcia-Gasulla", "D.", ""], ["B\u00e9jar", "J.", ""], ["Cort\u00e9s", "U.", ""], ["Ayguad\u00e9", "E.", ""], ["Labarta", "J.", ""], ["Suzumura", "T.", ""], ["Chen", "R.", ""]]}, {"id": "1507.08937", "submitter": "Ronald Hochreiter", "authors": "Stefan Haring and Ronald Hochreiter", "title": "Efficient and robust calibration of the Heston option pricing model for\n  American options using an improved Cuckoo Search Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an improved Cuckoo Search Algorithm is developed to allow for\nan efficient and robust calibration of the Heston option pricing model for\nAmerican options. Calibration of stochastic volatility models like the Heston\nis significantly harder than classical option pricing models as more parameters\nhave to be estimated. The difficult task of calibrating one of these models to\nAmerican Put options data is the main objective of this paper. Numerical\nresults are shown to substantiate the suitability of the chosen method to\ntackle this problem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 16:38:25 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Haring", "Stefan", ""], ["Hochreiter", "Ronald", ""]]}]