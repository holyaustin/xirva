[{"id": "1902.00016", "submitter": "Dimche Kostadinov", "authors": "Dimche Kostadinov, Behrooz Razdehi, Slava Voloshynovskiy", "title": "Network Parameter Learning Using Nonlinear Transforms, Local\n  Representation Goals and Local Propagation Constraints", "comments": "arXiv admin note: text overlap with arXiv:1805.07802", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel concept for learning of the parameters in\na neural network. Our idea is grounded on modeling a learning problem that\naddresses a trade-off between (i) satisfying local objectives at each node and\n(ii) achieving desired data propagation through the network under (iii) local\npropagation constraints. We consider two types of nonlinear transforms which\ndescribe the network representations. One of the nonlinear transforms serves as\nactivation function. The other one enables a locally adjusted, deviation\ncorrective components to be included in the update of the network weights in\norder to enable attaining target specific representations at the last network\nnode. Our learning principle not only provides insight into the understanding\nand the interpretation of the learning dynamics, but it offers theoretical\nguarantees over decoupled and parallel parameter estimation strategy that\nenables learning in synchronous and asynchronous mode. Numerical experiments\nvalidate the potential of our approach on image recognition task. The\npreliminary results show advantages in comparison to the state-of-the-art\nmethods, w.r.t. the learning time and the network size while having competitive\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:43:55 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Kostadinov", "Dimche", ""], ["Razdehi", "Behrooz", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1902.00107", "submitter": "Dirk Sudholt", "authors": "Per Kristian Lehre and Dirk Sudholt", "title": "Parallel Black-Box Complexity with Tail Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new black-box complexity model for search algorithms evaluating\n$\\lambda$ search points in parallel. The parallel unary unbiased black-box\ncomplexity gives lower bounds on the number of function evaluations every\nparallel unary unbiased black-box algorithm needs to optimise a given problem.\nIt captures the inertia caused by offspring populations in evolutionary\nalgorithms and the total computational effort in parallel metaheuristics.\n  We present complexity results for LeadingOnes and OneMax. Our main result is\na general performance limit: we prove that on every function every\n$\\lambda$-parallel unary unbiased algorithm needs at least\n$\\Omega(\\frac{\\lambda n}{\\ln \\lambda} + n \\log n)$ evaluations to find any\ndesired target set of up to exponential size, with an overwhelming probability.\nThis yields lower bounds for the typical optimisation time on unimodal and\nmultimodal problems, for the time to find any local optimum, and for the time\nto even get close to any optimum. The power and versatility of this approach is\nshown for a wide range of illustrative problems from combinatorial\noptimisation. Our performance limits can guide parameter choice and algorithm\ndesign; we demonstrate the latter by presenting an optimal $\\lambda$-parallel\nalgorithm for OneMax that uses parallelism most effectively.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 22:30:10 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Lehre", "Per Kristian", ""], ["Sudholt", "Dirk", ""]]}, {"id": "1902.00275", "submitter": "Jonathan Ho", "authors": "Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, Pieter Abbeel", "title": "Flow++: Improving Flow-Based Generative Models with Variational\n  Dequantization and Architecture Design", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models are powerful exact likelihood models with\nefficient sampling and inference. Despite their computational efficiency,\nflow-based models generally have much worse density modeling performance\ncompared to state-of-the-art autoregressive models. In this paper, we\ninvestigate and improve upon three limiting design choices employed by\nflow-based models in prior work: the use of uniform noise for dequantization,\nthe use of inexpressive affine flows, and the use of purely convolutional\nconditioning networks in coupling layers. Based on our findings, we propose\nFlow++, a new flow-based model that is now the state-of-the-art\nnon-autoregressive model for unconditional density estimation on standard image\nbenchmarks. Our work has begun to close the significant performance gap that\nhas so far existed between autoregressive models and flow-based models. Our\nimplementation is available at https://github.com/aravindsrinivas/flowpp\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 11:13:40 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 23:16:06 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ho", "Jonathan", ""], ["Chen", "Xi", ""], ["Srinivas", "Aravind", ""], ["Duan", "Yan", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1902.00304", "submitter": "Carola Doerr", "authors": "Benjamin Doerr, Carola Doerr, Frank Neumann", "title": "Fast Re-Optimization via Structural Diversity", "comments": "To appear at Genetic and Evolutionary Computation Conference (GECCO\n  '19)", "journal-ref": null, "doi": "10.1145/3321707.3321731", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a problem instance is perturbed by a small modification, one would hope\nto find a good solution for the new instance by building on a known good\nsolution for the previous one. Via a rigorous mathematical analysis, we show\nthat evolutionary algorithms, despite usually being robust problem solvers, can\nhave unexpected difficulties to solve such re-optimization problems. When\nstarted with a random Hamming neighbor of the optimum, the (1+1) evolutionary\nalgorithm takes $\\Omega(n^2)$ time to optimize the LeadingOnes benchmark\nfunction, which is the same asymptotic optimization time when started in a\nrandomly chosen solution. There is hence no significant advantage from\nre-optimizing a structurally good solution.\n  We then propose a way to overcome such difficulties. As our mathematical\nanalysis reveals, the reason for this undesired behavior is that during the\noptimization structurally good solutions can easily be replaced by structurally\nworse solutions of equal or better fitness. We propose a simple diversity\nmechanism that prevents this behavior, thereby reducing the re-optimization\ntime for LeadingOnes to $O(\\gamma\\delta n)$, where $\\gamma$ is the population\nsize used by the diversity mechanism and $\\delta \\le \\gamma$ the Hamming\ndistance of the new optimum from the previous solution. We show similarly fast\nre-optimization times for the optimization of linear functions with changing\nconstraints and for the minimum spanning tree problem.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 12:35:40 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 10:23:51 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Neumann", "Frank", ""]]}, {"id": "1902.00685", "submitter": "M. Hanefi Calp", "authors": "M. Hanefi Calp", "title": "Medical Diagnosis with a Novel SVM-CoDOA Based Hybrid Approach", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning is an important sub-field of the Artificial Intelligence and\nit has been become a very critical task to train Machine Learning techniques\nvia effective method or techniques. Recently, researchers try to use\nalternative techniques to improve ability of Machine Learning techniques.\nMoving from the explanations, objective of this study is to introduce a novel\nSVM-CoDOA (Cognitive Development Optimization Algorithm trained Support Vector\nMachines) system for general medical diagnosis. In detail, the system consists\nof a SVM, which is trained by CoDOA, a newly developed optimization algorithm.\nAs it is known, use of optimization algorithms is an essential task to train\nand improve Machine Learning techniques. In this sense, the study has provided\na medical diagnosis oriented problem scope in order to show effectiveness of\nthe SVM-CoDOA hybrid formation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 10:32:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Calp", "M. Hanefi", ""]]}, {"id": "1902.00703", "submitter": "Giovanni Iacca Dr.", "authors": "Stefano Fioravanzo and Giovanni Iacca", "title": "Evaluating MAP-Elites on Constrained Optimization Problems", "comments": null, "journal-ref": null, "doi": "10.1145/3319619.3321939", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained optimization problems are often characterized by multiple\nconstraints that, in the practice, must be satisfied with different tolerance\nlevels. While some constraints are hard and as such must be satisfied with\nzero-tolerance, others may be soft, such that non-zero violations are\nacceptable. Here, we evaluate the applicability of MAP-Elites to \"illuminate\"\nconstrained search spaces by mapping them into feature spaces where each\nfeature corresponds to a different constraint. On the one hand, MAP-Elites\nimplicitly preserves diversity, thus allowing a good exploration of the search\nspace. On the other hand, it provides an effective visualization that\nfacilitates a better understanding of how constraint violations correlate with\nthe objective function. We demonstrate the feasibility of this approach on a\nlarge set of benchmark problems, in various dimensionalities, and with\ndifferent algorithmic configurations. As expected, numerical results show that\na basic version of MAP-Elites cannot compete on all problems (especially those\nwith equality constraints) with state-of-the-art algorithms that use gradient\ninformation or advanced constraint handling techniques. Nevertheless, it has a\nhigher potential at finding constraint violations vs. objectives trade-offs and\nproviding new problem information. As such, it could be used in the future as\nan effective building-block for designing new constrained optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 11:59:29 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 08:04:27 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 14:53:05 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 07:09:37 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Fioravanzo", "Stefano", ""], ["Iacca", "Giovanni", ""]]}, {"id": "1902.00882", "submitter": "Bogdan Burlacu", "authors": "Bogdan Burlacu, Michael Affenzeller, Gabriel Kronberger, Michael\n  Kommenda", "title": "Online Diversity Control in Symbolic Regression via a Fast Hash-based\n  Tree Similarity Measure", "comments": "8 pages, conference, submitted to congress on evolutionary\n  computation", "journal-ref": null, "doi": "10.1109/CEC.2019.8790162", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity represents an important aspect of genetic programming, being\ndirectly correlated with search performance. When considered at the genotype\nlevel, diversity often requires expensive tree distance measures which have a\nnegative impact on the algorithm's runtime performance. In this work we\nintroduce a fast, hash-based tree distance measure to massively speed-up the\ncalculation of population diversity during the algorithmic run. We combine this\nmeasure with the standard GA and the NSGA-II genetic algorithms to steer the\nsearch towards higher diversity. We validate the approach on a collection of\nbenchmark problems for symbolic regression where our method consistently\noutperforms the standard GA as well as NSGA-II configurations with different\nsecondary objectives.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 11:20:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Burlacu", "Bogdan", ""], ["Affenzeller", "Michael", ""], ["Kronberger", "Gabriel", ""], ["Kommenda", "Michael", ""]]}, {"id": "1902.01028", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "Can SGD Learn Recurrent Neural Networks with Provable Generalization?", "comments": "V2 polishes writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are among the most popular models in\nsequential data analysis. Yet, in the foundational PAC learning language, what\nconcept class can it learn? Moreover, how can the same recurrent unit\nsimultaneously learn functions from different input tokens to different output\ntokens, without affecting each other? Existing generalization bounds for RNN\nscale exponentially with the input length, significantly limiting their\npractical implications.\n  In this paper, we show using the vanilla stochastic gradient descent (SGD),\nRNN can actually learn some notable concept class efficiently, meaning that\nboth time and sample complexity scale polynomially in the input length (or\nalmost polynomially, depending on the concept). This concept class at least\nincludes functions where each output token is generated from inputs of earlier\ntokens using a smooth two-layer neural network.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 04:21:00 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 10:43:43 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1902.01094", "submitter": "Qiang Yu", "authors": "Qiang Yu, Yanli Yao, Longbiao Wang, Huajin Tang, Jianwu Dang, Kay Chen\n  Tan", "title": "Robust Environmental Sound Recognition with Sparse Key-point Encoding\n  and Efficient Multi-spike Learning", "comments": "13 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability for environmental sound recognition (ESR) can determine the\nfitness of individuals in a way to avoid dangers or pursue opportunities when\ncritical sound events occur. It still remains mysterious about the fundamental\nprinciples of biological systems that result in such a remarkable ability.\nAdditionally, the practical importance of ESR has attracted an increasing\namount of research attention, but the chaotic and non-stationary difficulties\ncontinue to make it a challenging task. In this study, we propose a spike-based\nframework from a more brain-like perspective for the ESR task. Our framework is\na unifying system with a consistent integration of three major functional parts\nwhich are sparse encoding, efficient learning and robust readout. We first\nintroduce a simple sparse encoding where key-points are used for feature\nrepresentation, and demonstrate its generalization to both spike and non-spike\nbased systems. Then, we evaluate the learning properties of different learning\nrules in details with our contributions being added for improvements. Our\nresults highlight the advantages of the multi-spike learning, providing a\nselection reference for various spike-based developments. Finally, we combine\nthe multi-spike readout with the other parts to form a system for ESR.\nExperimental results show that our framework performs the best as compared to\nother baseline approaches. In addition, we show that our spike-based framework\nhas several advantageous characteristics including early decision making, small\ndataset acquiring and ongoing dynamic processing. Our framework is the first\nattempt to apply the multi-spike characteristic of nervous neurons to ESR. The\noutstanding performance of our approach would potentially contribute to draw\nmore research efforts to push the boundaries of spike-based paradigm to a new\nhorizon.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 09:24:11 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Yu", "Qiang", ""], ["Yao", "Yanli", ""], ["Wang", "Longbiao", ""], ["Tang", "Huajin", ""], ["Dang", "Jianwu", ""], ["Tan", "Kay Chen", ""]]}, {"id": "1902.01208", "submitter": "Rakshit Agrawal", "authors": "Rakshit Agrawal and Luca de Alfaro and David Helmbold", "title": "A New Family of Neural Networks Provably Resistant to Adversarial\n  Attacks", "comments": "arXiv admin note: text overlap with arXiv:1809.09262", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks add perturbations to the input features with the intent\nof changing the classification produced by a machine learning system. Small\nperturbations can yield adversarial examples which are misclassified despite\nbeing virtually indistinguishable from the unperturbed input. Classifiers\ntrained with standard neural network techniques are highly susceptible to\nadversarial examples, allowing an adversary to create misclassifications of\ntheir choice.\n  We introduce a new type of network unit, called MWD (max of weighed distance)\nunits that have a built-in resistant to adversarial attacks. These units are\nhighly non-linear, and we develop the techniques needed to effectively train\nthem. We show that simple interval techniques for propagating perturbation\neffects through the network enables the efficient computation of robustness\n(i.e., accuracy guarantees) for MWD networks under any perturbations, including\nadversarial attacks.\n  MWD networks are significantly more robust to input perturbations than ReLU\nnetworks. On permutation invariant MNIST, when test examples can be perturbed\nby 20% of the input range, MWD networks provably retain accuracy above 83%,\nwhile the accuracy of ReLU networks drops below 5%. The provable accuracy of\nMWD networks is superior even to the observed accuracy of ReLU networks trained\nwith the help of adversarial examples. In the absence of adversarial attacks,\nMWD networks match the performance of sigmoid networks, and have accuracy only\nslightly below that of ReLU networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 00:56:13 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Agrawal", "Rakshit", ""], ["de Alfaro", "Luca", ""], ["Helmbold", "David", ""]]}, {"id": "1902.01429", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan", "title": "A Spiking Neural Network with Local Learning Rules Derived From\n  Nonnegative Similarity Matching", "comments": "ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and analysis of spiking neural network algorithms will be\naccelerated by the advent of new theoretical approaches. In an attempt at such\napproach, we provide a principled derivation of a spiking algorithm for\nunsupervised learning, starting from the nonnegative similarity matching cost\nfunction. The resulting network consists of integrate-and-fire units and\nexhibits local learning rules, making it biologically plausible and also\nsuitable for neuromorphic hardware. We show in simulations that the algorithm\ncan perform sparse feature extraction and manifold learning, two tasks which\ncan be formulated as nonnegative similarity matching problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 19:16:08 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 18:13:17 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Pehlevan", "Cengiz", ""]]}, {"id": "1902.01492", "submitter": "Francesco Conti", "authors": "Arthur Stoutchinin, Francesco Conti, Luca Benini", "title": "Optimally Scheduling CNN Convolutions for Efficient Memory Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded inference engines for convolutional networks must be parsimonious in\nmemory bandwidth and buffer sizing to meet power and cost constraints. We\npresent an analytical memory bandwidth model for loop-nest optimization\ntargeting architectures with application managed buffers. We applied this model\nto optimize the CNN convolution loop-nest. We show that our model is more\naccurate than previously published models. Using this model we can identify\nnon-trivial dataflow schedules that result in lowest communication bandwidth\ngiven tight local buffering constraints. We show that optimal dataflow\nschedules are implementable in practice and that our model is accurate with\nrespect to a real implementation; moreover, we introduce an accelerator\narchitecture, named Hardware Convolution Block (HWC), which implements the\noptimal schedules, and we show it achieves up to 14x memory bandwidth reduction\ncompared to a previously published accelerator with a similar memory interface,\nbut implementing a non-optimal schedule.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 23:09:16 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Stoutchinin", "Arthur", ""], ["Conti", "Francesco", ""], ["Benini", "Luca", ""]]}, {"id": "1902.01599", "submitter": "Huyen Pham", "authors": "C\\^ome Hur\\'e (LPSM (UMR\\_8001), UPD7), Huy\\^en Pham (LPSM\n  (UMR\\_8001), UPD7, FiME Lab), Xavier Warin (EDF, FiME Lab)", "title": "Deep backward schemes for high-dimensional nonlinear PDEs", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA cs.NE math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new machine learning schemes for solving high dimensional\nnonlinear partial differential equations (PDEs). Relying on the classical\nbackward stochastic differential equation (BSDE) representation of PDEs, our\nalgorithms estimate simultaneously the solution and its gradient by deep neural\nnetworks. These approximations are performed at each time step from the\nminimization of loss functions defined recursively by backward induction. The\nmethodology is extended to variational inequalities arising in optimal stopping\nproblems. We analyze the convergence of the deep learning schemes and provide\nerror estimates in terms of the universal approximation of neural networks.\nNumerical results show that our algorithms give very good results till\ndimension 50 (and certainly above), for both PDEs and variational inequalities\nproblems. For the PDEs resolution, our results are very similar to those\nobtained by the recent method in \\cite{weinan2017deep} when the latter\nconverges to the right solution or does not diverge. Numerical tests indicate\nthat the proposed methods are not stuck in poor local minimaas it can be the\ncase with the algorithm designed in \\cite{weinan2017deep}, and no divergence is\nexperienced. The only limitation seems to be due to the inability of the\nconsidered deep neural networks to represent a solution with a too complex\nstructure in high dimension.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 09:20:30 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 07:18:58 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hur\u00e9", "C\u00f4me", "", "LPSM"], ["Pham", "Huy\u00ean", "", "LPSM"], ["Warin", "Xavier", "", "EDF, FiME Lab"]]}, {"id": "1902.01686", "submitter": "Sergei Volodin", "authors": "El-Mahdi El-Mhamdi, Rachid Guerraoui, Andrei Kucharavy, Sergei Volodin", "title": "The Probabilistic Fault Tolerance of Neural Networks in the Continuous\n  Limit", "comments": "10 pages (without references), 2 figures, 2 tables, 1 algorithm, 26\n  pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss of a few neurons in a brain rarely results in any visible loss of\nfunction. However, the insight into what \"few\" means in this context is\nunclear. How many random neuron failures will it take to lead to a visible loss\nof function? In this paper, we address the fundamental question of the impact\nof the crash of a random subset of neurons on the overall computation of a\nneural network and the error in the output it produces. We study fault\ntolerance of neural networks subject to small random neuron/weight crash\nfailures in a probabilistic setting. We give provable guarantees on the\nrobustness of the network to these crashes. Our main contribution is a bound on\nthe error in the output of a network under small random Bernoulli crashes\nproved by using a Taylor expansion in the continuous limit, where close-by\nneurons at a layer are similar. The failure mode we adopt in our model is\ncharacteristic of neuromorphic hardware, a promising technology to speed up\nartificial neural networks, as well as of biological networks. We show that our\ntheoretical bounds can be used to compare the fault tolerance of different\narchitectures and to design a regularizer improving the fault tolerance of a\ngiven architecture. We design an algorithm achieving fault tolerance using a\nreasonable number of neurons. In addition to the theoretical proof, we also\nprovide experimental validation of our results and suggest a connection to the\ngeneralization capacity problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 14:08:21 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 14:18:27 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["El-Mhamdi", "El-Mahdi", ""], ["Guerraoui", "Rachid", ""], ["Kucharavy", "Andrei", ""], ["Volodin", "Sergei", ""]]}, {"id": "1902.01722", "submitter": "Paavo Parmas", "authors": "Paavo Parmas", "title": "Total stochastic gradient algorithms and applications in reinforcement\n  learning", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation and the chain rule of derivatives have been prominent;\nhowever, the total derivative rule has not enjoyed the same amount of\nattention. In this work we show how the total derivative rule leads to an\nintuitive visual framework for creating gradient estimators on graphical\nmodels. In particular, previous \"policy gradient theorems\" are easily derived.\nWe derive new gradient estimators based on density estimation, as well as a\nlikelihood ratio gradient, which \"jumps\" to an intermediate node, not directly\nto the objective function. We evaluate our methods on model-based policy\ngradient algorithms, achieve good performance, and present evidence towards\ndemystifying the success of the popular PILCO algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 14:54:05 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Parmas", "Paavo", ""]]}, {"id": "1902.01724", "submitter": "Kai Arulkumaran", "authors": "Kai Arulkumaran, Antoine Cully, Julian Togelius", "title": "AlphaStar: An Evolutionary Computation Perspective", "comments": "Genetic and EvolutionaryComputation Conference Companion 2019", "journal-ref": null, "doi": "10.1145/3319619.3321894", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In January 2019, DeepMind revealed AlphaStar to the world-the first\nartificial intelligence (AI) system to beat a professional player at the game\nof StarCraft II-representing a milestone in the progress of AI. AlphaStar draws\non many areas of AI research, including deep learning, reinforcement learning,\ngame theory, and evolutionary computation (EC). In this paper we analyze\nAlphaStar primarily through the lens of EC, presenting a new look at the system\nand relating it to many concepts in the field. We highlight some of its most\ninteresting aspects-the use of Lamarckian evolution, competitive co-evolution,\nand quality diversity. In doing so, we hope to provide a bridge between the\nwider EC community and one of the most significant AI systems developed in\nrecent times.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 14:57:15 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 15:05:35 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 16:16:44 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Arulkumaran", "Kai", ""], ["Cully", "Antoine", ""], ["Togelius", "Julian", ""]]}, {"id": "1902.01737", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Antonio Bruno", "title": "Deep Tree Transductions - A Short Survey", "comments": "To appear in the Proceedings of the 2019 INNS Big Data and Deep\n  Learning (INNSBDDL 2019). arXiv admin note: text overlap with\n  arXiv:1809.09096", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper surveys recent extensions of the Long-Short Term Memory networks to\nhandle tree structures from the perspective of learning non-trivial forms of\nisomorph structured transductions. It provides a discussion of modern TreeLSTM\nmodels, showing the effect of the bias induced by the direction of tree\nprocessing. An empirical analysis is performed on real-world benchmarks,\nhighlighting how there is no single model adequate to effectively approach all\ntransduction problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:18:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Bacciu", "Davide", ""], ["Bruno", "Antonio", ""]]}, {"id": "1902.01838", "submitter": "Amritanshu Agrawal", "authors": "Amritanshu Agrawal, Wei Fu, Di Chen, Xipeng Shen, Tim Menzies", "title": "How to \"DODGE\" Complex Software Analytics?", "comments": "13 Pages, Accepted to IEEE Transactions in Software Engineering, 2019", "journal-ref": null, "doi": "10.1109/TSE.2019.2945020", "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques applied to software engineering tasks can be\nimproved by hyperparameter optimization, i.e., automatic tools that find good\nsettings for a learner's control parameters.\n  We show that such hyperparameter optimization can be unnecessarily slow,\nparticularly when the optimizers waste time exploring \"redundant tunings\"',\ni.e., pairs of tunings which lead to indistinguishable results. By ignoring\nredundant tunings, DODGE, a tuning tool, runs orders of magnitude faster, while\nalso generating learners with more accurate predictions than seen in prior\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 18:16:56 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 23:45:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Agrawal", "Amritanshu", ""], ["Fu", "Wei", ""], ["Chen", "Di", ""], ["Shen", "Xipeng", ""], ["Menzies", "Tim", ""]]}, {"id": "1902.01894", "submitter": "Ang Li", "authors": "Ang Li, Ola Spyra, Sagi Perel, Valentin Dalibard, Max Jaderberg,\n  Chenjie Gu, David Budden, Tim Harley, Pramod Gupta", "title": "A Generalized Framework for Population Based Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population Based Training (PBT) is a recent approach that jointly optimizes\nneural network weights and hyperparameters which periodically copies weights of\nthe best performers and mutates hyperparameters during training. Previous PBT\nimplementations have been synchronized glass-box systems. We propose a general,\nblack-box PBT framework that distributes many asynchronous \"trials\" (a small\nnumber of training steps with warm-starting) across a cluster, coordinated by\nthe PBT controller. The black-box design does not make assumptions on model\narchitectures, loss functions or training procedures. Our system supports\ndynamic hyperparameter schedules to optimize both differentiable and\nnon-differentiable metrics. We apply our system to train a state-of-the-art\nWaveNet generative model for human voice synthesis. We show that our PBT system\nachieves better accuracy, less sensitivity and faster convergence compared to\nexisting methods, given the same computational resource.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:11:17 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Li", "Ang", ""], ["Spyra", "Ola", ""], ["Perel", "Sagi", ""], ["Dalibard", "Valentin", ""], ["Jaderberg", "Max", ""], ["Gu", "Chenjie", ""], ["Budden", "David", ""], ["Harley", "Tim", ""], ["Gupta", "Pramod", ""]]}, {"id": "1902.02085", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Danilo Comminiello, Aurelio\n  Uncini", "title": "Widely Linear Kernels for Complex-Valued Kernel Activation Functions", "comments": "Accepted at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued neural networks (CVNNs) have been shown to be powerful\nnonlinear approximators when the input data can be properly modeled in the\ncomplex domain. One of the major challenges in scaling up CVNNs in practice is\nthe design of complex activation functions. Recently, we proposed a novel\nframework for learning these activation functions neuron-wise in a\ndata-dependent fashion, based on a cheap one-dimensional kernel expansion and\nthe idea of kernel activation functions (KAFs). In this paper we argue that,\ndespite its flexibility, this framework is still limited in the class of\nfunctions that can be modeled in the complex domain. We leverage the idea of\nwidely linear complex kernels to extend the formulation, allowing for a richer\nexpressiveness without an increase in the number of adaptable parameters. We\ntest the resulting model on a set of complex-valued image classification\nbenchmarks. Experimental results show that the resulting CVNNs can achieve\nhigher accuracy while at the same time converging faster.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 09:47:55 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Comminiello", "Danilo", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1902.02390", "submitter": "Alexander Ororbia", "authors": "Alexander Ororbia, Ahmed Ahmed Elsaid, Travis Desell", "title": "Investigating Recurrent Neural Network Memory Structures using\n  Neuro-Evolution", "comments": "Some corrections to language, title fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm, Evolutionary eXploration of Augmenting\nMemory Models (EXAMM), which is capable of evolving recurrent neural networks\n(RNNs) using a wide variety of memory structures, such as Delta-RNN, GRU, LSTM,\nMGU and UGRNN cells. EXAMM evolved RNNs to perform prediction of large-scale,\nreal world time series data from the aviation and power industries. These data\nsets consist of very long time series (thousands of readings), each with a\nlarge number of potentially correlated and dependent parameters. Four different\nparameters were selected for prediction and EXAMM runs were performed using\neach memory cell type alone, each cell type with feed forward nodes, and with\nall possible memory cell types. Evolved RNN performance was measured using\nrepeated k-fold cross validation, resulting in 1210 EXAMM runs which evolved\n2,420,000 RNNs in 12,100 CPU hours on a high performance computing cluster.\nGeneralization of the evolved RNNs was examined statistically, providing\ninteresting findings that can help refine the RNN memory cell design as well as\ninform future neuro-evolution algorithms development.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 20:33:12 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 20:23:06 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Ororbia", "Alexander", ""], ["Elsaid", "Ahmed Ahmed", ""], ["Desell", "Travis", ""]]}, {"id": "1902.02399", "submitter": "Jae-Wook Ahn", "authors": "Payel Das, Brian Quanz, Pin-Yu Chen, Jae-wook Ahn, Dhruv Shah", "title": "Toward A Neuro-inspired Creative Decoder", "comments": "Accepted to IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creativity, a process that generates novel and meaningful ideas, involves\nincreased association between task-positive (control) and task-negative\n(default) networks in the human brain. Inspired by this seminal finding, in\nthis study we propose a creative decoder within a deep generative framework,\nwhich involves direct modulation of the neuronal activation pattern after\nsampling from the learned latent space. The proposed approach is fully\nunsupervised and can be used off-the-shelf. Several novelty metrics and human\nevaluation were used to evaluate the creative capacity of the deep decoder. Our\nexperiments on different image datasets (MNIST, FMNIST, MNIST+FMNIST, WikiArt\nand CelebA) reveal that atypical co-activation of highly activated and weakly\nactivated neurons in a deep decoder promotes generation of novel and meaningful\nartifacts.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 21:06:58 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 17:09:42 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 18:24:27 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2020 02:42:12 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Das", "Payel", ""], ["Quanz", "Brian", ""], ["Chen", "Pin-Yu", ""], ["Ahn", "Jae-wook", ""], ["Shah", "Dhruv", ""]]}, {"id": "1902.02588", "submitter": "Carola Doerr", "authors": "Benjamin Doerr, Carola Doerr, Johannes Lengler", "title": "Self-Adjusting Mutation Rates with Provably Optimal Success Rules", "comments": "Conference version appeared at GECCO 2019. This full version is to\n  appear in Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-fifth success rule is one of the best-known and most widely accepted\ntechniques to control the parameters of evolutionary algorithms. While it is\noften applied in the literal sense, a common interpretation sees the one-fifth\nsuccess rule as a family of success-based updated rules that are determined by\nan update strength $F$ and a success rate. We analyze in this work how the\nperformance of the (1+1) Evolutionary Algorithm on LeadingOnes depends on these\ntwo hyper-parameters. Our main result shows that the best performance is\nobtained for small update strengths $F=1+o(1)$ and success rate $1/e$. We also\nprove that the running time obtained by this parameter setting is, apart from\nlower order terms, the same that is achieved with the best fitness-dependent\nmutation rate. We show similar results for the resampling variant of the (1+1)\nEvolutionary Algorithm, which enforces to flip at least one bit per iteration.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 12:38:15 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 00:24:38 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 17:50:16 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Lengler", "Johannes", ""]]}, {"id": "1902.02771", "submitter": "S.H. Shabbeer Basha", "authors": "S.H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, Snehasis\n  Mukherjee", "title": "Impact of Fully Connected Layers on Performance of Convolutional Neural\n  Networks for Image Classification", "comments": "This paper is accepted for publication in Neurocomputing Journal", "journal-ref": null, "doi": "10.1016/j.neucom.2019.10.008", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Networks (CNNs), in domains like computer vision,\nmostly reduced the need for handcrafted features due to its ability to learn\nthe problem-specific features from the raw input data. However, the selection\nof dataset-specific CNN architecture, which mostly performed by either\nexperience or expertise is a time-consuming and error-prone process. To\nautomate the process of learning a CNN architecture, this paper attempts at\nfinding the relationship between Fully Connected (FC) layers with some of the\ncharacteristics of the datasets. The CNN architectures, and recently datasets\nalso, are categorized as deep, shallow, wide, etc. This paper tries to\nformalize these terms along with answering the following questions. (i) What is\nthe impact of deeper/shallow architectures on the performance of the CNN w.r.t.\nFC layers?, (ii) How the deeper/wider datasets influence the performance of CNN\nw.r.t. FC layers?, and (iii) Which kind of architecture (deeper/ shallower) is\nbetter suitable for which kind of (deeper/ wider) datasets. To address these\nfindings, we have performed experiments with three CNN architectures having\ndifferent depths. The experiments are conducted by varying the number of FC\nlayers. We used four widely used datasets including CIFAR-10, CIFAR-100, Tiny\nImageNet, and CRCHistoPhenotypes to justify our findings in the context of the\nimage classification problem. The source code of this research is available at\nhttps://github.com/shabbeersh/Impact-of-FC-layers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 07:42:26 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 04:35:05 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 05:28:01 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Basha", "S. H. Shabbeer", ""], ["Dubey", "Shiv Ram", ""], ["Pulabaigari", "Viswanath", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1902.02947", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Jiawei Su", "title": "Understanding the One-Pixel Attack: Propagation Maps and Locality\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks were shown to be vulnerable to single pixel\nmodifications. However, the reason behind such phenomena has never been\nelucidated. Here, we propose Propagation Maps which show the influence of the\nperturbation in each layer of the network. Propagation Maps reveal that even in\nextremely deep networks such as Resnet, modification in one pixel easily\npropagates until the last layer. In fact, this initial local perturbation is\nalso shown to spread becoming a global one and reaching absolute difference\nvalues that are close to the maximum value of the original feature maps in a\ngiven layer. Moreover, we do a locality analysis in which we demonstrate that\nnearby pixels of the perturbed one in the one-pixel attack tend to share the\nsame vulnerability, revealing that the main vulnerability lies in neither\nneurons nor pixels but receptive fields. Hopefully, the analysis conducted in\nthis work together with a new technique called propagation maps shall shed\nlight into the inner workings of other adversarial samples and be the basis of\nnew defense systems to come.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 06:06:01 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Su", "Jiawei", ""]]}, {"id": "1902.02949", "submitter": "Andrew Lensen", "authors": "Andrew Lensen, Bing Xue, and Mengjie Zhang", "title": "Can Genetic Programming Do Manifold Learning Too?", "comments": "16 pages, accepted in EuroGP '19", "journal-ref": null, "doi": "10.1007/978-3-030-16670-0_8", "report-no": null, "categories": "cs.NE cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory data analysis is a fundamental aspect of knowledge discovery that\naims to find the main characteristics of a dataset. Dimensionality reduction,\nsuch as manifold learning, is often used to reduce the number of features in a\ndataset to a manageable level for human interpretation. Despite this, most\nmanifold learning techniques do not explain anything about the original\nfeatures nor the true characteristics of a dataset. In this paper, we propose a\ngenetic programming approach to manifold learning called GP-MaL which evolves\nfunctional mappings from a high-dimensional space to a lower dimensional space\nthrough the use of interpretable trees. We show that GP-MaL is competitive with\nexisting manifold learning algorithms, while producing models that can be\ninterpreted and re-used on unseen data. A number of promising future directions\nof research are found in the process.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 06:06:50 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Lensen", "Andrew", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1902.03011", "submitter": "Zhenisbek Assylbekov", "authors": "Abylay Zhumekenov, Malika Uteuliyeva, Olzhas Kabdolov, Rustem\n  Takhanov, Zhenisbek Assylbekov, and Alejandro J. Castro", "title": "Fourier Neural Networks: A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review neural network architectures which were motivated by Fourier series\nand integrals and which are referred to as Fourier neural networks. These\nnetworks are empirically evaluated in synthetic and real-world tasks. Neither\nof them outperforms the standard neural network with sigmoid activation\nfunction in the real-world tasks. All neural networks, both Fourier and the\nstandard one, empirically demonstrate lower approximation error than the\ntruncated Fourier series when it comes to an approximation of a known function\nof multiple variables.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 10:36:30 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Zhumekenov", "Abylay", ""], ["Uteuliyeva", "Malika", ""], ["Kabdolov", "Olzhas", ""], ["Takhanov", "Rustem", ""], ["Assylbekov", "Zhenisbek", ""], ["Castro", "Alejandro J.", ""]]}, {"id": "1902.03187", "submitter": "Jason Allred", "authors": "Jason M. Allred and Kaushik Roy", "title": "Controlled Forgetting: Targeted Stimulation and Dopaminergic Plasticity\n  Modulation for Unsupervised Lifelong Learning in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": "10.3389/fnins.2020.00007", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent requires that training samples be drawn from a\nuniformly random distribution of the data. For a deployed system that must\nlearn online from an uncontrolled and unknown environment, the ordering of\ninput samples often fails to meet this criterion, making lifelong learning a\ndifficult challenge. We exploit the locality of the unsupervised Spike Timing\nDependent Plasticity (STDP) learning rule to target local representations in a\nSpiking Neural Network (SNN) to adapt to novel information while protecting\nessential information in the remainder of the SNN from catastrophic forgetting.\nIn our Controlled Forgetting Networks (CFNs), novel information triggers\nstimulated firing and heterogeneously modulated plasticity, inspired by\nbiological dopamine signals, to cause rapid and isolated adaptation in the\nsynapses of neurons associated with outlier information. This targeting\ncontrols the forgetting process in a way that reduces the degradation of\naccuracy for older tasks while learning new tasks. Our experimental results on\nthe MNIST dataset validate the capability of CFNs to learn successfully over\ntime from an unknown, changing environment, achieving 95.36% accuracy, which we\nbelieve is the best unsupervised accuracy ever achieved by a fixed-size,\nsingle-layer SNN on a completely disjoint MNIST dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:50:33 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 15:42:10 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Allred", "Jason M.", ""], ["Roy", "Kaushik", ""]]}, {"id": "1902.03306", "submitter": "Andrea Apicella", "authors": "Andrea Apicella, Francesco Isgr\\`o, Roberto Prevete", "title": "A simple and efficient architecture for trainable activation functions", "comments": null, "journal-ref": "Neurocomputing 370 (2019) 1-15", "doi": "10.1016/j.neucom.2019.08.065", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning automatically the best activation function for the task is an active\ntopic in neural network research. At the moment, despite promising results, it\nis still difficult to determine a method for learning an activation function\nthat is at the same time theoretically simple and easy to implement. Moreover,\nmost of the methods proposed so far introduce new parameters or adopt different\nlearning techniques. In this work we propose a simple method to obtain trained\nactivation function which adds to the neural network local subnetworks with a\nsmall amount of neurons. Experiments show that this approach could lead to\nbetter result with respect to using a pre-defined activation function, without\nintroducing a large amount of extra parameters that need to be learned.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:13:54 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 14:13:06 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Apicella", "Andrea", ""], ["Isgr\u00f2", "Francesco", ""], ["Prevete", "Roberto", ""]]}, {"id": "1902.03326", "submitter": "Bhav Ashok", "authors": "Anubhav Ashok", "title": "Architecture Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to model compression termed\nArchitecture Compression. Instead of operating on the weight or filter space of\nthe network like classical model compression methods, our approach operates on\nthe architecture space. A 1-D CNN encoder-decoder is trained to learn a mapping\nfrom discrete architecture space to a continuous embedding and back.\nAdditionally, this embedding is jointly trained to regress accuracy and\nparameter count in order to incorporate information about the architecture's\neffectiveness on the dataset. During the compression phase, we first encode the\nnetwork and then perform gradient descent in continuous space to optimize a\ncompression objective function that maximizes accuracy and minimizes parameter\ncount. The final continuous feature is then mapped to a discrete architecture\nusing the decoder. We demonstrate the merits of this approach on visual\nrecognition tasks such as CIFAR-10, CIFAR-100, Fashion-MNIST and SVHN and\nachieve a greater than 20x compression on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 23:26:12 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 08:42:42 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 09:10:49 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Ashok", "Anubhav", ""]]}, {"id": "1902.03389", "submitter": "Yuki Saito", "authors": "Hiroki Tamaru, Yuki Saito, Shinnosuke Takamichi, Tomoki Koriyama,\n  Hiroshi Saruwatari", "title": "Generative Moment Matching Network-based Random Modulation Post-filter\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking", "comments": "5 pages, to appear in IEEE ICASSP 2019 (Paper Code: SLP-P22.11,\n  Session: Speech Synthesis III)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generative moment matching network (GMMN)-based\npost-filter that provides inter-utterance pitch variation for deep neural\nnetwork (DNN)-based singing voice synthesis. The natural pitch variation of a\nhuman singing voice leads to a richer musical experience and is used in\ndouble-tracking, a recording method in which two performances of the same\nphrase are recorded and mixed to create a richer, layered sound. However,\nsinging voices synthesized using conventional DNN-based methods never vary\nbecause the synthesis process is deterministic and only one waveform is\nsynthesized from one musical score. To address this problem, we use a GMMN to\nmodel the variation of the modulation spectrum of the pitch contour of natural\nsinging voices and add a randomized inter-utterance variation to the pitch\ncontour generated by conventional DNN-based singing voice synthesis.\nExperimental evaluations suggest that 1) our approach can provide perceptible\ninter-utterance pitch variation while preserving speech quality. We extend our\napproach to double-tracking, and the evaluation demonstrates that 2) GMMN-based\nneural double-tracking is perceptually closer to natural double-tracking than\nconventional signal processing-based artificial double-tracking is.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 07:49:42 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Tamaru", "Hiroki", ""], ["Saito", "Yuki", ""], ["Takamichi", "Shinnosuke", ""], ["Koriyama", "Tomoki", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1902.03419", "submitter": "J\\\"org Stork", "authors": "J\\\"org Stork, Martin Zaefferer, and Thomas Bartz-Beielstein", "title": "Improving NeuroEvolution Efficiency by Surrogate Model-based\n  Optimization with Phenotypic Distance Kernels", "comments": "The final authenticated version of this publication will appear in\n  the proceedings of the Applications of Evolutionary Computation - 22nd\n  International Conference EvoApplications 2019 in the LNCS by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In NeuroEvolution, the topologies of artificial neural networks are optimized\nwith evolutionary algorithms to solve tasks in data regression, data\nclassification, or reinforcement learning. One downside of NeuroEvolution is\nthe large amount of necessary fitness evaluations, which might render it\ninefficient for tasks with expensive evaluations, such as real-time learning.\nFor these expensive optimization tasks, surrogate model-based optimization is\nfrequently applied as it features a good evaluation efficiency. While a\ncombination of both procedures appears as a valuable solution, the definition\nof adequate distance measures for the surrogate modeling process is difficult.\nIn this study, we will extend cartesian genetic programming of artificial\nneural networks by the use of surrogate model-based optimization. We propose\ndifferent distance measures and test our algorithm on a replicable benchmark\ntask. The results indicate that we can significantly increase the evaluation\nefficiency and that a phenotypic distance, which is based on the behavior of\nthe associated neural networks, is most promising.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 12:39:16 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Stork", "J\u00f6rg", ""], ["Zaefferer", "Martin", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "1902.03856", "submitter": "Abbas Siddiqui", "authors": "Abbas Siddiqui and Dionysios Georgiadis", "title": "Global Collaboration through Local Interaction in Competitive Learning", "comments": "The behavior via simulation can be viewed at:\n  https://www.youtube.com/watch?v=lTxlVHXGO2Q", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature maps, that preserve the global topology of arbitrary datasets, can be\nformed by self-organizing competing agents. So far, it has been presumed that\nglobal interaction of agents is necessary for this process. We establish that\nthis is not the case, and that global topology can be uncovered through\nstrictly local interactions. Enforcing uniformity of map quality across all\nagents, results in an algorithm that is able to consistently uncover the global\ntopology of diversely challenging datasets.The applicability and scalability of\nthis approach is further tested on a large point cloud dataset, revealing a\nlinear relation between map training time and size. The presented work not only\nreduces algorithmic complexity but also constitutes first step towards a\ndistributed self organizing map.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 13:15:43 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Siddiqui", "Abbas", ""], ["Georgiadis", "Dionysios", ""]]}, {"id": "1902.03871", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Jianwen Xie, Siyuan Huang, Yufan Ren, Song-Chun Zhu and\n  Ying Nian Wu", "title": "Learning vector representation of local content and matrix\n  representation of local motion, with implications for V1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a representational model for image pair such as\nconsecutive video frames that are related by local pixel displacements, in the\nhope that the model may shed light on motion perception in primary visual\ncortex (V1). The model couples the following two components. (1) The vector\nrepresentations of local contents of images. (2) The matrix representations of\nlocal pixel displacements caused by the relative motions between the agent and\nthe objects in the 3D scene. When the image frame undergoes changes due to\nlocal pixel displacements, the vectors are multiplied by the matrices that\nrepresent the local displacements. Our experiments show that our model can\nlearn to infer local motions. Moreover, the model can learn Gabor-like filter\npairs of quadrature phases.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:09:19 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 23:22:38 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 05:51:22 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 09:18:13 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gao", "Ruiqi", ""], ["Xie", "Jianwen", ""], ["Huang", "Siyuan", ""], ["Ren", "Yufan", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1902.03983", "submitter": "Fabricio Olivetti de Franca", "authors": "Fabricio Olivetti de Franca and Guilherme Seidyo Imai Aldeia", "title": "Interaction-Transformation Evolutionary Algorithm for Symbolic\n  Regression", "comments": "25 pages, 9 tables, 3 figures, submitted to Evolutionary Computation\n  Journal, September 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Interaction-Transformation (IT) is a new representation for Symbolic\nRegression that restricts the search space into simpler, but expressive,\nfunction forms. This representation has the advantage of creating a smoother\nsearch space unlike the space generated by Expression Trees, the common\nrepresentation used in Genetic Programming. This paper introduces an\nEvolutionary Algorithm capable of evolving a population of IT expressions\nsupported only by the mutation operator. The results show that this\nrepresentation is capable of finding better approximations to real-world data\nsets when compared to traditional approaches and a state-of-the-art Genetic\nProgramming algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 16:43:32 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 16:45:20 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 17:33:29 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 14:04:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["de Franca", "Fabricio Olivetti", ""], ["Aldeia", "Guilherme Seidyo Imai", ""]]}, {"id": "1902.04106", "submitter": "Michael Hauser", "authors": "Michael Hauser", "title": "On Residual Networks Learning a Perturbation from Identity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to test and study the hypothesis that residual\nnetworks are learning a perturbation from identity. Residual networks are\nenormously important deep learning models, with many theories attempting to\nexplain how they function; learning a perturbation from identity is one such\ntheory. In order to answer this question, the magnitudes of the perturbations\nare measured in both an absolute sense as well as in a scaled sense, with each\nform having its relative benefits and drawbacks. Additionally, a stopping rule\nis developed that can be used to decide the depth of the residual network based\non the average perturbation magnitude being less than a given epsilon. With\nthis analysis a better understanding of how residual networks process and\ntransform data from input to output is formed. Parallel experiments are\nconducted on MNIST as well as CIFAR10 for various sized residual networks with\nbetween 6 and 300 residual blocks. It is found that, in this setting, the\naverage scaled perturbation magnitude is roughly inversely proportional to\nincreasing the number of residual blocks, and from this it follows that for\nsufficiently large residual networks, they are learning a perturbation from\nidentity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 19:34:43 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Hauser", "Michael", ""]]}, {"id": "1902.04118", "submitter": "Sean Sedwards", "authors": "Jaeyoung Lee, Aravind Balakrishnan, Ashish Gaurav, Krzysztof\n  Czarnecki, Sean Sedwards", "title": "WiseMove: A Framework for Safe Deep Reinforcement Learning for\n  Autonomous Driving", "comments": null, "journal-ref": "International Conference on Quantitative Evaluation of Systems\n  (QEST 2019)", "doi": "10.1007/978-3-030-30281-8_20", "report-no": null, "categories": "cs.LG cs.NE cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning can provide efficient solutions to the complex problems\nencountered in autonomous driving, but ensuring their safety remains a\nchallenge. A number of authors have attempted to address this issue, but there\nare few publicly-available tools to adequately explore the trade-offs between\nfunctionality, scalability, and safety.\n  We thus present WiseMove, a software framework to investigate safe deep\nreinforcement learning in the context of motion planning for autonomous\ndriving. WiseMove adopts a modular learning architecture that suits our current\nresearch questions and can be adapted to new technologies and new questions. We\npresent the details of WiseMove, demonstrate its use on a common traffic\nscenario, and describe how we use it in our ongoing safe learning research.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 19:59:23 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lee", "Jaeyoung", ""], ["Balakrishnan", "Aravind", ""], ["Gaurav", "Ashish", ""], ["Czarnecki", "Krzysztof", ""], ["Sedwards", "Sean", ""]]}, {"id": "1902.04346", "submitter": "Kai Olav Ellefsen", "authors": "Kai Olav Ellefsen, Joost Huizinga and Jim Torresen", "title": "Guiding Neuroevolution with Structural Objectives", "comments": null, "journal-ref": null, "doi": "10.1162/evco_a_00250", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure and performance of neural networks are intimately connected,\nand by use of evolutionary algorithms, neural network structures optimally\nadapted to a given task can be explored. Guiding such neuroevolution with\nadditional objectives related to network structure has been shown to improve\nperformance in some cases, especially when modular neural networks are\nbeneficial. However, apart from objectives aiming to make networks more\nmodular, such structural objectives have not been widely explored. We propose\ntwo new structural objectives and test their ability to guide evolving neural\nnetworks on two problems which can benefit from decomposition into subtasks.\nThe first structural objective guides evolution to align neural networks with a\nuser-recommended decomposition pattern. Intuitively, this should be a powerful\nguiding target for problems where human users can easily identify a structure.\nThe second structural objective guides evolution towards a population with a\nhigh diversity in decomposition patterns. This results in exploration of many\ndifferent ways to decompose a problem, allowing evolution to find good\ndecompositions faster. Tests on our target problems reveal that both methods\nperform well on a problem with a very clear and decomposable structure.\nHowever, on a problem where the optimal decomposition is less obvious, the\nstructural diversity objective is found to outcompete other structural\nobjectives -- and this technique can even increase performance on problems\nwithout any decomposable structure at all.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 11:54:23 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 09:31:24 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 06:52:22 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Ellefsen", "Kai Olav", ""], ["Huizinga", "Joost", ""], ["Torresen", "Jim", ""]]}, {"id": "1902.04546", "submitter": "Harris Chan", "authors": "Harris Chan, Yuhuai Wu, Jamie Kiros, Sanja Fidler, Jimmy Ba", "title": "ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse reward is one of the most challenging problems in reinforcement\nlearning (RL). Hindsight Experience Replay (HER) attempts to address this issue\nby converting a failed experience to a successful one by relabeling the goals.\nDespite its effectiveness, HER has limited applicability because it lacks a\ncompact and universal goal representation. We present Augmenting experienCe via\nTeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that\nextends the HER framework using natural language as the goal representation. We\nfirst analyze the differences among goal representation, and show that ACTRCE\ncan efficiently solve difficult reinforcement learning problems in challenging\n3D navigation tasks, whereas HER with non-language goal representation failed\nto learn. We also show that with language goal representations, the agent can\ngeneralize to unseen instructions, and even generalize to instructions with\nunseen lexicons. We further demonstrate it is crucial to use hindsight advice\nto solve challenging tasks, and even small amount of advice is sufficient for\nthe agent to achieve good performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 18:43:56 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Chan", "Harris", ""], ["Wu", "Yuhuai", ""], ["Kiros", "Jamie", ""], ["Fidler", "Sanja", ""], ["Ba", "Jimmy", ""]]}, {"id": "1902.04615", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling", "title": "Gauge Equivariant Convolutional Networks and the Icosahedral CNN", "comments": "Proceedings of the International Conference on Machine Learning\n  (ICML), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of equivariance to symmetry transformations enables a\ntheoretically grounded approach to neural network architecture design.\nEquivariant networks have shown excellent performance and data efficiency on\nvision and medical imaging problems that exhibit symmetries. Here we show how\nthis principle can be extended beyond global symmetries to local gauge\ntransformations. This enables the development of a very general class of\nconvolutional neural networks on manifolds that depend only on the intrinsic\ngeometry, and which includes many popular methods from equivariant and\ngeometric deep learning. We implement gauge equivariant CNNs for signals\ndefined on the surface of the icosahedron, which provides a reasonable\napproximation of the sphere. By choosing to work with this very regular\nmanifold, we are able to implement the gauge equivariant convolution using a\nsingle conv2d call, making it a highly scalable and practical alternative to\nSpherical CNNs. Using this method, we demonstrate substantial improvements over\nprevious methods on the task of segmenting omnidirectional images and global\nclimate patterns.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 17:01:05 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 09:50:05 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 23:03:52 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Cohen", "Taco S.", ""], ["Weiler", "Maurice", ""], ["Kicanaoglu", "Berkay", ""], ["Welling", "Max", ""]]}, {"id": "1902.04692", "submitter": "Vahid Roostapour", "authors": "Vahid Roostapour, Mojgan Pourhassan, and Frank Neumann", "title": "Analysis of Baseline Evolutionary Algorithms for the Packing While\n  Travelling Problem", "comments": "This paper has been accepted in FOGA 2019", "journal-ref": null, "doi": "10.1145/3299904.3340313", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of base-line Evolutionary Algorithms (EAs) on combinatorial\nproblems has been studied rigorously. From the theoretical viewpoint, the\nliterature extensively investigates the linear problems, while the theoretical\nanalysis of the non-linear problems is still far behind. In this paper,\nvariations of the Packing While Travelling (PWT) -- also known as the\nnon-linear knapsack problem -- are studied as an attempt to analyse the\nbehaviour of EAs on non-linear problems from theoretical perspective. We\ninvestigate PWT for two cities and $n$ items with correlated weights and\nprofits, using single-objective and multi-objective algorithms. Our results\nshow that RLS\\_swap, which differs from the classical RLS by having the ability\nto swap two bits in one iteration, finds the optimal solution in $O(n^3)$\nexpected time. We also study an enhanced version of GSEMO, which a specific\nselection operator to deal with exponential population size, and prove that it\nfinds the Pareto front in the same asymptotic expected time. In the case of\nuniform weights, (1+1)~EA is able to find the optimal solution in expected time\n$O(n^2\\log{(\\max\\{n,p_{\\max}\\})})$, where $p_{\\max}$ is the largest profit of\nthe given items. We also perform an experimental analysis to complement our\ntheoretical investigations and provide additional insights into the runtime\nbehavior.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 01:05:20 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 05:41:22 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Roostapour", "Vahid", ""], ["Pourhassan", "Mojgan", ""], ["Neumann", "Frank", ""]]}, {"id": "1902.04704", "submitter": "Tal Golan", "authors": "Nikolaus Kriegeskorte and Tal Golan", "title": "Neural network models and deep learning - a primer for biologists", "comments": "14 pages, 4 figures; added references, minor corrections", "journal-ref": "Current Biology 29(7) (2019) R231-R236", "doi": "10.1016/j.cub.2019.02.034", "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originally inspired by neurobiology, deep neural network models have become a\npowerful tool of machine learning and artificial intelligence, where they are\nused to approximate functions and dynamics by learning from examples. Here we\ngive a brief introduction to neural network models and deep learning for\nbiologists. We introduce feedforward and recurrent networks and explain the\nexpressive power of this modeling framework and the backpropagation algorithm\nfor setting the parameters. Finally, we consider how deep neural networks might\nhelp us understand the brain's computations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 02:09:26 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 00:19:24 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kriegeskorte", "Nikolaus", ""], ["Golan", "Tal", ""]]}, {"id": "1902.04724", "submitter": "Markus Wagner", "authors": "Domagoj Jakobovic, Stjepan Picek, Marcella S. R. Martins, Markus\n  Wagner", "title": "A characterisation of S-box fitness landscapes in cryptography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substitution Boxes (S-boxes) are nonlinear objects often used in the design\nof cryptographic algorithms. The design of high quality S-boxes is an\ninteresting problem that attracts a lot of attention. Many attempts have been\nmade in recent years to use heuristics to design S-boxes, but the results were\noften far from the previously known best obtained ones. Unfortunately, most of\nthe effort went into exploring different algorithms and fitness functions while\nlittle attention has been given to the understanding why this problem is so\ndifficult for heuristics. In this paper, we conduct a fitness landscape\nanalysis to better understand why this problem can be difficult. Among other,\nwe find that almost each initial starting point has its own local optimum, even\nthough the networks are highly interconnected.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 03:18:14 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Jakobovic", "Domagoj", ""], ["Picek", "Stjepan", ""], ["Martins", "Marcella S. R.", ""], ["Wagner", "Markus", ""]]}, {"id": "1902.04760", "submitter": "Greg Yang", "authors": "Greg Yang", "title": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian\n  Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation", "comments": "tldr: A theoretical tool for understanding the behavior of large\n  width randomly initialized neural networks for almost all deep learning\n  architectures. For a gentler introduction to Gaussian Process results here\n  and several extensions, we recommend the reader to look at arXiv:1910.12478", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.LG math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent trends in machine learning theory and practice, from the\ndesign of state-of-the-art Gaussian Process to the convergence analysis of deep\nneural nets (DNNs) under stochastic gradient descent (SGD), have found it\nfruitful to study wide random neural networks. Central to these approaches are\ncertain scaling limits of such networks. We unify these results by introducing\na notion of a straightline \\emph{tensor program} that can express most neural\nnetwork computations, and we characterize its scaling limit when its tensors\nare large and randomized. From our framework follows (1) the convergence of\nrandom neural networks to Gaussian processes for architectures such as\nrecurrent neural networks, convolutional neural networks, residual networks,\nattention, and any combination thereof, with or without batch normalization;\n(2) conditions under which the \\emph{gradient independence assumption} -- that\nweights in backpropagation can be assumed to be independent from weights in the\nforward pass -- leads to correct computation of gradient dynamics, and\ncorrections when it does not; (3) the convergence of the Neural Tangent Kernel,\na recently proposed kernel used to predict training dynamics of neural networks\nunder gradient descent, at initialization for all architectures in (1) without\nbatch normalization. Mathematically, our framework is general enough to\nrederive classical random matrix results such as the semicircle and the\nMarchenko-Pastur laws, as well as recent results in neural network Jacobian\nsingular values. We hope our work opens a way toward design of even stronger\nGaussian Processes, initialization schemes to avoid gradient\nexplosion/vanishing, and deeper understanding of SGD dynamics in modern\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 06:09:18 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 23:02:40 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 22:53:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yang", "Greg", ""]]}, {"id": "1902.04767", "submitter": "Yue Xie", "authors": "Yue Xie, Oscar Harper, Hirad Assimi, Aneta Neumann and Frank Neumann", "title": "Evolutionary Algorithms for the Chance-Constrained Knapsack Problem", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms have been widely used for a range of stochastic\noptimization problems. In most studies, the goal is to optimize the expected\nquality of the solution. Motivated by real-world problems where constraint\nviolations have extremely disruptive effects, we consider a variant of the\nknapsack problem where the profit is maximized under the constraint that the\nknapsack capacity bound is violated with a small probability of at most\n$\\alpha$. This problem is known as chance-constrained knapsack problem and\nchance-constrained optimization problems have so far gained little attention in\nthe evolutionary computation literature. We show how to use popular deviation\ninequalities such as Chebyshev's inequality and Chernoff bounds as part of the\nsolution evaluation when tackling these problems by evolutionary algorithms and\ncompare the effectiveness of our algorithms on a wide range of\nchance-constrained knapsack instances.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 06:48:06 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 23:27:28 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Xie", "Yue", ""], ["Harper", "Oscar", ""], ["Assimi", "Hirad", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""]]}, {"id": "1902.05167", "submitter": "Makoto Itoh", "authors": "Makoto Itoh", "title": "Some Interesting Features of Memristor CNN", "comments": "60 pages, 50 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce some interesting features of a memristor CNN\n(Cellular Neural Network). We first show that there is the similarity between\nthe dynamics of memristors and neurons. That is, some kind of flux-controlled\nmemristors can not respond to the sinusoidal voltage source quickly, namely,\nthey can not switch `on' rapidly. Furthermore, these memristors have refractory\nperiod after switch `on', which means that it can not respond to further\nsinusoidal inputs until the flux is decreased. We next show that the\nmemristor-coupled two-cell CNN can exhibit chaotic behavior. In this system,\nthe memristors switch `off' and `on' at irregular intervals, and the two cells\nare connected when either or both of the memristors switches `on'. We then\npropose the modified CNN model, which can hold a binary output image, even if\nall cells are disconnected and no signal is supplied to the cell after a\ncertain point of time. However, the modified CNN requires power to maintain the\noutput image, that is, it is volatile. We next propose a new memristor CNN\nmodel. It can also hold a binary output state (image), even if all cells are\ndisconnected, and no signal is supplied to the cell, by memristor's switching\nbehavior. Furthermore, even if we turn off the power of the system during the\ncomputation, it can resume from the previous average output state, since the\nmemristor CNN has functions of both short-term (volatile) memory and long-term\n(non-volatile) memory. The above suspend and resume feature are useful when we\nwant to save the current state, and continue work later from the previous\nstate. Finally, we show that the memristor CNN can exhibit interesting\ntwo-dimensional waves, if an inductor is connected to each memristor CNN cell.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 00:00:28 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Itoh", "Makoto", ""]]}, {"id": "1902.05446", "submitter": "Jorge Davila-Chacon", "authors": "Jorge, Davila-Chacon and Jindong, Liu and Stefan, Wermter", "title": "Enhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source\n  Localization", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (Volume:\n  30, Issue: 1, Jan. 2019)", "doi": "10.1109/TNNLS.2018.2830119", "report-no": null, "categories": "cs.SD cs.HC cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the behavior of humans talking in noisy environments, we propose\nan embodied embedded cognition approach to improve automatic speech recognition\n(ASR) systems for robots in challenging environments, such as with ego noise,\nusing binaural sound source localization (SSL). The approach is verified by\nmeasuring the impact of SSL with a humanoid robot head on the performance of an\nASR system. More specifically, a robot orients itself toward the angle where\nthe signal-to-noise ratio (SNR) of speech is maximized for one microphone\nbefore doing an ASR task. First, a spiking neural network inspired by the\nmidbrain auditory system based on our previous work is applied to calculate the\nsound signal angle. Then, a feedforward neural network is used to handle high\nlevels of ego noise and reverberation in the signal. Finally, the sound signal\nis fed into an ASR system. For ASR, we use a system developed by our group and\ncompare its performance with and without the support from SSL. We test our SSL\nand ASR systems on two humanoid platforms with different structural and\nmaterial properties. With our approach we halve the sentence error rate with\nrespect to the common downmixing of both channels. Surprisingly, the ASR\nperformance is more than two times better when the angle between the humanoid\nhead and the sound source allows sound waves to be reflected most intensely\nfrom the pinna to the ear microphone, rather than when sound waves arrive\nperpendicularly to the membrane.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:09:11 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Jorge", "", ""], ["Davila-Chacon", "", ""], ["Jindong", "", ""], ["Liu", "", ""], ["Stefan", "", ""], ["Wermter", "", ""]]}, {"id": "1902.05485", "submitter": "Tanja Katharina Kaiser", "authors": "Tanja Katharina Kaiser and Heiko Hamann", "title": "Engineered Self-Organization for Resilient Robot Self-Assembly with\n  Minimal Surprise", "comments": null, "journal-ref": null, "doi": "10.1016/j.robot.2019.103293", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In collective robotic systems, the automatic generation of controllers for\ncomplex tasks is still a challenging problem. Open-ended evolution of complex\nrobot behaviors can be a possible solution whereby an intrinsic driver for\npattern formation and self-organization may prove to be important. We implement\nsuch a driver in collective robot systems by evolving prediction networks as\nworld models in pair with action-selection networks. Fitness is given for good\npredictions which causes a bias towards easily predictable environments and\nbehaviors in the form of emergent patterns, that is, environments of minimal\nsurprise. There is no task-dependent bias or any other explicit\npredetermination for the different qualities of the emerging patterns. A\ncareful configuration of actions, sensor models, and the environment is\nrequired to stimulate the emergence of complex behaviors. We study\nself-assembly to increase the scenario's complexity for our minimal surprise\napproach and, at the same time, limit the complexity of our simulations to a\ngrid world to manage the feasibility of this approach. We investigate the\nimpact of different swarm densities and the shape of the environment on the\nemergent patterns. Furthermore, we study how evolution can be biased towards\nthe emergence of desired patterns. We analyze the resilience of the resulting\nself-assembly behaviors by causing damages to the assembled pattern and observe\nthe self-organized reassembly of the structure. In summary, we evolved swarm\nbehaviors for resilient self-assembly and successfully engineered\nself-organization in simulation. In future work, we plan to transfer our\napproach to a swarm of real robots.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 16:40:21 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 08:59:55 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 12:17:56 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kaiser", "Tanja Katharina", ""], ["Hamann", "Heiko", ""]]}, {"id": "1902.05522", "submitter": "Brian Cheung", "authors": "Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, Bruno\n  Olshausen", "title": "Superposition of many models into one", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for storing multiple models within a single set of\nparameters. Models can coexist in superposition and still be retrieved\nindividually. In experiments with neural networks, we show that a surprisingly\nlarge number of models can be effectively stored within a single parameter\ninstance. Furthermore, each of these models can undergo thousands of training\nsteps without significantly interfering with other models within the\nsuperposition. This approach may be viewed as the online complement of\ncompression: rather than reducing the size of a network after training, we make\nuse of the unrealized capacity of a network during training.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 17:59:13 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 17:58:36 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Cheung", "Brian", ""], ["Terekhov", "Alex", ""], ["Chen", "Yubei", ""], ["Agrawal", "Pulkit", ""], ["Olshausen", "Bruno", ""]]}, {"id": "1902.05546", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, Alexei A.\n  Efros", "title": "Learning to Control Self-Assembling Morphologies: A Study of\n  Generalization via Modularity", "comments": "NeurIPS 2019 (Spotlight). Videos at\n  https://pathak22.github.io/modular-assemblies/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary sensorimotor learning approaches typically start with an\nexisting complex agent (e.g., a robotic arm), which they learn to control. In\ncontrast, this paper investigates a modular co-evolution strategy: a collection\nof primitive agents learns to dynamically self-assemble into composite bodies\nwhile also learning to coordinate their behavior to control these bodies. Each\nprimitive agent consists of a limb with a motor attached at one end. Limbs may\nchoose to link up to form collectives. When a limb initiates a link-up action,\nand there is another limb nearby, the latter is magnetically connected to the\n'parent' limb's motor. This forms a new single agent, which may further link\nwith other agents. In this way, complex morphologies can emerge, controlled by\na policy whose architecture is in explicit correspondence with the morphology.\nWe evaluate the performance of these dynamic and modular agents in simulated\nenvironments. We demonstrate better generalization to test-time changes both in\nthe environment, as well as in the structure of the agent, compared to static\nand monolithic baselines. Project video and code are available at\nhttps://pathak22.github.io/modular-assemblies/\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 18:59:05 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 21:35:27 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Pathak", "Deepak", ""], ["Lu", "Chris", ""], ["Darrell", "Trevor", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1902.05705", "submitter": "Jibin Wu", "authors": "Jibin Wu, Yansong Chua, Malu Zhang, Qu Yang, Guoqi Li, Haizhou Li", "title": "Deep Spiking Neural Network with Spike Count based Learning Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep spiking neural networks (SNNs) support asynchronous event-driven\ncomputation, massive parallelism and demonstrate great potential to improve the\nenergy efficiency of its synchronous analog counterpart. However, insufficient\nattention has been paid to neural encoding when designing SNN learning rules.\nRemarkably, the temporal credit assignment has been performed on rate-coded\nspiking inputs, leading to poor learning efficiency. In this paper, we\nintroduce a novel spike-based learning rule for rate-coded deep SNNs, whereby\nthe spike count of each neuron is used as a surrogate for gradient\nbackpropagation. We evaluate the proposed learning rule by training deep\nspiking multi-layer perceptron (MLP) and spiking convolutional neural network\n(CNN) on the UCI machine learning and MNIST handwritten digit datasets. We show\nthat the proposed learning rule achieves state-of-the-art accuracies on all\nbenchmark datasets. The proposed learning rule allows introducing latency,\nspike rate and hardware constraints into the SNN learning, which is superior to\nthe indirect approach in which conventional artificial neural networks are\nfirst trained and then converted to SNNs. Hence, it allows direct deployment to\nthe neuromorphic hardware and supports efficient inference. Notably, a test\naccuracy of 98.40% was achieved on the MNIST dataset in our experiments with\nonly 10 simulation time steps, when the same latency constraint is imposed\nduring training.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 06:52:52 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Wu", "Jibin", ""], ["Chua", "Yansong", ""], ["Zhang", "Malu", ""], ["Yang", "Qu", ""], ["Li", "Guoqi", ""], ["Li", "Haizhou", ""]]}, {"id": "1902.06050", "submitter": "Khuong Vo", "authors": "Khuong Vo, Tri Nguyen, Dang Pham, Mao Nguyen, Minh Truong, Trung Mai,\n  Tho Quan", "title": "Combination of Domain Knowledge and Deep Learning for Sentiment Analysis\n  of Short and Informal Messages on Social Media", "comments": "A Preprint of an article accepted for publication by Inderscience in\n  IJCVR on September 2018", "journal-ref": "International Journal of Computational Vision and Robotics, 2019\n  Vol.9 No.5, pp.458 - 485", "doi": "10.1504/IJCVR.2019.102286", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis has been emerging recently as one of the major natural\nlanguage processing (NLP) tasks in many applications. Especially, as social\nmedia channels (e.g. social networks or forums) have become significant sources\nfor brands to observe user opinions about their products, this task is thus\nincreasingly crucial. However, when applied with real data obtained from social\nmedia, we notice that there is a high volume of short and informal messages\nposted by users on those channels. This kind of data makes the existing works\nsuffer from many difficulties to handle, especially ones using deep learning\napproaches. In this paper, we propose an approach to handle this problem. This\nwork is extended from our previous work, in which we proposed to combine the\ntypical deep learning technique of Convolutional Neural Networks with domain\nknowledge. The combination is used for acquiring additional training data\naugmentation and a more reasonable loss function. In this work, we further\nimprove our architecture by various substantial enhancements, including\nnegation-based data augmentation, transfer learning for word embeddings, the\ncombination of word-level embeddings and character-level embeddings, and using\nmultitask learning technique for attaching domain knowledge rules in the\nlearning process. Those enhancements, specifically aiming to handle short and\ninformal messages, help us to enjoy significant improvement in performance once\nexperimenting on real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 06:03:57 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 07:53:04 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Vo", "Khuong", ""], ["Nguyen", "Tri", ""], ["Pham", "Dang", ""], ["Nguyen", "Mao", ""], ["Truong", "Minh", ""], ["Mai", "Trung", ""], ["Quan", "Tho", ""]]}, {"id": "1902.06094", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva and Juan-Pablo Ortega", "title": "Differentiable reservoir computing", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much effort has been devoted in the last two decades to characterize the\nsituations in which a reservoir computing system exhibits the so-called echo\nstate (ESP) and fading memory (FMP) properties. These important features\namount, in mathematical terms, to the existence and continuity of global\nreservoir system solutions. That research is complemented in this paper with\nthe characterization of the differentiability of reservoir filters for very\ngeneral classes of discrete-time deterministic inputs. This constitutes a novel\nstrong contribution to the long line of research on the ESP and the FMP and, in\nparticular, links to existing research on the input-dependence of the ESP.\nDifferentiability has been shown in the literature to be a key feature in the\nlearning of attractors of chaotic dynamical systems. A Volterra-type series\nrepresentation for reservoir filters with semi-infinite discrete-time inputs is\nconstructed in the analytic case using Taylor's theorem and corresponding\napproximation bounds are provided. Finally, it is shown as a corollary of these\nresults that any fading memory filter can be uniformly approximated by a finite\nVolterra series with finite memory.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 11:47:04 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 12:14:48 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1902.06361", "submitter": "Baihong Jin", "authors": "Baihong Jin, Yuxin Chen, Dan Li, Kameshwar Poolla, Alberto\n  Sangiovanni-Vincentelli", "title": "A One-Class Support Vector Machine Calibration Method for Time Series\n  Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to identify the change point of a system's health status,\nwhich usually signifies an incipient fault under development. The One-Class\nSupport Vector Machine (OC-SVM) is a popular machine learning model for anomaly\ndetection and hence could be used for identifying change points; however, it is\nsometimes difficult to obtain a good OC-SVM model that can be used on sensor\nmeasurement time series to identify the change points in system health status.\nIn this paper, we propose a novel approach for calibrating OC-SVM models. The\napproach uses a heuristic search method to find a good set of input data and\nhyperparameters that yield a well-performing model. Our results on the C-MAPSS\ndataset demonstrate that OC-SVM can also achieve satisfactory accuracy in\ndetecting change point in time series with fewer training data, compared to\nstate-of-the-art deep learning approaches. In our case study, the OC-SVM\ncalibrated by the proposed model is shown to be useful especially in scenarios\nwith limited amount of training data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 00:34:58 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Jin", "Baihong", ""], ["Chen", "Yuxin", ""], ["Li", "Dan", ""], ["Poolla", "Kameshwar", ""], ["Sangiovanni-Vincentelli", "Alberto", ""]]}, {"id": "1902.06410", "submitter": "Lana Sinapayen", "authors": "Lana Sinapayen, Atsushi Masumori, Ikegami Takashi", "title": "Reactive, Proactive, and Inductive Agents: An evolutionary path for\n  biological and artificial spiking networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex environments provide structured yet variable sensory inputs. To best\nexploit information from these environments, organisms must evolve the ability\nto anticipate consequences of unknown stimuli, and act on these predictions. We\npropose an evolutionary path for neural networks, leading an organism from\nreactive behavior to simple proactive behavior and from simple proactive\nbehavior to induction-based behavior. Through in-vitro and in-silico\nexperiments, we define the conditions necessary in a network with spike-timing\ndependent plasticity for the organism to go from reactive to proactive\nbehavior. Our results support the existence of specific evolutionary steps and\nfour conditions necessary for embodied neural networks to evolve predictive and\ninductive abilities from an initial reactive strategy. We extend these\nconditions to more general structures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 05:38:39 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 06:58:49 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Sinapayen", "Lana", ""], ["Masumori", "Atsushi", ""], ["Takashi", "Ikegami", ""]]}, {"id": "1902.06468", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Minsoo Rhu", "title": "Beyond the Memory Wall: A Case for Memory-centric HPC System for Deep\n  Learning", "comments": "Published as a conference paper at the 51st IEEE/ACM International\n  Symposium on Microarchitecture (MICRO-51), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the models and the datasets to train deep learning (DL) models scale,\nsystem architects are faced with new challenges, one of which is the memory\ncapacity bottleneck, where the limited physical memory inside the accelerator\ndevice constrains the algorithm that can be studied. We propose a\nmemory-centric deep learning system that can transparently expand the memory\ncapacity available to the accelerators while also providing fast inter-device\ncommunication for parallel training. Our proposal aggregates a pool of memory\nmodules locally within the device-side interconnect, which are decoupled from\nthe host interface and function as a vehicle for transparent memory capacity\nexpansion. Compared to conventional systems, our proposal achieves an average\n2.8x speedup on eight DL applications and increases the system-wide memory\ncapacity to tens of TBs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 09:07:07 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kwon", "Youngeun", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1902.06553", "submitter": "Benedetta Franceschiello Dr.", "authors": "Fabio Anselmi, Micah M. Murray and Benedetta Franceschiello", "title": "A computational model for grid maps in neural populations", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid cells in the entorhinal cortex, together with head direction, place,\nspeed and border cells, are major contributors to the organization of spatial\nrepresentations in the brain. In this work we introduce a novel theoretical and\nalgorithmic framework able to explain the emergence of hexagonal grid-like\nresponse patterns from head direction cells' responses. We show that this\npattern is a result of minimal variance encoding of neurons. The novelty lies\ninto the formulation of the encoding problem through the modern Frame Theory\nlanguage, specifically that of equiangular Frames, providing new insights about\nthe optimality of hexagonal grid receptive fields. The model proposed overcomes\nsome crucial limitations of the current attractor and oscillatory models. It is\nbased on the well-accepted and tested hypothesis of Hebbian learning, providing\na simplified cortical-based framework that does not require the presence of\ntheta velocity-driven oscillations (oscillatory model) or translational\nsymmetries in the synaptic connections (attractor model). We moreover\ndemonstrate that the proposed encoding mechanism naturally explains axis\nalignment of neighbor grid cells and maps shifts, rotations and scaling of the\nstimuli onto the shape of grid cells' receptive fields, giving a\nstraightforward explanation of the experimental evidence of grid cells\nremapping under transformations of environmental cues.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 13:02:04 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 07:56:22 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 08:12:59 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Anselmi", "Fabio", ""], ["Murray", "Micah M.", ""], ["Franceschiello", "Benedetta", ""]]}, {"id": "1902.06703", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas and Junichi Murata", "title": "Spectrum-Diverse Neuroevolution with Unified Neural Models", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, Volume\n  28, Issue 8, 1759-1773, 2017", "doi": "10.1109/TNNLS.2016.2551748", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms are being increasingly adopted in various applications.\nHowever, further expansion will require methods that work more automatically.\nTo enable this level of automation, a more powerful solution representation is\nneeded. However, by increasing the representation complexity a second problem\narises. The search space becomes huge and therefore an associated scalable and\nefficient searching algorithm is also required. To solve both problems, first a\npowerful representation is proposed that unifies most of the neural networks\nfeatures from the literature into one representation. Secondly, a new diversity\npreserving method called Spectrum Diversity is created based on the new concept\nof chromosome spectrum that creates a spectrum out of the characteristics and\nfrequency of alleles in a chromosome. The combination of Spectrum Diversity\nwith a unified neuron representation enables the algorithm to either surpass or\nequal NeuroEvolution of Augmenting Topologies (NEAT) on all of the five classes\nof problems tested. Ablation tests justifies the good results, showing the\nimportance of added new features in the unified neuron representation. Part of\nthe success is attributed to the novelty-focused evolution and good scalability\nwith chromosome size provided by Spectrum Diversity. Thus, this study sheds\nlight on a new representation and diversity preserving mechanism that should\nimpact algorithms and applications to come.\n  To download the code please access the following\nhttps://github.com/zweifel/Physis-Shard.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 09:58:41 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Murata", "Junichi", ""]]}, {"id": "1902.06704", "submitter": "Sarath Chandar", "authors": "Sarath Chandar, Chinnadhurai Sankar, Eugene Vorontsov, Samira Ebrahimi\n  Kahou, Yoshua Bengio", "title": "Towards Non-saturating Recurrent Units for Modelling Long-term\n  Dependencies", "comments": "In Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling long-term dependencies is a challenge for recurrent neural\nnetworks. This is primarily due to the fact that gradients vanish during\ntraining, as the sequence length increases. Gradients can be attenuated by\ntransition operators and are attenuated or dropped by activation functions.\nCanonical architectures like LSTM alleviate this issue by skipping information\nthrough a memory mechanism. We propose a new recurrent architecture\n(Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but\nforgoes both saturating activation functions and saturating gates, in order to\nfurther alleviate vanishing gradients. In a series of synthetic and real world\ntasks, we demonstrate that the proposed model is the only model that performs\namong the top 2 models across all tasks with and without long-term\ndependencies, when compared against a range of other architectures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 15:24:27 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Chandar", "Sarath", ""], ["Sankar", "Chinnadhurai", ""], ["Vorontsov", "Eugene", ""], ["Kahou", "Samira Ebrahimi", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1902.06778", "submitter": "Zhicheng Ding", "authors": "Zhicheng Ding, Mehmet Kerem Turkcan, Albert Boulanger", "title": "Using an Ancillary Neural Network to Capture Weekends and Holidays in an\n  Adjoint Neural Network Architecture for Intelligent Building Management", "comments": "9 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The US EIA estimated in 2017 about 39\\% of total U.S. energy consumption was\nby the residential and commercial sectors. Therefore, Intelligent Building\nManagement (IBM) solutions that minimize consumption while maintaining tenant\ncomfort are an important component in addressing climate change. A forecasting\ncapability for accurate prediction of indoor temperatures in a planning horizon\nof 24 hours is essential to IBM. It should predict the indoor temperature in\nboth short-term (e.g. 15 minutes) and long-term (e.g. 24 hours) periods\naccurately including weekends, major holidays, and minor holidays. Other\nrequirements include the ability to predict the maximum and the minimum indoor\ntemperatures precisely and provide the confidence for each prediction. To\nachieve these requirements, we propose a novel adjoint neural network\narchitecture for time series prediction that uses an ancillary neural network\nto capture weekend and holiday information. We studied four long short-term\nmemory (LSTM) based time series prediction networks within this architecture.\nWe observed that the ancillary neural network helps to improve the prediction\naccuracy, the maximum and the minimum temperature prediction and model\nreliability for all networks tested.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 18:38:28 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Ding", "Zhicheng", ""], ["Turkcan", "Mehmet Kerem", ""], ["Boulanger", "Albert", ""]]}, {"id": "1902.06827", "submitter": "Jason Liang", "authors": "Jason Liang, Elliot Meyerson, Babak Hodjat, Dan Fink, Karl Mutch, and\n  Risto Miikkulainen", "title": "Evolutionary Neural AutoML for Deep Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have produced state-of-the-art results in many\nbenchmarks and problem domains. However, the success of DNNs depends on the\nproper configuration of its architecture and hyperparameters. Such a\nconfiguration is difficult and as a result, DNNs are often not used to their\nfull potential. In addition, DNNs in commercial applications often need to\nsatisfy real-world design constraints such as size or number of parameters. To\nmake configuration easier, automatic machine learning (AutoML) systems for deep\nlearning have been developed, focusing mostly on optimization of\nhyperparameters.\n  This paper takes AutoML a step further. It introduces an evolutionary AutoML\nframework called LEAF that not only optimizes hyperparameters but also network\narchitectures and the size of the network. LEAF makes use of both\nstate-of-the-art evolutionary algorithms (EAs) and distributed computing\nframeworks. Experimental results on medical image classification and natural\nlanguage analysis show that the framework can be used to achieve\nstate-of-the-art performance. In particular, LEAF demonstrates that\narchitecture optimization provides a significant boost over hyperparameter\noptimization, and that networks can be minimized at the same time with little\ndrop in performance. LEAF therefore forms a foundation for democratizing and\nimproving AI, as well as making AI practical in future applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 22:44:39 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 20:47:28 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 00:27:54 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Liang", "Jason", ""], ["Meyerson", "Elliot", ""], ["Hodjat", "Babak", ""], ["Fink", "Dan", ""], ["Mutch", "Karl", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1902.08001", "submitter": "Michael Lones", "authors": "Michael Adam Lones", "title": "Mitigating Metaphors: A Comprehensible Guide to Recent Nature-Inspired\n  Algorithms", "comments": null, "journal-ref": "SN Computer Science (2020) 1:49", "doi": "10.1007/s42979-019-0050-8", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a plethora of new metaheuristic algorithms have explored\ndifferent sources of inspiration within the biological and natural worlds. This\nnature-inspired approach to algorithm design has been widely criticised. A\nnotable issue is the tendency for authors to use terminology that is derived\nfrom the domain of inspiration, rather than the broader domains of\nmetaheuristics and optimisation. This makes it difficult to both comprehend how\nthese algorithms work and understand their relationships to other\nmetaheuristics. This paper attempts to address this issue, at least to some\nextent, by providing accessible descriptions of the most cited nature-inspired\nalgorithms published in the last twenty years. It also discusses commonalities\nbetween these algorithms and more classical nature-inspired metaheuristics such\nas evolutionary algorithms and particle swarm optimisation, and finishes with a\ndiscussion of future directions for the field.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 12:26:40 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 16:18:12 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Lones", "Michael Adam", ""]]}, {"id": "1902.08129", "submitter": "Greg Yang", "authors": "Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and\n  Samuel S. Schoenholz", "title": "A Mean Field Theory of Batch Normalization", "comments": "To appear in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mean field theory for batch normalization in fully-connected\nfeedforward neural networks. In so doing, we provide a precise characterization\nof signal propagation and gradient backpropagation in wide batch-normalized\nnetworks at initialization. Our theory shows that gradient signals grow\nexponentially in depth and that these exploding gradients cannot be eliminated\nby tuning the initial weight variances or by adjusting the nonlinear activation\nfunction. Indeed, batch normalization itself is the cause of gradient\nexplosion. As a result, vanilla batch-normalized networks without skip\nconnections are not trainable at large depths for common initialization\nschemes, a prediction that we verify with a variety of empirical simulations.\nWhile gradient explosion cannot be eliminated, it can be reduced by tuning the\nnetwork close to the linear regime, which improves the trainability of deep\nbatch-normalized networks without residual connections. Finally, we investigate\nthe learning dynamics of batch-normalized networks and observe that after a\nsingle step of optimization the networks achieve a relatively stable\nequilibrium in which gradients have dramatically smaller dynamic range. Our\ntheory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new\nidentities that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:36:13 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 21:42:13 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Yang", "Greg", ""], ["Pennington", "Jeffrey", ""], ["Rao", "Vinay", ""], ["Sohl-Dickstein", "Jascha", ""], ["Schoenholz", "Samuel S.", ""]]}, {"id": "1902.08192", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd\n  Domains", "comments": "IEEE ICASSP 2019. arXiv admin note: substantial text overlap with\n  arXiv:1805.08303", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization of deep convolutional neural networks (CNNs)\nsuch that they provide good performance while having reduced complexity if\ndeployed on either conventional systems with spatial-domain convolution or\nlower-complexity systems designed for Winograd convolution. The proposed\nframework produces one compressed model whose convolutional filters can be made\nsparse either in the spatial domain or in the Winograd domain. Hence, the\ncompressed model can be deployed universally on any platform, without need for\nre-training on the deployed platform. To get a better compression ratio, the\nsparse model is compressed in the spatial domain that has a fewer number of\nparameters. From our experiments, we obtain $24.2\\times$ and $47.7\\times$\ncompressed models for ResNet-18 and AlexNet trained on the ImageNet dataset,\nwhile their computational cost is also reduced by $4.5\\times$ and $5.1\\times$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 01:03:04 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1902.08261", "submitter": "Yingtao Tian", "authors": "Yingtao Tian, Jesse Engel", "title": "Latent Translation: Crossing Modalities by Bridging Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end optimization has achieved state-of-the-art performance on many\nspecific problems, but there is no straight-forward way to combine pretrained\nmodels for new problems. Here, we explore improving modularity by learning a\npost-hoc interface between two existing models to solve a new task.\nSpecifically, we take inspiration from neural machine translation, and cast the\nchallenging problem of cross-modal domain transfer as unsupervised translation\nbetween the latent spaces of pretrained deep generative models. By abstracting\naway the data representation, we demonstrate that it is possible to transfer\nacross different modalities (e.g., image-to-audio) and even different types of\ngenerative models (e.g., VAE-to-GAN). We compare to state-of-the-art techniques\nand find that a straight-forward variational autoencoder is able to best bridge\nthe two generative models through learning a shared latent space. We can\nfurther impose supervised alignment of attributes in both domains with a\nclassifier in the shared latent space. Through qualitative and quantitative\nevaluations, we demonstrate that locality and semantic alignment are preserved\nthrough the transfer process, as indicated by high transfer accuracies and\nsmooth interpolations within a class. Finally, we show this modular structure\nspeeds up training of new interface models by several orders of magnitude by\ndecoupling it from expensive retraining of base generative models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 20:54:51 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Tian", "Yingtao", ""], ["Engel", "Jesse", ""]]}, {"id": "1902.08410", "submitter": "Elena Pastorelli", "authors": "Elena Pastorelli and Cristiano Capone and Francesco Simula and Maria\n  V. Sanchez-Vives and Paolo Del Giudice and Maurizio Mattia and Pier Stanislao\n  Paolucci", "title": "Scaling of a large-scale simulation of synchronous slow-wave and\n  asynchronous awake-like activity of a cortical model with long-range\n  interconnections", "comments": "22 pages, 9 figures, 4 tables", "journal-ref": "Front Syst Neurosci. 2019;13:33. Published 2019 Jul 23", "doi": "10.3389/fnsys.2019.00033", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical synapse organization supports a range of dynamic states on multiple\nspatial and temporal scales, from synchronous slow wave activity (SWA),\ncharacteristic of deep sleep or anesthesia, to fluctuating, asynchronous\nactivity during wakefulness (AW). Such dynamic diversity poses a challenge for\nproducing efficient large-scale simulations that embody realistic metaphors of\nshort- and long-range synaptic connectivity. In fact, during SWA and AW\ndifferent spatial extents of the cortical tissue are active in a given timespan\nand at different firing rates, which implies a wide variety of loads of local\ncomputation and communication. A balanced evaluation of simulation performance\nand robustness should therefore include tests of a variety of cortical dynamic\nstates. Here, we demonstrate performance scaling of our proprietary Distributed\nand Plastic Spiking Neural Networks (DPSNN) simulation engine in both SWA and\nAW for bidimensional grids of neural populations, which reflects the modular\norganization of the cortex. We explored networks up to 192x192 modules, each\ncomposed of 1250 integrate-and-fire neurons with spike-frequency adaptation,\nand exponentially decaying inter-modular synaptic connectivity with varying\nspatial decay constant. For the largest networks the total number of synapses\nwas over 70 billion. The execution platform included up to 64 dual-socket\nnodes, each socket mounting 8 Intel Xeon Haswell processor cores @ 2.40GHz\nclock rates. Network initialization time, memory usage, and execution time\nshowed good scaling performances from 1 to 1024 processes, implemented using\nthe standard Message Passing Interface (MPI) protocol. We achieved simulation\nspeeds of between 2.3x10^9 and 4.1x10^9 synaptic events per second for both\ncortical states in the explored range of inter-modular interconnections.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 09:13:46 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 14:32:27 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Pastorelli", "Elena", ""], ["Capone", "Cristiano", ""], ["Simula", "Francesco", ""], ["Sanchez-Vives", "Maria V.", ""], ["Del Giudice", "Paolo", ""], ["Mattia", "Maurizio", ""], ["Paolucci", "Pier Stanislao", ""]]}, {"id": "1902.08673", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad and Shahrokh Valaee", "title": "Ising-Dropout: A Regularization Method for Training and Compression of\n  Deep Neural Networks", "comments": "This paper is accepted at 44th IEEE International Conference on\n  Acoustics, Speech and Signal Processing (IEEE ICASSP), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting is a major problem in training machine learning models,\nspecifically deep neural networks. This problem may be caused by imbalanced\ndatasets and initialization of the model parameters, which conforms the model\ntoo closely to the training data and negatively affects the generalization\nperformance of the model for unseen data. The original dropout is a\nregularization technique to drop hidden units randomly during training. In this\npaper, we propose an adaptive technique to wisely drop the visible and hidden\nunits in a deep neural network using Ising energy of the network. The\npreliminary results show that the proposed approach can keep the classification\nperformance competitive to the original network while eliminating optimization\nof unnecessary network parameters in each training cycle. The dropout state of\nunits can also be applied to the trained (inference) model. This technique\ncould compress the network in terms of number of parameters up to 41.18% and\n55.86% for the classification task on the MNIST and Fashion-MNIST datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 02:21:40 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Valaee", "Shahrokh", ""]]}, {"id": "1902.08792", "submitter": "Zhongyi Hu", "authors": "Zhongyi Hu, Raymond Chiong, Ilung Pranata, Willy Susilo, Yukun Bao", "title": "Identifying Malicious Web Domains Using Machine Learning Techniques with\n  Online Credibility and Performance Data", "comments": "10 pages, conference", "journal-ref": "2016 IEEE Congress on Evolutionary Computation (CEC)", "doi": "10.1109/CEC.2016.7748347", "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious web domains represent a big threat to web users' privacy and\nsecurity. With so much freely available data on the Internet about web domains'\npopularity and performance, this study investigated the performance of\nwell-known machine learning techniques used in conjunction with this type of\nonline data to identify malicious web domains. Two datasets consisting of\nmalware and phishing domains were collected to build and evaluate the machine\nlearning classifiers. Five single classifiers and four ensemble classifiers\nwere applied to distinguish malicious domains from benign ones. In addition, a\nbinary particle swarm optimisation (BPSO) based feature selection method was\nused to improve the performance of single classifiers. Experimental results\nshow that, based on the web domains' popularity and performance data features,\nthe examined machine learning techniques can accurately identify malicious\ndomains in different ways. Furthermore, the BPSO-based feature selection\nprocedure is shown to be an effective way to improve the performance of\nclassifiers.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 14:10:29 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Hu", "Zhongyi", ""], ["Chiong", "Raymond", ""], ["Pranata", "Ilung", ""], ["Susilo", "Willy", ""], ["Bao", "Yukun", ""]]}, {"id": "1902.09037", "submitter": "Ivan Chelombiev", "authors": "Ivan Chelombiev, Conor Houghton, Cian O'Donnell", "title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "comments": "Accepted as a poster presentation at ICLR 2019 and reviewed on\n  OpenReview (available at https://openreview.net/forum?id=SkeZisA5t7). Pages:\n  11. Figures: 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve how neural networks function it is crucial to understand their\nlearning process. The information bottleneck theory of deep learning proposes\nthat neural networks achieve good generalization by compressing their\nrepresentations to disregard information that is not relevant to the task.\nHowever, empirical evidence for this theory is conflicting, as compression was\nonly observed when networks used saturating activation functions. In contrast,\nnetworks with non-saturating activation functions achieved comparable levels of\ntask performance but did not show compression. In this paper we developed more\nrobust mutual information estimation techniques, that adapt to hidden activity\nof neural networks and produce more sensitive measurements of activations from\nall functions, especially unbounded functions. Using these adaptive estimation\ntechniques, we explored compression in networks with a range of different\nactivation functions. With two improved methods of estimation, firstly, we show\nthat saturation of the activation function is not required for compression, and\nthe amount of compression varies between different activation functions. We\nalso find that there is a large amount of variation in compression between\ndifferent network initializations. Secondary, we see that L2 regularization\nleads to significantly increased compression, while preventing overfitting.\nFinally, we show that only compression of the last layer is positively\ncorrelated with generalization.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 23:41:54 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Chelombiev", "Ivan", ""], ["Houghton", "Conor", ""], ["O'Donnell", "Cian", ""]]}, {"id": "1902.09215", "submitter": "W B Langdon", "authors": "W. B. Langdon and W. Banzhaf", "title": "Faster Genetic Programming GPquick via multicore and Advanced Vector\n  Extensions", "comments": "20 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": "RN/19/01", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evolve floating point Sextic polynomial populations of genetic programming\nbinary trees for up to a million generations. Programs with almost four hundred\nmillion instructions are created by crossover. To support unbounded Long-Term\nEvolution Experiment LTEE GP we use both SIMD parallel AVX 512 bit instructions\nand 48 threads to yield performance of up to 139 billion GP operations per\nsecond, 139 giga GPops, on a single Intel Xeon Gold 6126 2.60GHz server.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 12:01:18 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Langdon", "W. B.", ""], ["Banzhaf", "W.", ""]]}, {"id": "1902.09240", "submitter": "David Castillo Bolado", "authors": "David Castillo-Bolado, Cayetano Guerra-Artal, Mario Hernandez-Tejera", "title": "Modularity as a Means for Complexity Management in Neural Networks\n  Learning", "comments": "Full-paper submited to the AAAI-MAKE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Neural Network (NN) with lots of parameters or intricate\narchitectures creates undesired phenomena that complicate the optimization\nprocess. To address this issue we propose a first modular approach to NN\ndesign, wherein the NN is decomposed into a control module and several\nfunctional modules, implementing primitive operations. We illustrate the\nmodular concept by comparing performances between a monolithic and a modular NN\non a list sorting problem and show the benefits in terms of training speed,\ntraining stability and maintainability. We also discuss some questions that\narise in modular NNs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 12:57:58 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Castillo-Bolado", "David", ""], ["Guerra-Artal", "Cayetano", ""], ["Hernandez-Tejera", "Mario", ""]]}, {"id": "1902.09726", "submitter": "Tanmay Chavan", "authors": "Tanmay Chavan, Sangya Dutta, Nihar R. Mohapatra, and Udayan Ganguly", "title": "Band-to-Band Tunneling based Ultra-Energy Efficient Silicon Neuron", "comments": null, "journal-ref": null, "doi": "10.1109/TED.2020.2985167", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain comprises about a hundred billion neurons connected through\nquadrillion synapses. Spiking Neural Networks (SNNs) take inspiration from the\nbrain to model complex cognitive and learning tasks. Neuromorphic engineering\nimplements SNNs in hardware, aspiring to mimic the brain at scale (i.e., 100\nbillion neurons) with biological area and energy efficiency. The design of\nultra-energy efficient and compact neurons is essential for the large-scale\nimplementation of SNNs in hardware. In this work, we have experimentally\ndemonstrated a Partially Depleted (PD) Silicon-On-Insulator (SOI) MOSFET based\nLeaky-Integrate & Fire (LIF) neuron where energy-and area-efficiency is enabled\nby two elements of design - first tunneling based operation and second compact\nsub-threshold SOI control circuit design. Band-to-Band Tunneling (BTBT) induced\nhole storage in the body is used for the \"Integrate\" function of the neuron. A\ncompact control circuit \"Fires\" a spike when the body potential exceeds the\nfiring threshold. The neuron then \"Resets\" by removing the stored holes from\nthe body contact of the device. Additionally, the control circuit provides\n\"Leakiness\" in the neuron which is an essential property of biological neurons.\nThe proposed neuron provides 10x higher area efficiency compared to CMOS design\nwith equivalent energy/spike. Alternatively, it has 10^4x higher energy\nefficiency at area-equivalent neuron technologies. Biologically comparable\nenergy- and area-efficiency along with CMOS compatibility make the proposed\ndevice attractive for large-scale hardware implementation of SNNs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 04:43:06 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chavan", "Tanmay", ""], ["Dutta", "Sangya", ""], ["Mohapatra", "Nihar R.", ""], ["Ganguly", "Udayan", ""]]}, {"id": "1902.09791", "submitter": "Yulia Sandamirskaya", "authors": "Giacomo Indiveri and Yulia Sandamirskaya", "title": "The importance of space and time in neuromorphic cognitive agents", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2019.2928376", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks and computational neuroscience models have made\ntremendous progress, allowing computers to achieve impressive results in\nartificial intelligence (AI) applications, such as image recognition, natural\nlanguage processing, or autonomous driving. Despite this remarkable progress,\nbiological neural systems consume orders of magnitude less energy than today's\nartificial neural networks and are much more agile and adaptive. This\nefficiency and adaptivity gap is partially explained by the computing substrate\nof biological neural processing systems that is fundamentally different from\nthe way today's computers are built. Biological systems use in-memory computing\nelements operating in a massively parallel way rather than time-multiplexed\ncomputing units that are reused in a sequential fashion. Moreover, activity of\nbiological neurons follows continuous-time dynamics in real, physical time,\ninstead of operating on discrete temporal cycles abstracted away from\nreal-time. Here, we present neuromorphic processing devices that emulate the\nbiological style of processing by using parallel instances of mixed-signal\nanalog/digital circuits that operate in real time. We argue that this approach\nbrings significant advantages in efficiency of computation. We show examples of\nembodied neuromorphic agents that use such devices to interact with the\nenvironment and exhibit autonomous learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 08:23:16 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Indiveri", "Giacomo", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1902.09849", "submitter": "Chaoyue He", "authors": "Chaoyue He, Yong Liu, Qingyu Guo and Chunyan Miao", "title": "Multi-Scale Quasi-RNN for Next Item Recommendation", "comments": "7 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to better utilize sequential information has been extensively studied in\nthe setting of recommender systems. To this end, architectural inductive biases\nsuch as Markov-Chains, Recurrent models, Convolutional networks and many others\nhave demonstrated reasonable success on this task. This paper proposes a new\nneural architecture, multi-scale Quasi-RNN for next item Recommendation\n(QR-Rec) task. Our model provides the best of both worlds by exploiting\nmulti-scale convolutional features as the compositional gating functions of a\nrecurrent cell. The model is implemented in a multi-scale fashion, i.e.,\nconvolutional filters of various widths are implemented to capture different\nunion-level features of input sequences which influence the compositional\nencoder. The key idea aims to capture the recurrent relations between different\nkinds of local features, which has never been studied previously in the context\nof recommendation. Through extensive experiments, we demonstrate that our model\nachieves state-of-the-art performance on 15 well-established datasets,\noutperforming strong competitors such as FPMC, Fossil and Caser absolutely by\n0.57%-7.16% and relatively by 1.44%-17.65% in terms of MAP, Recall@10 and\nNDCG@10.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:33:00 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["He", "Chaoyue", ""], ["Liu", "Yong", ""], ["Guo", "Qingyu", ""], ["Miao", "Chunyan", ""]]}, {"id": "1902.09864", "submitter": "Jyotibdha Acharya", "authors": "Jyotibdha Acharya, Vandana Padala, Arindam Basu", "title": "Spiking Neural Network based Region Proposal Networks for Neuromorphic\n  Vision Sensors", "comments": "Accepted in IEEE ISCAS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a three layer spiking neural network based region\nproposal network operating on data generated by neuromorphic vision sensors.\nThe proposed architecture consists of refractory, convolution and clustering\nlayers designed with bio-realistic leaky integrate and fire (LIF) neurons and\nsynapses. The proposed algorithm is tested on traffic scene recordings from a\nDAVIS sensor setup. The performance of the region proposal network has been\ncompared with event based mean shift algorithm and is found to be far superior\n(~50% better) in recall for similar precision (~85%). Computational and memory\ncomplexity of the proposed method are also shown to be similar to that of event\nbased mean shift\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 11:21:28 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Acharya", "Jyotibdha", ""], ["Padala", "Vandana", ""], ["Basu", "Arindam", ""]]}, {"id": "1902.10073", "submitter": "Biwei Huang", "authors": "Biwei Huang, Kun Zhang, Ruben Sanchez-Romero, Joseph Ramsey, Madelyn\n  Glymour, Clark Glymour", "title": "Diagnosis of Autism Spectrum Disorder by Causal Influence Strength\n  Learned from Resting-State fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder (ASD) is one of the major developmental disorders\naffecting children. Recently, it has been hypothesized that ASD is associated\nwith atypical brain connectivities. A substantial body of researches use\nPearson's correlation coefficients, mutual information, or partial correlation\nto investigate the differences in brain connectivities between ASD and typical\ncontrols from functional Magnetic Resonance Imaging (fMRI). However,\ncorrelation or partial correlation does not directly reveal causal influences -\nthe information flow - between brain regions. Comparing to correlation,\ncausality pinpoints the key connectivity characteristics and removes redundant\nfeatures for diagnosis.\n  In this paper, we propose a two-step method for large-scale and cyclic causal\ndiscovery from fMRI. It can identify brain causal structures without doing\ninterventional experiments. The learned causal structure, as well as the causal\ninfluence strength, provides us the path and effectiveness of information flow.\nWith the recovered causal influence strength as candidate features, we then\nperform ASD diagnosis by further doing feature selection and classification. We\napply our methods to three datasets from Autism Brain Imaging Data Exchange\n(ABIDE).\n  From experimental results, it shows that with causal connectivities, the\ndiagnostic accuracy largely improves. A closer examination shows that\ninformation flows starting from the superior front gyrus to default mode\nnetwork and posterior areas are largely reduced. Moreover, all enhanced\ninformation flows are from posterior to anterior or in local areas. Overall, it\nshows that long-range influences have a larger proportion of reductions than\nlocal ones, while local influences have a larger proportion of increases than\nlong-range ones. By examining the graph properties of brain causal structure,\nthe group of ASD shows reduced small-worldness.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 05:09:55 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 16:28:50 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Huang", "Biwei", ""], ["Zhang", "Kun", ""], ["Sanchez-Romero", "Ruben", ""], ["Ramsey", "Joseph", ""], ["Glymour", "Madelyn", ""], ["Glymour", "Clark", ""]]}, {"id": "1902.10178", "submitter": "Wojciech Samek", "authors": "Sebastian Lapuschkin, Stephan W\\\"aldchen, Alexander Binder, Gr\\'egoire\n  Montavon, Wojciech Samek, Klaus-Robert M\\\"uller", "title": "Unmasking Clever Hans Predictors and Assessing What Machines Really\n  Learn", "comments": "Accepted for publication in Nature Communications", "journal-ref": null, "doi": "10.1038/s41467-019-08987-4", "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current learning machines have successfully solved hard application problems,\nreaching high accuracy and displaying seemingly \"intelligent\" behavior. Here we\napply recent techniques for explaining decisions of state-of-the-art learning\nmachines and analyze various tasks from computer vision and arcade games. This\nshowcases a spectrum of problem-solving behaviors ranging from naive and\nshort-sighted, to well-informed and strategic. We observe that standard\nperformance evaluation metrics can be oblivious to distinguishing these diverse\nproblem solving behaviors. Furthermore, we propose our semi-automated Spectral\nRelevance Analysis that provides a practically effective way of characterizing\nand validating the behavior of nonlinear learning machines. This helps to\nassess whether a learned model indeed delivers reliably for the problem that it\nwas conceived for. Furthermore, our work intends to add a voice of caution to\nthe ongoing excitement about machine intelligence and pledges to evaluate and\njudge some of these recent successes in a more nuanced manner.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 19:25:11 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Lapuschkin", "Sebastian", ""], ["W\u00e4ldchen", "Stephan", ""], ["Binder", "Alexander", ""], ["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1902.10369", "submitter": "Yael Hitron", "authors": "Yael Hitron, Merav Parter", "title": "Counting to Ten with Two Fingers: Compressed Counting with Spiking\n  Neurons", "comments": "Accepted to ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of measuring time with probabilistic threshold gates\nimplemented by bio-inspired spiking neurons. In the model of spiking neural\nnetworks, network evolves in discrete rounds, where in each round, neurons fire\nin pulses in response to a sufficiently high membrane potential. This potential\nis induced by spikes from neighboring neurons that fired in the previous round,\nwhich can have either an excitatory or inhibitory effect. We first consider a\ndeterministic implementation of a neural timer and show that $\\Theta(\\log t)$\n(deterministic) threshold gates are both sufficient and necessary. This raised\nthe question of whether randomness can be leveraged to reduce the number of\nneurons. We answer this question in the affirmative by considering neural\ntimers with spiking neurons where the neuron $y$ is required to fire for $t$\nconsecutive rounds with probability at least $1-\\delta$, and should stop firing\nafter at most $2t$ rounds with probability $1-\\delta$ for some input parameter\n$\\delta \\in (0,1)$. Our key result is a construction of a neural timer with\n$O(\\log\\log 1/\\delta)$ spiking neurons. Interestingly, this construction uses\nonly one spiking neuron, while the remaining neurons can be deterministic\nthreshold gates. We complement this construction with a matching lower bound of\n$\\Omega(\\min\\{\\log\\log 1/\\delta, \\log t\\})$ neurons. This provides the first\nseparation between deterministic and randomized constructions in the setting of\nspiking neural networks. Finally, we demonstrate the usefulness of compressed\ncounting networks for synchronizing neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:39:17 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 15:21:15 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 09:22:08 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hitron", "Yael", ""], ["Parter", "Merav", ""]]}, {"id": "1902.10630", "submitter": "Fangxin Shang", "authors": "Fangxin Shang, Hao Zhang", "title": "Alternating Synthetic and Real Gradients for Neural Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training recurrent neural networks (RNNs) with backpropagation through time\n(BPTT) has known drawbacks such as being difficult to capture longterm\ndependencies in sequences. Successful alternatives to BPTT have not yet been\ndiscovered. Recently, BP with synthetic gradients by a decoupled neural\ninterface module has been proposed to replace BPTT for training RNNs. On the\nother hand, it has been shown that the representations learned with synthetic\nand real gradients are different though they are functionally identical. In\nthis project, we explore ways of combining synthetic and real gradients with\napplication to neural language modeling tasks. Empirically, we demonstrate the\neffectiveness of alternating training with synthetic and real gradients after\nperiodic warm restarts on language modeling tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 16:48:20 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Shang", "Fangxin", ""], ["Zhang", "Hao", ""]]}, {"id": "1902.10834", "submitter": "Chris Watkins", "authors": "J\\\"uri Lember and Chris Watkins", "title": "An evolutionary model that satisfies detailed balance", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a class of evolutionary models that involves an arbitrary\nexchangeable process as the breeding process and different selection schemes.\nIn those models, a new genome is born according to the breeding process, and\nthen a genome is removed according to the selection scheme that involves\nfitness. Thus the population size remains constant. The process evolves\naccording to a Markov chain, and, unlike in many other existing models, the\nstationary distribution -- so called mutation-selection equilibrium -- can be\neasily found and studied. The behaviour of the stationary distribution when the\npopulation size increases is our main object of interest. Several\nphase-transition theorems are proved.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 23:41:35 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:26:30 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Watkins", "Chris", ""]]}, {"id": "1902.11100", "submitter": "Sergey Belim", "authors": "S.V. Belim, D.E. Vilkhovskiy", "title": "Usage of analytic hierarchy process for steganographic inserts detection\n  in images", "comments": null, "journal-ref": "2016 Dynamics of Systems, Mechanisms and Machines (Dynamics),\n  Omsk, Russia, pp. 1-5", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the method of steganography detection, which is formed\nby replacing the least significant bit (LSB). Detection is performed by\ndividing the image into layers and making an analysis of zero-layer of adjacent\nbits for every bit. First-layer and second-layer are analyzed too. Hierarchies\nanalysis method is used for making decision if current bit is changed.\nWeighting coefficients as part of the analytic hierarchy process are formed on\nthe values of bits. Then a matrix of corrupted pixels is generated.\nVisualization of matrix with corrupted pixels allows to determine size,\nlocation and presence of the embedded message. Computer experiment was\nperformed. Message was embedded in a bounded rectangular area of the image.\nThis method demonstrated efficiency even at low filling container, less than\n10\\%. Widespread statistical methods are unable to detect this steganographic\ninsert. The location and size of the embedded message can be determined with an\nerror which is not exceeding to five pixels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 12:34:21 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Belim", "S. V.", ""], ["Vilkhovskiy", "D. E.", ""]]}, {"id": "1902.11208", "submitter": "Gideon Maillette De Buy Wenniger", "authors": "Gideon Maillette de Buy Wenniger, Lambert Schomaker, Andy Way", "title": "No Padding Please: Efficient Neural Handwriting Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICDAR.2019.00064", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural handwriting recognition (NHR) is the recognition of handwritten text\nwith deep learning models, such as multi-dimensional long short-term memory\n(MDLSTM) recurrent neural networks. Models with MDLSTM layers have achieved\nstate-of-the art results on handwritten text recognition tasks. While\nmulti-directional MDLSTM-layers have an unbeaten ability to capture the\ncomplete context in all directions, this strength limits the possibilities for\nparallelization, and therefore comes at a high computational cost. In this work\nwe develop methods to create efficient MDLSTM-based models for NHR,\nparticularly a method aimed at eliminating computation waste that results from\npadding. This proposed method, called example-packing, replaces wasteful\nstacking of padded examples with efficient tiling in a 2-dimensional grid. For\nword-based NHR this yields a speed improvement of factor 6.6 over an already\nefficient baseline of minimal padding for each batch separately. For line-based\nNHR the savings are more modest, but still significant. In addition to\nexample-packing, we propose: 1) a technique to optimize parallelization for\ndynamic graph definition frameworks including PyTorch, using convolutions with\ngrouping, 2) a method for parallelization across GPUs for variable-length\nexample batches. All our techniques are thoroughly tested on our own PyTorch\nre-implementation of MDLSTM-based NHR models. A thorough evaluation on the IAM\ndataset shows that our models are performing similar to earlier implementations\nof state-of-the-art models. Our efficient NHR model and some of the reusable\ntechniques discussed with it offer ways to realize relatively efficient models\nfor the omnipresent scenario of variable-length inputs in deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:46:43 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wenniger", "Gideon Maillette de Buy", ""], ["Schomaker", "Lambert", ""], ["Way", "Andy", ""]]}, {"id": "1902.11261", "submitter": "Sirisha Rambhatla", "authors": "Sirisha Rambhatla, Xingguo Li, and Jarvis Haupt", "title": "NOODL: Provable Online Dictionary Learning and Sparse Coding", "comments": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2019; 42 Pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dictionary learning problem, where the aim is to model the\ngiven data as a linear combination of a few columns of a matrix known as a\ndictionary, where the sparse weights forming the linear combination are known\nas coefficients. Since the dictionary and coefficients, parameterizing the\nlinear model are unknown, the corresponding optimization is inherently\nnon-convex. This was a major challenge until recently, when provable algorithms\nfor dictionary learning were proposed. Yet, these provide guarantees only on\nthe recovery of the dictionary, without explicit recovery guarantees on the\ncoefficients. Moreover, any estimation error in the dictionary adversely\nimpacts the ability to successfully localize and estimate the coefficients.\nThis potentially limits the utility of existing provable dictionary learning\nmethods in applications where coefficient recovery is of interest. To this end,\nwe develop NOODL: a simple Neurally plausible alternating Optimization-based\nOnline Dictionary Learning algorithm, which recovers both the dictionary and\ncoefficients exactly at a geometric rate, when initialized appropriately. Our\nalgorithm, NOODL, is also scalable and amenable for large scale distributed\nimplementations in neural architectures, by which we mean that it only involves\nsimple linear and non-linear operations. Finally, we corroborate these\ntheoretical results via experimental evaluation of the proposed algorithm with\nthe current state-of-the-art techniques.\n  Keywords: dictionary learning, provable dictionary learning, online\ndictionary learning, non-convex, sparse coding, support recovery, iterative\nhard thresholding, matrix factorization, neural architectures, neural networks,\nnoodl, sparse representations, sparse signal processing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:08:01 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 18:46:33 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 17:23:39 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 19:58:36 GMT"}, {"version": "v5", "created": "Tue, 27 Aug 2019 21:54:27 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Li", "Xingguo", ""], ["Haupt", "Jarvis", ""]]}]