[{"id": "1807.00053", "submitter": "Aran Nayebi", "authors": "Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli,\n  David Sussillo, James J. DiCarlo, Daniel L. K. Yamins", "title": "Task-Driven Convolutional Recurrent Models of the Visual System", "comments": "NIPS 2018 Camera Ready Version, 16 pages including supplementary\n  information, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feed-forward convolutional neural networks (CNNs) are currently\nstate-of-the-art for object classification tasks such as ImageNet. Further,\nthey are quantitatively accurate models of temporally-averaged responses of\nneurons in the primate brain's visual system. However, biological visual\nsystems have two ubiquitous architectural features not shared with typical\nCNNs: local recurrence within cortical areas, and long-range feedback from\ndownstream areas to upstream areas. Here we explored the role of recurrence in\nimproving classification performance. We found that standard forms of\nrecurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the\nImageNet task. In contrast, novel cells that incorporated two structural\nfeatures, bypassing and gating, were able to boost task accuracy substantially.\nWe extended these design principles in an automated search over thousands of\nmodel architectures, which identified novel local recurrent cells and\nlong-range feedback connections useful for object recognition. Moreover, these\ntask-optimized ConvRNNs matched the dynamics of neural activity in the primate\nvisual system better than feedforward networks, suggesting a role for the\nbrain's recurrent connections in performing difficult visual behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 20:27:23 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 03:49:01 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nayebi", "Aran", ""], ["Bear", "Daniel", ""], ["Kubilius", "Jonas", ""], ["Kar", "Kohitij", ""], ["Ganguli", "Surya", ""], ["Sussillo", "David", ""], ["DiCarlo", "James J.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1807.00188", "submitter": "Stef Maree", "authors": "S.C. Maree, T. Alderliesten, D. Thierens, P.A.N. Bosman", "title": "Benchmarking the Hill-Valley Evolutionary Algorithm for the GECCO 2018\n  Competition on Niching Methods Multimodal Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents benchmarking results of the latest version of the\nHill-Valley Evolutionary Algorithm (HillVallEA) on the CEC2013 niching\nbenchmark suite. The benchmarking follows restrictions required by the GECCO\n2018 competition on Niching methods for Multimodal Optimization. In particular,\nno problem dependent parameter tuning is performed. A number of adjustments\nhave been made to original publication of HillVallEA that are discussed in this\nreport.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 15:08:48 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 09:26:40 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Maree", "S. C.", ""], ["Alderliesten", "T.", ""], ["Thierens", "D.", ""], ["Bosman", "P. A. N.", ""]]}, {"id": "1807.00284", "submitter": "Yong Xia", "authors": "Benteng Ma, Yong Xia", "title": "Autonomous Deep Learning: A Genetic DCNN Designer for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the breakthrough success of deep convolutional\nneural networks (DCNNs) in image classification and other vision applications.\nAlthough freeing users from the troublesome handcrafted feature extraction by\nproviding a uniform feature extraction-classification framework, DCNNs still\nrequire a handcrafted design of their architectures. In this paper, we propose\nthe genetic DCNN designer, an autonomous learning algorithm can generate a DCNN\narchitecture automatically based on the data available for a specific image\nclassification problem. We first partition a DCNN into multiple stacked meta\nconvolutional blocks and fully connected blocks, each containing the operations\nof convolution, pooling, fully connection, batch normalization, activation and\ndrop out, and thus convert the architecture into an integer vector. Then, we\nuse refined evolutionary operations, including selection, mutation and\ncrossover to evolve a population of DCNN architectures. Our results on the\nMNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets\nsuggest that the proposed genetic DCNN designer is able to produce\nautomatically DCNN architectures, whose performance is comparable to, if not\nbetter than, that of stateof- the-art DCNN models\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 07:11:54 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ma", "Benteng", ""], ["Xia", "Yong", ""]]}, {"id": "1807.00438", "submitter": "Omid Nezami", "authors": "Anvar Bahrampour, Omid Mohamad Nezami", "title": "Dynamic Swarm Dispersion in Particle Swarm Optimization for Mining\n  Unsearched Area in Solution Space (DSDPSO)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Premature convergence in particle swarm optimization (PSO) algorithm usually\nleads to gaining local optimum and preventing from surveying those regions of\nsolution space which have optimal points in. In this paper, by applying special\nmechanisms, suitable regions were detected and then swarm was guided to them by\ndispersing part of particles on proper times. This process is called dynamic\nswarm dispersion in PSO (DSDPSO) algorithm. In order to specify the proper\ntimes and to rein the evolutionary process alternating between exploring and\nexploiting behaviors, we used a diversity measuring approach and implemented\nthe dispersion mechanism. To promote the performance of DSDPSO algorithm, three\ndifferent policies including particle relocation, velocity settings of\ndispersed particles and parameters setting were applied. We compared the\npromoted algorithm with similar new approaches and according to the numerical\nresults, the proposed algorithm outperformed the basic GPSO, LPSO, DMS-PSO,\nCLPSO and APSO in most of the 12 standard benchmark problems with different\nproperties taken in this study.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 02:37:22 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Bahrampour", "Anvar", ""], ["Nezami", "Omid Mohamad", ""]]}, {"id": "1807.00456", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Chinmaya Devaraj, Michael Maynord, Cornelia Ferm\\\"uller,\n  Yiannis Aloimonos", "title": "Evenly Cascaded Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Evenly Cascaded convolutional Network (ECN), a neural network\ntaking inspiration from the cascade algorithm of wavelet analysis. ECN employs\ntwo feature streams - a low-level and high-level steam. At each layer these\nstreams interact, such that low-level features are modulated using advanced\nperspectives from the high-level stream. ECN is evenly structured through\nresizing feature map dimensions by a consistent ratio, which removes the burden\nof ad-hoc specification of feature map dimensions. ECN produces easily\ninterpretable features maps, a result whose intuition can be understood in the\ncontext of scale-space theory. We demonstrate that ECN's design facilitates the\ntraining process through providing easily trainable shortcuts. We report new\nstate-of-the-art results for small networks, without the need for additional\ntreatment such as pruning or compression - a consequence of ECN's simple\nstructure and direct training. A 6-layered ECN design with under 500k\nparameters achieves 95.24% and 78.99% accuracy on CIFAR-10 and CIFAR-100\ndatasets, respectively, outperforming the current state-of-the-art on small\nparameter networks, and a 3 million parameter ECN produces results competitive\nto the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 04:12:16 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:49:01 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ye", "Chengxi", ""], ["Devaraj", "Chinmaya", ""], ["Maynord", "Michael", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1807.00480", "submitter": "Jeff  (Jun) Zhang", "authors": "Jeff Zhang, Siddharth Garg", "title": "FATE: Fast and Accurate Timing Error Prediction Framework for Low Power\n  DNN Accelerator Design", "comments": "To appear at IEEE/ACM International Conference On Computer Aided\n  Design 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) are increasingly being accelerated on\napplication-specific hardware such as the Google TPU designed especially for\ndeep learning. Timing speculation is a promising approach to further increase\nthe energy efficiency of DNN accelerators. Architectural exploration for timing\nspeculation requires detailed gate-level timing simulations that can be\ntime-consuming for large DNNs that execute millions of multiply-and-accumulate\n(MAC) operations. In this paper we propose FATE, a new methodology for fast and\naccurate timing simulations of DNN accelerators like the Google TPU. FATE\nproposes two novel ideas: (i) DelayNet, a DNN based timing model for MAC units;\nand (ii) a statistical sampling methodology that reduces the number of MAC\noperations for which timing simulations are performed. We show that FATE\nresults in between 8 times-58 times speed-up in timing simulations, while\nintroducing less than 2% error in classification accuracy estimates. We\ndemonstrate the use of FATE by comparing to conventional DNN accelerator that\nuses 2's complement (2C) arithmetic with an alternative implementation that\nuses signed magnitude representations (SMR). We show that that the SMR\nimplementation provides 18% more energy savings for the same classification\naccuracy than 2C, a result that might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 06:21:23 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhang", "Jeff", ""], ["Garg", "Siddharth", ""]]}, {"id": "1807.00578", "submitter": "Roshan Gopalakrishnan", "authors": "Roshan Gopalakrishnan, Yansong Chua and Laxmi R Iyer", "title": "Classifying neuromorphic data using a deep learning framework for image\n  classification", "comments": "4 pages, 3 figures, submitted to ICARCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of artificial intelligence, neuromorphic computing has been\naround for several decades. Deep learning has however made much recent progress\nsuch that it consistently outperforms neuromorphic learning algorithms in\nclassification tasks in terms of accuracy. Specifically in the field of image\nclassification, neuromorphic computing has been traditionally using either the\ntemporal or rate code for encoding static images in datasets into spike trains.\nIt is only till recently, that neuromorphic vision sensors are widely used by\nthe neuromorphic research community, and provides an alternative to such\nencoding methods. Since then, several neuromorphic datasets as obtained by\napplying such sensors on image datasets (e.g. the neuromorphic CALTECH 101)\nhave been introduced. These data are encoded in spike trains and hence seem\nideal for benchmarking of neuromorphic learning algorithms. Specifically, we\ntrain a deep learning framework used for image classification on the CALTECH\n101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We\nobtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic\nCALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to\nthe best reported accuracy, while for neuromorphic CALTECH 101, it outperforms\nthe last best reported accuracy by over 10%. This raises the question of the\nsuitability of such datasets as benchmarks for neuromorphic learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 10:18:37 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Gopalakrishnan", "Roshan", ""], ["Chua", "Yansong", ""], ["Iyer", "Laxmi R", ""]]}, {"id": "1807.00930", "submitter": "Davide Nunes", "authors": "Davide Nunes, Luis Antunes", "title": "Neural Random Projections for Language Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based language models deal with data sparsity problems by\nmapping the large discrete space of words into a smaller continuous space of\nreal-valued vectors. By learning distributed vector representations for words,\neach training sample informs the neural network model about a combinatorial\nnumber of other patterns. In this paper, we exploit the sparsity in natural\nlanguage even further by encoding each unique input word using a fixed sparse\nrandom representation. These sparse codes are then projected onto a smaller\nembedding space which allows for the encoding of word occurrences from a\npossibly unknown vocabulary, along with the creation of more compact language\nmodels using a reduced number of parameters. We investigate the properties of\nour encoding mechanism empirically, by evaluating its performance on the widely\nused Penn Treebank corpus. We show that guaranteeing approximately equidistant\n(nearly orthogonal) vector representations for unique discrete inputs is enough\nto provide the neural network model with enough information to learn --and make\nuse-- of distributed representations for these inputs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 23:54:48 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 09:56:55 GMT"}, {"version": "v3", "created": "Wed, 15 Aug 2018 19:15:05 GMT"}, {"version": "v4", "created": "Wed, 26 Sep 2018 16:55:04 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Nunes", "Davide", ""], ["Antunes", "Luis", ""]]}, {"id": "1807.00962", "submitter": "Alex James Dr", "authors": "Olga Krestinskaya, Alex Pappachen James, Leon O. Chua", "title": "Neuro-memristive Circuits for Edge Computing: A review", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2019", "doi": "10.1109/TNNLS.2019.2899262", "report-no": null, "categories": "cs.ET cs.AI cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume, veracity, variability, and velocity of data produced from the\never-increasing network of sensors connected to Internet pose challenges for\npower management, scalability, and sustainability of cloud computing\ninfrastructure. Increasing the data processing capability of edge computing\ndevices at lower power requirements can reduce several overheads for cloud\ncomputing solutions. This paper provides the review of neuromorphic\nCMOS-memristive architectures that can be integrated into edge computing\ndevices. We discuss why the neuromorphic architectures are useful for edge\ndevices and show the advantages, drawbacks and open problems in the field of\nneuro-memristive circuits for edge computing.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 04:07:23 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 03:55:48 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Krestinskaya", "Olga", ""], ["James", "Alex Pappachen", ""], ["Chua", "Leon O.", ""]]}, {"id": "1807.00981", "submitter": "William La Cava", "authors": "William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, Jason\n  H. Moore", "title": "Learning concise representations for regression by evolving networks of\n  trees", "comments": "16 pages, 11 figures (including Appendix), published in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a method for learning interpretable representations for\nthe task of regression. Features are represented as networks of multi-type\nexpression trees comprised of activation functions common in neural networks in\naddition to other elementary functions. Differentiable features are trained via\ngradient descent, and the performance of features in a linear model is used to\nweight the rate of change among subcomponents of each representation. The\nsearch process maintains an archive of representations with accuracy-complexity\ntrade-offs to assist in generalization and interpretation. We compare several\nstochastic optimization approaches within this framework. We benchmark these\nvariants on 100 open-source regression problems in comparison to\nstate-of-the-art machine learning approaches. Our main finding is that this\napproach produces the highest average test scores across problems while\nproducing representations that are orders of magnitude smaller than the next\nbest performing method (gradient boosting). We also report a negative result in\nwhich attempts to directly optimize the disentanglement of the representation\nresult in more highly correlated features.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 05:21:30 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 21:22:08 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 16:07:28 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["La Cava", "William", ""], ["Singh", "Tilak Raj", ""], ["Taggart", "James", ""], ["Suri", "Srinivas", ""], ["Moore", "Jason H.", ""]]}, {"id": "1807.01011", "submitter": "Martin Zaefferer", "authors": "Martin Zaefferer and Daniel Horn", "title": "A First Analysis of Kernels for Kriging-based Optimization in\n  Hierarchical Search Spaces", "comments": "The final authenticated version of this publication will appear in\n  the proceedings of the 15th International Conference on Parallel Problem\n  Solving from Nature 2018 (PPSN XV), published in the LNCS by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world optimization problems require significant resources for\nobjective function evaluations. This is a challenge to evolutionary algorithms,\nas it limits the number of available evaluations. One solution are surrogate\nmodels, which replace the expensive objective. A particular issue in this\ncontext are hierarchical variables. Hierarchical variables only influence the\nobjective function if other variables satisfy some condition. We study how this\nkind of hierarchical structure can be integrated into the model based\noptimization framework. We discuss an existing kernel and propose alternatives.\nAn artificial test function is used to investigate how different kernels and\nassumptions affect model quality and search performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:16:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Zaefferer", "Martin", ""], ["Horn", "Daniel", ""]]}, {"id": "1807.01013", "submitter": "Laxmi R Iyer", "authors": "Laxmi R. Iyer and Yansong Chua and Haizhou Li", "title": "Is Neuromorphic MNIST neuromorphic? Analyzing the discriminative power\n  of neuromorphic datasets in the time domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advantage of spiking neural networks (SNNs) over their predecessors is\ntheir ability to spike, enabling them to use spike timing for coding and\nefficient computing. A neuromorphic dataset should allow a neuromorphic\nalgorithm to clearly show that a SNN is able to perform better on the dataset\nthan an ANN. We have analyzed both N-MNIST and N-Caltech101 along these lines,\nbut focus our study on N-MNIST. First we evaluate if additional information is\nencoded in the time domain in a neuromoprhic dataset. We show that an ANN\ntrained with backpropagation on frame based versions of N-MNIST and\nN-Caltech101 images achieve 99.23% and 78.01% accuracy. These are the best\nclassification accuracies obtained on these datasets to date. Second we present\nthe first unsupervised SNN to be trained on N-MNIST and demonstrate results of\n91.78%. We also use this SNN for further experiments on N-MNIST to show that\nrate based SNNs perform better, and precise spike timings are not important in\nN-MNIST. N-MNIST does not, therefore, highlight the unique ability of SNNs. The\nconclusion of this study opens an important question in neuromorphic\nengineering - what, then, constitutes a good neuromorphic dataset?\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:18:19 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Iyer", "Laxmi R.", ""], ["Chua", "Yansong", ""], ["Li", "Haizhou", ""]]}, {"id": "1807.01019", "submitter": "Martin Zaefferer", "authors": "Martin Zaefferer, J\\\"org Stork, Oliver Flasch, Thomas Bartz-Beielstein", "title": "Linear Combination of Distance Measures for Surrogate Models in Genetic\n  Programming", "comments": "The final authenticated version of this publication will appear in\n  the proceedings of the 15th International Conference on Parallel Problem\n  Solving from Nature 2018 (PPSN XV), published in the LNCS by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate models are a well established approach to reduce the number of\nexpensive function evaluations in continuous optimization. In the context of\ngenetic programming, surrogate modeling still poses a challenge, due to the\ncomplex genotype-phenotype relationships. We investigate how different\ngenotypic and phenotypic distance measures can be used to learn Kriging models\nas surrogates. We compare the measures and suggest to use their linear\ncombination in a kernel.\n  We test the resulting model in an optimization framework, using symbolic\nregression problem instances as a benchmark. Our experiments show that the\nmodel provides valuable information. Firstly, the model enables an improved\noptimization performance compared to a model-free algorithm. Furthermore, the\nmodel provides information on the contribution of different distance measures.\nThe data indicates that a phenotypic distance measure is important during the\nearly stages of an optimization run when less data is available. In contrast,\ngenotypic measures, such as the tree edit distance, contribute more during the\nlater stages.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:32:52 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Zaefferer", "Martin", ""], ["Stork", "J\u00f6rg", ""], ["Flasch", "Oliver", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "1807.01035", "submitter": "Manfred Eppe", "authors": "Manfred Eppe and Matthias Kerzel and Erik Strahl and Stefan Wermter", "title": "Deep Neural Object Analysis by Interactive Auditory Exploration with a\n  Humanoid Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for interactive auditory object analysis with a\nhumanoid robot. The robot elicits sensory information by physically shaking\nvisually indistinguishable plastic capsules. It gathers the resulting audio\nsignals from microphones that are embedded into the robotic ears. A neural\nnetwork architecture learns from these signals to analyze properties of the\ncontents of the containers. Specifically, we evaluate the material\nclassification and weight prediction accuracy and demonstrate that the\nframework is fairly robust to acoustic real-world noise.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 09:11:36 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 07:53:06 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Eppe", "Manfred", ""], ["Kerzel", "Matthias", ""], ["Strahl", "Erik", ""], ["Wermter", "Stefan", ""]]}, {"id": "1807.01194", "submitter": "Steve Dias Da Cruz", "authors": "Hans-Peter Beise, Steve Dias Da Cruz, Udo Schr\\\"oder", "title": "On decision regions of narrow deep neural networks", "comments": "This paper is accepted for publication in Neural Networks (Elsevier\n  Journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for neural network functions that have width less or equal to\nthe input dimension all connected components of decision regions are unbounded.\nThe result holds for continuous and strictly monotonic activation functions as\nwell as for the ReLU activation function. This complements recent results on\napproximation capabilities by [Hanin 2017 Approximating] and connectivity of\ndecision regions by [Nguyen 2018 Neural] for such narrow neural networks. Our\nresults are illustrated by means of numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 14:03:42 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 10:26:45 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 09:44:00 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 08:35:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Beise", "Hans-Peter", ""], ["Da Cruz", "Steve Dias", ""], ["Schr\u00f6der", "Udo", ""]]}, {"id": "1807.01418", "submitter": "Minho Ha", "authors": "Minho Ha, Younghoon Byeon, Youngjoo Lee, and Sunggu Lee", "title": "Selective Deep Convolutional Neural Network for Low Cost Distorted Image\n  Classification", "comments": "The authors think that the results of this paper are insufficient.\n  Therefore, we will improve the experiments and analyses, and then rewrite the\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have proven to be well suited for image\nclassification applications. However, if there is distortion in the image, the\nclassification accuracy can be significantly degraded, even with\nstate-of-the-art neural networks. The accuracy cannot be significantly improved\nby simply training with distorted images. Instead, this paper proposes a\nmultiple neural network topology referred to as a selective deep convolutional\nneural network. By modifying existing state-of-the-art neural networks in the\nproposed manner, it is shown that a similar level of classification accuracy\ncan be achieved, but at a significantly lower cost. The cost reduction is\nobtained primarily through the use of fewer weight parameters. Using fewer\nweights reduces the number of multiply-accumulate operations and also reduces\nthe energy required for data accesses. Finally, it is shown that the\neffectiveness of the proposed selective deep convolutional neural network can\nbe further improved by combining it with previously proposed network cost\nreduction methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 01:06:45 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 00:06:21 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Ha", "Minho", ""], ["Byeon", "Younghoon", ""], ["Lee", "Youngjoo", ""], ["Lee", "Sunggu", ""]]}, {"id": "1807.01430", "submitter": "Zhisheng Wang", "authors": "Zhisheng Wang, Fangxuan Sun, Jun Lin, Zhongfeng Wang and Bo Yuan", "title": "SGAD: Soft-Guided Adaptively-Dropped Neural Network", "comments": "9 pages, 4 figures; the first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been proven to have many redundancies.\nHence, many efforts have been made to compress DNNs. However, the existing\nmodel compression methods treat all the input samples equally while ignoring\nthe fact that the difficulties of various input samples being correctly\nclassified are different. To address this problem, DNNs with adaptive dropping\nmechanism are well explored in this work. To inform the DNNs how difficult the\ninput samples can be classified, a guideline that contains the information of\ninput samples is introduced to improve the performance. Based on the developed\nguideline and adaptive dropping mechanism, an innovative soft-guided\nadaptively-dropped (SGAD) neural network is proposed in this paper. Compared\nwith the 32 layers residual neural networks, the presented SGAD can reduce the\nFLOPs by 77% with less than 1% drop in accuracy on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 02:23:10 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Wang", "Zhisheng", ""], ["Sun", "Fangxuan", ""], ["Lin", "Jun", ""], ["Wang", "Zhongfeng", ""], ["Yuan", "Bo", ""]]}, {"id": "1807.01521", "submitter": "Adrien Laversanne-Finot", "authors": "Adrien Laversanne-Finot, Alexandre P\\'er\\'e and Pierre-Yves Oudeyer", "title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces", "comments": "The code used in the experiments is available at\n  https://github.com/flowersteam/Curiosity_Driven_Goal_Exploration", "journal-ref": "Proceedings of The 2nd Conference on Robot Learning, PMLR\n  87:487-504, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsically motivated goal exploration processes enable agents to\nautonomously sample goals to explore efficiently complex environments with\nhigh-dimensional continuous actions. They have been applied successfully to\nreal world robots to discover repertoires of policies producing a wide\ndiversity of effects. Often these algorithms relied on engineered goal spaces\nbut it was recently shown that one can use deep representation learning\nalgorithms to learn an adequate goal space in simple environments. However, in\nthe case of more complex environments containing multiple objects or\ndistractors, an efficient exploration requires that the structure of the goal\nspace reflects the one of the environment. In this paper we show that using a\ndisentangled goal space leads to better exploration performances than an\nentangled goal space. We further show that when the representation is\ndisentangled, one can leverage it by sampling goals that maximize learning\nprogress in a modular manner. Finally, we show that the measure of learning\nprogress, used to drive curiosity-driven exploration, can be used\nsimultaneously to discover abstract independently controllable features of the\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 11:23:57 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 16:33:56 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 16:36:39 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Laversanne-Finot", "Adrien", ""], ["P\u00e9r\u00e9", "Alexandre", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1807.01697", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Thomas G. Dietterich", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Surface\n  Variations", "comments": "Superseded by _Benchmarking Neural Network Robustness to Common\n  Corruptions and Perturbations_ arXiv:1903.12261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish rigorous benchmarks for image classifier\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\ncorruption robustness topic, while showing which classifiers are preferable in\nsafety-critical applications. Unlike recent robustness research, this benchmark\nevaluates performance on commonplace corruptions not worst-case adversarial\ncorruptions. We find that there are negligible changes in relative corruption\nrobustness from AlexNet to ResNet classifiers, and we discover ways to enhance\ncorruption robustness. Then we propose a new dataset called Icons-50 which\nopens research on a new kind of robustness, surface variation robustness. With\nthis dataset we evaluate the frailty of classifiers on new styles of known\nobjects and unexpected instances of known classes. We also demonstrate two\nmethods that improve surface variation robustness. Together our benchmarks may\naid future work toward networks that learn fundamental class structure and also\nrobustly generalize.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 17:57:11 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:57:31 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 20:30:27 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 21:36:39 GMT"}, {"version": "v5", "created": "Sat, 27 Apr 2019 18:19:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1807.01844", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Saeed Sharifian", "title": "Pontogammarus Maeoticus Swarm Optimization: A Metaheuristic Optimization\n  Algorithm", "comments": "15 pages, 13 figures, 11 tables, key words: Pontogammarus Maeoticus,\n  Gammarus swarm, metaheuristic optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, metaheuristic optimization algorithms are used to find the global\noptima in difficult search spaces. Pontogammarus Maeoticus Swarm Optimization\n(PMSO) is a metaheuristic algorithm imitating aquatic nature and foraging\nbehavior. Pontogammarus Maeoticus, also called Gammarus in short, is a tiny\ncreature found mostly in coast of Caspian Sea in Iran. In this algorithm,\nglobal optima is modeled as sea edge (coast) to which Gammarus creatures are\nwilling to move in order to rest from sea waves and forage in sand. Sea waves\nsatisfy exploration and foraging models exploitation. The strength of sea wave\nis determined according to distance of Gammarus from sea edge. The angles of\nwaves applied on several particles are set randomly helping algorithm not be\nstuck in local bests. Meanwhile, the neighborhood of particles change\nadaptively resulting in more efficient progress in searching. The proposed\nalgorithm, although is applicable on any optimization problem, is experimented\nfor partially shaded solar PV array. Experiments on CEC05 benchmarks, as well\nas solar PV array, show the effectiveness of this optimization algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 04:36:18 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Sharifian", "Saeed", ""]]}, {"id": "1807.02389", "submitter": "Akos Ferenc Kungl", "authors": "Akos F. Kungl, Sebastian Schmitt, Johann Kl\\\"ahn, Paul M\\\"uller,\n  Andreas Baumbach, Dominik Dold, Alexander Kugele, Nico G\\\"urtler, Luziwei\n  Leng, Eric M\\\"uller, Christoph Koke, Mitja Kleider, Christian Mauch, Oliver\n  Breitwieser, Maurice G\\\"uttler, Dan Husmann, Kai Husmann, Joscha Ilmberger,\n  Andreas Hartel, Vitali Karasenko, Andreas Gr\\\"ubl, Johannes Schemmel,\n  Karlheinz Meier, and Mihai A. Petrovici", "title": "Accelerated physical emulation of Bayesian inference in spiking neural\n  networks", "comments": "This preprint has been published 2019 November 14. Please cite as:\n  Kungl A. F. et al. (2019) Accelerated Physical Emulation of Bayesian\n  Inference in Spiking Neural Networks. Front. Neurosci. 13:1201. doi:\n  10.3389/fnins.2019.01201", "journal-ref": "Frontiers in Neuroscience - Neuromorphic Engineering, 14 November\n  2019", "doi": "10.3389/fnins.2019.01201", "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massively parallel nature of biological information processing plays an\nimportant role for its superiority to human-engineered computing devices. In\nparticular, it may hold the key to overcoming the von Neumann bottleneck that\nlimits contemporary computer architectures. Physical-model neuromorphic devices\nseek to replicate not only this inherent parallelism, but also aspects of its\nmicroscopic dynamics in analog circuits emulating neurons and synapses.\nHowever, these machines require network models that are not only adept at\nsolving particular tasks, but that can also cope with the inherent\nimperfections of analog substrates. We present a spiking network model that\nperforms Bayesian inference through sampling on the BrainScaleS neuromorphic\nplatform, where we use it for generative and discriminative computations on\nvisual data. By illustrating its functionality on this platform, we implicitly\ndemonstrate its robustness to various substrate-specific distortive effects, as\nwell as its accelerated capability for computation. These results showcase the\nadvantages of brain-inspired physical computation and provide important\nbuilding blocks for large-scale neuromorphic applications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 13:03:00 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 13:17:57 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 19:04:01 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 11:36:41 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Kungl", "Akos F.", ""], ["Schmitt", "Sebastian", ""], ["Kl\u00e4hn", "Johann", ""], ["M\u00fcller", "Paul", ""], ["Baumbach", "Andreas", ""], ["Dold", "Dominik", ""], ["Kugele", "Alexander", ""], ["G\u00fcrtler", "Nico", ""], ["Leng", "Luziwei", ""], ["M\u00fcller", "Eric", ""], ["Koke", "Christoph", ""], ["Kleider", "Mitja", ""], ["Mauch", "Christian", ""], ["Breitwieser", "Oliver", ""], ["G\u00fcttler", "Maurice", ""], ["Husmann", "Dan", ""], ["Husmann", "Kai", ""], ["Ilmberger", "Joscha", ""], ["Hartel", "Andreas", ""], ["Karasenko", "Vitali", ""], ["Gr\u00fcbl", "Andreas", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "1807.02397", "submitter": "Daniele Gravina", "authors": "Daniele Gravina, Antonios Liapis, and Georgios N. Yannakakis", "title": "Quality Diversity Through Surprise", "comments": null, "journal-ref": "D. Gravina, A. Liapis and G. N. Yannakakis, \"Quality Diversity\n  Through Surprise,\" in IEEE Transactions on Evolutionary Computation, 2018", "doi": "10.1109/TEVC.2018.2877215", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality diversity is a recent family of evolutionary search algorithms which\nfocus on finding several well-performing (quality) yet different (diversity)\nsolutions with the aim to maintain an appropriate balance between divergence\nand convergence during search. While quality diversity has already delivered\npromising results in complex problems, the capacity of divergent search\nvariants for quality diversity remains largely unexplored. Inspired by the\nnotion of surprise as an effective driver of divergent search and its\northogonal nature to novelty this paper investigates the impact of the former\nto quality diversity performance. For that purpose we introduce three new\nquality diversity algorithms which employ surprise as a diversity measure,\neither on its own or combined with novelty, and compare their performance\nagainst novelty search with local competition, the state of the art quality\ndiversity algorithm. The algorithms are tested in a robot navigation task\nacross 60 highly deceptive mazes. Our findings suggest that allowing surprise\nand novelty to operate synergistically for divergence and in combination with\nlocal competition leads to quality diversity algorithms of significantly higher\nefficiency, speed and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 13:18:39 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 13:24:54 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 10:23:28 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 15:15:32 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Gravina", "Daniele", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1807.02477", "submitter": "Igor Grabec Prof.", "authors": "Igor Grabec, Eva \\v{S}vegl, Mihael Sok", "title": "Development of a sensory-neural network for medical diagnosing", "comments": "International symposium on neural networks 2018, Minsk, Belarus, June\n  25-28, 2018", "journal-ref": "Advances in Neural Networks - ISNN 2018, pp. 91-98, Part of\n  Springer \"Lecture Notes in Computer Science\" book series (LNCS, volume 10878)", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of a sensory-neural network developed for diagnosing of diseases\nis described. Information about patient's condition is provided by answers to\nthe questionnaire. Questions correspond to sensors generating signals when\npatients acknowledge symptoms. These signals excite neurons in which\ncharacteristics of the diseases are represented by synaptic weights associated\nwith indicators of symptoms. The disease corresponding to the most excited\nneuron is proposed as the result of diagnosing. Its reliability is estimated by\nthe likelihood defined by the ratio of excitation of the most excited neuron\nand the complete neural network.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 16:30:53 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Grabec", "Igor", ""], ["\u0160vegl", "Eva", ""], ["Sok", "Mihael", ""]]}, {"id": "1807.02564", "submitter": "Mohammed AL Zamil Prof.", "authors": "Mohammed AL Zamil and Samer Samarah", "title": "Applications of Data Mining Techniques for Vehicular Ad hoc Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent advances in vehicular ad hoc networks (VANETs), smart\napplications have been incorporating the data generated from these networks to\nprovide quality of life services. In this paper, we have proposed taxonomy of\ndata mining techniques that have been applied in this domain in addition to a\nclassification of these techniques. Our contribution is to highlight the\nresearch methodologies in the literature and allow for comparing among them\nusing different characteristics. The proposed taxonomy covers elementary data\nmining techniques such as: preprocessing, outlier detection, clustering, and\nclassification of data. In addition, it covers centralized, distributed,\noffline, and online techniques from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:41:18 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Zamil", "Mohammed AL", ""], ["Samarah", "Samer", ""]]}, {"id": "1807.02581", "submitter": "Stanislav Fort", "authors": "Stanislav Fort, Adam Scherlis", "title": "The Goldilocks zone: Towards better understanding of neural network loss\n  landscapes", "comments": "8 pages, 15 figures. Accepted for publication at the Thirty-Third\n  AAAI Conference on Artificial Intelligence (AAAI-19). A subset of the paper\n  accepted at Modern Trends in Nonconvex Optimization for Machine Learning\n  workshop at the 35th International Conference on Machine Learning (ICML\n  2018), and BayLearn 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the loss landscape of fully-connected and convolutional neural\nnetworks using random, low-dimensional hyperplanes and hyperspheres. Evaluating\nthe Hessian, $H$, of the loss function on these hypersurfaces, we observe 1) an\nunusual excess of the number of positive eigenvalues of $H$, and 2) a large\nvalue of $\\mathrm{Tr}(H) / ||H||$ at a well defined range of configuration\nspace radii, corresponding to a thick, hollow, spherical shell we refer to as\nthe \\textit{Goldilocks zone}. We observe this effect for fully-connected neural\nnetworks over a range of network widths and depths on MNIST and CIFAR-10\ndatasets with the $\\mathrm{ReLU}$ and $\\tanh$ non-linearities, and a similar\neffect for convolutional networks. Using our observations, we demonstrate a\nclose connection between the Goldilocks zone, measures of local\nconvexity/prevalence of positive curvature, and the suitability of a network\ninitialization. We show that the high and stable accuracy reached when\noptimizing on random, low-dimensional hypersurfaces is directly related to the\noverlap between the hypersurface and the Goldilocks zone, and as a corollary\ndemonstrate that the notion of intrinsic dimension is initialization-dependent.\nWe note that common initialization techniques initialize neural networks in\nthis particular region of unusually high convexity/prevalence of positive\ncurvature, and offer a geometric intuition for their success. Furthermore, we\ndemonstrate that initializing a neural network at a number of points and\nselecting for high measures of local convexity such as $\\mathrm{Tr}(H) /\n||H||$, number of positive eigenvalues of $H$, or low initial loss, leads to\nstatistically significantly faster training on MNIST. Based on our\nobservations, we hypothesize that the Goldilocks zone contains an unusually\nhigh density of suitable initialization configurations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 22:31:53 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 10:29:42 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Fort", "Stanislav", ""], ["Scherlis", "Adam", ""]]}, {"id": "1807.02621", "submitter": "Juan-Pablo Ortega", "authors": "Lukas Gonon and Juan-Pablo Ortega", "title": "Reservoir Computing Universality With Stochastic Inputs", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The universal approximation properties with respect to $L ^p $-type criteria\nof three important families of reservoir computers with stochastic\ndiscrete-time semi-infinite inputs is shown. First, it is proved that linear\nreservoir systems with either polynomial or neural network readout maps are\nuniversal. More importantly, it is proved that the same property holds for two\nfamilies with linear readouts, namely, trigonometric state-affine systems and\necho state networks, which are the most widely used reservoir systems in\napplications. The linearity in the readouts is a key feature in supervised\nmachine learning applications. It guarantees that these systems can be used in\nhigh-dimensional situations and in the presence of large datasets. The $L ^p $\ncriteria used in this paper allow the formulation of universality results that\ndo not necessarily impose almost sure uniform boundedness in the inputs or the\nfading memory property in the filter that needs to be approximated.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 06:33:13 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Gonon", "Lukas", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1807.02754", "submitter": "Ameer Tamoor Khan", "authors": "Ameer Tamoor Khan, Shuai Li Senior, Predrag S. Stanimirovic, Yinyan\n  Zhang", "title": "Model-Free Optimization Using Eagle Perching Optimizer", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel nature-inspired technique of optimization. It\nmimics the perching nature of eagles and uses mathematical formulations to\nintroduce a new addition to metaheuristic algorithms. The nature of the\nproposed algorithm is based on exploration and exploitation. The proposed\nalgorithm is developed into two versions with some modifications. In the first\nphase, it undergoes a rigorous analysis to find out their performance. In the\nsecond phase it is benchmarked using ten functions of two categories; uni-modal\nfunctions and multi-modal functions. In the third phase, we conducted a\ndetailed analysis of the algorithm by exploiting its controlling units or\nvariables. In the fourth and last phase, we consider real world optimization\nproblems with constraints. Both versions of the algorithm show an appreciable\nperformance, but analysis puts more weight to the modified version. The\ncompetitive analysis shows that the proposed algorithm outperforms the other\ntested metaheuristic algorithms. The proposed method has better robustness and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 04:49:48 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Khan", "Ameer Tamoor", ""], ["Senior", "Shuai Li", ""], ["Stanimirovic", "Predrag S.", ""], ["Zhang", "Yinyan", ""]]}, {"id": "1807.03001", "submitter": "C. H. Huck Yang", "authors": "C. H. Huck Yang, Rise Ooi, Tom Hiscock, Victor Eguiluz, Jesper\n  Tegn\\'er", "title": "Learning Functions in Large Networks requires Modularity and produces\n  Multi-Agent Dynamics", "comments": "Accepted at the Joint ICML and IJCAI Workshop on Computational\n  Biology (ICML-IJCAI WCB) to be held in Stockholm SWEDEN, 2018. Referring to\n  https://sites.google.com/view/wcb2018/accepted-papers?authuser=0 update the\n  team-learning figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Networks are abundant in biological systems. Small sized over-represented\nnetwork motifs have been discovered, and it has been suggested that these\nconstitute functional building blocks. We ask whether larger dynamical network\nmotifs exist in biological networks, thus contributing to the higher-order\norganization of a network. To end this, we introduce a gradient descent machine\nlearning (ML) approach and genetic algorithms to learn larger functional motifs\nin contrast to an (unfeasible) exhaustive search. We use the French Flag (FF)\nand Switch functional motif as case studies motivated from biology. While our\nalgorithm successfully learns large functional motifs, we identify a threshold\nsize of approximately 20 nodes beyond which learning breaks down. Therefore we\ninvestigate the stability of the motifs. We find that the size of the real\nnegative eigenvalues of the Jacobian decreases with increasing system size,\nthus conferring instability. Finally, without imposing learning an input-output\nfor all the components of the network, we observe that unconstrained middle\ncomponents of the network still learn the desired function, a form of\nhomogeneous team learning. We conclude that the size limitation of\nlearnability, most likely due to stability constraints, impose a definite\nrequirement for modularity in networked systems while enabling team learning\nwithin unconstrained parts of the module. Thus, the observation that community\nstructures and modularity are abundant in biological networks could be\naccounted for by a computational compositional network structure.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:25:06 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 21:26:16 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Yang", "C. H. Huck", ""], ["Ooi", "Rise", ""], ["Hiscock", "Tom", ""], ["Eguiluz", "Victor", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1807.03010", "submitter": "Francesco Conti", "authors": "Francesco Conti, Pasquale Davide Schiavone and Luca Benini", "title": "XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary\n  Neural Network Inference", "comments": "11 pages, 8 figures, 2 tables, 3 listings. Accepted for presentation\n  at CODES'18 and for publication in IEEE Transactions on Computer-Aided Design\n  of Circuits and Systems (TCAD) as part of the ESWEEK-TCAD special issue", "journal-ref": null, "doi": "10.1109/TCAD.2018.2857019", "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) are promising to deliver accuracy comparable to\nconventional deep neural networks at a fraction of the cost in terms of memory\nand energy. In this paper, we introduce the XNOR Neural Engine (XNE), a fully\ndigital configurable hardware accelerator IP for BNNs, integrated within a\nmicrocontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid\nSRAM / standard cell memory. The XNE is able to fully compute convolutional and\ndense layers in autonomy or in cooperation with the core in the MCU to realize\nmore complex behaviors. We show post-synthesis results in 65nm and 22nm\ntechnology for the XNE IP and post-layout results in 22nm for the full MCU\nindicating that this system can drop the energy cost per binary operation to\n21.6fJ per operation at 0.4V, and at the same time is flexible and performant\nenough to execute state-of-the-art BNN topologies such as ResNet-34 in less\nthan 2.2mJ per frame at 8.9 fps.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:40:37 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Conti", "Francesco", ""], ["Schiavone", "Pasquale Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1807.03165", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Hayden Jananthan, Lauren Milechin, Sid\n  Samsi", "title": "Sparse Deep Neural Network Exact Solutions", "comments": "8 pages, 10 figures, accepted to IEEE HPEC 2018. arXiv admin note:\n  text overlap with arXiv:1708.02937", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547742", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have emerged as key enablers of machine learning.\nApplying larger DNNs to more diverse applications is an important challenge.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nlayers and more neurons per layers, these weight matrices may be required to be\nsparse because of memory limitations. Sparse DNNs are one possible approach,\nbut the underlying theory is in the early stages of development and presents a\nnumber of challenges, including determining the accuracy of inference and\nselecting nonzero weights for training. Associative array algebra has been\ndeveloped by the big data community to combine and extend database, matrix, and\ngraph/network concepts for use in large, sparse data problems. Applying this\nmathematics to DNNs simplifies the formulation of DNN mathematics and reveals\nthat DNNs are linear over oscillating semirings. This work uses associative\narray DNNs to construct exact solutions and corresponding perturbation models\nto the rectified linear unit (ReLU) DNN equations that can be used to construct\ntest vectors for sparse DNN implementations over various precisions. These\nsolutions can be used for DNN verification, theoretical explorations of DNN\nproperties, and a starting point for the challenge of sparse training.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 00:47:12 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Milechin", "Lauren", ""], ["Samsi", "Sid", ""]]}, {"id": "1807.03215", "submitter": "Fenglei Fan", "authors": "Fenglei Fan, Ge Wang", "title": "Fuzzy Logic Interpretation of Quadratic Networks", "comments": "10 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over past several years, deep learning has achieved huge successes in various\napplications. However, such a data-driven approach is often criticized for lack\nof interpretability. Recently, we proposed artificial quadratic neural networks\nconsisting of second-order neurons in potentially many layers. In each\nsecond-order neuron, a quadratic function is used in the place of the inner\nproduct in a traditional neuron, and then undergoes a nonlinear activation.\nWith a single second-order neuron, any fuzzy logic operation, such as XOR, can\nbe implemented. In this sense, any deep network constructed with quadratic\nneurons can be interpreted as a deep fuzzy logic system. Since traditional\nneural networks and second-order counterparts can represent each other and\nfuzzy logic operations are naturally implemented in second-order neural\nnetworks, it is plausible to explain how a deep neural network works with a\nsecond-order network as the system model. In this paper, we generalize and\ncategorize fuzzy logic operations implementable with individual second-order\nneurons, and then perform statistical/information theoretic analyses of\nexemplary quadratic neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 12:45:25 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 19:57:58 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 00:21:40 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Fan", "Fenglei", ""], ["Wang", "Ge", ""]]}, {"id": "1807.03346", "submitter": "Maisa Daoud", "authors": "Maisa Doaud and Michael Mayo", "title": "Using Swarm Optimization To Enhance Autoencoders Images", "comments": "14 pages,2 figures and 17 graphs, conference paper (MDAI2017)\n  http://mdai.cat/mdai2017/mdai2017.usb.pdf", "journal-ref": "In The 14th International Conference on Modeling Decisions for\n  Artificial Intelligence. USB Proceedings., pages 118-131, Kitakyushu, Japan\n  (October 18 - 20, 2017), 2017", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders learn data representations through reconstruction. Robust\ntraining is the key factor affecting the quality of the learned representations\nand, consequently, the accuracy of the application that use them. Previous\nworks suggested methods for deciding the optimal autoencoder configuration\nwhich allows for robust training. Nevertheless, improving the accuracy of a\ntrained autoencoder has got limited, if no, attention. We propose a new\napproach that improves the accuracy of a trained autoencoders results and\nanswers the following question, Given a trained autoencoder, a test image, and\nusing a real-parameter optimizer, can we generate better quality reconstructed\nimage version than the one generated by the autoencoder?. Our proposed approach\ncombines both the decoder part of a trained Resitricted Boltman Machine-based\nautoencoder with the Competitive Swarm Optimization algorithm. Experiments show\nthat it is possible to reconstruct images using trained decoder from randomly\ninitialized representations. Results also show that our approach reconstructed\nbetter quality images than the autoencoder in most of the test cases.\nIndicating that, we can use the approach for improving the performance of a\npre-trained autoencoder if it does not give satisfactory results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 19:15:30 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Doaud", "Maisa", ""], ["Mayo", "Michael", ""]]}, {"id": "1807.03361", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Marc Modat, Eli Gibson, Wenqi Li, Nooshin Ghavami, Ester\n  Bonmati, Guotai Wang, Steven Bandula, Caroline M. Moore, Mark Emberton,\n  S\\'ebastien Ourselin, J. Alison Noble, Dean C. Barratt, Tom Vercauteren", "title": "Weakly-Supervised Convolutional Neural Networks for Multimodal Image\n  Registration", "comments": "Accepted manuscript in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2018.07.002", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental challenges in supervised learning for multimodal image\nregistration is the lack of ground-truth for voxel-level spatial\ncorrespondence. This work describes a method to infer voxel-level\ntransformation from higher-level correspondence information contained in\nanatomical labels. We argue that such labels are more reliable and practical to\nobtain for reference sets of image pairs than voxel-level correspondence.\nTypical anatomical labels of interest may include solid organs, vessels, ducts,\nstructure boundaries and other subject-specific ad hoc landmarks. The proposed\nend-to-end convolutional neural network approach aims to predict displacement\nfields to align multiple labelled corresponding structures for individual image\npairs during the training, while only unlabelled image pairs are used as the\nnetwork input for inference. We highlight the versatility of the proposed\nstrategy, for training, utilising diverse types of anatomical labels, which\nneed not to be identifiable over all training image pairs. At inference, the\nresulting 3D deformable image registration algorithm runs in real-time and is\nfully-automated without requiring any anatomical labels or initialisation.\nSeveral network architecture variants are compared for registering T2-weighted\nmagnetic resonance images and 3D transrectal ultrasound images from prostate\ncancer patients. A median target registration error of 3.6 mm on landmark\ncentroids and a median Dice of 0.87 on prostate glands are achieved from\ncross-validation experiments, in which 108 pairs of multimodal images from 76\npatients were tested with high-quality anatomical labels.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 19:53:16 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Hu", "Yipeng", ""], ["Modat", "Marc", ""], ["Gibson", "Eli", ""], ["Li", "Wenqi", ""], ["Ghavami", "Nooshin", ""], ["Bonmati", "Ester", ""], ["Wang", "Guotai", ""], ["Bandula", "Steven", ""], ["Moore", "Caroline M.", ""], ["Emberton", "Mark", ""], ["Ourselin", "S\u00e9bastien", ""], ["Noble", "J. Alison", ""], ["Barratt", "Dean C.", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1807.03392", "submitter": "Joost Huizinga", "authors": "Joost Huizinga and Jeff Clune", "title": "Evolving Multimodal Robot Behavior via Many Stepping Stones with the\n  Combinatorial Multi-Objective Evolutionary Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in reinforcement learning, including evolutionary\nrobotics, is to solve multimodal problems, where agents have to act in\nqualitatively different ways depending on the circumstances. Because multimodal\nproblems are often too difficult to solve directly, it is helpful to take\nadvantage of staging, where a difficult task is divided into simpler subtasks\nthat can serve as stepping stones for solving the overall problem.\nUnfortunately, choosing an effective ordering for these subtasks is difficult,\nand a poor ordering can reduce the speed and performance of the learning\nprocess. Here, we provide a thorough introduction and investigation of the\nCombinatorial Multi-Objective Evolutionary Algorithm (CMOEA), which avoids\nordering subtasks by allowing all combinations of subtasks to be explored\nsimultaneously. We compare CMOEA against two algorithms that can similarly\noptimize on multiple subtasks simultaneously: NSGA-II and Lexicase Selection.\nThe algorithms are tested on a multimodal robotics problem with six subtasks as\nwell as a maze navigation problem with a hundred subtasks. On these problems,\nCMOEA either outperforms or is competitive with the controls. Separately, we\nshow that adding a linear combination over all objectives can improve the\nability of NSGA-II to solve these multimodal problems. Lastly, we show that, in\ncontrast to NSGA-II and Lexicase Selection, CMOEA can effectively leverage\nsecondary objectives to achieve state-of-the-art results on the robotics task.\nIn general, our experiments suggest that CMOEA is a promising, state-of-the-art\nalgorithm for solving multimodal problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 21:23:36 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 21:58:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Huizinga", "Joost", ""], ["Clune", "Jeff", ""]]}, {"id": "1807.03403", "submitter": "Carola Doerr", "authors": "Benjamin Doerr, Carola Doerr, Jing Yang", "title": "Optimal Parameter Choices via Precise Black-Box Analysis", "comments": "Thoroughly revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed that some working principles of evolutionary algorithms,\nin particular, the influence of the parameters, cannot be understood from\nresults on the asymptotic order of the runtime, but only from more precise\nresults. In this work, we complement the emerging topic of precise runtime\nanalysis with a first precise complexity theoretic result. Our vision is that\nthe interplay between algorithm analysis and complexity theory becomes a\nfruitful tool also for analyses more precise than asymptotic orders of\nmagnitude.\n  As particular result, we prove that the unary unbiased black-box complexity\nof the OneMax benchmark function class is $n \\ln(n) - cn \\pm o(n)$ for a\nconstant $c$ which is between $0.2539$ and $0.2665$. This runtime can be\nachieved with a simple (1+1)-type algorithm using a fitness-dependent mutation\nstrength. When translated into the fixed-budget perspective, our algorithm\nfinds solutions which are roughly 13\\% closer to the optimum than those of the\nbest previously known algorithms. To prove our results, we formulate several\nnew versions of the variable drift theorems, which also might be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 22:04:30 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 07:56:20 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Yang", "Jing", ""]]}, {"id": "1807.03478", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "An Adaptive Learning Method of Restricted Boltzmann Machine by Neuron\n  Generation and Annihilation Algorithm", "comments": "6 pages, 6 figures", "journal-ref": "Proc. of 2016 IEEE International Conference on Systems, Man, and\n  Cybernetics (IEEE SMC 2016)", "doi": "10.1109/SMC.2016.7844417", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based\nmodel of artificial neural network for unsupervised learning. Recently, RBM is\nwell known to be a pre-training method of Deep Learning. In addition to visible\nand hidden neurons, the structure of RBM has a number of parameters such as the\nweights between neurons and the coefficients for them. Therefore, we may meet\nsome difficulties to determine an optimal network structure to analyze big\ndata. In order to evade the problem, we investigated the variance of parameters\nto find an optimal structure during learning. For the reason, we should check\nthe variance of parameters to cause the fluctuation for energy function in RBM\nmodel. In this paper, we propose the adaptive learning method of RBM that can\ndiscover an optimal number of hidden neurons according to the training\nsituation by applying the neuron generation and annihilation algorithm. In this\nmethod, a new hidden neuron is generated if the energy function is not still\nconverged and the variance of the parameters is large. Moreover, the\ninactivated hidden neuron will be annihilated if the neuron does not affect the\nlearning situation. The experimental results for some benchmark data sets were\ndiscussed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 04:39:18 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:06:52 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.03486", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "An Adaptive Learning Method of Deep Belief Network by Layer Generation\n  Algorithm", "comments": "4 pages, 2 figures, Proc. of 2016 IEEE Region 10 Conference\n  (TENCON2016)", "journal-ref": null, "doi": "10.1109/TENCON.2016.7848589", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Belief Network (DBN) has a deep architecture that represents multiple\nfeatures of input patterns hierarchically with the pre-trained Restricted\nBoltzmann Machines (RBM). A traditional RBM or DBN model cannot change its\nnetwork structure during the learning phase. Our proposed adaptive learning\nmethod can discover the optimal number of hidden neurons and weights and/or\nlayers according to the input space. The model is an important method to take\naccount of the computational cost and the model stability. The regularities to\nhold the sparse structure of network is considerable problem, since the\nextraction of explicit knowledge from the trained network should be required.\nIn our previous research, we have developed the hybrid method of adaptive\nstructural learning method of RBM and Learning Forgetting method to the trained\nRBM. In this paper, we propose the adaptive learning method of DBN that can\ndetermine the optimal number of layers during the learning. We evaluated our\nproposed model on some benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 05:55:26 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:04:14 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.03487", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "Fine Tuning Method by using Knowledge Acquisition from Deep Belief\n  Network", "comments": "6 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1807.03486", "journal-ref": "Proc. of IEEE 9th International Workshop on Computational\n  Intelligence and Applications (IWCIA2016)", "doi": "10.1109/IWCIA.2016.7805759", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an adaptive structure learning method of Restricted Boltzmann\nMachine (RBM) which can generate/annihilate neurons by self-organizing learning\nmethod according to input patterns. Moreover, the adaptive Deep Belief Network\n(DBN) in the assemble process of pre-trained RBM layer was developed. The\nproposed method presents to score a great success to the training data set for\nbig data benchmark test such as CIFAR-10. However, the classification\ncapability of the test data set, which are included unknown patterns, is high,\nbut does not lead perfect correct solution. We investigated the wrong specified\ndata and then some characteristic patterns were found. In this paper, the\nknowledge related to the patterns is embedded into the classification algorithm\nof trained DBN. As a result, the classification capability can achieve a great\nsuccess (97.1\\% to unknown data set).\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 06:07:13 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 07:54:26 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.03495", "submitter": "Martin Krejca", "authors": "Benjamin Doerr and Martin Krejca", "title": "Significance-based Estimation-of-Distribution Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation-of-distribution algorithms (EDAs) are randomized search heuristics\nthat create a probabilistic model of the solution space, which is updated\niteratively, based on the quality of the solutions sampled according to the\nmodel. As previous works show, this iteration-based perspective can lead to\nerratic updates of the model, in particular, to bit-frequencies approaching a\nrandom boundary value.\n  In order to overcome this problem, we propose a new EDA based on the classic\ncompact genetic algorithm (cGA) that takes into account a longer history of\nsamples and updates its model only with respect to information which it\nclassifies as statistically significant. We prove that this significance-based\ncompact genetic algorithm (sig-cGA) optimizes the commonly regarded benchmark\nfunctions OneMax, LeadingOnes, and BinVal all in quasilinear time, a result\nshown for no other EDA or evolutionary algorithm so far.\n  For the recently proposed scGA -- an EDA that tries to prevent erratic model\nupdates by imposing a bias to the uniformly distributed model -- we prove that\nit optimizes OneMax only in a time exponential in its hypothetical population\nsize. Similarly, we show that the convex search algorithm cannot optimize\nOneMax in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 06:35:57 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 07:15:18 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 16:16:16 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Doerr", "Benjamin", ""], ["Krejca", "Martin", ""]]}, {"id": "1807.03523", "submitter": "Andr\\'es Camero", "authors": "Andr\\'es Camero, Jamal Toutouh and Enrique Alba", "title": "DLOPT: Deep Learning Optimization Library", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning hyper-parameter optimization is a tough task. Finding an\nappropriate network configuration is a key to success, however most of the\ntimes this labor is roughly done. In this work we introduce a novel library to\ntackle this problem, the Deep Learning Optimization Library: DLOPT. We briefly\ndescribe its architecture and present a set of use examples. This is an open\nsource project developed under the GNU GPL v3 license and it is freely\navailable at https://github.com/acamero/dlopt\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 08:34:25 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Camero", "Andr\u00e9s", ""], ["Toutouh", "Jamal", ""], ["Alba", "Enrique", ""]]}, {"id": "1807.03710", "submitter": "Timothy Wong", "authors": "Timothy Wong, Zhiyuan Luo", "title": "Recurrent Auto-Encoder Model for Large-Scale Industrial Sensor Signal\n  Analysis", "comments": "Accepted paper at the 19th International Conference on Engineering\n  Applications of Neural Networks (EANN 2018)", "journal-ref": "E. Pimenidis and C. Jayne (Eds.): EANN 2018, CCIS 893", "doi": "10.1007/978-3-319-98204-5_17", "report-no": null, "categories": "cs.LG cs.AI cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent auto-encoder model summarises sequential data through an encoder\nstructure into a fixed-length vector and then reconstructs the original\nsequence through the decoder structure. The summarised vector can be used to\nrepresent time series features. In this paper, we propose relaxing the\ndimensionality of the decoder output so that it performs partial\nreconstruction. The fixed-length vector therefore represents features in the\nselected dimensions only. In addition, we propose using rolling fixed window\napproach to generate training samples from unbounded time series data. The\nchange of time series features over time can be summarised as a smooth\ntrajectory path. The fixed-length vectors are further analysed using additional\nvisualisation and unsupervised clustering techniques. The proposed method can\nbe applied in large-scale industrial processes for sensors signal analysis\npurpose, where clusters of the vector representations can reflect the operating\nstates of the industrial system.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 15:26:33 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Wong", "Timothy", ""], ["Luo", "Zhiyuan", ""]]}, {"id": "1807.03952", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "Shortening Time Required for Adaptive Structural Learning Method of Deep\n  Belief Network with Multi-Modal Data Arrangement", "comments": "6 pages, 5 figures, Proc. of IEEE 10th International Workshop on\n  Computational Intelligence and Applications (IWCIA2017)", "journal-ref": null, "doi": "10.1109/IWCIA.2017.8203568", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Learning has been applied in the techniques of artificial\nintelligence. Especially, Deep Learning performed good results in the field of\nimage recognition. Most new Deep Learning architectures are naturally developed\nin image recognition. For this reason, not only the numerical data and text\ndata but also the time-series data are transformed to the image data format.\nMulti-modal data consists of two or more kinds of data such as picture and\ntext. The arrangement in a general method is formed in the squared array with\nno specific aim. In this paper, the data arrangement are modified according to\nthe similarity of input-output pattern in Adaptive Structural Learning method\nof Deep Belief Network. The similarity of output signals of hidden neurons is\nmade by the order rearrangement of hidden neurons. The experimental results for\nthe data rearrangement in squared array showed the shortening time required for\nDBN learning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 05:30:20 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.03953", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Shin Kamada", "title": "Adaptive Learning Method of Recurrent Temporal Deep Belief Network to\n  Analyze Time Series Data", "comments": "8 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1807.03487, arXiv:1807.03486", "journal-ref": "Proc. of The International Joint Conference on Neural Networks\n  (IJCNN 2017)", "doi": "10.1109/IJCNN.2017.7966140", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has the hierarchical network architecture to represent the\ncomplicated features of input patterns. Such architecture is well known to\nrepresent higher learning capability compared with some conventional models if\nthe best set of parameters in the optimal network structure is found. We have\nbeen developing the adaptive learning method that can discover the optimal\nnetwork structure in Deep Belief Network (DBN). The learning method can\nconstruct the network structure with the optimal number of hidden neurons in\neach Restricted Boltzmann Machine and with the optimal number of layers in the\nDBN during learning phase. The network structure of the learning method can be\nself-organized according to given input patterns of big data set. In this\npaper, we embed the adaptive learning method into the recurrent temporal RBM\nand the self-generated layer into DBN. In order to verify the effectiveness of\nour proposed method, the experimental results are higher classification\ncapability than the conventional methods in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 05:34:32 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Ichimura", "Takumi", ""], ["Kamada", "Shin", ""]]}, {"id": "1807.03954", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "Knowledge Extracted from Recurrent Deep Belief Network for Real Time\n  Deterministic Control", "comments": "6 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1807.03953", "journal-ref": "Proc. of 2017 IEEE International Conference on Systems, Man, and\n  Cybernetics (IEEE SMC2017)", "doi": "10.1109/SMC.2017.8122711", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the market on deep learning including not only software but also\nhardware is developing rapidly. Big data is collected through IoT devices and\nthe industry world will analyze them to improve their manufacturing process.\nDeep Learning has the hierarchical network architecture to represent the\ncomplicated features of input patterns. Although deep learning can show the\nhigh capability of classification, prediction, and so on, the implementation on\nGPU devices are required. We may meet the trade-off between the higher\nprecision by deep learning and the higher cost with GPU devices. We can success\nthe knowledge extraction from the trained deep learning with high\nclassification capability. The knowledge that can realize faster inference of\npre-trained deep network is extracted as IF-THEN rules from the network signal\nflow given input data. Some experiment results with benchmark tests for time\nseries data sets showed the effectiveness of our proposed method related to the\ncomputational speed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 05:37:02 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.04013", "submitter": "Yongming Shen", "authors": "Yongming Shen (1), Tianchu Ji (1), Michael Ferdman (1), Peter Milder\n  (1) ((1) Stony Brook University)", "title": "Medusa: A Scalable Interconnect for Many-Port DNN Accelerators and Wide\n  DRAM Controller Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the increasing demand and computational intensity of deep neural\nnetworks (DNNs), industry and academia have turned to accelerator technologies.\nIn particular, FPGAs have been shown to provide a good balance between\nperformance and energy efficiency for accelerating DNNs. While significant\nresearch has focused on how to build efficient layer processors, the\ncomputational building blocks of DNN accelerators, relatively little attention\nhas been paid to the on-chip interconnects that sit between the layer\nprocessors and the FPGA's DRAM controller.\n  We observe a disparity between DNN accelerator interfaces, which tend to\ncomprise many narrow ports, and FPGA DRAM controller interfaces, which tend to\nbe wide buses. This mismatch causes traditional interconnects to consume\nsignificant FPGA resources. To address this problem, we designed Medusa: an\noptimized FPGA memory interconnect which transposes data in the interconnect\nfabric, tailoring the interconnect to the needs of DNN layer processors.\nCompared to a traditional FPGA interconnect, our design can reduce LUT and FF\nuse by 4.7x and 6.0x, and improves frequency by 1.8x.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 09:06:20 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Shen", "Yongming", "", "Stony Brook University"], ["Ji", "Tianchu", "", "Stony Brook University"], ["Ferdman", "Michael", "", "Stony Brook University"], ["Milder", "Peter", "", "Stony Brook University"]]}, {"id": "1807.04065", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Danilo Comminiello, Simone\n  Totaro, Aurelio Uncini", "title": "Recurrent Neural Networks with Flexible Gates using Kernel Activation\n  Functions", "comments": "Accepted for presentation at 2018 IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gated recurrent neural networks have achieved remarkable results in the\nanalysis of sequential data. Inside these networks, gates are used to control\nthe flow of information, allowing to model even very long-term dependencies in\nthe data. In this paper, we investigate whether the original gate equation (a\nlinear projection followed by an element-wise sigmoid) can be improved. In\nparticular, we design a more flexible architecture, with a small number of\nadaptable parameters, which is able to model a wider range of gating functions\nthan the classical one. To this end, we replace the sigmoid function in the\nstandard gate with a non-parametric formulation extending the recently proposed\nkernel activation function (KAF), with the addition of a residual\nskip-connection. A set of experiments on sequential variants of the MNIST\ndataset shows that the adoption of this novel gate allows to improve accuracy\nwith a negligible cost in terms of computational power and with a large\nspeed-up in the number of training iterations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:54:46 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Comminiello", "Danilo", ""], ["Totaro", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1807.04098", "submitter": "C. H. Bryan Liu", "authors": "Georg L. Grob, \\^Angelo Cardoso, C. H. Bryan Liu, Duncan A. Little,\n  Benjamin Paul Chamberlain", "title": "A Recurrent Neural Network Survival Model: Predicting Web User Return\n  Time", "comments": "Accepted into ECML PKDD 2018; 8 figures and 1 table", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2018. Lecture Notes in Computer Science, vol 11053. pp 152-168", "doi": "10.1007/978-3-030-10997-4_10", "report-no": null, "categories": "cs.LG cs.CY cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The size of a website's active user base directly affects its value. Thus, it\nis important to monitor and influence a user's likelihood to return to a site.\nEssential to this is predicting when a user will return. Current state of the\nart approaches to solve this problem come in two flavors: (1) Recurrent Neural\nNetwork (RNN) based solutions and (2) survival analysis methods. We observe\nthat both techniques are severely limited when applied to this problem.\nSurvival models can only incorporate aggregate representations of users instead\nof automatically learning a representation directly from a raw time series of\nuser actions. RNNs can automatically learn features, but can not be directly\ntrained with examples of non-returning users who have no target value for their\nreturn time. We develop a novel RNN survival model that removes the limitations\nof the state of the art methods. We demonstrate that this model can\nsuccessfully be applied to return time prediction on a large e-commerce dataset\nwith a superior ability to discriminate between returning and non-returning\nusers than either method applied in isolation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 12:12:48 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Grob", "Georg L.", ""], ["Cardoso", "\u00c2ngelo", ""], ["Liu", "C. H. Bryan", ""], ["Little", "Duncan A.", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1807.04118", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Takuya Uemoto, Akira Hara", "title": "Emergence of Altruism Behavior for Multi Feeding Areas in Army Ant\n  Social Evolutionary System", "comments": "6 pages, 11 figures", "journal-ref": "Proc. of 2014 IEEE International Conference on Systems, Man, and\n  Cybernetics (IEEE SMC 2014)", "doi": "10.1109/SMC.2014.6973902", "report-no": null, "categories": "cs.MA cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Army ants perform the altruism that an ant sacrifices its own well-being for\nthe benefit of another ants. Army ants build bridges using their own bodies\nalong the path from a food to the nest. We developed the army ant inspired\nsocial evolutionary system which can perform the altruism. The system has 2\nkinds of ant agents, `Major ant' and `Minor ant' and the ants communicate with\neach other via pheromones. One ants can recognize them as the signals from the\nother ants. The pheromones evaporate with the certain ratio and diffused into\nthe space of neighbors stochastically. If the optimal bridge is found, the path\nthrough the bridge is the shortest route from foods to the nest. We define the\nprobability for an ant to leave a bridge at a low occupancy condition of ants\nand propose the constructing method of the optimal route. In this paper, the\nbehaviors of ant under the environment with two or more feeding spots are\nobserved. Some experimental results show the behaviors of great interest with\nrespect to altruism of ants. The description in some computer simulation is\nreported in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 04:40:38 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Ichimura", "Takumi", ""], ["Uemoto", "Takuya", ""], ["Hara", "Akira", ""]]}, {"id": "1807.04505", "submitter": "Stefano Nichele", "authors": "Erik Aaron Hansen, Stefano Nichele, Anis Yazidi, H{\\aa}rek Haugerud,\n  Asieh Abolpour Mofrad, Alex Alcocer", "title": "Achieving Connectivity Between Wide Areas Through Self-Organising Robot\n  Swarm Using Embodied Evolution", "comments": "Submitted to IEEE ICES 2018 (SSCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abruptions to the communication infrastructure happens occasionally, where\nmanual dedicated personnel will go out to fix the interruptions, restoring\ncommunication abilities. However, sometimes this can be dangerous to the\npersonnel carrying out the task, which can be the case in war situations,\nenvironmental disasters like earthquakes or toxic spills or in the occurrence\nof fire. Therefore, human casualties can be minimised if autonomous robots are\ndeployed that can achieve the same outcome: to establish a communication link\nbetween two previously distant but connected sites. In this paper we\ninvestigate the deployment of mobile ad hoc robots which relay traffic between\nthem. In order to get the robots to locate themselves appropriately, we take\ninspiration from self-organisation and emergence in artificial life, where a\ncommon overall goal may be achieved if the correct local rules on the agents in\nsystem are invoked. We integrate the aspect of connectivity between two sites\ninto the multirobot simulation platform known as JBotEvolver. The robot swarm\nis composed of Thymio II robots. In addition, we compare three heuristics, of\nwhich one uses neuroevolution (evolution of neural networks) to show how\nself-organisation and embodied evolution can be used within the integration.\nOur use of embodiment in robotic controllers shows promising results and\nprovide solid knowledge and guidelines for further investigations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 10:08:21 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Hansen", "Erik Aaron", ""], ["Nichele", "Stefano", ""], ["Yazidi", "Anis", ""], ["Haugerud", "H\u00e5rek", ""], ["Mofrad", "Asieh Abolpour", ""], ["Alcocer", "Alex", ""]]}, {"id": "1807.04587", "submitter": "Sergey Bartunov", "authors": "Sergey Bartunov, Adam Santoro, Blake A. Richards, Luke Marris,\n  Geoffrey E. Hinton, Timothy Lillicrap", "title": "Assessing the Scalability of Biologically-Motivated Deep Learning\n  Algorithms and Architectures", "comments": "NIPS 2018. Version 2 contains more experimental data including best\n  hyperparameters found", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation of error algorithm (BP) is impossible to implement in a\nreal brain. The recent success of deep networks in machine learning and AI,\nhowever, has inspired proposals for understanding how the brain might learn\nacross multiple layers, and hence how it might approximate BP. As of yet, none\nof these proposals have been rigorously evaluated on tasks where BP-guided deep\nlearning has proved critical, or in architectures more structured than simple\nfully-connected networks. Here we present results on scaling up biologically\nmotivated models of deep learning on datasets which need deep networks with\nappropriate architectures to achieve good performance. We present results on\nthe MNIST, CIFAR-10, and ImageNet datasets and explore variants of\ntarget-propagation (TP) and feedback alignment (FA) algorithms, and explore\nperformance in both fully- and locally-connected architectures. We also\nintroduce weight-transport-free variants of difference target propagation (DTP)\nmodified to remove backpropagation from the penultimate layer. Many of these\nalgorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP\nand FA variants perform significantly worse than BP, especially for networks\ncomposed of locally connected units, opening questions about whether new\narchitectures and algorithms are required to scale these approaches. Our\nresults and implementation details help establish baselines for biologically\nmotivated deep learning schemes going forward.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 12:53:50 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 14:26:44 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Bartunov", "Sergey", ""], ["Santoro", "Adam", ""], ["Richards", "Blake A.", ""], ["Marris", "Luke", ""], ["Hinton", "Geoffrey E.", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1807.04640", "submitter": "Michael Chang", "authors": "Michael B. Chang, Abhishek Gupta, Sergey Levine, Thomas L. Griffiths", "title": "Automatically Composing Representation Transformations as a Means for\n  Generalization", "comments": "Accepted to the International Conference on Learning Representations\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generally intelligent learner should generalize to more complex tasks than\nit has previously encountered, but the two common paradigms in machine learning\n-- either training a separate learner per task or training a single learner for\nall tasks -- both have difficulty with such generalization because they do not\nleverage the compositional structure of the task distribution. This paper\nintroduces the compositional problem graph as a broadly applicable formalism to\nrelate tasks of different complexity in terms of problems with shared\nsubproblems. We propose the compositional generalization problem for measuring\nhow readily old knowledge can be reused and hence built upon. As a first step\nfor tackling compositional generalization, we introduce the compositional\nrecursive learner, a domain-general framework for learning algorithmic\nprocedures for composing representation transformations, producing a learner\nthat reasons about what computation to execute by making analogies to\npreviously seen problems. We show on a symbolic and a high-dimensional domain\nthat our compositional approach can generalize to more complex problems than\nthe learner has previously encountered, whereas baselines that are not\nexplicitly compositional do not.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 14:33:49 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 06:41:28 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Chang", "Michael B.", ""], ["Gupta", "Abhishek", ""], ["Levine", "Sergey", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1807.04723", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Chinnadhurai Sankar, Michael Pieper, Joelle\n  Pineau, Yoshua Bengio", "title": "The Bottleneck Simulator: A Model-based Deep Reinforcement Learning\n  Approach", "comments": "26 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has recently shown many impressive successes.\nHowever, one major obstacle towards applying such methods to real-world\nproblems is their lack of data-efficiency. To this end, we propose the\nBottleneck Simulator: a model-based reinforcement learning method which\ncombines a learned, factorized transition model of the environment with rollout\nsimulations to learn an effective policy from few examples. The learned\ntransition model employs an abstract, discrete (bottleneck) state, which\nincreases sample efficiency by reducing the number of model parameters and by\nexploiting structural properties of the environment. We provide a mathematical\nanalysis of the Bottleneck Simulator in terms of fixed points of the learned\npolicy, which reveals how performance is affected by four distinct sources of\nerror: an error related to the abstract space structure, an error related to\nthe transition model estimation variance, an error related to the transition\nmodel estimation bias, and an error related to the transition model class bias.\nFinally, we evaluate the Bottleneck Simulator on two natural language\nprocessing tasks: a text adventure game and a real-world, complex dialogue\nresponse selection task. On both tasks, the Bottleneck Simulator yields\nexcellent performance beating competing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 16:59:28 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Sankar", "Chinnadhurai", ""], ["Pieper", "Michael", ""], ["Pineau", "Joelle", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1807.04912", "submitter": "Francisco Silva", "authors": "Francisco Silva, Mikel Sanz, Jo\\~ao Seixas, Enrique Solano, and Yasser\n  Omar", "title": "Perceptrons from Memristors", "comments": "Added new result on universality of memristors, minor changes in the\n  introduction and algorithm, references updated", "journal-ref": "Neural Networks, Volume 122, 273-278 (2020)", "doi": "10.1016/j.neunet.2019.10.013", "report-no": null, "categories": "cs.ET cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristors, resistors with memory whose outputs depend on the history of\ntheir inputs, have been used with success in neuromorphic architectures,\nparticularly as synapses and non-volatile memories. However, to the best of our\nknowledge, no model for a network in which both the synapses and the neurons\nare implemented using memristors has been proposed so far. In the present work\nwe introduce models for single and multilayer perceptrons based exclusively on\nmemristors. We adapt the delta rule to the memristor-based single-layer\nperceptron and the backpropagation algorithm to the memristor-based multilayer\nperceptron. Our results show that both perform as expected for perceptrons,\nincluding satisfying Minsky-Papert's theorem. As a consequence of the Universal\nApproximation Theorem, they also show that memristors are universal function\napproximators. By using memristors for both the neurons and the synapses, our\nmodels pave the way for novel memristor-based neural network architectures and\nalgorithms. A neural network based on memristors could show advantages in terms\nof energy conservation and open up possibilities for other learning systems to\nbe adapted to a memristor-based paradigm, both in the classical and quantum\nlearning realms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 04:54:29 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 11:07:41 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Silva", "Francisco", ""], ["Sanz", "Mikel", ""], ["Seixas", "Jo\u00e3o", ""], ["Solano", "Enrique", ""], ["Omar", "Yasser", ""]]}, {"id": "1807.05076", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Adam Trischler", "title": "Metalearning with Hebbian Fast Weights", "comments": "8 pages, 3 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:1712.09926", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify recent neural approaches to one-shot learning with older ideas of\nassociative memory in a model for metalearning. Our model learns jointly to\nrepresent data and to bind class labels to representations in a single shot. It\nbuilds representations via slow weights, learned across tasks through SGD,\nwhile fast weights constructed by a Hebbian learning rule implement one-shot\nbinding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebank\none-shot learning benchmarks, our model achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 14:40:06 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Trischler", "Adam", ""]]}, {"id": "1807.05096", "submitter": "Ramses Sala", "authors": "Ramses Sala, Niccol\\`o Baldanzini and Marco Pierini", "title": "Global optimization test problems based on random field composition", "comments": null, "journal-ref": "Optimization Letters 11(4) 699-713, 2017", "doi": "10.1007/s11590-016-1037-1", "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development and identification of effective optimization algorithms for\nnon-convex real-world problems is a challenge in global optimization. Because\ntheoretical performance analysis is difficult, and problems based on models of\nreal-world systems are often computationally expensive, several artificial\nperformance test problems and test function generators have been proposed for\nempirical comparative assessment and analysis of metaheuristic optimization\nalgorithms. These test problems however often lack the complex function\nstructures and forthcoming difficulties that can appear in real-world problems.\nThis communication presents a method to systematically build test problems with\nvarious types and degrees of difficulty. By weighted composition of\nparameterized random fields, challenging test functions with tunable function\nfeatures such as, variance contribution distribution, interaction order, and\nnonlinearity can be constructed. The method is described, and its applicability\nto optimization performance analysis is described by means of a few basic\nexamples. The method aims to set a step forward in the systematic generation of\nglobal optimization test problems, which could lead to a better understanding\nof the performance of optimization algorithms on problem types with particular\ncharacteristics. On request an introductive MATLAB implementation of a test\nfunction generator based on the presented method is available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 14:10:57 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Sala", "Ramses", ""], ["Baldanzini", "Niccol\u00f2", ""], ["Pierini", "Marco", ""]]}, {"id": "1807.05222", "submitter": "German I. Parisi", "authors": "German I. Parisi, Jonathan Tong, Pablo Barros, Brigitte R\\\"oder,\n  Stefan Wermter", "title": "Towards Modeling the Interaction of Spatial-Associative Neural Network\n  Representations for Multisensory Perception", "comments": "Workshop on Computational Models for Crossmodal Learning, IEEE\n  ICDL-EPIROB 2017, Lisbon, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our daily perceptual experience is driven by different neural mechanisms that\nyield multisensory interaction as the interplay between exogenous stimuli and\nendogenous expectations. While the interaction of multisensory cues according\nto their spatiotemporal properties and the formation of multisensory\nfeature-based representations have been widely studied, the interaction of\nspatial-associative neural representations has received considerably less\nattention. In this paper, we propose a neural network architecture that models\nthe interaction of spatial-associative representations to perform causal\ninference of audiovisual stimuli. We investigate the spatial alignment of\nexogenous audiovisual stimuli modulated by associative congruence. In the\nspatial layer, topographically arranged networks account for the interaction of\naudiovisual input in terms of population codes. In the associative layer,\ncongruent audiovisual representations are obtained via the experience-driven\ndevelopment of feature-based associations. Levels of congruency are obtained as\na by-product of the neurodynamics of self-organizing networks, where the amount\nof neural activation triggered by the input can be expressed via a nonlinear\ndistance function. Our novel proposal is that activity-driven levels of\ncongruency can be used as top-down modulatory projections to spatially\ndistributed representations of sensory input, e.g. semantically related\naudiovisual pairs will yield a higher level of integration than unrelated\npairs. Furthermore, levels of neural response in unimodal layers may be seen as\nsensory reliability for the dynamic weighting of crossmodal cues. We describe a\nseries of planned experiments to validate our model in the tasks of\nmultisensory interaction on the basis of semantic congruence and unimodal cue\nreliability.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 13:12:01 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Parisi", "German I.", ""], ["Tong", "Jonathan", ""], ["Barros", "Pablo", ""], ["R\u00f6der", "Brigitte", ""], ["Wermter", "Stefan", ""]]}, {"id": "1807.05636", "submitter": "Aravindh Mahendran", "authors": "Aravindh Mahendran, James Thewlis, Andrea Vedaldi", "title": "Cross Pixel Optical Flow Similarity for Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for learning convolutional neural image\nrepresentations without manual supervision. We use motion cues in the form of\noptical flow, to supervise representations of static images. The obvious\napproach of training a network to predict flow from a single image can be\nneedlessly difficult due to intrinsic ambiguities in this prediction task. We\ninstead propose a much simpler learning goal: embed pixels such that the\nsimilarity between their embeddings matches that between their optical flow\nvectors. At test time, the learned deep network can be used without access to\nvideo or flow information and transferred to tasks such as image\nclassification, detection, and segmentation. Our method, which significantly\nsimplifies previous attempts at using motion for self-supervision, achieves\nstate-of-the-art results in self-supervision using motion cues, competitive\nresults for self-supervision in general, and is overall state of the art in\nself-supervised pretraining for semantic image segmentation, as demonstrated on\nstandard benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 23:48:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Mahendran", "Aravindh", ""], ["Thewlis", "James", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1807.05948", "submitter": "Dennis George Wilson", "authors": "Dennis G Wilson, Kyle Harrington, Sylvain Cussat-Blanc, Herv\\'e Luga", "title": "Evolving Differentiable Gene Regulatory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past twenty years, artificial Gene Regulatory Networks (GRNs) have\nshown their capacity to solve real-world problems in various domains such as\nagent control, signal processing and artificial life experiments. They have\nalso benefited from new evolutionary approaches and improvements to dynamic\nwhich have increased their optimization efficiency. In this paper, we present\nan additional step toward their usability in machine learning applications. We\ndetail an GPU-based implementation of differentiable GRNs, allowing for local\noptimization of GRN architectures with stochastic gradient descent (SGD). Using\na standard machine learning dataset, we evaluate the ways in which evolution\nand SGD can be combined to further GRN optimization. We compare these\napproaches with neural network models trained by SGD and with support vector\nmachines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:16:11 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wilson", "Dennis G", ""], ["Harrington", "Kyle", ""], ["Cussat-Blanc", "Sylvain", ""], ["Luga", "Herv\u00e9", ""]]}, {"id": "1807.05976", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin, Robert McKay, Tom Gedeon", "title": "Why don't the modules dominate - Investigating the Structure of a\n  Well-Known Modularity-Inducing Problem Domain", "comments": null, "journal-ref": null, "doi": "10.1145/3205651.3205737", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wagner's modularity inducing problem domain is a key contribution to the\nstudy of the evolution of modularity, including both evolutionary theory and\nevolutionary computation. We study its behavior under classical genetic\nalgorithms. Unlike what we seem to observe in nature, the emergence of\nmodularity is highly conditional and dependent, for example, on the eagerness\nof search. In nature, modular solutions generally dominate populations, whereas\nin this domain, modularity, when it emerges, is a relatively rare variant.\nEmergence of modularity depends heavily on random fluctuations in the fitness\nfunction, with a randomly varied but unchanging fitness function, modularity\nevolved far more rarely. Interestingly, high-fitness non-modular solutions\ncould frequently be converted into even-higher-fitness modular solutions by\nmanually removing all inter-module edges. Despite careful exploration, we do\nnot yet have a full explanation of why the genetic algorithm was unable to find\nthese better solutions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 13:36:59 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 06:30:46 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Qin", "Zhenyue", ""], ["McKay", "Robert", ""], ["Gedeon", "Tom", ""]]}, {"id": "1807.06173", "submitter": "Michael Burkhart", "authors": "Michael C. Burkhart", "title": "A Discriminative Approach to Bayesian Filtering with Applications to\n  Human Neural Decoding", "comments": "Ph.D. dissertation, Brown University, Division of Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a stationary state-space model that relates a sequence of hidden states\nand corresponding measurements or observations, Bayesian filtering provides a\nprincipled statistical framework for inferring the posterior distribution of\nthe current state given all measurements up to the present time. For example,\nthe Apollo lunar module implemented a Kalman filter to infer its location from\na sequence of earth-based radar measurements and land safely on the moon.\n  To perform Bayesian filtering, we require a measurement model that describes\nthe conditional distribution of each observation given state. The Kalman filter\ntakes this measurement model to be linear, Gaussian. Here we show how a\nnonlinear, Gaussian approximation to the distribution of state given\nobservation can be used in conjunction with Bayes' rule to build a nonlinear,\nnon-Gaussian measurement model. The resulting approach, called the\nDiscriminative Kalman Filter (DKF), retains fast closed-form updates for the\nposterior. We argue there are many cases where the distribution of state given\nmeasurement is better-approximated as Gaussian, especially when the\ndimensionality of measurements far exceeds that of states and the Bernstein-von\nMises theorem applies. Online neural decoding for brain-computer interfaces\nprovides a motivating example, where filtering incorporates increasingly\ndetailed measurements of neural activity to provide users control over external\ndevices. Within the BrainGate2 clinical trial, the DKF successfully enabled\nthree volunteers with quadriplegia to control an on-screen cursor in real-time\nusing mental imagery alone. Participant \"T9\" used the DKF to type out messages\non a tablet PC.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 01:36:57 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Burkhart", "Michael C.", ""]]}, {"id": "1807.06230", "submitter": "Sergei Khashin", "authors": "S. I. Khashin, and S. E. Vaganov", "title": "Genetic algorithms in Forth", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for automatically finding a program (bytecode) realizing the given\nalgorithm is developed. The algorithm is specified as a set of tests\n(input\\_data) $ \\rightarrow $ (output\\_data). Genetic methods made it possible\nto find the implementation of relatively complex algorithms: sorting, decimal\ndigits, GCD, LCM, factorial, prime divisors, binomial coefficients, and others.\nThe algorithms are implemented on a highly simplified version of Forth\nlanguage.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 05:34:07 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Khashin", "S. I.", ""], ["Vaganov", "S. E.", ""]]}, {"id": "1807.06244", "submitter": "Thierry Dumas", "authors": "Thierry Dumas (Sirocco), Aline Roumy (Sirocco), Christine Guillemot\n  (Sirocco)", "title": "Context-adaptive neural network based prediction for image compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a set of neural network architectures, called Prediction\nNeural Networks Set (PNNS), based on both fully-connected and convolutional\nneural networks, for intra image prediction. The choice of neural network for\npredicting a given image block depends on the block size, hence does not need\nto be signalled to the decoder. It is shown that, while fully-connected neural\nnetworks give good performance for small block sizes, convolutional neural\nnetworks provide better predictions in large blocks with complex textures.\nThanks to the use of masks of random sizes during training, the neural networks\nof PNNS well adapt to the available context that may vary, depending on the\nposition of the image block to be predicted. When integrating PNNS into a H.265\ncodec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained.\nThese gains are on average 0.99% larger than those of prior neural network\nbased methods. Unlike the H.265 intra prediction modes, which are each\nspecialized in predicting a specific texture, the proposed PNNS can model a\nlarge set of complex textures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 06:46:32 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 15:08:43 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Dumas", "Thierry", "", "Sirocco"], ["Roumy", "Aline", "", "Sirocco"], ["Guillemot", "Christine", "", "Sirocco"]]}, {"id": "1807.06399", "submitter": "Maxwell Nye", "authors": "Maxwell Nye, Andrew Saxe", "title": "Are Efficient Deep Representations Learnable?", "comments": "Presented at ICLR 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theories of deep learning have shown that a deep network can require\ndramatically fewer resources to represent a given function compared to a\nshallow network. But a question remains: can these efficient representations be\nlearned using current deep learning techniques? In this work, we test whether\nstandard deep learning methods can in fact find the efficient representations\nposited by several theories of deep representation. Specifically, we train deep\nneural networks to learn two simple functions with known efficient solutions:\nthe parity function and the fast Fourier transform. We find that using\ngradient-based optimization, a deep network does not learn the parity function,\nunless initialized very close to a hand-coded exact solution. We also find that\na deep linear neural network does not learn the fast Fourier transform, even in\nthe best-case scenario of infinite training data, unless the weights are\ninitialized very close to the exact hand-coded solution. Our results suggest\nthat not every element of the class of compositional functions can be learned\nefficiently by a deep network, and further restrictions are necessary to\nunderstand what functions are both efficiently representable and learnable.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 13:08:21 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Nye", "Maxwell", ""], ["Saxe", "Andrew", ""]]}, {"id": "1807.06540", "submitter": "Konno Tomohiko", "authors": "Tomohiko Konno and Michiaki Iwazume", "title": "Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try\n  After Deep Learning", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We found an easy and quick post-learning method named \"Icing on the Cake\" to\nenhance a classification performance in deep learning. The method is that we\ntrain only the final classifier again after an ordinary training is done.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:35:51 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Konno", "Tomohiko", ""], ["Iwazume", "Michiaki", ""]]}, {"id": "1807.06699", "submitter": "Kai Arulkumaran", "authors": "Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio\n  Criminisi, Aditya Nori", "title": "Adaptive Neural Trees", "comments": "International Conference on Machine Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and decision trees operate on largely separate\nparadigms; typically, the former performs representation learning with\npre-specified architectures, while the latter is characterised by learning\nhierarchies over pre-specified features with data-driven architectures. We\nunite the two via adaptive neural trees (ANTs) that incorporates representation\nlearning into edges, routing functions and leaf nodes of a decision tree, along\nwith a backpropagation-based training algorithm that adaptively grows the\narchitecture from primitive modules (e.g., convolutional layers). We\ndemonstrate that, whilst achieving competitive performance on classification\nand regression datasets, ANTs benefit from (i) lightweight inference via\nconditional computation, (ii) hierarchical separation of features useful to the\ntask e.g. learning meaningful class associations, such as separating natural\nvs. man-made objects, and (iii) a mechanism to adapt the architecture to the\nsize and complexity of the training dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:01:35 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 20:36:03 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 12:24:20 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 16:00:57 GMT"}, {"version": "v5", "created": "Sun, 9 Jun 2019 19:32:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tanno", "Ryutaro", ""], ["Arulkumaran", "Kai", ""], ["Alexander", "Daniel C.", ""], ["Criminisi", "Antonio", ""], ["Nori", "Aditya", ""]]}, {"id": "1807.06731", "submitter": "Felipe Campelo", "authors": "Felipe Campelo, Lucas S. Batista, Claus Aranha", "title": "The MOEADr Package - A Component-Based Framework for Multiobjective\n  Evolutionary Algorithms Based on Decomposition", "comments": "41 pages. 5 figures. Submitted to the Journal of Statistical Software", "journal-ref": null, "doi": "10.18637/jss.v092.i06", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiobjective Evolutionary Algorithms based on Decomposition (MOEA/D)\nrepresent a widely used class of population-based metaheuristics for the\nsolution of multicriteria optimization problems. We introduce the MOEADr\npackage, which offers many of these variants as instantiations of a\ncomponent-oriented framework. This approach contributes for easier\nreproducibility of existing MOEA/D variants from the literature, as well as for\nfaster development and testing of new composite algorithms. The package offers\nan standardized, modular implementation of MOEA/D based on this framework,\nwhich was designed aiming at providing researchers and practitioners with a\nstandard way to discuss and express MOEA/D variants. In this paper we introduce\nthe design principles behind the MOEADr package, as well as its current\ncomponents. Three case studies are provided to illustrate the main aspects of\nthe package.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 01:14:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Campelo", "Felipe", ""], ["Batista", "Lucas S.", ""], ["Aranha", "Claus", ""]]}, {"id": "1807.07362", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Nicol\\'as Navarro-Guerrero, Sven Magg, Stefan Wermter", "title": "Speeding up the Hyperparameter Optimization of Deep Convolutional Neural\n  Networks", "comments": "15 pages, published in the International Journal of Computational\n  Intelligence and Applications", "journal-ref": "International Journal of Computational Intelligence and\n  Applications (2018), Vol. 17, No. 02", "doi": "10.1142/S1469026818500086", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning algorithms require the practitioner to manually set the values\nof many hyperparameters before the learning process can begin. However, with\nmodern algorithms, the evaluation of a given hyperparameter setting can take a\nconsiderable amount of time and the search space is often very\nhigh-dimensional. We suggest using a lower-dimensional representation of the\noriginal data to quickly identify promising areas in the hyperparameter space.\nThis information can then be used to initialize the optimization algorithm for\nthe original, higher-dimensional data. We compare this approach with the\nstandard procedure of optimizing the hyperparameters only on the original\ninput.\n  We perform experiments with various state-of-the-art hyperparameter\noptimization algorithms such as random search, the tree of parzen estimators\n(TPEs), sequential model-based algorithm configuration (SMAC), and a genetic\nalgorithm (GA). Our experiments indicate that it is possible to speed up the\noptimization process by using lower-dimensional data representations at the\nbeginning, while increasing the dimensionality of the input later in the\noptimization process. This is independent of the underlying optimization\nprocedure, making the approach promising for many existing hyperparameter\noptimization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 12:25:29 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Hinz", "Tobias", ""], ["Navarro-Guerrero", "Nicol\u00e1s", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1807.07627", "submitter": "Daniel Canaday", "authors": "Daniel Canaday, Aaron Griffith, Daniel Gauthier", "title": "Rapid Time Series Prediction with a Hardware-Based Reservoir Computer", "comments": null, "journal-ref": null, "doi": "10.1063/1.5048199", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a neural network approach for processing\ntime-dependent signals that has seen rapid development in recent years.\nPhysical implementations of the technique using optical reservoirs have\ndemonstrated remarkable accuracy and processing speed at benchmark tasks.\nHowever, these approaches require an electronic output layer to maintain high\nperformance, which limits their use in tasks such as time-series prediction,\nwhere the output is fed back into the reservoir. We present here a reservoir\ncomputing scheme that has rapid processing speed both by the reservoir and the\noutput layer. The reservoir is realized by an autonomous, time-delay, Boolean\nnetwork configured on a field-programmable gate array. We investigate the\ndynamical properties of the network and observe the fading memory property that\nis critical for successful reservoir computing. We demonstrate the utility of\nthe technique by training a reservoir to learn the short- and long-term\nbehavior of a chaotic system. We find accuracy comparable to state-of-the-art\nsoftware approaches of similar network size, but with a superior real-time\nprediction rate up to 160 MHz.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 20:13:03 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:33:30 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Canaday", "Daniel", ""], ["Griffith", "Aaron", ""], ["Gauthier", "Daniel", ""]]}, {"id": "1807.07839", "submitter": "J\\\"org Stork", "authors": "J\\\"org Stork, Martin Zaefferer, Thomas Bartz-Beielstein", "title": "Distance-based Kernels for Surrogate Model-based Neuroevolution", "comments": "4 pages, 1 figure. This publication was accepted to the Developmental\n  Neural Networks Workshop of the Parallel Problem Solving from Nature 2018\n  (PPSN XV) conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topology optimization of artificial neural networks can be particularly\ndifficult if the fitness evaluations require expensive experiments or\nsimulations. For that reason, the optimization methods may need to be supported\nby surrogate models. We propose different distances for a suitable surrogate\nmodel, and compare them in a simple numerical test scenario.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 13:50:55 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Stork", "J\u00f6rg", ""], ["Zaefferer", "Martin", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "1807.07868", "submitter": "Michael Kampffmeyer", "authors": "Michael Kampffmeyer, Sigurd L{\\o}kse, Filippo M. Bianchi, Robert\n  Jenssen, Lorenzo Livi", "title": "The Deep Kernelized Autoencoder", "comments": "This work extends the preliminary (conference) version of this paper\n  (arXiv:1702.02526), Applied Soft Computing, Elsevier, 2018", "journal-ref": null, "doi": "10.1016/j.asoc.2018.07.029", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders learn data representations (codes) in such a way that the input\nis reproduced at the output of the network. However, it is not always clear\nwhat kind of properties of the input data need to be captured by the codes.\nKernel machines have experienced great success by operating via inner-products\nin a theoretically well-defined reproducing kernel Hilbert space, hence\ncapturing topological properties of input data. In this paper, we enhance the\nautoencoder's ability to learn effective data representations by aligning inner\nproducts between codes with respect to a kernel matrix. By doing so, the\nproposed kernelized autoencoder allows learning similarity-preserving\nembeddings of input data, where the notion of similarity is explicitly\ncontrolled by the user and encoded in a positive semi-definite kernel matrix.\nExperiments are performed for evaluating both reconstruction and kernel\nalignment performance in classification tasks and visualization of\nhigh-dimensional data. Additionally, we show that our method is capable to\nemulate kernel principal component analysis on a denoising task, obtaining\ncompetitive results at a much lower computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:47:32 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 10:03:50 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Kampffmeyer", "Michael", ""], ["L\u00f8kse", "Sigurd", ""], ["Bianchi", "Filippo M.", ""], ["Jenssen", "Robert", ""], ["Livi", "Lorenzo", ""]]}, {"id": "1807.07979", "submitter": "Souma Chowdhury", "authors": "Sharat Chidambaran, Amir Behjat, Souma Chowdhury", "title": "Multi-criteria Evolution of Neural Network Topologies: Balancing\n  Experience and Performance in Autonomous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of Artificial Neural Network (ANN) implementations in autonomous\nsystems use a fixed/user-prescribed network topology, leading to sub-optimal\nperformance and low portability. The existing neuro-evolution of augmenting\ntopology or NEAT paradigm offers a powerful alternative by allowing the network\ntopology and the connection weights to be simultaneously optimized through an\nevolutionary process. However, most NEAT implementations allow the\nconsideration of only a single objective. There also persists the question of\nhow to tractably introduce topological diversification that mitigates\noverfitting to training scenarios. To address these gaps, this paper develops a\nmulti-objective neuro-evolution algorithm. While adopting the basic elements of\nNEAT, important modifications are made to the selection, speciation, and\nmutation processes. With the backdrop of small-robot path-planning\napplications, an experience-gain criterion is derived to encapsulate the amount\nof diverse local environment encountered by the system. This criterion\nfacilitates the evolution of genes that support exploration, thereby seeking to\ngeneralize from a smaller set of mission scenarios than possible with\nperformance maximization alone. The effectiveness of the single-objective\n(optimizing performance) and the multi-objective (optimizing performance and\nexperience-gain) neuro-evolution approaches are evaluated on two different\nsmall-robot cases, with ANNs obtained by the multi-objective optimization\nobserved to provide superior performance in unseen scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 18:00:28 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Chidambaran", "Sharat", ""], ["Behjat", "Amir", ""], ["Chowdhury", "Souma", ""]]}, {"id": "1807.08133", "submitter": "John Kelleher", "authors": "John D. Kelleher and Simon Dobnik", "title": "What is not where: the challenge of integrating spatial representations\n  into deep learning architectures", "comments": "15 pages, 10 figures, Appears in CLASP Papers in Computational\n  Linguistics Vol 1: Proceedings of the Conference on Logic and Machine\n  Learning in Natural Language (LaML 2017), pp. 41-52", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines to what degree current deep learning architectures for\nimage caption generation capture spatial language. On the basis of the\nevaluation of examples of generated captions from the literature we argue that\nsystems capture what objects are in the image data but not where these objects\nare located: the captions generated by these systems are the output of a\nlanguage model conditioned on the output of an object detector that cannot\ncapture fine-grained location information. Although language models provide\nuseful knowledge for image captions, we argue that deep learning image\ncaptioning architectures should also model geometric relations between objects.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 11:55:17 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Kelleher", "John D.", ""], ["Dobnik", "Simon", ""]]}, {"id": "1807.08194", "submitter": "Abdullah Al-Dujaili", "authors": "Abdullah Al-Dujaili and Tom Schmiedlechner and and Erik Hemberg and\n  Una-May O'Reilly", "title": "Towards Distributed Coevolutionary GANs", "comments": "Accepted at AAAI 2018 Fall Symposium Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become one of the dominant\nmethods for deep generative modeling. Despite their demonstrated success on\nmultiple vision tasks, GANs are difficult to train and much research has been\ndedicated towards understanding and improving their gradient-based learning\ndynamics. Here, we investigate the use of coevolution, a class of black-box\n(gradient-free) co-optimization techniques and a powerful tool in evolutionary\ncomputing, as a supplement to gradient-based GAN training techniques.\nExperiments on a simple model that exhibits several of the GAN gradient-based\ndynamics (e.g., mode collapse, oscillatory behavior, and vanishing gradients)\nshow that coevolution is a promising framework for escaping degenerate GAN\ntraining behaviors.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 19:29:21 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 14:34:15 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 18:19:20 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Al-Dujaili", "Abdullah", ""], ["Schmiedlechner", "Tom", ""], ["Hemberg", "and Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1807.08655", "submitter": "Aki Nikolaidis", "authors": "Aki Nikolaidis", "title": "Training Humans and Machines", "comments": "4 pages, Computational Cognitive Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many years, researchers in psychology, education, statistics, and machine\nlearning have been developing practical methods to improve learning speed,\nretention, and generalizability, and this work has been successful. Many of\nthese methods are rooted in common underlying principles that seem to drive\nlearning and overlearning in both humans and machines. I present a review of a\nsmall part of this work to point to potentially novel applications in both\nmachine and human learning that may be worth exploring.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 21:06:06 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Nikolaidis", "Aki", ""]]}, {"id": "1807.08716", "submitter": "Mahdi Nazemi", "authors": "Mahdi Nazemi, Ghasem Pasandi, Massoud Pedram", "title": "NullaNet: Training Deep Neural Networks for Reduced-Memory-Access\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully deployed in a wide variety of\napplications including computer vision and speech recognition. However,\ncomputational and storage complexity of these models has forced the majority of\ncomputations to be performed on high-end computing platforms or on the cloud.\nTo cope with computational and storage complexity of these models, this paper\npresents a training method that enables a radically different approach for\nrealization of deep neural networks through Boolean logic minimization. The\naforementioned realization completely removes the energy-hungry step of\naccessing memory for obtaining model parameters, consumes about two orders of\nmagnitude fewer computing resources compared to realizations that use\nfloatingpoint operations, and has a substantially lower latency.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:50:31 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 05:22:38 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Nazemi", "Mahdi", ""], ["Pasandi", "Ghasem", ""], ["Pedram", "Massoud", ""]]}, {"id": "1807.09174", "submitter": "Hadi Jahanshahi", "authors": "Hadi Jahanshahi and Naeimeh Najafizadeh Sari", "title": "On the computational analysis of the genetic algorithm for attitude\n  control of a carrier system", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper intends to cover three main topics. First, a fuzzy-PID controller\nis designed to control the thrust vector of a launch vehicle, accommodating a\nCanSat. Then, the genetic algorithm (GA) is employed to optimize the controller\nperformance. Finally, through adjusting the algorithm parameters, their impact\non the optimization process is examined. In this regard, the motion vector\ncontrol is programmed based on the governing dynamic equations of motion for\npayload delivery in the desired altitude and flight-path angle. This utilizes\none single input and one preferential fuzzy inference engine, where the latter\nacts to avoid the system instability in large angles for the thrust vector. The\noptimization objective functions include the deviations of the thrust vector\nand the system from the equilibrium state, which must be met simultaneously.\nSensitivity analysis of the parameters of the genetic algorithm involves\nexamining nine different cases and discussing their impact on the optimization\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:58:14 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Jahanshahi", "Hadi", ""], ["Sari", "Naeimeh Najafizadeh", ""]]}, {"id": "1807.09177", "submitter": "Raul Fernandez", "authors": "Raul Fernandez-Fernandez, Juan G. Victores, David Estevez and Carlos\n  Balaguer", "title": "Robot Imitation through Vision, Kinesthetic and Force Features with\n  Online Adaptation to Changing Environments", "comments": "2018 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous Goal-Directed Actions (CGDA) is a robot imitation framework that\nencodes actions as the changes they produce on the environment. While it\npresents numerous advantages with respect to other robot imitation frameworks\nin terms of generalization and portability, final robot joint trajectories for\nthe execution of actions are not necessarily encoded within the model. This is\nstudied as an optimization problem, and the solution is computed through\nevolutionary algorithms in simulated environments. Evolutionary algorithms\nrequire a large number of evaluations, which had made the use of these\nalgorithms in real world applications very challenging. This paper presents\nonline evolutionary strategies, as a change of paradigm within CGDA execution.\nOnline evolutionary strategies shift and merge motor execution into the\nplanning loop. A concrete online evolutionary strategy, Online Evolved\nTrajectories (OET), is presented. OET drastically reduces computational times\nbetween motor executions, and enables working in real world dynamic\nenvironments and/or with human collaboration. Its performance has been measured\nagainst Full Trajectory Evolution (FTE) and Incrementally Evolved Trajectories\n(IET), obtaining the best overall results. Experimental evaluations are\nperformed on the TEO full-sized humanoid robot with \"paint\" and \"iron\" actions\nthat together involve vision, kinesthetic and force features.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:22:24 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 16:55:06 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 10:20:47 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Fernandez-Fernandez", "Raul", ""], ["Victores", "Juan G.", ""], ["Estevez", "David", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1807.09203", "submitter": "Yu-Fan Tung", "authors": "Yu-Fan Tung, Tian-Li Yu", "title": "Theoretical Perspective of Convergence Complexity of Evolutionary\n  Algorithms Adopting Optimal Mixing", "comments": "8 pages, 2015 GECCO oral paper", "journal-ref": "Proceedings of the 2015 Annual Conference on Genetic and\n  Evolutionary Computation, pages 535-542, 2015", "doi": "10.1145/2739480.2754685", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal mixing evolutionary algorithms (OMEAs) have recently drawn much\nattention for their robustness, small size of required population, and\nefficiency in terms of number of function evaluations (NFE). In this paper, the\nperformances and behaviors of OMEAs are studied by investigating the mechanism\nof optimal mixing (OM), the variation operator in OMEAs, under two scenarios --\none-layer and two-layer masks. For the case of one-layer masks, the required\npopulation size is derived from the viewpoint of initial supply, while the\nconvergence time is derived by analyzing the progress of sub-solution growth.\nNFE is then asymptotically bounded with rational probability by estimating the\nprobability of performing evaluations. For the case of two-layer masks,\nempirical results indicate that the required population size is proportional to\nboth the degree of cross competition and the results from the one-layer-mask\ncase. The derived models also indicate that population sizing is decided by\ninitial supply when disjoint masks are adopted, that the high selection\npressure imposed by OM makes the composition of sub-problems impact little on\nNFE, and that the population size requirement for two-layer masks increases\nwith the reverse-growth probability.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 16:04:16 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Tung", "Yu-Fan", ""], ["Yu", "Tian-Li", ""]]}, {"id": "1807.09217", "submitter": "Aboul Ella Hassanien Abo", "authors": "Amr M. Sauber, Mohammed M. Nasef, Essam H. Houssein, and Aboul Ella\n  Hassanien", "title": "Parallel Whale Optimization Algorithm for Solving Constrained and\n  Unconstrained Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the engineering optimization problems require large computational\ndemands and long solution time even on high multi-processors computational\ndevices. In this paper, an OpenMP inspired parallel version of the whale\noptimization algorithm (PWOA) to obtain enhanced computational throughput and\nglobal search capability is presented. It automatically detects the number of\navailable processors and divides the workload among them to accomplish the\neffective utilization of the available resources. PWOA is applied on twenty\nunconstrained optimization functions on multiple dimensions and five\nconstrained optimization engineering functions. The proposed parallelism PWOA\nalgorithms performance is evaluated using parallel metrics such as speedup,\nefficiency. The comparison illustrates that the proposed PWOA algorithm has\nobtained the same results while exceeding the sequential version in\nperformance. Furthermore, PWOA algorithm in the term of computational time and\nspeed of parallel metric was achieved better results over the sequential\nprocessing compared to the standard WOA.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:41:22 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Sauber", "Amr M.", ""], ["Nasef", "Mohammed M.", ""], ["Houssein", "Essam H.", ""], ["Hassanien", "Aboul Ella", ""]]}, {"id": "1807.09226", "submitter": "Alexey Potapov", "authors": "Alexey Potapov, Oleg Shcherbakov, Innokentii Zhdanov, Sergey Rodionov,\n  Nikolai Skorobogatko", "title": "HyperNets and their application to learning spatial transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a conceptual framework for higher-order artificial\nneural networks. The idea of higher-order networks arises naturally when a\nmodel is required to learn some group of transformations, every element of\nwhich is well-approximated by a traditional feedforward network. Thus the group\nas a whole can be represented as a hyper network. One of typical examples of\nsuch groups is spatial transformations. We show that the proposed framework,\nwhich we call HyperNets, is able to deal with at least two basic spatial\ntransformations of images: rotation and affine transformation. We show that\nHyperNets are able not only to generalize rotation and affine transformation,\nbut also to compensate the rotation of images bringing them into canonical\nforms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:39:26 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Potapov", "Alexey", ""], ["Shcherbakov", "Oleg", ""], ["Zhdanov", "Innokentii", ""], ["Rodionov", "Sergey", ""], ["Skorobogatko", "Nikolai", ""]]}, {"id": "1807.09341", "submitter": "Thanard Kurutach", "authors": "Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, Pieter Abbeel", "title": "Learning Plannable Representations with Causal InfoGAN", "comments": "ICML / IJCAI / AAMAS 2018 Workshop on Planning and Learning (PAL-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep generative models have been shown to 'imagine'\nconvincing high-dimensional observations such as images, audio, and even video,\nlearning directly from raw data. In this work, we ask how to imagine\ngoal-directed visual plans -- a plausible sequence of observations that\ntransition a dynamical system from its current configuration to a desired goal\nstate, which can later be used as a reference trajectory for control. We focus\non systems with high-dimensional observations, such as images, and propose an\napproach that naturally combines representation learning and planning. Our\nframework learns a generative model of sequential observations, where the\ngenerative process is induced by a transition in a low-dimensional planning\nmodel, and an additional noise. By maximizing the mutual information between\nthe generated observations and the transition in the planning model, we obtain\na low-dimensional representation that best explains the causal nature of the\ndata. We structure the planning model to be compatible with efficient planning\nalgorithms, and we propose several such models based on either discrete or\ncontinuous states. Finally, to generate a visual plan, we project the current\nand goal observations onto their respective states in the planning model, plan\na trajectory, and then use the generative model to transform the trajectory to\na sequence of observations. We demonstrate our method on imagining plausible\nvisual plans of rope manipulation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 20:46:05 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Kurutach", "Thanard", ""], ["Tamar", "Aviv", ""], ["Yang", "Ge", ""], ["Russell", "Stuart", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1807.09374", "submitter": "Hananel Hazan", "authors": "Hananel Hazan, Daniel J. Saunders, Darpan T. Sanghavi, Hava T.\n  Siegelmann, Robert Kozma", "title": "Unsupervised Learning with Self-Organizing Spiking Neural Networks", "comments": null, "journal-ref": "Proceeding WCCI 2018", "doi": "10.1109/IJCNN.2018.8489673", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a system comprising a hybridization of self-organized map (SOM)\nproperties with spiking neural networks (SNNs) that retain many of the features\nof SOMs. Networks are trained in an unsupervised manner to learn a\nself-organized lattice of filters via excitatory-inhibitory interactions among\npopulations of neurons. We develop and test various inhibition strategies, such\nas growing with inter-neuron distance and two distinct levels of inhibition.\nThe quality of the unsupervised learning algorithm is evaluated using examples\nwith known labels. Several biologically-inspired classification tools are\nproposed and compared, including population-level confidence rating, and\nn-grams using spike motif algorithm. Using the optimal choice of parameters,\nour approach produces improvements over state-of-art spiking neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 22:08:57 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hazan", "Hananel", ""], ["Saunders", "Daniel J.", ""], ["Sanghavi", "Darpan T.", ""], ["Siegelmann", "Hava T.", ""], ["Kozma", "Robert", ""]]}, {"id": "1807.09488", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Alexander Asteroth, Thomas B\\\"ack", "title": "Prototype Discovery using Quality-Diversity", "comments": "Parallel Problem Solving using Nature 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An iterative computer-aided ideation procedure is introduced, building on\nrecent quality-diversity algorithms, which search for diverse as well as\nhigh-performing solutions. Dimensionality reduction is used to define a\nsimilarity space, in which solutions are clustered into classes. These classes\nare represented by prototypes, which are presented to the user for selection.\nIn the next iteration, quality-diversity focuses on searching within the\nselected class. A quantitative analysis is performed on a 2D airfoil, and a\nmore complex 3D side view mirror domain shows how computer-aided ideation can\nhelp to enhance engineers' intuition while allowing their design decisions to\ninfluence the design process.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 08:55:45 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Hagg", "Alexander", ""], ["Asteroth", "Alexander", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1807.09760", "submitter": "Mo'taz Al-Hami", "authors": "Mo'taz Al-Hami, Marcin Pietron, Rishi Kumar, Raul A. Casas, Samer L.\n  Hijazi, Chris Rowen", "title": "Method for Hybrid Precision Convolutional Neural Network Representation", "comments": "Cadence Design Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This invention addresses fixed-point representations of convolutional neural\nnetworks (CNN) in integrated circuits. When quantizing a CNN for a practical\nimplementation there is a trade-off between the precision used for operations\nbetween coefficients and data and the accuracy of the system. A homogenous\nrepresentation may not be sufficient to achieve the best level of performance\nat a reasonable cost in implementation complexity or power consumption.\nParsimonious ways of representing data and coefficients are needed to improve\npower efficiency and throughput while maintaining accuracy of a CNN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 19:13:22 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Al-Hami", "Mo'taz", ""], ["Pietron", "Marcin", ""], ["Kumar", "Rishi", ""], ["Casas", "Raul A.", ""], ["Hijazi", "Samer L.", ""], ["Rowen", "Chris", ""]]}, {"id": "1807.09830", "submitter": "Luis Argerich", "authors": "Leandro Palma, Luis Argerich", "title": "Iterative evaluation of LSTM cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we present a modification in the conventional flow of\ninformation through a LSTM network, which we consider well suited for RNNs in\ngeneral. The modification leads to a iterative scheme where the computations\nperformed by the LSTM cell are repeated over a constant input and cell state\nvalues, while updating the hidden state a finite number of times. We provide\ntheoretical and empirical evidence to support the augmented capabilities of the\niterative scheme and show examples related to language modeling. The\nmodification yields an enhancement in the model performance comparable with the\noriginal model augmented more than 3 times in terms of the total amount of\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 13:57:23 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Palma", "Leandro", ""], ["Argerich", "Luis", ""]]}, {"id": "1807.09844", "submitter": "John Kelleher", "authors": "Simon Dobnik and John D. Kelleher", "title": "Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing", "comments": "18 pages, 1 figure, Appears in CLASP Papers in Computational\n  Linguistics Vol. 1: Proceedings of the Conference on Logic and Machine\n  Learning in Natural Language (LaML 2017)", "journal-ref": "CLASP Papers in Computational Linguistics Vol. 1: Proceedings of\n  the Conference on Logic and Machine Learning in Natural Language (LaML 2017).\n  ISSN: 2002-9764. URI: http://hdl.handle.net/2077/54911", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) can be done using either top-down (theory\ndriven) and bottom-up (data driven) approaches, which we call mechanistic and\nphenomenological respectively. The approaches are frequently considered to\nstand in opposition to each other. Examining some recent approaches in deep\nlearning we argue that deep neural networks incorporate both perspectives and,\nfurthermore, that leveraging this aspect of deep learning may help in solving\ncomplex problems within language technology, such as modelling language and\nperception in the domain of spatial cognition.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 11:37:15 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 15:45:24 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Dobnik", "Simon", ""], ["Kelleher", "John D.", ""]]}, {"id": "1807.09946", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Jocelin Su, Anshul Kundaje", "title": "Computationally Efficient Measures of Internal Neuron Importance", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of assigning importance to individual neurons in a network is\nof interest when interpreting deep learning models. In recent work, Dhamdhere\net al. proposed Total Conductance, a \"natural refinement of Integrated\nGradients\" for attributing importance to internal neurons. Unfortunately, the\nauthors found that calculating conductance in tensorflow required the addition\nof several custom gradient operators and did not scale well. In this work, we\nshow that the formula for Total Conductance is mathematically equivalent to\nPath Integrated Gradients computed on a hidden layer in the network. We provide\na scalable implementation of Total Conductance using standard tensorflow\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\napproach that is applicable to calculating internal neuron importance. We find\nthat DeepLIFT produces strong empirical results and is faster to compute, but\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\nmay not always be preferred in practice. Colab notebook reproducing results:\nhttp://bit.ly/neuronintegratedgradients\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 03:47:45 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Su", "Jocelin", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1807.10038", "submitter": "Phan Trung Hai Nguyen", "authors": "Duc-Cuong Dang, Per Kristian Lehre and Phan Trung Hai Nguyen", "title": "Level-Based Analysis of the Univariate Marginal Distribution Algorithm", "comments": "To appear in Algorithmica Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of Distribution Algorithms (EDAs) are stochastic heuristics that\nsearch for optimal solutions by learning and sampling from probabilistic\nmodels. Despite their popularity in real-world applications, there is little\nrigorous understanding of their performance. Even for the Univariate Marginal\nDistribution Algorithm (UMDA) -- a simple population-based EDA assuming\nindependence between decision variables -- the optimisation time on the linear\nproblem OneMax was until recently undetermined. The incomplete theoretical\nunderstanding of EDAs is mainly due to lack of appropriate analytical tools.\n  We show that the recently developed level-based theorem for non-elitist\npopulations combined with anti-concentration results yield upper bounds on the\nexpected optimisation time of the UMDA. This approach results in the bound\n$\\mathcal{O}(n\\lambda\\log \\lambda+n^2)$ on two problems, LeadingOnes and\nBinVal, for population sizes $\\lambda>\\mu=\\Omega(\\log n)$, where $\\mu$ and\n$\\lambda$ are parameters of the algorithm. We also prove that the UMDA with\npopulation sizes $\\mu\\in \\mathcal{O}(\\sqrt{n}) \\cap \\Omega(\\log n)$ optimises\nOneMax in expected time $\\mathcal{O}(\\lambda n)$, and for larger population\nsizes $\\mu=\\Omega(\\sqrt{n}\\log n)$, in expected time\n$\\mathcal{O}(\\lambda\\sqrt{n})$. The facility and generality of our arguments\nsuggest that this is a promising approach to derive bounds on the expected\noptimisation time of EDAs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:46:03 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Dang", "Duc-Cuong", ""], ["Lehre", "Per Kristian", ""], ["Nguyen", "Phan Trung Hai", ""]]}, {"id": "1807.10068", "submitter": "Michael Hellwig", "authors": "Michael Hellwig and Hans-Georg Beyer", "title": "A Linear Constrained Optimization Benchmark For Probabilistic Search\n  Algorithms: The Rotated Klee-Minty Problem", "comments": "This preprint consists of 12 pages including 3 figures and 4 tables.\n  The final authenticated publication will be referred to as soon as possible.\n  Current status: submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development, assessment, and comparison of randomized search algorithms\nheavily rely on benchmarking. Regarding the domain of constrained optimization,\nthe number of currently available benchmark environments bears no relation to\nthe number of distinct problem features. The present paper advances a proposal\nof a scalable linear constrained optimization problem that is suitable for\nbenchmarking Evolutionary Algorithms. By comparing two recent EA variants, the\nlinear benchmarking environment is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 11:17:12 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Hellwig", "Michael", ""], ["Beyer", "Hans-Georg", ""]]}, {"id": "1807.10275", "submitter": "Yingyu Zhang", "authors": "Yingyu Zhang, Yuanzhen Li, Quan-Ke Panb, P.N. Suganthan", "title": "A Many-Objective Evolutionary Algorithm Based on Decomposition and Local\n  Dominance", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.06282,\n  arXiv:1806.10950", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many-objective evolutionary algorithms (MOEAs), especially the\ndecomposition-based MOEAs, have attracted wide attention in recent years.\nRecent studies show that a well designed combination of the decomposition\nmethod and the domination method can improve the performance ,i.e., convergence\nand diversity, of a MOEA. In this paper, a novel way of combining the\ndecomposition method and the domination method is proposed. More precisely, a\nset of weight vectors is employed to decompose a given many-objective\noptimization problem(MaOP), and a hybrid method of the penalty-based boundary\nintersection function and dominance is proposed to compare local solutions\nwithin a subpopulation defined by a weight vector. A MOEA based on the hybrid\nmethod is implemented and tested on problems chosen from two famous test\nsuites, i.e., DTLZ and WFG. The experimental results show that our algorithm is\nvery competitive in dealing with MaOPs. Subsequently, our algorithm is extended\nto solve constraint MaOPs, and the constrained version of our algorithm also\nshows good performance in terms of convergence and diversity. These reveals\nthat using dominance locally and combining it with the decomposition method can\neffectively improve the performance of a MOEA.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 08:56:43 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 10:49:47 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhang", "Yingyu", ""], ["Li", "Yuanzhen", ""], ["Panb", "Quan-Ke", ""], ["Suganthan", "P. N.", ""]]}, {"id": "1807.10470", "submitter": "Jiangyu Wang", "authors": "Jiangyu Wang and Huanxin Chen", "title": "BSAS: Beetle Swarm Antennae Search Algorithm for Optimization Problems", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beetle antennae search (BAS) is an efficient meta-heuristic algorithm.\nHowever, the convergent results of BAS rely heavily on the random beetle\ndirection in every iterations. More specifically, different random seeds may\ncause different optimized results. Besides, the step-size update algorithm of\nBAS cannot guarantee objective become smaller in iterative process. In order to\nsolve these problems, this paper proposes Beetle Swarm Antennae Search\nAlgorithm (BSAS) which combines swarm intelligence algorithm with\nfeedback-based step-size update strategy. BSAS employs k beetles to find more\noptimal position in each moving rather than one beetle. The step-size updates\nonly when k beetles return without better choices. Experiments are carried out\non building system identification. The results reveal the efficacy of the BSAS\nalgorithm to avoid influence of random direction of Beetle. In addition, the\nestimation errors decrease as the beetles number goes up.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:49:10 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Wang", "Jiangyu", ""], ["Chen", "Huanxin", ""]]}, {"id": "1807.10562", "submitter": "Carlos Camacho", "authors": "Carlos Camacho-G\\'omez", "title": "Contributions to the development of the CRO-SL algorithm: Engineering\n  applications problems", "comments": "arXiv admin note: text overlap with arXiv:1806.02654 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Ph.D. thesis discusses advanced design issues of the evolutionary-based\nalgorithm \\textit{\"Coral Reef Optimization\"}, in its Substrate-Layer (CRO-SL)\nversion, for optimization problems in Engineering Applications. The problems\nthat can be tackled with meta-heuristic approaches is very wide and varied, and\nit is not exclusive of engineering. However we focus the Thesis on it area, one\nof the most prominent in our time. One of the proposed application is battery\nscheduling problem in Micro-Grids (MGs). Specifically, we consider an MG that\nincludes renewable distributed generation and different loads, defined by its\npower profiles, and is equipped with an energy storage device (battery) to\naddress its programming (duration of loading / discharging and occurrence) in a\nreal scenario with variable electricity prices. Also, we discuss a problem of\nvibration cancellation over structures of two and four floors, using Tuned Mass\nDampers (TMD's). The optimization algorithm will try to find the best solution\nby obtaining three physical parameters and the TMD location. As another related\napplication, CRO-SL is used to design Multi-Input-Multi-Output Active Vibration\nControl (MIMO-AVC) via inertial-mass actuators, for structures subjected to\nhuman induced vibration. In this problem, we will optimize the location of each\nactuator and tune control gains. Finally, we tackle the optimization of a\ntextile modified meander-line Inverted-F Antenna (IFA) with variable width and\nspacing meander, for RFID systems. Specifically, the CRO-SL is used to obtain\nan optimal antenna design, with a good bandwidth and radiation pattern, ideal\nfor RFID readers. Radio Frequency Identification (RFID) has become one of the\nmost numerous manufactured devices worldwide due to a reliable and inexpensive\nmeans of locating people. They are used in access and money cards and product\nlabels and many other applications.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:31:32 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Camacho-G\u00f3mez", "Carlos", ""]]}, {"id": "1807.10698", "submitter": "Tasio Gonzalez-Raya", "authors": "Tasio Gonzalez-Raya, Xiao-Hang Cheng, I\\~nigo L. Egusquiza, Xi Chen,\n  Mikel Sanz, Enrique Solano", "title": "Quantized Single-Ion-Channel Hodgkin-Huxley Model for Quantum Neurons", "comments": null, "journal-ref": "Phys. Rev. Applied 12, 014037 (2019)", "doi": "10.1103/PhysRevApplied.12.014037", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hodgkin-Huxley model describes the behavior of the cell membrane in\nneurons, treating each part of it as an electric circuit element, namely\ncapacitors, memristors, and voltage sources. We focus on the activation channel\nof potassium ions, due to its simplicity, while keeping most of the features\ndisplayed by the original model. This reduced version is essentially a\nclassical memristor, a resistor whose resistance depends on the history of\nelectric signals that have crossed it, coupled to a voltage source and a\ncapacitor. Here, we will consider a quantized Hodgkin-Huxley model based on a\nquantum memristor formalism. We compare the behavior of the membrane voltage\nand the potassium channel conductance, when the circuit is subjected to AC\nsources, in both classical and quantum realms. Numerical simulations show an\nexpected adaptation of the considered channel conductance depending on the\nsignal history in all regimes. Remarkably, the computation of higher moments of\nthe voltage manifest purely quantum features related to the circuit zero-point\nenergy. Finally, we study the implementation of the Hodgkin-Huxley quantum\nmemristor as an asymmetric rf SQUID in superconducting circuits. This study may\nallow the construction of quantum neuron networks inspired in the brain\nfunction, as well as the design of neuromorphic quantum architectures for\nquantum machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 15:48:04 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 14:14:37 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 11:08:34 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 14:19:34 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Gonzalez-Raya", "Tasio", ""], ["Cheng", "Xiao-Hang", ""], ["Egusquiza", "I\u00f1igo L.", ""], ["Chen", "Xi", ""], ["Sanz", "Mikel", ""], ["Solano", "Enrique", ""]]}, {"id": "1807.11091", "submitter": "Tianyun Zhang", "authors": "Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Xiaolong Ma, Ning Liu, Linfeng\n  Zhang, Jian Tang, Kaisheng Ma, Xue Lin, Makan Fardad and Yanzhi Wang", "title": "StructADMM: A Systematic, High-Efficiency Framework of Structured Weight\n  Pruning for DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning methods of DNNs have been demonstrated to achieve a good model\npruning rate without loss of accuracy, thereby alleviating the significant\ncomputation/storage requirements of large-scale DNNs. Structured weight pruning\nmethods have been proposed to overcome the limitation of irregular network\nstructure and demonstrated actual GPU acceleration. However, in prior work the\npruning rate (degree of sparsity) and GPU acceleration are limited (to less\nthan 50%) when accuracy needs to be maintained. In this work,we overcome these\nlimitations by proposing a unified, systematic framework of structured weight\npruning for DNNs. It is a framework that can be used to induce different types\nof structured sparsity, such as filter-wise, channel-wise, and shape-wise\nsparsity, as well non-structured sparsity. The proposed framework incorporates\nstochastic gradient descent with ADMM, and can be understood as a dynamic\nregularization method in which the regularization target is analytically\nupdated in each iteration. Without loss of accuracy on the AlexNet model, we\nachieve 2.58X and 3.65X average measured speedup on two GPUs, clearly\noutperforming the prior work. The average speedups reach 3.15X and 8.52X when\nallowing a moderate ac-curacy loss of 2%. In this case the model compression\nfor convolutional layers is 15.0X, corresponding to 11.93X measured CPU\nspeedup. Our experiments on ResNet model and on other data sets like UCF101 and\nCIFAR-10 demonstrate the consistently higher performance of our framework.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 18:07:04 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 18:52:21 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 02:37:46 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Zhang", "Tianyun", ""], ["Ye", "Shaokai", ""], ["Zhang", "Kaiqi", ""], ["Ma", "Xiaolong", ""], ["Liu", "Ning", ""], ["Zhang", "Linfeng", ""], ["Tang", "Jian", ""], ["Ma", "Kaisheng", ""], ["Lin", "Xue", ""], ["Fardad", "Makan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1807.11215", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec, Valentin Vielzeuf, St\\'ephane Pateux, Alexis\n  Lechervy, Fr\\'ed\\'eric Jurie", "title": "CAKE: Compact and Accurate K-dimensional representation of Emotion", "comments": null, "journal-ref": "Image Analysis for Human Facial and Activity Recognition (BMVC\n  Workshop), Sep 2018, Newcastle, United Kingdom.\n  http://juz-dev.myweb.port.ac.uk/BMVCWorkshop/index.html", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous models describing the human emotional states have been built by the\npsychology community. Alongside, Deep Neural Networks (DNN) are reaching\nexcellent performances and are becoming interesting features extraction tools\nin many computer vision tasks.Inspired by works from the psychology community,\nwe first study the link between the compact two-dimensional representation of\nthe emotion known as arousal-valence, and discrete emotion classes (e.g. anger,\nhappiness, sadness, etc.) used in the computer vision community. It enables to\nassess the benefits -- in terms of discrete emotion inference -- of adding an\nextra dimension to arousal-valence (usually named dominance). Building on these\nobservations, we propose CAKE, a 3-dimensional representation of emotion\nlearned in a multi-domain fashion, achieving accurate emotion recognition on\nseveral public datasets. Moreover, we visualize how emotions boundaries are\norganized inside DNN representations and show that DNNs are implicitly learning\narousal-valence-like descriptions of emotions. Finally, we use the CAKE\nrepresentation to compare the quality of the annotations of different public\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:03:09 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 08:07:32 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Kervadec", "Corentin", ""], ["Vielzeuf", "Valentin", ""], ["Pateux", "St\u00e9phane", ""], ["Lechervy", "Alexis", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1807.11551", "submitter": "Karol Antczak", "authors": "Karol Antczak", "title": "Deep Recurrent Neural Networks for ECG Signal Denoising", "comments": "7 pages + 2 pages of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrocardiographic signal is a subject to multiple noises, caused by\nvarious factors. It is therefore a standard practice to denoise such signal\nbefore further analysis. With advances of new branch of machine learning,\ncalled deep learning, new methods are available that promises state-of-the-art\nperformance for this task. We present a novel approach to denoise\nelectrocardiographic signals with deep recurrent denoising neural networks. We\nutilize a transfer learning technique by pretraining the network using\nsynthetic data, generated by a dynamic ECG model, and fine-tuning it with a\nreal data. We also investigate the impact of the synthetic training data on the\nnetwork performance on real signals. The proposed method was tested on a real\ndataset with varying amount of noise. The results indicate that four-layer deep\nrecurrent neural network can outperform reference methods for heavily noised\nsignal. Moreover, networks pretrained with synthetic data seem to have better\nresults than network trained with real data only. We show that it is possible\nto create state-of-the art denoising neural network that, pretrained on\nartificial data, can perform exceptionally well on real ECG signals after\nproper fine-tuning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 20:00:03 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 13:40:05 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 08:39:06 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Antczak", "Karol", ""]]}, {"id": "1807.11669", "submitter": "Shih-Huan Hsu", "authors": "Shih-Huan Hsu, Tian-Li Yu", "title": "Optimization by Pairwise Linkage Detection, Incremental Linkage Set, and\n  Restricted / Back Mixing: DSMGA-II", "comments": "Genetic and Evolutionary Computation Conference (GECCO'15)", "journal-ref": null, "doi": "10.1145/2739480.2754737", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new evolutionary algorithm, called DSMGA-II, to\nefficiently solve optimization problems via exploiting problem substructures.\nThe proposed algorithm adopts pairwise linkage detection and stores the\ninformation in the form of dependency structure matrix (DSM). A new linkage\nmodel, called the incremental linkage set, is then constructed by using the\nDSM. Inspired by the idea of optimal mixing, the restricted mixing and the back\nmixing are proposed. The former aims at efficient exploration under certain\nconstrains. The latter aims at exploitation by refining the DSM so as to reduce\nunnecessary evaluations. Experimental results show that DSMGA-II outperforms\nLT-GOMEA and hBOA in terms of number of function evaluations on the\nconcatenated/folded/cyclic trap problems, NK-landscape problems with various\ndegrees of overlapping, 2D Ising spin-glass problems, and MAX-SAT. The\ninvestigation of performance comparison with P3 is also included.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 05:42:51 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Hsu", "Shih-Huan", ""], ["Yu", "Tian-Li", ""]]}]