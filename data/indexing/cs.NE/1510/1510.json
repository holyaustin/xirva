[{"id": "1510.00149", "submitter": "Song Han", "authors": "Song Han, Huizi Mao, William J. Dally", "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding", "comments": "Published as a conference paper at ICLR 2016 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 09:03:44 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 23:53:10 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 06:35:19 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 09:04:04 GMT"}, {"version": "v5", "created": "Mon, 15 Feb 2016 06:25:40 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Han", "Song", ""], ["Mao", "Huizi", ""], ["Dally", "William J.", ""]]}, {"id": "1510.00419", "submitter": "Maxim Buzdalov", "authors": "Viktor Arkhipov, Maxim Buzdalov, Anatoly Shalyto", "title": "An Asynchronous Implementation of the Limited Memory CMA-ES", "comments": "9 pages, 4 figures, 4 tables; this is a full version of a paper which\n  has been accepted as a poster to IEEE ICMLA conference 2015", "journal-ref": null, "doi": "10.1109/ICMLA.2015.97", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our asynchronous implementation of the LM-CMA-ES algorithm, which\nis a modern evolution strategy for solving complex large-scale continuous\noptimization problems. Our implementation brings the best results when the\nnumber of cores is relatively high and the computational complexity of the\nfitness function is also high. The experiments with benchmark functions show\nthat it is able to overcome its origin on the Sphere function, reaches certain\nthresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly\nperforms much better than the original version on the Rastrigin function.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 20:34:37 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Arkhipov", "Viktor", ""], ["Buzdalov", "Maxim", ""], ["Shalyto", "Anatoly", ""]]}, {"id": "1510.00556", "submitter": "Nasser Metwally NM", "authors": "M. Zidan, A. Sagheer and N. Metwally", "title": "Autonomous Perceptron Neural Network Inspired from Quantum computing", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This abstract will be modified after correcting the minor error in Eq.(2)\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 10:48:56 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 07:06:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Zidan", "M.", ""], ["Sagheer", "A.", ""], ["Metwally", "N.", ""]]}, {"id": "1510.01378", "submitter": "Phil\\'emon Brakel", "authors": "C\\'esar Laurent, Gabriel Pereyra, Phil\\'emon Brakel, Ying Zhang and\n  Yoshua Bengio", "title": "Batch Normalized Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are powerful models for sequential data that\nhave the potential to learn long-term dependencies. However, they are\ncomputationally expensive to train and difficult to parallelize. Recent work\nhas shown that normalizing intermediate representations of neural networks can\nsignificantly improve convergence rates in feedforward neural networks . In\nparticular, batch normalization, which uses mini-batch statistics to\nstandardize features, was shown to significantly reduce training time. In this\npaper, we show that applying batch normalization to the hidden-to-hidden\ntransitions of our RNNs doesn't help the training procedure. We also show that\nwhen applied to the input-to-hidden transitions, batch normalization can lead\nto a faster convergence of the training criterion but doesn't seem to improve\nthe generalization performance on both our language modelling and speech\nrecognition tasks. All in all, applying batch normalization to RNNs turns out\nto be more challenging than applying it to feedforward networks, but certain\nvariants of it can still be beneficial.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 21:45:31 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Laurent", "C\u00e9sar", ""], ["Pereyra", "Gabriel", ""], ["Brakel", "Phil\u00e9mon", ""], ["Zhang", "Ying", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1510.01624", "submitter": "Christian Igel", "authors": "Oswin Krause, Asja Fischer, Christian Igel", "title": "Population-Contrastive-Divergence: Does Consistency help with RBM\n  training?", "comments": "An updated version is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the log-likelihood gradient with respect to the parameters of a\nRestricted Boltzmann Machine (RBM) typically requires sampling using Markov\nChain Monte Carlo (MCMC) techniques. To save computation time, the Markov\nchains are only run for a small number of steps, which leads to a biased\nestimate. This bias can cause RBM training algorithms such as Contrastive\nDivergence (CD) learning to deteriorate. We adopt the idea behind Population\nMonte Carlo (PMC) methods to devise a new RBM training algorithm termed\nPopulation-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a\nconsistent estimate and may have a significantly lower bias. Its computational\noverhead is negligible compared to CD. However, the variance of the gradient\nestimate increases. We experimentally show that pop-CD can significantly\noutperform CD. In many cases, we observed a smaller bias and achieved higher\nlog-likelihood values. However, when the RBM distribution has many hidden\nneurons, the consistent estimate of pop-CD may still have a considerable bias\nand the variance of the gradient estimate requires a smaller learning rate.\nThus, despite its superior theoretical properties, it is not advisable to use\npop-CD in its current form on large problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 15:29:04 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 12:30:35 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 17:37:27 GMT"}, {"version": "v4", "created": "Wed, 28 Jun 2017 09:35:12 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Krause", "Oswin", ""], ["Fischer", "Asja", ""], ["Igel", "Christian", ""]]}, {"id": "1510.02513", "submitter": "Hossein Sharifi Noghabi", "authors": "H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei", "title": "A novel mutation operator based on the union of fitness and design\n  spaces information for Differential Evolution", "comments": null, "journal-ref": "Soft Comput (2016)", "doi": "10.1007/s00500-016-2359-8", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Evolution (DE) is one of the most successful and powerful\nevolutionary algorithms for global optimization problem. The most important\noperator in this algorithm is mutation operator which parents are selected\nrandomly to participate in it. Recently, numerous papers are tried to make this\noperator more intelligent by selection of parents for mutation intelligently.\nThe intelligent selection for mutation vectors is performed by applying design\nspace (also known as decision space) criterion or fitness space criterion,\nhowever, in both cases, half of valuable information of the problem space is\ndisregarded. In this article, a Universal Differential Evolution (UDE) is\nproposed which takes advantage of both design and fitness spaces criteria for\nintelligent selection of mutation vectors. The experimental analysis on UDE are\nperformed on CEC2005 benchmarks and the results stated that UDE significantly\nimproved the performance of differential evolution in comparison with other\nmethods that only use one criterion for intelligent selection.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 21:17:37 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 07:03:11 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Noghabi", "H. Sharifi", ""], ["Mashhadi", "H. Rajabi", ""], ["Shojaei", "K.", ""]]}, {"id": "1510.02516", "submitter": "Hossein Sharifi Noghabi", "authors": "H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei", "title": "Differential Evolution with Generalized Mutation Operator for Parameters\n  Optimization in Gene Selection for Cancer Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Evolution (DE) proved to be one of the most successful\nevolutionary algorithms for global optimization purposes in continuous\nproblems. The core operator in DE is mutation which can provide the algorithm\nwith both exploration and exploitation. In this article, a new notation for DE\nis proposed which has a formula that can be utilized for generating and\nextracting novel mutations and by applying this new notation, four novel\nmutations are proposed. More importantly, by combining these novel trial vector\ngeneration strategies and four other well-known ones, we proposed Generalized\nMutation Differential Evolution (GMDE) that takes advantage of two mutation\npools that have both explorative and exploitative strategies inside them.\nResults and experimental analysis are performed on CEC2005 benchmarks and the\nresults stated that GMDE is surprisingly competitive and significantly improved\nthe performance of this algorithm. Finally, GMDE is also applied to parameters\noptimization, modification and improvement of a feature selection method for\ncancer classification purposes over gene expression microarray profiles.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 21:21:40 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 07:42:22 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Noghabi", "H. Sharifi", ""], ["Mashhadi", "H. Rajabi", ""], ["Shojaei", "K.", ""]]}, {"id": "1510.02693", "submitter": "ShiLiang Zhang", "authors": "ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "comments": "4 pages, 1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new structure for memory neural networks, called feedforward\nsequential memory networks (FSMN), which can learn long-term dependency without\nusing recurrent feedback. The proposed FSMN is a standard feedforward neural\nnetworks equipped with learnable sequential memory blocks in the hidden layers.\nIn this work, we have applied FSMN to several language modeling (LM) tasks.\nExperimental results have shown that the memory blocks in FSMN can learn\neffective representations of long history. Experiments have shown that FSMN\nbased language models can significantly outperform not only feedforward neural\nnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)\nLMs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:04:11 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Zhang", "ShiLiang", ""], ["Jiang", "Hui", ""], ["Wei", "Si", ""], ["Dai", "LiRong", ""]]}, {"id": "1510.02709", "submitter": "Kairan Sun", "authors": "Kairan Sun, Xu Wei, Gengtao Jia, Risheng Wang, Ruizhi Li", "title": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with continuously increasing scale of data, original back-propagation\nneural network based machine learning algorithm presents two non-trivial\nchallenges: huge amount of data makes it difficult to maintain both efficiency\nand accuracy; redundant data aggravates the system workload. This project is\nmainly focused on the solution to the issues above, combining deep learning\nalgorithm with cloud computing platform to deal with large-scale data. A\nMapReduce-based handwriting character recognizer will be designed in this\nproject to verify the efficiency improvement this mechanism will achieve on\ntraining and practical large-scale data. Careful discussion and experiment will\nbe developed to illustrate how deep learning algorithm works to train\nhandwritten digits data, how MapReduce is implemented on deep learning neural\nnetwork, and why this combination accelerates computation. Besides performance,\nthe scalability and robustness will be mentioned in this report as well. Our\nsystem comes with two demonstration software that visually illustrates our\nhandwritten digit recognition/encoding application.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:45:44 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Sun", "Kairan", ""], ["Wei", "Xu", ""], ["Jia", "Gengtao", ""], ["Wang", "Risheng", ""], ["Li", "Ruizhi", ""]]}, {"id": "1510.02855", "submitter": "Abraham Heifets", "authors": "Izhar Wallach and Michael Dzamba and Abraham Heifets", "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction\n  in Structure-based Drug Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks comprise a subclass of deep neural\nnetworks (DNN) with a constrained architecture that leverages the spatial and\ntemporal structure of the domain they model. Convolutional networks achieve the\nbest predictive performance in areas such as speech and image recognition by\nhierarchically composing simple local features into complex models. Although\nDNNs have been used in drug discovery for QSAR and ligand-based bioactivity\npredictions, none of these models have benefited from this powerful\nconvolutional architecture. This paper introduces AtomNet, the first\nstructure-based, deep convolutional neural network designed to predict the\nbioactivity of small molecules for drug discovery applications. We demonstrate\nhow to apply the convolutional concepts of feature locality and hierarchical\ncomposition to the modeling of bioactivity and chemical interactions. In\nfurther contrast to existing DNN techniques, we show that AtomNet's application\nof local convolutional filters to structural target information successfully\npredicts new active molecules for targets with no previously known modulators.\nFinally, we show that AtomNet outperforms previous docking approaches on a\ndiverse set of benchmarks by a large margin, achieving an AUC greater than 0.9\non 57.8% of the targets in the DUDE benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 00:09:00 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Wallach", "Izhar", ""], ["Dzamba", "Michael", ""], ["Heifets", "Abraham", ""]]}, {"id": "1510.02969", "submitter": "Pooya Khorrami", "authors": "Pooya Khorrami, Tom Le Paine, Thomas S. Huang", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression\n  Recognition?", "comments": "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Tables 2\n  and 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being the appearance-based classifier of choice in recent years,\nrelatively few works have examined how much convolutional neural networks\n(CNNs) can improve performance on accepted expression recognition benchmarks\nand, more importantly, examine what it is they actually learn. In this work,\nnot only do we show that CNNs can achieve strong performance, but we also\nintroduce an approach to decipher which portions of the face influence the\nCNN's predictions. First, we train a zero-bias CNN on facial expression data\nand achieve, to our knowledge, state-of-the-art performance on two expression\nrecognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto\nFace Dataset (TFD). We then qualitatively analyze the network by visualizing\nthe spatial patterns that maximally excite different neurons in the\nconvolutional layers and show how they resemble Facial Action Units (FAUs).\nFinally, we use the FAU labels provided in the CK+ dataset to verify that the\nFAUs observed in our filter visualizations indeed align with the subject's\nfacial movements.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 18:53:21 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 06:12:07 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 03:07:21 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Khorrami", "Pooya", ""], ["Paine", "Tom Le", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1510.03009", "submitter": "Zhouhan Lin", "authors": "Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, Yoshua Bengio", "title": "Neural Networks with Few Multiplications", "comments": "Published as a conference paper at ICLR 2016. 9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most deep learning algorithms training is notoriously time consuming.\nSince most of the computation in training neural networks is typically spent on\nfloating point multiplications, we investigate an approach to training that\neliminates the need for most of these. Our method consists of two parts: First\nwe stochastically binarize weights to convert multiplications involved in\ncomputing hidden states to sign changes. Second, while back-propagating error\nderivatives, in addition to binarizing the weights, we quantize the\nrepresentations at each layer to convert the remaining multiplications into\nbinary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10,\nSVHN) show that this approach not only does not hurt classification performance\nbut can result in even better performance than standard stochastic gradient\ndescent training, paving the way to fast, hardware-friendly training of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 04:32:39 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 20:16:10 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 05:24:30 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Lin", "Zhouhan", ""], ["Courbariaux", "Matthieu", ""], ["Memisevic", "Roland", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1510.03765", "submitter": "Nicholas Allgaier Ph.D.", "authors": "Nicholas Allgaier, Tobias Banaschewski, Gareth Barker, Arun L.W.\n  Bokde, Josh C. Bongard, Uli Bromberg, Christian B\\\"uchel, Anna Cattrell,\n  Patricia J.Conrod, Christopher M. Danforth, Sylvane Desrivi\\`eres, Peter S.\n  Dodds, Herta Flor, Vincent Frouin, J\\\"urgen Gallinat, Penny Gowland, Andreas\n  Heinz, Bernd Ittermann, Scott Mackey, Jean-Luc Martinot, Kevin Murphy, Frauke\n  Nees, Dimitri Papadopoulos-Orfanos, Luise Poustka, Michael N. Smolka, Henrik\n  Walter, Robert Whelan, Gunter Schumann, Hugh Garavan, IMAGEN Consortium", "title": "Nonlinear functional mapping of the human brain", "comments": "21 pages, 12 figures, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neuroimaging has truly become data rich, and novel analytical\nmethods capable of gleaning meaningful information from large stores of imaging\ndata are in high demand. Those methods that might also be applicable on the\nlevel of individual subjects, and thus potentially useful clinically, are of\nspecial interest. In the present study, we introduce just such a method, called\nnonlinear functional mapping (NFM), and demonstrate its application in the\nanalysis of resting state fMRI from a 242-subject subset of the IMAGEN project,\na European study of adolescents that includes longitudinal phenotypic,\nbehavioral, genetic, and neuroimaging data. NFM employs a computational\ntechnique inspired by biological evolution to discover and mathematically\ncharacterize interactions among ROI (regions of interest), without making\nlinear or univariate assumptions. We show that statistics of the resulting\ninteraction relationships comport with recent independent work, constituting a\npreliminary cross-validation. Furthermore, nonlinear terms are ubiquitous in\nthe models generated by NFM, suggesting that some of the interactions\ncharacterized here are not discoverable by standard linear methods of analysis.\nWe discuss one such nonlinear interaction in the context of a direct comparison\nwith a procedure involving pairwise correlation, designed to be an analogous\nlinear version of functional mapping. We find another such interaction that\nsuggests a novel distinction in brain function between drinking and\nnon-drinking adolescents: a tighter coupling of ROI associated with emotion,\nreward, and interoceptive processes such as thirst, among drinkers. Finally, we\noutline many improvements and extensions of the methodology to reduce\ncomputational expense, complement other analytical tools like graph-theoretic\nanalysis, and allow for voxel level NFM to eliminate the necessity of ROI\nselection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 20:56:47 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Allgaier", "Nicholas", ""], ["Banaschewski", "Tobias", ""], ["Barker", "Gareth", ""], ["Bokde", "Arun L. W.", ""], ["Bongard", "Josh C.", ""], ["Bromberg", "Uli", ""], ["B\u00fcchel", "Christian", ""], ["Cattrell", "Anna", ""], ["Conrod", "Patricia J.", ""], ["Danforth", "Christopher M.", ""], ["Desrivi\u00e8res", "Sylvane", ""], ["Dodds", "Peter S.", ""], ["Flor", "Herta", ""], ["Frouin", "Vincent", ""], ["Gallinat", "J\u00fcrgen", ""], ["Gowland", "Penny", ""], ["Heinz", "Andreas", ""], ["Ittermann", "Bernd", ""], ["Mackey", "Scott", ""], ["Martinot", "Jean-Luc", ""], ["Murphy", "Kevin", ""], ["Nees", "Frauke", ""], ["Papadopoulos-Orfanos", "Dimitri", ""], ["Poustka", "Luise", ""], ["Smolka", "Michael N.", ""], ["Walter", "Henrik", ""], ["Whelan", "Robert", ""], ["Schumann", "Gunter", ""], ["Garavan", "Hugh", ""], ["Consortium", "IMAGEN", ""]]}, {"id": "1510.03776", "submitter": "Michiel Hermans", "authors": "Michiel Hermans, Thomas Van Vaerenbergh", "title": "Towards Trainable Media: Using Waves for Neural Network-Style Training", "comments": "submitted to Scientific Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the concept of using the interaction between waves and\na trainable medium in order to construct a matrix-vector multiplier. In\nparticular we study such a device in the context of the backpropagation\nalgorithm, which is commonly used for training neural networks. Here, the\nweights of the connections between neurons are trained by multiplying a\n`forward' signal with a backwards propagating `error' signal. We show that this\nconcept can be extended to trainable media, where the gradient for the local\nwave number is given by multiplying signal waves and error waves. We provide a\nnumerical example of such a system with waves traveling freely in a trainable\nmedium, and we discuss a potential way to build such a device in an integrated\nphotonics chip.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 14:17:55 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Hermans", "Michiel", ""], ["Van Vaerenbergh", "Thomas", ""]]}, {"id": "1510.03820", "submitter": "Ye Zhang", "authors": "Ye Zhang and Byron Wallace", "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional\n  Neural Networks for Sentence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently achieved remarkably strong\nperformance on the practically important task of sentence classification (kim\n2014, kalchbrenner 2014, johnson 2014). However, these models require\npractitioners to specify an exact model architecture and set accompanying\nhyperparameters, including the filter region size, regularization parameters,\nand so on. It is currently unknown how sensitive model performance is to\nchanges in these configurations for the task of sentence classification. We\nthus conduct a sensitivity analysis of one-layer CNNs to explore the effect of\narchitecture components on model performance; our aim is to distinguish between\nimportant and comparatively inconsequential design decisions for sentence\nclassification. We focus on one-layer CNNs (to the exclusion of more complex\nmodels) due to their comparative simplicity and strong empirical performance,\nwhich makes it a modern standard baseline method akin to Support Vector Machine\n(SVMs) and logistic regression. We derive practical advice from our extensive\nempirical results for those interested in getting the most out of CNNs for\nsentence classification in real world settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 19:00:57 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 19:26:44 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2016 07:01:52 GMT"}, {"version": "v4", "created": "Wed, 6 Apr 2016 23:20:27 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Zhang", "Ye", ""], ["Wallace", "Byron", ""]]}, {"id": "1510.03826", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Tim Oates, James Lo", "title": "Adopting Robustness and Optimality in Fitting and Learning", "comments": "This paper has been withdrawn by the authors due to some errors and\n  confusions in terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalized a modified exponentialized estimator by pushing the\nrobust-optimal (RO) index $\\lambda$ to $-\\infty$ for achieving robustness to\noutliers by optimizing a quasi-Minimin function. The robustness is realized and\ncontrolled adaptively by the RO index without any predefined threshold.\nOptimality is guaranteed by expansion of the convexity region in the Hessian\nmatrix to largely avoid local optima. Detailed quantitative analysis on both\nrobustness and optimality are provided. The results of proposed experiments on\nfitting tasks for three noisy non-convex functions and the digits recognition\ntask on the MNIST dataset consolidate the conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 19:14:43 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 04:52:35 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 21:38:52 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""], ["Lo", "James", ""]]}, {"id": "1510.03891", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva, Julie Henriques, Laurent Larger, and Juan-Pablo\n  Ortega", "title": "Nonlinear memory capacity of parallel time-delay reservoir computers in\n  the processing of multidimensional signals", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the reservoir design problem in the context of\ndelay-based reservoir computers for multidimensional input signals, parallel\narchitectures, and real-time multitasking. First, an approximating reservoir\nmodel is presented in those frameworks that provides an explicit functional\nlink between the reservoir parameters and architecture and its performance in\nthe execution of a specific task. Second, the inference properties of the ridge\nregression estimator in the multivariate context is used to assess the impact\nof finite sample training on the decrease of the reservoir capacity. Finally,\nan empirical study is conducted that shows the adequacy of the theoretical\nresults with the empirical performances exhibited by various reservoir\narchitectures in the execution of several nonlinear tasks with multidimensional\ninputs. Our results confirm the robustness properties of the parallel reservoir\narchitecture with respect to task misspecification and parameter choice that\nhad already been documented in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 20:56:49 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Henriques", "Julie", ""], ["Larger", "Laurent", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1510.03931", "submitter": "Wei Zhang", "authors": "Wei Zhang, Yang Yu, Bowen Zhou", "title": "Structured Memory for Neural Turing Machines", "comments": "4 pages, accepted to Reasoning, Attention, Memory (RAM) NIPS 2015\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neural Turing Machines (NTM) contain memory component that simulates \"working\nmemory\" in the brain to store and retrieve information to ease simple\nalgorithms learning. So far, only linearly organized memory is proposed, and\nduring experiments, we observed that the model does not always converge, and\noverfits easily when handling certain tasks. We think memory component is key\nto some faulty behaviors of NTM, and better organization of memory component\ncould help fight those problems. In this paper, we propose several different\nstructures of memory for NTM, and we proved in experiments that two of our\nproposed structured-memory NTMs could lead to better convergence, in term of\nspeed and prediction accuracy on copy task and associative recall task as in\n(Graves et al. 2014).\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 00:08:17 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 21:52:04 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2015 03:12:49 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Zhang", "Wei", ""], ["Yu", "Yang", ""], ["Zhou", "Bowen", ""]]}, {"id": "1510.04609", "submitter": "Bharat Singh", "authors": "Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin\n  Taylor", "title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "comments": "ICMLA 2015, deep learning, adaptive learning rates for training,\n  layer specific learning rate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of deep learning architectures is resulting in\ntraining time requiring weeks or even months. This slow training is due in part\nto vanishing gradients, in which the gradients used by back-propagation are\nextremely large for weights connecting deep layers (layers near the output\nlayer), and extremely small for shallow layers (near the input layer); this\nresults in slow learning in the shallow layers. Additionally, it has also been\nshown that in highly non-convex problems, such as deep neural networks, there\nis a proliferation of high-error low curvature saddle points, which slows down\nlearning dramatically. In this paper, we attempt to overcome the two above\nproblems by proposing an optimization method for training deep neural networks\nwhich uses learning rates which are both specific to each layer in the network\nand adaptive to the curvature of the function, increasing the learning rate at\nlow curvature points. This enables us to speed up learning in the shallow\nlayers of the network and quickly escape high-error low curvature saddle\npoints. We test our method on standard image classification datasets such as\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\nas well as reduces the required training time over standard algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:31:46 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Singh", "Bharat", ""], ["De", "Soham", ""], ["Zhang", "Yangmuzi", ""], ["Goldstein", "Thomas", ""], ["Taylor", "Gavin", ""]]}, {"id": "1510.04709", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Eva Hasler", "title": "Multilingual Image Description with Neural Sequence Models", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to multi-language image description\nbringing together insights from neural machine translation and neural image\ndescription. To create a description of an image for a given target language,\nour sequence generation models condition on feature vectors from the image, the\ndescription from the source language, and/or a multimodal vector computed over\nthe image and a description in the source language. In image description\nexperiments on the IAPR-TC12 dataset of images aligned with English and German\nsentences, we find significant and substantial improvements in BLEU4 and Meteor\nscores for models trained over multiple languages, compared to a monolingual\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 20:29:21 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:04:35 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Hasler", "Eva", ""]]}, {"id": "1510.04781", "submitter": "Haohan Wang", "authors": "Haohan Wang and Bhiksha Raj", "title": "A Survey: Time Travel in Deep Learning Space: An Introduction to Deep\n  Learning Models and How Deep Learning Models Evolved from the Initial Ideas", "comments": "43 pages, 31 figures. Fix typos in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report will show the history of deep learning evolves. It will trace\nback as far as the initial belief of connectionism modelling of brain, and come\nback to look at its early stage realization: neural networks. With the\nbackground of neural network, we will gradually introduce how convolutional\nneural network, as a representative of deep discriminative models, is developed\nfrom neural networks, together with many practical techniques that can help in\noptimization of neural networks. On the other hand, we will also trace back to\nsee the evolution history of deep generative models, to see how researchers\nbalance the representation power and computation complexity to reach Restricted\nBoltzmann Machine and eventually reach Deep Belief Nets. Further, we will also\nlook into the development history of modelling time series data with neural\nnetworks. We start with Time Delay Neural Networks and move further to\ncurrently famous model named Recurrent Neural Network and its extension Long\nShort Term Memory. We will also briefly look into how to construct deep\nrecurrent neural networks. Finally, we will conclude this report with some\ninteresting open-ended questions of deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 05:37:06 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 19:41:13 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Wang", "Haohan", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1510.04953", "submitter": "Benjamin Krause", "authors": "Ben Krause", "title": "Optimizing and Contrasting Recurrent Neural Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have long been recognized for their\npotential to model complex time series. However, it remains to be determined\nwhat optimization techniques and recurrent architectures can be used to best\nrealize this potential. The experiments presented take a deep look into Hessian\nfree optimization, a powerful second order optimization method that has shown\npromising results, but still does not enjoy widespread use. This algorithm was\nused to train to a number of RNN architectures including standard RNNs, long\nshort-term memory, multiplicative RNNs, and stacked RNNs on the task of\ncharacter prediction. The insights from these experiments led to the creation\nof a new multiplicative LSTM hybrid architecture that outperformed both LSTM\nand multiplicative RNNs. When tested on a larger scale, multiplicative LSTM\nachieved character level modelling results competitive with the state of the\nart for RNNs using very different methodology.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 17:16:14 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Krause", "Ben", ""]]}, {"id": "1510.05328", "submitter": "Pedro Tabacof", "authors": "Pedro Tabacof, Eduardo Valle", "title": "Exploring the Space of Adversarial Images", "comments": "Copyright 2016 IEEE. This manuscript was accepted at the IEEE\n  International Joint Conference on Neural Networks (IJCNN) 2016. We will link\n  the published version as soon as the DOI is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have raised questions regarding the robustness and\nsecurity of deep neural networks. In this work we formalize the problem of\nadversarial images given a pretrained classifier, showing that even in the\nlinear case the resulting optimization problem is nonconvex. We generate\nadversarial images using shallow and deep classifiers on the MNIST and ImageNet\ndatasets. We probe the pixel space of adversarial images using noise of varying\nintensity and distribution. We bring novel visualizations that showcase the\nphenomenon and its high variability. We show that adversarial images appear in\nlarge regions in the pixel space, but that, for the same task, a shallow\nclassifier seems more robust to adversarial images than a deep convolutional\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 00:54:37 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2015 17:40:25 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 01:14:49 GMT"}, {"version": "v4", "created": "Tue, 10 May 2016 22:36:20 GMT"}, {"version": "v5", "created": "Thu, 23 Jun 2016 04:14:32 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Tabacof", "Pedro", ""], ["Valle", "Eduardo", ""]]}, {"id": "1510.05705", "submitter": "Ella Gale", "authors": "Ella Gale", "title": "Single Memristor Logic Gates: From NOT to a Full Adder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.mes-hall cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristors have been suggested as a novel route to neuromorphic computing\nbased on the similarity between them and neurons (specifically synapses and ion\npumps). The d.c. action of the memristor is a current spike which imparts a\nshort-term memory to the device. Here it is demonstrated that this short-term\nmemory works exactly like habituation (e.g. in \\emph{Aplysia}). We elucidate\nthe physical rules, based on energy conservation, governing the interaction of\nthese current spikes: summation, `bounce-back', directionality and `diminishing\nreturns'. Using these rules, we introduce 4 different logical systems to\nimplement sequential logic in the memristor and demonstrate how sequential\nlogic works by instantiating a NOT gate, an AND gate, an XOR gate and a Full\nAdder with a single memristor. The Full Adder makes use of the memristor's\nshort-term memory to add together three binary values and outputs the sum, the\ncarry digit and even the order they were input in. A memristor full adder also\noutputs the arithmetical sum of bits, allowing for a logically (but not\nphysically) reversible system. Essentially, we can replace an input/output port\nwith an extra time-step, allowing a single memristor to do a hither-to\nunexpectedly large amount of computation. This makes up for the memristor's\nslow operation speed and may relate to how neurons do a similarly-large\ncomputation with such slow operations speeds. We propose that using spiking\nlogic, either in gates or as neuron-analogues, with plastic rewritable\nconnections between them, would allow the building of a neuromorphic computer.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 22:20:42 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Gale", "Ella", ""]]}, {"id": "1510.05711", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Qualitative Projection Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) abstract by demodulating the output of linear\nfilters. In this article, we refine this definition of abstraction to show that\nthe inputs of a DNN are abstracted with respect to the filters. Or, to restate,\nthe abstraction is qualified by the filters. This leads us to introduce the\nnotion of qualitative projection. We use qualitative projection to abstract\nMNIST hand-written digits with respect to the various dogs, horses, planes and\ncars of the CIFAR dataset. We then classify the MNIST digits according to the\nmagnitude of their dogness, horseness, planeness and carness qualities,\nillustrating the generality of qualitative projection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 22:38:09 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 08:42:54 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1510.05970", "submitter": "Jure \\v{Z}bontar", "authors": "Jure \\v{Z}bontar and Yann LeCun", "title": "Stereo Matching by Training a Convolutional Neural Network to Compare\n  Image Patches", "comments": null, "journal-ref": "JMLR 17(65):1-32, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting depth information from a rectified image\npair. Our approach focuses on the first stage of many stereo algorithms: the\nmatching cost computation. We approach the problem by learning a similarity\nmeasure on small image patches using a convolutional neural network. Training\nis carried out in a supervised manner by constructing a binary classification\ndata set with examples of similar and dissimilar pairs of patches. We examine\ntwo network architectures for this task: one tuned for speed, the other for\naccuracy. The output of the convolutional neural network is used to initialize\nthe stereo matching cost. A series of post-processing steps follow: cross-based\ncost aggregation, semiglobal matching, a left-right consistency check, subpixel\nenhancement, a median filter, and a bilateral filter. We evaluate our method on\nthe KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it\noutperforms other approaches on all three data sets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 17:15:05 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 19:53:41 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["\u017dbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "1510.06706", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski, Kisuk Lee and H. Sebastian Seung", "title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional\n  Networks on Multi-Core and Many-Core Shared Memory Machines", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS.2016.119", "report-no": null, "categories": "cs.NE cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks (ConvNets) have become a popular approach to computer\nvision. It is important to accelerate ConvNet training, which is\ncomputationally costly. We propose a novel parallel algorithm based on\ndecomposition into a set of tasks, most of which are convolutions or FFTs.\nApplying Brent's theorem to the task dependency graph implies that linear\nspeedup with the number of processors is attainable within the PRAM model of\nparallel computation, for wide network architectures. To attain such\nperformance on real shared-memory machines, our algorithm computes convolutions\nconverging on the same node of the network with temporal locality to reduce\ncache misses, and sums the convergent convolution outputs via an almost\nwait-free concurrent method to reduce time spent in critical sections. We\nimplement the algorithm with a publicly available software package called ZNN.\nBenchmarking with multi-core CPUs shows that ZNN can attain speedup roughly\nequal to the number of physical cores. We also show that ZNN can attain over\n90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are\nachieved for network architectures with widths that are in common use. The task\nparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism\nof previous algorithms is compatible with GPUs. Through examples, we show that\nZNN can be either faster or slower than certain GPU implementations depending\non specifics of the network architecture, kernel sizes, and density and size of\nthe output patch. ZNN may be less costly to develop and maintain, due to the\nrelative ease of general-purpose CPU programming.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 18:14:42 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1510.06925", "submitter": "Leigh Robinson", "authors": "Leigh Robinson, Benjamin Graham", "title": "Confusing Deep Convolution Networks by Relabelling", "comments": "Submitted to BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have become the gold standard for image\nrecognition tasks, demonstrating many current state-of-the-art results and even\nachieving near-human level performance on some tasks. Despite this fact it has\nbeen shown that their strong generalisation qualities can be fooled to\nmisclassify previously correctly classified natural images and give erroneous\nhigh confidence classifications to nonsense synthetic images. In this paper we\nextend that work, by presenting a straightforward way to perturb an image in\nsuch a way as to cause it to acquire any other label from within the dataset\nwhile leaving this perturbed image visually indistinguishable from the\noriginal.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 13:02:55 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 18:38:08 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Robinson", "Leigh", ""], ["Graham", "Benjamin", ""]]}, {"id": "1510.07146", "submitter": "Lorenzo Livi", "authors": "Enrico Maiorino, Filippo Maria Bianchi, Lorenzo Livi, Antonello Rizzi,\n  Alireza Sadeghian", "title": "Data-driven detrending of nonstationary fractal time series with echo\n  state networks", "comments": "Revised version", "journal-ref": null, "doi": "10.1016/j.ins.2016.12.015", "report-no": null, "categories": "physics.data-an cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data-driven approach for removing trends\n(detrending) from nonstationary, fractal and multifractal time series. We\nconsider real-valued time series relative to measurements of an underlying\ndynamical system that evolves through time. We assume that such a dynamical\nprocess is predictable to a certain degree by means of a class of recurrent\nnetworks called Echo State Network (ESN), which are capable to model a generic\ndynamical process. In order to isolate the superimposed (multi)fractal\ncomponent of interest, we define a data-driven filter by leveraging on the ESN\nprediction capability to identify the trend component of a given input time\nseries. Specifically, the (estimated) trend is removed from the original time\nseries and the residual signal is analyzed with the multifractal detrended\nfluctuation analysis procedure to verify the correctness of the detrending\nprocedure. In order to demonstrate the effectiveness of the proposed technique,\nwe consider several synthetic time series consisting of different types of\ntrends and fractal noise components with known characteristics. We also process\na real-world dataset, the sunspot time series, which is well-known for its\nmultifractal features and has recently gained attention in the complex systems\nfield. Results demonstrate the validity and generality of the proposed\ndetrending method based on ESNs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 13:38:13 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 18:19:30 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Maiorino", "Enrico", ""], ["Bianchi", "Filippo Maria", ""], ["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1510.07163", "submitter": "Maumita Bhattacharya", "authors": "Maumita Bhattacharya", "title": "Evolutionary Landscape and Management of Population Diversity", "comments": "Combinations of Intelligent Methods and Application, Smart\n  Innovations, Systems & Technologies series, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search ability of an Evolutionary Algorithm (EA) depends on the variation\namong the individuals in the population [3, 4, 8]. Maintaining an optimal level\nof diversity in the EA population is imperative to ensure that progress of the\nEA search is unhindered by premature convergence to suboptimal solutions.\nClearer understanding of the concept of population diversity, in the context of\nevolutionary search and premature convergence in particular, is the key to\ndesigning efficient EAs. To this end, this paper first presents a brief\nanalysis of the EA population diversity issues. Next we present an\ninvestigation on a counter-niching EA technique [4] that introduces and\nmaintains constructive diversity in the population. The proposed approach uses\ninformed genetic operations to reach promising, but unexplored or\nunder-explored areas of the search space, while discouraging premature local\nconvergence. Simulation runs on a suite of standard benchmark test functions\nwith Genetic Algorithm (GA) implementation shows promising results.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 16:56:11 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Bhattacharya", "Maumita", ""]]}, {"id": "1510.07208", "submitter": "Yuan Ma", "authors": "Joe Lemieux, Yuan Ma", "title": "Vehicle Speed Prediction using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Global optimization of the energy consumption of dual power source vehicles\nsuch as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug in\nfuel cell electric vehicles requires knowledge of the complete route\ncharacteristics at the beginning of the trip. One of the main characteristics\nis the vehicle speed profile across the route. The profile will translate\ndirectly into energy requirements for a given vehicle. However, the vehicle\nspeed that a given driver chooses will vary from driver to driver and from time\nto time, and may be slower, equal to, or faster than the average traffic flow.\nIf the specific driver speed profile can be predicted, the energy usage can be\noptimized across the route chosen. The purpose of this paper is to research the\napplication of Deep Learning techniques to this problem to identify at the\nbeginning of a drive cycle the driver specific vehicle speed profile for an\nindividual driver repeated drive cycle, which can be used in an optimization\nalgorithm to minimize the amount of fossil fuel energy used during the trip.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 05:52:59 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Lemieux", "Joe", ""], ["Ma", "Yuan", ""]]}, {"id": "1510.07957", "submitter": "Dan Lessin", "authors": "Dan Lessin, Don Fussell, Risto Miikkulainen, Sebastian Risi", "title": "Increasing Behavioral Complexity for Evolved Virtual Creatures with the\n  ESP Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs)\nhave employed the coevolution of morphology and control to produce high-impact\nwork in multiple fields, including graphics, evolutionary computation,\nrobotics, and artificial life. However, in contrast to fixed-morphology\ncreatures, there has been no clear increase in the behavioral complexity of\nEVCs in those two decades. This paper describes a method for moving beyond this\nlimit, making use of high-level human input in the form of a syllabus of\nintermediate learning tasks--along with mechanisms for preservation, reuse, and\ncombination of previously learned tasks. This method--named ESP for its three\ncomponents: encapsulation, syllabus, and pandemonium--is presented in two\ncomplementary versions: Fast ESP, which constrains later morphological changes\nto achieve linear growth in computation time as behavioral complexity is added,\nand General ESP, which allows this restriction to be removed when sufficient\ncomputational resources are available. Experiments demonstrate that the ESP\nmethod allows evolved virtual creatures to reach new levels of behavioral\ncomplexity in the co-evolution of morphology and control, approximately\ndoubling the previous state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 16:16:22 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Lessin", "Dan", ""], ["Fussell", "Don", ""], ["Miikkulainen", "Risto", ""], ["Risi", "Sebastian", ""]]}, {"id": "1510.08565", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao and Geoffrey Zweig and Baolin Peng", "title": "Attention with Intention for a Neural Network Conversation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a conversation or a dialogue process, attention and intention play\nintrinsic roles. This paper proposes a neural network based approach that\nmodels the attention and intention processes. It essentially consists of three\nrecurrent networks. The encoder network is a word-level model representing\nsource side sentences. The intention network is a recurrent network that models\nthe dynamics of the intention process. The decoder network is a recurrent\nnetwork produces responses to the input from the source side. It is a language\nmodel that is dependent on the intention and has an attention mechanism to\nattend to particular source side words, when predicting a symbol in the\nresponse. The model is trained end-to-end without labeling data. Experiments\nshow that this model generates natural responses to user inputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 05:31:28 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 02:17:15 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2015 07:26:01 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Yao", "Kaisheng", ""], ["Zweig", "Geoffrey", ""], ["Peng", "Baolin", ""]]}, {"id": "1510.08568", "submitter": "Wanru Gao", "authors": "Wanru Gao, Samadhi Nallaperuma and Frank Neumann", "title": "Feature-Based Diversity Optimization for Problem Instance Classification", "comments": "20 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behaviour of heuristic search methods is a challenge. This\neven holds for simple local search methods such as 2-OPT for the Traveling\nSalesperson problem. In this paper, we present a general framework that is able\nto construct a diverse set of instances that are hard or easy for a given\nsearch heuristic. Such a diverse set is obtained by using an evolutionary\nalgorithm for constructing hard or easy instances that are diverse with respect\nto different features of the underlying problem. Examining the constructed\ninstance sets, we show that many combinations of two or three features give a\ngood classification of the TSP instances in terms of whether they are hard to\nbe solved by 2-OPT.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 05:40:54 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 05:12:48 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 08:40:06 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Gao", "Wanru", ""], ["Nallaperuma", "Samadhi", ""], ["Neumann", "Frank", ""]]}, {"id": "1510.08829", "submitter": "Eric Hunsberger", "authors": "Eric Hunsberger and Chris Eliasmith", "title": "Spiking Deep Networks with LIF Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train spiking deep networks using leaky integrate-and-fire (LIF) neurons,\nand achieve state-of-the-art results for spiking networks on the CIFAR-10 and\nMNIST datasets. This demonstrates that biologically-plausible spiking LIF\nneurons can be integrated into deep networks can perform as well as other\nspiking models (e.g. integrate-and-fire). We achieved this result by softening\nthe LIF response function, such that its derivative remains bounded, and by\ntraining the network with noise to provide robustness against the variability\nintroduced by spikes. Our method is general and could be applied to other\nneuron types, including those used on modern neuromorphic hardware. Our work\nbrings more biological realism into modern image classification models, with\nthe hope that these models can inform how the brain performs this difficult\ntask. It also provides new methods for training deep networks to run on\nneuromorphic hardware, with the aim of fast, power-efficient image\nclassification for robotics applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 19:24:03 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Hunsberger", "Eric", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1510.08983", "submitter": "Yu Zhang", "authors": "Yu Zhang and Guoguo Chen and Dong Yu and Kaisheng Yao and Sanjeev\n  Khudanpur and James Glass", "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:40:14 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 09:48:01 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chen", "Guoguo", ""], ["Yu", "Dong", ""], ["Yao", "Kaisheng", ""], ["Khudanpur", "Sanjeev", ""], ["Glass", "James", ""]]}, {"id": "1510.08985", "submitter": "Yu Zhang", "authors": "Yu Zhang, Ekapol Chuangsuwanich, James Glass, Dong Yu", "title": "Prediction-Adaptation-Correction Recurrent Neural Networks for\n  Low-Resource Language Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of prediction-adaptation-correction\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\nPAC-RNN is comprised of a pair of neural networks in which a {\\it correction}\nnetwork uses auxiliary information given by a {\\it prediction} network to help\nestimate the state probability. The information from the correction network is\nalso used by the prediction network in a recurrent loop. Our model outperforms\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\nMoreover, transfer learning from a language that is similar to the target\nlanguage can help improve performance further.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:42:03 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chuangsuwanich", "Ekapol", ""], ["Glass", "James", ""], ["Yu", "Dong", ""]]}, {"id": "1510.09142", "submitter": "Greg Wayne", "authors": "Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval\n  Tassa, and Tom Erez", "title": "Learning Continuous Control Policies by Stochastic Value Gradients", "comments": "13 pages, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for learning continuous control policies using\nbackpropagation. It supports stochastic control by treating stochasticity in\nthe Bellman equation as a deterministic function of exogenous noise. The\nproduct is a spectrum of general policy gradient algorithms that range from\nmodel-free methods with value functions to model-based methods without value\nfunctions. We use learned models but only require observations from the\nenvironment in- stead of observations from model-predicted trajectories,\nminimizing the impact of compounded model errors. We apply these algorithms\nfirst to a toy stochastic control problem and then to several physics-based\ncontrol problems in simulation. One of these variants, SVG(1), shows the\neffectiveness of learning models, value functions, and policies simultaneously\nin continuous domains.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 16:07:51 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Heess", "Nicolas", ""], ["Wayne", "Greg", ""], ["Silver", "David", ""], ["Lillicrap", "Timothy", ""], ["Tassa", "Yuval", ""], ["Erez", "Tom", ""]]}, {"id": "1510.09202", "submitter": "Hongyu Guo Ph.D", "authors": "Hongyu Guo", "title": "Generating Text with Deep Reinforcement Learning", "comments": "Accepted to the NIPS2015 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel schema for sequence to sequence learning with a Deep\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\nto enable the decoder to first tackle easier portions of the sequences, and\nthen turn to cope with difficult parts. Specifically, in each iteration, an\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\ninput sequence, automatically create features to represent the internal states\nof and formulate a list of potential actions for the DQN. Take rephrasing a\nnatural sentence as an example. This list can contain ranked potential words.\nNext, the DQN learns to make decision on which action (e.g., word) will be\nselected from the list to modify the current decoded sequence. The newly\nmodified output sequence is subsequently used as the input to the DQN for the\nnext decoding iteration. In each iteration, we also bias the reinforcement\nlearning's attention to explore sequence portions which are previously\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\ndecode ten thousands natural sentences. Our experiments indicate that, when\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\nmethod performed competitively well when decoding sentences from the training\nset, but significantly outperformed the baseline when decoding unseen\nsentences, in terms of BLEU score obtained.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 19:02:53 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Guo", "Hongyu", ""]]}]