[{"id": "1406.0175", "submitter": "Z H", "authors": "Zahid Halim", "title": "Evolutionary Search in the Space of Rules for Creation of New Two-Player\n  Board Games", "comments": null, "journal-ref": null, "doi": "10.1142/S0218213013500280", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Games have always been a popular test bed for artificial intelligence\ntechniques. Game developers are always in constant search for techniques that\ncan automatically create computer games minimizing the developer's task. In\nthis work we present an evolutionary strategy based solution towards the\nautomatic generation of two player board games. To guide the evolutionary\nprocess towards games, which are entertaining, we propose a set of metrics.\nThese metrics are based upon different theories of entertainment in computer\ngames. This work also compares the entertainment value of the evolved games\nwith the existing popular board based games. Further to verify the\nentertainment value of the evolved games with the entertainment value of the\nhuman user a human user survey is conducted. In addition to the user survey we\ncheck the learnability of the evolved games using an artificial neural network\nbased controller. The proposed metrics and the evolutionary process can be\nemployed for generating new and entertaining board games, provided an initial\nsearch space is given to the evolutionary algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 16:04:04 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Halim", "Zahid", ""]]}, {"id": "1406.0416", "submitter": "Anya Johnson", "authors": "Anya Elaine Johnson, Eli Strauss, Rodney Pickett, Christoph Adami, Ian\n  Dworkin, and Heather J. Goldsby", "title": "More Bang For Your Buck: Quorum-Sensing Capabilities Improve the\n  Efficacy of Suicidal Altruism", "comments": "8 pages, 8 figures, ALIFE '14 conference", "journal-ref": "ALIFE 14: The Fourteenth Conference on the Synthesis and\n  Simulation of Living Systems. (2014) Vol. 14", "doi": null, "report-no": null, "categories": "cs.NE cs.CE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of evolution, an altruistic act that benefits the\nreceiving individual at the expense of the acting individual is a puzzling\nphenomenon. An extreme form of altruism can be found in colicinogenic E. coli.\nThese suicidal altruists explode, releasing colicins that kill unrelated\nindividuals, which are not colicin resistant. By committing suicide, the\naltruist makes it more likely that its kin will have less competition. The\nbenefits of this strategy rely on the number of competitors and kin nearby. If\nthe organism explodes at an inopportune time, the suicidal act may not harm any\ncompetitors. Communication could enable organisms to act altruistically when\nenvironmental conditions suggest that that strategy would be most beneficial.\nQuorum sensing is a form of communication in which bacteria produce a protein\nand gauge the amount of that protein around them. Quorum sensing is one means\nby which bacteria sense the biotic factors around them and determine when to\nproduce products, such as antibiotics, that influence competition. Suicidal\naltruists could use quorum sensing to determine when exploding is most\nbeneficial, but it is challenging to study the selective forces at work in\nmicrobes. To address these challenges, we use digital evolution (a form of\nexperimental evolution that uses self-replicating computer programs as\norganisms) to investigate the effects of enabling altruistic organisms to\ncommunicate via quorum sensing. We found that quorum-sensing altruists killed a\ngreater number of competitors per explosion, winning competitions against\nnon-communicative altruists. These findings indicate that quorum sensing could\nincrease the beneficial effect of altruism and the suite of conditions under\nwhich it will evolve.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 15:41:44 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Johnson", "Anya Elaine", ""], ["Strauss", "Eli", ""], ["Pickett", "Rodney", ""], ["Adami", "Christoph", ""], ["Dworkin", "Ian", ""], ["Goldsby", "Heather J.", ""]]}, {"id": "1406.0930", "submitter": "Aaron Lee", "authors": "Aaron Lee and Livia King", "title": "ACO Implementation for Sequence Alignment with Genetic Algorithms", "comments": "Report 6 pages, 4 figures, Supplementary material 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we implement Ant Colony Optimization (ACO) for sequence\nalignment. ACO is a meta-heuristic recently developed for nearest neighbor\napproximations in large, NP-hard search spaces. Here we use a genetic algorithm\napproach to evolve the best parameters for an ACO designed to align two\nsequences. We then used the best parameters found to interpolate approximate\noptimal parameters for a given string length within a range. The basis of our\ncomparison is the alignment given by the Needleman-Wunsch algorithm. We found\nthat ACO can indeed be applied to sequence alignment. While it is\ncomputationally expensive compared to other equivalent algorithms, it is a\npromising algorithm that can be readily applied to a variety of other\nbiological problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 02:44:57 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Lee", "Aaron", ""], ["King", "Livia", ""]]}, {"id": "1406.0968", "submitter": "Christopher Kirk PhD", "authors": "Christopher S Kirk", "title": "Integration of a Predictive, Continuous Time Neural Network into\n  Securities Market Trading Operations", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes recent development and test implementation of a\ncontinuous time recurrent neural network that has been configured to predict\nrates of change in securities. It presents outcomes in the context of popular\ntechnical analysis indicators and highlights the potential impact of continuous\npredictive capability on securities market trading operations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 08:25:56 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Kirk", "Christopher S", ""]]}, {"id": "1406.1078", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry\n  Bahdanau, Fethi Bougares, Holger Schwenk and Yoshua Bengio", "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation", "comments": "EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 17:47:08 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 20:07:13 GMT"}, {"version": "v3", "created": "Wed, 3 Sep 2014 00:25:02 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Cho", "Kyunghyun", ""], ["van Merrienboer", "Bart", ""], ["Gulcehre", "Caglar", ""], ["Bahdanau", "Dzmitry", ""], ["Bougares", "Fethi", ""], ["Schwenk", "Holger", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.1231", "submitter": "George Dahl", "authors": "George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov", "title": "Multi-task Neural Networks for QSAR Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although artificial neural networks have occasionally been used for\nQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in\nthe past, the literature has of late been dominated by other machine learning\ntechniques such as random forests. However, a variety of new neural net\ntechniques along with successful applications in other domains have renewed\ninterest in network approaches. In this work, inspired by the winning team's\nuse of neural networks in a recent QSAR competition, we used an artificial\nneural network to learn a function that predicts activities of compounds for\nmultiple assays at the same time. We conducted experiments leveraging recent\nmethods for dealing with overfitting in neural networks as well as other tricks\nfrom the neural networks literature. We compared our methods to alternative\nmethods reported to perform well on these tasks and found that our neural net\nmethods provided superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:00:05 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Dahl", "George E.", ""], ["Jaitly", "Navdeep", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1406.1509", "submitter": "Wojciech Ja\\'skowski", "authors": "Wojciech Ja\\'skowski", "title": "Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in\n  the Othello League", "comments": "Added technical report number", "journal-ref": "ICGA Journal 37(2), 2014, pp. 85-96", "doi": null, "report-no": "RA-06/2014", "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  N-tuple networks have been successfully used as position evaluation functions\nfor board games such as Othello or Connect Four. The effectiveness of such\nnetworks depends on their architecture, which is determined by the placement of\nconstituent n-tuples, sequences of board locations, providing input to the\nnetwork. The most popular method of placing n-tuples consists in randomly\ngenerating a small number of long, snake-shaped board location sequences. In\ncomparison, we show that learning n-tuple networks is significantly more\neffective if they involve a large number of systematically placed, short,\nstraight n-tuples. Moreover, we demonstrate that in order to obtain the best\nperformance and the steepest learning curve for Othello it is enough to use\nn-tuples of size just 2, yielding a network consisting of only 288 weights. The\nbest such network evolved in this study has been evaluated in the online\nOthello League, obtaining the performance of nearly 96% --- more than any other\nplayer to date.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 20:10:48 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 10:06:29 GMT"}, {"version": "v3", "created": "Wed, 25 Jun 2014 20:12:19 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Ja\u015bkowski", "Wojciech", ""]]}, {"id": "1406.1626", "submitter": "Khalid Raza", "authors": "Khalid Raza and Mahish Kohli", "title": "Ant Colony Optimization for Inferring Key Gene Interactions", "comments": "8 pages, 2 figures and 4 tables", "journal-ref": "Proc. of 9th INDIACom-2015, 2nd International Conference on\n  Computing for Sustainable Global Development, March 11-13, 2015 pp. 1242-1246", "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring gene interaction network from gene expression data is an important\ntask in systems biology research. The gene interaction network, especially key\ninteractions, plays an important role in identifying biomarkers for disease\nthat further helps in drug design. Ant colony optimization is an optimization\nalgorithm based on natural evolution and has been used in many optimization\nproblems. In this paper, we applied ant colony optimization algorithm for\ninferring the key gene interactions from gene expression data. The algorithm\nhas been tested on two different kinds of benchmark datasets and observed that\nit successfully identify some key gene interactions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 10:06:35 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Raza", "Khalid", ""], ["Kohli", "Mahish", ""]]}, {"id": "1406.1691", "submitter": "Vanessa Lange", "authors": "Vanessa Lange, Manuel Schmitt, Rolf Wanka", "title": "Towards a Better Understanding of the Local Attractor in Particle Swarm\n  Optimization: Speed and Solution Quality", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-11298-5_10", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristic\nfor solving continuous optimization problems. Although this technique is widely\nused, the understanding of the mechanisms that make swarms so successful is\nstill limited. We present the first substantial experimental investigation of\nthe influence of the local attractor on the quality of exploration and\nexploitation. We compare in detail classical PSO with the social-only variant\nwhere local attractors are ignored. To measure the exploration capabilities, we\ndetermine how frequently both variants return results in the neighborhood of\nthe global optimum. We measure the quality of exploitation by considering only\nfunction values from runs that reached a search point sufficiently close to the\nglobal optimum and then comparing in how many digits such values still deviate\nfrom the global minimum value. It turns out that the local attractor\nsignificantly improves the exploration, but sometimes reduces the quality of\nthe exploitation. As a compromise, we propose and evaluate a hybrid PSO which\nswitches off its local attractors at a certain point in time. The effects\nmentioned can also be observed by measuring the potential of the swarm.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 14:00:46 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Lange", "Vanessa", ""], ["Schmitt", "Manuel", ""], ["Wanka", "Rolf", ""]]}, {"id": "1406.1827", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Christopher Potts, Christopher D. Manning", "title": "Recursive Neural Networks Can Learn Logical Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured recursive neural networks (TreeRNNs) for sentence meaning\nhave been successful for many applications, but it remains an open question\nwhether the fixed-length representations that they learn can support tasks as\ndemanding as logical deduction. We pursue this question by evaluating whether\ntwo such models---plain TreeRNNs and tree-structured neural tensor networks\n(TreeRNTNs)---can correctly learn to identify logical relationships such as\nentailment and contradiction using these representations. In our first set of\nexperiments, we generate artificial data from a logical grammar and use it to\nevaluate the models' ability to learn to handle basic relational reasoning,\nrecursive structures, and quantification. We then evaluate the models on the\nmore natural SICK challenge data. Both models perform competitively on the SICK\ndata and generalize well in all three experiments on simulated data, suggesting\nthat they can learn suitable representations for logical inference in natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:09:27 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 20:37:33 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 19:48:45 GMT"}, {"version": "v4", "created": "Thu, 14 May 2015 19:37:38 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Potts", "Christopher", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1406.1831", "submitter": "Ben Poole", "authors": "Ben Poole, Jascha Sohl-Dickstein, Surya Ganguli", "title": "Analyzing noise in autoencoders and deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders have emerged as a useful framework for unsupervised learning of\ninternal representations, and a wide variety of apparently conceptually\ndisparate regularization techniques have been proposed to generate useful\nfeatures. Here we extend existing denoising autoencoders to additionally inject\nnoise before the nonlinearity, and at the hidden unit activations. We show that\na wide variety of previous methods, including denoising, contractive, and\nsparse autoencoders, as well as dropout can be interpreted using this\nframework. This noise injection framework reaps practical benefits by providing\na unified strategy to develop new internal representations by designing the\nnature of the injected noise. We show that noisy autoencoders outperform\ndenoising autoencoders at the very task of denoising, and are competitive with\nother single-layer techniques on MNIST, and CIFAR-10. We also show that types\nof noise other than dropout improve performance in a deep network through\nsparsifying, decorrelating, and spreading information across representations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:49:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Poole", "Ben", ""], ["Sohl-Dickstein", "Jascha", ""], ["Ganguli", "Surya", ""]]}, {"id": "1406.1833", "submitter": "Kenneth Stanley", "authors": "Paul A. Szerlip, Gregory Morse, Justin K. Pugh, and Kenneth O. Stanley", "title": "Unsupervised Feature Learning through Divergent Discriminative Feature\n  Accumulation", "comments": "Corrected citation formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike unsupervised approaches such as autoencoders that learn to reconstruct\ntheir inputs, this paper introduces an alternative approach to unsupervised\nfeature learning called divergent discriminative feature accumulation (DDFA)\nthat instead continually accumulates features that make novel discriminations\namong the training set. Thus DDFA features are inherently discriminative from\nthe start even though they are trained without knowledge of the ultimate\nclassification problem. Interestingly, DDFA also continues to add new features\nindefinitely (so it does not depend on a hidden layer size), is not based on\nminimizing error, and is inherently divergent instead of convergent, thereby\nproviding a unique direction of research for unsupervised feature learning. In\nthis paper the quality of its learned features is demonstrated on the MNIST\ndataset, where its performance confirms that indeed DDFA is a viable technique\nfor learning useful features.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 23:45:03 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 03:37:45 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Szerlip", "Paul A.", ""], ["Morse", "Gregory", ""], ["Pugh", "Justin K.", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "1406.2080", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev and\n  Rob Fergus", "title": "Training Convolutional Networks with Noisy Labels", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large labeled datasets has allowed Convolutional Network\nmodels to achieve impressive recognition results. However, in many settings\nmanual annotation of the data is impractical; instead our data has noisy\nlabels, i.e. there is some freely available label for each image which may or\nmay not be accurate. In this paper, we explore the performance of\ndiscriminatively-trained Convnets when trained on such noisy data. We introduce\nan extra noise layer into the network which adapts the network outputs to match\nthe noisy label distribution. The parameters of this noise layer can be\nestimated as part of the training process and involve simple modifications to\ncurrent training infrastructures for deep networks. We demonstrate the\napproaches on several datasets, including large scale experiments on the\nImageNet classification benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:45:12 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 21:10:03 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 21:13:47 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 16:44:00 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Bruna", "Joan", ""], ["Paluri", "Manohar", ""], ["Bourdev", "Lubomir", ""], ["Fergus", "Rob", ""]]}, {"id": "1406.2235", "submitter": "Michael Smith", "authors": "Michael R. Smith, Tony Martinez, Michael Gashler", "title": "A Hybrid Latent Variable Neural Network Model for Item Recommendation", "comments": "10 pages, 3 tables. arXiv admin note: text overlap with\n  arXiv:1312.5394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is used to recommend items to a user without\nrequiring a knowledge of the item itself and tends to outperform other\ntechniques. However, collaborative filtering suffers from the cold-start\nproblem, which occurs when an item has not yet been rated or a user has not\nrated any items. Incorporating additional information, such as item or user\ndescriptions, into collaborative filtering can address the cold-start problem.\nIn this paper, we present a neural network model with latent input variables\n(latent neural network or LNN) as a hybrid collaborative filtering technique\nthat addresses the cold-start problem. LNN outperforms a broad selection of\ncontent-based filters (which make recommendations based on item descriptions)\nand other hybrid approaches while maintaining the accuracy of state-of-the-art\ncollaborative filtering techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:21:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""], ["Gashler", "Michael", ""]]}, {"id": "1406.2507", "submitter": "Tim Taylor", "authors": "Tim Taylor", "title": "WebAL-1: Workshop on Artificial Life and the Web 2014 Proceedings", "comments": "Editors: Tim Taylor, Josh Auerbach, Josh Bongard, Jeff Clune, Simon\n  Hickinbotham, Greg Hornby", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of WebAL-1: Workshop on Artificial Life and the Web 2014, held at\nthe 14th International Conference on the Synthesis and Simulation of Living\nSystems (ALIFE 14), New York, NY, 31 July 2014.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 11:00:54 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 03:18:05 GMT"}, {"version": "v3", "created": "Sat, 26 Jul 2014 01:05:30 GMT"}, {"version": "v4", "created": "Thu, 7 Aug 2014 05:17:20 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Taylor", "Tim", ""]]}, {"id": "1406.2539", "submitter": "Fabricio de Franca Olivetti", "authors": "Fabricio Olivetti de Franca", "title": "Maximizing Diversity for Multimodal Optimization", "comments": "submitted to PPSN'14 Workshop Advances in Multimodal Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most multimodal optimization algorithms use the so called \\textit{niching\nmethods}~\\cite{mahfoud1995niching} in order to promote diversity during\noptimization, while others, like \\textit{Artificial Immune\nSystems}~\\cite{de2010conceptual} try to find multiple solutions as its main\nobjective. One of such algorithms, called\n\\textit{dopt-aiNet}~\\cite{de2005artificial}, introduced the Line Distance that\nmeasures the distance between two solutions regarding their basis of\nattraction. In this short abstract I propose the use of the Line Distance\nmeasure as the main objective-function in order to locate multiple optima at\nonce in a population.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:22:13 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["de Franca", "Fabricio Olivetti", ""]]}, {"id": "1406.2545", "submitter": "Fabricio de Franca Olivetti", "authors": "Fabricio Olivetti de Franca and Guilherme Palermo Coelho", "title": "A Flexible Fitness Function for Community Detection in Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most community detection algorithms from the literature work as optimization\ntools that minimize a given \\textit{fitness function}, while assuming that each\nnode belongs to a single community. Since there is no hard concept of what a\ncommunity is, most proposed fitness functions focus on a particular definition.\nAs such, these functions do not always lead to partitions that correspond to\nthose observed in practice. This paper proposes a new flexible fitness function\nthat allows the identification of communities with distinct characteristics.\nSuch flexibility was evaluated through the adoption of an immune-inspired\noptimization algorithm, named cob-aiNet[C], to identify both disjoint and\noverlapping communities in a set of benchmark networks. The results have shown\nthat the obtained partitions are much closer to the ground-truth than those\nobtained by the optimization of the modularity function.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:39:06 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["de Franca", "Fabricio Olivetti", ""], ["Coelho", "Guilherme Palermo", ""]]}, {"id": "1406.2613", "submitter": "Z H", "authors": "Shahab U. Ansari and Sameen Mansha", "title": "Simulation based Hardness Evaluation of a Multi-Objective Genetic\n  Algorithm", "comments": "International Conference on Modeling & Simulation, November, 25-27,\n  2013, Islamabad", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that multi-objective optimization problems are hard\nproblems. Such problems either require longer time to converge to an optimum\nsolution, or may not converge at all. Recently some researchers have claimed\nthat real culprit for increasing the hardness of multi-objective problems are\nnot the number of objectives themselves rather it is the increased size of\nsolution set, incompatibility of solutions, and high probability of finding\nsuboptimal solution due to increased number of local maxima. In this work, we\nhave setup a simple framework for the evaluation of hardness of multi-objective\ngenetic algorithms (MOGA). The algorithm is designed for a pray-predator game\nwhere a player is to improve its lifespan, challenging level and usability of\nthe game arena through number of generations. A rigorous set of experiments are\nperformed for quantifying the hardness in terms of evolution for increasing\nnumber of objective functions. In genetic algorithm, crossover and mutation\nwith equal probability are applied to create offspring in each generation.\nFirst, each objective function is maximized individually by ranking the\ncompeting players on the basis of the fitness (cost) function, and then a\nmulti-objective cost function (sum of individual cost functions) is maximized\nwith ranking, and also without ranking where dominated solutions are also\nallowed to evolve.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 20:13:58 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Ansari", "Shahab U.", ""], ["Mansha", "Sameen", ""]]}, {"id": "1406.2614", "submitter": "Rizwana Kalsoom", "authors": "Rizwana Kalsoom and Moomal Qureshi", "title": "Application and Verification of Algorithm Learning Based Neural Network", "comments": "This paper has been withdrawn by the author due to a crucial accuracy\n  error in Fig. 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author due to a crucial accuracy error\nin Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work\nin the following article. Yoon, H., Park, C. S., Kim, J. S., & Baek, J. G.\n(2013). Algorithm learning based neural network integrating feature selection\nand classification. Expert Systems with Applications, 40(1), 231-241.\nhttp://www.sciencedirect.com/science/article/pii/S0957417412008731\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 20:20:11 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 14:50:41 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 09:50:06 GMT"}, {"version": "v4", "created": "Thu, 26 Feb 2015 07:06:17 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Kalsoom", "Rizwana", ""], ["Qureshi", "Moomal", ""]]}, {"id": "1406.2623", "submitter": "Loshchilov Ilya", "authors": "Ilya Loshchilov (LIS), Marc Schoenauer (LRI, INRIA Saclay - Ile de\n  France), Mich\\`ele Sebag (LRI), Nikolaus Hansen (INRIA Saclay - Ile de\n  France)", "title": "Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES", "comments": "13th International Conference on Parallel Problem Solving from Nature\n  (PPSN 2014) (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is widely\naccepted as a robust derivative-free continuous optimization algorithm for\nnon-linear and non-convex optimization problems. CMA-ES is well known to be\nalmost parameterless, meaning that only one hyper-parameter, the population\nsize, is proposed to be tuned by the user. In this paper, we propose a\nprincipled approach called self-CMA-ES to achieve the online adaptation of\nCMA-ES hyper-parameters in order to improve its overall performance.\nExperimental results show that for larger-than-default population size, the\ndefault settings of hyper-parameters of CMA-ES are far from being optimal, and\nthat self-CMA-ES allows for dynamically approaching optimal settings.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 16:44:32 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 09:54:27 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Loshchilov", "Ilya", "", "LIS"], ["Schoenauer", "Marc", "", "LRI, INRIA Saclay - Ile de\n  France"], ["Sebag", "Mich\u00e8le", "", "LRI"], ["Hansen", "Nikolaus", "", "INRIA Saclay - Ile de\n  France"]]}, {"id": "1406.2639", "submitter": "Holger Roth", "authors": "Holger R. Roth and Le Lu and Ari Seff and Kevin M. Cherry and Joanne\n  Hoffman and Shijun Wang and Jiamin Liu and Evrim Turkbey and Ronald M.\n  Summers", "title": "A New 2.5D Representation for Lymph Node Detection using Random Sets of\n  Deep Convolutional Neural Network Observations", "comments": "This article will be presented at MICCAI (Medical Image Computing and\n  Computer-Assisted Interventions) 2014", "journal-ref": "Medical Image Computing and Computer-Assisted Intervention -\n  MICCAI 2014 Volume 8673 of the series Lecture Notes in Computer Science pp\n  520-527", "doi": "10.1007/978-3-319-10404-1_65", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automated Lymph Node (LN) detection is an important clinical diagnostic task\nbut very challenging due to the low contrast of surrounding structures in\nComputed Tomography (CT) and to their varying sizes, poses, shapes and sparsely\ndistributed locations. State-of-the-art studies show the performance range of\n52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1\nFP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this\npaper, we first operate a preliminary candidate generation stage, towards 100%\nsensitivity at the cost of high FP levels (40 per patient), to harvest volumes\nof interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by\nresampling 2D reformatted orthogonal views N times, via scale, random\ntranslations, and rotations with respect to the VOI centroid coordinates. These\nrandom views are then used to train a deep Convolutional Neural Network (CNN)\nclassifier. In testing, the CNN is employed to assign LN probabilities for all\nN random views that can be simply averaged (as a set) to compute the final\nclassification probability per VOI. We validate the approach on two datasets:\n90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs.\nWe achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in\nmediastinum and abdomen respectively, which drastically improves over the\nprevious state-of-the-art work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:43:42 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Seff", "Ari", ""], ["Cherry", "Kevin M.", ""], ["Hoffman", "Joanne", ""], ["Wang", "Shijun", ""], ["Liu", "Jiamin", ""], ["Turkbey", "Evrim", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1406.2671", "submitter": "Herbert Jaeger", "authors": "Herbert Jaeger", "title": "Conceptors: an easy introduction", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptors provide an elementary neuro-computational mechanism which sheds a\nfresh and unifying light on a diversity of cognitive phenomena. A number of\ndemanding learning and processing tasks can be solved with unprecedented ease,\nrobustness and accuracy. Some of these tasks were impossible to solve before.\nThis entirely informal paper introduces the basic principles of conceptors and\nhighlights some of their usages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 19:21:33 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Jaeger", "Herbert", ""]]}, {"id": "1406.2823", "submitter": "Roberto Lopez-Herrejon", "authors": "Roberto E. Lopez-Herrejon and Javier Ferrer and Francisco Chicano and\n  Lukas Linsbauer and Alexander Egyed and Enrique Alba", "title": "A Hitchhiker's Guide to Search-Based Software Engineering for Software\n  Product Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search Based Software Engineering (SBSE) is an emerging discipline that\nfocuses on the application of search-based optimization techniques to software\nengineering problems. The capacity of SBSE techniques to tackle problems\ninvolving large search spaces make their application attractive for Software\nProduct Lines (SPLs). In recent years, several publications have appeared that\napply SBSE techniques to SPL problems. In this paper, we present the results of\na systematic mapping study of such publications. We identified the stages of\nthe SPL life cycle where SBSE techniques have been used, what case studies have\nbeen employed and how they have been analysed. This mapping study revealed\npotential venues for further research as well as common misunderstanding and\npitfalls when applying SBSE techniques that we address by providing a guideline\nfor researchers and practitioners interested in exploiting these techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 08:43:51 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Lopez-Herrejon", "Roberto E.", ""], ["Ferrer", "Javier", ""], ["Chicano", "Francisco", ""], ["Linsbauer", "Lukas", ""], ["Egyed", "Alexander", ""], ["Alba", "Enrique", ""]]}, {"id": "1406.2889", "submitter": "Jonathan Tapson", "authors": "Jonathan Tapson, Philip de Chazal and Andr\\'e van Schaik", "title": "Explicit Computation of Input Weights in Extreme Learning Machines", "comments": "In submission for the ELM 2014 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a closed form expression for initializing the input weights in a\nmulti-layer perceptron, which can be used as the first step in synthesis of an\nExtreme Learning Ma-chine. The expression is based on the standard function for\na separating hyperplane as computed in multilayer perceptrons and linear\nSupport Vector Machines; that is, as a linear combination of input data\nsamples. In the absence of supervised training for the input weights, random\nlinear combinations of training data samples are used to project the input data\nto a higher dimensional hidden layer. The hidden layer weights are solved in\nthe standard ELM fashion by computing the pseudoinverse of the hidden layer\noutputs and multiplying by the desired output values. All weights for this\nmethod can be computed in a single pass, and the resulting networks are more\naccurate and more consistent on some standard problems than regular ELM\nnetworks of the same size.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 12:39:10 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Tapson", "Jonathan", ""], ["de Chazal", "Philip", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1406.2989", "submitter": "Mathias Berglund", "authors": "Tapani Raiko, Mathias Berglund, Guillaume Alain, Laurent Dinh", "title": "Techniques for Learning Binary Stochastic Feedforward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic binary hidden units in a multi-layer perceptron (MLP) network give\nat least three potential benefits when compared to deterministic MLP networks.\n(1) They allow to learn one-to-many type of mappings. (2) They can be used in\nstructured prediction problems, where modeling the internal structure of the\noutput is important. (3) Stochasticity has been shown to be an excellent\nregularizer, which makes generalization performance potentially better in\ngeneral. However, training stochastic networks is considerably more difficult.\nWe study training using M samples of hidden activations per input. We show that\nthe case M=1 leads to a fundamentally different behavior where the network\ntries to avoid stochasticity. We propose two new estimators for the training\ngradient and propose benchmark tests for comparing training algorithms. Our\nexperiments confirm that training stochastic networks is difficult and show\nthat the proposed two estimators perform favorably among all the five known\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 18:29:27 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 13:37:05 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 12:58:06 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Raiko", "Tapani", ""], ["Berglund", "Mathias", ""], ["Alain", "Guillaume", ""], ["Dinh", "Laurent", ""]]}, {"id": "1406.3100", "submitter": "Jonathan Tapson", "authors": "Philip de Chazal, Jonathan Tapson and Andr\\'e van Schaik", "title": "Learning ELM network weights using linear discriminant analysis", "comments": "In submission to the ELM 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative to the pseudo-inverse method for determining the\nhidden to output weight values for Extreme Learning Machines performing\nclassification tasks. The method is based on linear discriminant analysis and\nprovides Bayes optimal single point estimates for the weight values.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 02:08:31 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["de Chazal", "Philip", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1406.3149", "submitter": "Christian Napoli", "authors": "Francesco Bonanno, Giacomo Capizzi, Grazia Lo Sciuto, Christian\n  Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "A Cascade Neural Network Architecture investigating Surface Plasmon\n  Polaritons propagation for thin metals in OpenMP", "comments": null, "journal-ref": "International conference on Artificial Intelligence and Soft\n  Computing (ICAISC 2014), Vol I, 22-33 (2014)", "doi": null, "report-no": null, "categories": "cs.NE cond-mat.mes-hall cond-mat.mtrl-sci cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Surface plasmon polaritons (SPPs) confined along metal-dielectric interface\nhave attracted a relevant interest in the area of ultracompact photonic\ncircuits, photovoltaic devices and other applications due to their strong field\nconfinement and enhancement. This paper investigates a novel cascade neural\nnetwork (NN) architecture to find the dependance of metal thickness on the SPP\npropagation. Additionally, a novel training procedure for the proposed cascade\nNN has been developed using an OpenMP-based framework, thus greatly reducing\ntraining time. The performed experiments confirm the effectiveness of the\nproposed NN architecture for the problem at hand.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 08:40:04 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Bonanno", "Francesco", ""], ["Capizzi", "Giacomo", ""], ["Sciuto", "Grazia Lo", ""], ["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1406.3156", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "A hybrid neuro--wavelet predictor for QoS control and stability", "comments": null, "journal-ref": "Proceedings of AI*IA 2013: Advances in Artificial Intelligence,\n  pages 527-538. Springer, 2013", "doi": "10.1007/978-3-319-03524-6_45", "report-no": null, "categories": "cs.NE cs.DC cs.NI cs.PF cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  For distributed systems to properly react to peaks of requests, their\nadaptation activities would benefit from the estimation of the amount of\nrequests. This paper proposes a solution to produce a short-term forecast based\non data characterising user behaviour of online services. We use \\emph{wavelet\nanalysis}, providing compression and denoising on the observed time series of\nthe amount of past user requests; and a \\emph{recurrent neural network} trained\nwith observed data and designed so as to provide well-timed estimations of\nfuture requests. The said ensemble has the ability to predict the amount of\nfuture user requests with a root mean squared error below 0.06\\%. Thanks to\nprediction, advance resource provision can be performed for the duration of a\nrequest peak and for just the right amount of resources, hence avoiding\nover-provisioning and associated costs. Moreover, reliable provision lets users\nenjoy a level of availability of services unaffected by load variations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:04:49 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1406.3282", "submitter": "Erik Cuevas", "authors": "Erik Cuevas, Miguel Cienfuegos, Daniel Zaldivar and Marco Perez", "title": "A swarm optimization algorithm inspired in the behavior of the\n  social-spider", "comments": "21 Pages", "journal-ref": "Expert Systems with Applications, 40 (16), (2013), pp. 6374-6384", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm intelligence is a research field that models the collective behavior in\nswarms of insects or animals. Several algorithms arising from such models have\nbeen proposed to solve a wide range of complex optimization problems. In this\npaper, a novel swarm algorithm called the Social Spider Optimization (SSO) is\nproposed for solving optimization tasks. The SSO algorithm is based on the\nsimulation of cooperative behavior of social-spiders. In the proposed\nalgorithm, individuals emulate a group of spiders which interact to each other\nbased on the biological laws of the cooperative colony. The algorithm considers\ntwo different search agents (spiders): males and females. Depending on gender,\neach individual is conducted by a set of different evolutionary operators which\nmimic different cooperative behaviors that are typically found in the colony.\nIn order to illustrate the proficiency and robustness of the proposed approach,\nit is compared to other well-known evolutionary methods. The comparison\nexamines several standard benchmark functions that are commonly considered\nwithin the literature of evolutionary algorithms. The outcome shows a high\nperformance of the proposed method for searching a global optimum with several\nbenchmark functions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 16:29:14 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Cuevas", "Erik", ""], ["Cienfuegos", "Miguel", ""], ["Zaldivar", "Daniel", ""], ["Perez", "Marco", ""]]}, {"id": "1406.3284", "submitter": "Charles Cadieu", "authors": "Charles F. Cadieu, Ha Hong, Daniel L. K. Yamins, Nicolas Pinto, Diego\n  Ardila, Ethan A. Solomon, Najib J. Majaj, James J. DiCarlo", "title": "Deep Neural Networks Rival the Representation of Primate IT Cortex for\n  Core Visual Object Recognition", "comments": "35 pages, 12 figures, extends and expands upon arXiv:1301.3530", "journal-ref": null, "doi": "10.1371/journal.pcbi.1003963", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primate visual system achieves remarkable visual object recognition\nperformance even in brief presentations and under changes to object exemplar,\ngeometric transformations, and background variation (a.k.a. core visual object\nrecognition). This remarkable performance is mediated by the representation\nformed in inferior temporal (IT) cortex. In parallel, recent advances in\nmachine learning have led to ever higher performing models of object\nrecognition using artificial deep neural networks (DNNs). It remains unclear,\nhowever, whether the representational performance of DNNs rivals that of the\nbrain. To accurately produce such a comparison, a major difficulty has been a\nunifying metric that accounts for experimental limitations such as the amount\nof noise, the number of neural recording sites, and the number trials, and\ncomputational limitations such as the complexity of the decoding classifier and\nthe number of classifier training examples. In this work we perform a direct\ncomparison that corrects for these experimental limitations and computational\nconsiderations. As part of our methodology, we propose an extension of \"kernel\nanalysis\" that measures the generalization accuracy as a function of\nrepresentational complexity. Our evaluations show that, unlike previous\nbio-inspired models, the latest DNNs rival the representational performance of\nIT cortex on this visual object recognition task. Furthermore, we show that\nmodels that perform well on measures of representational performance also\nperform well on measures of representational similarity to IT and on measures\nof predicting individual IT multi-unit responses. Whether these DNNs rely on\ncomputational mechanisms similar to the primate visual system is yet to be\ndetermined, but, unlike all previous bio-inspired models, that possibility\ncannot be ruled out merely on representational performance grounds.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 16:38:07 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Cadieu", "Charles F.", ""], ["Hong", "Ha", ""], ["Yamins", "Daniel L. K.", ""], ["Pinto", "Nicolas", ""], ["Ardila", "Diego", ""], ["Solomon", "Ethan A.", ""], ["Majaj", "Najib J.", ""], ["DiCarlo", "James J.", ""]]}, {"id": "1406.3337", "submitter": "Jared Moore", "authors": "Jared Moore, Anthony Clark, Philip McKinley", "title": "Evolutionary Robotics on the Web with WebGL and Javascript", "comments": "Presented at WebAL-1: Workshop on Artificial Life and the Web 2014\n  (arXiv:1406.2507)", "journal-ref": null, "doi": null, "report-no": "WebAL1/2014/02", "categories": "cs.NE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based applications are highly accessible to users, providing rich,\ninteractive content while eliminating the need to install software locally.\nHowever, evolutionary robotics (ER) has faced challenges in this domain as\nweb-based technologies have not been amenable to 3D physics simulations.\nTraditionally, physics-based simulations require a local installation and a\nhigh degree of user knowledge to configure an environment, but the emergence of\nJavascript-based physics engines enables complex simulations to be executed in\nweb browsers. These developments create opportunities for ER research to reach\nnew audiences by increasing accessibility. In this work, we introduce two\nweb-based tools we have built to facilitate the exchange of ideas with other\nresearchers as well as outreach to K-12 students and the general public. The\nfirst tool is intended to distribute and exchange ER research results, while\nthe second is a completely browser-based implementation of an ER environment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 19:49:16 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 13:54:17 GMT"}, {"version": "v3", "created": "Fri, 11 Jul 2014 01:20:24 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Moore", "Jared", ""], ["Clark", "Anthony", ""], ["McKinley", "Philip", ""]]}, {"id": "1406.3474", "submitter": "Sijin Li", "authors": "Sijin Li, Zhi-Qiang Liu, Antoni B. Chan", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an heterogeneous multi-task learning framework for human pose\nestimation from monocular image with deep convolutional neural network. In\nparticular, we simultaneously learn a pose-joint regressor and a sliding-window\nbody-part detector in a deep network architecture. We show that including the\nbody-part detection task helps to regularize the network, directing it to\nconverge to a good solution. We report competitive and state-of-art results on\nseveral data sets. We also empirically show that the learned neurons in the\nmiddle layer of our network are tuned to localized body parts.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:11:18 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Li", "Sijin", ""], ["Liu", "Zhi-Qiang", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1406.3793", "submitter": "Cheston Tan", "authors": "Cheston Tan, Tomaso Poggio", "title": "Neural tuning size is a key factor underlying holistic face processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faces are a class of visual stimuli with unique significance, for a variety\nof reasons. They are ubiquitous throughout the course of a person's life, and\nface recognition is crucial for daily social interaction. Faces are also unlike\nany other stimulus class in terms of certain physical stimulus characteristics.\nFurthermore, faces have been empirically found to elicit certain characteristic\nbehavioral phenomena, which are widely held to be evidence of \"holistic\"\nprocessing of faces. However, little is known about the neural mechanisms\nunderlying such holistic face processing. In other words, for the processing of\nfaces by the primate visual system, the input and output characteristics are\nrelatively well known, but the internal neural computations are not. The main\naim of this work is to further the fundamental understanding of what causes the\nvisual processing of faces to be different from that of objects. In this\ncomputational modeling work, we show that a single factor - \"neural tuning\nsize\" - is able to account for three key phenomena that are characteristic of\nface processing, namely the Composite Face Effect (CFE), Face Inversion Effect\n(FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle\nprovides specific neural tuning properties that correspond to the\npoorly-understood notion of holistic face processing, and connects these neural\nproperties to psychophysical behavior. Overall, our work provides a unified and\nparsimonious theoretical account for the disparate empirical data on\nface-specific processing, deepening the fundamental understanding of face\nprocessing.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 03:05:07 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Tan", "Cheston", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1406.4237", "submitter": "Eswari J", "authors": "Eswari.J and Dr. S. Jeyadevi", "title": "An Evolutionary Approach for Optimal Citing and Sizing of Micro-Grid in\n  Radial Distribution Systems", "comments": null, "journal-ref": "J.Eswari , Dr.S.Jeyadevi. \"An Evolutionary Approach for Optimal\n  Citing and Sizing of Micro-Grid in Radial Distribution Systems\",\n  International Journal of Engineering Trends and Technology (IJETT),\n  V11(9),429-433 May 2014", "doi": "10.14445/22315381/IJETT-V11P286", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This Paper presents the methodology of penetration of Micro-Grids (MG) in the\nradial distribution system (RDS). The aim of this paper is to minimize a total\nreal power loss that descends the performance of the radial distribution system\nby integrating various renewable resources as Distributed Generation (DG). The\ncombination of different types of renewable energy resources contributes a\nsustainable MG. These resources are optimally sized and located using\nevolutionary approach in various penetration levels. The optimal solutions are\nexperimented with IEEE 33 radial distribution system using Particle Swarm\nOptimization (PSO) technique. The results are quite promising and authenticate\nits potential to solve problem in radial distribution system effectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 04:52:23 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["J", "Eswari.", ""], ["Jeyadevi", "Dr. S.", ""]]}, {"id": "1406.4518", "submitter": "Erfan Khaji Mr.", "authors": "Erfan Khaji, Amin Satlikh Mohammadi", "title": "A Heuristic Method to Generate Better Initial Population for\n  Evolutionary Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initial population plays an important role in heuristic algorithms such as GA\nas it help to decrease the time those algorithms need to achieve an acceptable\nresult. Furthermore, it may influence the quality of the final answer given by\nevolutionary algorithms. In this paper, we shall introduce a heuristic method\nto generate a target based initial population which possess two mentioned\ncharacteristics. The efficiency of the proposed method has been shown by\npresenting the results of our tests on the benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 15:20:17 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Khaji", "Erfan", ""], ["Mohammadi", "Amin Satlikh", ""]]}, {"id": "1406.4619", "submitter": "Alexandre Chotard", "authors": "Alexandre Chotard (INRIA Saclay - Ile de France, LRI), Martin Holena", "title": "A Generalized Markov-Chain Modelling Approach to $(1,\\lambda)$-ES Linear\n  Optimization: Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent publications investigated Markov-chain modelling of linear\noptimization by a $(1,\\lambda)$-ES, considering both unconstrained and linearly\nconstrained optimization, and both constant and varying step size. All of them\nassume normality of the involved random steps, and while this is consistent\nwith a black-box scenario, information on the function to be optimized (e.g.\nseparability) may be exploited by the use of another distribution. The\nobjective of our contribution is to complement previous studies realized with\nnormal steps, and to give sufficient conditions on the distribution of the\nrandom steps for the success of a constant step-size $(1,\\lambda)$-ES on the\nsimple problem of a linear function with a linear constraint. The decomposition\nof a multidimensional distribution into its marginals and the copula combining\nthem is applied to the new distributional assumptions, particular attention\nbeing paid to distributions with Archimedean copulas.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 06:58:38 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chotard", "Alexandre", "", "INRIA Saclay - Ile de France, LRI"], ["Holena", "Martin", ""]]}, {"id": "1406.4951", "submitter": "Sukru Burc Eryilmaz", "authors": "Sukru Burc Eryilmaz, Duygu Kuzum, Rakesh Jeyasingh, SangBum Kim,\n  Matthew BrightSky, Chung Lam and H.-S. Philip Wong", "title": "Brain-like associative learning using a nanoscale non-volatile phase\n  change synaptic device array", "comments": "Original article can be found here:\n  http://journal.frontiersin.org/Journal/10.3389/fnins.2014.00205/abstract", "journal-ref": "Front Neurosci. 8, 205 (2014)", "doi": "10.3389/fnins.2014.00205", "report-no": null, "categories": "cs.NE cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neuroscience together with nanoscale electronic device\ntechnology have resulted in huge interests in realizing brain-like computing\nhardwares using emerging nanoscale memory devices as synaptic elements.\nAlthough there has been experimental work that demonstrated the operation of\nnanoscale synaptic element at the single device level, network level studies\nhave been limited to simulations. In this work, we demonstrate, using\nexperiments, array level associative learning using phase change synaptic\ndevices connected in a grid like configuration similar to the organization of\nthe biological brain. Implementing Hebbian learning with phase change memory\ncells, the synaptic grid was able to store presented patterns and recall\nmissing patterns in an associative brain-like fashion. We found that the system\nis robust to device variations, and large variations in cell resistance states\ncan be accommodated by increasing the number of training epochs. We illustrated\nthe tradeoff between variation tolerance of the network and the overall energy\nconsumption, and found that energy consumption is decreased significantly for\nlower variation tolerance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 05:49:02 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 16:41:34 GMT"}, {"version": "v3", "created": "Mon, 30 Jun 2014 21:54:45 GMT"}, {"version": "v4", "created": "Mon, 14 Jul 2014 00:12:49 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Eryilmaz", "Sukru Burc", ""], ["Kuzum", "Duygu", ""], ["Jeyasingh", "Rakesh", ""], ["Kim", "SangBum", ""], ["BrightSky", "Matthew", ""], ["Lam", "Chung", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1406.5633", "submitter": "Timothy Molter", "authors": "M. Alexander Nugent and Timothy W. Molter", "title": "Thermodynamic-RAM Technology Stack", "comments": null, "journal-ref": null, "doi": "10.1080/17445760.2017.1314472", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technology stack or specification describing the multiple\nlevels of abstraction and specialization needed to implement a neuromorphic\nprocessor (NPU) based on the previously-described concept of AHaH Computing and\nintegrate it into today's digital computing systems. The general purpose NPU\nimplementation described here is called Thermodynamic-RAM (kT-RAM) and is just\none of many possible architectures, each with varying advantages and trade\noffs. Bringing us closer to brain-like neural computation, kT-RAM will provide\na general-purpose adaptive hardware resource to existing computing platforms\nenabling fast and low-power machine learning capabilities that are currently\nhampered by the separation of memory and processing, a.k.a the von Neumann\nbottleneck. Because understanding such a processor based on non-traditional\nprinciples can be difficult, by presenting the various levels of the stack from\nthe bottom up, layer by layer, explaining kT-RAM becomes a much easier task.\nThe levels of the Thermodynamic-RAM technology stack include the memristor,\nsynapse, AHaH node, kT-RAM, instruction set, sparse spike encoding, kT-RAM\nemulator, and SENSE server.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 17:03:11 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 11:32:23 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Nugent", "M. Alexander", ""], ["Molter", "Timothy W.", ""]]}, {"id": "1406.5947", "submitter": "Thomas Martinetz", "authors": "Bogdan Miclut, Thomas Kaester, Thomas Martinetz, Erhardt Barth", "title": "Committees of deep feedforward networks trained with few data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are known to give good results on image\nclassification tasks. In this paper we present a method to improve the\nclassification result by combining multiple such networks in a committee. We\nadopt the STL-10 dataset which has very few training examples and show that our\nmethod can achieve results that are better than the state of the art. The\nnetworks are trained layer-wise and no backpropagation is used. We also explore\nthe effects of dataset augmentation by mirroring, rotation, and scaling.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 15:34:54 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Miclut", "Bogdan", ""], ["Kaester", "Thomas", ""], ["Martinetz", "Thomas", ""], ["Barth", "Erhardt", ""]]}, {"id": "1406.6291", "submitter": "Hiroki Sayama", "authors": "Hiroki Sayama, Shelley D. Dionne", "title": "Studying Collective Human Decision Making and Creativity with\n  Evolutionary Computation", "comments": "20 pages, 7 figures, 1 table (Supplemental materials not included)", "journal-ref": "Artificial Life, 21:379-393 (2015)", "doi": null, "report-no": null, "categories": "cs.NE cs.MA nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a summary of our interdisciplinary research project \"Evolutionary\nPerspective on Collective Decision Making\" that was conducted through close\ncollaboration between computational, organizational and social scientists at\nBinghamton University. We redefined collective human decision making and\ncreativity as evolution of ecologies of ideas, where populations of ideas\nevolve via continual applications of evolutionary operators such as\nreproduction, recombination, mutation, selection, and migration of ideas, each\nconducted by participating humans. Based on this evolutionary perspective, we\ngenerated hypotheses about collective human decision making using agent-based\ncomputer simulations. The hypotheses were then tested through several\nexperiments with real human subjects. Throughout this project, we utilized\nevolutionary computation (EC) in non-traditional ways---(1) as a theoretical\nframework for reinterpreting the dynamics of idea generation and selection, (2)\nas a computational simulation model of collective human decision making\nprocesses, and (3) as a research tool for collecting high-resolution\nexperimental data of actual collaborative design and decision making from human\nsubjects. We believe our work demonstrates untapped potential of EC for\ninterdisciplinary research involving human and social dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 16:08:10 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Sayama", "Hiroki", ""], ["Dionne", "Shelley D.", ""]]}, {"id": "1406.6453", "submitter": "Peilei Liu", "authors": "Peilei Liu and Ting Wang", "title": "A Quantitative Neural Coding Model of Sensory Memory", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coding mechanism of sensory memory on the neuron scale is one of the most\nimportant questions in neuroscience. We have put forward a quantitative neural\nnetwork model, which is self organized, self similar, and self adaptive, just\nlike an ecosystem following Darwin theory. According to this model, neural\ncoding is a mult to one mapping from objects to neurons. And the whole cerebrum\nis a real-time statistical Turing Machine, with powerful representing and\nlearning ability. This model can reconcile some important disputations, such\nas: temporal coding versus rate based coding, grandmother cell versus\npopulation coding, and decay theory versus interference theory. And it has also\nprovided explanations for some key questions such as memory consolidation,\nepisodic memory, consciousness, and sentiment. Philosophical significance is\nindicated at last.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 04:17:21 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Liu", "Peilei", ""], ["Wang", "Ting", ""]]}, {"id": "1406.6560", "submitter": "Erik Cuevas", "authors": "Erik Cuevas, Felipe Sencion-Echauri, Daniel Zaldivar and Marco Perez\n  Cisneros", "title": "Multi Circle Detection on Images Using Artificial Bee Colony (ABC)\n  Optimization", "comments": "19 Pages", "journal-ref": "Soft Computing, 16 (2), (2012), pp. 281-296", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hough transform (HT) has been the most common method for circle detection,\nexhibiting robustness, but adversely demanding considerable computational\neffort and large memory requirements. Alternative approaches include heuristic\nmethods that employ iterative optimization procedures for detecting multiple\ncircles. Since only one circle can be marked at each optimization cycle,\nmultiple executions must be enforced in order to achieve multi detection. This\npaper presents an algorithm for automatic detection of multiple circular shapes\nthat considers the overall process as a multi-modal optimization problem. The\napproach is based on the artificial bee colony (ABC) algorithm, a swarm\noptimization algorithm inspired by the intelligent foraging behavior of honey\nbees. Unlike the original ABC algorithm, the proposed approach presents the\naddition of a memory for discarded solutions. Such memory allows holding\nimportant information regarding other local optima which might have emerged\nduring the optimization process. The detector uses a combination of three\nnon-collinear edge points as parameters to determine circle candidates. A\nmatching function (nectar- amount) determines if such circle candidates\n(bee-food-sources) are actually present in the image. Guided by the values of\nsuch matching functions, the set of encoded candidate circles are evolved\nthrough the ABC algorithm so that the best candidate (global optimum) can be\nfitted into an actual circle within the edge only image. Then, an analysis of\nthe incorporated memory is executed in order to identify potential local\noptima, i.e., other circles.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:15:08 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Cuevas", "Erik", ""], ["Sencion-Echauri", "Felipe", ""], ["Zaldivar", "Daniel", ""], ["Cisneros", "Marco Perez", ""]]}, {"id": "1406.6909", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin\n  Riedmiller and Thomas Brox", "title": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional\n  Neural Networks", "comments": "PAMI submission. Includes matching experiments as in\n  arXiv:1405.5769v1. Also includes new network architectures, experiments on\n  Caltech-256, experiment on combining Exemplar-CNN with clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have proven to be very successful in learning\ntask specific features that allow for unprecedented performance on various\ncomputer vision tasks. Training of such networks follows mostly the supervised\nlearning paradigm, where sufficiently many input-output pairs are required for\ntraining. Acquisition of large training sets is one of the key challenges, when\napproaching a new task. In this paper, we aim for generic feature learning and\npresent an approach for training a convolutional network using only unlabeled\ndata. To this end, we train the network to discriminate between a set of\nsurrogate classes. Each surrogate class is formed by applying a variety of\ntransformations to a randomly sampled 'seed' image patch. In contrast to\nsupervised network training, the resulting feature representation is not class\nspecific. It rather provides robustness to the transformations that have been\napplied during training. This generic feature representation allows for\nclassification results that outperform the state of the art for unsupervised\nlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,\nCaltech-256). While such generic features cannot compete with class specific\nfeatures from supervised training on a classification task, we show that they\nare advantageous on geometric matching problems, where they also outperform the\nSIFT descriptor.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 15:07:14 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 11:43:36 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Fischer", "Philipp", ""], ["Springenberg", "Jost Tobias", ""], ["Riedmiller", "Martin", ""], ["Brox", "Thomas", ""]]}, {"id": "1406.7362", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho and Yoshua Bengio", "title": "Exponentially Increasing the Capacity-to-Computation Ratio for\n  Conditional Computation in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art results obtained with deep networks are achieved with\nthe largest models that could be trained, and if more computation power was\navailable, we might be able to exploit much larger datasets in order to improve\ngeneralization ability. Whereas in learning algorithms such as decision trees\nthe ratio of capacity (e.g., the number of parameters) to computation is very\nfavorable (up to exponentially more parameters than computation), the ratio is\nessentially 1 for deep neural networks. Conditional computation has been\nproposed as a way to increase the capacity of a deep neural network without\nincreasing the amount of computation required, by activating some parameters\nand computation \"on-demand\", on a per-example basis. In this note, we propose a\nnovel parametrization of weight matrices in neural networks which has the\npotential to increase up to exponentially the ratio of the number of parameters\nto computation. The proposed approach is based on turning on some parameters\n(weight matrices) when specific bit patterns of hidden unit activations are\nobtained. In order to better control for the overfitting that might result, we\npropose a parametrization that is tree-structured, where each node of the tree\ncorresponds to a prefix of a sequence of sign bits, or gating units, associated\nwith hidden units.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 06:45:51 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.7399", "submitter": "Ghassan Samara", "authors": "Ghassan Samara, Tareq Alhmiedat", "title": "Intelligent Emergency Message Broadcasting in VANET Using PSO", "comments": "11 pages", "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT) ISSN: 2221-0741 Vol. 4, No. 7, 90-100, 2014", "doi": null, "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new type of Mobile Ad hoc Network which is called Vehicular Ad hoc\nNetworks (VANET) created a fertile environment for research. In this research,\na protocol Particle Swarm Optimization Contention Based Broadcast (PCBB) is\nproposed, for fast andeffective dissemination of emergency messages within a\ngeographical area to distribute the emergency message and achieve the safety\nsystem, this research will help the VANET system to achieve its safety goals in\nintelligent and efficient way.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 13:31:17 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Samara", "Ghassan", ""], ["Alhmiedat", "Tareq", ""]]}, {"id": "1406.7539", "submitter": "Wei Quan", "authors": "Wei Quan and Andy D. Pimentel", "title": "Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist\n  Genetic Algorithm", "comments": "9 pages, 11 figures, uses algorithm2e.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration of task mappings plays a crucial role in achieving high\nperformance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.\nThe problem of optimally mapping a set of tasks onto a set of given\nheterogeneous processors for maximal throughput has been known, in general, to\nbe NP-complete. The problem is further exacerbated when multiple applications\n(i.e., bigger task sets) and the communication between tasks are also\nconsidered. Previous research has shown that Genetic Algorithms (GA) typically\nare a good choice to solve this problem when the solution space is relatively\nsmall. However, when the size of the problem space increases, classic genetic\nalgorithms still suffer from the problem of long evolution times. To address\nthis problem, this paper proposes a novel bias-elitist genetic algorithm that\nis guided by domain-specific heuristics to speed up the evolution process.\nExperimental results reveal that our proposed algorithm is able to handle large\nscale task mapping problems and produces high-quality mapping solutions in only\na short time period.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 19:23:49 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Quan", "Wei", ""], ["Pimentel", "Andy D.", ""]]}, {"id": "1406.7806", "submitter": "Andrew Maas", "authors": "Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T.\n  Lengerich, Daniel Jurafsky and Andrew Y. Ng", "title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are now a central component of nearly all\nstate-of-the-art speech recognition systems. Building neural network acoustic\nmodels requires several design decisions including network architecture, size,\nand training loss function. This paper offers an empirical investigation on\nwhich aspects of DNN acoustic model design are most important for speech\nrecognition system performance. We report DNN classifier performance and final\nspeech recognizer word error rates, and compare DNNs using several metrics to\nquantify factors influencing differences in task performance. Our first set of\nexperiments use the standard Switchboard benchmark corpus, which contains\napproximately 300 hours of conversational telephone speech. We compare standard\nDNNs to convolutional networks, and present the first experiments using\nlocally-connected, untied neural networks for acoustic modeling. We\nadditionally build systems on a corpus of 2,100 hours of training data by\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\nmore thoroughly examine performance of large DNN models -- with up to ten times\nmore parameters than those typically used in speech recognition systems. Our\nresults suggest that a relatively simple DNN architecture and optimization\ntechnique produces strong results. These findings, along with previous work,\nhelp establish a set of best practices for building DNN hybrid speech\nrecognition systems with maximum likelihood training. Our experiments in DNN\noptimization additionally serve as a case study for training DNNs with\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\ngenerally.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:42:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 07:44:15 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Maas", "Andrew L.", ""], ["Qi", "Peng", ""], ["Xie", "Ziang", ""], ["Hannun", "Awni Y.", ""], ["Lengerich", "Christopher T.", ""], ["Jurafsky", "Daniel", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1406.7811", "submitter": "Erik Cuevas", "authors": "Erik Cuevas and Mauricio Gonzalez", "title": "An optimization algorithm for multimodal functions inspired by\n  collective animal behavior", "comments": "18 Pages. arXiv admin note: text overlap with arXiv:1405.5164", "journal-ref": "Soft Computing 17 (3) , (2013), pp. 489-502", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in multimodal function optimization is expanding rapidly since real\nworld optimization problems often demand locating multiple optima within a\nsearch space. This article presents a new multimodal optimization algorithm\nnamed as the Collective Animal Behavior (CAB). Animal groups, such as schools\nof fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit a\nvariety of behaviors including swarming about a food source, milling around a\ncentral location or migrating over large distances in aligned groups. These\ncollective behaviors are often advantageous to groups, allowing them to\nincrease their harvesting efficiency to follow better migration routes, to\nimprove their aerodynamic and to avoid predation. In the proposed algorithm,\nsearcher agents are a group of animals which interact to each other based on\nthe biological laws of collective motion. Experimental results demonstrate that\nthe proposed algorithm is capable of finding global and local optima of\nbenchmark multimodal optimization problems with a higher efficiency in\ncomparison to other methods reported in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:48:05 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Cuevas", "Erik", ""], ["Gonzalez", "Mauricio", ""]]}]