[{"id": "1501.00092", "submitter": "Chao Dong", "authors": "Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang", "title": "Image Super-Resolution Using Deep Convolutional Networks", "comments": "14 pages, 14 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning method for single image super-resolution (SR). Our\nmethod directly learns an end-to-end mapping between the low/high-resolution\nimages. The mapping is represented as a deep convolutional neural network (CNN)\nthat takes the low-resolution image as the input and outputs the\nhigh-resolution one. We further show that traditional sparse-coding-based SR\nmethods can also be viewed as a deep convolutional network. But unlike\ntraditional methods that handle each component separately, our method jointly\noptimizes all layers. Our deep CNN has a lightweight structure, yet\ndemonstrates state-of-the-art restoration quality, and achieves fast speed for\npractical on-line usage. We explore different network structures and parameter\nsettings to achieve trade-offs between performance and speed. Moreover, we\nextend our network to cope with three color channels simultaneously, and show\nbetter overall reconstruction quality.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 08:35:09 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 03:47:06 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 09:13:32 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Dong", "Chao", ""], ["Loy", "Chen Change", ""], ["He", "Kaiming", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1501.00299", "submitter": "Mohammad Pezeshki", "authors": "Mohammad Pezeshki", "title": "Sequence Modeling using Gated Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have used Recurrent Neural Networks to capture and model\nhuman motion data and generate motions by prediction of the next immediate data\npoint at each time-step. Our RNN is armed with recently proposed Gated\nRecurrent Units which has shown promising results in some sequence modeling\nproblems such as Machine Translation and Speech Synthesis. We demonstrate that\nthis model is able to capture long-term dependencies in data and generate\nrealistic motions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 18:37:36 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Pezeshki", "Mohammad", ""]]}, {"id": "1501.00436", "submitter": "Sebasti\\'an Basterrech", "authors": "Sebasti\\'an Basterrech and Enrique Alba and V\\'aclav Sn\\'a\\v{s}el", "title": "An Experimental Analysis of the Echo State Network Initialization Using\n  the Particle Swarm Optimization", "comments": "IEEE Sixth World Congress on Nature and Biologically Inspired\n  Computing (NaBIC2014), Porto, Portugal, July 30-August 1, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a robust hybrid method for solving supervised\nlearning tasks, which uses the Echo State Network (ESN) model and the Particle\nSwarm Optimization (PSO) algorithm. An ESN is a Recurrent Neural Network with\nthe hidden-hidden weights fixed in the learning process. The recurrent part of\nthe network stores the input information in internal states of the network.\nAnother structure forms a free-memory method used as supervised learning tool.\nThe setting procedure for initializing the recurrent structure of the ESN model\ncan impact on the model performance. On the other hand, the PSO has been shown\nto be a successful technique for finding optimal points in complex spaces.\nHere, we present an approach to use the PSO for finding some initial\nhidden-hidden weights of the ESN model. We present empirical results that\ncompare the canonical ESN model with this hybrid method on a wide range of\nbenchmark problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 16:49:29 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Basterrech", "Sebasti\u00e1n", ""], ["Alba", "Enrique", ""], ["Sn\u00e1\u0161el", "V\u00e1clav", ""]]}, {"id": "1501.00503", "submitter": "Sebasti\\'an Basterrech", "authors": "Sebasti\\'an Basterrech", "title": "An Empirical Study of the L2-Boost technique with Echo State Networks", "comments": "To appear in Journal of Network and Innovative Computing, Volume 2,\n  Issue 1, pp. 120 - 127, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particular case of Recurrent Neural Network (RNN) was introduced at the\nbeginning of the 2000s under the name of Echo State Networks (ESNs). The ESN\nmodel overcomes the limitations during the training of the RNNs while\nintroducing no significant disadvantages. Although the model presents some\nwell-identified drawbacks when the parameters are not well initialised. The\nperformance of an ESN is highly dependent on its internal parameters and\npattern of connectivity of the hidden-hidden weights Often, the tuning of the\nnetwork parameters can be hard and can impact in the accuracy of the models.\n  In this work, we investigate the performance of a specific boosting technique\n(called L2-Boost) with ESNs as single predictors. The L2-Boost technique has\nbeen shown to be an effective tool to combine \"weak\" predictors in regression\nproblems. In this study, we use an ensemble of random initialized ESNs (without\ncontrol their parameters) as \"weak\" predictors of the boosting procedure. We\nevaluate our approach on five well-know time-series benchmark problems.\nAdditionally, we compare this technique with a baseline approach that consists\nof averaging the prediction of an ensemble of ESNs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 21:15:00 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Basterrech", "Sebasti\u00e1n", ""]]}, {"id": "1501.00653", "submitter": "Souham Biswas", "authors": "Souham Biswas, Manisha J. Nene", "title": "Hostile Intent Identification by Movement Pattern Analysis: Using\n  Artificial Neural Networks", "comments": "To appear in IEEE Xplore as a part of the proceedings of the 3rd IEEE\n  International Conference on Parallel, Distributed and Grid Computing, 2014,\n  at Jaypee University of Information Technology, Solan", "journal-ref": null, "doi": "10.13140/2.1.4429.7281", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, the problem of identifying suspicious behavior has\ngained importance and identifying this behavior using computational systems and\nautonomous algorithms is highly desirable in a tactical scenario. So far, the\nsolutions have been primarily manual which elicit human observation of entities\nto discern the hostility of the situation. To cater to this problem statement,\na number of fully automated and partially automated solutions exist. But, these\nsolutions lack the capability of learning from experiences and work in\nconjunction with human supervision which is extremely prone to error. In this\npaper, a generalized methodology to predict the hostility of a given object\nbased on its movement patterns is proposed which has the ability to learn and\nis based upon the mechanism of humans of learning from experiences. The\nmethodology so proposed has been implemented in a computer simulation. The\nresults show that the posited methodology has the potential to be applied in\nreal world tactical scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 09:07:45 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Biswas", "Souham", ""], ["Nene", "Manisha J.", ""]]}, {"id": "1501.00777", "submitter": "Jun Li", "authors": "Jun Li, Heyou Chang, Jian Yang", "title": "Sparse Deep Stacking Network for Image Classification", "comments": "8 pages, 3 figures, AAAI-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sparse coding can learn good robust representation to noise and model more\nhigher-order representation for image classification. However, the inference\nalgorithm is computationally expensive even though the supervised signals are\nused to learn compact and discriminative dictionaries in sparse coding\ntechniques. Luckily, a simplified neural network module (SNNM) has been\nproposed to directly learn the discriminative dictionaries for avoiding the\nexpensive inference. But the SNNM module ignores the sparse representations.\nTherefore, we propose a sparse SNNM module by adding the mixed-norm\nregularization (l1/l2 norm). The sparse SNNM modules are further stacked to\nbuild a sparse deep stacking network (S-DSN). In the experiments, we evaluate\nS-DSN with four databases, including Extended YaleB, AR, 15 scene and\nCaltech101. Experimental results show that our model outperforms related\nclassification methods with only a linear classifier. It is worth noting that\nwe reach 98.8% recognition accuracy on 15 scene.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 08:07:31 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Li", "Jun", ""], ["Chang", "Heyou", ""], ["Yang", "Jian", ""]]}, {"id": "1501.01457", "submitter": "Inaki Fernandez", "authors": "I\\~naki Fern\\'andez P\\'erez (INRIA Nancy - Grand Est / LORIA), Amine\n  Boumaza (INRIA Nancy - Grand Est / LORIA), Fran\\c{c}ois Charpillet (INRIA\n  Nancy - Grand Est / LORIA)", "title": "Comparison of Selection Methods in On-line Distributed Evolutionary\n  Robotics", "comments": null, "journal-ref": "ALIFE 14, Jul 2014, New York, United States. Artificial Life 14 in\n  Complex Adaptive Systems, MIT Press, Artificial Life 14", "doi": "10.7551/978-0-262-32621-6-ch046", "report-no": null, "categories": "cs.AI cs.MA cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the impact of selection methods in the context of\non-line on-board distributed evolutionary algorithms. We propose a variant of\nthe mEDEA algorithm in which we add a selection operator, and we apply it in a\ntaskdriven scenario. We evaluate four selection methods that induce different\nintensity of selection pressure in a multi-robot navigation with obstacle\navoidance task and a collective foraging task. Experiments show that a small\nintensity of selection pressure is sufficient to rapidly obtain good\nperformances on the tasks at hand. We introduce different measures to compare\nthe selection methods, and show that the higher the selection pressure, the\nbetter the performances obtained, especially for the more challenging food\nforaging task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 12:11:27 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["P\u00e9rez", "I\u00f1aki Fern\u00e1ndez", "", "INRIA Nancy - Grand Est / LORIA"], ["Boumaza", "Amine", "", "INRIA Nancy - Grand Est / LORIA"], ["Charpillet", "Fran\u00e7ois", "", "INRIA\n  Nancy - Grand Est / LORIA"]]}, {"id": "1501.02128", "submitter": "Junzhi Li", "authors": "Ying Tan, Junzhi Li and Zhongyang Zheng", "title": "Introduction and Ranking Results of the ICSI 2014 Competition on Single\n  Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report includes the introduction and ranking results of the\nICSI 2014 Competition on Single Objective Optimization.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 13:21:11 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Tan", "Ying", ""], ["Li", "Junzhi", ""], ["Zheng", "Zhongyang", ""]]}, {"id": "1501.02192", "submitter": "Mohammad Alhawarat Dr.", "authors": "M. Alhawarat, T. Olde Scheper and N.T. Crook", "title": "Investigation of a chaotic spiking neuron model", "comments": null, "journal-ref": "International Journal of Computer Applications 99(17):1-8, August\n  2014", "doi": "10.5120/17462-8258", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chaos provides many interesting properties that can be used to achieve\ncomputational tasks. Such properties are sensitivity to initial conditions,\nspace filling, control and synchronization. Chaotic neural models have been\ndevised to exploit such properties. In this paper, a chaotic spiking neuron\nmodel is investigated experimentally. This investigation is performed to\nunderstand the dynamic behaviours of the model.\n  The aim of this research is to investigate the dynamics of the nonlinear\ndynamic state neuron (NDS) experimentally. The experimental approach has\nrevealed some quantitative and qualitative properties of the NDS model such as\nthe control mechanism, the reset mechanism, and the way the model may exhibit\ndynamic behaviours in phase space. It is shown experimentally in this paper\nthat both the reset mechanism and the self-feed back control mechanism are\nimportant for the NDS model to work and to stabilise to one of the large number\nof available unstable periodic orbits (UPOs) that are embedded in its\nattractor. The experimental investigation suggests that the internal dynamics\nof the NDS neuron provide a rich set of dynamic behaviours that can be\ncontrolled and stabilised. These wide range of dynamic behaviours may be\nexploited to carry out information processing tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 16:20:42 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Alhawarat", "M.", ""], ["Scheper", "T. Olde", ""], ["Crook", "N. T.", ""]]}, {"id": "1501.02592", "submitter": "Michiel Hermans", "authors": "Michiel Hermans, Miguel Soriano, Joni Dambre, Peter Bienstman, Ingo\n  Fischer", "title": "Photonic Delay Systems as Machine Learning Implementations", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol. 16, pp. 2081-2097\n  (2015)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear photonic delay systems present interesting implementation platforms\nfor machine learning models. They can be extremely fast, offer great degrees of\nparallelism and potentially consume far less power than digital processors. So\nfar they have been successfully employed for signal processing using the\nReservoir Computing paradigm. In this paper we show that their range of\napplicability can be greatly extended if we use gradient descent with\nbackpropagation through time on a model of the system to optimize the input\nencoding of such systems. We perform physical experiments that demonstrate that\nthe obtained input encodings work well in reality, and we show that optimized\nsystems perform significantly better than the common Reservoir Computing\napproach. The results presented here demonstrate that common gradient descent\ntechniques from machine learning may well be applicable on physical\nneuro-inspired analog computers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 10:25:31 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Hermans", "Michiel", ""], ["Soriano", "Miguel", ""], ["Dambre", "Joni", ""], ["Bienstman", "Peter", ""], ["Fischer", "Ingo", ""]]}, {"id": "1501.03209", "submitter": "Milad Kharratzadeh", "authors": "Milad Kharratzadeh and Thomas R. Shultz", "title": "Neural Implementation of Probabilistic Models of Cognition", "comments": null, "journal-ref": "Cognitive Systems Research, 40, (2016) 99-113", "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models of cognition hypothesize that human brains make sense of data\nby representing probability distributions and applying Bayes' rule to find the\nbest explanation for available data. Understanding the neural mechanisms\nunderlying probabilistic models remains important because Bayesian models\nprovide a computational framework, rather than specifying mechanistic\nprocesses. Here, we propose a deterministic neural-network model which\nestimates and represents probability distributions from observable events --- a\nphenomenon related to the concept of probability matching. Our model learns to\nrepresent probabilities without receiving any representation of them from the\nexternal world, but rather by experiencing the occurrence patterns of\nindividual events. Our neural implementation of probability matching is paired\nwith a neural module applying Bayes' rule, forming a comprehensive neural\nscheme to simulate human Bayesian learning and inference. Our model also\nprovides novel explanations of base-rate neglect, a notable deviation from\nBayes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 23:46:05 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 15:54:11 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kharratzadeh", "Milad", ""], ["Shultz", "Thomas R.", ""]]}, {"id": "1501.03969", "submitter": "Vijay Manikandan Janakiraman", "authors": "Vijay Manikandan Janakiraman and XuanLong Nguyen and Dennis Assanis", "title": "Nonlinear Model Predictive Control of A Gasoline HCCI Engine Using\n  Extreme Learning Machines", "comments": "This paper was written as an extract from my PhD thesis (July 2013)\n  and so references may not be to date as of this submission (Jan 2015). The\n  article is in review and contains 10 figures, 35 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homogeneous charge compression ignition (HCCI) is a futuristic combustion\ntechnology that operates with a high fuel efficiency and reduced emissions.\nHCCI combustion is characterized by complex nonlinear dynamics which\nnecessitates a model based control approach for automotive application. HCCI\nengine control is a nonlinear, multi-input multi-output problem with state and\nactuator constraints which makes controller design a challenging task. Typical\nHCCI controllers make use of a first principles based model which involves a\nlong development time and cost associated with expert labor and calibration. In\nthis paper, an alternative approach based on machine learning is presented\nusing extreme learning machines (ELM) and nonlinear model predictive control\n(MPC). A recurrent ELM is used to learn the nonlinear dynamics of HCCI engine\nusing experimental data and is shown to accurately predict the engine behavior\nseveral steps ahead in time, suitable for predictive control. Using the ELM\nengine models, an MPC based control algorithm with a simplified quadratic\nprogram update is derived for real time implementation. The working and\neffectiveness of the MPC approach has been analyzed on a nonlinear HCCI engine\nmodel for tracking multiple reference quantities along with constraints defined\nby HCCI states, actuators and operational limits.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 13:01:33 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Janakiraman", "Vijay Manikandan", ""], ["Nguyen", "XuanLong", ""], ["Assanis", "Dennis", ""]]}, {"id": "1501.03975", "submitter": "Vijay Manikandan Janakiraman", "authors": "Vijay Manikandan Janakiraman and XuanLong Nguyen and Dennis Assanis", "title": "Stochastic Gradient Based Extreme Learning Machines For Online Learning\n  of Advanced Combustion Engines", "comments": "This paper was written as an extract from my PhD thesis (July 2013)\n  and so references may not be to date as of this submission (Jan 2015). The\n  article is in review and contains 10 figures, 35 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a stochastic gradient based online learning algorithm for\nExtreme Learning Machines (ELM) is developed (SG-ELM). A stability criterion\nbased on Lyapunov approach is used to prove both asymptotic stability of\nestimation error and stability in the estimated parameters suitable for\nidentification of nonlinear dynamic systems. The developed algorithm not only\nguarantees stability, but also reduces the computational demand compared to the\nOS-ELM approach based on recursive least squares. In order to demonstrate the\neffectiveness of the algorithm on a real-world scenario, an advanced combustion\nengine identification problem is considered. The algorithm is applied to two\ncase studies: An online regression learning for system identification of a\nHomogeneous Charge Compression Ignition (HCCI) Engine and an online\nclassification learning (with class imbalance) for identifying the dynamic\noperating envelope of the HCCI Engine. The results indicate that the accuracy\nof the proposed SG-ELM is comparable to that of the state-of-the-art but adds\nstability and a reduction in computational effort.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 13:18:34 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Janakiraman", "Vijay Manikandan", ""], ["Nguyen", "XuanLong", ""], ["Assanis", "Dennis", ""]]}, {"id": "1501.04010", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Coevolutionary intransitivity in games: A landscape analysis", "comments": null, "journal-ref": "In: Applications of Evolutionary Computation - EvoApplications\n  2015, (Eds.: A. M. Mora, G. Squillero), Lecture Notes in Computer Science,\n  Vol. 9028, Springer-Verlag, Berlin, 2015, 869-881", "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intransitivity is supposed to be a main reason for deficits in coevolutionary\nprogress and inheritable superiority. Besides, coevolutionary dynamics is\ncharacterized by interactions yielding subjective fitness, but aiming at\nsolutions that are superior with respect to an objective measurement. Such an\napproximation of objective fitness may be, for instance, generalization\nperformance. In the paper a link between rating-- and ranking--based measures\nof intransitivity and fitness landscapes that can address the dichotomy between\nsubjective and objective fitness is explored. The approach is illustrated by\nnumerical experiments involving a simple random game with continuously tunable\ndegree of randomness.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 15:22:19 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "1501.04587", "submitter": "Naiyan Wang", "authors": "Naiyan Wang, Siyi Li, Abhinav Gupta, Dit-Yan Yeung", "title": "Transferring Rich Feature Hierarchies for Robust Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) models have demonstrated great success in\nvarious computer vision tasks including image classification and object\ndetection. However, some equally important tasks such as visual tracking remain\nrelatively unexplored. We believe that a major hurdle that hinders the\napplication of CNN to visual tracking is the lack of properly labeled training\ndata. While existing applications that liberate the power of CNN often need an\nenormous amount of training data in the order of millions, visual tracking\napplications typically have only one labeled example in the first frame of each\nvideo. We address this research issue here by pre-training a CNN offline and\nthen transferring the rich feature hierarchies learned to online tracking. The\nCNN is also fine-tuned during online tracking to adapt to the appearance of the\ntracked target specified in the first video frame. To fit the characteristics\nof object tracking, we first pre-train the CNN to recognize what is an object,\nand then propose to generate a probability map instead of producing a simple\nclass label. Using two challenging open benchmarks for performance evaluation,\nour proposed tracker has demonstrated substantial improvement over other\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 18:54:34 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 06:18:09 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Wang", "Naiyan", ""], ["Li", "Siyi", ""], ["Gupta", "Abhinav", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1501.04659", "submitter": "Lorenzo Livi", "authors": "Francesca Possemato, Maurizio Paschero, Lorenzo Livi, Antonello Rizzi,\n  Alireza Sadeghian", "title": "On the impact of topological properties of smart grids in power losses\n  optimization problems", "comments": "35 pages, 38 references", "journal-ref": null, "doi": "10.1016/j.ijepes.2015.12.022", "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power losses reduction is one of the main targets for any electrical energy\ndistribution company. In this paper, we face the problem of joint optimization\nof both topology and network parameters in a real smart grid. We consider a\nportion of the Italian electric distribution network managed by the ACEA\nDistribuzione S.p.A. located in Rome. We perform both the power factor\ncorrection (PFC) for tuning the generators and the distributed feeder\nreconfiguration (DFR) to set the state of the breakers. This joint optimization\nproblem is faced considering a suitable objective function and by adopting\ngenetic algorithms as global optimization strategy. We analyze admissible\nnetwork configurations, showing that some of these violate constraints on\ncurrent and voltage at branches and nodes. Such violations depend only on pure\ntopological properties of the configurations. We perform tests by feeding the\nsimulation environment with real data concerning hourly samples of dissipated\nand generated active and reactive power values of the ACEA smart grid. Results\nshow that removing the configurations violating the electrical constraints from\nthe solution space leads to interesting improvements in terms of power loss\nreduction. To conclude, we provide also an electrical interpretation of the\nphenomenon using graph-based pattern analysis techniques.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 22:19:16 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 20:29:58 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Possemato", "Francesca", ""], ["Paschero", "Maurizio", ""], ["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1501.06115", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Jun Miao, Laiyun Qing", "title": "Constrained Extreme Learning Machines: A Study on Classification Cases", "comments": "14 pages, 6 figure, journel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM) is an extremely fast learning method and has a\npowerful performance for pattern recognition tasks proven by enormous\nresearches and engineers. However, its good generalization ability is built on\nlarge numbers of hidden neurons, which is not beneficial to real time response\nin the test process. In this paper, we proposed new ways, named \"constrained\nextreme learning machines\" (CELMs), to randomly select hidden neurons based on\nsample distribution. Compared to completely random selection of hidden nodes in\nELM, the CELMs randomly select hidden nodes from the constrained vector space\ncontaining some basic combinations of original sample vectors. The experimental\nresults show that the CELMs have better generalization ability than traditional\nELM, SVM and some other related methods. Additionally, the CELMs have a similar\nfast learning speed as ELM.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 05:11:34 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 11:42:01 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Zhu", "Wentao", ""], ["Miao", "Jun", ""], ["Qing", "Laiyun", ""]]}, {"id": "1501.06633", "submitter": "Andrew Lavin", "authors": "Andrew Lavin", "title": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell\n  GPUs", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes maxDNN, a computationally efficient convolution kernel\nfor deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%\ncomputational efficiency on typical deep learning network architectures. The\ndesign combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We\nonly address forward propagation (FPROP) operation of the network, but we\nbelieve that the same techniques used here will be effective for backward\npropagation (BPROP) as well.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 01:19:12 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 01:16:33 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 23:50:49 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lavin", "Andrew", ""]]}, {"id": "1501.06721", "submitter": "Daniel Krzywicki", "authors": "D. Krzywicki, W. Turek, A. Byrski, M. Kisiel-Dorohinicki", "title": "Massively-concurrent Agent-based Evolutionary Computing", "comments": "Journal of Computational Science, Available online 29 July 2015", "journal-ref": null, "doi": "10.1016/j.jocs.2015.07.003", "report-no": null, "categories": "cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fusion of the multi-agent paradigm with evolutionary computation yielded\npromising results in many optimization problems. Evolutionary multi-agent\nsystem (EMAS) are more similar to biological evolution than classical\nevolutionary algorithms. However, technological limitations prevented the use\nof fully asynchronous agents in previous EMAS implementations. In this paper we\npresent a new algorithm for agent-based evolutionary computations. The\nindividuals are represented as fully autonomous and asynchronous agents. An\nefficient implementation of this algorithm was possible through the use of\nmodern technologies based on functional languages (namely Erlang and Scala),\nwhich natively support lightweight processes and asynchronous communication.\nOur experiments show that such an asynchronous approach is both faster and more\nefficient in solving common optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 10:03:14 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 07:19:25 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Krzywicki", "D.", ""], ["Turek", "W.", ""], ["Byrski", "A.", ""], ["Kisiel-Dorohinicki", "M.", ""]]}, {"id": "1501.07227", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "A Neural Network Anomaly Detector Using the Random Cluster Model", "comments": "These writings are part of a longer writing which has been submitted\n  for publication. I plan to replace this writing (and the other 2 writings)\n  with the single writing that has been submitted for publication. The other\n  writings to be withdrawn are 1503.03488 and 1412.4178", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random cluster model is used to define an upper bound on a distance\nmeasure as a function of the number of data points to be classified and the\nexpected value of the number of classes to form in a hybrid K-means and\nregression classification methodology, with the intent of detecting anomalies.\nConditions are given for the identification of classes which contain anomalies\nand individual anomalies within identified classes. A neural network model\ndescribes the decision region-separating surface for offline storage and recall\nin any new anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 18:42:42 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2015 22:49:15 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 02:35:37 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2015 20:49:32 GMT"}, {"version": "v5", "created": "Wed, 10 Feb 2016 22:29:26 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1501.07399", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Josep Lluis Arcos", "title": "Particle swarm optimization for time series motif discovery", "comments": "12 pages, 9 figures, 2 tables", "journal-ref": "Knowledge-Based Systems 92: 127-137. Jan 2016", "doi": "10.1016/j.knosys.2015.10.021", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently finding similar segments or motifs in time series data is a\nfundamental task that, due to the ubiquity of these data, is present in a wide\nrange of domains and situations. Because of this, countless solutions have been\ndevised but, to date, none of them seems to be fully satisfactory and flexible.\nIn this article, we propose an innovative standpoint and present a solution\ncoming from it: an anytime multimodal optimization algorithm for time series\nmotif discovery based on particle swarms. By considering data from a variety of\ndomains, we show that this solution is extremely competitive when compared to\nthe state-of-the-art, obtaining comparable motifs in considerably less time\nusing minimal memory. In addition, we show that it is robust to different\nimplementation choices and see that it offers an unprecedented degree of\nflexibility with regard to the task. All these qualities make the presented\nsolution stand out as one of the most prominent candidates for motif discovery\nin long time series streams. Besides, we believe the proposed standpoint can be\nexploited in further time series analysis and mining tasks, widening the scope\nof research and potentially yielding novel effective solutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 10:18:46 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1501.07873", "submitter": "Yongxin Yang", "authors": "Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang and Timothy Hospedales", "title": "Sketch-a-Net that Beats Humans", "comments": "Accepted to BMVC 2015 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-scale multi-channel deep neural network framework that,\nfor the first time, yields sketch recognition performance surpassing that of\nhumans. Our superior performance is a result of explicitly embedding the unique\ncharacteristics of sketches in our model: (i) a network architecture designed\nfor sketch rather than natural photo statistics, (ii) a multi-channel\ngeneralisation that encodes sequential ordering in the sketching process, and\n(iii) a multi-scale network ensemble with joint Bayesian fusion that accounts\nfor the different levels of abstraction exhibited in free-hand sketches. We\nshow that state-of-the-art deep networks specifically engineered for photos of\nnatural objects fail to perform well on sketch recognition, regardless whether\nthey are trained using photo or sketch. Our network on the other hand not only\ndelivers the best performance on the largest human sketch dataset to date, but\nalso is small in size making efficient training possible using just CPUs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 18:35:59 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 18:59:06 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 15:59:05 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Yu", "Qian", ""], ["Yang", "Yongxin", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy", ""]]}]