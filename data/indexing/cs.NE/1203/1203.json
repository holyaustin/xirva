[{"id": "1203.0197", "submitter": "Olivia Saierli", "authors": "G. S. Raghavendra and N. Prasanna Kumar", "title": "Statistical Approach for Selecting Elite Ants", "comments": "22 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 69-90", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of ACO algorithms to obtain better solutions for combinatorial\noptimization problems have become very popular in recent years. In ACO\nalgorithms, group of agents repeatedly perform well defined actions and\ncollaborate with other ants in order to accomplish the defined task. In this\npaper, we introduce new mechanisms for selecting the Elite ants dynamically\nbased on simple statistical tools. We also investigate the performance of newly\nproposed mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 14:25:50 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 09:02:14 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Raghavendra", "G. S.", ""], ["Kumar", "N. Prasanna", ""]]}, {"id": "1203.3097", "submitter": "Abdoun Otman AO", "authors": "Otman Abdoun, Jaafar Abouchabaka", "title": "A Comparative Study of Adaptive Crossover Operators for Genetic\n  Algorithms to Resolve the Traveling Salesman Problem", "comments": null, "journal-ref": "International Journal of Computer Applications (0975 - 8887)\n  Volume 31 - No.11, October 2011", "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithm includes some parameters that should be adjusting so that\nthe algorithm can provide positive results. Crossover operators play very\nimportant role by constructing competitive Genetic Algorithms (GAs). In this\npaper, the basic conceptual features and specific characteristics of various\ncrossover operators in the context of the Traveling Salesman Problem (TSP) are\ndiscussed. The results of experimental comparison of more than six different\ncrossover operators for the TSP are presented. The experiment results show that\nOX operator enables to achieve a better solutions than other operators tested.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 14:32:45 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Abdoun", "Otman", ""], ["Abouchabaka", "Jaafar", ""]]}, {"id": "1203.3099", "submitter": "Abdoun Otman AO", "authors": "Otman Abdoun, Jaafar Abouchabaka, Chakir Tajani", "title": "Analyzing the Performance of Mutation Operators to Solve the Travelling\n  Salesman Problem", "comments": "ISSN: 2222-4254", "journal-ref": "IJES, International Journal of Emerging Sciences , 2(1), 61-77,\n  March 2012", "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genetic algorithm includes some parameters that should be adjusted, so as\nto get reliable results. Choosing a representation of the problem addressed, an\ninitial population, a method of selection, a crossover operator, mutation\noperator, the probabilities of crossover and mutation, and the insertion method\ncreates a variant of genetic algorithms. Our work is part of the answer to this\nperspective to find a solution for this combinatorial problem. What are the\nbest parameters to select for a genetic algorithm that creates a variety\nefficient to solve the Travelling Salesman Problem (TSP)? In this paper, we\npresent a comparative analysis of different mutation operators, surrounded by a\ndilated discussion that justifying the relevance of genetic operators chosen to\nsolving the TSP problem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 14:38:18 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Abdoun", "Otman", ""], ["Abouchabaka", "Jaafar", ""], ["Tajani", "Chakir", ""]]}, {"id": "1203.3838", "submitter": "Chittineni Suneetha", "authors": "Suneetha Chittineni and Raveendra Babu Bhogapathi", "title": "A Study on the Behavior of a Neural Network for Grouping the Data", "comments": "7 pages,2 figures,9 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 1, January 2012, pp:228-234", "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the frequently stated advantages of neural networks is that they can\nwork effectively with non-normally distributed data. But optimal results are\npossible with normalized data.In this paper, how normality of the input affects\nthe behaviour of a K-means fast learning artificial neural network(KFLANN) for\ngrouping the data is presented. Basically, the grouping of high dimensional\ninput data is controlled by additional neural network input parameters namely\nvigilance and tolerance.Neural networks learn faster and give better\nperformance if the input variables are pre-processed before being fed to the\ninput units of the neural network. A common way of dealing with data that is\nnot normally distributed is to perform some form of mathematical transformation\non the data that shifts it towards a normal distribution.In a neural network,\ndata preprocessing transforms the data into a format that will be more easily\nand effectively processed for the purpose of the user. Among various methods,\nNormalization is one which organizes data for more efficient access.\nExperimental results on several artificial and synthetic data sets indicate\nthat the groups formed in the data vary with non-normally distributed data and\nnormalized data and also depends on the normalization method used.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 05:06:21 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Chittineni", "Suneetha", ""], ["Bhogapathi", "Raveendra Babu", ""]]}, {"id": "1203.3847", "submitter": "Anshuman Sharma Mr.", "authors": "Anshuman Sharma", "title": "Handwritten digit Recognition using Support Vector Machine", "comments": "7 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten Numeral recognition plays a vital role in postal automation\nservices especially in countries like India where multiple languages and\nscripts are used Discrete Hidden Markov Model (HMM) and hybrid of Neural\nNetwork (NN) and HMM are popular methods in handwritten word recognition\nsystem. The hybrid system gives better recognition result due to better\ndiscrimination capability of the NN. A major problem in handwriting recognition\nis the huge variability and distortions of patterns. Elastic models based on\nlocal observations and dynamic programming such HMM are not efficient to absorb\nthis variability. But their vision is local. But they cannot face to length\nvariability and they are very sensitive to distortions. Then the SVM is used to\nestimate global correlations and classify the pattern. Support Vector Machine\n(SVM) is an alternative to NN. In Handwritten recognition, SVM gives a better\nrecognition result. The aim of this paper is to develop an approach which\nimprove the efficiency of handwritten recognition using artificial neural\nnetwork\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 09:17:21 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Sharma", "Anshuman", ""]]}, {"id": "1203.4111", "submitter": "Carola Winzen", "authors": "Benjamin Doerr and Carola Winzen", "title": "Reducing the Arity in Unbiased Black-Box Complexity", "comments": "An extended abstract of this paper has been accepted for inclusion in\n  the proceedings of the Genetic and Evolutionary Computation Conference (GECCO\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for all $1<k \\leq \\log n$ the $k$-ary unbiased black-box\ncomplexity of the $n$-dimensional $\\onemax$ function class is $O(n/k)$. This\nindicates that the power of higher arity operators is much stronger than what\nthe previous $O(n/\\log k)$ bound by Doerr et al. (Faster black-box algorithms\nthrough higher arity operators, Proc. of FOGA 2011, pp. 163--172, ACM, 2011)\nsuggests.\n  The key to this result is an encoding strategy, which might be of independent\ninterest. We show that, using $k$-ary unbiased variation operators only, we may\nsimulate an unrestricted memory of size $O(2^k)$ bits.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 14:12:52 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Doerr", "Benjamin", ""], ["Winzen", "Carola", ""]]}, {"id": "1203.4416", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins and Aaron Courville and Yoshua Bengio", "title": "On Training Deep Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Boltzmann machine (DBM) has been an important development in the\nquest for powerful \"deep\" probabilistic models. To date, simultaneous or joint\ntraining of all layers of the DBM has been largely unsuccessful with existing\ntraining methods. We introduce a simple regularization scheme that encourages\nthe weight vectors associated with each hidden unit to have similar norms. We\ndemonstrate that this regularization can be easily combined with standard\nstochastic maximum likelihood to yield an effective training strategy for the\nsimultaneous training of all layers of the deep Boltzmann machine.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 12:59:15 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1203.4881", "submitter": "Frank Neumann", "authors": "Frank Neumann", "title": "Computational Complexity Analysis of Multi-Objective Genetic Programming", "comments": "A conference version has been accepted for GECCO 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity analysis of genetic programming (GP) has been\nstarted recently by analyzing simple (1+1) GP algorithms for the problems ORDER\nand MAJORITY. In this paper, we study how taking the complexity as an\nadditional criteria influences the runtime behavior. We consider\ngeneralizations of ORDER and MAJORITY and present a computational complexity\nanalysis of (1+1) GP using multi-criteria fitness functions that take into\naccount the original objective and the complexity of a syntax tree as a\nsecondary measure. Furthermore, we study the expected time until\npopulation-based multi-objective genetic programming algorithms have computed\nthe Pareto front when taking the complexity of a syntax tree as an equally\nimportant objective.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 04:22:36 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Neumann", "Frank", ""]]}, {"id": "1203.5028", "submitter": "Abdoun Otman AO", "authors": "Otman Abdoun, Chakir Tajani and Jaafar Abouchabka", "title": "Hybridizing PSM and RSM Operator for Solving NP-Complete Problems:\n  Application to Travelling Salesman Problem", "comments": "ISSN (Online): 1694-0814", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 1, 2012, 374-378", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new mutation operator, Hybrid Mutation (HPRM),\nfor a genetic algorithm that generates high quality solutions to the Traveling\nSalesman Problem (TSP). The Hybrid Mutation operator constructs an offspring\nfrom a pair of parents by hybridizing two mutation operators, PSM and RSM. The\nefficiency of the HPRM is compared as against some existing mutation operators;\nnamely, Reverse Sequence Mutation (RSM) and Partial Shuffle Mutation (PSM) for\nBERLIN52 as instance of TSPLIB. Experimental results show that the new mutation\noperator is better than the RSM and PSM.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 16:09:51 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Abdoun", "Otman", ""], ["Tajani", "Chakir", ""], ["Abouchabka", "Jaafar", ""]]}, {"id": "1203.5443", "submitter": "Martin Pelikan", "authors": "Martin Pelikan, Mark W. Hauschild, and Pier Luca Lanzi", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "comments": "Accepted at Parallel Problem Solving from Nature (PPSN XII), 10\n  pages. arXiv admin note: substantial text overlap with arXiv:1201.2241", "journal-ref": null, "doi": null, "report-no": "MEDAL Report No. 2012004", "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated technique has recently been proposed to transfer learning in the\nhierarchical Bayesian optimization algorithm (hBOA) based on distance-based\nstatistics. The technique enables practitioners to improve hBOA efficiency by\ncollecting statistics from probabilistic models obtained in previous hBOA runs\nand using the obtained statistics to bias future hBOA runs on similar problems.\nThe purpose of this paper is threefold: (1) test the technique on several\nclasses of NP-complete problems, including MAXSAT, spin glasses and minimum\nvertex cover; (2) demonstrate that the technique is effective even when\nprevious runs were done on problems of different size; (3) provide empirical\nevidence that combining transfer learning with other efficiency enhancement\ntechniques can often yield nearly multiplicative speedups.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 20:11:21 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 12:47:30 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Pelikan", "Martin", ""], ["Hauschild", "Mark W.", ""], ["Lanzi", "Pier Luca", ""]]}, {"id": "1203.6276", "submitter": "Pekka Malo", "authors": "Ankur Sinha, Pekka Malo, Timo Kuosmanen", "title": "A Multi-objective Exploratory Procedure for Regression Model Selection", "comments": "in Journal of Computational and Graphical Statistics, Vol. 24, Iss.\n  1, 2015", "journal-ref": null, "doi": "10.1080/10618600.2014.899236", "report-no": null, "categories": "stat.CO cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is recognized as one of the most critical steps in\nstatistical modeling. The problems encountered in engineering and social\nsciences are commonly characterized by over-abundance of explanatory variables,\nnon-linearities and unknown interdependencies between the regressors. An added\ndifficulty is that the analysts may have little or no prior knowledge on the\nrelative importance of the variables. To provide a robust method for model\nselection, this paper introduces the Multi-objective Genetic Algorithm for\nVariable Selection (MOGA-VS) that provides the user with an optimal set of\nregression models for a given data-set. The algorithm considers the regression\nproblem as a two objective task, and explores the Pareto-optimal (best subset)\nmodels by preferring those models over the other which have less number of\nregression coefficients and better goodness of fit. The model exploration can\nbe performed based on in-sample or generalization error minimization. The model\nselection is proposed to be performed in two steps. First, we generate the\nfrontier of Pareto-optimal regression models by eliminating the dominated\nmodels without any user intervention. Second, a decision making process is\nexecuted which allows the user to choose the most preferred model using\nvisualisations and simple metrics. The method has been evaluated on a recently\npublished real dataset on Communities and Crime within United States.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 14:15:24 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 15:54:33 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2013 22:34:01 GMT"}, {"version": "v4", "created": "Wed, 13 Jul 2016 07:53:01 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Sinha", "Ankur", ""], ["Malo", "Pekka", ""], ["Kuosmanen", "Timo", ""]]}, {"id": "1203.6286", "submitter": "Jun He", "authors": "Jun He and Tianshi Chen and Xin Yao", "title": "On the Easiest and Hardest Fitness Functions", "comments": null, "journal-ref": "IEEE Transactions on evolutionary computation 19.2 (2015): 295-305", "doi": "10.1109/TEVC.2014.2318025", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardness of fitness functions is an important research topic in the field\nof evolutionary computation. In theory, the study can help understanding the\nability of evolutionary algorithms. In practice, the study may provide a\nguideline to the design of benchmarks. The aim of this paper is to answer the\nfollowing research questions: Given a fitness function class, which functions\nare the easiest with respect to an evolutionary algorithm? Which are the\nhardest? How are these functions constructed? The paper provides theoretical\nanswers to these questions. The easiest and hardest fitness functions are\nconstructed for an elitist (1+1) evolutionary algorithm to maximise a class of\nfitness functions with the same optima. It is demonstrated that the unimodal\nfunctions are the easiest and deceptive functions are the hardest in terms of\nthe time-fitness landscape. The paper also reveals that the easiest fitness\nfunction to one algorithm may become the hardest to another algorithm, and vice\nversa.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 15:01:53 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2013 10:48:34 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2013 14:56:32 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 09:38:59 GMT"}, {"version": "v5", "created": "Thu, 12 Feb 2015 10:25:00 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["He", "Jun", ""], ["Chen", "Tianshi", ""], ["Yao", "Xin", ""]]}]