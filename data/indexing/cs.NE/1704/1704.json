[{"id": "1704.00026", "submitter": "Carsten Witt", "authors": "Carsten Witt", "title": "Upper Bounds on the Runtime of the Univariate Marginal Distribution\n  Algorithm on OneMax", "comments": "Version 4: added illustrations and experiments; improved presentation\n  in Section 2.2; to appear in Algorithmica; the final publication is available\n  at Springer via http://dx.doi.org/10.1007/s00453-018-0463-0", "journal-ref": null, "doi": "10.1007/s00453-018-0463-0", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A runtime analysis of the Univariate Marginal Distribution Algorithm (UMDA)\nis presented on the OneMax function for wide ranges of its parameters $\\mu$ and\n$\\lambda$. If $\\mu\\ge c\\log n$ for some constant $c>0$ and\n$\\lambda=(1+\\Theta(1))\\mu$, a general bound $O(\\mu n)$ on the expected runtime\nis obtained. This bound crucially assumes that all marginal probabilities of\nthe algorithm are confined to the interval $[1/n,1-1/n]$. If $\\mu\\ge c'\n\\sqrt{n}\\log n$ for a constant $c'>0$ and $\\lambda=(1+\\Theta(1))\\mu$, the\nbehavior of the algorithm changes and the bound on the expected runtime becomes\n$O(\\mu\\sqrt{n})$, which typically even holds if the borders on the marginal\nprobabilities are omitted.\n  The results supplement the recently derived lower bound\n$\\Omega(\\mu\\sqrt{n}+n\\log n)$ by Krejca and Witt (FOGA 2017) and turn out as\ntight for the two very different values $\\mu=c\\log n$ and $\\mu=c'\\sqrt{n}\\log\nn$. They also improve the previously best known upper bound $O(n\\log n\\log\\log\nn)$ by Dang and Lehre (GECCO 2015).\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 19:00:08 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 12:59:41 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 18:13:20 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 11:42:45 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Witt", "Carsten", ""]]}, {"id": "1704.00207", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "A Brownian Motion Model and Extreme Belief Machine for Modeling Sensor\n  Data Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the title suggests, we will describe (and justify through the presentation\nof some of the relevant mathematics) prediction methodologies for sensor\nmeasurements. This exposition will mainly be concerned with the mathematics\nrelated to modeling the sensor measurements.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 18:22:33 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 23:48:24 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1704.00260", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Kevin Shih, Saurabh Singh, and Derek Hoiem", "title": "Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks", "comments": "Accepted in ICCV 2017. The arxiv version has an extra analysis on\n  correlation with human attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 08:01:30 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 05:34:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gupta", "Tanmay", ""], ["Shih", "Kevin", ""], ["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""]]}, {"id": "1704.00616", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, Gabriel L. Oliveira, Nima Sedaghat, and\n  Thomas Brox", "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection", "comments": "10 pages, 7 figures, ICCV 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 14:29:40 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:40:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["Oliveira", "Gabriel L.", ""], ["Sedaghat", "Nima", ""], ["Brox", "Thomas", ""]]}, {"id": "1704.00646", "submitter": "Sebastian Seung", "authors": "H. Sebastian Seung and Jonathan Zung", "title": "A correlation game for unsupervised learning yields computational\n  interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse\n  elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much has been learned about plasticity of biological synapses from empirical\nstudies. Hebbian plasticity is driven by correlated activity of presynaptic and\npostsynaptic neurons. Synapses that converge onto the same neuron often behave\nas if they compete for a fixed resource; some survive the competition while\nothers are eliminated. To provide computational interpretations of these\naspects of synaptic plasticity, we formulate unsupervised learning as a\nzero-sum game between Hebbian excitation and anti-Hebbian inhibition in a\nneural network model. The game formalizes the intuition that Hebbian excitation\ntries to maximize correlations of neurons with their inputs, while anti-Hebbian\ninhibition tries to decorrelate neurons from each other. We further include a\nmodel of synaptic competition, which enables a neuron to eliminate all\nconnections except those from its most strongly correlated inputs. Through\nempirical studies, we show that this facilitates the learning of sensory\nfeatures that resemble parts of objects.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:39:19 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Seung", "H. Sebastian", ""], ["Zung", "Jonathan", ""]]}, {"id": "1704.00702", "submitter": "Lu\\'is F. Sim\\~oes", "authors": "Lu\\'is F. Sim\\~oes, Dario Izzo, Evert Haasdijk, A. E. Eiben", "title": "Multi-rendezvous Spacecraft Trajectory Optimization with Beam P-ACO", "comments": "Code available at https://github.com/lfsimoes/beam_paco__gtoc5", "journal-ref": "EvoCOP 2017, LNCS 10197, pp. 141-156, 2017", "doi": "10.1007/978-3-319-55453-2_10", "report-no": null, "categories": "cs.NE physics.space-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of spacecraft trajectories for missions visiting multiple\ncelestial bodies is here framed as a multi-objective bilevel optimization\nproblem. A comparative study is performed to assess the performance of\ndifferent Beam Search algorithms at tackling the combinatorial problem of\nfinding the ideal sequence of bodies. Special focus is placed on the\ndevelopment of a new hybridization between Beam Search and the Population-based\nAnt Colony Optimization algorithm. An experimental evaluation shows all\nalgorithms achieving exceptional performance on a hard benchmark problem. It is\nfound that a properly tuned deterministic Beam Search always outperforms the\nremaining variants. Beam P-ACO, however, demonstrates lower parameter\nsensitivity, while offering superior worst-case performance. Being an anytime\nalgorithm, it is then found to be the preferable choice for certain practical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:38:04 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Sim\u00f5es", "Lu\u00eds F.", ""], ["Izzo", "Dario", ""], ["Haasdijk", "Evert", ""], ["Eiben", "A. E.", ""]]}, {"id": "1704.00764", "submitter": "Masanori Suganuma", "authors": "Masanori Suganuma, Shinichi Shirakawa, Tomoharu Nagao", "title": "A Genetic Programming Approach to Designing Convolutional Neural Network\n  Architectures", "comments": "This is the revised version of the GECCO 2017 paper. The code of our\n  method is available at https://github.com/sg-nm/cgp-cnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN), which is one of the deep learning\nmodels, has seen much success in a variety of computer vision tasks. However,\ndesigning CNN architectures still requires expert knowledge and a lot of trial\nand error. In this paper, we attempt to automatically construct CNN\narchitectures for an image classification task based on Cartesian genetic\nprogramming (CGP). In our method, we adopt highly functional modules, such as\nconvolutional blocks and tensor concatenation, as the node functions in CGP.\nThe CNN structure and connectivity represented by the CGP encoding method are\noptimized to maximize the validation accuracy. To evaluate the proposed method,\nwe constructed a CNN architecture for the image classification task with the\nCIFAR-10 dataset. The experimental result shows that the proposed method can be\nused to automatically find the competitive CNN architecture compared with\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:06:16 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 06:06:03 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Suganuma", "Masanori", ""], ["Shirakawa", "Shinichi", ""], ["Nagao", "Tomoharu", ""]]}, {"id": "1704.00828", "submitter": "Vin\\'icius Veloso de Melo", "authors": "L\\'eo Fran\\c{c}oso Dal Piccol Sotto and Vin\\'icius Veloso de Melo", "title": "A Probabilistic Linear Genetic Programming with Stochastic Context-Free\n  Grammar for solving Symbolic Regression problems", "comments": "Genetic and Evolutionary Computation Conference (GECCO) 2017, Berlin,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Linear Genetic Programming (LGP) algorithms are based only on the\nselection mechanism to guide the search. Genetic operators combine or mutate\nrandom portions of the individuals, without knowing if the result will lead to\na fitter individual. Probabilistic Model Building Genetic Programming (PMB-GP)\nmethods were proposed to overcome this issue through a probability model that\ncaptures the structure of the fit individuals and use it to sample new\nindividuals. This work proposes the use of LGP with a Stochastic Context-Free\nGrammar (SCFG), that has a probability distribution that is updated according\nto selected individuals. We proposed a method for adapting the grammar into the\nlinear representation of LGP. Tests performed with the proposed probabilistic\nmethod, and with two hybrid approaches, on several symbolic regression\nbenchmark problems show that the results are statistically better than the\nobtained by the traditional LGP.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 22:28:25 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Sotto", "L\u00e9o Fran\u00e7oso Dal Piccol", ""], ["de Melo", "Vin\u00edcius Veloso", ""]]}, {"id": "1704.01046", "submitter": "Rajkumar Ramamurthy Rajkumar Ramamurthy", "authors": "Rajkumar Ramamurthy, Christian Bauckhage, Krisztian Buza, Stefan\n  Wrobel", "title": "Using Echo State Networks for Cryptography", "comments": "8 pages, ICANN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state networks are simple recurrent neural networks that are easy to\nimplement and train. Despite their simplicity, they show a form of memory and\ncan predict or regenerate sequences of data. We make use of this property to\nrealize a novel neural cryptography scheme. The key idea is to assume that\nAlice and Bob share a copy of an echo state network. If Alice trains her copy\nto memorize a message, she can communicate the trained part of the network to\nBob who plugs it into his copy to regenerate the message. Considering a\nbyte-level representation of in- and output, the technique applies to arbitrary\ntypes of data (texts, images, audio files, etc.) and practical experiments\nreveal it to satisfy the fundamental cryptographic properties of diffusion and\nconfusion.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:00:46 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Ramamurthy", "Rajkumar", ""], ["Bauckhage", "Christian", ""], ["Buza", "Krisztian", ""], ["Wrobel", "Stefan", ""]]}, {"id": "1704.01137", "submitter": "Swagath Venkataramani", "authors": "Sanjay Ganapathy, Swagath Venkataramani, Balaraman Ravindran, Anand\n  Raghunathan", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety\nof machine learning tasks and are deployed in increasing numbers of products\nand services. However, the computational requirements of training and\nevaluating large-scale DNNs are growing at a much faster pace than the\ncapabilities of the underlying hardware platforms that they are executed upon.\nIn this work, we propose Dynamic Variable Effort Deep Neural Networks\n(DyVEDeep) to reduce the computational requirements of DNNs during inference.\nPrevious efforts propose specialized hardware implementations for DNNs,\nstatically prune the network, or compress the weights. Complementary to these\napproaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in\nthe inputs to DNNs to improve their compute efficiency with comparable\nclassification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms\nthat, in the course of processing an input, identify how critical a group of\ncomputations are to classify the input. DyVEDeep dynamically focuses its\ncompute effort only on the critical computa- tions, while skipping or\napproximating the rest. We propose 3 effort knobs that operate at different\nlevels of granularity viz. neuron, feature and layer levels. We build DyVEDeep\nversions for 5 popular image recognition benchmarks - one for CIFAR-10 and four\nfor ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across\nall benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar\noperations, which translates to 1.8x-2.3x performance improvement over a\nCaffe-based implementation, with < 0.5% loss in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 18:14:02 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Ganapathy", "Sanjay", ""], ["Venkataramani", "Swagath", ""], ["Ravindran", "Balaraman", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1704.01444", "submitter": "Alec Radford", "authors": "Alec Radford, Rafal Jozefowicz, Ilya Sutskever", "title": "Learning to Generate Reviews and Discovering Sentiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:20:28 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 09:48:20 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Radford", "Alec", ""], ["Jozefowicz", "Rafal", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1704.01523", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\n  Neural Networks", "comments": "Accepted at SemEval 2017. The first two authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 50 million scholarly articles have been published: they constitute a\nunique repository of knowledge. In particular, one may infer from them\nrelations between scientific concepts, such as synonyms and hyponyms.\nArtificial neural networks have been recently explored for relation extraction.\nIn this work, we continue this line of work and present a system based on a\nconvolutional neural network to extract relations. Our model ranked first in\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\narticles (subtask C).\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:54:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1704.01552", "submitter": "Yoav Levine Mr.", "authors": "Yoav Levine, David Yakira, Nadav Cohen and Amnon Shashua", "title": "Deep Learning and Quantum Entanglement: Fundamental Connections with\n  Implications to Network Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have witnessed unprecedented success in various\nmachine learning applications. Formal understanding on what makes these\nnetworks so successful is gradually unfolding, but for the most part there are\nstill significant mysteries to unravel. The inductive bias, which reflects\nprior knowledge embedded in the network architecture, is one of them. In this\nwork, we establish a fundamental connection between the fields of quantum\nphysics and deep learning. We use this connection for asserting novel\ntheoretical observations regarding the role that the number of channels in each\nlayer of the convolutional network fulfills in the overall inductive bias.\nSpecifically, we show an equivalence between the function realized by a deep\nconvolutional arithmetic circuit (ConvAC) and a quantum many-body wave\nfunction, which relies on their common underlying tensorial structure. This\nfacilitates the use of quantum entanglement measures as well-defined\nquantifiers of a deep network's expressive ability to model intricate\ncorrelation structures of its inputs. Most importantly, the construction of a\ndeep ConvAC in terms of a Tensor Network is made available. This description\nenables us to carry a graph-theoretic analysis of a convolutional network, with\nwhich we demonstrate a direct control over the inductive bias of the deep\nnetwork via its channel numbers, that are related to the min-cut in the\nunderlying graph. This result is relevant to any practitioner designing a\nnetwork for a specific task. We theoretically analyze ConvACs, and empirically\nvalidate our findings on more common ConvNets which involve ReLU activations\nand max pooling. Beyond the results described above, the description of a deep\nconvolutional network in well-defined graph-theoretic tools and the formal\nconnection to quantum entanglement, are two interdisciplinary bridges that are\nbrought forth by this work.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:53:13 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 14:29:34 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Levine", "Yoav", ""], ["Yakira", "David", ""], ["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1704.01568", "submitter": "Leslie Smith", "authors": "Leslie N. Smith", "title": "Best Practices for Applying Deep Learning to Novel Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": "NRL Technical Note 5510-052", "categories": "cs.SE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is targeted to groups who are subject matter experts in their\napplication but deep learning novices. It contains practical advice for those\ninterested in testing the use of deep neural networks on applications that are\nnovel for deep learning. We suggest making your project more manageable by\ndividing it into phases. For each phase this report contains numerous\nrecommendations and insights to assist novice practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:59:07 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Smith", "Leslie N.", ""]]}, {"id": "1704.01859", "submitter": "Raluca Necula", "authors": "Raluca Necula (1), Mihaela Breaban (1) and Madalina Raschip (1) ((1)\n  Faculty of Computer Science, Alexandru Ioan Cuza University, Iasi, Romania)", "title": "Tackling Dynamic Vehicle Routing Problem with Time Windows by means of\n  Ant Colony System", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) is an\nextension of the well-known Vehicle Routing Problem (VRP), which takes into\naccount the dynamic nature of the problem. This aspect requires the vehicle\nroutes to be updated in an ongoing manner as new customer requests arrive in\nthe system and must be incorporated into an evolving schedule during the\nworking day. Besides the vehicle capacity constraint involved in the classical\nVRP, DVRPTW considers in addition time windows, which are able to better\ncapture real-world situations. Despite this, so far, few studies have focused\non tackling this problem of greater practical importance. To this end, this\nstudy devises for the resolution of DVRPTW, an ant colony optimization based\nalgorithm, which resorts to a joint solution construction mechanism, able to\nconstruct in parallel the vehicle routes. This method is coupled with a local\nsearch procedure, aimed to further improve the solutions built by ants, and\nwith an insertion heuristics, which tries to reduce the number of vehicles used\nto service the available customers. The experiments indicate that the proposed\nalgorithm is competitive and effective, and on DVRPTW instances with a higher\ndynamicity level, it is able to yield better results compared to existing\nant-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:29:14 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Necula", "Raluca", ""], ["Breaban", "Mihaela", ""], ["Raschip", "Madalina", ""]]}, {"id": "1704.02012", "submitter": "Aditya Shukla", "authors": "Aditya Shukla, Vinay Kumar, Udayan Ganguly", "title": "A Software-equivalent SNN Hardware using RRAM-array for Asynchronous\n  Real-time Learning", "comments": "Eight pages, ten figures and two tables", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7966447", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Network (SNN) naturally inspires hardware implementation as it\nis based on biology. For learning, spike time dependent plasticity (STDP) may\nbe implemented using an energy efficient waveform superposition on memristor\nbased synapse. However, system level implementation has three challenges.\nFirst, a classic dilemma is that recognition requires current reading for short\nvoltage$-$spikes which is disturbed by large voltage$-$waveforms that are\nsimultaneously applied on the same memristor for real$-$time learning i.e. the\nsimultaneous read$-$write dilemma. Second, the hardware needs to exactly\nreplicate software implementation for easy adaptation of algorithm to hardware.\nThird, the devices used in hardware simulations must be realistic. In this\npaper, we present an approach to address the above concerns. First, the\nlearning and recognition occurs in separate arrays simultaneously in\nreal$-$time, asynchronously $-$ avoiding non$-$biomimetic clocking based\ncomplex signal management. Second, we show that the hardware emulates software\nat every stage by comparison of SPICE (circuit$-$simulator) with MATLAB\n(mathematical SNN algorithm implementation in software) implementations. As an\nexample, the hardware shows 97.5 per cent accuracy in classification which is\nequivalent to software for a Fisher$-$Iris dataset. Third, the STDP is\nimplemented using a model of synaptic device implemented using HfO2 memristor.\nWe show that an increasingly realistic memristor model slightly reduces the\nhardware performance (85 per cent), which highlights the need to engineer RRAM\ncharacteristics specifically for SNN.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 20:25:36 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Shukla", "Aditya", ""], ["Kumar", "Vinay", ""], ["Ganguly", "Udayan", ""]]}, {"id": "1704.02019", "submitter": "Rishidev Chaudhuri", "authors": "Rishidev Chaudhuri and Ila Fiete", "title": "Associative content-addressable networks with exponentially many robust\n  stable states", "comments": "42 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain must robustly store a large number of memories, corresponding to\nthe many events encountered over a lifetime. However, the number of memory\nstates in existing neural network models either grows weakly with network size\nor recall fails catastrophically with vanishingly little noise. We construct an\nassociative content-addressable memory with exponentially many stable states\nand robust error-correction. The network possesses expander graph connectivity\non a restricted Boltzmann machine architecture. The expansion property allows\nsimple neural network dynamics to perform at par with modern error-correcting\ncodes. Appropriate networks can be constructed with sparse random connections,\nglomerular nodes, and associative learning using low dynamic-range weights.\nThus, sparse quasi-random structures---characteristic of important\nerror-correcting codes---may provide for high-performance computation in\nartificial neural networks and the brain.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 20:46:16 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 18:30:38 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Chaudhuri", "Rishidev", ""], ["Fiete", "Ila", ""]]}, {"id": "1704.02081", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Elnaz Barshan, and Alexander Wong", "title": "Evolution in Groups: A deeper look at synaptic cluster driven evolution\n  of deep neural networks", "comments": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:1609.01360", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising paradigm for achieving highly efficient deep neural networks is\nthe idea of evolutionary deep intelligence, which mimics biological evolution\nprocesses to progressively synthesize more efficient networks. A crucial design\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\nsimulate heredity and determine the architectures of offspring networks. In\nthis study, we take a deeper look at the notion of synaptic cluster-driven\nevolution of deep neural networks which guides the evolution process towards\nthe formation of a highly sparse set of synaptic clusters in offspring\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\nprobabilistic encoding of synaptic traits considers not only individual\nsynaptic properties but also inter-synaptic relationships within a deep neural\nnetwork. This process results in highly sparse offspring networks which are\nparticularly tailored for parallel computational devices such as GPUs and deep\nneural network accelerator chips. Comprehensive experimental results using four\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\nDetectNet) on two different tasks (object categorization and object detection)\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\nencoding scheme synthesizes networks that can achieve state-of-the-art\nperformance with significantly smaller number of synapses than that of the\noriginal ancestor network. ($\\sim$125-fold decrease in synapses for MNIST).\nFurthermore, the improved cluster efficiency in the generated offspring\nnetworks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold\ndecrease in clusters for KITTI) is particularly useful for accelerated\nperformance on parallel computing hardware architectures such as those in GPUs\nand deep neural network accelerator chips.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 03:28:02 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Barshan", "Elnaz", ""], ["Wong", "Alexander", ""]]}, {"id": "1704.02191", "submitter": "Carsten Witt", "authors": "Benjamin Doerr and Christian Gie{\\ss}en and Carsten Witt and Jing Yang", "title": "The (1+$\\lambda$) Evolutionary Algorithm with Self-Adjusting Mutation\n  Rate", "comments": "An extended abstract of this report appeared in the proceedings of\n  the 2017 Genetic and Evolutionary Computation Conference (GECCO 2017),\n  https://doi.org/10.1145/3071178.3071279. Version 2: several extensions, most\n  notably regarding experimental results; Version 3: revised presentation and\n  added more experiments", "journal-ref": null, "doi": "10.1145/3071178.3071279", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new way to self-adjust the mutation rate in population-based\nevolutionary algorithms in discrete search spaces. Roughly speaking, it\nconsists of creating half the offspring with a mutation rate that is twice the\ncurrent mutation rate and the other half with half the current rate. The\nmutation rate is then updated to the rate used in that subpopulation which\ncontains the best offspring.\n  We analyze how the $(1+\\lambda)$ evolutionary algorithm with this\nself-adjusting mutation rate optimizes the OneMax test function. We prove that\nthis dynamic version of the $(1+\\lambda)$ EA finds the optimum in an expected\noptimization time (number of fitness evaluations) of\n$O(n\\lambda/\\log\\lambda+n\\log n)$. This time is asymptotically smaller than the\noptimization time of the classic $(1+\\lambda)$ EA. Previous work shows that\nthis performance is best-possible among all $\\lambda$-parallel mutation-based\nunbiased black-box algorithms.\n  This result shows that the new way of adjusting the mutation rate can find\noptimal dynamic parameter values on the fly. Since our adjustment mechanism is\nsimpler than the ones previously used for adjusting the mutation rate and does\nnot have parameters itself, we are optimistic that it will find other\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 11:27:05 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 07:35:48 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 12:07:01 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Doerr", "Benjamin", ""], ["Gie\u00dfen", "Christian", ""], ["Witt", "Carsten", ""], ["Yang", "Jing", ""]]}, {"id": "1704.02286", "submitter": "Xavier Bellekens", "authors": "Elike Hodo and Xavier Bellekens and Andrew Hamilton and Pierre-louis\n  Dubouilh and Ephraim Iorkyase and Christos Tachtatzis and Robert Atkinson", "title": "Threat analysis of IoT networks Using Artificial Neural Network\n  Intrusion Detection System", "comments": "Published in The 2016 International Symposium on Networks, Computers\n  and Communications (IEEE ISNCC'16) , Hammamet, Tunisia, 2016", "journal-ref": null, "doi": "10.1109/ISNCC.2016.7746067", "report-no": null, "categories": "cs.NE cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet of things (IoT) is still in its infancy and has attracted much\ninterest in many industrial sectors including medical fields, logistics\ntracking, smart cities and automobiles. However as a paradigm, it is\nsusceptible to a range of significant intrusion threats. This paper presents a\nthreat analysis of the IoT and uses an Artificial Neural Network (ANN) to\ncombat these threats. A multi-level perceptron, a type of supervised ANN, is\ntrained using internet packet traces, then is assessed on its ability to thwart\nDistributed Denial of Service (DDoS/DoS) attacks. This paper focuses on the\nclassification of normal and threat patterns on an IoT Network. The ANN\nprocedure is validated against a simulated IoT network. The experimental\nresults demonstrate 99.4% accuracy and can successfully detect various DDoS/DoS\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 16:40:13 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Hodo", "Elike", ""], ["Bellekens", "Xavier", ""], ["Hamilton", "Andrew", ""], ["Dubouilh", "Pierre-louis", ""], ["Iorkyase", "Ephraim", ""], ["Tachtatzis", "Christos", ""], ["Atkinson", "Robert", ""]]}, {"id": "1704.02312", "submitter": "Yaoyuan Zhang", "authors": "Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan Zhao, Rui Yan", "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence\n  Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence simplification reduces semantic complexity to benefit people with\nlanguage impairments. Previous simplification studies on the sentence level and\nword level have achieved promising results but also meet great challenges. For\nsentence-level studies, sentences after simplification are fluent but sometimes\nare not really simplified. For word-level studies, words are simplified but\nalso have potential grammar errors due to different usages of words before and\nafter simplification. In this paper, we propose a two-step simplification\nframework by combining both the word-level and the sentence-level\nsimplifications, making use of their corresponding advantages. Based on the\ntwo-step framework, we implement a novel constrained neural generation model to\nsimplify sentences given simplified words. The final results on Wikipedia and\nSimple Wikipedia aligned datasets indicate that our method yields better\nperformance than various baselines.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:53:24 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Zhang", "Yaoyuan", ""], ["Ye", "Zhenxu", ""], ["Feng", "Yansong", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1704.02340", "submitter": "Ke Li Kl", "authors": "Mengyuan Wu, Ke Li, Sam Kwong, Qingfu Zhang", "title": "Evolutionary Many-Objective Optimization Based on Adversarial\n  Decomposition", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decomposition-based method has been recognized as a major approach for\nmulti-objective optimization. It decomposes a multi-objective optimization\nproblem into several single-objective optimization subproblems, each of which\nis usually defined as a scalarizing function using a weight vector. Due to the\ncharacteristics of the contour line of a particular scalarizing function, the\nperformance of the decomposition-based method strongly depends on the Pareto\nfront's shape by merely using a single scalarizing function, especially when\nfacing a large number of objectives. To improve the flexibility of the\ndecomposition-based method, this paper develops an adversarial decomposition\nmethod that leverages the complementary characteristics of two different\nscalarizing functions within a single paradigm. More specifically, we maintain\ntwo co-evolving populations simultaneously by using different scalarizing\nfunctions. In order to avoid allocating redundant computational resources to\nthe same region of the Pareto front, we stably match these two co-evolving\npopulations into one-one solution pairs according to their working regions of\nthe Pareto front. Then, each solution pair can at most contribute one mating\nparent during the mating selection process. Comparing with nine\nstate-of-the-art many-objective optimizers, we have witnessed the competitive\nperformance of our proposed algorithm on 130 many-objective test instances with\nvarious characteristics and Pareto front's shapes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 18:28:19 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Wu", "Mengyuan", ""], ["Li", "Ke", ""], ["Kwong", "Sam", ""], ["Zhang", "Qingfu", ""]]}, {"id": "1704.02681", "submitter": "Vincenzo Liguori", "authors": "Vincenzo Liguori", "title": "Pyramid Vector Quantization for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce\nthe computational cost for a variety of neural networks (NNs) while, at the\nsame time, compressing the weights that describe them. This is based on the\nfact that the dot product between an N dimensional vector of real numbers and\nan N dimensional PVQ vector can be calculated with only additions and\nsubtractions and one multiplication. This is advantageous since tensor\nproducts, commonly used in NNs, can be re-conduced to a dot product or a set of\ndot products. Finally, it is stressed that any NN architecture that is based on\nan operation that can be re-conduced to a dot product can benefit from the\ntechniques described here.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 01:17:43 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Liguori", "Vincenzo", ""]]}, {"id": "1704.02685", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Peyton Greenside, Anshul Kundaje", "title": "Learning Important Features Through Propagating Activation Differences", "comments": "Updated to include changes present in the ICML camera-ready paper,\n  and other small corrections", "journal-ref": "PMLR 70:3145-3153, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purported \"black box\" nature of neural networks is a barrier to adoption\nin applications where interpretability is essential. Here we present DeepLIFT\n(Deep Learning Important FeaTures), a method for decomposing the output\nprediction of a neural network on a specific input by backpropagating the\ncontributions of all neurons in the network to every feature of the input.\nDeepLIFT compares the activation of each neuron to its 'reference activation'\nand assigns contribution scores according to the difference. By optionally\ngiving separate consideration to positive and negative contributions, DeepLIFT\ncan also reveal dependencies which are missed by other approaches. Scores can\nbe computed efficiently in a single backward pass. We apply DeepLIFT to models\ntrained on MNIST and simulated genomic data, and show significant advantages\nover gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:\nbit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:\nhttp://goo.gl/RM8jvH.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 02:23:57 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 22:13:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Greenside", "Peyton", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1704.02789", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Plamen P. Angelov, Edwin Lughofer", "title": "Parsimonious Random Vector Functional Link Network for Data Streams", "comments": "this paper is submitted for publication in Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2017.11.050", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of random vector functional link network (RVFLN) has provided a\nbreakthrough in the design of neural networks (NNs) since it conveys solid\ntheoretical justification of randomized learning. Existing works in RVFLN are\nhardly scalable for data stream analytics because they are inherent to the\nissue of complexity as a result of the absence of structural learning\nscenarios. A novel class of RVLFN, namely parsimonious random vector functional\nlink network (pRVFLN), is proposed in this paper. pRVFLN features an open\nstructure paradigm where its network structure can be built from scratch and\ncan be automatically generated in accordance with degree of nonlinearity and\ntime-varying property of system being modelled. pRVFLN is equipped with\ncomplexity reduction scenarios where inconsequential hidden nodes can be pruned\nand input features can be dynamically selected. pRVFLN puts into perspective an\nonline active learning mechanism which expedites the training process and\nrelieves operator labelling efforts. In addition, pRVFLN introduces a\nnon-parametric type of hidden node, developed using an interval-valued data\ncloud. The hidden node completely reflects the real data distribution and is\nnot constrained by a specific shape of the cluster. All learning procedures of\npRVFLN follow a strictly single-pass learning mode, which is applicable for an\nonline real-time deployment. The efficacy of pRVFLN was rigorously validated\nthrough numerous simulations and comparisons with state-of-the art algorithms\nwhere it produced the most encouraging numerical results. Furthermore, the\nrobustness of pRVFLN was investigated and a new conclusion is made to the scope\nof random parameters where it plays vital role to the success of randomized\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:24:34 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 11:59:53 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Angelov", "Plamen P.", ""], ["Lughofer", "Edwin", ""]]}, {"id": "1704.02848", "submitter": "Hai-Jun Zhou", "authors": "Huiling Zhen, Shang-Nan Wang, and Hai-Jun Zhou", "title": "Unsupervised prototype learning in an associative-memory network", "comments": "We found serious inconsistence between the numerical protocol\n  described in the text and the actual numerical code used by the first author\n  to produce the data. Because of this inconsistence, we decide to withdraw the\n  preprint. The corresponding author (Hai-Jun Zhou) deeply apologizes for not\n  being able to detect this inconsistence earlier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning in a generalized Hopfield associative-memory network is\ninvestigated in this work. First, we prove that the (generalized) Hopfield\nmodel is equivalent to a semi-restricted Boltzmann machine with a layer of\nvisible neurons and another layer of hidden binary neurons, so it could serve\nas the building block for a multilayered deep-learning system. We then\ndemonstrate that the Hopfield network can learn to form a faithful internal\nrepresentation of the observed samples, with the learned memory patterns being\nprototypes of the input data. Furthermore, we propose a spectral method to\nextract a small set of concepts (idealized prototypes) as the most concise\nsummary or abstraction of the empirical data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:20:23 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 02:45:12 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Zhen", "Huiling", ""], ["Wang", "Shang-Nan", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1704.02901", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Nikos Komodakis", "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on\n  Graphs", "comments": "Accepted to CVPR 2017; extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems can be formulated as prediction on graph-structured\ndata. In this work, we generalize the convolution operator from regular grids\nto arbitrary graphs while avoiding the spectral domain, which allows us to\nhandle graphs of varying size and connectivity. To move beyond a simple\ndiffusion, filter weights are conditioned on the specific edge labels in the\nneighborhood of a vertex. Together with the proper choice of graph coarsening,\nwe explore constructing deep neural networks for graph classification. In\nparticular, we demonstrate the generality of our formulation in point cloud\nclassification, where we set the new state of the art, and on a graph\nclassification dataset, where we outperform other deep learning approaches. The\nsource code is available at https://github.com/mys007/ecc\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:18:54 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 18:05:11 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 09:31:17 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1704.03003", "submitter": "Alex Graves", "authors": "Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, Koray\n  Kavukcuoglu", "title": "Automated Curriculum Learning for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for automatically selecting the path, or syllabus, that\na neural network follows through a curriculum so as to maximise learning\nefficiency. A measure of the amount that the network learns from each data\nsample is provided as a reward signal to a nonstationary multi-armed bandit\nalgorithm, which then determines a stochastic syllabus. We consider a range of\nsignals derived from two distinct indicators of learning progress: rate of\nincrease in prediction accuracy, and rate of increase in network complexity.\nExperimental results for LSTM networks on three curricula demonstrate that our\napproach can significantly accelerate learning, in some cases halving the time\nrequired to attain a satisfactory performance level.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:25:29 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Graves", "Alex", ""], ["Bellemare", "Marc G.", ""], ["Menick", "Jacob", ""], ["Munos", "Remi", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1704.03012", "submitter": "Carlos Florensa Campo", "authors": "Carlos Florensa, Yan Duan, Pieter Abbeel", "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": "International Conference on Learning Representations 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has achieved many impressive results in recent\nyears. However, tasks with sparse rewards or long horizons continue to pose\nsignificant challenges. To tackle these important problems, we propose a\ngeneral framework that first learns useful skills in a pre-training\nenvironment, and then leverages the acquired skills for learning faster in\ndownstream tasks. Our approach brings together some of the strengths of\nintrinsic motivation and hierarchical methods: the learning of useful skill is\nguided by a single proxy reward, the design of which requires very minimal\ndomain knowledge about the downstream tasks. Then a high-level policy is\ntrained on top of these skills, providing a significant improvement of the\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\ncombined with an information-theoretic regularizer. Our experiments show that\nthis combination is effective in learning a wide span of interpretable skills\nin a sample-efficient way, and can significantly boost the learning performance\nuniformly across a wide range of downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:41:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Florensa", "Carlos", ""], ["Duan", "Yan", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1704.03079", "submitter": "Asit Mishra", "authors": "Asit Mishra, Jeffrey J Cook, Eriko Nurvitadhi and Debbie Marr", "title": "WRPN: Training and Inference using Wide Reduced-Precision Networks", "comments": "Under submission to CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For computer vision applications, prior works have shown the efficacy of\nreducing the numeric precision of model parameters (network weights) in deep\nneural networks but also that reducing the precision of activations hurts model\naccuracy much more than reducing the precision of model parameters. We study\nschemes to train networks from scratch using reduced-precision activations\nwithout hurting the model accuracy. We reduce the precision of activation maps\n(along with model parameters) using a novel quantization scheme and increase\nthe number of filter maps in a layer, and find that this scheme compensates or\nsurpasses the accuracy of the baseline full-precision network. As a result, one\ncan significantly reduce the dynamic memory footprint, memory bandwidth,\ncomputational energy and speed up the training and inference process with\nappropriate hardware support. We call our scheme WRPN - wide reduced-precision\nnetworks. We report results using our proposed schemes and show that our\nresults are better than previously reported accuracies on ILSVRC-12 dataset\nwhile being computationally less expensive compared to previously reported\nreduced-precision networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 22:54:38 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mishra", "Asit", ""], ["Cook", "Jeffrey J", ""], ["Nurvitadhi", "Eriko", ""], ["Marr", "Debbie", ""]]}, {"id": "1704.03477", "submitter": "David Ha", "authors": "David Ha and Douglas Eck", "title": "A Neural Representation of Sketch Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sketch-rnn, a recurrent neural network (RNN) able to construct\nstroke-based drawings of common objects. The model is trained on thousands of\ncrude human-drawn images representing hundreds of classes. We outline a\nframework for conditional and unconditional sketch generation, and describe new\nrobust training methods for generating coherent sketch drawings in a vector\nformat.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 01:26:05 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 16:28:23 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 16:40:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ha", "David", ""], ["Eck", "Douglas", ""]]}, {"id": "1704.03522", "submitter": "Nhien-An Le-Khac", "authors": "Van Loi Cao, Nhien-An Le-Khac, Miguel Nicolau, Michael ONeill, James\n  McDermott", "title": "Improving Fitness Functions in Genetic Programming for Classification on\n  Unbalanced Credit Card Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit card fraud detection based on machine learning has recently attracted\nconsiderable interest from the research community. One of the most important\ntasks in this area is the ability of classifiers to handle the imbalance in\ncredit card data. In this scenario, classifiers tend to yield poor accuracy on\nthe fraud class (minority class) despite realizing high overall accuracy. This\nis due to the influence of the majority class on traditional training criteria.\nIn this paper, we aim to apply genetic programming to address this issue by\nadapting existing fitness functions. We examine two fitness functions from\nprevious studies and develop two new fitness functions to evolve GP classifier\nwith superior accuracy on the minority class and overall. Two UCI credit card\ndatasets are used to evaluate the effectiveness of the proposed fitness\nfunctions. The results demonstrate that the proposed fitness functions augment\nGP classifiers, encouraging fitter solutions on both the minority and the\nmajority classes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 20:09:16 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cao", "Van Loi", ""], ["Le-Khac", "Nhien-An", ""], ["Nicolau", "Miguel", ""], ["ONeill", "Michael", ""], ["McDermott", "James", ""]]}, {"id": "1704.03664", "submitter": "Francesco Quinzan", "authors": "Ankit Chauhan and Tobias Friedrich and Francesco Quinzan", "title": "Approximating Optimization Problems using EAs on Scale-Free Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed that many complex real-world networks have certain\nproperties, such as a high clustering coefficient, a low diameter, and a\npower-law degree distribution. A network with a power-law degree distribution\nis known as scale-free network. In order to study these networks, various\nrandom graph models have been proposed, e.g. Preferential Attachment, Chung-Lu,\nor Hyperbolic.\n  We look at the interplay between the power-law degree distribution and the\nrun time of optimization techniques for well known combinatorial problems. We\nobserve that on scale-free networks, simple evolutionary algorithms (EAs)\nquickly reach a constant-factor approximation ratio on common covering problems\n  We prove that the single-objective (1+1)EA reaches a constant-factor\napproximation ratio on the Minimum Dominating Set problem, the Minimum Vertex\nCover problem, the Minimum Connected Dominating Set problem, and the Maximum\nIndependent Set problem in expected polynomial number of calls to the fitness\nfunction.\n  Furthermore, we prove that the multi-objective GSEMO algorithm reaches a\nbetter approximation ratio than the (1+1)EA on those problems, within\npolynomial fitness evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 09:06:06 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 14:08:52 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 13:15:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chauhan", "Ankit", ""], ["Friedrich", "Tobias", ""], ["Quinzan", "Francesco", ""]]}, {"id": "1704.03847", "submitter": "Nikolay Savinov", "authors": "Timo Hackel, Nikolay Savinov, Lubor Ladicky, Jan D. Wegner, Konrad\n  Schindler, Marc Pollefeys", "title": "Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark", "comments": "Accepted to ISPRS Annals. The benchmark website is available at\n  http://www.semantic3d.net/ . The baseline code is available at\n  https://github.com/nsavinov/semantic3dnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new 3D point cloud classification benchmark data set\nwith over four billion manually labelled points, meant as input for data-hungry\n(deep) learning methods. We also discuss first submissions to the benchmark\nthat use deep convolutional neural networks (CNNs) as a work horse, which\nalready show remarkable performance improvements over state-of-the-art. CNNs\nhave become the de-facto standard for many tasks in computer vision and machine\nlearning like semantic segmentation or object detection in images, but have no\nyet led to a true breakthrough for 3D point cloud labelling tasks due to lack\nof training data. With the massive data set presented in this paper, we aim at\nclosing this data gap to help unleash the full potential of deep learning\nmethods for 3D labelling tasks. Our semantic3D.net data set consists of dense\npoint clouds acquired with static terrestrial laser scanners. It contains 8\nsemantic classes and covers a wide range of urban outdoor scenes: churches,\nstreets, railroad tracks, squares, villages, soccer fields and castles. We\ndescribe our labelling interface and show that our data set provides more dense\nand complete point clouds with much higher overall number of labelled points\ncompared to those already available to the research community. We further\nprovide baseline method descriptions and comparison between methods submitted\nto our online system. We hope semantic3D.net will pave the way for deep\nlearning methods in 3D point cloud labelling to learn richer, more general 3D\nrepresentations, and first submissions after only a few months indicate that\nthis might indeed be the case.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:12:57 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Hackel", "Timo", ""], ["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Wegner", "Jan D.", ""], ["Schindler", "Konrad", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1704.03993", "submitter": "Xiaojing Xu", "authors": "Xiaojing Xu, Srinjoy Das, Ken Kreutz-Delgado", "title": "ApproxDBN: Approximate Computing for Discriminative Deep Belief Networks", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic generative neural networks are useful for many applications,\nsuch as image classification, speech recognition and occlusion removal.\nHowever, the power budget for hardware implementations of neural networks can\nbe extremely tight. To address this challenge we describe a design methodology\nfor using approximate computing methods to implement Approximate Deep Belief\nNetworks (ApproxDBNs) by systematically exploring the use of (1) limited\nprecision of variables; (2) criticality analysis to identify the nodes in the\nnetwork which can operate with such limited precision while allowing the\nnetwork to maintain target accuracy levels; and (3) a greedy search methodology\nwith incremental retraining to determine the optimal reduction in precision to\nenable maximize power savings under user-specified accuracy constraints.\nExperimental results show that significant bit-length reduction can be achieved\nby our ApproxDBN with constrained accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 05:04:44 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 03:46:41 GMT"}, {"version": "v3", "created": "Sat, 6 May 2017 17:14:00 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Xu", "Xiaojing", ""], ["Das", "Srinjoy", ""], ["Kreutz-Delgado", "Ken", ""]]}, {"id": "1704.04095", "submitter": "Mohsen Moradi", "authors": "Mohsen Moradi", "title": "Training Neural Networks Based on Imperialist Competitive Algorithm for\n  Predicting Earthquake Intensity", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study we determined neural network weights and biases by Imperialist\nCompetitive Algorithm (ICA) in order to train network for predicting earthquake\nintensity in Richter. For this reason, we used dependent parameters like\nearthquake occurrence time, epicenter's latitude and longitude in degree, focal\ndepth in kilometer, and the seismological center distance from epicenter and\nearthquake focal center in kilometer which has been provided by Berkeley data\nbase. The studied neural network has two hidden layer: its first layer has 16\nneurons and the second layer has 24 neurons. By using ICA algorithm, average\nerror for testing data is 0.0007 with a variance equal to 0.318. The earthquake\nprediction error in Richter by MSE criteria for ICA algorithm is 0.101, but by\nusing GA, the MSE value is 0.115.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 22:42:52 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Moradi", "Mohsen", ""]]}, {"id": "1704.04119", "submitter": "Brendan Cody-Kenny", "authors": "Brendan Cody-Kenny, Michael Fenton, Adrian Ronayne, Eoghan Considine,\n  Thomas McGuire, Michael O'Neill", "title": "A Search for Improved Performance in Regular Expressions", "comments": "Submitted to the Search-Based Software Engineering (SBSE) track at\n  the Genetic and Evolutionary Computation Conference (GECCO) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary aim of automated performance improvement is to reduce the running\ntime of programs while maintaining (or improving on) functionality. In this\npaper, Genetic Programming is used to find performance improvements in regular\nexpressions for an array of target programs, representing the first application\nof automated software improvement for run-time performance in the Regular\nExpression language. This particular problem is interesting as there may be\nmany possible alternative regular expressions which perform the same task while\nexhibiting subtle differences in performance. A benchmark suite of candidate\nregular expressions is proposed for improvement. We show that the application\nof Genetic Programming techniques can result in performance improvements in all\ncases.\n  As we start evolution from a known good regular expression, diversity is\ncritical in escaping the local optima of the seed expression. In order to\nunderstand diversity during evolution we compare an initial population\nconsisting of only seed programs with a population initialised using a\ncombination of a single seed individual with individuals generated using PI\nGrow and Ramped-half-and-half initialisation mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:23:21 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Cody-Kenny", "Brendan", ""], ["Fenton", "Michael", ""], ["Ronayne", "Adrian", ""], ["Considine", "Eoghan", ""], ["McGuire", "Thomas", ""], ["O'Neill", "Michael", ""]]}, {"id": "1704.04199", "submitter": "Madhavun Candadai Vasu", "authors": "Madhavun Candadai Vasu, Eduardo J. Izquierdo", "title": "Evolution and Analysis of Embodied Spiking Neural Networks Reveals\n  Task-Specific Clusters of Effective Networks", "comments": "Camera ready version of accepted for GECCO'17", "journal-ref": null, "doi": "10.1145/3071178.3071336", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elucidating principles that underlie computation in neural networks is\ncurrently a major research topic of interest in neuroscience. Transfer Entropy\n(TE) is increasingly used as a tool to bridge the gap between network\nstructure, function, and behavior in fMRI studies. Computational models allow\nus to bridge the gap even further by directly associating individual neuron\nactivity with behavior. However, most computational models that have analyzed\nembodied behaviors have employed non-spiking neurons. On the other hand,\ncomputational models that employ spiking neural networks tend to be restricted\nto disembodied tasks. We show for the first time the artificial evolution and\nTE-analysis of embodied spiking neural networks to perform a\ncognitively-interesting behavior. Specifically, we evolved an agent controlled\nby an Izhikevich neural network to perform a visual categorization task. The\nsmallest networks capable of performing the task were found by repeating\nevolutionary runs with different network sizes. Informational analysis of the\nbest solution revealed task-specific TE-network clusters, suggesting that\nwithin-task homogeneity and across-task heterogeneity were key to behavioral\nsuccess. Moreover, analysis of the ensemble of solutions revealed that\ntask-specificity of TE-network clusters correlated with fitness. This provides\nan empirically testable hypothesis that links network structure to behavior.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 16:23:19 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 00:53:16 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Vasu", "Madhavun Candadai", ""], ["Izquierdo", "Eduardo J.", ""]]}, {"id": "1704.04238", "submitter": "David Kappel", "authors": "David Kappel, Robert Legenstein, Stefan Habenschuss, Michael Hsieh and\n  Wolfgang Maass", "title": "A dynamic connectome supports the emergence of stable computational\n  function of neural circuits through reward-based learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic connections between neurons in the brain are dynamic because of\ncontinuously ongoing spine dynamics, axonal sprouting, and other processes. In\nfact, it was recently shown that the spontaneous synapse-autonomous component\nof spine dynamics is at least as large as the component that depends on the\nhistory of pre- and postsynaptic neural activity. These data are inconsistent\nwith common models for network plasticity, and raise the questions how neural\ncircuits can maintain a stable computational function in spite of these\ncontinuously ongoing processes, and what functional uses these ongoing\nprocesses might have. Here, we present a rigorous theoretical framework for\nthese seemingly stochastic spine dynamics and rewiring processes in the context\nof reward-based learning tasks. We show that spontaneous synapse-autonomous\nprocesses, in combination with reward signals such as dopamine, can explain the\ncapability of networks of neurons in the brain to configure themselves for\nspecific computational tasks, and to compensate automatically for later changes\nin the network or task. Furthermore we show theoretically and through computer\nsimulations that stable computational performance is compatible with\ncontinuously ongoing synapse-autonomous changes. After reaching good\ncomputational performance it causes primarily a slow drift of network\narchitecture and dynamics in task-irrelevant dimensions, as observed for neural\nactivity in motor cortex and other areas. On the more abstract level of\nreinforcement learning the resulting model gives rise to an understanding of\nreward-driven network plasticity as continuous sampling of network\nconfigurations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 15:52:14 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 10:34:44 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 08:11:34 GMT"}, {"version": "v4", "created": "Fri, 5 Jan 2018 12:56:42 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kappel", "David", ""], ["Legenstein", "Robert", ""], ["Habenschuss", "Stefan", ""], ["Hsieh", "Michael", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1704.04366", "submitter": "Maxim Buzdalov", "authors": "Maxim Buzdalov and Benjamin Doerr", "title": "Runtime Analysis of the $(1+(\\lambda,\\lambda))$ Genetic Algorithm on\n  Random Satisfiable 3-CNF Formulas", "comments": "An extended abstract of this report will appear in the proceedings of\n  the 2017 Genetic and Evolutionary Computation Conference (GECCO 2017)", "journal-ref": null, "doi": "10.1145/3071178.3071297", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $(1+(\\lambda,\\lambda))$ genetic algorithm, first proposed at GECCO 2013,\nshowed a surprisingly good performance on so me optimization problems. The\ntheoretical analysis so far was restricted to the OneMax test function, where\nthis GA profited from the perfect fitness-distance correlation. In this work,\nwe conduct a rigorous runtime analysis of this GA on random 3-SAT instances in\nthe planted solution model having at least logarithmic average degree, which\nare known to have a weaker fitness distance correlation.\n  We prove that this GA with fixed not too large population size again obtains\nruntimes better than $\\Theta(n \\log n)$, which is a lower bound for most\nevolutionary algorithms on pseudo-Boolean problems with unique optimum.\nHowever, the self-adjusting version of the GA risks reaching population sizes\nat which the intermediate selection of the GA, due to the weaker\nfitness-distance correlation, is not able to distinguish a profitable offspring\nfrom others. We show that this problem can be overcome by equipping the\nself-adjusting GA with an upper limit for the population size. Apart from\nsparse instances, this limit can be chosen in a way that the asymptotic\nperformance does not worsen compared to the idealistic OneMax case. Overall,\nthis work shows that the $(1+(\\lambda,\\lambda))$ GA can provably have a good\nperformance on combinatorial search and optimization problems also in the\npresence of a weaker fitness-distance correlation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 06:54:08 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Buzdalov", "Maxim", ""], ["Doerr", "Benjamin", ""]]}, {"id": "1704.04640", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Antonella Nardin, Enrico Natalizio", "title": "A fast ILP-based Heuristic for the robust design of Body Wireless Sensor\n  Networks", "comments": "This is the authors' final version of the paper published in G.\n  Squillero and K. Sim (Eds.): EvoApplications 2017, Part I, LNCS 10199, pp.\n  1-17, 2017. DOI: 10.1007/978-3-319-55849-3\\_16. The final publication is\n  available at Springer via http://dx.doi.org/10.1007/978-3-319-55849-3_16", "journal-ref": "EvoApplications 2017, Springer LNCS 10199 (2017) 1-17", "doi": "10.1007/978-3-319-55849-3_16", "report-no": null, "categories": "math.OC cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimally designing a body wireless sensor\nnetwork, while taking into account the uncertainty of data generation of\nbiosensors. Since the related min-max robustness Integer Linear Programming\n(ILP) problem can be difficult to solve even for state-of-the-art commercial\noptimization solvers, we propose an original heuristic for its solution. The\nheuristic combines deterministic and probabilistic variable fixing strategies,\nguided by the information coming from strengthened linear relaxations of the\nILP robust model, and includes a very large neighborhood search for reparation\nand improvement of generated solutions, formulated as an ILP problem solved\nexactly. Computational tests on realistic instances show that our heuristic\nfinds solutions of much higher quality than a state-of-the-art solver and than\nan effective benchmark heuristic.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 14:00:48 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Nardin", "Antonella", ""], ["Natalizio", "Enrico", ""]]}, {"id": "1704.04760", "submitter": "David Patterson David Patterson", "authors": "Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav\n  Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,\n  Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell,\n  Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,\n  Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug\n  Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek\n  Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve\n  Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle\n  Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran\n  Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas\n  Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt\n  Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew\n  Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory\n  Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter,\n  Walter Wang, Eric Wilcox, and Doe Hyun Yoon", "title": "In-Datacenter Performance Analysis of a Tensor Processing Unit", "comments": "17 pages, 11 figures, 8 tables. To appear at the 44th International\n  Symposium on Computer Architecture (ISCA), Toronto, Canada, June 24-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many architects believe that major improvements in cost-energy-performance\nmust now come from domain-specific hardware. This paper evaluates a custom\nASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since\n2015 that accelerates the inference phase of neural networks (NN). The heart of\nthe TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak\nthroughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed\non-chip memory. The TPU's deterministic execution model is a better match to\nthe 99th-percentile response-time requirement of our NN applications than are\nthe time-varying optimizations of CPUs and GPUs (caches, out-of-order\nexecution, multithreading, multiprocessing, prefetching, ...) that help average\nthroughput more than guaranteed latency. The lack of such features helps\nexplain why, despite having myriad MACs and a big memory, the TPU is relatively\nsmall and low power. We compare the TPU to a server-class Intel Haswell CPU and\nan Nvidia K80 GPU, which are contemporaries deployed in the same datacenters.\nOur workload, written in the high-level TensorFlow framework, uses production\nNN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters'\nNN inference demand. Despite low utilization for some applications, the TPU is\non average about 15X - 30X faster than its contemporary GPU or CPU, with\nTOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the\nTPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and\n200X the CPU.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 12:07:54 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Jouppi", "Norman P.", ""], ["Young", "Cliff", ""], ["Patil", "Nishant", ""], ["Patterson", "David", ""], ["Agrawal", "Gaurav", ""], ["Bajwa", "Raminder", ""], ["Bates", "Sarah", ""], ["Bhatia", "Suresh", ""], ["Boden", "Nan", ""], ["Borchers", "Al", ""], ["Boyle", "Rick", ""], ["Cantin", "Pierre-luc", ""], ["Chao", "Clifford", ""], ["Clark", "Chris", ""], ["Coriell", "Jeremy", ""], ["Daley", "Mike", ""], ["Dau", "Matt", ""], ["Dean", "Jeffrey", ""], ["Gelb", "Ben", ""], ["Ghaemmaghami", "Tara Vazir", ""], ["Gottipati", "Rajendra", ""], ["Gulland", "William", ""], ["Hagmann", "Robert", ""], ["Ho", "C. Richard", ""], ["Hogberg", "Doug", ""], ["Hu", "John", ""], ["Hundt", "Robert", ""], ["Hurt", "Dan", ""], ["Ibarz", "Julian", ""], ["Jaffey", "Aaron", ""], ["Jaworski", "Alek", ""], ["Kaplan", "Alexander", ""], ["Khaitan", "Harshit", ""], ["Koch", "Andy", ""], ["Kumar", "Naveen", ""], ["Lacy", "Steve", ""], ["Laudon", "James", ""], ["Law", "James", ""], ["Le", "Diemthu", ""], ["Leary", "Chris", ""], ["Liu", "Zhuyuan", ""], ["Lucke", "Kyle", ""], ["Lundin", "Alan", ""], ["MacKean", "Gordon", ""], ["Maggiore", "Adriana", ""], ["Mahony", "Maire", ""], ["Miller", "Kieran", ""], ["Nagarajan", "Rahul", ""], ["Narayanaswami", "Ravi", ""], ["Ni", "Ray", ""], ["Nix", "Kathy", ""], ["Norrie", "Thomas", ""], ["Omernick", "Mark", ""], ["Penukonda", "Narayana", ""], ["Phelps", "Andy", ""], ["Ross", "Jonathan", ""], ["Ross", "Matt", ""], ["Salek", "Amir", ""], ["Samadiani", "Emad", ""], ["Severn", "Chris", ""], ["Sizikov", "Gregory", ""], ["Snelham", "Matthew", ""], ["Souter", "Jed", ""], ["Steinberg", "Dan", ""], ["Swing", "Andy", ""], ["Tan", "Mercedes", ""], ["Thorson", "Gregory", ""], ["Tian", "Bo", ""], ["Toma", "Horia", ""], ["Tuttle", "Erick", ""], ["Vasudevan", "Vijay", ""], ["Walter", "Richard", ""], ["Wang", "Walter", ""], ["Wilcox", "Eric", ""], ["Yoon", "Doe Hyun", ""]]}, {"id": "1704.04777", "submitter": "He Jiang", "authors": "He Jiang, Jingyuan Zhang, Jifeng Xuan, Zhilei Ren, Yan Hu", "title": "A Hybrid ACO Algorithm for the Next Release Problem", "comments": "6 pages, 2 figures, Proceedings of 2nd International Conference on\n  Software Engineering and Data Mining (SEDM 2010), 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Hybrid Ant Colony Optimization algorithm (HACO)\nfor Next Release Problem (NRP). NRP, a NP-hard problem in requirement\nengineering, is to balance customer requests, resource constraints, and\nrequirement dependencies by requirement selection. Inspired by the successes of\nAnt Colony Optimization algorithms (ACO) for solving NP-hard problems, we\ndesign our HACO to approximately solve NRP. Similar to traditional ACO\nalgorithms, multiple artificial ants are employed to construct new solutions.\nDuring the solution construction phase, both pheromone trails and neighborhood\ninformation will be taken to determine the choices of every ant. In addition, a\nlocal search (first found hill climbing) is incorporated into HACO to improve\nthe solution quality. Extensively wide experiments on typical NRP test\ninstances show that HACO outperforms the existing algorithms (GRASP and\nsimulated annealing) in terms of both solution uality and running time.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 13:43:51 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Jiang", "He", ""], ["Zhang", "Jingyuan", ""], ["Xuan", "Jifeng", ""], ["Ren", "Zhilei", ""], ["Hu", "Yan", ""]]}, {"id": "1704.04853", "submitter": "Llewyn Salt Mr.", "authors": "Llewyn Salt, David Howard, Giacomo Indiveri, Yulia Sandamirskaya", "title": "Differential Evolution and Bayesian Optimisation for Hyper-Parameter\n  Selection in Mixed-Signal Neuromorphic Circuits Applied to UAV Obstacle\n  Avoidance", "comments": "Submitted to TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2941506", "report-no": null, "categories": "cs.NE cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lobula Giant Movement Detector (LGMD) is a an identified neuron of the\nlocust that detects looming objects and triggers its escape responses.\nUnderstanding the neural principles and networks that lead to these fast and\nrobust responses can lead to the design of efficient facilitate obstacle\navoidance strategies in robotic applications. Here we present a neuromorphic\nspiking neural network model of the LGMD driven by the output of a neuromorphic\nDynamic Vision Sensor (DVS), which has been optimised to produce robust and\nreliable responses in the face of the constraints and variability of its mixed\nsignal analogue-digital circuits. As this LGMD model has many parameters, we\nuse the Differential Evolution (DE) algorithm to optimise its parameter space.\nWe also investigate the use of Self-Adaptive Differential Evolution (SADE)\nwhich has been shown to ameliorate the difficulties of finding appropriate\ninput parameters for DE. We explore the use of two biological mechanisms:\nsynaptic plasticity and membrane adaptivity in the LGMD. We apply DE and SADE\nto find parameters best suited for an obstacle avoidance system on an unmanned\naerial vehicle (UAV), and show how it outperforms state-of-the-art Bayesian\noptimisation used for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 03:14:18 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 14:06:32 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 11:31:55 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Salt", "Llewyn", ""], ["Howard", "David", ""], ["Indiveri", "Giacomo", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1704.04879", "submitter": "Thiradet Jiarasuksakun", "authors": "Tinnaluk Rutjanisarakul, Thiradet Jiarasuksakun", "title": "A Sport Tournament Scheduling by Genetic Algorithm with Swapping Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sport tournament problem is considered the Traveling Tournament Problem\n(TTP). One interesting type is the mirrored Traveling Tournament Problem\n(mTTP). The objective of the problem is to minimize either the total number of\ntraveling or the total distances of traveling or both. This research aims to\nfind an optimized solution of the mirrored Traveling Tournament Problem with\nminimum total number of traveling. The solutions consisting of traveling and\nscheduling tables are solved by using genetic algorithm (GA) with swapping\nmethod. The number of traveling of all teams from obtained solutions are close\nto the lower bound theory of number of traveling. Moreover, this algorithm\ngenerates better solutions than known results for most cases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 06:04:02 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Rutjanisarakul", "Tinnaluk", ""], ["Jiarasuksakun", "Thiradet", ""]]}, {"id": "1704.04960", "submitter": "Zhitao Gong", "authors": "Zhitao Gong, Wenlu Wang, Wei-Shinn Ku", "title": "Adversarial and Clean Data Are Not Twins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial attack has cast a shadow on the massive success of deep neural\nnetworks. Despite being almost visually identical to the clean data, the\nadversarial images can fool deep neural networks into wrong predictions with\nvery high confidence. In this paper, however, we show that we can build a\nsimple binary classifier separating the adversarial apart from the clean data\nwith accuracy over 99%. We also empirically show that the binary classifier is\nrobust to a second-round adversarial attack. In other words, it is difficult to\ndisguise adversarial samples to bypass the binary classifier. Further more, we\nempirically investigate the generalization limitation which lingers on all\ncurrent defensive methods, including the binary classifier approach. And we\nhypothesize that this is the result of intrinsic property of adversarial\ncrafting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:25:17 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Gong", "Zhitao", ""], ["Wang", "Wenlu", ""], ["Ku", "Wei-Shinn", ""]]}, {"id": "1704.04998", "submitter": "Grant Dick", "authors": "Grant Dick", "title": "Interval Arithmetic and Interval-Aware Operators for Genetic Programming", "comments": "Extended version of: Grant Dick. 2017. Revisiting Interval Arithmetic\n  for Regression Problems in Genetic Programming. In Proceedings of the 2017\n  Annual Conference on Genetic and Evolutionary Computation. ACM. To appear 8\n  pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression via genetic programming is a flexible approach to machine\nlearning that does not require up-front specification of model structure.\nHowever, traditional approaches to symbolic regression require the use of\nprotected operators, which can lead to perverse model characteristics and poor\ngeneralisation. In this paper, we revisit interval arithmetic as one possible\nsolution to allow genetic programming to perform regression using unprotected\noperators. Using standard benchmarks, we show that using interval arithmetic\nwithin model evaluation does not prevent invalid solutions from entering the\npopulation, meaning that search performance remains compromised. We extend the\nbasic interval arithmetic concept with `safe' search operators that integrate\ninterval information into their process, thereby greatly reducing the number of\ninvalid solutions produced during search. The resulting algorithms are able to\nmore effectively identify good models that generalise well to unseen data. We\nconclude with an analysis of the sensitivity of interval arithmetic-based\noperators with respect to the accuracy of the supplied input feature intervals.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 15:16:02 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Dick", "Grant", ""]]}, {"id": "1704.05132", "submitter": "Nikolaos Antoniadis", "authors": "Nikolaos Antoniadis, Angelo Sifaleras", "title": "A hybrid CPU-GPU parallelization scheme of variable neighborhood search\n  for inventory optimization problems", "comments": "8 pages, 1 figure", "journal-ref": "Electronic Notes in Discrete Mathematics, Volume 58, April 2017,\n  Pages 47-54, ISSN 1571-0653", "doi": "10.1016/j.endm.2017.03.007", "report-no": null, "categories": "cs.NE cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study various parallelization schemes for the Variable\nNeighborhood Search (VNS) metaheuristic on a CPU-GPU system via OpenMP and\nOpenACC. A hybrid parallel VNS method is applied to recent benchmark problem\ninstances for the multi-product dynamic lot sizing problem with product returns\nand recovery, which appears in reverse logistics and is known to be NP-hard. We\nreport our findings regarding these parallelization approaches and present\npromising computational results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:31:14 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Antoniadis", "Nikolaos", ""], ["Sifaleras", "Angelo", ""]]}, {"id": "1704.05134", "submitter": "Jan \\v{Z}egklitz", "authors": "Jan \\v{Z}egklitz, Petr Po\\v{s}\\'ik", "title": "Learning Linear Feature Space Transformations in Symbolic Regression", "comments": "Changed the word \"affine\" to \"linear\" in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new type of leaf node for use in Symbolic Regression (SR) that\nperforms linear combinations of feature variables (LCF). These nodes can be\nhandled in three different modes -- an unsynchronized mode, where all LCFs are\nfree to change on their own, a synchronized mode, where LCFs are sorted into\ngroups in which they are forced to be identical throughout the whole\nindividual, and a globally synchronized mode, which is similar to the previous\nmode but the grouping is done across the whole population. We also present two\nmethods of evolving the weights of the LCFs -- a purely stochastic way via\nmutation and a gradient-based way based on the backpropagation algorithm known\nfrom neural networks -- and also a combination of both. We experimentally\nevaluate all configurations of LCFs in Multi-Gene Genetic Programming (MGGP),\nwhich was chosen as baseline, on a number of benchmarks. According to the\nresults, we identified two configurations which increase the performance of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:37:58 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 05:43:12 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["\u017degklitz", "Jan", ""], ["Po\u0161\u00edk", "Petr", ""]]}, {"id": "1704.05143", "submitter": "Joost Huizinga", "authors": "Joost Huizinga, Kenneth O. Stanley, and Jeff Clune", "title": "The Emergence of Canalization and Evolvability in an Open-Ended,\n  Interactive Evolutionary System", "comments": "SI can be found at: http://www.evolvingai.org/files/SI_0.zip", "journal-ref": "Artificial life, 24(3), pp.157-181 (2018)", "doi": "10.1162/artl_a_00263", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural evolution has produced a tremendous diversity of functional\norganisms. Many believe an essential component of this process was the\nevolution of evolvability, whereby evolution speeds up its ability to innovate\nby generating a more adaptive pool of offspring. One hypothesized mechanism for\nevolvability is developmental canalization, wherein certain dimensions of\nvariation become more likely to be traversed and others are prevented from\nbeing explored (e.g. offspring tend to have similarly sized legs, and mutations\naffect the length of both legs, not each leg individually). While ubiquitous in\nnature, canalization almost never evolves in computational simulations of\nevolution. Not only does that deprive us of in silico models in which to study\nthe evolution of evolvability, but it also raises the question of which\nconditions give rise to this form of evolvability. Answering this question\nwould shed light on why such evolvability emerged naturally and could\naccelerate engineering efforts to harness evolution to solve important\nengineering challenges. In this paper we reveal a unique system in which\ncanalization did emerge in computational evolution. We document that genomes\nentrench certain dimensions of variation that were frequently explored during\ntheir evolutionary history. The genetic representation of these organisms also\nevolved to be highly modular and hierarchical, and we show that these\norganizational properties correlate with increased fitness. Interestingly, the\ntype of computational evolutionary experiment that produced this evolvability\nwas very different from traditional digital evolution in that there was no\nobjective, suggesting that open-ended, divergent evolutionary processes may be\nnecessary for the evolution of evolvability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 22:50:04 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 18:46:48 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Huizinga", "Joost", ""], ["Stanley", "Kenneth O.", ""], ["Clune", "Jeff", ""]]}, {"id": "1704.05174", "submitter": "Joao Papa", "authors": "Joao Paulo Papa, Gustavo Henrique Rosa, Douglas Rodrigues, Xin-She\n  Yang", "title": "LibOPT: An Open-Source Platform for Fast Prototyping Soft Optimization\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimization techniques play an important role in several scientific and\nreal-world applications, thus becoming of great interest for the community. As\na consequence, a number of open-source libraries are available in the\nliterature, which ends up fostering the research and development of new\ntechniques and applications. In this work, we present a new library for the\nimplementation and fast prototyping of nature-inspired techniques called\nLibOPT. Currently, the library implements 15 techniques and 112 benchmarking\nfunctions, as well as it also supports 11 hypercomplex-based optimization\napproaches, which makes it one of the first of its kind. We showed how one can\neasily use and also implement new techniques in LibOPT under the C paradigm.\nExamples are provided with samples of source-code using benchmarking functions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 02:16:13 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Papa", "Joao Paulo", ""], ["Rosa", "Gustavo Henrique", ""], ["Rodrigues", "Douglas", ""], ["Yang", "Xin-She", ""]]}, {"id": "1704.05255", "submitter": "Miguel Aguilera", "authors": "Miguel Aguilera and Manuel G. Bedia", "title": "Criticality as It Could Be: organizational invariance as self-organized\n  criticality in embodied agents", "comments": null, "journal-ref": null, "doi": "10.7551/ecal_a_009", "report-no": null, "categories": "nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a methodological approach for designing adaptive agents\ndriving themselves near points of criticality. Using a synthetic approach we\nconstruct a conceptual model that, instead of specifying mechanistic\nrequirements to generate criticality, exploits the maintenance of an\norganizational structure capable of reproducing critical behavior. Our approach\nexploits the well-known principle of universality, which classifies critical\nphenomena inside a few universality classes of systems independently of their\nspecific mechanisms or topologies. In particular, we implement an artificial\nembodied agent controlled by a neural network maintaining a correlation\nstructure randomly sampled from a lattice Ising model at a critical point. We\nevaluate the agent in two classical reinforcement learning scenarios: the\nMountain Car benchmark and the Acrobot double pendulum, finding that in both\ncases the neural controller reaches a point of criticality, which coincides\nwith a transition point between two regimes of the agent's behaviour,\nmaximizing the mutual information between neurons and sensorimotor patterns.\nFinally, we discuss the possible applications of this synthetic approach to the\ncomprehension of deeper principles connected to the pervasive presence of\ncriticality in biological and cognitive systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 09:53:25 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 08:31:30 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 10:51:29 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Aguilera", "Miguel", ""], ["Bedia", "Manuel G.", ""]]}, {"id": "1704.05367", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni", "title": "On Improving the Capacity of Solving Large-scale Wireless Network Design\n  Problems by Genetic Algorithms", "comments": "This is the authors' final version of the paper published in Di Chio\n  C. et al. (Eds): EvoApplications 2011, LNCS 6625, pp. 11-20, 2011. The final\n  publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-642-20520-0_2", "journal-ref": "Di Chio C. et al. (Eds), EvoApplications 2011, Springer LNCS vol.\n  6625, pp. 11-20, 2011", "doi": "10.1007/978-3-642-20520-0_2", "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, wireless networks have experienced an impressive growth\nand now play a main role in many telecommunications systems. As a consequence,\nscarce radio resources, such as frequencies, became congested and the need for\neffective and efficient assignment methods arose. In this work, we present a\nGenetic Algorithm for solving large instances of the Power, Frequency and\nModulation Assignment Problem, arising in the design of wireless networks. To\nour best knowledge, this is the first Genetic Algorithm that is proposed for\nsuch problem. Compared to previous works, our approach allows a wider\nexploration of the set of power solutions, while eliminating sources of\nnumerical problems. The performance of the algorithm is assessed by tests over\na set of large realistic instances of a Fixed WiMAX Network.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 16:50:41 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""]]}, {"id": "1704.05396", "submitter": "Fran\\c{c}ois Leduc-Primeau", "authors": "Jean-Charles Vialatte and Fran\\c{c}ois Leduc-Primeau", "title": "A Study of Deep Learning Robustness Against Computation Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many types of integrated circuits, accepting larger failure rates in\ncomputations can be used to improve energy efficiency. We study the performance\nof faulty implementations of certain deep neural networks based on pessimistic\nand optimistic models of the effect of hardware faults. After identifying the\nimpact of hyperparameters such as the number of layers on robustness, we study\nthe ability of the network to compensate for computational failures through an\nincrease of the network size. We show that some networks can achieve equivalent\nperformance under faulty implementations, and quantify the required increase in\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 15:33:10 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Vialatte", "Jean-Charles", ""], ["Leduc-Primeau", "Fran\u00e7ois", ""]]}, {"id": "1704.05420", "submitter": "Cem Subakan", "authors": "Y. Cem Subakan, Paris Smaragdis", "title": "Diagonal RNNs in Symbolic Music Modeling", "comments": "Submitted to Waspaa 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new Recurrent Neural Network (RNN) architecture.\nThe novelty is simple: We use diagonal recurrent matrices instead of full. This\nresults in better test likelihood and faster convergence compared to regular\nfull RNNs in most of our experiments. We show the benefits of using diagonal\nrecurrent matrices with popularly used LSTM and GRU architectures as well as\nwith the vanilla RNN architecture, on four standard symbolic music datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 23:36:18 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Subakan", "Y. Cem", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1704.05554", "submitter": "Elliot Meyerson", "authors": "Elliot Meyerson and Risto Miikkulainen", "title": "Discovering Evolutionary Stepping Stones through Behavior Domination", "comments": "To Appear in Proceedings of the Genetic and Evolutionary Computation\n  Conference (GECCO 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior domination is proposed as a tool for understanding and harnessing\nthe power of evolutionary systems to discover and exploit useful stepping\nstones. Novelty search has shown promise in overcoming deception by collecting\ndiverse stepping stones, and several algorithms have been proposed that combine\nnovelty with a more traditional fitness measure to refocus search and help\nnovelty search scale to more complex domains. However, combinations of novelty\nand fitness do not necessarily preserve the stepping stone discovery that\nnovelty search affords. In several existing methods, competition between\nsolutions can lead to an unintended loss of diversity. Behavior domination\ndefines a class of algorithms that avoid this problem, while inheriting\ntheoretical guarantees from multiobjective optimization. Several existing\nalgorithms are shown to be in this class, and a new algorithm is introduced\nbased on fast non-dominated sorting. Experimental results show that this\nalgorithm outperforms existing approaches in domains that contain useful\nstepping stones, and its advantage is sustained with scale. The conclusion is\nthat behavior domination can help illuminate the complex dynamics of\nbehavior-driven search, and can thus lead to the design of more scalable and\nrobust algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 22:27:44 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1704.05712", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker\n  Fischer", "title": "Universal Adversarial Perturbations Against Semantic Image Segmentation", "comments": "Final version for ICCV including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is remarkably successful on perceptual tasks, it was also\nshown to be vulnerable to adversarial perturbations of the input. These\nperturbations denote noise added to the input that was generated specifically\nto fool the system while being quasi-imperceptible for humans. More severely,\nthere even exist universal perturbations that are input-agnostic but fool the\nnetwork on the majority of inputs. While recent work has focused on image\nclassification, this work proposes attacks against semantic image segmentation:\nwe present an approach for generating (universal) adversarial perturbations\nthat make the network yield a desired target segmentation as output. We show\nempirically that there exist barely perceptible universal noise patterns which\nresult in nearly the same predicted segmentation for arbitrary inputs.\nFurthermore, we also show the existence of universal noise which removes a\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\nsegmentation mostly unchanged otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:48:52 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 08:35:25 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 18:55:54 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Kumar", "Mummadi Chaithanya", ""], ["Brox", "Thomas", ""], ["Fischer", "Volker", ""]]}, {"id": "1704.05724", "submitter": "Simon Wessing", "authors": "Simon Wessing and Mike Preuss", "title": "The True Destination of EGO is Multi-local Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient global optimization is a popular algorithm for the optimization of\nexpensive multimodal black-box functions. One important reason for its\npopularity is its theoretical foundation of global convergence. However, as the\nbudgets in expensive optimization are very small, the asymptotic properties\nonly play a minor role and the algorithm sometimes comes off badly in\nexperimental comparisons. Many alternative variants have therefore been\nproposed over the years. In this work, we show experimentally that the\nalgorithm instead has its strength in a setting where multiple optima are to be\nidentified.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 13:23:32 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Wessing", "Simon", ""], ["Preuss", "Mike", ""]]}, {"id": "1704.05761", "submitter": "Bin Liu", "authors": "Bin Liu, Ke-Jia Chen", "title": "Maximum Likelihood Estimation based on Random Subspace EDA: Application\n  to Extrasolar Planet Detection", "comments": "12 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses maximum likelihood (ML) estimation based model fitting\nin the context of extrasolar planet detection. This problem is featured by the\nfollowing properties: 1) the candidate models under consideration are highly\nnonlinear; 2) the likelihood surface has a huge number of peaks; 3) the\nparameter space ranges in size from a few to dozens of dimensions. These\nproperties make the ML search a very challenging problem, as it lacks any\nanalytical or gradient based searching solution to explore the parameter space.\nA population based searching method, called estimation of distribution\nalgorithm (EDA), is adopted to explore the model parameter space starting from\na batch of random locations. EDA is featured by its ability to reveal and\nutilize problem structures. This property is desirable for characterizing the\ndetections. However, it is well recognized that EDAs can not scale well to\nlarge scale problems, as it consists of iterative random sampling and model\nfitting procedures, which results in the well-known dilemma curse of\ndimensionality. A novel mechanism to perform EDAs in interactive random\nsubspaces spanned by correlated variables is proposed and the hope is to\nalleviate the curse of dimensionality for EDAs by performing the operations of\nsampling and model fitting in lower dimensional subspaces. The effectiveness of\nthe proposed algorithm is verified via both benchmark numerical studies and\nreal data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 05:37:32 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 14:14:00 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Liu", "Bin", ""], ["Chen", "Ke-Jia", ""]]}, {"id": "1704.05907", "submitter": "Hongyu Guo", "authors": "Hongyu Guo and Colin Cherry and Jiang Su", "title": "End-to-End Multi-View Networks for Text Classification", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-view network for text classification. Our method\nautomatically creates various views of its input text, each taking the form of\nsoft attention weights that distribute the classifier's focus among a set of\nbase features. For a bag-of-words representation, each view focuses on a\ndifferent subset of the text's words. Aggregating many such views results in a\nmore discriminative and robust representation. Through a novel architecture\nthat both stacks and concatenates views, we produce a network that emphasizes\nboth depth and width, allowing training to converge quickly. Using our\nmulti-view architecture, we establish new state-of-the-art accuracies on two\nbenchmark tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 19:33:38 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Guo", "Hongyu", ""], ["Cherry", "Colin", ""], ["Su", "Jiang", ""]]}, {"id": "1704.06016", "submitter": "Erkan Bostanci", "authors": "Hamide Ozlem Dalgic and Erkan Bostanci and Mehmet Serdar Guzel", "title": "Genetic Algorithm Based Floor Planning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms are widely used in many different optimization problems\nincluding layout design. The layout of the shelves play an important role in\nthe total sales metrics for superstores since this affects the customers'\nshopping behaviour. This paper employed a genetic algorithm based approach to\ndesign shelf layout of superstores. The layout design problem was tackled by\nusing a novel chromosome representation which takes many different parameters\nto prevent dead-ends and improve shelf visibility into consideration. Results\nshow that the approach can produce reasonably good layout designs in very short\namounts of time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 05:09:39 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Dalgic", "Hamide Ozlem", ""], ["Bostanci", "Erkan", ""], ["Guzel", "Mehmet Serdar", ""]]}, {"id": "1704.06191", "submitter": "Min Lin", "authors": "Min Lin", "title": "Softmax GAN", "comments": "NIPS 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The\nkey idea of Softmax GAN is to replace the classification loss in the original\nGAN with a softmax cross-entropy loss in the sample space of one single batch.\nIn the adversarial learning of $N$ real training samples and $M$ generated\nsamples, the target of discriminator training is to distribute all the\nprobability mass to the real samples, each with probability $\\frac{1}{M}$, and\ndistribute zero probability to generated data. In the generator training phase,\nthe target is to assign equal probability to all data points in the batch, each\nwith probability $\\frac{1}{M+N}$. While the original GAN is closely related to\nNoise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance\nSampling version of GAN. We futher demonstrate with experiments that this\nsimple change stabilizes GAN training.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:35:14 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 15:09:00 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Lin", "Min", ""]]}, {"id": "1704.06194", "submitter": "Mo Yu", "authors": "Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos, Bing Xiang,\n  Bowen Zhou", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "comments": "Accepted by ACL 2017 (updated for camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation detection is a core component for many NLP applications including\nKnowledge Base Question Answering (KBQA). In this paper, we propose a\nhierarchical recurrent neural network enhanced by residual learning that\ndetects KB relations given an input question. Our method uses deep residual\nbidirectional LSTMs to compare questions and relation names via different\nhierarchies of abstraction. Additionally, we propose a simple KBQA system that\nintegrates entity linking and our proposed relation detector to enable one\nenhance another. Experimental results evidence that our approach achieves not\nonly outstanding relation detection performance, but more importantly, it helps\nour KBQA system to achieve state-of-the-art accuracy for both single-relation\n(SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:48:05 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 17:45:35 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yu", "Mo", ""], ["Yin", "Wenpeng", ""], ["Hasan", "Kazi Saidul", ""], ["Santos", "Cicero dos", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1704.06258", "submitter": "Mustapha Oudani", "authors": "Abdelhamid Benaini, Achraf Berrajaa, Jaouad Boukachour, Mustapha\n  Oudani", "title": "Solving the Uncapacitated Single Allocation p-Hub Median Problem on GPU", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-95104-1_2", "report-no": null, "categories": "cs.DM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel genetic algorithm (GA) implemented on GPU clusters is proposed to\nsolve the Uncapacitated Single Allocation p-Hub Median problem. The GA uses\nbinary and integer encoding and genetic operators adapted to this problem. Our\nGA is improved by generated initial solution with hubs located at middle nodes.\nThe obtained experimental results are compared with the best known solutions on\nall benchmarks on instances up to 1000 nodes. Furthermore, we solve our own\nrandomly generated instances up to 6000 nodes. Our approach outperforms most\nwell-known heuristics in terms of solution quality and time execution and it\nallows hitherto unsolved problems to be solved.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 07:57:16 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Benaini", "Abdelhamid", ""], ["Berrajaa", "Achraf", ""], ["Boukachour", "Jaouad", ""], ["Oudani", "Mustapha", ""]]}, {"id": "1704.06415", "submitter": "Sebastien Wong", "authors": "Sebastien C. Wong, Victor Stamatescu, Adam Gatt, David Kearney, Ivan\n  Lee, and Mark D. McDonnell", "title": "Track Everything: Limiting Prior Knowledge in Online Multi-Object\n  Recognition", "comments": "15 pages", "journal-ref": null, "doi": "10.1109/TIP.2017.2696744", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of online tracking and classification of\nmultiple objects in an image sequence. Our proposed solution is to first track\nall objects in the scene without relying on object-specific prior knowledge,\nwhich in other systems can take the form of hand-crafted features or user-based\ntrack initialization. We then classify the tracked objects with a fast-learning\nimage classifier that is based on a shallow convolutional neural network\narchitecture and demonstrate that object recognition improves when this is\ncombined with object state information from the tracking algorithm. We argue\nthat by transferring the use of prior knowledge from the detection and tracking\nstages to the classification stage we can design a robust, general purpose\nobject recognition system with the ability to detect and track a variety of\nobject types. We describe our biologically inspired implementation, which\nadaptively learns the shape and motion of tracked objects, and apply it to the\nNeovision2 Tower benchmark data set, which contains multiple object types. An\nexperimental evaluation demonstrates that our approach is competitive with\nstate-of-the-art video object recognition systems that do make use of\nobject-specific prior knowledge in detection and tracking, while providing\nadditional practical advantages by virtue of its generality.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 06:49:51 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Wong", "Sebastien C.", ""], ["Stamatescu", "Victor", ""], ["Gatt", "Adam", ""], ["Kearney", "David", ""], ["Lee", "Ivan", ""], ["McDonnell", "Mark D.", ""]]}, {"id": "1704.06567", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Libovick\\'y and Jind\\v{r}ich Helcl", "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "comments": "7 pages; Accepted to ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling attention in neural multi-source sequence-to-sequence learning\nremains a relatively unexplored area, despite its usefulness in tasks that\nincorporate multiple source languages or modalities. We propose two novel\napproaches to combine the outputs of attention mechanisms over each source\nsequence, flat and hierarchical. We compare the proposed methods with existing\ntechniques and present results of systematic evaluation of those methods on the\nWMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the\nproposed methods achieve competitive results on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 14:39:27 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Helcl", "Jind\u0159ich", ""]]}, {"id": "1704.06593", "submitter": "Jacek Bialowas", "authors": "Jacek Bialowas, Beata Grzyb, Pawel Poszumski", "title": "Firing Cell: An Artificial Neuron with a Simulation of\n  Long-Term-Potentiation-Related Memory", "comments": "4 pages, 3 figures", "journal-ref": "ISAROB 2006, pp.731-734", "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational model of neuron, called firing cell (FC),\nproperties of which cover such phenomena as attenuation of receptors for\nexternal stimuli, delay and decay of postsynaptic potentials, modification of\ninternal weights due to propagation of postsynaptic potentials through the\ndendrite, modification of properties of the analog memory for each input due to\na pattern of short-time synaptic potentiation or long-time synaptic\npotentiation (LTP), output-spike generation when the sum of all inputs exceeds\na threshold, and refraction. The cell may take one of the three forms:\nexcitatory, inhibitory, and receptory. The computer simulations showed that,\ndepending on the phase of input signals, the artificial neuron's output\nfrequency may demonstrate various chaotic behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 15:31:33 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Bialowas", "Jacek", ""], ["Grzyb", "Beata", ""], ["Poszumski", "Pawel", ""]]}, {"id": "1704.06611", "submitter": "Jonathon Cai", "authors": "Jonathon Cai, Richard Shin, Dawn Song", "title": "Making Neural Programming Architectures Generalize via Recursion", "comments": "Published in ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirically, neural networks that attempt to learn programs from data have\nexhibited poor generalizability. Moreover, it has traditionally been difficult\nto reason about the behavior of these models beyond a certain level of input\ncomplexity. In order to address these issues, we propose augmenting neural\narchitectures with a key abstraction: recursion. As an application, we\nimplement recursion in the Neural Programmer-Interpreter framework on four\ntasks: grade-school addition, bubble sort, topological sort, and quicksort. We\ndemonstrate superior generalizability and interpretability with small amounts\nof training data. Recursion divides the problem into smaller pieces and\ndrastically reduces the domain of each neural network component, making it\ntractable to prove guarantees about the overall system's behavior. Our\nexperience suggests that in order for neural architectures to robustly learn\nprogram semantics, it is necessary to incorporate a concept like recursion.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:02:26 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Cai", "Jonathon", ""], ["Shin", "Richard", ""], ["Song", "Dawn", ""]]}, {"id": "1704.06645", "submitter": "Dylan Muir", "authors": "Dylan Richard Muir", "title": "Feed-forward approximations to dynamic recurrent network architectures", "comments": "Author's final version, accepted for publication in Neural\n  Computation", "journal-ref": null, "doi": "10.1162/neco_a_01042", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network architectures can have useful computational\nproperties, with complex temporal dynamics and input-sensitive attractor\nstates. However, evaluation of recurrent dynamic architectures requires\nsolution of systems of differential equations, and the number of evaluations\nrequired to determine their response to a given input can vary with the input,\nor can be indeterminate altogether in the case of oscillations or instability.\nIn feed-forward networks, by contrast, only a single pass through the network\nis needed to determine the response to a given input. Modern machine-learning\nsystems are designed to operate efficiently on feed-forward architectures. We\nhypothesised that two-layer feedforward architectures with simple,\ndeterministic dynamics could approximate the responses of single-layer\nrecurrent network architectures. By identifying the fixed-point responses of a\ngiven recurrent network, we trained two-layer networks to directly approximate\nthe fixed-point response to a given input. These feed-forward networks then\nembodied useful computations, including competitive interactions, information\ntransformations and noise rejection. Our approach was able to find useful\napproximations to recurrent networks, which can then be evaluated in linear and\ndeterministic time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 17:28:47 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 09:53:08 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Muir", "Dylan Richard", ""]]}, {"id": "1704.06684", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni", "title": "A hybrid exact-ACO algorithm for the joint scheduling, power and cluster\n  assignment in cooperative wireless networks", "comments": "This is the author's final version of the paper published in G. Di\n  Caro, G. Theraulaz (eds.), BIONETICS 2012: Bio-Inspired Models of Network,\n  Information, and Computing Systems. LNICST, vol. 134, pp. 3-17. Springer,\n  Heidelberg, 2014, DOI: 10.1007/978-3-319-06944-9_1 ). The final publication\n  is available at Springer via http://dx.doi.org/10.1007/978-3-319-06944-9_1", "journal-ref": "G. Di Caro, G. Theraulaz (eds.), BIONETICS 2012: Bio-Inspired\n  Models of Network, Information, and Computing Systems. LNICST, vol. 134, pp.\n  3-17, Springer, Heidelberg, 2014, DOI: 10.1007/978-3-319-06944-9_1", "doi": "10.1007/978-3-319-06944-9_1", "report-no": null, "categories": "math.OC cs.DS cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Base station cooperation (BSC) has recently arisen as a promising way to\nincrease the capacity of a wireless network. Implementing BSC adds a new design\ndimension to the classical wireless network design problem: how to define the\nsubset of base stations (clusters) that coordinate to serve a user. Though the\nproblem of forming clusters has been extensively discussed from a technical\npoint of view, there is still a lack of effective optimization models for its\nrepresentation and algorithms for its solution. In this work, we make a further\nstep towards filling such gap: 1) we generalize the classical network design\nproblem by adding cooperation as an additional decision dimension; 2) we\ndevelop a strong formulation for the resulting problem; 3) we define a new\nhybrid solution algorithm that combines exact large neighborhood search and ant\ncolony optimization. Finally, we assess the performance of our new model and\nalgorithm on a set of realistic instances of a WiMAX network.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:14:05 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""]]}, {"id": "1704.06713", "submitter": "Lotfi Hajjem Mr", "authors": "Lotfi Hajjem, Salah Benabdallah, Fouad Ben Abdelaziz", "title": "A dynamic resource allocation decision model for IT security", "comments": "6 pages, 2010 Second International Conference on Engineering System\n  Management and Applications", "journal-ref": null, "doi": null, "report-no": "INSPEC Accession Number: 11466975", "categories": "cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, with the continued growth in using information and communication\ntechnologies (ICT) for business purposes, business organizations become\nincreasingly dependent on their information systems. Thus, they need to protect\nthem from the different attacks exploiting their vulnerabilities. To do so, the\norganization has to use security technologies, which may be proactive or\nreactive ones. Each security technology has a relative cost and addresses\nspecific vulnerabilities. Therefore, the organization has to put in place the\nappropriate security technologies set that minimizes the information system s\nvulnerabilities with a minimal cost. This bi objective problem will be\nconsidered as a resources allocation problem (RAP) where security technologies\nrepresent the resources to be allocated. However, the set of vulnerabilities\nmay change, periodically, with the continual appearance of new ones. Therefore,\nthe security technologies set should be flexible to face these changes, in real\ntime, and the problem becomes a dynamic one. In this paper, we propose a\nharmony search based algorithm to solve the bi objective dynamic resource\nallocation decision model. This approach was compared to a genetic algorithm\nand provided good results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 21:48:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hajjem", "Lotfi", ""], ["Benabdallah", "Salah", ""], ["Abdelaziz", "Fouad Ben", ""]]}, {"id": "1704.06847", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj", "title": "A hybrid primal heuristic for Robust Multiperiod Network Design", "comments": "This is the authors' final version of the paper published in:\n  Esparcia-Alc\\'azar A., Mora A. (eds), EvoApplications 2014: Applications of\n  Evolutionary Computation, LNCS 8602, pp. 15-26, 2014. DOI:\n  10.1007/978-3-662-45523-4\\_2. The final publication is available at Springer\n  via http://dx.doi.org/10.1007/978-3-662-45523-4_2. arXiv admin note:\n  substantial text overlap with arXiv:1410.5850", "journal-ref": "Esparcia-Alc\\'azar A., Mora A. (Eds), EvoApplications 2014:\n  Applications of Evolutionary Computation, Springer LNCS vol. 8602, pp. 15-26,\n  2014", "doi": "10.1007/978-3-662-45523-4_2", "report-no": null, "categories": "math.OC cs.DS cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Robust Multiperiod Network Design Problem, a\ngeneralization of the classical Capacitated Network Design Problem that\nadditionally considers multiple design periods and provides solutions protected\nagainst traffic uncertainty. Given the intrinsic difficulty of the problem,\nwhich proves challenging even for state-of-the art commercial solvers, we\npropose a hybrid primal heuristic based on the combination of ant colony\noptimization and an exact large neighborhood search. Computational experiments\non a set of realistic instances from the SNDlib show that our heuristic can\nfind solutions of extremely good quality with low optimality gap.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 20:44:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Krolikowski", "Jonatan", ""], ["Pulaj", "Jonad", ""]]}, {"id": "1704.06942", "submitter": "Diego Perez Liebana Dr.", "authors": "Rauca D. Gaina and Simon M. Lucas and Diego Perez-Liebana", "title": "Population Seeding Techniques for Rolling Horizon Evolution in General\n  Video Game Playing", "comments": "Proceedings of the IEEE Conference on Evolutionary Computation 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Monte Carlo Tree Search and closely related methods have dominated\nGeneral Video Game Playing, recent research has demonstrated the promise of\nRolling Horizon Evolutionary Algorithms as an interesting alternative. However,\nthere is little attention paid to population initialization techniques in the\nsetting of general real-time video games. Therefore, this paper proposes the\nuse of population seeding to improve the performance of Rolling Horizon\nEvolution and presents the results of two methods, One Step Look Ahead and\nMonte Carlo Tree Search, tested on 20 games of the General Video Game AI corpus\nwith multiple evolution parameter values (population size and individual\nlength). An in-depth analysis is carried out between the results of the seeding\nmethods and the vanilla Rolling Horizon Evolution. In addition, the paper\npresents a comparison to a Monte Carlo Tree Search algorithm. The results are\npromising, with seeding able to boost performance significantly over baseline\nevolution and even match the high level of play obtained by the Monte Carlo\nTree Search.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 15:53:29 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Gaina", "Rauca D.", ""], ["Lucas", "Simon M.", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "1704.06960", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Anca Dragan, Dan Klein", "title": "Translating Neuralese", "comments": "Fixes typos and cleans ups some model presentation details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches have recently been proposed for learning decentralized\ndeep multiagent policies that coordinate via a differentiable communication\nchannel. While these policies are effective for many tasks, interpretation of\ntheir induced communication strategies has remained a challenge. Here we\npropose to interpret agents' messages by translating them. Unlike in typical\nmachine translation problems, we have no parallel data to learn from. Instead\nwe develop a translation model based on the insight that agent messages and\nnatural language strings mean the same thing if they induce the same belief\nabout the world in a listener. We present theoretical guarantees and empirical\nevidence that our approach preserves both the semantics and pragmatics of\nmessages by ensuring that players communicating through a translation layer do\nnot suffer a substantial loss in reward relative to players with a common\nlanguage.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 18:46:42 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 21:28:48 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 15:10:24 GMT"}, {"version": "v4", "created": "Sat, 6 Jan 2018 15:29:58 GMT"}, {"version": "v5", "created": "Sat, 22 Dec 2018 19:13:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Andreas", "Jacob", ""], ["Dragan", "Anca", ""], ["Klein", "Dan", ""]]}, {"id": "1704.06970", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick", "title": "Differentiable Scheduled Sampling for Credit Assignment", "comments": "Accepted at ACL2017 (http://bit.ly/2oj1muX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a continuous relaxation of the argmax operation can be\nused to create a differentiable approximation to greedy decoding for\nsequence-to-sequence (seq2seq) models. By incorporating this approximation into\nthe scheduled sampling training procedure (Bengio et al., 2015)--a well-known\ntechnique for correcting exposure bias--we introduce a new training objective\nthat is continuous and differentiable everywhere and that can provide\ninformative gradients near points where previous decoding decisions change\ntheir value. In addition, by using a related approximation, we demonstrate a\nsimilar approach to sampled-based training. Finally, we show that our approach\noutperforms cross-entropy training and scheduled sampling procedures in two\nsequence prediction tasks: named entity recognition and machine translation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:05:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1704.07055", "submitter": "Rupayan Chakraborty", "authors": "Sri Harsha Dumpala, Rupayan Chakraborty, Sunil Kumar Kopparapu", "title": "k-FFNN: A priori knowledge infused Feed-forward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN) are being extensively used over feed-forward\nneural networks (FFNN) because of their inherent capability to capture temporal\nrelationships that exist in the sequential data such as speech. This aspect of\nRNN is advantageous especially when there is no a priori knowledge about the\ntemporal correlations within the data. However, RNNs require large amount of\ndata to learn these temporal correlations, limiting their advantage in low\nresource scenarios. It is not immediately clear (a) how a priori temporal\nknowledge can be used in a FFNN architecture (b) how a FFNN performs when\nprovided with this knowledge about temporal correlations (assuming available)\nduring training. The objective of this paper is to explore k-FFNN, namely a\nFFNN architecture that can incorporate the a priori knowledge of the temporal\nrelationships within the data sequence during training and compare k-FFNN\nperformance with RNN in a low resource scenario. We evaluate the performance of\nk-FFNN and RNN by extensive experimentation on MediaEval 2016 audio data\n(\"Emotional Impact of Movies\" task). Experimental results show that the\nperformance of k-FFNN is comparable to RNN, and in some scenarios k-FFNN\nperforms better than RNN when temporal knowledge is injected into FFNN\narchitecture. The main contributions of this paper are (a) fusing a priori\nknowledge into FFNN architecture to construct a k-FFNN and (b) analyzing the\nperformance of k-FFNN with respect to RNN for different size of training data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 06:54:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Dumpala", "Sri Harsha", ""], ["Chakraborty", "Rupayan", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1704.07156", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Semi-supervised Multitask Learning for Sequence Labeling", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequence labeling framework with a secondary training objective,\nlearning to predict surrounding words for every word in the dataset. This\nlanguage modeling objective incentivises the system to learn general-purpose\npatterns of semantic and syntactic composition, which are also useful for\nimproving accuracy on different sequence labeling tasks. The architecture was\nevaluated on a range of datasets, covering the tasks of error detection in\nlearner texts, named entity recognition, chunking and POS-tagging. The novel\nlanguage modeling objective provided consistent performance improvements on\nevery benchmark, without requiring any additional annotated or unannotated\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:47:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1704.07187", "submitter": "Irina Petrova", "authors": "Irina Petrova, Arina Buzdalova", "title": "Reinforcement Learning Based Dynamic Selection of Auxiliary Objectives\n  with Preserving of the Best Found Solution", "comments": "this is a full version of a paper which has been accepted as a\n  student workshop paper to GECCO conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency of single-objective optimization can be improved by introducing\nsome auxiliary objectives. Ideally, auxiliary objectives should be helpful.\nHowever, in practice, objectives may be efficient on some optimization stages\nbut obstructive on others. In this paper we propose a modification of the EA+RL\nmethod which dynamically selects optimized objectives using reinforcement\nlearning. The proposed modification prevents from losing the best found\nsolution. We analysed the proposed modification and compared it with the EA+RL\nmethod and Random Local Search on XdivK, Generalized OneMax and LeadingOnes\nproblems. The proposed modification outperforms the EA+RL method on all problem\ninstances. It also outperforms the single objective approach on the most\nproblem instances. We also provide detailed analysis of how different\ncomponents of the considered algorithms influence efficiency of optimization.\nIn addition, we present theoretical analysis of the proposed modification on\nthe XdivK problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 12:52:51 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Petrova", "Irina", ""], ["Buzdalova", "Arina", ""]]}, {"id": "1704.07207", "submitter": "Jinbo Xu", "authors": "Zhen Li, Sheng Wang, Yizhou Yu and Jinbo Xu", "title": "Predicting membrane protein contacts from non-membrane proteins by deep\n  transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational prediction of membrane protein (MP) structures is very\nchallenging partially due to lack of sufficient solved structures for homology\nmodeling. Recently direct evolutionary coupling analysis (DCA) sheds some light\non protein contact prediction and accordingly, contact-assisted folding, but\nDCA is effective only on some very large-sized families since it uses\ninformation only in a single protein family. This paper presents a deep\ntransfer learning method that can significantly improve MP contact prediction\nby learning contact patterns and complex sequence-contact relationship from\nthousands of non-membrane proteins (non-MPs). Tested on 510 non-redundant MPs,\nour deep model (learned from only non-MPs) has top L/10 long-range contact\nprediction accuracy 0.69, better than our deep model trained by only MPs (0.63)\nand much better than a representative DCA method CCMpred (0.47) and the CASP11\nwinner MetaPSICOV (0.55). The accuracy of our deep model can be further\nimproved to 0.72 when trained by a mix of non-MPs and MPs. When only contacts\nin transmembrane regions are evaluated, our method has top L/10 long-range\naccuracy 0.62, 0.57, and 0.53 when trained by a mix of non-MPs and MPs, by\nnon-MPs only, and by MPs only, respectively, still much better than MetaPSICOV\n(0.45) and CCMpred (0.40). All these results suggest that sequence-structure\nrelationship learned by our deep model from non-MPs generalizes well to MP\ncontact prediction. Improved contact prediction also leads to better\ncontact-assisted folding. Using only top predicted contacts as restraints, our\ndeep learning method can fold 160 and 200 of 510 MPs with TMscore>0.6 when\ntrained by non-MPs only and by a mix of non-MPs and MPs, respectively, while\nCCMpred and MetaPSICOV can do so for only 56 and 77 MPs, respectively. Our\ncontact-assisted folding also greatly outperforms homology modeling.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:27:22 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Li", "Zhen", ""], ["Wang", "Sheng", ""], ["Yu", "Yizhou", ""], ["Xu", "Jinbo", ""]]}, {"id": "1704.07313", "submitter": "Chen Chen", "authors": "Chen Chen, Changtong Luo, Zonglin Jiang", "title": "Elite Bases Regression: A Real-time Algorithm for Symbolic Regression", "comments": "The 2017 13th International Conference on Natural Computation, Fuzzy\n  Systems and Knowledge Discovery (ICNC-FSKD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression is an important but challenging research topic in data\nmining. It can detect the underlying mathematical models. Genetic programming\n(GP) is one of the most popular methods for symbolic regression. However, its\nconvergence speed might be too slow for large scale problems with a large\nnumber of variables. This drawback has become a bottleneck in practical\napplications. In this paper, a new non-evolutionary real-time algorithm for\nsymbolic regression, Elite Bases Regression (EBR), is proposed. EBR generates a\nset of candidate basis functions coded with parse-matrix in specific mapping\nrules. Meanwhile, a certain number of elite bases are preserved and updated\niteratively according to the correlation coefficients with respect to the\ntarget model. The regression model is then spanned by the elite bases. A\ncomparative study between EBR and a recent proposed machine learning method for\nsymbolic regression, Fast Function eXtraction (FFX), are conducted. Numerical\nresults indicate that EBR can solve symbolic regression problems more\neffectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 16:21:10 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 14:26:06 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Chen", "Chen", ""], ["Luo", "Changtong", ""], ["Jiang", "Zonglin", ""]]}, {"id": "1704.07622", "submitter": "Oswald Berthold", "authors": "Oswald Berthold and Verena Hafner", "title": "Tapping the sensorimotor trajectory", "comments": "Paper accepted for the ICDL-Epirob 2017 conference. Proceedings\n  pending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the concept of sensorimotor tappings, a new\ngraphical technique that explicitly represents relations between the time steps\nof an agent's sensorimotor loop and a single training step of an adaptive\ninternal model. In the simplest case this is a relation linking two time steps.\nIn realistic cases these relations can extend over several time steps and over\ndifferent sensory channels. The aim is to capture the footprint of information\nintake relative to the agent's current time step. We argue that this view\nallows us to make prior considerations explicit and then use them in\nimplementations without modification once they are established.\n  Here we explain the basic idea, provide example tappings for standard\nconfigurations used in developmental models, and show how tappings can be\napplied to problems in related fields.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:42:19 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 11:19:29 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Berthold", "Oswald", ""], ["Hafner", "Verena", ""]]}, {"id": "1704.07734", "submitter": "Xiaodong Gu", "authors": "Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim", "title": "DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning", "comments": "Accepted at IJCAI 2017 (The 26th International Joint Conference on\n  Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer programs written in one language are often required to be ported to\nother languages to support multiple devices and environments. When programs use\nlanguage specific APIs (Application Programming Interfaces), it is very\nchallenging to migrate these APIs to the corresponding APIs written in other\nlanguages. Existing approaches mine API mappings from projects that have\ncorresponding versions in two languages. They rely on the sparse availability\nof bilingual projects, thus producing a limited number of API mappings. In this\npaper, we propose an intelligent system called DeepAM for automatically mining\nAPI mappings from a large-scale code corpus without bilingual projects. The key\ncomponent of DeepAM is based on the multimodal sequence to sequence learning\narchitecture that aims to learn joint semantic representations of bilingual API\nsequences from big source code data. Experimental results indicate that DeepAM\nsignificantly increases the accuracy of API mappings as well as the number of\nAPI mappings, when compared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:09:51 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Gu", "Xiaodong", ""], ["Zhang", "Hongyu", ""], ["Zhang", "Dongmei", ""], ["Kim", "Sunghun", ""]]}, {"id": "1704.07816", "submitter": "Long Jin", "authors": "Long Jin, Justin Lazarow, Zhuowen Tu", "title": "Introspective Classification with Convolutional Nets", "comments": "12 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose introspective convolutional networks (ICN) that emphasize the\nimportance of having convolutional neural networks empowered with generative\ncapabilities. We employ a reclassification-by-synthesis algorithm to perform\ntraining using a formulation stemmed from the Bayes theory. Our ICN tries to\niteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by\nimproving the classification. The single CNN classifier learned is at the same\ntime generative --- being able to directly synthesize new samples within its\nown discriminative model. We conduct experiments on benchmark datasets\nincluding MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures,\nand observe improved classification results.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:49:03 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 05:09:48 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Jin", "Long", ""], ["Lazarow", "Justin", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1704.07820", "submitter": "Long Jin", "authors": "Justin Lazarow, Long Jin, Zhuowen Tu", "title": "Introspective Generative Modeling: Decide Discriminatively", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised learning by developing introspective generative\nmodeling (IGM) that attains a generator using progressively learned deep\nconvolutional neural networks. The generator is itself a discriminator, capable\nof introspection: being able to self-evaluate the difference between its\ngenerated samples and the given training data. When followed by repeated\ndiscriminative learning, desirable properties of modern discriminative\nclassifiers are directly inherited by the generator. IGM learns a cascade of\nCNN classifiers using a synthesis-by-classification algorithm. In the\nexperiments, we observe encouraging results on a number of applications\nincluding texture modeling, artistic style transferring, face modeling, and\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:57:33 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lazarow", "Justin", ""], ["Jin", "Long", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1704.07911", "submitter": "Urs Muller", "authors": "Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof\n  Choromanski, Bernhard Firner, Lawrence Jackel, Urs Muller", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning\n  Steers a Car", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of a complete software stack for autonomous driving, NVIDIA has\ncreated a neural-network-based system, known as PilotNet, which outputs\nsteering angles given images of the road ahead. PilotNet is trained using road\nimages paired with the steering angles generated by a human driving a\ndata-collection car. It derives the necessary domain knowledge by observing\nhuman drivers. This eliminates the need for human engineers to anticipate what\nis important in an image and foresee all the necessary rules for safe driving.\nRoad tests demonstrated that PilotNet can successfully perform lane keeping in\na wide variety of driving conditions, regardless of whether lane markings are\npresent or not.\n  The goal of the work described here is to explain what PilotNet learns and\nhow it makes its decisions. To this end we developed a method for determining\nwhich elements in the road image most influence PilotNet's steering decision.\nResults show that PilotNet indeed learns to recognize relevant objects on the\nroad.\n  In addition to learning the obvious features such as lane markings, edges of\nroads, and other cars, PilotNet learns more subtle features that would be hard\nto anticipate and program by engineers, for example, bushes lining the edge of\nthe road and atypical vehicle classes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 21:25:41 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Yeres", "Philip", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""], ["Firner", "Bernhard", ""], ["Jackel", "Lawrence", ""], ["Muller", "Urs", ""]]}, {"id": "1704.08045", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen and Matthias Hein", "title": "The loss surface of deep and wide neural networks", "comments": "ICML 2017. Main results now hold for larger classes of loss functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the optimization problem behind deep neural networks is highly\nnon-convex, it is frequently observed in practice that training deep networks\nseems possible without getting stuck in suboptimal points. It has been argued\nthat this is the case as all local minima are close to being globally optimal.\nWe show that this is (almost) true, in fact almost all local minima are\nglobally optimal, for a fully connected network with squared loss and analytic\nactivation function given that the number of hidden units of one layer of the\nnetwork is larger than the number of training points and the network structure\nfrom this layer on is pyramidal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:24:54 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 19:43:39 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1704.08092", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Niko Schenk, Christian Chiarcos", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese\n  Implicit Discourse Relations", "comments": "To appear at ACL2017, code available at\n  https://github.com/sronnqvist/discourse-ablstm", "journal-ref": null, "doi": "10.18653/v1/P17-2040", "report-no": "Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (ACL'17)", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an attention-based Bi-LSTM for Chinese implicit discourse\nrelations and demonstrate that modeling argument pairs as a joint sequence can\noutperform word order-agnostic approaches. Our model benefits from a partial\nsampling scheme and is conceptually simple, yet achieves state-of-the-art\nperformance on the Chinese Discourse Treebank. We also visualize its attention\nactivity to illustrate the model's ability to selectively focus on the relevant\nparts of an input sequence.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:10:12 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Schenk", "Niko", ""], ["Chiarcos", "Christian", ""]]}, {"id": "1704.08306", "submitter": "Michael Smith", "authors": "Michael R. Smith, Aaron J. Hill, Kristofor D. Carlson, Craig M.\n  Vineyard, Jonathon Donaldson, David R. Follett, Pamela L. Follett, John H.\n  Naegle, Conrad D. James, James B. Aimone", "title": "A Digital Neuromorphic Architecture Efficiently Facilitating Complex\n  Synaptic Response Functions Applied to Liquid State Machines", "comments": "8 pages, 4 Figures, Preprint of 2017 IJCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information in neural networks is represented as weighted connections, or\nsynapses, between neurons. This poses a problem as the primary computational\nbottleneck for neural networks is the vector-matrix multiply when inputs are\nmultiplied by the neural network weights. Conventional processing architectures\nare not well suited for simulating neural networks, often requiring large\namounts of energy and time. Additionally, synapses in biological neural\nnetworks are not binary connections, but exhibit a nonlinear response function\nas neurotransmitters are emitted and diffuse between neurons. Inspired by\nneuroscience principles, we present a digital neuromorphic architecture, the\nSpiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex\nsynaptic response functions without requiring additional hardware components.\nWe consider the paradigm of spiking neurons with temporally coded information\nas opposed to non-spiking rate coded neurons used in most neural networks. In\nthis paradigm we examine liquid state machines applied to speech recognition\nand show how a liquid state machine with temporal dynamics maps onto the\nSTPU-demonstrating the flexibility and efficiency of the STPU for instantiating\nneural algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:12:31 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Smith", "Michael R.", ""], ["Hill", "Aaron J.", ""], ["Carlson", "Kristofor D.", ""], ["Vineyard", "Craig M.", ""], ["Donaldson", "Jonathon", ""], ["Follett", "David R.", ""], ["Follett", "Pamela L.", ""], ["Naegle", "John H.", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "1704.08362", "submitter": "Ge Wang", "authors": "Fenglei Fan, Wenxiang Cong, Ge Wang", "title": "A New Type of Neurons for Machine Learning", "comments": "5 pages, 8 figures, 11 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the use of an artificial neural network is the\nmainstream approach. Such a network consists of layers of neurons. These\nneurons are of the same type characterized by the two features: (1) an inner\nproduct of an input vector and a matching weighting vector of trainable\nparameters and (2) a nonlinear excitation function. Here we investigate the\npossibility of replacing the inner product with a quadratic function of the\ninput vector, thereby upgrading the 1st order neuron to the 2nd order neuron,\nempowering individual neurons, and facilitating the optimization of neural\nnetworks. Also, numerical examples are provided to illustrate the feasibility\nand merits of the 2nd order neurons. Finally, further topics are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 22:02:25 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Fan", "Fenglei", ""], ["Cong", "Wenxiang", ""], ["Wang", "Ge", ""]]}, {"id": "1704.08774", "submitter": "Thomas Gabor", "authors": "Thomas Gabor and Lenz Belzner", "title": "Genealogical Distance as a Diversity Estimate in Evolutionary Algorithms", "comments": "Measuring and Promoting Diversity in Evolutionary Algorithms @ GECCO\n  2017", "journal-ref": null, "doi": "10.1145/3067695.3082529", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolutionary edit distance between two individuals in a population, i.e.,\nthe amount of applications of any genetic operator it would take the\nevolutionary process to generate one individual starting from the other, seems\nlike a promising estimate for the diversity between said individuals. We\nintroduce genealogical diversity, i.e., estimating two individuals' degree of\nrelatedness by analyzing large, unused parts of their genome, as a\ncomputationally efficient method to approximate that measure for diversity.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 23:38:36 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Gabor", "Thomas", ""], ["Belzner", "Lenz", ""]]}, {"id": "1704.08818", "submitter": "Yong Xia", "authors": "Benteng Ma, Yong Xia", "title": "A Tribe Competition-Based Genetic Algorithm for Feature Selection in\n  Pattern Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2017.04.042", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection has always been a critical step in pattern recognition, in\nwhich evolutionary algorithms, such as the genetic algorithm (GA), are most\ncommonly used. However, the individual encoding scheme used in various GAs\nwould either pose a bias on the solution or require a pre-specified number of\nfeatures, and hence may lead to less accurate results. In this paper, a tribe\ncompetition-based genetic algorithm (TCbGA) is proposed for feature selection\nin pattern classification. The population of individuals is divided into\nmultiple tribes, and the initialization and evolutionary operations are\nmodified to ensure that the number of selected features in each tribe follows a\nGaussian distribution. Thus each tribe focuses on exploring a specific part of\nthe solution space. Meanwhile, tribe competition is introduced to the evolution\nprocess, which allows the winning tribes, which produce better individuals, to\nenlarge their sizes, i.e. having more individuals to search their parts of the\nsolution space. This algorithm, therefore, avoids the bias on solutions and\nrequirement of a pre-specified number of features. We have evaluated our\nalgorithm against several state-of-the-art feature selection approaches on 20\nbenchmark datasets. Our results suggest that the proposed TCbGA algorithm can\nidentify the optimal feature subset more effectively and produce more accurate\npattern classification.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 06:25:50 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Ma", "Benteng", ""], ["Xia", "Yong", ""]]}]