[{"id": "0904.1888", "submitter": "Stevan Harnad", "authors": "Stevan Harnad", "title": "On Fodor on Darwin on Evolution", "comments": "21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jerry Fodor argues that Darwin was wrong about \"natural selection\" because\n(1) it is only a tautology rather than a scientific law that can support\ncounterfactuals (\"If X had happened, Y would have happened\") and because (2)\nonly minds can select. Hence Darwin's analogy with \"artificial selection\" by\nanimal breeders was misleading and evolutionary explanation is nothing but\npost-hoc historical narrative. I argue that Darwin was right on all counts.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2009 00:59:10 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Harnad", "Stevan", ""]]}, {"id": "0904.3063", "submitter": "Juan J. Merelo Pr.", "authors": "C. M. Fernandes, J.J. Merelo and A.C. Rosa", "title": "Using Dissortative Mating Genetic Algorithms to Track the Extrema of\n  Dynamic Deceptive Functions", "comments": "Technical report complementing Carlos Fernandes' PhD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Traditional Genetic Algorithms (GAs) mating schemes select individuals for\ncrossover independently of their genotypic or phenotypic similarities. In\nNature, this behaviour is known as random mating. However, non-random schemes -\nin which individuals mate according to their kinship or likeness - are more\ncommon in natural systems. Previous studies indicate that, when applied to GAs,\nnegative assortative mating (a specific type of non-random mating, also known\nas dissortative mating) may improve their performance (on both speed and\nreliability) in a wide range of problems. Dissortative mating maintains the\ngenetic diversity at a higher level during the run, and that fact is frequently\nobserved as an explanation for dissortative GAs ability to escape local optima\ntraps. Dynamic problems, due to their specificities, demand special care when\ntuning a GA, because diversity plays an even more crucial role than it does\nwhen tackling static ones. This paper investigates the behaviour of\ndissortative mating GAs, namely the recently proposed Adaptive Dissortative\nMating GA (ADMGA), on dynamic trap functions. ADMGA selects parents according\nto their Hamming distance, via a self-adjustable threshold value. The method,\nby keeping population diversity during the run, provides an effective means to\ndeal with dynamic problems. Tests conducted with deceptive and nearly deceptive\ntrap functions indicate that ADMGA is able to outperform other GAs, some\nspecifically designed for tracking moving extrema, on a wide range of tests,\nbeing particularly effective when speed of change is not very fast. When\ncomparing the algorithm to a previously proposed dissortative GA, results show\nthat performance is equivalent on the majority of the experiments, but ADMGA\nperforms better when solving the hardest instances of the test set.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2009 15:57:20 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Fernandes", "C. M.", ""], ["Merelo", "J. J.", ""], ["Rosa", "A. C.", ""]]}, {"id": "0904.3650", "submitter": "Florentina Pintea", "authors": "Dan L. Lacrama, Ioan Snep", "title": "The use of invariant moments in hand-written character recognition", "comments": "12 pages,exposed on 1st \"European Conference on Computer Sciences &\n  Applications\" - XA2006, Timisoara, Romania", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 91-102", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to present the implementation of a Radial Basis\nFunction neural network with built-in knowledge to recognize hand-written\ncharacters. The neural network includes in its architecture gates controlled by\nan attraction/repulsion system of coefficients. These coefficients are derived\nfrom a preprocessing stage which groups the characters according to their\nascendant, central, or descendent components. The neural network is trained\nusing data from invariant moment functions. Results are compared with those\nobtained using a K nearest neighbor method on the same moment data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2009 10:44:21 GMT"}], "update_date": "2009-04-24", "authors_parsed": [["Lacrama", "Dan L.", ""], ["Snep", "Ioan", ""]]}, {"id": "0904.4587", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno and Mirta B. Gordon", "title": "Adaptive Learning with Binary Neurons", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A efficient incremental learning algorithm for classification tasks, called\nNetLines, well adapted for both binary and real-valued input patterns is\npresented. It generates small compact feedforward neural networks with one\nhidden layer of binary units and binary output units. A convergence theorem\nensures that solutions with a finite number of hidden units exist for both\nbinary and real-valued input patterns. An implementation for problems with more\nthan two classes, valid for any binary classifier, is proposed. The\ngeneralization error and the size of the resulting networks are compared to the\nbest published results on well-known classification benchmarks. Early stopping\nis shown to decrease overfitting, without improving the generalization\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2009 11:49:45 GMT"}], "update_date": "2009-04-30", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Gordon", "Mirta B.", ""]]}]