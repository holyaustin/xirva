[{"id": "1404.0554", "submitter": "Dr. Paul J. Werbos", "authors": "Paul J Werbos", "title": "From ADP to the Brain: Foundations, Roadmap, Challenges and Research\n  Priorities", "comments": "5p, to appear in Proc. of the Interantional Joint Conference on\n  Neural Networks 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper defines and discusses Mouse Level Computational Intelligence\n(MLCI) as a grand challenge for the coming century. It provides a specific\nroadmap to reach that target, citing relevant work and review papers and\ndiscussing the relation to funding priorities in two NSF funding activities:\nthe ongoing Energy, Power and Adaptive Systems program (EPAS) and the recent\ninitiative in Cognitive Optimization and Prediction (COPN). It elaborates on\nthe first step, vector intelligence, a challenge in the development of\nuniversal learning systems, which itself will require considerable new research\nto attain. This in turn is a crucial prerequisite to true functional\nunderstanding of how mammal brains achieve such general learning capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 13:46:23 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Werbos", "Paul J", ""]]}, {"id": "1404.0695", "submitter": "Xin-She Yang", "authors": "Xin-She Yang, M. Karamanoglu, Xingshi He", "title": "Multi-objective Flower Algorithm for Optimization", "comments": "2 figures. arXiv admin note: substantial text overlap with\n  arXiv:1312.5673", "journal-ref": "X. S. Yang, M. Karamanoglu, X. S. He, Multi-objective Flower\n  Algorithm for Optimization, Procedia Computer Science, vol. 18, pp. 861-868\n  (2013)", "doi": "10.1016/j.procs.2013.05.251", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flower pollination algorithm is a new nature-inspired algorithm, based on the\ncharacteristics of flowering plants. In this paper, we extend this flower\nalgorithm to solve multi-objective optimization problems in engineering. By\nusing the weighted sum method with random weights, we show that the proposed\nmulti-objective flower algorithm can accurately find the Pareto fronts for a\nset of test functions. We then solve a bi-objective disc brake design problem,\nwhich indeed converges quickly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 20:28:51 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Yang", "Xin-She", ""], ["Karamanoglu", "M.", ""], ["He", "Xingshi", ""]]}, {"id": "1404.0708", "submitter": "Xin-She Yang", "authors": "Xin-She Yang, Slawomir Koziel, Leifur Leifsson", "title": "Computational Optimization, Modelling and Simulation: Recent Trends and\n  Challenges", "comments": null, "journal-ref": "X. S. Yang, S. Koziel, L. Leifsson, Computational Optimization,\n  Modelling and Simulation: Recent Trends and Challenges, Procedia Computer\n  Science, vol. 18, pp. 855-860 (2013)", "doi": "10.1016/j.procs.2013.05.250", "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling, simulation and optimization form an integrated part of modern\ndesign practice in engineering and industry. Tremendous progress has been\nobserved for all three components over the last few decades. However, many\nchallenging issues remain unresolved, and the current trends tend to use\nnature-inspired algorithms and surrogate-based techniques for modelling and\noptimization. This 4th workshop on Computational Optimization, Modelling and\nSimulation (COMS 2013) at ICCS 2013 will further summarize the latest\ndevelopments of optimization and modelling and their applications in science,\nengineering and industry. In this review paper, we will analyse the recent\ntrends in modelling and optimization, and their associated challenges. We will\ndiscuss important topics for further research, including parameter-tuning,\nlarge-scale problems, and the gaps between theory and applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 21:07:51 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Yang", "Xin-She", ""], ["Koziel", "Slawomir", ""], ["Leifsson", "Leifur", ""]]}, {"id": "1404.0868", "submitter": "Jun He", "authors": "Jun He, Feidun He and Hongbin Dong", "title": "A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 0-1 knapsack problem is a well-known combinatorial optimisation problem.\nApproximation algorithms have been designed for solving it and they return\nprovably good solutions within polynomial time. On the other hand, genetic\nalgorithms are well suited for solving the knapsack problem and they find\nreasonably good solutions quickly. A naturally arising question is whether\ngenetic algorithms are able to find solutions as good as approximation\nalgorithms do. This paper presents a novel multi-objective optimisation genetic\nalgorithm for solving the 0-1 knapsack problem. Experiment results show that\nthe new algorithm outperforms its rivals, the greedy algorithm, mixed strategy\ngenetic algorithm, and greedy algorithm + mixed strategy genetic algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 11:46:42 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["He", "Jun", ""], ["He", "Feidun", ""], ["Dong", "Hongbin", ""]]}, {"id": "1404.1292", "submitter": "Omaima Al-Allaf Nazar", "authors": "Omaima N. A. AL-Allaf", "title": "Review of Face Detection Systems Based Artificial Neural Networks\n  Algorithms", "comments": "16 pages, 12 figures, 1 table, IJMA Journal", "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.6, No.1, February 2014", "doi": "10.5121/ijma.2013.6101", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is one of the most relevant applications of image processing\nand biometric systems. Artificial neural networks (ANN) have been used in the\nfield of image processing and pattern recognition. There is lack of literature\nsurveys which give overview about the studies and researches related to the\nusing of ANN in face detection. Therefore, this research includes a general\nreview of face detection studies and systems which based on different ANN\napproaches and algorithms. The strengths and limitations of these literature\nstudies and systems were included also.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 19:47:58 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["AL-Allaf", "Omaima N. A.", ""]]}, {"id": "1404.1559", "submitter": "Jai  Priyankka", "authors": "R. Vidya, Dr.G.M.Nasira, R. P. Jaia Priyankka", "title": "Sparse Coding: A Deep Learning using Unlabeled Data for High - Level\n  Representation", "comments": "4 Pages, 3 Figures, 2014 World Congress on Computing and\n  Communication Technologies (WCCCT)", "journal-ref": "Vidya R, Dr. Naisra G.M, Priyankka R.P. Jaia, \"Sparse Coding: A\n  Deep Learning using Unlabeled Data for High - Level Representation\" IEEE\n  Xplore 2014", "doi": "10.1109/WCCCT.2014.69", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sparse coding algorithm is an learning algorithm mainly for unsupervised\nfeature for finding succinct, a little above high - level Representation of\ninputs, and it has successfully given a way for Deep learning. Our objective is\nto use High - Level Representation data in form of unlabeled category to help\nunsupervised learning task. when compared with labeled data, unlabeled data is\neasier to acquire because, unlike labeled data it does not follow some\nparticular class labels. This really makes the Deep learning wider and\napplicable to practical problems and learning. The main problem with sparse\ncoding is it uses Quadratic loss function and Gaussian noise mode. So, its\nperforms is very poor when binary or integer value or other Non- Gaussian type\ndata is applied. Thus first we propose an algorithm for solving the L1 -\nregularized convex optimization algorithm for the problem to allow High - Level\nRepresentation of unlabeled data. Through this we derive a optimal solution for\ndescribing an approach to Deep learning algorithm by using sparse code.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 09:50:45 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Vidya", "R.", ""], ["Nasira", "Dr. G. M.", ""], ["Priyankka", "R. P. Jaia", ""]]}, {"id": "1404.1614", "submitter": "Alexander Churchill", "authors": "Alexander W. Churchill and Siddharth Sigtia and Chrisantha Fernando", "title": "A Denoising Autoencoder that Guides Stochastic Search", "comments": "Submitted to Parallel Problem Solving from Nature 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is described that adaptively learns a non-linear mutation\ndistribution. It works by training a denoising autoencoder (DA) online at each\ngeneration of a genetic algorithm to reconstruct a slowly decaying memory of\nthe best genotypes so far. A compressed hidden layer forces the autoencoder to\nlearn hidden features in the training set that can be used to accelerate search\non novel problems with similar structure. Its output neurons define a\nprobability distribution that we sample from to produce offspring solutions.\nThe algorithm outperforms a canonical genetic algorithm on several\ncombinatorial optimisation problems, e.g. multidimensional 0/1 knapsack\nproblem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastrigin\nand Rosenbrock functions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 20:10:37 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Churchill", "Alexander W.", ""], ["Sigtia", "Siddharth", ""], ["Fernando", "Chrisantha", ""]]}, {"id": "1404.1999", "submitter": "Jonathon Shlens", "authors": "Jonathon Shlens", "title": "Notes on Generalized Linear Models of Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental neuroscience increasingly requires tractable models for\nanalyzing and predicting the behavior of neurons and networks. The generalized\nlinear model (GLM) is an increasingly popular statistical framework for\nanalyzing neural data that is flexible, exhibits rich dynamic behavior and is\ncomputationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo et\nal., 2005). What follows is a brief summary of the primary equations governing\nthe application of GLM's to spike trains with a few sentences linking this work\nto the larger statistical literature. Latter sections include extensions of a\nbasic GLM to model spatio-temporal receptive fields as well as network activity\nin an arbitrary numbers of neurons.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 03:41:50 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Shlens", "Jonathon", ""]]}, {"id": "1404.2903", "submitter": "Marius Leordeanu", "authors": "Marius Leordeanu and Rahul Sukthankar", "title": "Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep\n  Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general multi-class visual recognition model, termed the\nClassifier Graph, which aims to generalize and integrate ideas from many of\ntoday's successful hierarchical recognition approaches. Our graph-based model\nhas the advantage of enabling rich interactions between classes from different\nlevels of interpretation and abstraction. The proposed multi-class system is\nefficiently learned using step by step updates. The structure consists of\nsimple logistic linear layers with inputs from features that are automatically\nselected from a large pool. Each newly learned classifier becomes a potential\nnew feature. Thus, our feature pool can consist both of initial manually\ndesigned features as well as learned classifiers from previous steps (graph\nnodes), each copied many times at different scales and locations. In this\nmanner we can learn and grow both a deep, complex graph of classifiers and a\nrich pool of features at different levels of abstraction and interpretation.\nOur proposed graph of classifiers becomes a multi-class system with a recursive\nstructure, suitable for deep detection and recognition of several classes\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 11:38:35 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Leordeanu", "Marius", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1404.3023", "submitter": "Alexandre Chotard", "authors": "Alexandre Chotard (INRIA Saclay - Ile de France, LRI), Anne Auger\n  (INRIA Saclay - Ile de France), Nikolaus Hansen (INRIA Saclay - Ile de\n  France)", "title": "Markov Chain Analysis of Evolution Strategies on a Linear Constraint\n  Optimization Problem", "comments": "Amir Hussain; Zhigang Zeng; Nian Zhang. IEEE Congress on Evolutionary\n  Computation, Jul 2014, Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyses a $(1,\\lambda)$-Evolution Strategy, a randomised\ncomparison-based adaptive search algorithm, on a simple constraint optimisation\nproblem. The algorithm uses resampling to handle the constraint and optimizes a\nlinear function with a linear constraint. Two cases are investigated: first the\ncase where the step-size is constant, and second the case where the step-size\nis adapted using path length control. We exhibit for each case a Markov chain\nwhose stability analysis would allow us to deduce the divergence of the\nalgorithm depending on its internal parameters. We show divergence at a\nconstant rate when the step-size is constant. We sketch that with step-size\nadaptation geometric divergence takes place. Our results complement previous\nstudies where stability was assumed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 07:38:37 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 08:40:55 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Chotard", "Alexandre", "", "INRIA Saclay - Ile de France, LRI"], ["Auger", "Anne", "", "INRIA Saclay - Ile de France"], ["Hansen", "Nikolaus", "", "INRIA Saclay - Ile de\n  France"]]}, {"id": "1404.3520", "submitter": "Jun He", "authors": "Jun He, Boris Mitavskiy and Yuren Zhou", "title": "A Theoretical Assessment of Solution Quality in Evolutionary Algorithms\n  for the Knapsack Problem", "comments": null, "journal-ref": null, "doi": "10.1109/CEC.2014.6900442", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms are well suited for solving the knapsack problem.\nSome empirical studies claim that evolutionary algorithms can produce good\nsolutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigations\naddress the quality of solutions that evolutionary algorithms may produce for\nthe knapsack problem. The current paper focuses on a theoretical investigation\nof three types of (N+1) evolutionary algorithms that exploit bitwise mutation,\ntruncation selection, plus different repair methods for the 0-1 knapsack\nproblem. It assesses the solution quality in terms of the approximation ratio.\nOur work indicates that the solution produced by pure strategy and mixed\nstrategy evolutionary algorithms is arbitrarily bad. Nevertheless, the\nevolutionary algorithm using helper objectives may produce 1/2-approximation\nsolutions to the 0-1 knapsack problem.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 10:00:48 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["He", "Jun", ""], ["Mitavskiy", "Boris", ""], ["Zhou", "Yuren", ""]]}, {"id": "1404.3606", "submitter": "Tsung-Han  Chan", "authors": "Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng and Yi Ma", "title": "PCANet: A Simple Deep Learning Baseline for Image Classification?", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2475625", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a very simple deep learning network for image\nclassification which comprises only the very basic data processing components:\ncascaded principal component analysis (PCA), binary hashing, and block-wise\nhistograms. In the proposed architecture, PCA is employed to learn multistage\nfilter banks. It is followed by simple binary hashing and block histograms for\nindexing and pooling. This architecture is thus named as a PCA network (PCANet)\nand can be designed and learned extremely easily and efficiently. For\ncomparison and better understanding, we also introduce and study two simple\nvariations to the PCANet, namely the RandNet and LDANet. They share the same\ntopology of PCANet but their cascaded filters are either selected randomly or\nlearned from LDA. We have tested these basic networks extensively on many\nbenchmark visual datasets for different tasks, such as LFW for face\nverification, MultiPIE, Extended Yale B, AR, FERET datasets for face\nrecognition, as well as MNIST for hand-written digits recognition.\nSurprisingly, for all tasks, such a seemingly naive PCANet model is on par with\nthe state of the art features, either prefixed, highly hand-crafted or\ncarefully learned (by DNNs). Even more surprisingly, it sets new records for\nmany classification tasks in Extended Yale B, AR, FERET datasets, and MNIST\nvariations. Additional experiments on other public datasets also demonstrate\nthe potential of the PCANet serving as a simple but highly competitive baseline\nfor texture classification and object recognition.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 15:02:17 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 15:20:44 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chan", "Tsung-Han", ""], ["Jia", "Kui", ""], ["Gao", "Shenghua", ""], ["Lu", "Jiwen", ""], ["Zeng", "Zinan", ""], ["Ma", "Yi", ""]]}, {"id": "1404.4067", "submitter": "Tamal Ghosh", "authors": "Tamal Ghosh, Tanmoy Chakraborty and Pranab K Dan", "title": "An effective AHP-based metaheuristic approach to solve supplier\n  selection problem", "comments": null, "journal-ref": "International Journal of Procurement Management, Vol. 5, No. 2,\n  2012", "doi": "10.1504/IJPM.2012.045647", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The supplier selection problem is based on electing the best supplier from a\ngroup of pre-specified candidates, is identified as a Multi Criteria Decision\nMaking (MCDM), is proportionately significant in terms of qualitative and\nquantitative attributes. It is a fundamental issue to achieve a trade-off\nbetween such quantifiable and unquantifiable attributes with an aim to\naccomplish the best solution to the abovementioned problem. This article\nportrays a metaheuristic based optimization model to solve this NP-Complete\nproblem. Initially the Analytic Hierarchy Process (AHP) is implemented to\ngenerate an initial feasible solution of the problem. Thereafter a Simulated\nAnnealing (SA) algorithm is exploited to improve the quality of the obtained\nsolution. The Taguchi robust design method is exploited to solve the critical\nissues on the subject of the parameter selection of the SA technique. In order\nto verify the proposed methodology the numerical results are demonstrated based\non tangible industry data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 20:21:31 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Ghosh", "Tamal", ""], ["Chakraborty", "Tanmoy", ""], ["Dan", "Pranab K", ""]]}, {"id": "1404.4797", "submitter": "Christian Schulz", "authors": "Henning Meyerhenke, Peter Sanders, Christian Schulz", "title": "Parallel Graph Partitioning for Complex Networks", "comments": "Review article. Parallelization of our previous approach\n  arXiv:1402.3281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NE cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large complex networks like social networks or web graphs has\nrecently attracted considerable interest. In order to do this in parallel, we\nneed to partition them into pieces of about equal size. Unfortunately, previous\nparallel graph partitioners originally developed for more regular mesh-like\nnetworks do not work well for these networks. This paper addresses this problem\nby parallelizing and adapting the label propagation technique originally\ndeveloped for graph clustering. By introducing size constraints, label\npropagation becomes applicable for both the coarsening and the refinement phase\nof multilevel graph partitioning. We obtain very high quality by applying a\nhighly parallel evolutionary algorithm to the coarsened graph. The resulting\nsystem is both more scalable and achieves higher quality than state-of-the-art\nsystems like ParMetis or PT-Scotch. For large complex networks the performance\ndifferences are very big. For example, our algorithm can partition a web graph\nwith 3.3 billion edges in less than sixteen seconds using 512 cores of a high\nperformance cluster while producing a high quality partition -- none of the\ncompeting systems can handle this graph on our system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 14:30:04 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 13:40:34 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 10:07:38 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Meyerhenke", "Henning", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1404.5144", "submitter": "Sacha Gomez", "authors": "M. Konomi and G. M. Sacha", "title": "Influence of the learning method in the performance of feedforward\n  neural networks when the activity of neurons is modified", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method that allows us to give a different treatment to any neuron inside\nfeedforward neural networks is presented. The algorithm has been implemented\nwith two very different learning methods: a standard Back-propagation (BP)\nprocedure and an evolutionary algorithm. First, we have demonstrated that the\nEA training method converges faster and gives more accurate results than BP.\nThen we have made a full analysis of the effects of turning off different\ncombinations of neurons after the training phase. We demonstrate that EA is\nmuch more robust than BP for all the cases under study. Even in the case when\ntwo hidden neurons are lost, EA training is still able to give good average\nresults. This difference implies that we must be very careful when pruning or\nredundancy effects are being studied since the network performance when losing\nneurons strongly depends on the training method. Moreover, the influence of the\nindividual inputs will also depend on the training algorithm. Since EA keeps a\ngood classification performance when units are lost, this method could be a\ngood way to simulate biological learning systems since they must be robust\nagainst deficient neuron performance. Although biological systems are much more\ncomplex than the simulations shown in this article, we propose that a smart\ntraining strategy such as the one shown here could be considered as a first\nprotection against the losing of a certain number of neurons.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 09:00:19 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Konomi", "M.", ""], ["Sacha", "G. M.", ""]]}, {"id": "1404.5417", "submitter": "Claudius Gros", "authors": "Claudius Gros, Mathias Linkerhand, Valentin Walther", "title": "Attractor Metadynamics in Adapting Neural Networks", "comments": null, "journal-ref": "Artificial Neural Networks and Machine Learning-ICANN 2014 , S.\n  Wermter et al. (Eds), pp. 65-72. Springer (2014)", "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow adaption processes, like synaptic and intrinsic plasticity, abound in\nthe brain and shape the landscape for the neural dynamics occurring on\nsubstantially faster timescales. At any given time the network is characterized\nby a set of internal parameters, which are adapting continuously, albeit\nslowly. This set of parameters defines the number and the location of the\nrespective adiabatic attractors. The slow evolution of network parameters hence\ninduces an evolving attractor landscape, a process which we term attractor\nmetadynamics. We study the nature of the metadynamics of the attractor\nlandscape for several continuous-time autonomous model networks. We find both\nfirst- and second-order changes in the location of adiabatic attractors and\nargue that the study of the continuously evolving attractor landscape\nconstitutes a powerful tool for understanding the overall development of the\nneural dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 08:25:23 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Gros", "Claudius", ""], ["Linkerhand", "Mathias", ""], ["Walther", "Valentin", ""]]}, {"id": "1404.5520", "submitter": "Loshchilov Ilya", "authors": "Ilya Loshchilov (LIS)", "title": "A Computationally Efficient Limited Memory CMA-ES for Large Scale\n  Optimization", "comments": "Genetic and Evolutionary Computation Conference (GECCO'2014) (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient limited memory Covariance Matrix\nAdaptation Evolution Strategy for large scale optimization, which we call the\nLM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for\nnumerical optimization of non-linear, non-convex optimization problems in\ncontinuous domain. Inspired by the limited memory BFGS method of Liu and\nNocedal (1989), the LM-CMA-ES samples candidate solutions according to a\ncovariance matrix reproduced from $m$ direction vectors selected during the\noptimization process. The decomposition of the covariance matrix into Cholesky\nfactors allows to reduce the time and memory complexity of the sampling to\n$O(mn)$, where $n$ is the number of decision variables. When $n$ is large\n(e.g., $n$ > 1000), even relatively small values of $m$ (e.g., $m=20,30$) are\nsufficient to efficiently solve fully non-separable problems and to reduce the\noverall run-time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 06:10:51 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Loshchilov", "Ilya", "", "LIS"]]}, {"id": "1404.5767", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates", "comments": null, "journal-ref": "In: Proc. IEEE Congress on Evolutionary Computation, IEEE CEC\n  2014, (Ed.: C. A. Coello, Coello), IEEE Press, Piscataway, NJ, 2014,\n  2692-2699", "doi": "10.1109/CEC.2014.6900272", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coevolutionary minimal substrates are simple and abstract models that allow\nstudying the relationships and codynamics between objective and subjective\nfitness. Using these models an approach is presented for defining and analyzing\nfitness landscapes of coevolutionary problems. We devise similarity measures of\ncodynamic fitness landscapes and experimentally study minimal substrates of\ntest--based and compositional problems for both cooperative and competitive\ninteraction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 09:53:47 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "1404.5772", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian,\n  Bin Wang and Tie-Yan Liu", "title": "Sequential Click Prediction for Sponsored Search with Recurrent Neural\n  Networks", "comments": "Accepted by AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click prediction is one of the fundamental problems in sponsored search. Most\nof existing studies took advantage of machine learning approaches to predict ad\nclick for each event of ad view independently. However, as observed in the\nreal-world sponsored search system, user's behaviors on ads yield high\ndependency on how the user behaved along with the past time, especially in\nterms of what queries she submitted, what ads she clicked or ignored, and how\nlong she spent on the landing pages of clicked ads, etc. Inspired by these\nobservations, we introduce a novel framework based on Recurrent Neural Networks\n(RNN). Compared to traditional methods, this framework directly models the\ndependency on user's sequential behaviors into the click prediction process\nthrough the recurrent structure in RNN. Large scale evaluations on the\nclick-through logs from a commercial search engine demonstrate that our\napproach can significantly improve the click prediction accuracy, compared to\nsequence-independent approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 10:14:41 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 05:56:18 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 13:59:03 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Zhang", "Yuyu", ""], ["Dai", "Hanjun", ""], ["Xu", "Chang", ""], ["Feng", "Jun", ""], ["Wang", "Taifeng", ""], ["Bian", "Jiang", ""], ["Wang", "Bin", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1404.5997", "submitter": "Alex Krizhevsky", "authors": "Alex Krizhevsky", "title": "One weird trick for parallelizing convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a new way to parallelize the training of convolutional neural\nnetworks across multiple GPUs. The method scales significantly better than all\nalternatives when applied to modern convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 22:37:56 GMT"}, {"version": "v2", "created": "Sat, 26 Apr 2014 23:10:51 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Krizhevsky", "Alex", ""]]}, {"id": "1404.6334", "submitter": "N. Michael Mayer", "authors": "Norbert Michael Mayer", "title": "Input anticipating critical reservoirs show power law forgetting of\n  unexpected input events", "comments": null, "journal-ref": "Neural Computation May 2015, Vol. 27, No. 5: 1102-1119", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, reservoir computing shows an exponential memory decay. This paper\ninvestigates under which circumstances echo state networks can show a power law\nforgetting. That means traces of earlier events can be found in the reservoir\nfor very long time spans. Such a setting requires critical connectivity exactly\nat the limit of what is permissible according the echo state condition.\nHowever, for general matrices the limit cannot be determined exactly from\ntheory. In addition, the behavior of the network is strongly influenced by the\ninput flow. Results are presented that use certain types of restricted\nrecurrent connectivity and anticipation learning with regard to the input,\nwhere indeed power law forgetting can be achieved.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 06:36:28 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 04:28:54 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 06:16:44 GMT"}, {"version": "v4", "created": "Tue, 25 Nov 2014 10:52:02 GMT"}, {"version": "v5", "created": "Sat, 8 Aug 2015 09:56:09 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Mayer", "Norbert Michael", ""]]}, {"id": "1404.6955", "submitter": "Kenric Nelson", "authors": "Kenric P. Nelson, Madalina Barbu, Brian J. Scannell", "title": "Probabilistic graphs using coupled random variables", "comments": "Submitted for presentation at the Machine Intelligence and\n  Bio-inspired Computation: Theory and Applications Conference, SPIE Sensing\n  Technology and Applications, Baltimore, MD, May 8, 2014", "journal-ref": null, "doi": "10.1117/12.2050759", "report-no": null, "categories": "cs.LG cs.IT cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network design has utilized flexible nonlinear processes which can\nmimic biological systems, but has suffered from a lack of traceability in the\nresulting network. Graphical probabilistic models ground network design in\nprobabilistic reasoning, but the restrictions reduce the expressive capability\nof each node making network designs complex. The ability to model coupled\nrandom variables using the calculus of nonextensive statistical mechanics\nprovides a neural node design incorporating nonlinear coupling between input\nstates while maintaining the rigor of probabilistic reasoning. A generalization\nof Bayes rule using the coupled product enables a single node to model\ncorrelation between hundreds of random variables. A coupled Markov random field\nis designed for the inferencing and classification of UCI's MLR 'Multiple\nFeatures Data Set' such that thousands of linear correlation parameters can be\nreplaced with a single coupling parameter with just a (3%, 4%) percent\nreduction in (classification, inference) performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 19:25:48 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Nelson", "Kenric P.", ""], ["Barbu", "Madalina", ""], ["Scannell", "Brian J.", ""]]}, {"id": "1404.7514", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Bernhard Vogginger, Paul M\\\"uller, Oliver\n  Breitwieser, Mikael Lundqvist, Lyle Muller, Matthias Ehrlich, Alain Destexhe,\n  Anders Lansner, Ren\\'e Sch\\\"uffny, Johannes Schemmel, Karlheinz Meier", "title": "Characterization and Compensation of Network-Level Anomalies in\n  Mixed-Signal Neuromorphic Modeling Platforms", "comments": null, "journal-ref": "PLOS ONE, October 10th 2014", "doi": "10.1371/journal.pone.0108590", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancing the size and complexity of neural network models leads to an ever\nincreasing demand for computational resources for their simulation.\nNeuromorphic devices offer a number of advantages over conventional computing\narchitectures, such as high emulation speed or low power consumption, but this\nusually comes at the price of reduced configurability and precision. In this\narticle, we investigate the consequences of several such factors that are\ncommon to neuromorphic devices, more specifically limited hardware resources,\nlimited parameter configurability and parameter variations. Our final aim is to\nprovide an array of methods for coping with such inevitable distortion\nmechanisms. As a platform for testing our proposed strategies, we use an\nexecutable system specification (ESS) of the BrainScaleS neuromorphic system,\nwhich has been designed as a universal emulation back-end for neuroscientific\nmodeling. We address the most essential limitations of this device in detail\nand study their effects on three prototypical benchmark network models within a\nwell-defined, systematic workflow. For each network model, we start by defining\nquantifiable functionality measures by which we then assess the effects of\ntypical hardware-specific distortion mechanisms, both in idealized software\nsimulations and on the ESS. For those effects that cause unacceptable\ndeviations from the original network dynamics, we suggest generic compensation\nmechanisms and demonstrate their effectiveness. Both the suggested workflow and\nthe investigated compensation mechanisms are largely back-end independent and\ndo not require additional hardware configurability beyond the one required to\nemulate the benchmark networks in the first place. We hereby provide a generic\nmethodological environment for configurable neuromorphic devices that are\ntargeted at emulating large-scale, functional neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 20:20:13 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 14:05:53 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Vogginger", "Bernhard", ""], ["M\u00fcller", "Paul", ""], ["Breitwieser", "Oliver", ""], ["Lundqvist", "Mikael", ""], ["Muller", "Lyle", ""], ["Ehrlich", "Matthias", ""], ["Destexhe", "Alain", ""], ["Lansner", "Anders", ""], ["Sch\u00fcffny", "Ren\u00e9", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1404.7765", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Ramon Lopez de Mantaras, Santiago Ontanon", "title": "A semantic network-based evolutionary algorithm for computational\n  creativity", "comments": "20 pages, 14 figures, revision after reviews, changed title", "journal-ref": "Evolutionary Intelligence, 8(1):3-21 (2015)", "doi": "10.1007/s12065-014-0119-1", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel evolutionary algorithm (EA) with a semantic\nnetwork-based representation. For enabling this, we establish new formulations\nof EA variation operators, crossover and mutation, that we adapt to work on\nsemantic networks. The algorithm employs commonsense reasoning to ensure all\noperations preserve the meaningfulness of the networks, using ConceptNet and\nWordNet knowledge bases. The algorithm can be interpreted as a novel memetic\nalgorithm (MA), given that (1) individuals represent pieces of information that\nundergo evolution, as in the original sense of memetics as it was introduced by\nDawkins; and (2) this is different from existing MA, where the word \"memetic\"\nhas been used as a synonym for local refinement after global optimization. For\nevaluating the approach, we introduce an analogical similarity-based fitness\nmeasure that is computed through structure mapping. This setup enables the\nopen-ended generation of networks analogous to a given base network.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 15:45:01 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 22:23:25 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["de Mantaras", "Ramon Lopez", ""], ["Ontanon", "Santiago", ""]]}, {"id": "1404.7828", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "Deep Learning in Neural Networks: An Overview", "comments": "88 pages, 888 references", "journal-ref": "Neural Networks, Vol 61, pp 85-117, Jan 2015", "doi": "10.1016/j.neunet.2014.09.003", "report-no": "Technical Report IDSIA-03-14", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep artificial neural networks (including recurrent ones)\nhave won numerous contests in pattern recognition and machine learning. This\nhistorical survey compactly summarises relevant work, much of it from the\nprevious millennium. Shallow and deep learners are distinguished by the depth\nof their credit assignment paths, which are chains of possibly learnable,\ncausal links between actions and effects. I review deep supervised learning\n(also recapitulating the history of backpropagation), unsupervised learning,\nreinforcement learning & evolutionary computation, and indirect search for\nshort programs encoding deep and large networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 18:39:00 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 15:33:51 GMT"}, {"version": "v3", "created": "Wed, 2 Jul 2014 16:05:33 GMT"}, {"version": "v4", "created": "Wed, 8 Oct 2014 10:00:38 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}]