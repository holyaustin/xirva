[{"id": "0805.0197", "submitter": "Wan Ahmad Tajuddin Wan Abdullah", "authors": "Saratha Sathasivam (USM), Wan Ahmad Tajuddin Wan Abdullah (Univ.\n  Malaya)", "title": "Flatness of the Energy Landscape for Horn Clauses", "comments": null, "journal-ref": "Matematika 23 (2007) 147-156", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Little-Hopfield neural network programmed with Horn clauses is studied.\nWe argue that the energy landscape of the system, corresponding to the\ninconsistency function for logical interpretations of the sets of Horn clauses,\nhas minimal ruggedness. This is supported by computer simulations.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2008 09:20:11 GMT"}], "update_date": "2008-05-05", "authors_parsed": [["Sathasivam", "Saratha", "", "USM"], ["Abdullah", "Wan Ahmad Tajuddin Wan", "", "Univ.\n  Malaya"]]}, {"id": "0805.0231", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (INRIA Futurs)", "title": "CMA-ES with Two-Point Step-Size Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-6527", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine a refined version of two-point step-size adaptation with the\ncovariance matrix adaptation evolution strategy (CMA-ES). Additionally, we\nsuggest polished formulae for the learning rate of the covariance matrix and\nthe recombination weights. In contrast to cumulative step-size adaptation or to\nthe 1/5-th success rule, the refined two-point adaptation (TPA) does not rely\non any internal model of optimality. In contrast to conventional\nself-adaptation, the TPA will achieve a better target step-size in particular\nwith large populations. The disadvantage of TPA is that it relies on two\nadditional objective function\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2008 13:55:37 GMT"}, {"version": "v2", "created": "Sat, 3 May 2008 06:16:04 GMT"}, {"version": "v3", "created": "Tue, 13 May 2008 08:17:14 GMT"}, {"version": "v4", "created": "Sun, 18 May 2008 06:38:04 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Hansen", "Nikolaus", "", "INRIA Futurs"]]}, {"id": "0805.0697", "submitter": "Tshilidzi Marwala", "authors": "Meir Perez and Tshilidzi Marwala", "title": "Stochastic Optimization Approaches for Solving Sudoku", "comments": "13 pages", "journal-ref": null, "doi": "10.1016/j.eswa.2012.04.019", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the Sudoku problem is solved using stochastic search techniques\nand these are: Cultural Genetic Algorithm (CGA), Repulsive Particle Swarm\nOptimization (RPSO), Quantum Simulated Annealing (QSA) and the Hybrid method\nthat combines Genetic Algorithm with Simulated Annealing (HGASA). The results\nobtained show that the CGA, QSA and HGASA are able to solve the Sudoku puzzle\nwith CGA finding a solution in 28 seconds, while QSA finding a solution in 65\nseconds and HGASA in 1.447 seconds. This is mainly because HGASA combines the\nparallel searching of GA with the flexibility of SA. The RPSO was found to be\nunable to solve the puzzle.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2008 11:06:49 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Perez", "Meir", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0805.1153", "submitter": "\\^Hamed  \\\"Owladeghaffari", "authors": "H.Owladeghaffari", "title": "Contact state analysis using NFIS and SOM", "comments": "Proc. International Symposium on Computational Mechanics (ISCM2007),\n  Yao ZH & Yuan MW (eds.), Beijing: Tsinghua University Press & Springer, July\n  30-August 1, 2007, Beijing, China,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports application of neuro- fuzzy inference system (NFIS) and\nself organizing feature map neural networks (SOM) on detection of contact state\nin a block system. In this manner, on a simple system, the evolution of contact\nstates, by parallelization of DDA, has been investigated. So, a comparison\nbetween NFIS and SOM results has been presented. The results show applicability\nof the proposed methods, by different accuracy, on detection of contact's\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2008 12:10:21 GMT"}], "update_date": "2008-05-09", "authors_parsed": [["Owladeghaffari", "H.", ""]]}, {"id": "0805.1154", "submitter": "Finn {\\AA}rup Nielsen", "authors": "Finn Aarup Nielsen", "title": "Clustering of scientific citations in Wikipedia", "comments": "7 pages; 2 figures, Wikimania 2008; Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The instances of templates in Wikipedia form an interesting data set of\nstructured information. Here I focus on the cite journal template that is\nprimarily used for citation to articles in scientific journals. These citations\ncan be extracted and analyzed: Non-negative matrix factorization is performed\non a (article x journal) matrix resulting in a soft clustering of Wikipedia\narticles and scientific journals, each cluster more or less representing a\nscientific topic.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2008 12:29:36 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2008 11:54:46 GMT"}], "update_date": "2008-06-12", "authors_parsed": [["Nielsen", "Finn Aarup", ""]]}, {"id": "0805.1296", "submitter": "Christoph Schommer", "authors": "Christoph Schommer", "title": "A Simple Dynamic Mind-map Framework To Discover Associative\n  Relationships in Transactional Data Streams", "comments": "12 pages, 8 Figures. Updated version of a paper presented at the\n  Workshop on Symbolic Networks, ECAI 2004, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we informally introduce dynamic mind-maps that represent a new\napproach on the basis of a dynamic construction of connectionist structures\nduring the processing of a data stream. This allows the representation and\nprocessing of recursively defined structures and avoids the problem of a more\ntraditional, fixed-size architecture with the processing of input structures of\nunknown size. For a data stream analysis with association discovery, the\nincremental analysis of data leads to results on demand. Here, we describe a\nframework that uses symbolic cells to calculate associations based on\ntransactional data streams as it exists in e.g. bibliographic databases. We\nfollow a natural paradigm of applying simple operations on cells yielding on a\nmind-map structure that adapts over time.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2008 08:10:09 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Schommer", "Christoph", ""]]}, {"id": "0805.1319", "submitter": "S. D. Makovetskiy", "authors": "S. D. Makovetskiy, D. N. Makovetskii", "title": "Emergence, Competition and Dynamical Stabilization of Dissipative\n  Rotating Spiral Waves in an Excitable Medium: A Computational Model Based on\n  Cellular Automata", "comments": "9 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report some qualitatively new features of emergence, competition and\ndynamical stabilization of dissipative rotating spiral waves (RSWs) in the\ncellular-automaton model of laser-like excitable media proposed in\narXiv:cond-mat/0410460v2 ; arXiv:cond-mat/0602345 . Part of the observed\nfeatures are caused by unusual mechanism of excitation vorticity when the RSW's\ncore get into the surface layer of an active medium. Instead of the well known\nscenario of RSW collapse, which takes place after collision of RSW's core with\nabsorbing boundary, we observed complicated transformations of the core leading\nto regeneration (nonlinear \"reflection\" from the boundary) of the RSW or even\nto birth of several new RSWs in the surface layer. Computer experiments on\nbottlenecked evolution of such the RSW-ensembles (vortex matter) are reported\nand a possible explanation of real experiments on spin-lattice relaxation in\ndilute paramagnets is proposed on the basis of an analysis of the RSWs\ndynamics. Chimera states in RSW-ensembles are revealed and compared with\nanalogous states in ensembles of nonlocally coupled oscillators. Generally, our\ncomputer experiments have shown that vortex matter states in laser-like\nexcitable media have some important features of aggregate states of the usual\nmatter.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2008 10:15:28 GMT"}], "update_date": "2008-05-15", "authors_parsed": [["Makovetskiy", "S. D.", ""], ["Makovetskii", "D. N.", ""]]}, {"id": "0805.1696", "submitter": "Manuel Cebrian", "authors": "Manuel Cebrian, Manuel Alfonseca and Alfonso Ortega", "title": "Grammatical Evolution with Restarts for Fast Fractal Generation", "comments": "26 pages, 13 figures, Extended version of the paper presented at\n  ANNIE'04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work, the authors proposed a Grammatical Evolution algorithm to\nautomatically generate Lindenmayer Systems which represent fractal curves with\na pre-determined fractal dimension. This paper gives strong statistical\nevidence that the probability distributions of the execution time of that\nalgorithm exhibits a heavy tail with an hyperbolic probability decay for long\nexecutions, which explains the erratic performance of different executions of\nthe algorithm. Three different restart strategies have been incorporated in the\nalgorithm to mitigate the problems associated to heavy tail distributions: the\nfirst assumes full knowledge of the execution time probability distribution,\nthe second and third assume no knowledge. These strategies exploit the fact\nthat the probability of finding a solution in short executions is\nnon-negligible and yield a severe reduction, both in the expected execution\ntime (up to one order of magnitude) and in its variance, which is reduced from\nan infinite to a finite value.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2008 17:55:59 GMT"}, {"version": "v2", "created": "Fri, 15 Oct 2010 15:30:53 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Cebrian", "Manuel", ""], ["Alfonseca", "Manuel", ""], ["Ortega", "Alfonso", ""]]}, {"id": "0805.3126", "submitter": "Robert Burger PhD", "authors": "J. R. Burger", "title": "Cognitive Architecture for Direction of Attention Founded on Subliminal\n  Memory Searches, Pseudorandom and Nonstop", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By way of explaining how a brain works logically, human associative memory is\nmodeled with logical and memory neurons, corresponding to standard digital\ncircuits. The resulting cognitive architecture incorporates basic psychological\nelements such as short term and long term memory. Novel to the architecture are\nmemory searches using cues chosen pseudorandomly from short term memory.\nRecalls alternated with sensory images, many tens per second, are analyzed\nsubliminally as an ongoing process, to determine a direction of attention in\nshort term memory.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2008 17:37:31 GMT"}], "update_date": "2008-05-21", "authors_parsed": [["Burger", "J. R.", ""]]}, {"id": "0805.3800", "submitter": "Vitaly Schetinin", "authors": "Vitaly Schetinin, Dayou Li, Carsten Maple", "title": "An Evolutionary-Based Approach to Learning Multiple Decision Models from\n  Underrepresented Data", "comments": "5 pages, 3 figures, 2 tables, The 4 th International Conference on\n  Natural Computation (ICNC'08)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of multiple Decision Models (DMs) enables to enhance the accuracy in\ndecisions and at the same time allows users to evaluate the confidence in\ndecision making. In this paper we explore the ability of multiple DMs to learn\nfrom a small amount of verified data. This becomes important when data samples\nare difficult to collect and verify. We propose an evolutionary-based approach\nto solving this problem. The proposed technique is examined on a few clinical\nproblems presented by a small amount of data.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2008 23:37:51 GMT"}], "update_date": "2008-05-27", "authors_parsed": [["Schetinin", "Vitaly", ""], ["Li", "Dayou", ""], ["Maple", "Carsten", ""]]}, {"id": "0805.4247", "submitter": "Ralph Linsker", "authors": "Ralph Linsker", "title": "Neural network learning of optimal Kalman prediction and control", "comments": null, "journal-ref": null, "doi": null, "report-no": "IBM Research Report RC24390", "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are many neural network (NN) algorithms for prediction and for\ncontrol, and although methods for optimal estimation (including filtering and\nprediction) and for optimal control in linear systems were provided by Kalman\nin 1960 (with nonlinear extensions since then), there has been, to my\nknowledge, no NN algorithm that learns either Kalman prediction or Kalman\ncontrol (apart from the special case of stationary control). Here we show how\noptimal Kalman prediction and control (KPC), as well as system identification,\ncan be learned and executed by a recurrent neural network composed of\nlinear-response nodes, using as input only a stream of noisy measurement data.\n  The requirements of KPC appear to impose significant constraints on the\nallowed NN circuitry and signal flows. The NN architecture implied by these\nconstraints bears certain resemblances to the local-circuit architecture of\nmammalian cerebral cortex. We discuss these resemblances, as well as caveats\nthat limit our current ability to draw inferences for biological function. It\nhas been suggested that the local cortical circuit (LCC) architecture may\nperform core functions (as yet unknown) that underlie sensory, motor,and other\ncortical processing. It is reasonable to conjecture that such functions may\ninclude prediction, the estimation or inference of missing or noisy sensory\ndata, and the goal-driven generation of control signals. The resemblances found\nbetween the KPC NN architecture and that of the LCC are consistent with this\nconjecture.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2008 01:57:11 GMT"}], "update_date": "2008-05-29", "authors_parsed": [["Linsker", "Ralph", ""]]}]