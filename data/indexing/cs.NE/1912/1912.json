[{"id": "1912.00009", "submitter": "Shiyuan Li", "authors": "Shiyuan Li", "title": "MSTDP: A More Biologically Plausible Learning", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-timing dependent plasticity (STDP) which observed in the brain has\nproven to be important in biological learning. On the other hand, artificial\nneural networks use a different way to learn, such as Back-Propagation or\nContrastive Hebbian Learning. In this work, we propose a new framework called\nmstdp that learn almost the same way biological learning use, it only uses STDP\nrules for supervised and unsupervised learning and don' t need a global loss or\nother supervise information. The framework works like an auto-encoder by making\neach input neuron also an output neuron. It can make predictions or generate\npatterns in one model without additional configuration. We also brought a new\niterative inference method using momentum to make the framework more efficient,\nwhich can be used in training and testing phases. Finally, we verified our\nframework on MNIST dataset for classification and generation task.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 05:42:50 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 02:33:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Li", "Shiyuan", ""]]}, {"id": "1912.00079", "submitter": "Eli (Omid) David", "authors": "Itay Mosafi, Eli David, Nathan S. Netanyahu", "title": "DeepMimic: Mentor-Student Unlabeled Data Based Training", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11731, pp. 440-455, Munich, Germany, September 2019", "doi": "10.1007/978-3-030-30493-5_44", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep neural network (DNN) training approach\ncalled the \"DeepMimic\" training method. Enormous amounts of data are available\nnowadays for training usage. Yet, only a tiny portion of these data is manually\nlabeled, whereas almost all of the data are unlabeled. The training approach\npresented utilizes, in a most simplified manner, the unlabeled data to the\nfullest, in order to achieve remarkable (classification) results. Our DeepMimic\nmethod uses a small portion of labeled data and a large amount of unlabeled\ndata for the training process, as expected in a real-world scenario. It\nconsists of a mentor model and a student model. Employing a mentor model\ntrained on a small portion of the labeled data and then feeding it only with\nunlabeled data, we show how to obtain a (simplified) student model that reaches\nthe same accuracy and loss as the mentor model, on the same test set, without\nusing any of the original data labels in the training of the student model. Our\nexperiments demonstrate that even on challenging classification tasks the\nstudent network architecture can be simplified significantly with a minor\ninfluence on the performance, i.e., we need not even know the original network\narchitecture of the mentor. In addition, the time required for training the\nstudent model to reach the mentor's performance level is shorter, as a result\nof a simplified architecture and more available data. The proposed method\nhighlights the disadvantages of regular supervised training and demonstrates\nthe benefits of a less traditional training approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 02:31:36 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mosafi", "Itay", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.00200", "submitter": "Abdullah Salama", "authors": "Abdullah Salama, Oleksiy Ostapenko, Tassilo Klein, Moin Nabi", "title": "Pruning at a Glance: Global Neural Pruning for Model Compression", "comments": "Extended version of the ICASSP paper\n  (https://ieeexplore.ieee.org/document/8683224)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning models have become the dominant approach in several areas due\nto their high performance. Unfortunately, the size and hence computational\nrequirements of operating such models can be considerably high. Therefore, this\nconstitutes a limitation for deployment on memory and battery constrained\ndevices such as mobile phones or embedded systems. To address these\nlimitations, we propose a novel and simple pruning method that compresses\nneural networks by removing entire filters and neurons according to a global\nthreshold across the network without any pre-calculation of layer sensitivity.\nThe resulting model is compact, non-sparse, with the same accuracy as the\nnon-compressed model, and most importantly requires no special infrastructure\nfor deployment. We prove the viability of our method by producing highly\ncompressed models, namely VGG-16, ResNet-56, and ResNet-110 respectively on\nCIFAR10 without losing any performance compared to the baseline, as well as\nResNet-34 and ResNet-50 on ImageNet without a significant loss of accuracy. We\nalso provide a well-retrained 30% compressed ResNet-50 that slightly surpasses\nthe base model accuracy. Additionally, compressing more than 56% and 97% of\nAlexNet and LeNet-5 respectively. Interestingly, the resulted models' pruning\npatterns are highly similar to the other methods using layer sensitivity\npre-calculation step. Our method does not only exhibit good performance but\nwhat is more also easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 13:17:48 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 09:44:06 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Salama", "Abdullah", ""], ["Ostapenko", "Oleksiy", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1912.00554", "submitter": "Hiroyasu Ando", "authors": "Hiroyasu Ando and Hanten Chang", "title": "Road traffic reservoir computing", "comments": "Submitted to ESANN 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing derived from recurrent neural networks is more applicable\nto real world systems than deep learning because of its low computational cost\nand potential for physical implementation. Specifically, physical reservoir\ncomputing, which replaces the dynamics of reservoir units with physical\nphenomena, has recently received considerable attention. In this study, we\npropose a method of exploiting the dynamics of road traffic as a reservoir, and\nnumerically confirm its feasibility by applying several prediction tasks based\non a simple mathematical model of the traffic flow.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:28:45 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ando", "Hiroyasu", ""], ["Chang", "Hanten", ""]]}, {"id": "1912.00873", "submitter": "Ehsan Kharazmi", "authors": "E. Kharazmi, Z. Zhang, G. E. Karniadakis", "title": "Variational Physics-Informed Neural Networks For Solving Partial\n  Differential Equations", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (PINNs) [31] use automatic differentiation\nto solve partial differential equations (PDEs) by penalizing the PDE in the\nloss function at a random set of points in the domain of interest. Here, we\ndevelop a Petrov-Galerkin version of PINNs based on the nonlinear approximation\nof deep neural networks (DNNs) by selecting the {\\em trial space} to be the\nspace of neural networks and the {\\em test space} to be the space of Legendre\npolynomials. We formulate the \\textit{variational residual} of the PDE using\nthe DNN approximation by incorporating the variational form of the problem into\nthe loss function of the network and construct a \\textit{variational\nphysics-informed neural network} (VPINN). By integrating by parts the integrand\nin the variational form, we lower the order of the differential operators\nrepresented by the neural networks, hence effectively reducing the training\ncost in VPINNs while increasing their accuracy compared to PINNs that\nessentially employ delta test functions. For shallow networks with one hidden\nlayer, we analytically obtain explicit forms of the \\textit{variational\nresidual}. We demonstrate the performance of the new formulation for several\nexamples that show clear advantages of VPINNs over PINNs in terms of both\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:51:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kharazmi", "E.", ""], ["Zhang", "Z.", ""], ["Karniadakis", "G. E.", ""]]}, {"id": "1912.01088", "submitter": "Campbell Scott", "authors": "J. Campbell Scott, Thomas F. Hayes, Ahmet S. Ozcan and Winfried W.\n  Wilcke", "title": "Simulation of neural function in an artificial Hebbian network", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have diverged far from their early inspiration in\nneurology. In spite of their technological and commercial success, they have\nseveral shortcomings, most notably the need for a large number of training\nexamples and the resulting computation resources required for iterative\nlearning. Here we describe an approach to neurological network simulation, both\narchitectural and algorithmic, that adheres more closely to established\nbiological principles and overcomes some of the shortcomings of conventional\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:41:15 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Scott", "J. Campbell", ""], ["Hayes", "Thomas F.", ""], ["Ozcan", "Ahmet S.", ""], ["Wilcke", "Winfried W.", ""]]}, {"id": "1912.01105", "submitter": "Javier Trejos", "authors": "Jeffry Chavarria-Molina, Juan Jose Fallas-Monge, Javier Trejos-Zelaya", "title": "Clustering via Ant Colonies: Parameter Analysis and Improvement of the\n  Algorithm", "comments": "19 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An ant colony optimization approach for partitioning a set of objects is\nproposed. In order to minimize the intra-variance, or within sum-of-squares, of\nthe partitioned classes, we construct ant-like solutions by a constructive\napproach that selects objects to be put in a class with a probability that\ndepends on the distance between the object and the centroid of the class\n(visibility) and the pheromone trail; the latter depends on the class\nmemberships that have been defined along the iterations. The procedure is\nimproved with the application of K-means algorithm in some iterations of the\nant colony method. We performed a simulation study in order to evaluate the\nmethod with a Monte Carlo experiment that controls some sensitive parameters of\nthe clustering problem. After some tuning of the parameters, the method has\nalso been applied to some benchmark real-data sets. Encouraging results were\nobtained in nearly all cases.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:38:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chavarria-Molina", "Jeffry", ""], ["Fallas-Monge", "Juan Jose", ""], ["Trejos-Zelaya", "Javier", ""]]}, {"id": "1912.01111", "submitter": "Jayanta Mandi", "authors": "Dipankar Chakrabarti, Neelam Patodia, Udayan Bhattacharya, Indranil\n  Mitra, Satyaki Roy, Jayanta Mandi, Nandini Roy, Prasun Nandy", "title": "Use of Artificial Intelligence to Analyse Risk in Legal Documents for a\n  Better Decision Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assessing risk for voluminous legal documents such as request for proposal;\ncontracts is tedious and error prone. We have developed \"risk-o-meter\", a\nframework, based on machine learning and natural language processing to review\nand assess risks of any legal document. Our framework uses Paragraph Vector, an\nunsupervised model to generate vector representation of text. This enables the\nframework to learn contextual relations of legal terms and generate sensible\ncontext aware embedding. The framework then feeds the vector space into a\nsupervised classification algorithm to predict whether a paragraph belongs to a\nper-defined risk category or not. The framework thus extracts risk prone\nparagraphs. This technique efficiently overcomes the limitations of\nkeyword-based search. We have achieved an accuracy of 91% for the risk category\nhaving the largest training dataset. This framework will help organizations\noptimize effort to identify risk from large document base with minimal human\nintervention and thus will help to have risk mitigated sustainable growth. Its\nmachine learning capability makes it scalable to uncover relevant information\nfrom any type of document apart from legal documents, provided the library is\nper-populated and rich.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:07:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chakrabarti", "Dipankar", ""], ["Patodia", "Neelam", ""], ["Bhattacharya", "Udayan", ""], ["Mitra", "Indranil", ""], ["Roy", "Satyaki", ""], ["Mandi", "Jayanta", ""], ["Roy", "Nandini", ""], ["Nandy", "Prasun", ""]]}, {"id": "1912.01116", "submitter": "Jeremy Gordon", "authors": "Jeremy Gordon, David Rawlinson, Subutai Ahmad", "title": "Long Distance Relationships without Time Travel: Boosting the\n  Performance of a Sparse Predictive Autoencoder in Sequence Modeling", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequence learning tasks such as language modelling, Recurrent Neural\nNetworks must learn relationships between input features separated by time.\nState of the art models such as LSTM and Transformer are trained by\nbackpropagation of losses into prior hidden states and inputs held in memory.\nThis allows gradients to flow from present to past and effectively learn with\nperfect hindsight, but at a significant memory cost. In this paper we show that\nit is possible to train high performance recurrent networks using information\nthat is local in time, and thereby achieve a significantly reduced memory\nfootprint. We describe a predictive autoencoder called bRSM featuring recurrent\nconnections, sparse activations, and a boosting rule for improved cell\nutilization. The architecture demonstrates near optimal performance on a\nnon-deterministic (stochastic) partially-observable sequence learning task\nconsisting of high-Markov-order sequences of MNIST digits. We find that this\nmodel learns these sequences faster and more completely than an LSTM, and offer\nseveral possible explanations why the LSTM architecture might struggle with the\npartially observable sequence structure in this task. We also apply our model\nto a next word prediction task on the Penn Treebank (PTB) dataset. We show that\na 'flattened' RSM network, when paired with a modern semantic word embedding\nand the addition of boosting, achieves 103.5 PPL (a 20-point improvement over\nthe best N-gram models), beating ordinary RNNs trained with BPTT and\napproaching the scores of early LSTM implementations. This work provides\nencouraging evidence that strong results on challenging tasks such as language\nmodelling may be possible using less memory intensive, biologically-plausible\ntraining regimes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:00:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Gordon", "Jeremy", ""], ["Rawlinson", "David", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1912.01137", "submitter": "Pitoyo Hartono", "authors": "Pitoyo Hartono", "title": "Mixing autoencoder with classifier: conceptual data visualization", "comments": null, "journal-ref": "IEEE Access, vol. 8, no. 1, pp. 105301-105310, 2020", "doi": "10.1109/ACCESS.2020.2999155", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, a neural network that is able to form a low dimensional\ntopological hidden representation is explained. The neural network can be\ntrained as an autoencoder, a classifier or mix of both, and produces different\nlow dimensional topological map for each of them. When it is trained as an\nautoencoder, the inherent topological structure of the data can be visualized,\nwhile when it is trained as a classifier, the topological structure is further\nconstrained by the concept, for example the labels the data, hence the\nvisualization is not only structural but also conceptual. The proposed neural\nnetwork significantly differ from many dimensional reduction models, primarily\nin its ability to execute both supervised and unsupervised dimensional\nreduction. The neural network allows multi perspective visualization of the\ndata, and thus giving more flexibility in data analysis. This paper is\nsupported by preliminary but intuitive visualization experiments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:33:26 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:46:11 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 10:27:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hartono", "Pitoyo", ""]]}, {"id": "1912.01268", "submitter": "Martino Sorbaro", "authors": "Martino Sorbaro, Qian Liu, Massimo Bortone, Sadique Sheik", "title": "Optimizing the energy consumption of spiking neural networks for\n  neuromorphic applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last few years, spiking neural networks have been demonstrated to\nperform on par with regular convolutional neural networks. Several works have\nproposed methods to convert a pre-trained CNN to a Spiking CNN without a\nsignificant sacrifice of performance. We demonstrate first that\nquantization-aware training of CNNs leads to better accuracy in SNNs. One of\nthe benefits of converting CNNs to spiking CNNs is to leverage the sparse\ncomputation of SNNs and consequently perform equivalent computation at a lower\nenergy consumption. Here we propose an efficient optimization strategy to train\nspiking networks at lower energy consumption, while maintaining similar\naccuracy levels. We demonstrate results on the MNIST-DVS and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:54:57 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 15:53:25 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Sorbaro", "Martino", ""], ["Liu", "Qian", ""], ["Bortone", "Massimo", ""], ["Sheik", "Sadique", ""]]}, {"id": "1912.01369", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu, Ian Whalen, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman,\n  Wolfgang Banzhaf, Vishnu Naresh Boddeti", "title": "Multi-Objective Evolutionary Design of Deep Convolutional Neural\n  Networks for Image Classification", "comments": "Published in IEEE Transactions on Evolutionary Computation, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early advancements in convolutional neural networks (CNNs) architectures are\nprimarily driven by human expertise and by elaborate design processes.\nRecently, neural architecture search was proposed with the aim of automating\nthe network design process and generating task-dependent architectures. While\nexisting approaches have achieved competitive performance in image\nclassification, they are not well suited to problems where the computational\nbudget is limited for two reasons: (1) the obtained architectures are either\nsolely optimized for classification performance, or only for one deployment\nscenario; (2) the search process requires vast computational resources in most\napproaches. To overcome these limitations, we propose an evolutionary algorithm\nfor searching neural architectures under multiple objectives, such as\nclassification performance and floating-point operations (FLOPs). The proposed\nmethod addresses the first shortcoming by populating a set of architectures to\napproximate the entire Pareto frontier through genetic operations that\nrecombine and modify architectural components progressively. Our approach\nimproves computational efficiency by carefully down-scaling the architectures\nduring the search as well as reinforcing the patterns commonly shared among\npast successful architectures through Bayesian model learning. The integration\nof these two main contributions allows an efficient design of architectures\nthat are competitive and in most cases outperform both manually and\nautomatically designed architectures on benchmark image classification\ndatasets: CIFAR, ImageNet, and human chest X-ray. The flexibility provided from\nsimultaneously obtaining multiple architecture choices for different compute\nrequirements further differentiates our approach from other methods in the\nliterature. Code is available at https://github.com/mikelzc1990/nsganetv1\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:57:25 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 18:38:21 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 13:35:27 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lu", "Zhichao", ""], ["Whalen", "Ian", ""], ["Dhebar", "Yashesh", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""], ["Banzhaf", "Wolfgang", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1912.01493", "submitter": "Eli (Omid) David", "authors": "Ishai Rosenberg, Guillaume Sicard, Eli David", "title": "End-to-End Deep Neural Networks and Transfer Learning for Automatic\n  Analysis of Nation-State Malware", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09666", "journal-ref": "Entropy, Vol. 20, No. 5, pp. 390-401, May 2018", "doi": "10.3390/e20050390", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware allegedly developed by nation-states, also known as advanced\npersistent threats (APT), are becoming more common. The task of attributing an\nAPT to a specific nation-state or classifying it to the correct APT family is\nchallenging for several reasons. First, each nation-state has more than a\nsingle cyber unit that develops such malware, rendering traditional authorship\nattribution algorithms useless. Furthermore, the dataset of such available APTs\nis still extremely small. Finally, those APTs use state-of-the-art evasion\ntechniques, making feature extraction challenging. In this paper, we use a deep\nneural network (DNN) as a classifier for nation-state APT attribution. We\nrecord the dynamic behavior of the APT when run in a sandbox and use it as raw\ninput for the neural network, allowing the DNN to learn high level feature\nabstractions of the APTs itself. We also use the same raw features for APT\nfamily classification. Finally, we use the feature abstractions learned by the\nAPT family classifier to solve the attribution problem. Using a test set of\n1000 Chinese and Russian developed APTs, we achieved an accuracy rate of 98.6%.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:21:26 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Rosenberg", "Ishai", ""], ["Sicard", "Guillaume", ""], ["David", "Eli", ""]]}, {"id": "1912.01494", "submitter": "Eli (Omid) David", "authors": "Ido Cohen, Eli David, Nathan S. Netanyahu", "title": "Supervised and Unsupervised End-to-End Deep Learning for Gene Ontology\n  Classification of Neural In Situ Hybridization Images", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09663", "journal-ref": "Entropy, Vol. 21, No. 3, pp. 221-238, February 2019", "doi": "10.3390/e21030221", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large datasets of high-resolution mammalian neural images\nhave become available, which has prompted active research on the analysis of\ngene expression data. Traditional image processing methods are typically\napplied for learning functional representations of genes, based on their\nexpressions in these brain images. In this paper, we describe a novel\nend-to-end deep learning-based method for generating compact representations of\nin situ hybridization (ISH) images, which are invariant-to-translation. In\ncontrast to traditional image processing methods, our method relies, instead,\non deep convolutional denoising autoencoders (CDAE) for processing raw pixel\ninputs, and generating the desired compact image representations. We provide an\nin-depth description of our deep learning-based approach, and present extensive\nexperimental results, demonstrating that representations extracted by CDAE can\nhelp learn features of functional gene ontology categories for their\nclassification in a highly accurate manner. Our methods improve the previous\nstate-of-the-art classification rate (Liscovitch, et al.) from an average AUC\nof 0.92 to 0.997, i.e., it achieves 96% reduction in error rate. Furthermore,\nthe representation vectors generated due to our method are more compact in\ncomparison to previous state-of-the-art methods, allowing for a more efficient\nhigh-level representation of images. These results are obtained with\nsignificantly downsampled images in comparison to the original high-resolution\nones, further underscoring the robustness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:20:12 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Cohen", "Ido", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.01792", "submitter": "Yawen Zhang", "authors": "Songtao Lu, Yawen Zhang, Yunlong Wang, Christina Mack", "title": "Learn Electronic Health Records by Fully Decentralized Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning opens a number of research opportunities due to its high\ncommunication efficiency in distributed training problems within a star\nnetwork. In this paper, we focus on improving the communication efficiency for\nfully decentralized federated learning over a graph, where the algorithm\nperforms local updates for several iterations and then enables communications\namong the nodes. In such a way, the communication rounds of exchanging the\ncommon interest of parameters can be saved significantly without loss of\noptimality of the solutions. Multiple numerical simulations based on large,\nreal-world electronic health record databases showcase the superiority of the\ndecentralized federated learning compared with classic methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 04:28:05 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 01:31:36 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lu", "Songtao", ""], ["Zhang", "Yawen", ""], ["Wang", "Yunlong", ""], ["Mack", "Christina", ""]]}, {"id": "1912.01816", "submitter": "Eli (Omid) David", "authors": "Evyatar Illouz, Eli David, and Nathan S. Netanyahu", "title": "Handwriting-Based Gender Classification Using End-to-End Deep Neural\n  Networks", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11141, pp. 613-621, Rhodes, Greece, October 2018", "doi": "10.1007/978-3-030-01424-7_60", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting-based gender classification is a well-researched problem that has\nbeen approached mainly by traditional machine learning techniques. In this\npaper, we propose a novel deep learning-based approach for this task.\nSpecifically, we present a convolutional neural network (CNN), which performs\nautomatic feature extraction from a given handwritten image, followed by\nclassification of the writer's gender. Also, we introduce a new dataset of\nlabeled handwritten samples, in Hebrew and English, of 405 participants.\nComparing the gender classification accuracy on this dataset against human\nexaminers, our results show that the proposed deep learning-based approach is\nsubstantially more accurate than that of humans.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:24:31 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Illouz", "Evyatar", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.02134", "submitter": "Alexandru Strimbu", "authors": "Alexandru Strimbu", "title": "Simulating Evolution on Fitness Landscapes represented by Valued\n  Constraint Satisfaction Problems", "comments": "60 pages, 21 figures, third year project", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical research proposes that computational complexity can be\nseen as an ultimate constraint that allows for open-ended biological evolution\non finite static fitness landscapes. Whereas on easy fitness landscapes,\nevolution will quickly converge to a local fitness peaks, on hard fitness\nlandscapes this computational constraints prevents evolution from reaching any\nlocal fitness peak in polynomial time. Valued constraint satisfaction problems\n(VCSPs) can be used to represent both easy and hard fitness landscapes. Thus\nVCSPS can be seen as a natural way of linking the theory of evolution with\nnotions of computer science to better understand the features that make\nlandscapes hard. However, there are currently no simulators that study\nVCSP-structured fitness landscapes.\n  This report describes the design and build of an evolution simulator for\nVCSP-structured fitness landscapes. The platform is used for simulating various\ninstances of easy and hard fitness landscapes. In particular, we look at\nevolution under more realistic assumptions than fittest mutant strong-selection\nweak mutation dynamics on the winding semismooth fitness landscape. The results\nobtained match with the theoretical expectations, while also providing new\ninformation about the limits of evolution. The last part of the report\nintroduces a mathematical model for smooth fitness landscapes and uses it to\nbetter understand why these landscapes are easy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:31:22 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Strimbu", "Alexandru", ""]]}, {"id": "1912.02292", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak,\n  Ilya Sutskever", "title": "Deep Double Descent: Where Bigger Models and More Data Hurt", "comments": "G.K. and Y.B. contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a variety of modern deep learning tasks exhibit a\n\"double-descent\" phenomenon where, as we increase model size, performance first\ngets worse and then gets better. Moreover, we show that double descent occurs\nnot just as a function of model size, but also as a function of the number of\ntraining epochs. We unify the above phenomena by defining a new complexity\nmeasure we call the effective model complexity and conjecture a generalized\ndouble descent with respect to this measure. Furthermore, our notion of model\ncomplexity allows us to identify certain regimes where increasing (even\nquadrupling) the number of train samples actually hurts test performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:47:31 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Kaplun", "Gal", ""], ["Bansal", "Yamini", ""], ["Yang", "Tristan", ""], ["Barak", "Boaz", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1912.02316", "submitter": "Malhar Jere", "authors": "Malhar Jere, Loris Rossi, Briland Hitaj, Gabriela Ciocarlie, Giacomo\n  Boracchi, Farinaz Koushanfar", "title": "Scratch that! An Evolution-based Adversarial Attack against Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study black-box adversarial attacks for image classifiers in a constrained\nthreat model, where adversaries can only modify a small fraction of pixels in\nthe form of scratches on an image. We show that it is possible for adversaries\nto generate localized \\textit{adversarial scratches} that cover less than $5\\%$\nof the pixels in an image and achieve targeted success rates of $98.77\\%$ and\n$97.20\\%$ on ImageNet and CIFAR-10 trained ResNet-50 models, respectively. We\ndemonstrate that our scratches are effective under diverse shapes, such as\nstraight lines or parabolic B\\a'ezier curves, with single or multiple colors.\nIn an extreme condition, in which our scratches are a single color, we obtain a\ntargeted attack success rate of $66\\%$ on CIFAR-10 with an order of magnitude\nfewer queries than comparable attacks. We successfully launch our attack\nagainst Microsoft's Cognitive Services Image Captioning API and propose various\nmitigation strategies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:11:34 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 01:46:06 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 16:11:53 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Jere", "Malhar", ""], ["Rossi", "Loris", ""], ["Hitaj", "Briland", ""], ["Ciocarlie", "Gabriela", ""], ["Boracchi", "Giacomo", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1912.02405", "submitter": "Hossein Kamalzadeh", "authors": "Hossein Kamalzadeh, Abbas Ahmadi, Saeed Mansour", "title": "Clustering Time-Series by a Novel Slope-Based Similarity Measure\n  Considering Particle Swarm Optimization", "comments": "27 pages, 8 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been an increase in the studies on time-series data mining\nspecifically time-series clustering due to the vast existence of time-series in\nvarious domains. The large volume of data in the form of time-series makes it\nnecessary to employ various techniques such as clustering to understand the\ndata and to extract information and hidden patterns. In the field of clustering\nspecifically, time-series clustering, the most important aspects are the\nsimilarity measure used and the algorithm employed to conduct the clustering.\nIn this paper, a new similarity measure for time-series clustering is developed\nbased on a combination of a simple representation of time-series, slope of each\nsegment of time-series, Euclidean distance and the so-called dynamic time\nwarping. It is proved in this paper that the proposed distance measure is\nmetric and thus indexing can be applied. For the task of clustering, the\nParticle Swarm Optimization algorithm is employed. The proposed similarity\nmeasure is compared to three existing measures in terms of various criteria\nused for the evaluation of clustering algorithms. The results indicate that the\nproposed similarity measure outperforms the rest in almost every dataset used\nin this paper.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 06:22:04 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Kamalzadeh", "Hossein", ""], ["Ahmadi", "Abbas", ""], ["Mansour", "Saeed", ""]]}, {"id": "1912.02535", "submitter": "Markus Wagner", "authors": "Aldeida Aleti, Mark Wallace, Markus Wagner", "title": "Is perturbation an effective restart strategy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Premature convergence can be detrimental to the performance of search\nmethods, which is why many search algorithms include restart strategies to deal\nwith it. While it is common to perturb the incumbent solution with\ndiversification steps of various sizes with the hope that the search method\nwill find a new basin of attraction leading to a better local optimum, it is\nusually not clear how big the perturbation step should be. We introduce a new\nproperty of fitness landscapes termed \"Neighbours with Similar Fitness\" and we\ndemonstrate that the effectiveness of a restart strategy depends on this\nproperty.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:33:40 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Aleti", "Aldeida", ""], ["Wallace", "Mark", ""], ["Wagner", "Markus", ""]]}, {"id": "1912.02574", "submitter": "Sanchita Basak", "authors": "Sanchita Basak, Fangzhou Sun, Saptarshi Sengupta and Abhishek Dubey", "title": "Data-Driven Optimization of Public Transit Schedule", "comments": "20 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bus transit systems are the backbone of public transportation in the United\nStates. An important indicator of the quality of service in such\ninfrastructures is on-time performance at stops, with published transit\nschedules playing an integral role governing the level of success of the\nservice. However there are relatively few optimization architectures leveraging\nstochastic search that focus on optimizing bus timetables with the objective of\nmaximizing probability of bus arrivals at timepoints with delays within desired\non-time ranges. In addition to this, there is a lack of substantial research\nconsidering monthly and seasonal variations of delay patterns integrated with\nsuch optimization strategies. To address these,this paper makes the following\ncontributions to the corpus of studies on transit on-time performance\noptimization: (a) an unsupervised clustering mechanism is presented which\ngroups months with similar seasonal delay patterns, (b) the problem is\nformulated as a single-objective optimization task and a greedy algorithm, a\ngenetic algorithm (GA) as well as a particle swarm optimization (PSO) algorithm\nare employed to solve it, (c) a detailed discussion on empirical results\ncomparing the algorithms are provided and sensitivity analysis on\nhyper-parameters of the heuristics are presented along with execution times,\nwhich will help practitioners looking at similar problems. The analyses\nconducted are insightful in the local context of improving public transit\nscheduling in the Nashville metro region as well as informative from a global\nperspective as an elaborate case study which builds upon the growing corpus of\nempirical studies using nature-inspired approaches to transit schedule\noptimization.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:28:11 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Basak", "Sanchita", ""], ["Sun", "Fangzhou", ""], ["Sengupta", "Saptarshi", ""], ["Dubey", "Abhishek", ""]]}, {"id": "1912.02707", "submitter": "Eli (Omid) David", "authors": "Daniel Rika, Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Novel Hybrid Scheme Using Genetic Algorithms and Deep Learning for the\n  Reconstruction of Portuguese Tile Panels", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1319-1327, Prague, Czech Republic, July 2019", "doi": "10.1145/3321707.3321821", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel scheme, based on a unique combination of genetic\nalgorithms (GAs) and deep learning (DL), for the automatic reconstruction of\nPortuguese tile panels, a challenging real-world variant of the jigsaw puzzle\nproblem (JPP) with important national heritage implications. Specifically, we\nintroduce an enhanced GA-based puzzle solver, whose integration with a novel\nDL-based compatibility measure (DLCM) yields state-of-the-art performance,\nregarding the above application. Current compatibility measures consider\ntypically (the chromatic information of) edge pixels (between adjacent tiles),\nand help achieve high accuracy for the synthetic JPP variant. However, such\nmeasures exhibit rather poor performance when applied to the Portuguese tile\npanels, which are susceptible to various real-world effects, e.g.,\nmonochromatic panels, non-squared tiles, edge degradation, etc. To overcome\nsuch difficulties, we have developed a novel DLCM to extract high-level\ntexture/color statistics from the entire tile information.\n  Integrating this measure with our enhanced GA-based puzzle solver, we have\ndemonstrated, for the first time, how to deal most effectively with large-scale\nreal-world problems, such as the Portuguese tile problem. Specifically, we have\nachieved 82% accuracy for the reconstruction of Portuguese tile panels with\nunknown piece rotation and puzzle dimension (compared to merely 3.5% average\naccuracy achieved by the best method known for solving this problem variant).\nThe proposed method outperforms even human experts in several cases, correcting\ntheir mistakes in the manual tile assembly.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:24:21 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rika", "Daniel", ""], ["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.02811", "submitter": "Siyu Zhou", "authors": "Siyu Zhou, Mariano Phielipp, Jorge A. Sefair, Sara I. Walker, Heni Ben\n  Amor", "title": "Clone Swarms: Learning to Predict and Control Multi-Robot Systems by\n  Imitation", "comments": null, "journal-ref": null, "doi": "10.1109/IROS40897.2019.8967824", "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose SwarmNet -- a neural network architecture that can\nlearn to predict and imitate the behavior of an observed swarm of agents in a\ncentralized manner. Tested on artificially generated swarm motion data, the\nnetwork achieves high levels of prediction accuracy and imitation authenticity.\nWe compare our model to previous approaches for modelling interaction systems\nand show how modifying components of other models gradually approaches the\nperformance of ours. Finally, we also discuss an extension of SwarmNet that can\ndeal with nondeterministic, noisy, and uncertain environments, as often found\nin robotics applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:55:56 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:24:04 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhou", "Siyu", ""], ["Phielipp", "Mariano", ""], ["Sefair", "Jorge A.", ""], ["Walker", "Sara I.", ""], ["Amor", "Heni Ben", ""]]}, {"id": "1912.02983", "submitter": "Eli (Omid) David", "authors": "Katia Huri, Eli David, Nathan S. Netanyahu", "title": "DeepEthnic: Multi-Label Ethnic Classification from Face Images", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11141, pp. 604-612, Rhodes, Greece, October 2018", "doi": "10.1007/978-3-030-01424-7_59", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethnic group classification is a well-researched problem, which has been\npursued mainly during the past two decades via traditional approaches of image\nprocessing and machine learning. In this paper, we propose a method of\nclassifying an image face into an ethnic group by applying transfer learning\nfrom a previously trained classification network for large-scale data\nrecognition. Our proposed method yields state-of-the-art success rates of\n99.02%, 99.76%, 99.2%, and 96.7%, respectively, for the four ethnic groups:\nAfrican, Asian, Caucasian, and Indian.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 05:59:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Huri", "Katia", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.03063", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec (LIRIS), Grigory Antipov, Moez Baccouche, Christian\n  Wolf (LIRIS)", "title": "Weak Supervision helps Emergence of Word-Object Alignment and improves\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large adoption of the self-attention (i.e. transformer model) and\nBERT-like training principles has recently resulted in a number of high\nperforming models on a large panoply of vision-and-language problems (such as\nVisual Question Answering (VQA), image retrieval, etc.). In this paper we claim\nthat these State-Of-The-Art (SOTA) approaches perform reasonably well in\nstructuring information inside a single modality but, despite their impressive\nperformances , they tend to struggle to identify fine-grained inter-modality\nrelationships. Indeed, such relations are frequently assumed to be implicitly\nlearned during training from application-specific losses, mostly cross-entropy\nfor classification. While most recent works provide inductive bias for\ninter-modality relationships via cross attention modules, in this work, we\ndemonstrate (1) that the latter assumption does not hold, i.e. modality\nalignment does not necessarily emerge automatically, and (2) that adding weak\nsupervision for alignment between visual objects and words improves the quality\nof the learned models on tasks requiring reasoning. In particular , we\nintegrate an object-word alignment loss into SOTA vision-language reasoning\nmodels and evaluate it on two tasks VQA and Language-driven Comparison of\nImages. We show that the proposed fine-grained inter-modality supervision\nsignificantly improves performance on both tasks. In particular, this new\nlearning signal allows obtaining SOTA-level performances on GQA dataset (VQA\ntask) with pre-trained models without finetuning on the task, and a new SOTA on\nNLVR2 dataset (Language-driven Comparison of Images). Finally, we also\nillustrate the impact of the contribution on the models reasoning by\nvisualizing attention distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 11:04:08 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kervadec", "Corentin", "", "LIRIS"], ["Antipov", "Grigory", "", "LIRIS"], ["Baccouche", "Moez", "", "LIRIS"], ["Wolf", "Christian", "", "LIRIS"]]}, {"id": "1912.03126", "submitter": "Nicolas Rougier", "authors": "Ikram Chraibi Kaadoud and Nicolas P. Rougier and Fr\\'ed\\'eric\n  Alexandre", "title": "Knowledge extraction from the learning of sequences in a long short term\n  memory (LSTM) architecture", "comments": "18 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a general method to extract knowledge from a recurrent neural\nnetwork (Long Short Term Memory) that has learnt to detect if a given input\nsequence is valid or not, according to an unknown generative automaton. Based\non the clustering of the hidden states, we explain how to build and validate an\nautomaton that corresponds to the underlying (unknown) automaton, and allows to\npredict if a given sequence is valid or not. The method is illustrated on\nartificial grammars (Reber's grammar variations) as well as on a real use-case\nwhose underlying grammar is unknown.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:00:21 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kaadoud", "Ikram Chraibi", ""], ["Rougier", "Nicolas P.", ""], ["Alexandre", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1912.03201", "submitter": "Rene Larisch", "authors": "Ren\\'e Larisch and Michael Teichmann and Fred H. Hamker", "title": "A Neural Spiking Approach Compared to Deep Feedforward Networks on\n  Stepwise Pixel Erasement", "comments": "Published in ICANN 2018: Artificial Neural Networks and Machine\n  Learning - ICANN 2018\n  https://link.springer.com/chapter/10.1007/978-3-030-01418-6_25 The final\n  authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-01418-6_25", "journal-ref": null, "doi": "10.1007/978-3-030-01418-6_25", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world scenarios, objects are often partially occluded. This requires\na robustness for object recognition against these perturbations. Convolutional\nnetworks have shown good performances in classification tasks. The learned\nconvolutional filters seem similar to receptive fields of simple cells found in\nthe primary visual cortex. Alternatively, spiking neural networks are more\nbiological plausible. We developed a two layer spiking network, trained on\nnatural scenes with a biologically plausible learning rule. It is compared to\ntwo deep convolutional neural networks using a classification task of stepwise\npixel erasement on MNIST. In comparison to these networks the spiking approach\nachieves good accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:08:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Larisch", "Ren\u00e9", ""], ["Teichmann", "Michael", ""], ["Hamker", "Fred H.", ""]]}, {"id": "1912.03216", "submitter": "Daouda Diouf Dr", "authors": "Daouda Diouf and Djibril Seck", "title": "Modeling the Chlorophyll-a from Sea Surface Reflectance in West Africa\n  by Deep Learning Methods: A Comparison of Multiple Algorithms", "comments": "8 pages, 4 figures, 1 table", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA) Vol.10, No.6, November 2019", "doi": "10.5121/ijaia.2019.10603", "report-no": null, "categories": "cs.NE cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning provide successful applications in many fields. Recently,\nmachines learning are involved for oceans remote sensing applications. In this\nstudy, we use and compare about eight (8) deep learning estimators for\nretrieval of a mainly pigment of phytoplankton. Depending on the water case and\nthe multiple instruments simultaneouslyobserving the earth on a variety of\nplatforms, several algorithm are used to estimate the chlolophyll-a from marine\nreflectance. By using a long-term multi-sensor time-series of satellite\nocean-colour data, as MODIS, SeaWifs, VIIRS, MERIS, etc, we make a unique deep\nnetwork model able to establish a relationship between sea surface reflectance\nand chlorophyll-a from any measurement satellite sensor over West Africa. These\ndata fusion take into account the bias between case water and instruments. We\nconstruct several chlorophyll-a concentration prediction deep learning based\nmodels, compare them and therefore use the best for our study. Results obtained\nfor accuracy training and test are quite good. The mean absolute error are very\nlow and vary between 0,07 to 0,13 mg/m3.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:36:18 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Diouf", "Daouda", ""], ["Seck", "Djibril", ""]]}, {"id": "1912.03310", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Hanlin Goh, Ruslan Salakhutdinov", "title": "Geometric Capsule Autoencoders for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn object representations from 3D point clouds\nusing bundles of geometrically interpretable hidden units, which we call\ngeometric capsules. Each geometric capsule represents a visual entity, such as\nan object or a part, and consists of two components: a pose and a feature. The\npose encodes where the entity is, while the feature encodes what it is. We use\nthese capsules to construct a Geometric Capsule Autoencoder that learns to\ngroup 3D points into parts (small local surfaces), and these parts into the\nwhole object, in an unsupervised manner. Our novel Multi-View Agreement voting\nmechanism is used to discover an object's canonical pose and its pose-invariant\nfeature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the\nproperties of the learned representations and show the benefits of having\nmultiple votes agree. We perform alignment and retrieval of arbitrarily rotated\nobjects -- tasks that evaluate our model's object identification and canonical\npose recovery capabilities -- and obtained insightful results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:10:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Srivastava", "Nitish", ""], ["Goh", "Hanlin", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1912.03341", "submitter": "Jose Manuel Vera", "authors": "Jose Manuel Vera and Andres G. Abad", "title": "Deep Reinforcement Learning for Routing a Heterogeneous Fleet of\n  Vehicles", "comments": "6th Latin American Conference on Computational Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the promising advances of deep-reinforcement learning (DRL)\napplied to cooperative multi-agent systems we propose a model and learning\nprocedure to solve the Capacitated Multi-Vehicle Routing Problem (CMVRP) with\nfixed fleet size. Our learning procedure follows a centralized-training and\ndecentralized-execution paradigm. We empirically test our model and showed its\ncapability for producing near-optimal solutions through cooperative actions. In\nlarge instances, our model generates better solutions than other commonly used\nheuristics. Additionally, our model can solve arbitrary instances of the CMVRP\nwithout requiring re-training.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 20:58:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Vera", "Jose Manuel", ""], ["Abad", "Andres G.", ""]]}, {"id": "1912.03347", "submitter": "Jose Fontanari", "authors": "Sandro M. Reia, Larissa F. Aquino and Jos\\'e F. Fontanari", "title": "The surprising little effectiveness of cooperative algorithms in\n  parallel problem solving", "comments": null, "journal-ref": "Eur. Phys. J. B (2020) 93: 140", "doi": "10.1140/epjb/e2020-10199-9", "report-no": null, "categories": "cs.MA cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological and cultural inspired optimization algorithms are nowadays part of\nthe basic toolkit of a great many research domains. By mimicking processes in\nnature and animal societies, these general-purpose search algorithms promise to\ndeliver optimal or near-optimal solutions using hardly any information on the\noptimization problems they are set to tackle. Here we study the performances of\na cultural-inspired algorithm -- the imitative learning search -- as well as of\nasexual and sexual variants of evolutionary algorithms in finding the global\nmaxima of NK-fitness landscapes. The main performance measure is the total\nnumber of agent updates required by the algorithms to find those global maxima\nand the baseline performance, which establishes the effectiveness of the\ncooperative algorithms, is set by the blind search in which the agents explore\nthe problem space (binary strings) by flipping bits at random. We find that\neven for smooth landscapes that exhibit a single maximum, the evolutionary\nalgorithms do not perform much better than the blind search due to the\nstochastic effects of the genetic roulette. The imitative learning is immune to\nthis effect thanks to the deterministic choice of the fittest string in the\npopulation, which is used as a model for imitation. The tradeoff is that for\nrugged landscapes the imitative learning search is more prone to be trapped in\nlocal maxima than the evolutionary algorithms. In fact, in the case of rugged\nlandscapes with a mild density of local maxima, the blind search either beats\nor matches the cooperative algorithms regardless of whether the task is to find\nthe global maximum or to find the fittest state within a given runtime.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:22:42 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 16:01:51 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 17:37:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Reia", "Sandro M.", ""], ["Aquino", "Larissa F.", ""], ["Fontanari", "Jos\u00e9 F.", ""]]}, {"id": "1912.03395", "submitter": "Jakub Otwinowski", "authors": "Jakub Otwinowski, Colin LaMont", "title": "Information-geometric optimization with natural selection", "comments": "changed title", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms, inspired by natural evolution, aim to optimize\ndifficult objective functions without computing derivatives. Here we detail the\nrelationship between population genetics and evolutionary optimization and\nformulate a new evolutionary algorithm. Optimization of a continuous objective\nfunction is analogous to searching for high fitness phenotypes on a fitness\nlandscape. We summarize how natural selection moves a population along the\nnon-euclidean gradient that is induced by the population on the fitness\nlandscape (the natural gradient). Under normal approximations common in\nquantitative genetics, we show how selection is related to Newton's method in\noptimization. We find that intermediate selection is most informative of the\nfitness landscape. We describe the generation of new phenotypes and introduce\nan operator that recombines the whole population to generate variants that\npreserve normal statistics. Finally, we introduce a proof-of-principle\nalgorithm that combines natural selection, our recombination operator, and an\nadaptive method to increase selection. Our algorithm is similar to covariance\nmatrix adaptation and natural evolutionary strategies in optimization, and has\nsimilar performance. The algorithm is extremely simple in implementation with\nno matrix inversion or factorization, does not require storing a covariance\nmatrix, and may form the basis of more general model-based optimization\nalgorithms with natural gradient updates.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 23:57:16 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 23:52:24 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Otwinowski", "Jakub", ""], ["LaMont", "Colin", ""]]}, {"id": "1912.03918", "submitter": "Uddeshya Upadhyay", "authors": "Uddeshya Upadhyay, Nikunj Shah, Sucheta Ravikanti, Mayanka Medhe", "title": "Transformer Based Reinforcement Learning For Games", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent times have witnessed sharp improvements in reinforcement learning\ntasks using deep reinforcement learning techniques like Deep Q Networks, Policy\nGradients, Actor Critic methods which are based on deep learning based models\nand back-propagation of gradients to train such models. An active area of\nresearch in reinforcement learning is about training agents to play complex\nvideo games, which so far has been something accomplished only by human\nintelligence. Some state of the art performances in video game playing using\ndeep reinforcement learning are obtained by processing the sequence of frames\nfrom video games, passing them through a convolutional network to obtain\nfeatures and then using recurrent neural networks to figure out the action\nleading to optimal rewards. The recurrent neural network will learn to extract\nthe meaningful signal out of the sequence of such features. In this work, we\npropose a method utilizing a transformer network which have recently replaced\nRNNs in Natural Language Processing (NLP), and perform experiments to compare\nwith existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:35:48 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Upadhyay", "Uddeshya", ""], ["Shah", "Nikunj", ""], ["Ravikanti", "Sucheta", ""], ["Medhe", "Mayanka", ""]]}, {"id": "1912.03937", "submitter": "Johannes M\\\"uller", "authors": "Johannes M\\\"uller, Marius Zeinhofer", "title": "Deep Ritz revisited", "comments": "10 pages, work in progress, corrected typos in the second version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA cs.NE math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, progress has been made in the application of neural networks to the\nnumerical analysis of partial differential equations (PDEs). In the latter the\nvariational formulation of the Poisson problem is used in order to obtain an\nobjective function - a regularised Dirichlet energy - that was used for the\noptimisation of some neural networks. In this notes we use the notion of\n$\\Gamma$-convergence to show that ReLU networks of growing architecture that\nare trained with respect to suitably regularised Dirichlet energies converge to\nthe true solution of the Poisson problem. We discuss how this approach\ngeneralises to arbitrary variational problems under certain universality\nassumptions of neural networks and see that this covers some nonlinear\nstationary PDEs like the $p$-Laplace.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:59:38 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 19:25:16 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["M\u00fcller", "Johannes", ""], ["Zeinhofer", "Marius", ""]]}, {"id": "1912.03959", "submitter": "Eli (Omid) David", "authors": "Itay Mosafi, Eli David, Nathan S. Netanyahu", "title": "Stealing Knowledge from Protected Deep Neural Networks Using Composite\n  Unlabeled Data", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), pages\n  1-8, Budapest, Hungary, July 2019", "doi": "10.1109/IJCNN.2019.8851798", "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As state-of-the-art deep neural networks are deployed at the core of more\nadvanced Al-based products and services, the incentive for copying them (i.e.,\ntheir intellectual properties) by rival adversaries is expected to increase\nconsiderably over time. The best way to extract or steal knowledge from such\nnetworks is by querying them using a large dataset of random samples and\nrecording their output, followed by training a student network to mimic these\noutputs, without making any assumption about the original networks. The most\neffective way to protect against such a mimicking attack is to provide only the\nclassification result, without confidence values associated with the softmax\nlayer.In this paper, we present a novel method for generating composite images\nfor attacking a mentor neural network using a student model. Our method assumes\nno information regarding the mentor's training dataset, architecture, or\nweights. Further assuming no information regarding the mentor's softmax output\nvalues, our method successfully mimics the given neural network and steals all\nof its knowledge. We also demonstrate that our student network (which copies\nthe mentor) is impervious to watermarking protection methods, and thus would\nnot be detected as a stolen model.Our results imply, essentially, that all\ncurrent neural networks are vulnerable to mimicking attacks, even if they do\nnot divulge anything but the most basic required output, and that the student\nmodel which mimics them cannot be easily detected and singled out as a stolen\ncopy using currently available techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 10:57:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Mosafi", "Itay", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.04034", "submitter": "Swapnil Kumar", "authors": "Sasikanth Goteti, Swapnil Kumar", "title": "Novel Approach for Solving a Variant of Equal Flow Problem", "comments": "10 pages", "journal-ref": null, "doi": "10.7287/peerj.preprints.27264v1", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider a certain sub class of Integer Equal Flow\nproblem, which are known NP hard [8]. Currently there exist no direct solutions\nfor the same. It is a common problem in various inventory management systems.\nHere we discuss a local minima solution which uses projection of the convex\nspaces to resolve the equal flows and turn the problem into a known linear\ninteger programming or constraint satisfaction problem which have reasonable\nknown solutions and can be effectively solved using simplex or other standard\noptimization strategies.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:38:44 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:39:13 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Goteti", "Sasikanth", ""], ["Kumar", "Swapnil", ""]]}, {"id": "1912.04068", "submitter": "Farid Kenarangi", "authors": "Farid Kenarangi, Xuan Hu, Yihan Liu, Jean Anne C. Incorvia, Joseph S.\n  Friedman, Inna Partin-Vaisband", "title": "Exploiting Dual-Gate Ambipolar CNFETs for Scalable Machine Learning\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambipolar carbon nanotube based field-effect transistors (AP-CNFETs) exhibit\nunique electrical characteristics, such as tri-state operation and\nbi-directionality, enabling systems with complex and reconfigurable computing.\nIn this paper, AP-CNFETs are used to design a mixed-signal machine learning\n(ML) classifier. The classifier is designed in SPICE with feature size of 15 nm\nand operates at 250 MHz. The system is demonstrated based on MNIST digit\ndataset, yielding 90% accuracy and no accuracy degradation as compared with the\nclassification of this dataset in Python. The system also exhibits lower power\nconsumption and smaller physical size as compared with the state-of-the-art\nCMOS and memristor based mixed-signal classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:15:40 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Kenarangi", "Farid", ""], ["Hu", "Xuan", ""], ["Liu", "Yihan", ""], ["Incorvia", "Jean Anne C.", ""], ["Friedman", "Joseph S.", ""], ["Partin-Vaisband", "Inna", ""]]}, {"id": "1912.04161", "submitter": "Hanten Chang", "authors": "Hanten Chang and Katsuya Futagami", "title": "Reinforcement Learning with Convolutional Reservoir Computing", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.08040", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, reinforcement learning models have achieved great success,\nmastering complex tasks such as Go and other games with higher scores than\nhuman players. Many of these models store considerable data on the tasks and\nachieve high performance by extracting visual and time-series features using\nconvolutional neural networks (CNNs) and recurrent neural networks,\nrespectively. However, these networks have very high computational costs\nbecause they need to be trained by repeatedly using the stored data. In this\nstudy, we propose a novel practical approach called reinforcement learning with\nconvolutional reservoir computing (RCRC) model. The RCRC model uses a fixed\nrandom-weight CNN and a reservoir computing model to extract visual and\ntime-series features. Using these extracted features, it decides actions with\nan evolution strategy method. Thereby, the RCRC model has several desirable\nfeatures: (1) there is no need to train the feature extractor, (2) there is no\nneed to store training data, (3) it can take a wide range of actions, and (4)\nthere is only a single task-dependent weight parameter to be trained.\nFurthermore, we show the RCRC model can solve multiple reinforcement learning\ntasks with a completely identical feature extractor.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 19:59:57 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chang", "Hanten", ""], ["Futagami", "Katsuya", ""]]}, {"id": "1912.04246", "submitter": "Santosh Manicka", "authors": "Santosh Manicka and Michael Levin", "title": "Modeling somatic computation with non-neural bioelectric networks", "comments": "30 pages, 8 figures in main article", "journal-ref": "Sci Rep 9, 18612 (2019)", "doi": "10.1038/s41598-019-54859-8", "report-no": null, "categories": "q-bio.NC cs.NE nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of basal cognition seeks to understand how adaptive,\ncontext-specific behavior occurs in non-neural biological systems.\nEmbryogenesis and regeneration require plasticity in many tissue types to\nachieve structural and functional goals in diverse circumstances. Thus,\nadvances in both evolutionary cell biology and regenerative medicine require an\nunderstanding of how non-neural tissues could process information. Neurons\nevolved from ancient cell types that used bioelectric signaling to perform\ncomputation. However, it has not been shown whether or how non-neural\nbioelectric cell networks can support computation. We generalize connectionist\nmethods to non-neural tissue architectures, showing that a minimal non-neural\nBio-Electric Network (BEN) model that utilizes the general principles of\nbioelectricity (electrodiffusion and gating) can compute. We characterize BEN\nbehaviors ranging from elementary logic gates to pattern detectors, using both\nfixed and transient inputs to recapitulate various biological scenarios. We\ncharacterize the mechanisms of such networks using dynamical-systems and\ninformation-theory tools, demonstrating that logic can manifest in\nbidirectional, continuous, and relatively slow bioelectrical systems,\ncomplementing conventional neural-centric architectures. Our results reveal a\nvariety of non-neural decision-making processes as manifestations of general\ncellular biophysical mechanisms and suggest novel bioengineering approaches to\nconstruct functional tissues for regenerative medicine and synthetic biology as\nwell as new machine learning architectures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:36:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Manicka", "Santosh", ""], ["Levin", "Michael", ""]]}, {"id": "1912.04508", "submitter": "Mohammed Amer", "authors": "Mohammed Amer, Tom\\'as Maul", "title": "Reducing Catastrophic Forgetting in Modular Neural Networks by Dynamic\n  Information Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning is a very important step toward realizing robust autonomous\nartificial agents. Neural networks are the main engine of deep learning, which\nis the current state-of-the-art technique in formulating adaptive artificial\nintelligent systems. However, neural networks suffer from catastrophic\nforgetting when stressed with the challenge of continual learning. We\ninvestigate how to exploit modular topology in neural networks in order to\ndynamically balance the information load between different modules by routing\ninputs based on the information content in each module so that information\ninterference is minimized. Our dynamic information balancing (DIB) technique\nadapts a reinforcement learning technique to guide the routing of different\ninputs based on a reward signal derived from a measure of the information load\nin each module. Our empirical results show that DIB combined with elastic\nweight consolidation (EWC) regularization outperforms models with similar\ncapacity and EWC regularization across different task formulations and\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 05:41:44 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Amer", "Mohammed", ""], ["Maul", "Tom\u00e1s", ""]]}, {"id": "1912.04635", "submitter": "Alessandro Betti", "authors": "Alessandro Betti and Marco Gori", "title": "Backprop Diffusion is Biologically Plausible", "comments": "9 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1907.05106", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Backpropagation algorithm relies on the abstraction of using a neural\nmodel that gets rid of the notion of time, since the input is mapped\ninstantaneously to the output. In this paper, we claim that this abstraction of\nignoring time, along with the abrupt input changes that occur when feeding the\ntraining set, are in fact the reasons why, in some papers, Backprop biological\nplausibility is regarded as an arguable issue. We show that as soon as a deep\nfeedforward network operates with neurons with time-delayed response, the\nbackprop weight update turns out to be the basic equation of a biologically\nplausible diffusion process based on forward-backward waves. We also show that\nsuch a process very well approximates the gradient for inputs that are not too\nfast with respect to the depth of the network. These remarks somewhat disclose\nthe diffusion process behind the backprop equation and leads us to interpret\nthe corresponding algorithm as a degeneration of a more general diffusion\nprocess that takes place also in neural networks with cyclic connections.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:50:15 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 10:04:48 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""]]}, {"id": "1912.04825", "submitter": "Samuel Kim", "authors": "Samuel Kim, Peter Y. Lu, Srijon Mukherjee, Michael Gilbert, Li Jing,\n  Vladimir \\v{C}eperi\\'c, and Marin Solja\\v{c}i\\'c", "title": "Integration of Neural Network-Based Symbolic Regression in Deep Learning\n  for Scientific Discovery", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression is a powerful technique that can discover analytical\nequations that describe data, which can lead to explainable models and\ngeneralizability outside of the training data set. In contrast, neural networks\nhave achieved amazing levels of accuracy on image recognition and natural\nlanguage processing tasks, but are often seen as black-box models that are\ndifficult to interpret and typically extrapolate poorly. Here we use a neural\nnetwork-based architecture for symbolic regression called the Equation Learner\n(EQL) network and integrate it with other deep learning architectures such that\nthe whole system can be trained end-to-end through backpropagation. To\ndemonstrate the power of such systems, we study their performance on several\nsubstantially different tasks. First, we show that the neural network can\nperform symbolic regression and learn the form of several functions. Next, we\npresent an MNIST arithmetic task where a separate part of the neural network\nextracts the digits. Finally, we demonstrate prediction of dynamical systems\nwhere an unknown parameter is extracted through an encoder. We find that the\nEQL-based architecture can extrapolate quite well outside of the training data\nset compared to a standard neural network-based architecture, paving the way\nfor deep learning to be applied in scientific exploration and discovery.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:07:52 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 18:40:43 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kim", "Samuel", ""], ["Lu", "Peter Y.", ""], ["Mukherjee", "Srijon", ""], ["Gilbert", "Michael", ""], ["Jing", "Li", ""], ["\u010ceperi\u0107", "Vladimir", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "1912.04958", "submitter": "Samuli Laine", "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko\n  Lehtinen, Timo Aila", "title": "Analyzing and Improving the Image Quality of StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:44:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 17:21:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Karras", "Tero", ""], ["Laine", "Samuli", ""], ["Aittala", "Miika", ""], ["Hellsten", "Janne", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "1912.04968", "submitter": "David Ahmedt-Aristizabal", "authors": "David Ahmedt-Aristizabal, Tharindu Fernando, Simon Denman, Lars\n  Petersson, Matthew J. Aburn, Clinton Fookes", "title": "Neural Memory Networks for Seizure Type Classification", "comments": "Proceedings of the IEEE International Conference of Engineering in\n  Medicine and Biology Society. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of seizure type is a key step in the clinical process for\nevaluating an individual who presents with seizures. It determines the course\nof clinical diagnosis and treatment, and its impact stretches beyond the\nclinical domain to epilepsy research and the development of novel therapies.\nAutomated identification of seizure type may facilitate understanding of the\ndisease, and seizure detection and prediction has been the focus of recent\nresearch that has sought to exploit the benefits of machine learning and deep\nlearning architectures. Nevertheless, there is not yet a definitive solution\nfor automating the classification of seizure type, a task that must currently\nbe performed by an expert epileptologist. Inspired by recent advances in neural\nmemory networks (NMNs), we introduce a novel approach for the classification of\nseizure type using electrophysiological data. We first explore the performance\nof traditional deep learning techniques which use convolutional and recurrent\nneural networks, and enhance these architectures by using external memory\nmodules with trainable neural plasticity. We show that our model achieves a\nstate-of-the-art weighted F1 score of 0.945 for seizure type classification on\nthe TUH EEG Seizure Corpus with the IBM TUSZ preprocessed data. This work\nhighlights the potential of neural memory networks to support the field of\nepilepsy research, along with biomedical research and signal analysis more\nbroadly.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:27:40 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 02:04:44 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ahmedt-Aristizabal", "David", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Petersson", "Lars", ""], ["Aburn", "Matthew J.", ""], ["Fookes", "Clinton", ""]]}, {"id": "1912.05063", "submitter": "Aaron Eberhart", "authors": "Aaron Eberhart, Monireh Ebrahimi, Lu Zhou, Cogan Shimizu, and Pascal\n  Hitzler", "title": "Completion Reasoning Emulation for the Description Logic EL+", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to integrating deep learning with knowledge-based\nsystems that we believe shows promise. Our approach seeks to emulate reasoning\nstructure, which can be inspected part-way through, rather than simply learning\nreasoner answers, which is typical in many of the black-box systems currently\nin use. We demonstrate that this idea is feasible by training a long short-term\nmemory (LSTM) artificial neural network to learn EL+ reasoning patterns with\ntwo different data sets. We also show that this trained system is resistant to\nnoise by corrupting a percentage of the test data and comparing the reasoner's\nand LSTM's predictions on corrupt data with correct answers.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 00:29:18 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Eberhart", "Aaron", ""], ["Ebrahimi", "Monireh", ""], ["Zhou", "Lu", ""], ["Shimizu", "Cogan", ""], ["Hitzler", "Pascal", ""]]}, {"id": "1912.05198", "submitter": "Angshul Majumdar Dr.", "authors": "Megha Gupta and Angshul Majumdar", "title": "Recurrent Transform Learning", "comments": "A slightly different version has been accepted at Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to improve the accuracy of building demand\nforecasting. This is a more challenging task than grid level forecasting. For\nthe said purpose, we develop a new technique called recurrent transform\nlearning (RTL). Two versions are proposed. The first one (RTL) is unsupervised;\nthis is used as a feature extraction tool that is further fed into a regression\nmodel. The second formulation embeds regression into the RTL framework leading\nto regressing recurrent transform learning (R2TL). Forecasting experiments have\nbeen carried out on three popular publicly available datasets. Both of our\nproposed techniques yield results superior to the state-of-the-art like long\nshort term memory network, echo state network and sparse coding regression.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:29:57 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Gupta", "Megha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.05239", "submitter": "Stefano Nolfi", "authors": "Paolo Pagliuca, Nicola Milano, and Stefano Nolfi", "title": "Efficacy of Modern Neuro-Evolutionary Strategies for Continuous Control\n  Optimization", "comments": "17 pages, 5 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the efficacy of modern neuro-evolutionary strategies for\ncontinuous control optimization. Overall, the results collected on a wide\nvariety of qualitatively different benchmark problems indicate that these\nmethods are generally effective and scale well with respect to the number of\nparameters and the complexity of the problem. Moreover, they are relatively\nrobust with respect to the setting of hyper-parameters. The comparison of the\nmost promising methods indicates that the OpenAI-ES algorithm outperforms or\nequals the other algorithms on all considered problems. Moreover, we\ndemonstrate how the reward functions optimized for reinforcement learning\nmethods are not necessarily effective for evolutionary strategies and vice\nversa. This finding can lead to reconsideration of the relative efficacy of the\ntwo classes of algorithm since it implies that the comparisons performed to\ndate are biased toward one or the other class.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:29:12 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 09:50:08 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Pagliuca", "Paolo", ""], ["Milano", "Nicola", ""], ["Nolfi", "Stefano", ""]]}, {"id": "1912.05416", "submitter": "Xiaolong Ma", "authors": "Geng Yuan, Xiaolong Ma, Sheng Lin, Zhengang Li, Caiwen Ding", "title": "A SOT-MRAM-based Processing-In-Memory Engine for Highly Compressed DNN\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.DC cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computing wall and data movement challenges of deep neural networks\n(DNNs) have exposed the limitations of conventional CMOS-based DNN\naccelerators. Furthermore, the deep structure and large model size will make\nDNNs prohibitive to embedded systems and IoT devices, where low power\nconsumption are required. To address these challenges, spin orbit torque\nmagnetic random-access memory (SOT-MRAM) and SOT-MRAM based\nProcessing-In-Memory (PIM) engines have been used to reduce the power\nconsumption of DNNs since SOT-MRAM has the characteristic of near-zero standby\npower, high density, none-volatile. However, the drawbacks of SOT-MRAM based\nPIM engines such as high writing latency and requiring low bit-width data\ndecrease its popularity as a favorable energy efficient DNN accelerator. To\nmitigate these drawbacks, we propose an ultra energy efficient framework by\nusing model compression techniques including weight pruning and quantization\nfrom the software level considering the architecture of SOT-MRAM PIM. And we\nincorporate the alternating direction method of multipliers (ADMM) into the\ntraining phase to further guarantee the solution feasibility and satisfy\nSOT-MRAM hardware constraints. Thus, the footprint and power consumption of\nSOT-MRAM PIM can be reduced, while increasing the overall system throughput at\nthe meantime, making our proposed ADMM-based SOT-MRAM PIM more energy\nefficiency and suitable for embedded systems or IoT devices. Our experimental\nresults show the accuracy and compression rate of our proposed framework is\nconsistently outperforming the reference works, while the efficiency (area \\&\npower) and throughput of SOT-MRAM PIM engine is significantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:03:26 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Li", "Zhengang", ""], ["Ding", "Caiwen", ""]]}, {"id": "1912.05671", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael\n  Carbin", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis", "comments": "Published in ICML 2020. This submission subsumes arXiv:1903.01611\n  (\"Stabilizing the Lottery Ticket Hypothesis\" and \"The Lottery Ticket\n  Hypothesis at Scale\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study whether a neural network optimizes to the same, linearly connected\nminimum under different samples of SGD noise (e.g., random data order and\naugmentation). We find that standard vision models become stable to SGD noise\nin this way early in training. From then on, the outcome of optimization is\ndetermined to a linearly connected region. We use this technique to study\niterative magnitude pruning (IMP), the procedure used by work on the lottery\nticket hypothesis to identify subnetworks that could have trained in isolation\nto full accuracy. We find that these subnetworks only reach full accuracy when\nthey are stable to SGD noise, which either occurs at initialization for\nsmall-scale settings (MNIST) or early in training for large-scale settings\n(ResNet-50 and Inception-v3 on ImageNet).\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:22:21 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 20:39:29 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 19:36:46 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 20:31:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Frankle", "Jonathan", ""], ["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Carbin", "Michael", ""]]}, {"id": "1912.05699", "submitter": "Alvin Chan", "authors": "Alvin Chan, Yi Tay and Yew-Soon Ong", "title": "What it Thinks is Important is Important: Robustness Transfers through\n  Input Gradients", "comments": "Accepted as Oral in CVPR 2020, Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial perturbations are imperceptible changes to input pixels that can\nchange the prediction of deep learning models. Learned weights of models robust\nto such perturbations are previously found to be transferable across different\ntasks but this applies only if the model architecture for the source and target\ntasks is the same. Input gradients characterize how small changes at each input\npixel affect the model output. Using only natural images, we show here that\ntraining a student model's input gradients to match those of a robust teacher\nmodel can gain robustness close to a strong baseline that is robustly trained\nfrom scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and\nTiny-ImageNet, we show that our proposed method, input gradient adversarial\nmatching, can transfer robustness across different tasks and even across\ndifferent model architectures. This demonstrates that directly targeting the\nsemantics of input gradients is a feasible way towards adversarial robustness.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:51:37 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 07:50:06 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 13:45:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chan", "Alvin", ""], ["Tay", "Yi", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1912.05831", "submitter": "Shayan Hassantabar", "authors": "Shayan Hassantabar, Xiaoliang Dai, Niraj K. Jha", "title": "STEERAGE: Synthesis of Neural Networks Using Architecture Search and\n  Grow-and-Prune Methods", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks (NNs) have been successfully deployed in many applications.\nHowever, architectural design of these models is still a challenging problem.\nMoreover, neural networks are known to have a lot of redundancy. This increases\nthe computational cost of inference and poses an obstacle to deployment on\nInternet-of-Thing sensors and edge devices. To address these challenges, we\npropose the STEERAGE synthesis methodology. It consists of two complementary\napproaches: efficient architecture search, and grow-and-prune NN synthesis. The\nfirst step, covered in a global search module, uses an accuracy predictor to\nefficiently navigate the architectural search space. The predictor is built\nusing boosted decision tree regression, iterative sampling, and efficient\nevolutionary search. The second step involves local search. By using various\ngrow-and-prune methodologies for synthesizing convolutional and feed-forward\nNNs, it reduces the network redundancy, while boosting its performance. We have\nevaluated STEERAGE performance on various datasets, including MNIST and\nCIFAR-10. On MNIST dataset, our CNN architecture achieves an error rate of\n0.66%, with 8.6x fewer parameters compared to the LeNet-5 baseline. For the\nCIFAR-10 dataset, we used the ResNet architectures as the baseline. Our\nSTEERAGE-synthesized ResNet-18 has a 2.52% accuracy improvement over the\noriginal ResNet-18, 1.74% over ResNet-101, and 0.16% over ResNet-1001, while\nhaving comparable number of parameters and FLOPs to the original ResNet-18.\nThis shows that instead of just increasing the number of layers to increase\naccuracy, an alternative is to use a better NN architecture with fewer layers.\nIn addition, STEERAGE achieves an error rate of just 3.86% with a variant of\nResNet architecture with 40 layers. To the best of our knowledge, this is the\nhighest accuracy obtained by ResNet-based architectures on the CIFAR-10\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:42:13 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Hassantabar", "Shayan", ""], ["Dai", "Xiaoliang", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1912.05869", "submitter": "Dorothea Kolossa", "authors": "Ahmed Hussen Abdelaziz, Shuo-Yiin Chang, Nelson Morgan, Erik Edwards,\n  Dorothea Kolossa, Dan Ellis, David A. Moses, Edward F. Chang", "title": "On Neural Phone Recognition of Mixed-Source ECoG Signals", "comments": "5 pages, showing algorithms, results and references from our\n  collaboration during a 2017 postdoc stay of the first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.NE cs.SD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging field of neural speech recognition (NSR) using\nelectrocorticography has recently attracted remarkable research interest for\nstudying how human brains recognize speech in quiet and noisy surroundings. In\nthis study, we demonstrate the utility of NSR systems to objectively prove the\nability of human beings to attend to a single speech source while suppressing\nthe interfering signals in a simulated cocktail party scenario. The\nexperimental results show that the relative degradation of the NSR system\nperformance when tested in a mixed-source scenario is significantly lower than\nthat of automatic speech recognition (ASR). In this paper, we have\nsignificantly enhanced the performance of our recently published framework by\nusing manual alignments for initialization instead of the flat start technique.\nWe have also improved the NSR system performance by accounting for the possible\ntranscription mismatch between the acoustic and neural signals.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 10:37:22 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Abdelaziz", "Ahmed Hussen", ""], ["Chang", "Shuo-Yiin", ""], ["Morgan", "Nelson", ""], ["Edwards", "Erik", ""], ["Kolossa", "Dorothea", ""], ["Ellis", "Dan", ""], ["Moses", "David A.", ""], ["Chang", "Edward F.", ""]]}, {"id": "1912.05899", "submitter": "Diederick Vermetten", "authors": "Diederick Vermetten, Hao Wang, Carola Doerr, Thomas B\\\"ack", "title": "Sequential vs. Integrated Algorithm Selection and Configuration: A Case\n  Study for the Modular CMA-ES", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with a specific optimization problem, choosing which algorithm to\nuse is always a tough task. Not only is there a vast variety of algorithms to\nselect from, but these algorithms often are controlled by many hyperparameters,\nwhich need to be tuned in order to achieve the best performance possible.\nUsually, this problem is separated into two parts: algorithm selection and\nalgorithm configuration. With the significant advances made in Machine\nLearning, however, these problems can be integrated into a combined algorithm\nselection and hyperparameter optimization task, commonly known as the CASH\nproblem. In this work we compare sequential and integrated algorithm selection\nand configuration approaches for the case of selecting and tuning the best out\nof 4608 variants of the Covariance Matrix Adaptation Evolution Strategy\n(CMA-ES) tested on the Black Box Optimization Benchmark (BBOB) suite. We first\nshow that the ranking of the modular CMA-ES variants depends to a large extent\non the quality of the hyperparameters. This implies that even a sequential\napproach based on complete enumeration of the algorithm space will likely\nresult in sub-optimal solutions. In fact, we show that the integrated approach\nmanages to provide competitive results at a much smaller computational cost. We\nalso compare two different mixed-integer algorithm configuration techniques,\ncalled irace and Mixed-Integer Parallel Efficient Global Optimization\n(MIP-EGO). While we show that the two methods differ significantly in their\ntreatment of the exploration-exploitation balance, their overall performances\nare very similar.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 12:47:48 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 14:23:26 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Vermetten", "Diederick", ""], ["Wang", "Hao", ""], ["Doerr", "Carola", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1912.06059", "submitter": "Petro Liashchynskyi", "authors": "Petro Liashchynskyi and Pavlo Liashchynskyi", "title": "Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS", "comments": "11 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare the three most popular algorithms for\nhyperparameter optimization (Grid Search, Random Search, and Genetic Algorithm)\nand attempt to use them for neural architecture search (NAS). We use these\nalgorithms for building a convolutional neural network (search architecture).\nExperimental results on CIFAR-10 dataset further demonstrate the performance\ndifference between compared algorithms. The comparison results are based on the\nexecution time of the above algorithms and accuracy of the proposed models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:07:20 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Liashchynskyi", "Petro", ""], ["Liashchynskyi", "Pavlo", ""]]}, {"id": "1912.06172", "submitter": "Victor Costa", "authors": "Victor Costa, Nuno Louren\\c{c}o and Penousal Machado", "title": "Coevolution of Generative Adversarial Networks", "comments": "Published in EvoApplications 2019", "journal-ref": null, "doi": "10.1007/978-3-030-16692-2_32", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GAN) became a hot topic, presenting\nimpressive results in the field of computer vision. However, there are still\nopen problems with the GAN model, such as the training stability and the\nhand-design of architectures. Neuroevolution is a technique that can be used to\nprovide the automatic design of network architectures even in large search\nspaces as in deep neural networks. Therefore, this project proposes COEGAN, a\nmodel that combines neuroevolution and coevolution in the coordination of the\nGAN training algorithm. The proposal uses the adversarial characteristic\nbetween the generator and discriminator components to design an algorithm using\ncoevolution techniques. Our proposal was evaluated in the MNIST dataset. The\nresults suggest the improvement of the training stability and the automatic\ndiscovery of efficient network architectures for GANs. Our model also partially\nsolves the mode collapse problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:29:09 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Costa", "Victor", ""], ["Louren\u00e7o", "Nuno", ""], ["Machado", "Penousal", ""]]}, {"id": "1912.06180", "submitter": "Victor Costa", "authors": "Victor Costa, Nuno Louren\\c{c}o, Jo\\~ao Correia and Penousal Machado", "title": "COEGAN: Evaluating the Coevolution Effect in Generative Adversarial\n  Networks", "comments": "Published in GECCO 2019. arXiv admin note: text overlap with\n  arXiv:1912.06172", "journal-ref": null, "doi": "10.1145/3321707.3321746", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GAN) present state-of-the-art results in the\ngeneration of samples following the distribution of the input dataset. However,\nGANs are difficult to train, and several aspects of the model should be\npreviously designed by hand. Neuroevolution is a well-known technique used to\nprovide the automatic design of network architectures which was recently\nexpanded to deep neural networks. COEGAN is a model that uses neuroevolution\nand coevolution in the GAN training algorithm to provide a more stable training\nmethod and the automatic design of neural network architectures. COEGAN makes\nuse of the adversarial aspect of the GAN components to implement coevolutionary\nstrategies in the training algorithm. Our proposal was evaluated in the\nFashion-MNIST and MNIST dataset. We compare our results with a baseline based\non DCGAN and also with results from a random search algorithm. We show that our\nmethod is able to discover efficient architectures in the Fashion-MNIST and\nMNIST datasets. The results also suggest that COEGAN can be used as a training\nalgorithm for GANs to avoid common issues, such as the mode collapse problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:43:04 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Costa", "Victor", ""], ["Louren\u00e7o", "Nuno", ""], ["Correia", "Jo\u00e3o", ""], ["Machado", "Penousal", ""]]}, {"id": "1912.06208", "submitter": "Olivier Tieleman", "authors": "Olivier Tieleman, Angeliki Lazaridou, Shibl Mourad, Charles Blundell,\n  Doina Precup", "title": "Shaping representations through communication: community size effect in\n  artificial learning systems", "comments": "NeurIPS 2019 workshop on visually grounded interaction and language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by theories of language and communication that explain why\ncommunities with large numbers of speakers have, on average, simpler languages\nwith more regularity, we cast the representation learning problem in terms of\nlearning to communicate. Our starting point sees the traditional autoencoder\nsetup as a single encoder with a fixed decoder partner that must learn to\ncommunicate. Generalizing from there, we introduce community-based autoencoders\nin which multiple encoders and decoders collectively learn representations by\nbeing randomly paired up on successive training iterations. We find that\nincreasing community sizes reduce idiosyncrasies in the learned codes,\nresulting in representations that better encode concept categories and\ncorrelate with human feature norms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:56:59 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tieleman", "Olivier", ""], ["Lazaridou", "Angeliki", ""], ["Mourad", "Shibl", ""], ["Blundell", "Charles", ""], ["Precup", "Doina", ""]]}, {"id": "1912.06310", "submitter": "Shuai Han", "authors": "Shuai L\\\"u and Shuai Han and Wenbo Zhou and Junwei Zhang", "title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning, evolutionary algorithms and imitation learning are\nthree principal methods to deal with continuous control tasks. Reinforcement\nlearning is sample efficient, yet sensitive to hyper-parameters setting and\nneeds efficient exploration; Evolutionary algorithms are stable, but with low\nsample efficiency; Imitation learning is both sample efficient and stable,\nhowever it requires the guidance of expert data. In this paper, we propose\nRecruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning,\na scalable framework that combines advantages of the three methods mentioned\nabove. The core of this framework is a dual-actors and single critic\nreinforcement learning agent. This agent can recruit high-fitness actors from\nthe population of evolutionary algorithms, which instructs itself to learn from\nexperience replay buffer. At the same time, low-fitness actors in the\nevolutionary population can imitate behavior patterns of the reinforcement\nlearning agent and improve their adaptability. Reinforcement and imitation\nlearners in this framework can be replaced with any off-policy actor-critic\nreinforcement learner or data-driven imitation learner. We evaluate RIM on a\nseries of benchmarks for continuous control tasks in Mujoco. The experimental\nresults show that RIM outperforms prior evolutionary or reinforcement learning\nmethods. The performance of RIM's components is significantly better than\ncomponents of previous evolutionary reinforcement learning algorithm, and the\nrecruitment using soft update enables reinforcement learning agent to learn\nfaster than that using hard update.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:26:14 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["L\u00fc", "Shuai", ""], ["Han", "Shuai", ""], ["Zhou", "Wenbo", ""], ["Zhang", "Junwei", ""]]}, {"id": "1912.06472", "submitter": "Thomas Carroll", "authors": "Thomas L. Carroll", "title": "Dimension of Reservoir Computers", "comments": "submitted to Chaos", "journal-ref": "Chaos vol. 30 issue 1 013102 2020", "doi": "10.1063/1.5128898", "report-no": null, "categories": "nlin.AO cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A reservoir computer is a complex dynamical system, often created by coupling\nnonlinear nodes in a network. The nodes are all driven by a common driving\nsignal. In this work, three dimension estimation methods, false nearest\nneighbor, covariance and Kaplan-Yorke dimensions, are used to estimate the\ndimension of the reservoir dynamical system. It is shown that the signals in\nthe reservoir system exist on a relatively low dimensional surface. Changing\nthe spectral radius of the reservoir network can increase the fractal dimension\nof the reservoir signals, leading to an increase in testing error.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 12:14:08 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Carroll", "Thomas L.", ""]]}, {"id": "1912.07242", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran", "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this expository note we describe a surprising phenomenon in\noverparameterized linear regression, where the dimension exceeds the number of\nsamples: there is a regime where the test risk of the estimator found by\ngradient descent increases with additional samples. In other words, more data\nactually hurts the estimator. This behavior is implicit in a recent line of\ntheoretical works analyzing \"double-descent\" phenomenon in linear models. In\nthis note, we isolate and understand this behavior in an extremely simple\nsetting: linear regression with isotropic Gaussian covariates. In particular,\nthis occurs due to an unconventional type of bias-variance tradeoff in the\noverparameterized regime: the bias decreases with more samples, but variance\nincreases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:28:26 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Nakkiran", "Preetum", ""]]}, {"id": "1912.07319", "submitter": "Micha{\\l} Idzik", "authors": "Micha{\\l} Idzik", "title": "Multi-Objective Evolutionary Algorithms platform with support for\n  flexible hybridization tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with complex, high-level MOEA meta-models such as Multiobjec-tive\nOptimization Hierarchic Genetic Strategy (MO-mHGS) with multi-deme support\nusually requires dedicated implementation and configuration for each internal\n(single-deme) algorithm variant. If we generalize meta-model, we can simplify\nwhole simulation process and bind any internal algorithm (we denote it as a\ndriver), without providing redundant meta-model implementations. This idea has\nbecome a fundamental of Evogil platform. Our aim was to allow construct-ing\ncustom hybrid models or combine existing solutions in runtime simulation\nenvironment. We define hybrid solution as a composition of a meta-model and a\ndriver (or multiple drivers). Meta-model uses drivers to perform evolutionary\ncalculations and process their results. Moreover, Evogil provides set of\nready-made solutions divided into two groups (multi-deme meta-models and\nsingle-deme drivers), as well as processing tools (quality metrics, statistics\nand plotting scripts), simulation management and results persistence layer.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 12:32:21 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Idzik", "Micha\u0142", ""]]}, {"id": "1912.07388", "submitter": "Daouda Diouf Dr", "authors": "Daouda Diouf, Awa Niang and Sylvie Thiria", "title": "Deep Learning based Multiple Regression to Predict Total Column Water\n  Vapor (TCWV) from Physical Parameters in West Africa by using Keras Library", "comments": "8pages, 6 figures, 3 tables", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process, Vol.9, No.6, November 2019", "doi": "10.5121/ijdkp.2019.9602", "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total column water vapor is an important factor for the weather and climate.\nThis study apply deep learning based multiple regression to map the TCWV with\nelements that can improve spatiotemporal prediction. In this study, we predict\nthe TCWV with the use of ERA5 that is the fifth generation ECMWF atmospheric\nreanalysis of the global climate. We use an appropriate deep learning based\nmultiple regression algorithm using Keras library to improve nonlinear\nprediction between Total Column water vapor and predictors as Mean sea level\npressure, Surface pressure, Sea surface temperature, 100 metre U wind\ncomponent, 100 metre V wind component, 10 metre U wind component, 10 metre V\nwind component, 2 metre dew point temperature, 2 metre temperature. The results\nobtained permit to build a predictor which modelling TCWV with a mean abs error\n(MAE) equal to 3.60 kg/m2 and a coefficient of determination R2 equal to 0.90.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 10:18:12 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Diouf", "Daouda", ""], ["Niang", "Awa", ""], ["Thiria", "Sylvie", ""]]}, {"id": "1912.07423", "submitter": "Dennis Bautembach", "authors": "Dennis Bautembach, Iason Oikonomidis, Nikolaos Kyriazis, Antonis\n  Argyros", "title": "Faster and Simpler SNN Simulation with Work Queues", "comments": "Camera-ready version, as accepted by IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a clock-driven Spiking Neural Network simulator which is up to 3x\nfaster than the state of the art while, at the same time, being more general\nand requiring less programming effort on both the user's and maintainer's side.\nThis is made possible by designing our pipeline around \"work queues\" which act\nas interfaces between stages and greatly reduce implementation complexity. We\nevaluate our work using three well-established SNN models on a series of\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:49:37 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 11:16:47 GMT"}, {"version": "v3", "created": "Sun, 24 May 2020 17:42:31 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bautembach", "Dennis", ""], ["Oikonomidis", "Iason", ""], ["Kyriazis", "Nikolaos", ""], ["Argyros", "Antonis", ""]]}, {"id": "1912.07589", "submitter": "Paul Bertens", "authors": "Paul Bertens, Seong-Whan Lee", "title": "Network of Evolvable Neural Units: Evolving to Learn at a Synaptic Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Deep Neural Networks have seen great success in recent years through\nvarious changes in overall architectures and optimization strategies, their\nfundamental underlying design remains largely unchanged. Computational\nneuroscience on the other hand provides more biologically realistic models of\nneural processing mechanisms, but they are still high level abstractions of the\nactual experimentally observed behaviour. Here a model is proposed that bridges\nNeuroscience, Machine Learning and Evolutionary Algorithms to evolve individual\nsoma and synaptic compartment models of neurons in a scalable manner. Instead\nof attempting to manually derive models for all the observed complexity and\ndiversity in neural processing, we propose an Evolvable Neural Unit (ENU) that\ncan approximate the function of each individual neuron and synapse. We\ndemonstrate that this type of unit can be evolved to mimic Integrate-And-Fire\nneurons and synaptic Spike-Timing-Dependent Plasticity. Additionally, by\nconstructing a new type of neural network where each synapse and neuron is such\nan evolvable neural unit, we show it is possible to evolve an agent capable of\nlearning to solve a T-maze environment task. This network independently\ndiscovers spiking dynamics and reinforcement type learning rules, opening up a\nnew path towards biologically inspired artificial intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:57:50 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bertens", "Paul", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "1912.07712", "submitter": "Marco Ciccone", "authors": "Andrea Celli, Marco Ciccone, Raffaele Bongo, Nicola Gatti", "title": "Coordination in Adversarial Sequential Team Games via Multi-Agent Deep\n  Reinforcement Learning", "comments": "Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications involve teams of agents that have to coordinate\ntheir actions to reach a common goal against potential adversaries. This paper\nfocuses on zero-sum games where a team of players faces an opponent, as is the\ncase, for example, in Bridge, collusion in poker, and collusion in bidding. The\npossibility for the team members to communicate before gameplay---that is,\ncoordinate their strategies ex ante---makes the use of behavioral strategies\nunsatisfactory. We introduce Soft Team Actor-Critic (STAC) as a solution to the\nteam's coordination problem that does not require any prior domain knowledge.\nSTAC allows team members to effectively exploit ex ante communication via\nexogenous signals that are shared among the team. STAC reaches near-optimal\ncoordinated strategies both in perfectly observable and partially observable\ngames, where previous deep RL algorithms fail to reach optimal coordinated\nbehaviors.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 21:30:04 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Celli", "Andrea", ""], ["Ciccone", "Marco", ""], ["Bongo", "Raffaele", ""], ["Gatti", "Nicola", ""]]}, {"id": "1912.08116", "submitter": "Mariah Schrum", "authors": "Mariah Schrum, Matthew Gombolay", "title": "When Your Robot Breaks: Active Learning During Plant Failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and adapting to catastrophic failures in robotic systems requires a\nrobot to learn its new dynamics quickly and safely to best accomplish its\ngoals. To address this challenging problem, we propose probabilistically-safe,\nonline learning techniques to infer the altered dynamics of a robot at the\nmoment a failure (e.g., physical damage) occurs. We combine model predictive\ncontrol and active learning within a chance-constrained optimization framework\nto safely and efficiently learn the new plant model of the robot. We leverage a\nneural network for function approximation in learning the latent dynamics of\nthe robot under failure conditions. Our framework generalizes to various damage\nconditions while being computationally light-weight to advance real-time\ndeployment. We empirically validate within a virtual environment that we can\nregain control of a severely damaged aircraft in seconds and require only 0.1\nseconds to find safe, information-rich trajectories, outperforming\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 16:18:32 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Schrum", "Mariah", ""], ["Gombolay", "Matthew", ""]]}, {"id": "1912.08124", "submitter": "Luca Manneschi", "authors": "Luca Manneschi, Andrew C. Lin, Eleni Vasilaki", "title": "SpaRCe: Improved Learning of Reservoir Computing Systems through Sparse\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Sparse\" neural networks, in which relatively few neurons or connections are\nactive, are common in both machine learning and neuroscience. Whereas in\nmachine learning, \"sparsity\" is related to a penalty term that leads to some\nconnecting weights becoming small or zero, in biological brains, sparsity is\noften created when high spiking thresholds prevent neuronal activity. Here we\nintroduce sparsity into a reservoir computing network via neuron-specific\nlearnable thresholds of activity, allowing neurons with low thresholds to\ncontribute to decision-making but suppressing information from neurons with\nhigh thresholds. This approach, which we term \"SpaRCe\", optimises the sparsity\nlevel of the reservoir without affecting the reservoir dynamics. The read-out\nweights and the thresholds are learned by an on-line gradient rule that\nminimises an error function on the outputs of the network. Threshold learning\noccurs by the balance of two opposing forces: reducing inter-neuronal\ncorrelations in the reservoir by deactivating redundant neurons, while\nincreasing the activity of neurons participating in correct decisions. We test\nSpaRCe on classification problems and find that threshold learning improves\nperformance compared to standard reservoir computing. SpaRCe alleviates the\nproblem of catastrophic forgetting, a problem most evident in standard echo\nstate networks and recurrent neural networks in general, due to increasing the\nnumber of task-specialised neurons that are included in the network decisions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:05:26 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 17:51:11 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 16:25:07 GMT"}, {"version": "v4", "created": "Sun, 18 Apr 2021 05:26:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Manneschi", "Luca", ""], ["Lin", "Andrew C.", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1912.08168", "submitter": "Adri\\'an Hern\\'andez", "authors": "Adri\\'an Hern\\'andez, Jos\\'e M. Amig\\'o", "title": "Differentiable programming and its applications to dynamical systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable programming is the combination of classical neural networks\nmodules with algorithmic ones in an end-to-end differentiable model. These new\nmodels, that use automatic differentiation to calculate gradients, have new\nlearning capabilities (reasoning, attention and memory). In this tutorial,\naimed at researchers in nonlinear systems with prior knowledge of deep\nlearning, we present this new programming paradigm, describe some of its new\nfeatures such as attention mechanisms, and highlight the benefits they bring.\nThen, we analyse the uses and limitations of traditional deep learning models\nin the modeling and prediction of dynamical systems. Here, a dynamical system\nis meant to be a set of state variables that evolve in time under general\ninternal and external interactions. Finally, we review the advantages and\napplications of differentiable programming to dynamical systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:12:06 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 20:35:33 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hern\u00e1ndez", "Adri\u00e1n", ""], ["Amig\u00f3", "Jos\u00e9 M.", ""]]}, {"id": "1912.08203", "submitter": "Xavier Porte", "authors": "Johnny Moughames, Xavier Porte, Michael Thiel, Gwenn Ulliac, Maxime\n  Jacquot, Laurent Larger, Muamer Kadic, Daniel Brunner", "title": "Three dimensional waveguide-interconnects for scalable integration of\n  photonic neural networks", "comments": "7 pages, 6 figures", "journal-ref": "Optica 7, 640-646 (2020)", "doi": "10.1364/OPTICA.388205", "report-no": null, "categories": "cs.ET cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photonic waveguides are prime candidates for integrated and parallel photonic\ninterconnects. Such interconnects correspond to large-scale vector matrix\nproducts, which are at the heart of neural network computation. However,\nparallel interconnect circuits realized in two dimensions, for example by\nlithography, are strongly limited in size due to disadvantageous scaling. We\nuse three dimensional (3D) printed photonic waveguides to overcome this\nlimitation. 3D optical-couplers with fractal topology efficiently connect large\nnumbers of input and output channels, and we show that the substrate's\nfootprint area scales linearly. Going beyond simple couplers, we introduce\nfunctional circuits for discrete spatial filters identical to those used in\ndeep convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:22:28 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 14:59:17 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Moughames", "Johnny", ""], ["Porte", "Xavier", ""], ["Thiel", "Michael", ""], ["Ulliac", "Gwenn", ""], ["Jacquot", "Maxime", ""], ["Larger", "Laurent", ""], ["Kadic", "Muamer", ""], ["Brunner", "Daniel", ""]]}, {"id": "1912.08324", "submitter": "Kai Arulkumaran", "authors": "Tianhong Dai, Kai Arulkumaran, Tamara Gerbert, Samyakh Tukra, Feryal\n  Behbahani, Anil Anthony Bharath", "title": "Analysing Deep Reinforcement Learning Agents Trained with Domain\n  Randomisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has the potential to train robots to perform\ncomplex tasks in the real world without requiring accurate models of the robot\nor its environment. A practical approach is to train agents in simulation, and\nthen transfer them to the real world. One popular method for achieving\ntransferability is to use domain randomisation, which involves randomly\nperturbing various aspects of a simulated environment in order to make trained\nagents robust to the reality gap. However, less work has gone into\nunderstanding such agents - which are deployed in the real world - beyond task\nperformance. In this work we examine such agents, through qualitative and\nquantitative comparisons between agents trained with and without visual domain\nrandomisation. We train agents for Fetch and Jaco robots on a visuomotor\ncontrol task and evaluate how well they generalise using different testing\nconditions. Finally, we investigate the internals of the trained agents by\nusing a suite of interpretability techniques. Our results show that the primary\noutcome of domain randomisation is more robust, entangled representations,\naccompanied with larger weights with greater spatial structure; moreover, the\ntypes of changes are heavily influenced by the task setup and presence of\nadditional proprioceptive inputs. Additionally, we demonstrate that our domain\nrandomised agents require higher sample complexity, can overfit and more\nheavily rely on recurrent processing. Furthermore, even with an improved\nsaliency method introduced in this work, we show that qualitative studies may\nnot always correspond with quantitative measures, necessitating the combination\nof inspection tools in order to provide sufficient insights into the behaviour\nof trained agents.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 00:18:17 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 20:53:30 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Dai", "Tianhong", ""], ["Arulkumaran", "Kai", ""], ["Gerbert", "Tamara", ""], ["Tukra", "Samyakh", ""], ["Behbahani", "Feryal", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1912.08333", "submitter": "Stephen Whitelam", "authors": "Stephen Whitelam and Isaac Tamblyn", "title": "Learning to grow: control of material self-assembly using evolutionary\n  reinforcement learning", "comments": null, "journal-ref": "Phys. Rev. E 101, 052604 (2020)", "doi": "10.1103/PhysRevE.101.052604", "report-no": null, "categories": "cond-mat.stat-mech cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that neural networks trained by evolutionary reinforcement learning\ncan enact efficient molecular self-assembly protocols. Presented with molecular\nsimulation trajectories, networks learn to change temperature and chemical\npotential in order to promote the assembly of desired structures or choose\nbetween competing polymorphs. In the first case, networks reproduce in a\nqualitative sense the results of previously-known protocols, but faster and\nwith higher fidelity; in the second case they identify strategies previously\nunknown, from which we can extract physical insight. Networks that take as\ninput the elapsed time of the simulation or microscopic information from the\nsystem are both effective, the latter more so. The evolutionary scheme we have\nused is simple to implement and can be applied to a broad range of examples of\nexperimental self-assembly, whether or not one can monitor the experiment as it\nproceeds. Our results have been achieved with no human input beyond the\nspecification of which order parameter to promote, pointing the way to the\ndesign of synthesis protocols by artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:06:34 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 05:21:08 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 19:07:49 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Whitelam", "Stephen", ""], ["Tamblyn", "Isaac", ""]]}, {"id": "1912.08541", "submitter": "Uehwan Kim", "authors": "In-Ug Yoon, Ue-Hwan Kim and Jong-Hwan", "title": "s-DRN: Stabilized Developmental Resonance Network", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online incremental clustering of sequentially incoming data without prior\nknowledge suffers from changing cluster numbers and tends to fall into local\nextrema according to given data order. To overcome these limitations, we\npropose a stabilized developmental resonance network (s-DRN). First, we analyze\nthe instability of the conventional choice function during the node activation\nprocess and design a scalable activation function to make clustering\nperformance stable over all input data scales. Next, we devise three criteria\nfor the node grouping algorithm: distance, intersection over union (IoU) and\nsize criteria. The proposed node grouping algorithm effectively excludes\nunnecessary clusters from incrementally created clusters, diminishes the\nperformance dependency on vigilance parameters and makes the clustering process\nrobust. To verify the performance of the proposed s-DRN model, comparative\nstudies are conducted on six real-world datasets whose statistical\ncharacteristics are distinctive. The comparative studies demonstrate the\nproposed s-DRN outperforms baselines in terms of stability and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:47:44 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 05:06:52 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yoon", "In-Ug", ""], ["Kim", "Ue-Hwan", ""], ["Jong-Hwan", "", ""]]}, {"id": "1912.08716", "submitter": "Fan Zhang", "authors": "Fan Zhang, Miao Hu", "title": "Mitigate Parasitic Resistance in Resistive Crossbar-based Convolutional\n  Neural Networks", "comments": "Proceedings of ACM Journal on Emerging Technologies in Computing\n  (JETC) SI: Nanoelectronic Device, Circuit, Architecture Design. arXiv admin\n  note: text overlap with arXiv:1810.02225", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional computing hardware often encounters on-chip memory bottleneck on\nlarge scale Convolution Neural Networks (CNN) applications. With its unique\nin-memory computing feature, resistive crossbar-based computing attracts\nresearchers' attention as a promising solution to the memory bottleneck issue\nin von Neumann architectures. However, the parasitic resistances in the\ncrossbar deviate its behavior from the ideal weighted summation operation. In\nlarge-scale implementations, the impact of parasitic resistances must be\ncarefully considered and mitigated to ensure circuits' functionality. In this\nwork, we implemented and simulated CNNs on resistive crossbar circuits with\nconsideration of parasitic resistances. Moreover, we carried out a new mapping\nscheme for high utilization of crossbar arrays on convolution, and a mitigation\nalgorithm to mitigate parasitic resistances in CNN applications. The mitigation\nalgorithm considers parasitic resistances as well as data/kernel patterns of\neach layer to minimize the computing error in crossbar-based convolutions of\nCNNs. We demonstrated the proposed methods with implementations of a 4-layer\nCNN on MNIST and ResNet(20, 32, and 56) on CIFAR-10. Simulation results show\nthe proposed methods well mitigate the parasitic resistances in crossbars. With\nour methods, modern CNNs on crossbars can preserve ideal(software) level\nclassification accuracy with 6-bit ADCs and DACs implementation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 07:08:26 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Zhang", "Fan", ""], ["Hu", "Miao", ""]]}, {"id": "1912.08785", "submitter": "Piotr S. Maci\\k{a}g", "authors": "Piotr S. Maci\\k{a}g (1), Marzena Kryszkiewicz (1), Robert Bembenik\n  (1), Jesus L. Lobo (2), Javier Del Ser (2 and 3) ((1) Institute of Computer\n  Science, Warsaw University of Technology, Warsaw, Poland, (2) TECNALIA Parque\n  Tecnol\\'ogico de Bizkaia, Derio, Spain, (3) University of the Basque Country\n  UPV/EHU, Bilbao, Spain)", "title": "Unsupervised Anomaly Detection in Stream Data with Online Evolving\n  Spiking Neural Networks", "comments": "52 pages", "journal-ref": "Neural Networks, Volume 139, 2021, Pages 118-139", "doi": "10.1016/j.neunet.2021.02.017", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised anomaly discovery in stream data is a research topic with many\npractical applications. However, in many cases, it is not easy to collect\nenough training data with labeled anomalies for supervised learning of an\nanomaly detector in order to deploy it later for identification of real\nanomalies in streaming data. It is thus important to design anomalies detectors\nthat can correctly detect anomalies without access to labeled training data.\nOur idea is to adapt the Online evolving Spiking Neural Network (OeSNN)\nclassifier to the anomaly detection task. As a result, we offer an Online\nevolving Spiking Neural Network for Unsupervised Anomaly Detection algorithm\n(OeSNN-UAD), which, unlike OeSNN, works in an unsupervised way and does not\nseparate output neurons into disjoint decision classes. OeSNN-UAD uses our\nproposed new two-step anomaly detection method. Also, we derive new theoretical\nproperties of neuronal model and input layer encoding of OeSNN, which enable\nmore effective and efficient detection of anomalies in our OeSNN-UAD approach.\nThe proposed OeSNN-UAD detector was experimentally compared with\nstate-of-the-art unsupervised and semi-supervised detectors of anomalies in\nstream data from the Numenta Anomaly Benchmark and Yahoo Anomaly Datasets\nrepositories. Our approach outperforms the other solutions provided in the\nliterature in the case of data streams from the Numenta Anomaly Benchmark\nrepository. Also, in the case of real data files of the Yahoo Anomaly Benchmark\nrepository, OeSNN-UAD outperforms other selected algorithms, whereas in the\ncase of Yahoo Anomaly Benchmark synthetic data files, it provides competitive\nresults to the results recently reported in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:36:01 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 20:17:42 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Maci\u0105g", "Piotr S.", "", "2 and 3"], ["Kryszkiewicz", "Marzena", "", "2 and 3"], ["Bembenik", "Robert", "", "2 and 3"], ["Lobo", "Jesus L.", "", "2 and 3"], ["Del Ser", "Javier", "", "2 and 3"]]}, {"id": "1912.08866", "submitter": "James Harrison", "authors": "James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone", "title": "Continuous Meta-Learning without Tasks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning is a promising strategy for learning to efficiently learn\nwithin new tasks, using data gathered from a distribution of tasks. However,\nthe meta-learning literature thus far has focused on the task segmented\nsetting, where at train-time, offline data is assumed to be split according to\nthe underlying task, and at test-time, the algorithms are optimized to learn in\na single task. In this work, we enable the application of generic meta-learning\nalgorithms to settings where this task segmentation is unavailable, such as\ncontinual online learning with a time-varying task. We present meta-learning\nvia online changepoint analysis (MOCA), an approach which augments a\nmeta-learning algorithm with a differentiable Bayesian changepoint detection\nscheme. The framework allows both training and testing directly on time series\ndata without segmenting it into discrete tasks. We demonstrate the utility of\nthis approach on a nonlinear meta-regression benchmark as well as two\nmeta-image-classification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:10:40 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 00:14:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Harrison", "James", ""], ["Sharma", "Apoorva", ""], ["Finn", "Chelsea", ""], ["Pavone", "Marco", ""]]}, {"id": "1912.08881", "submitter": "Sebastian Lapuschkin", "authors": "Seul-Ki Yeom, Philipp Seegerer, Sebastian Lapuschkin, Alexander\n  Binder, Simon Wiedemann, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Pruning by Explaining: A Novel Criterion for Deep Neural Network Pruning", "comments": "25 pages + 5 supplementary pages, 13 figures, 6 tables", "journal-ref": "Pattern Recognition, Volume 115, pp.107899, 2021", "doi": "10.1016/j.patcog.2021.107899", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of convolutional neural networks (CNNs) in various applications\nis accompanied by a significant increase in computation and parameter storage\ncosts. Recent efforts to reduce these overheads involve pruning and compressing\nthe weights of various layers while at the same time aiming to not sacrifice\nperformance. In this paper, we propose a novel criterion for CNN pruning\ninspired by neural network interpretability: The most relevant units, i.e.\nweights or filters, are automatically found using their relevance scores\nobtained from concepts of explainable AI (XAI). By exploring this idea, we\nconnect the lines of interpretability and model compression research. We show\nthat our proposed method can efficiently prune CNN models in transfer-learning\nsetups in which networks pre-trained on large corpora are adapted to\nspecialized tasks. The method is evaluated on a broad range of computer vision\ndatasets. Notably, our novel criterion is not only competitive or better\ncompared to state-of-the-art pruning criteria when successive retraining is\nperformed, but clearly outperforms these previous criteria in the\nresource-constrained application scenario in which the data of the task to be\ntransferred to is very scarce and one chooses to refrain from fine-tuning. Our\nmethod is able to compress the model iteratively while maintaining or even\nimproving accuracy. At the same time, it has a computational cost in the order\nof gradient computation and is comparatively simple to apply without the need\nfor tuning hyperparameters for pruning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:42:30 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 14:43:54 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 11:30:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yeom", "Seul-Ki", ""], ["Seegerer", "Philipp", ""], ["Lapuschkin", "Sebastian", ""], ["Binder", "Alexander", ""], ["Wiedemann", "Simon", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1912.08956", "submitter": "Carola Doerr", "authors": "Jakob Bossek and Pascal Kerschke and Aneta Neumann and Frank Neumann\n  and Carola Doerr", "title": "One-Shot Decision-Making with and without Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot decision making is required in situations in which we can evaluate a\nfixed number of solution candidates but do not have any possibility for\nfurther, adaptive sampling. Such settings are frequently encountered in neural\nnetwork design, hyper-parameter optimization, and many simulation-based\nreal-world optimization tasks, in which evaluations are costly and time sparse.\nIt seems intuitive that well-distributed samples should be more meaningful in\none-shot decision making settings than uniform or grid-based samples, since\nthey show a better coverage of the decision space. In practice, quasi-random\ndesigns such as Latin Hypercube Samples and low-discrepancy point sets form\nindeed the state of the art, as confirmed by a number of recent studies and\ncompetitions. In this work we take a closer look into the correlation between\nthe distribution of the quasi-random designs and their performance in one-shot\ndecision making tasks, with the goal to investigate whether the assumed\ncorrelation between uniform distribution and performance can be confirmed. We\nstudy three different decision tasks: classic one-shot optimization (only the\nbest sample matters), one-shot optimization with surrogates (allowing to use\nsurrogate models for selecting a design that need not necessarily be one of the\nevaluated samples), and one-shot regression (i.e., function approximation, with\nminimization of mean squared error as objective). Our results confirm an\nadvantage of low-discrepancy designs for all three settings. The overall\ncorrelation, however, is rather weak. We complement our study by evolving\nproblem-specific samples that show significantly better performance for the\nregression task than the standard approaches based on low-discrepancy\nsequences, giving strong indication that significant performance gains over\nstate-of-the-art one-shot sampling techniques are possible.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 00:20:34 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 10:42:28 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Bossek", "Jakob", ""], ["Kerschke", "Pascal", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""], ["Doerr", "Carola", ""]]}, {"id": "1912.08986", "submitter": "Nicholas Roberts", "authors": "Nicholas Roberts, Dian Ang Yap, Vinay Uday Prabhu", "title": "Deep Connectomics Networks: Neural Network Architectures Inspired by\n  Neuronal Networks", "comments": "Presented at the Real Neurons & Hidden Units Workshop, 33rd\n  Conference on Neural Information ProcessingSystems (NeurIPS 2019), Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between inter-neuronal network topology and cognition has been\nstudied deeply by connectomics researchers and network scientists, which is\ncrucial towards understanding the remarkable efficacy of biological neural\nnetworks. Curiously, the deep learning revolution that revived neural networks\nhas not paid much attention to topological aspects. The architectures of deep\nneural networks (DNNs) do not resemble their biological counterparts in the\ntopological sense. We bridge this gap by presenting initial results of Deep\nConnectomics Networks (DCNs) as DNNs with topologies inspired by real-world\nneuronal networks. We show high classification accuracy obtained by DCNs whose\narchitecture was inspired by the biological neuronal networks of C. Elegans and\nthe mouse visual cortex.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 01:59:20 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Roberts", "Nicholas", ""], ["Yap", "Dian Ang", ""], ["Prabhu", "Vinay Uday", ""]]}, {"id": "1912.09237", "submitter": "Carola Doerr", "authors": "Carola Doerr and Furong Ye and Naama Horesh and Hao Wang and Ofer M.\n  Shir and Thomas B\\\"ack", "title": "Benchmarking Discrete Optimization Heuristics with IOHprofiler", "comments": null, "journal-ref": "Applied Soft Computing. Volume 88, March 2020, 106027", "doi": "10.1016/j.asoc.2019.106027", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated benchmarking environments aim to support researchers in\nunderstanding how different algorithms perform on different types of\noptimization problems. Such comparisons provide insights into the strengths and\nweaknesses of different approaches, which can be leveraged into designing new\nalgorithms and into the automation of algorithm selection and configuration.\nWith the ultimate goal to create a meaningful benchmark set for iterative\noptimization heuristics, we have recently released IOHprofiler, a software\nbuilt to create detailed performance comparisons between iterative optimization\nheuristics.\n  With this present work we demonstrate that IOHprofiler provides a suitable\nenvironment for automated benchmarking. We compile and assess a selection of 23\ndiscrete optimization problems that subscribe to different types of fitness\nlandscapes. For each selected problem we compare performances of twelve\ndifferent heuristics, which are as of now available as baseline algorithms in\nIOHprofiler.\n  We also provide a new module for IOHprofiler which extents the fixed-target\nand fixed-budget results for the individual problems by ECDF results, which\nallows one to derive aggregated performance statistics for groups of problems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:53:39 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 14:55:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Doerr", "Carola", ""], ["Ye", "Furong", ""], ["Horesh", "Naama", ""], ["Wang", "Hao", ""], ["Shir", "Ofer M.", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1912.09399", "submitter": "Julian Zilly", "authors": "Julian Zilly, Lorenz Hetzel, Andrea Censi, Emilio Frazzoli", "title": "Quantifying the effect of representations on task complexity", "comments": "Workshop paper at Information Theory and Machine Learning Workshop at\n  NeurIPS'19. 13 pages (8 pages + 2 bibliography + 3 appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the influence of input data representations on learning\ncomplexity. For learning, we posit that each model implicitly uses a candidate\nmodel distribution for unexplained variations in the data, its noise model. If\nthe model distribution is not well aligned to the true distribution, then even\nrelevant variations will be treated as noise. Crucially however, the alignment\nof model and true distribution can be changed, albeit implicitly, by changing\ndata representations. \"Better\" representations can better align the model to\nthe true distribution, making it easier to approximate the input-output\nrelationship in the data without discarding useful data variations. To quantify\nthis alignment effect of data representations on the difficulty of a learning\ntask, we make use of an existing task complexity score and show its connection\nto the representation-dependent information coding length of the input.\nEmpirically we extract the necessary statistics from a linear regression\napproximation and show that these are sufficient to predict relative learning\nperformance outcomes of different data representations and neural network types\nobtained when utilizing an extensive neural network architecture search. We\nconclude that to ensure better learning outcomes, representations may need to\nbe tailored to both task and model to align with the implicit distribution of\nmodel and task.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:19:14 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Zilly", "Julian", ""], ["Hetzel", "Lorenz", ""], ["Censi", "Andrea", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1912.09423", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Smon Hessner, Yao-Chong Lim, Louis-Phlippe Morency", "title": "Pseudo-Encoded Stochastic Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior inference in directed graphical models is commonly done using a\nprobabilistic encoder (a.k.a inference model) conditioned on the input. Often\nthis inference model is trained jointly with the probabilistic decoder (a.k.a\ngenerator model). If probabilistic encoder encounters complexities during\ntraining (e.g. suboptimal complxity or parameterization), then learning reaches\na suboptimal objective; a phenomena commonly called inference suboptimality. In\nVariational Inference (VI), optimizing the ELBo using Stochastic Variational\nInference (SVI) can eliminate the inference suboptimality (as demonstrated in\nthis paper), however, this solution comes at a substantial computational cost\nwhen inference needs to be done on new data points. Essentially, a long\nsequential chain of gradient updates is required to fully optimize approximate\nposteriors. In this paper, we present an approach called Pseudo-Encoded\nStochastic Variational Inference (PE-SVI), to reduce the inference complexity\nof SVI during test time. Our approach relies on finding a suitable initial\nstart point for gradient operations, which naturally reduces the required\ngradient steps. Furthermore, this initialization allows for adopting larger\nstep sizes (compared to random initialization used in SVI), which further\nreduces the inference time complexity. PE-SVI reaches the same ELBo objective\nas SVI using less than one percent of required steps, on average.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:50:18 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Zadeh", "Amir", ""], ["Hessner", "Smon", ""], ["Lim", "Yao-Chong", ""], ["Morency", "Louis-Phlippe", ""]]}, {"id": "1912.09503", "submitter": "Alexandre Trudeau", "authors": "Alexandre Trudeau, Christopher M. Clark", "title": "Multi-Robot Path Planning Via Genetic Programming", "comments": "ARMS 2019 Workshop (AAMAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Genetic Programming (GP) approach to solving\nmulti-robot path planning (MRPP) problems in single-lane workspaces,\nspecifically those easily mapped to graph representations. GP's versatility\nenables this approach to produce programs optimizing for multiple attributes\nrather than a single attribute such as path length or completeness. When\noptimizing for the number of time steps needed to solve individual MRPP\nproblems, the GP constructed programs outperformed complete MRPP algorithms,\ni.e. Push-Swap-Wait (PSW), by $54.1\\%$. The GP constructed programs also\nconsistently outperformed PSW in solving problems that did not meet PSW's\ncompleteness conditions. Furthermore, the GP constructed programs exhibited a\ngreater capacity for scaling than PSW as the number of robots navigating within\nan MRPP environment increased. This research illustrates the benefits of using\nGenetic Programming for solving individual MRPP problems, including instances\nin which the number of robots exceeds the number of leaves in the tree-modeled\nworkspace.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:08:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Trudeau", "Alexandre", ""], ["Clark", "Christopher M.", ""]]}, {"id": "1912.09524", "submitter": "David Dewhurst", "authors": "David Rushing Dewhurst, Yi Li, Alexander Bogdan, Jasmine Geng", "title": "Evolving ab initio trading strategies in heterogeneous environments", "comments": "20 pages (10 main body, 10 appendix), 11 figures (6 main body, 5\n  appendix), open-source matching engine implementation available at\n  https://gitlab.com/daviddewhurst/verdantcurve", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.PE q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Securities markets are quintessential complex adaptive systems in which\nheterogeneous agents compete in an attempt to maximize returns. Species of\ntrading agents are also subject to evolutionary pressure as entire classes of\nstrategies become obsolete and new classes emerge. Using an agent-based model\nof interacting heterogeneous agents as a flexible environment that can\nendogenously model many diverse market conditions, we subject deep neural\nnetworks to evolutionary pressure to create dominant trading agents. After\nanalyzing the performance of these agents and noting the emergence of anomalous\nsuperdiffusion through the evolutionary process, we construct a method to turn\nhigh-fitness agents into trading algorithms. We backtest these trading\nalgorithms on real high-frequency foreign exchange data, demonstrating that\nelite trading algorithms are consistently profitable in a variety of market\nconditions---even though these algorithms had never before been exposed to real\nfinancial data. These results provide evidence to suggest that developing\n\\textit{ab initio} trading strategies by repeated simulation and evolution in a\nmechanistic market model may be a practical alternative to explicitly training\nmodels with past observed market data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:00:06 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Dewhurst", "David Rushing", ""], ["Li", "Yi", ""], ["Bogdan", "Alexander", ""], ["Geng", "Jasmine", ""]]}, {"id": "1912.09600", "submitter": "Mohammad Kachuee Mr.", "authors": "Mohammad Kachuee, Sajad Darabi, Shayan Fazeli, Majid Sarrafzadeh", "title": "Group-Connected Multilayer Perceptron Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep learning in domains such as image, voice, and\ngraphs, there has been little progress in deep representation learning for\ndomains without a known structure between features. For instance, a tabular\ndataset of different demographic and clinical factors where the feature\ninteractions are not given as a prior. In this paper, we propose\nGroup-Connected Multilayer Perceptron (GMLP) networks to enable deep\nrepresentation learning in these domains. GMLP is based on the idea of learning\nexpressive feature combinations (groups) and exploiting them to reduce the\nnetwork complexity by defining local group-wise operations. During the training\nphase, GMLP learns a sparse feature grouping matrix using temperature annealing\nsoftmax with an added entropy loss term to encourage the sparsity. Furthermore,\nan architecture is suggested which resembles binary trees, where group-wise\noperations are followed by pooling operations to combine information; reducing\nthe number of groups as the network grows in depth. To evaluate the proposed\nmethod, we conducted experiments on different real-world datasets covering\nvarious application areas. Additionally, we provide visualizations on MNIST and\nsynthesized data. According to the results, GMLP is able to successfully learn\nand exploit expressive feature combinations and achieve state-of-the-art\nclassification performance on different datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:49:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 16:22:39 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 16:56:55 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kachuee", "Mohammad", ""], ["Darabi", "Sajad", ""], ["Fazeli", "Shayan", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1912.09666", "submitter": "Qing Jin", "authors": "Qing Jin, Linjie Yang, Zhenyu Liao", "title": "AdaBits: Neural Network Quantization with Adaptive Bit-Widths", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with adaptive configurations have gained increasing\nattention due to the instant and flexible deployment of these models on\nplatforms with different resource budgets. In this paper, we investigate a\nnovel option to achieve this goal by enabling adaptive bit-widths of weights\nand activations in the model. We first examine the benefits and challenges of\ntraining quantized model with adaptive bit-widths, and then experiment with\nseveral approaches including direct adaptation, progressive training and joint\ntraining. We discover that joint training is able to produce comparable\nperformance on the adaptive model as individual models. We further propose a\nnew technique named Switchable Clipping Level (S-CL) to further improve\nquantized models at the lowest bit-width. With our proposed techniques applied\non a bunch of models including MobileNet-V1/V2 and ResNet-50, we demonstrate\nthat bit-width of weights and activations is a new option for adaptively\nexecutable deep neural networks, offering a distinct opportunity for improved\naccuracy-efficiency trade-off as well as instant adaptation according to the\nplatform constraints in real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:10:23 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 19:42:05 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Jin", "Qing", ""], ["Yang", "Linjie", ""], ["Liao", "Zhenyu", ""]]}, {"id": "1912.09986", "submitter": "Andrei Ivanov", "authors": "Andrei Ivanov, Anna Golovkina, Uwe Iben", "title": "Polynomial Neural Networks and Taylor maps for Dynamical Systems\n  Simulation and Learning", "comments": "arXiv admin note: text overlap with arXiv:1908.06088", "journal-ref": "24th European Conference on Artificial Intelligence - ECAI 2020", "doi": null, "report-no": "464", "categories": "cs.NE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection of Taylor maps and polynomial neural networks (PNN) to solve\nordinary differential equations (ODEs) numerically is considered. Having the\nsystem of ODEs, it is possible to calculate weights of PNN that simulates the\ndynamics of these equations. It is shown that proposed PNN architecture can\nprovide better accuracy with less computational time in comparison with\ntraditional numerical solvers. Moreover, neural network derived from the ODEs\ncan be used for simulation of system dynamics with different initial\nconditions, but without training procedure. On the other hand, if the equations\nare unknown, the weights of the PNN can be fitted in a data-driven way. In the\npaper we describe the connection of PNN with differential equations in a\ntheoretical way along with the examples for both dynamics simulation and\nlearning with data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:44:18 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ivanov", "Andrei", ""], ["Golovkina", "Anna", ""], ["Iben", "Uwe", ""]]}, {"id": "1912.10103", "submitter": "Luca Mocerino", "authors": "Luca Mocerino, Andrea Calimera", "title": "TentacleNet: A Pseudo-Ensemble Template for Accurate Binary\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization is an attractive strategy for implementing lightweight Deep\nConvolutional Neural Networks (CNNs). Despite the unquestionable savings\noffered, memory footprint above all, it may induce an excessive accuracy loss\nthat prevents a widespread use. This work elaborates on this aspect introducing\nTentacleNet, a new template designed to improve the predictive performance of\nbinarized CNNs via parallelization. Inspired by the ensemble learning theory,\nit consists of a compact topology that is end-to-end trainable and organized to\nminimize memory utilization. Experimental results collected over three\nrealistic benchmarks show TentacleNet fills the gap left by classical binary\nmodels, ensuring substantial memory savings w.r.t. state-of-the-art binary\nensemble methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 21:18:16 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 12:37:23 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mocerino", "Luca", ""], ["Calimera", "Andrea", ""]]}, {"id": "1912.10185", "submitter": "Alvin Chan", "authors": "Alvin Chan, Yi Tay, Yew Soon Ong, Jie Fu", "title": "Jacobian Adversarially Regularized Networks for Robustness", "comments": "ICLR 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial examples are crafted with imperceptible perturbations with the\nintent to fool neural networks. Against such attacks, adversarial training and\nits variants stand as the strongest defense to date. Previous studies have\npointed out that robust models that have undergone adversarial training tend to\nproduce more salient and interpretable Jacobian matrices than their non-robust\ncounterparts. A natural question is whether a model trained with an objective\nto produce salient Jacobian can result in better robustness. This paper answers\nthis question with affirmative empirical results. We propose Jacobian\nAdversarially Regularized Networks (JARN) as a method to optimize the saliency\nof a classifier's Jacobian by adversarially regularizing the model's Jacobian\nto resemble natural training images. Image classifiers trained with JARN show\nimproved robust accuracy compared to standard models on the MNIST, SVHN and\nCIFAR-10 datasets, uncovering a new angle to boost robustness without using\nadversarial training examples.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 02:46:50 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 08:06:12 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Chan", "Alvin", ""], ["Tay", "Yi", ""], ["Ong", "Yew Soon", ""], ["Fu", "Jie", ""]]}, {"id": "1912.10189", "submitter": "Christopher J. Cueva", "authors": "Christopher J. Cueva, Peter Y. Wang, Matthew Chin, Xue-Xin Wei", "title": "Emergence of functional and structural properties of the head direction\n  system by optimization of recurrent neural networks", "comments": "International Conference on Learning Representations (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests goal-driven training of neural networks can be used to\nmodel neural activity in the brain. While response properties of neurons in\nartificial neural networks bear similarities to those in the brain, the network\narchitectures are often constrained to be different. Here we ask if a neural\nnetwork can recover both neural representations and, if the architecture is\nunconstrained and optimized, the anatomical properties of neural circuits. We\ndemonstrate this in a system where the connectivity and the functional\norganization have been characterized, namely, the head direction circuits of\nthe rodent and fruit fly. We trained recurrent neural networks (RNNs) to\nestimate head direction through integration of angular velocity. We found that\nthe two distinct classes of neurons observed in the head direction system, the\nCompass neurons and the Shifter neurons, emerged naturally in artificial neural\nnetworks as a result of training. Furthermore, connectivity analysis and\nin-silico neurophysiology revealed structural and mechanistic similarities\nbetween artificial networks and the head direction system. Overall, our results\nshow that optimization of RNNs in a goal-driven task can recapitulate the\nstructure and function of biological circuits, suggesting that artificial\nneural networks can be used to study the brain at the level of both neural\nactivity and anatomical organization.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 03:51:58 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 09:06:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Cueva", "Christopher J.", ""], ["Wang", "Peter Y.", ""], ["Chin", "Matthew", ""], ["Wei", "Xue-Xin", ""]]}, {"id": "1912.10305", "submitter": "Jordan Ott", "authors": "Jordan Ott", "title": "Questions to Guide the Future of Artificial Intelligence Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of machine learning has focused, primarily, on discretized\nsub-problems (i.e. vision, speech, natural language) of intelligence. While\nneuroscience tends to be observation heavy, providing few guiding theories. It\nis unlikely that artificial intelligence will emerge through only one of these\ndisciplines. Instead, it is likely to be some amalgamation of their algorithmic\nand observational findings. As a result, there are a number of problems that\nshould be addressed in order to select the beneficial aspects of both fields.\nIn this article, we propose leading questions to guide the future of artificial\nintelligence research. There are clear computational principles on which the\nbrain operates. The problem is finding these computational needles in a\nhaystack of biological complexity. Biology has clear constraints but by not\nusing it as a guide we are constraining ourselves.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 17:48:31 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 15:31:34 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Ott", "Jordan", ""]]}, {"id": "1912.10703", "submitter": "Dongqi Han", "authors": "Dongqi Han, Kenji Doya, Jun Tani", "title": "Variational Recurrent Models for Solving Partially Observable Control\n  Tasks", "comments": "Published as a conference paper at the Eighth International\n  Conference on Learning Representations (ICLR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In partially observable (PO) environments, deep reinforcement learning (RL)\nagents often suffer from unsatisfactory performance, since two problems need to\nbe tackled together: how to extract information from the raw observations to\nsolve the task, and how to improve the policy. In this study, we propose an RL\nalgorithm for solving PO tasks. Our method comprises two parts: a variational\nrecurrent model (VRM) for modeling the environment, and an RL controller that\nhas access to both the environment and the VRM. The proposed algorithm was\ntested in two types of PO robotic control tasks, those in which either\ncoordinates or velocities were not observable and those that require long-term\nmemorization. Our experiments show that the proposed algorithm achieved better\ndata efficiency and/or learned more optimal policy than other alternative\napproaches in tasks in which unobserved states cannot be inferred from raw\nobservations in a simple manner.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:43:16 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 05:05:00 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Han", "Dongqi", ""], ["Doya", "Kenji", ""], ["Tani", "Jun", ""]]}, {"id": "1912.10729", "submitter": "Yujing Wang", "authors": "Yujing Wang, Yaming Yang, Yiren Chen, Jing Bai, Ce Zhang, Guinan Su,\n  Xiaoyu Kou, Yunhai Tong, Mao Yang, Lidong Zhou", "title": "TextNAS: A Neural Architecture Search Space tailored for Text\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning text representation is crucial for text classification and other\nlanguage related tasks. There are a diverse set of text representation networks\nin the literature, and how to find the optimal one is a non-trivial problem.\nRecently, the emerging Neural Architecture Search (NAS) techniques have\ndemonstrated good potential to solve the problem. Nevertheless, most of the\nexisting works of NAS focus on the search algorithms and pay little attention\nto the search space. In this paper, we argue that the search space is also an\nimportant human prior to the success of NAS in different applications. Thus, we\npropose a novel search space tailored for text representation. Through\nautomatic search, the discovered network architecture outperforms\nstate-of-the-art models on various public datasets on text classification and\nnatural language inference tasks. Furthermore, some of the design principles\nfound in the automatic network agree well with human intuition.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:51:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Yujing", ""], ["Yang", "Yaming", ""], ["Chen", "Yiren", ""], ["Bai", "Jing", ""], ["Zhang", "Ce", ""], ["Su", "Guinan", ""], ["Kou", "Xiaoyu", ""], ["Tong", "Yunhai", ""], ["Yang", "Mao", ""], ["Zhou", "Lidong", ""]]}, {"id": "1912.10730", "submitter": "Yingshi Chen", "authors": "Yingshi Chen, Jinfeng Zhu", "title": "An optical diffractive deep neural network with multiple\n  frequency-channels", "comments": "5 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffractive deep neural network (DNNet) is a novel machine learning framework\non the modulation of optical transmission. Diffractive network would get\npredictions at the speed of light. It's pure passive architecture, no\nadditional power consumption. We improved the accuracy of diffractive network\nwith optical waves at different frequency. Each layers have multiple\nfrequency-channels (optical distributions at different frequency). These\nchannels are merged at the output plane to get final output. The experiment in\nthe fasion-MNIST and EMNIST datasets showed multiple frequency-channels would\nincrease the accuracy a lot. We also give detailed analysis to show the\ndifference between DNNet and MLP. The modulation process in DNNet is actually\noptical activation function. We develop an open source package ONNet. The\nsource codes are available at https://github.com/closest-git/ONNet.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:54:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chen", "Yingshi", ""], ["Zhu", "Jinfeng", ""]]}, {"id": "1912.10752", "submitter": "Balaji Selvaraj", "authors": "S. Balaji, T. Kavya, Natasha Sebastian", "title": "Learn-able parameter guided Activation Functions", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we explore the concept of adding learn-able slope and mean\nshift parameters to an activation function to improve the total response\nregion. The characteristics of an activation function depend highly on the\nvalue of parameters. Making the parameters learn-able, makes the activation\nfunction more dynamic and capable to adapt as per the requirements of its\nneighboring layers. The introduced slope parameter is independent of other\nparameters in the activation function. The concept was applied to ReLU to\ndevelop Dual Line and DualParametric ReLU activation function. Evaluation on\nMNIST and CIFAR10 show that the proposed activation function Dual Line achieves\ntop-5 position for mean accuracy among 43 activation functions tested with\nLENET4, LENET5, and WideResNet architectures. This is the first time more than\n40 activation functions were analyzed on MNIST andCIFAR10 dataset at the same\ntime. The study on the distribution of positive slope parameter beta indicates\nthat the activation function adapts as per the requirements of the neighboring\nlayers. The study shows that model performance increases with the proposed\nactivation functions\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:54:05 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Balaji", "S.", ""], ["Kavya", "T.", ""], ["Sebastian", "Natasha", ""]]}, {"id": "1912.10818", "submitter": "Daniel Acuna", "authors": "Lizhen Liang and Daniel E. Acuna", "title": "Artificial mental phenomena: Psychophysics as a framework to detect\n  perception biases in AI models", "comments": "FAT Conference 2020", "journal-ref": null, "doi": "10.1145/3351095.3375623", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting biases in artificial intelligence has become difficult because of\nthe impenetrable nature of deep learning. The central difficulty is in relating\nunobservable phenomena deep inside models with observable, outside quantities\nthat we can measure from inputs and outputs. For example, can we detect\ngendered perceptions of occupations (e.g., female librarian, male electrician)\nusing questions to and answers from a word embedding-based system? Current\ntechniques for detecting biases are often customized for a task, dataset, or\nmethod, affecting their generalization. In this work, we draw from\nPsychophysics in Experimental Psychology---meant to relate quantities from the\nreal world (i.e., \"Physics\") into subjective measures in the mind (i.e.,\n\"Psyche\")---to propose an intellectually coherent and generalizable framework\nto detect biases in AI. Specifically, we adapt the two-alternative forced\nchoice task (2AFC) to estimate potential biases and the strength of those\nbiases in black-box models. We successfully reproduce previously-known biased\nperceptions in word embeddings and sentiment analysis predictions. We discuss\nhow concepts in experimental psychology can be naturally applied to\nunderstanding artificial mental phenomena, and how psychophysics can form a\nuseful methodological foundation to study fairness in AI.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 19:48:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liang", "Lizhen", ""], ["Acuna", "Daniel E.", ""]]}, {"id": "1912.10905", "submitter": "Anand Kumar Mukhopadhyay", "authors": "Anand Kumar Mukhopadhyay, Naligala Moses Prabhakar, Divya Lakshmi\n  Duggisetty, Indrajit Chakrabarti, and Mrigank Sharad", "title": "Intelligent Wireless Sensor Nodes for Human Footstep Sound\n  Classification for Security Application", "comments": "12 pages, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor nodes present in a wireless sensor network (WSN) for security\nsurveillance applications should preferably be small, energy-efficient and\ninexpensive with on-sensor computational abilities. An appropriate data\nprocessing scheme in the sensor node can help in reducing the power dissipation\nof the transceiver through compression of information to be communicated. In\nthis paper, authors have attempted a simulation-based study of human footstep\nsound classification in natural surroundings using simple time-domain features.\nWe used a spiking neural network (SNN), a computationally low weight\nclassifier, derived from an artificial neural network (ANN), for\nclassification. A classification accuracy greater than 85% is achieved using an\nSNN, degradation of ~5% as compared to ANN. The SNN scheme, along with the\nrequired feature extraction scheme, can be amenable to low power sub-threshold\nanalog implementation. Results show that all analog implementation of the\nproposed SNN scheme can achieve significant power savings over the digital\nimplementation of the same computing scheme and also over other conventional\ndigital architectures using frequency-domain feature extraction and ANN-based\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:11:04 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mukhopadhyay", "Anand Kumar", ""], ["Prabhakar", "Naligala Moses", ""], ["Duggisetty", "Divya Lakshmi", ""], ["Chakrabarti", "Indrajit", ""], ["Sharad", "Mrigank", ""]]}, {"id": "1912.10986", "submitter": "Thanh Pham Dinh", "authors": "Tran Ba Trung and Huynh Thi Thanh Binh and Le Tien Thanh and Ly Trung\n  Hieu and Pham Dinh Thanh", "title": "Multifactorial Evolutionary Algorithm For Clustered Minimum Routing Cost\n  Problem", "comments": null, "journal-ref": null, "doi": "10.1145/3368926.3369712", "report-no": null, "categories": "cs.NE cs.AI cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimum Routing Cost Clustered Tree Problem (CluMRCT) is applied in various\nfields in both theory and application. Because the CluMRCT is NP-Hard, the\napproximate approaches are suitable to find the solution for this problem.\nRecently, Multifactorial Evolutionary Algorithm (MFEA) has emerged as one of\nthe most efficient approximation algorithms to deal with many different kinds\nof problems. Therefore, this paper studies to apply MFEA for solving CluMRCT\nproblems. In the proposed MFEA, we focus on crossover and mutation operators\nwhich create a valid solution of CluMRCT problem in two levels: first level\nconstructs spanning trees for graphs in clusters while the second level builds\na spanning tree for connecting among clusters. To reduce the consuming\nresources, we will also introduce a new method of calculating the cost of\nCluMRCT solution. The proposed algorithm is experimented on numerous types of\ndatasets. The experimental results demonstrate the effectiveness of the\nproposed algorithm, partially on large instances\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:27:22 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Trung", "Tran Ba", ""], ["Binh", "Huynh Thi Thanh", ""], ["Thanh", "Le Tien", ""], ["Hieu", "Ly Trung", ""], ["Thanh", "Pham Dinh", ""]]}, {"id": "1912.11121", "submitter": "Alexander Sax", "authors": "Alexander Sax, Jeffrey O. Zhang, Bradley Emi, Amir Zamir, Silvio\n  Savarese, Leonidas Guibas, Jitendra Malik", "title": "Learning to Navigate Using Mid-Level Visual Priors", "comments": "In Conference on Robot Learning, 2019. See project website and demos\n  at http://perceptual.actor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much does having visual priors about the world (e.g. the fact that the\nworld is 3D) assist in learning to perform downstream motor tasks (e.g.\nnavigating a complex environment)? What are the consequences of not utilizing\nsuch visual priors in learning? We study these questions by integrating a\ngeneric perceptual skill set (a distance estimator, an edge detector, etc.)\nwithin a reinforcement learning framework (see Fig. 1). This skill set\n(\"mid-level vision\") provides the policy with a more processed state of the\nworld compared to raw images.\n  Our large-scale study demonstrates that using mid-level vision results in\npolicies that learn faster, generalize better, and achieve higher final\nperformance, when compared to learning from scratch and/or using\nstate-of-the-art visual and non-visual representation learning methods. We show\nthat conventional computer vision objectives are particularly effective in this\nregard and can be conveniently integrated into reinforcement learning\nframeworks. Finally, we found that no single visual representation was\nuniversally useful for all downstream tasks, hence we computationally derive a\ntask-agnostic set of representations optimized to support arbitrary downstream\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:45:50 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Sax", "Alexander", ""], ["Zhang", "Jeffrey O.", ""], ["Emi", "Bradley", ""], ["Zamir", "Amir", ""], ["Savarese", "Silvio", ""], ["Guibas", "Leonidas", ""], ["Malik", "Jitendra", ""]]}, {"id": "1912.11126", "submitter": "Ryan Grgurich", "authors": "Ryan Grgurich and Hugh T. Blair", "title": "An uncertainty principle for neural coding: Conjugate representations of\n  position and velocity are mapped onto firing rates and co-firing rates of\n  neural spike trains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hippocampal system contains neural populations that encode an animal's\nposition and velocity as it navigates through space. Here, we show that such\npopulations can embed two codes within their spike trains: a firing rate code\n(R) conveyed by within-cell spike intervals, and a co-firing rate code (R')\nconveyed by between-cell spike intervals. These two codes behave as conjugates\nof one another, obeying an analog of the uncertainty principle from physics:\ninformation conveyed in R comes at the expense of information in R', and vice\nversa. An exception to this trade-off occurs when spike trains encode a pair of\nconjugate variables, such as position and velocity, which do not compete for\ncapacity across R and R'. To illustrate this, we describe two biologically\ninspired methods for decoding R and R', referred to as sigma and sigma-chi\ndecoding, respectively. Simulations of head direction (HD) and grid cells show\nthat if firing rates are tuned for position (but not velocity), then position\nis recovered by sigma decoding, whereas velocity is recovered by sigma-chi\ndecoding. Conversely, simulations of oscillatory interference among\ntheta-modulated \"speed cells\" show that if co-firing rates are tuned for\nposition (but not velocity), then position is recovered by sigma-chi decoding,\nwhereas velocity is recovered by sigma decoding. Between these two extremes,\ninformation about both variables can be distributed across both channels, and\npartially recovered by both decoders. These results suggest that neurons with\ndifferent spatial and temporal tuning properties-such as speed versus grid\ncells-might not encode different information, but rather, distribute similar\ninformation about position and velocity in different ways across R and R'.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 22:02:06 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Grgurich", "Ryan", ""], ["Blair", "Hugh T.", ""]]}, {"id": "1912.11141", "submitter": "Matthias Karlbauer", "authors": "Matthias Karlbauer, Sebastian Otte, Hendrik P.A. Lensch, Thomas\n  Scholten, Volker Wulfmeyer, and Martin V. Butz", "title": "A Distributed Neural Network Architecture for Robust Non-Linear\n  Spatio-Temporal Prediction", "comments": "8 pages, 4 figures, video on\n  https://www.youtube.com/watch?v=4VHhHYeWTzo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a distributed spatio-temporal artificial neural network\narchitecture (DISTANA). It encodes mesh nodes using recurrent, neural\nprediction kernels (PKs), while neural transition kernels (TKs) transfer\ninformation between neighboring PKs, together modeling and predicting\nspatio-temporal time series dynamics. As a consequence, DISTANA assumes that\ngenerally applicable causes, which may be locally modified, generate the\nobserved data. DISTANA learns in a parallel, spatially distributed manner,\nscales to large problem spaces, is capable of approximating complex dynamics,\nand is particularly robust to overfitting when compared to other competitive\nANN models. Moreover, it is applicable to heterogeneously structured meshes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 23:15:17 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Karlbauer", "Matthias", ""], ["Otte", "Sebastian", ""], ["Lensch", "Hendrik P. A.", ""], ["Scholten", "Thomas", ""], ["Wulfmeyer", "Volker", ""], ["Butz", "Martin V.", ""]]}, {"id": "1912.11213", "submitter": "Taichi Haruna", "authors": "Taichi Haruna, Kohei Nakajima", "title": "Optimal short-term memory before the edge of chaos in driven random\n  recurrent networks", "comments": null, "journal-ref": "Phys. Rev. E 100 (2019) 062312", "doi": "10.1103/PhysRevE.100.062312", "report-no": null, "categories": "nlin.AO cond-mat.dis-nn cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of discrete-time nonlinear recurrent neural networks to store\ntime-varying small input signals is investigated by mean-field theory. The\ncombination of a small input strength and mean-field assumptions makes it\npossible to derive an approximate expression for the conditional probability\ndensity of the state of a neuron given a past input signal. From this\nconditional probability density, we can analytically calculate short-term\nmemory measures, such as memory capacity, mutual information, and Fisher\ninformation, and determine the relationships among these measures, which have\nnot been clarified to date to the best of our knowledge. We show that the\nnetwork contribution of these short-term memory measures peaks before the edge\nof chaos, where the dynamics of input-driven networks is stable but\ncorresponding systems without input signals are unstable.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 05:44:30 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Haruna", "Taichi", ""], ["Nakajima", "Kohei", ""]]}, {"id": "1912.11423", "submitter": "Sihao Huang", "authors": "Sihao Huang", "title": "Towards Multicellular Biological Deep Neural Nets Based on\n  Transcriptional Regulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neurons built on synthetic gene networks have potential\napplications ranging from complex cellular decision-making to bioreactor\nregulation. Furthermore, due to the high information throughput of natural\nsystems, it provides an interesting candidate for biologically-based\nsupercomputing and analog simulations of traditionally intractable problems. In\nthis paper, we propose an architecture for constructing multicellular neural\nnetworks and programmable nonlinear systems. We design an artificial neuron\nbased on gene regulatory networks and optimize its dynamics for modularity.\nUsing gene expression models, we simulate its ability to perform arbitrary\nlinear classifications from multiple inputs. Finally, we construct a two-layer\nneural network to demonstrate scalability and nonlinear decision boundaries and\ndiscuss future directions for utilizing uncontrolled neurons in computational\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 16:01:01 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 18:49:53 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Huang", "Sihao", ""]]}, {"id": "1912.11425", "submitter": "Christopher J. Anders", "authors": "Christopher J. Anders, Leander Weber, David Neumann, Wojciech Samek,\n  Klaus-Robert M\\\"uller, Sebastian Lapuschkin", "title": "Finding and Removing Clever Hans: Using Explanation Methods to Debug and\n  Improve Deep Models", "comments": "47 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Contemporary learning models for computer vision are typically trained on\nvery large (benchmark) datasets with millions of samples. These may, however,\ncontain biases, artifacts, or errors that have gone unnoticed and are\nexploitable by the model. In the worst case, the trained model does not learn a\nvalid and generalizable strategy to solve the problem it was trained for, and\nbecomes a 'Clever-Hans' (CH) predictor that bases its decisions on spurious\ncorrelations in the training data, potentially yielding an unrepresentative or\nunfair, and possibly even hazardous predictor. In this paper, we contribute by\nproviding a comprehensive analysis framework based on a scalable statistical\nanalysis of attributions from explanation methods for large data corpora. Based\non a recent technique - Spectral Relevance Analysis - we propose the following\ntechnical contributions and resulting findings: (a) a scalable quantification\nof artifactual and poisoned classes where the machine learning models under\nstudy exhibit CH behavior, (b) several approaches denoted as Class Artifact\nCompensation (ClArC), which are able to effectively and significantly reduce a\nmodel's CH behavior. I.e., we are able to un-Hans models trained on (poisoned)\ndatasets, such as the popular ImageNet data corpus. We demonstrate that ClArC,\ndefined in a simple theoretical framework, may be implemented as part of a\nNeural Network's training or fine-tuning process, or in a post-hoc manner by\ninjecting additional layers, preventing any further propagation of undesired CH\nfeatures, into the network architecture. Using our proposed methods, we provide\nqualitative and quantitative analyses of the biases and artifacts in various\ndatasets. We demonstrate that these insights can give rise to improved, more\nrepresentative and fairer models operating on implicitly cleaned data corpora.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:40:27 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 20:13:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Anders", "Christopher J.", ""], ["Weber", "Leander", ""], ["Neumann", "David", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Lapuschkin", "Sebastian", ""]]}, {"id": "1912.11443", "submitter": "Julian G\\\"oltz", "authors": "Julian G\\\"oltz, Laura Kriener, Andreas Baumbach, Sebastian\n  Billaudelle, Oliver Breitwieser, Benjamin Cramer, Dominik Dold, Akos Ferenc\n  Kungl, Walter Senn, Johannes Schemmel, Karlheinz Meier, Mihai Alexandru\n  Petrovici", "title": "Fast and energy-efficient neuromorphic deep learning with first-spike\n  times", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a biological agent operating under environmental pressure, energy\nconsumption and reaction times are of critical importance. Similarly,\nengineered systems are optimized for short time-to-solution and low\nenergy-to-solution characteristics. At the level of neuronal implementation,\nthis implies achieving the desired results with as few and as early spikes as\npossible. With time-to-first-spike coding both of these goals are inherently\nemerging features of learning. Here, we describe a rigorous derivation of a\nlearning rule for such first-spike times in networks of leaky\nintegrate-and-fire neurons, relying solely on input and output spike times, and\nshow how this mechanism can implement error backpropagation in hierarchical\nspiking networks. Furthermore, we emulate our framework on the BrainScaleS-2\nneuromorphic system and demonstrate its capability of harnessing the system's\nspeed and energy characteristics. Finally, we examine how our approach\ngeneralizes to other neuromorphic platforms by studying how its performance is\naffected by typical distortive effects induced by neuromorphic substrates.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 17:18:07 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 16:27:45 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 18:43:48 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 15:35:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["G\u00f6ltz", "Julian", ""], ["Kriener", "Laura", ""], ["Baumbach", "Andreas", ""], ["Billaudelle", "Sebastian", ""], ["Breitwieser", "Oliver", ""], ["Cramer", "Benjamin", ""], ["Dold", "Dominik", ""], ["Kungl", "Akos Ferenc", ""], ["Senn", "Walter", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Petrovici", "Mihai Alexandru", ""]]}, {"id": "1912.11527", "submitter": "Francisco Erivaldo Fernandes Junior", "authors": "Francisco Erivaldo Fernandes Junior, Gary G. Yen", "title": "Pruning Deep Convolutional Neural Networks Architectures with Evolution\n  Strategy", "comments": "Accepted at Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2020.11.009", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Currently, Deep Convolutional Neural Networks (DCNNs) are used to solve all\nkinds of problems in the field of machine learning and artificial intelligence\ndue to their learning and adaptation capabilities. However, most successful\nDCNN models have a high computational complexity making them difficult to\ndeploy on mobile or embedded platforms. This problem has prompted many\nresearchers to develop algorithms and approaches to help reduce the\ncomputational complexity of such models. One of them is called filter pruning,\nwhere convolution filters are eliminated to reduce the number of parameters\nand, consequently, the computational complexity of the given model. In the\npresent work, we propose a novel algorithm to perform filter pruning by using\nMulti-Objective Evolution Strategy (ES) algorithm, called DeepPruningES. Our\napproach avoids the need for using any knowledge during the pruning procedure\nand helps decision-makers by returning three pruned CNN models with different\ntrade-offs between performance and computational complexity. We show that\nDeepPruningES can significantly reduce a model's computational complexity by\ntesting it on three DCNN architectures: Convolutional Neural Networks (CNNs),\nResidual Neural Networks (ResNets), and Densely Connected Neural Networks\n(DenseNets).\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 20:48:00 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 13:13:47 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Junior", "Francisco Erivaldo Fernandes", ""], ["Yen", "Gary G.", ""]]}, {"id": "1912.11831", "submitter": "Mustafizur Rahman Shahid", "authors": "Mustafizur Rahman Shahid (SAMOVAR), Gregory Blanc (SAMOVAR), Zonghua\n  Zhang (SAMOVAR), Herv\\'e Debar (SAMOVAR)", "title": "Anomalous Communications Detection in IoT Networks Using Sparse\n  Autoencoders", "comments": null, "journal-ref": "2019 IEEE 18th International Symposium on Network Computing and\n  Applications (NCA), Sep 2019, Cambridge, United States. pp.1-5", "doi": "10.1109/NCA.2019.8935007", "report-no": null, "categories": "cs.CR cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, IoT devices have been widely deployed for enabling various smart\nservices, such as, smart home or e-healthcare. However, security remains as one\nof the paramount concern as many IoT devices are vulnerable. Moreover, IoT\nmalware are constantly evolving and getting more sophisticated. IoT devices are\nintended to perform very specific tasks, so their networking behavior is\nexpected to be reasonably stable and predictable. Any significant behavioral\ndeviation from the normal patterns would indicate anomalous events. In this\npaper, we present a method to detect anomalous network communications in IoT\nnetworks using a set of sparse autoencoders. The proposed approach allows us to\ndifferentiate malicious communications from legitimate ones. So that, if a\ndevice is compromised only malicious communications can be dropped while the\nservice provided by the device is not totally interrupted. To characterize\nnetwork behavior, bidirectional TCP flows are extracted and described using\nstatistics on the size of the first N packets sent and received, along with\nstatistics on the corresponding inter-arrival times between packets. A set of\nsparse autoencoders is then trained to learn the profile of the legitimate\ncommunications generated by an experimental smart home network. Depending on\nthe value of N, the developed model achieves attack detection rates ranging\nfrom 86.9% to 91.2%, and false positive rates ranging from 0.1% to 0.5%.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 10:47:35 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Shahid", "Mustafizur Rahman", "", "SAMOVAR"], ["Blanc", "Gregory", "", "SAMOVAR"], ["Zhang", "Zonghua", "", "SAMOVAR"], ["Debar", "Herv\u00e9", "", "SAMOVAR"]]}, {"id": "1912.11970", "submitter": "Natalia Arzeno", "authors": "Natalia M. Arzeno, Haris Vikalo", "title": "Evolutionary Clustering via Message Passing", "comments": "To be published in IEEE Transactions on Knowledge and Data\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are often interested in clustering objects that evolve over time and\nidentifying solutions to the clustering problem for every time step.\nEvolutionary clustering provides insight into cluster evolution and temporal\nchanges in cluster memberships while enabling performance superior to that\nachieved by independently clustering data collected at different time points.\nIn this paper we introduce evolutionary affinity propagation (EAP), an\nevolutionary clustering algorithm that groups data points by exchanging\nmessages on a factor graph. EAP promotes temporal smoothness of the solution to\nclustering time-evolving data by linking the nodes of the factor graph that are\nassociated with adjacent data snapshots, and introduces consensus nodes to\nenable cluster tracking and identification of cluster births and deaths. Unlike\nexisting evolutionary clustering methods that require additional processing to\napproximate the number of clusters or match them across time, EAP determines\nthe number of clusters and tracks them automatically. A comparison with\nexisting methods on simulated and experimental data demonstrates effectiveness\nof the proposed EAP algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 03:09:16 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Arzeno", "Natalia M.", ""], ["Vikalo", "Haris", ""]]}, {"id": "1912.12047", "submitter": "Sebastian Billaudelle", "authors": "Sebastian Billaudelle, Benjamin Cramer, Mihai A. Petrovici, Korbinian\n  Schreiber, David Kappel, Johannes Schemmel, Karlheinz Meier", "title": "Structural plasticity on an accelerated analog neuromorphic hardware\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In computational neuroscience, as well as in machine learning, neuromorphic\ndevices promise an accelerated and scalable alternative to neural network\nsimulations. Their neural connectivity and synaptic capacity depends on their\nspecific design choices, but is always intrinsically limited. Here, we present\na strategy to achieve structural plasticity that optimizes resource allocation\nunder these constraints by constantly rewiring the pre- and gpostsynaptic\npartners while keeping the neuronal fan-in constant and the connectome sparse.\nIn particular, we implemented this algorithm on the analog neuromorphic system\nBrainScaleS-2. It was executed on a custom embedded digital processor located\non chip, accompanying the mixed-signal substrate of spiking neurons and synapse\ncircuits. We evaluated our implementation in a simple supervised learning\nscenario, showing its ability to optimize the network topology with respect to\nthe nature of its training data, as well as its overall computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 10:15:58 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 08:20:35 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Billaudelle", "Sebastian", ""], ["Cramer", "Benjamin", ""], ["Petrovici", "Mihai A.", ""], ["Schreiber", "Korbinian", ""], ["Kappel", "David", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1912.12049", "submitter": "Luca Scrucca", "authors": "Luca Scrucca and Alessio Serafini", "title": "Projection pursuit based on Gaussian mixtures and evolutionary\n  algorithms", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 2019, 28:4,\n  847-860", "doi": "10.1080/10618600.2019.1598871", "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a projection pursuit (PP) algorithm based on Gaussian mixture\nmodels (GMMs). The negentropy obtained from a multivariate density estimated by\nGMMs is adopted as the PP index to be maximised. For a fixed dimension of the\nprojection subspace, the GMM-based density estimation is projected onto that\nsubspace, where an approximation of the negentropy for Gaussian mixtures is\ncomputed. Then, Genetic Algorithms (GAs) are used to find the optimal,\northogonal projection basis by maximising the former approximation. We show\nthat this semi-parametric approach to PP is flexible and allows highly\ninformative structures to be detected, by projecting multivariate datasets onto\na subspace, where the data can be feasibly visualised. The performance of the\nproposed approach is shown on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 10:25:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Scrucca", "Luca", ""], ["Serafini", "Alessio", ""]]}, {"id": "1912.12187", "submitter": "Amina Asif", "authors": "Fayyaz ul Amir Afsar Minhas and Amina Asif", "title": "Learning Neural Activations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial neuron is modelled as a weighted summation followed by an\nactivation function which determines its output. A wide variety of activation\nfunctions such as rectified linear units (ReLU), leaky-ReLU, Swish, MISH, etc.\nhave been explored in the literature. In this short paper, we explore what\nhappens when the activation function of each neuron in an artificial neural\nnetwork is learned natively from data alone. This is achieved by modelling the\nactivation function of each neuron as a small neural network whose weights are\nshared by all neurons in the original network. We list our primary findings in\nthe conclusions section. The code for our analysis is available at:\nhttps://github.com/amina01/Learning-Neural-Activations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:52:07 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Minhas", "Fayyaz ul Amir Afsar", ""], ["Asif", "Amina", ""]]}, {"id": "1912.12636", "submitter": "Tzofnat Greenberg-Toledo", "authors": "Tzofnat Greenberg Toledo, Ben Perach, Daniel Soudry and Shahar\n  Kvatinsky", "title": "MTJ-Based Hardware Synapse Design for Quantized Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized neural networks (QNNs) are being actively researched as a solution\nfor the computational complexity and memory intensity of deep neural networks.\nThis has sparked efforts to develop algorithms that support both inference and\ntraining with quantized weight and activation values without sacrificing\naccuracy. A recent example is the GXNOR framework for stochastic training of\nternary and binary neural networks. In this paper, we introduce a novel\nhardware synapse circuit that uses magnetic tunnel junction (MTJ) devices to\nsupport the GXNOR training. Our solution enables processing near memory (PNM)\nof QNNs, therefore can further reduce the data movements from and into the\nmemory. We simulated MTJ-based stochastic training of a TNN over the MNIST and\nSVHN datasets and achieved an accuracy of 98.61% and 93.99%, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:36:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Toledo", "Tzofnat Greenberg", ""], ["Perach", "Ben", ""], ["Soudry", "Daniel", ""], ["Kvatinsky", "Shahar", ""]]}, {"id": "1912.12667", "submitter": "Yi Mei", "authors": "Yuzhou Zhang, Yi Mei, Buzhong Zhang, Keqin Jiang", "title": "Divide-and-Conquer Large Scale Capacitated Arc Routing Problems with\n  Route Cutting Off Decomposition", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2020.11.011", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacitated arc routing problem is a very important problem with many\npractical applications. This paper focuses on the large scale capacitated arc\nrouting problem. Traditional solution optimization approaches usually fail\nbecause of their poor scalability. The divide-and-conquer strategy has achieved\ngreat success in solving large scale optimization problems by decomposing the\noriginal large problem into smaller sub-problems and solving them separately.\nFor arc routing, a commonly used divide-and-conquer strategy is to divide the\ntasks into subsets, and then solve the sub-problems induced by the task subsets\nseparately. However, the success of a divide-and-conquer strategy relies on a\nproper task division, which is non-trivial due to the complex interactions\nbetween the tasks. This paper proposes a novel problem decomposition operator,\nnamed the route cutting off operator, which considers the interactions between\nthe tasks in a sophisticated way. To examine the effectiveness of the route\ncutting off operator, we integrate it with two state-of-the-art\ndivide-and-conquer algorithms, and compared with the original counterparts on a\nwide range of benchmark instances. The results show that the route cutting off\noperator can improve the effectiveness of the decomposition, and lead to\nsignificantly better results especially when the problem size is very large and\nthe time budget is very tight.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 14:52:21 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 10:30:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Yuzhou", ""], ["Mei", "Yi", ""], ["Zhang", "Buzhong", ""], ["Jiang", "Keqin", ""]]}, {"id": "1912.12719", "submitter": "Mirza Rami\\v{c}i\\'c", "authors": "Mirza Ramicic, Andrea Bonarini", "title": "Augmented Replay Memory in Reinforcement Learning With Continuous\n  Control", "comments": null, "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems (2021)\n  1-12", "doi": "10.1109/TCDS.2021.3050723", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reinforcement learning agents are currently able to process an\nincreasing amount of data by converting it into a higher order value functions.\nThis expansion of the information collected from the environment increases the\nagent's state space enabling it to scale up to a more complex problems but also\nincreases the risk of forgetting by learning on redundant or conflicting data.\nTo improve the approximation of a large amount of data, a random mini-batch of\nthe past experiences that are stored in the replay memory buffer is often\nreplayed at each learning step. The proposed work takes inspiration from a\nbiological mechanism which act as a protective layer of human brain higher\ncognitive functions: active memory consolidation mitigates the effect of\nforgetting of previous memories by dynamically processing the new ones. The\nsimilar dynamics are implemented by a proposed augmented memory replay AMR\ncapable of optimizing the replay of the experiences from the agent's memory\nstructure by altering or augmenting their relevance. Experimental results show\nthat an evolved AMR augmentation function capable of increasing the\nsignificance of the specific memories is able to further increase the stability\nand convergence speed of the learning algorithms dealing with the complexity of\ncontinuous action domains.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 20:07:18 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ramicic", "Mirza", ""], ["Bonarini", "Andrea", ""]]}, {"id": "1912.12814", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Jiang Wang, Joshua Slocum, Ming-Hsuan Yang, Shengyang\n  Dai, Shuicheng Yan, Jiashi Feng", "title": "RC-DARTS: Resource Constrained Differentiable Architecture Search", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances show that Neural Architectural Search (NAS) method is able to\nfind state-of-the-art image classification deep architectures. In this paper,\nwe consider the one-shot NAS problem for resource constrained applications.\nThis problem is of great interest because it is critical to choose different\narchitectures according to task complexity when the resource is constrained.\nPrevious techniques are either too slow for one-shot learning or does not take\nthe resource constraint into consideration. In this paper, we propose the\nresource constrained differentiable architecture search (RC-DARTS) method to\nlearn architectures that are significantly smaller and faster while achieving\ncomparable accuracy. Specifically, we propose to formulate the RC-DARTS task as\na constrained optimization problem by adding the resource constraint. An\niterative projection method is proposed to solve the given constrained\noptimization problem. We also propose a multi-level search strategy to enable\nlayers at different depths to adaptively learn different types of neural\narchitectures. Through extensive experiments on the Cifar10 and ImageNet\ndatasets, we show that the RC-DARTS method learns lightweight neural\narchitectures which have smaller model size and lower computational complexity\nwhile achieving comparable or better performances than the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 05:02:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jin", "Xiaojie", ""], ["Wang", "Jiang", ""], ["Slocum", "Joshua", ""], ["Yang", "Ming-Hsuan", ""], ["Dai", "Shengyang", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1912.12980", "submitter": "Benjamin Cramer", "authors": "Sebastian Billaudelle, Yannik Stradmann, Korbinian Schreiber, Benjamin\n  Cramer, Andreas Baumbach, Dominik Dold, Julian G\\\"oltz, Akos F. Kungl, Timo\n  C. Wunderlich, Andreas Hartel, Eric M\\\"uller, Oliver Breitwieser, Christian\n  Mauch, Mitja Kleider, Andreas Gr\\\"ubl, David St\\\"ockel, Christian Pehle,\n  Arthur Heimbrecht, Philipp Spilger, Gerd Kiene, Vitali Karasenko, Walter\n  Senn, Mihai A. Petrovici, Johannes Schemmel, Karlheinz Meier", "title": "Versatile emulation of spiking neural networks on an accelerated\n  neuromorphic substrate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present first experimental results on the novel BrainScaleS-2 neuromorphic\narchitecture based on an analog neuro-synaptic core and augmented by embedded\nmicroprocessors for complex plasticity and experiment control. The high\nacceleration factor of 1000 compared to biological dynamics enables the\nexecution of computationally expensive tasks, by allowing the fast emulation of\nlong-duration experiments or rapid iteration over many consecutive trials. The\nflexibility of our architecture is demonstrated in a suite of five distinct\nexperiments, which emphasize different aspects of the BrainScaleS-2 system.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:12:14 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Billaudelle", "Sebastian", ""], ["Stradmann", "Yannik", ""], ["Schreiber", "Korbinian", ""], ["Cramer", "Benjamin", ""], ["Baumbach", "Andreas", ""], ["Dold", "Dominik", ""], ["G\u00f6ltz", "Julian", ""], ["Kungl", "Akos F.", ""], ["Wunderlich", "Timo C.", ""], ["Hartel", "Andreas", ""], ["M\u00fcller", "Eric", ""], ["Breitwieser", "Oliver", ""], ["Mauch", "Christian", ""], ["Kleider", "Mitja", ""], ["Gr\u00fcbl", "Andreas", ""], ["St\u00f6ckel", "David", ""], ["Pehle", "Christian", ""], ["Heimbrecht", "Arthur", ""], ["Spilger", "Philipp", ""], ["Kiene", "Gerd", ""], ["Karasenko", "Vitali", ""], ["Senn", "Walter", ""], ["Petrovici", "Mihai A.", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1912.13002", "submitter": "Gustavo de Rosa", "authors": "Gustavo H. de Rosa, Douglas Rodrigues, Jo\\~ao P. Papa", "title": "Opytimizer: A Nature-Inspired Python Optimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimization aims at selecting a feasible set of parameters in an attempt to\nsolve a particular problem, being applied in a wide range of applications, such\nas operations research, machine learning fine-tuning, and control engineering,\namong others. Nevertheless, traditional iterative optimization methods use the\nevaluation of gradients and Hessians to find their solutions, not being\npractical due to their computational burden and when working with non-convex\nfunctions. Recent biological-inspired methods, known as meta-heuristics, have\narisen in an attempt to fulfill these problems. Even though they do not\nguarantee to find optimal solutions, they usually find a suitable solution. In\nthis paper, we proposed a Python-based meta-heuristic optimization framework\ndenoted as Opytimizer. Several methods and classes are implemented to provide a\nuser-friendly workspace among diverse meta-heuristics, ranging from\nevolutionary- to swarm-based techniques.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:50:55 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 15:15:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["de Rosa", "Gustavo H.", ""], ["Rodrigues", "Douglas", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "1912.13201", "submitter": "Erfan Amini", "authors": "Erfan Amini and Seyed Taghi Omid Naeeni and Pedram Ghaderi and\n  Fereidoun Amini", "title": "Investigating Wave Energy Potential in Southern Coasts of the Caspian\n  Sea Using Grey Wolf Optimizer Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a significantly accelerating trend in the application of the marine\nwave energy converters in recent years. As a result, it is imperative to adopt\na suitable point for implementing these systems. Besides, the Caspian Sea, as\none of the most important marine renewable energy sources in Asia, is capable\nof supplying the coastal areas with a large amount of energy. Therefore, areas\naround nine ports in the southern coasts of the Caspian Sea were selected to\nmeasure their wave energy potential. Initially, the amount of energy on these\npoints was measured using the irregular energy theory. It was observed that the\nwave power was higher in the southwestern areas (within the Kiashahr coast and\nAnzali port) than the southeastern areas. A new approach was developed to\ncompare these points and measure their fitnesses in supplying the maximum\nenergy using the Grey Wolf optimizer (GWO) algorithm and time history analysis.\nIn this method, the optimal parameters were first extracted from the algorithm\nfor assessing the points within the southern areas of the Caspian Sea. These\nvalues were regarded as the assessment indices. Then, the fitness of each point\nwas obtained using the correlation function and the norm vector to present the\nmost optimal position with maximum wave energy exploitation potential. This new\napproach was validated with analytical data, and its accuracy in predicting and\ncomparing the wave power on different points was approved. Finally, by a\nside-by-side comparison of the parameters affecting the wave energy, the\noptimum range of significant wave height and wave energy period was achieved.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 07:02:50 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 16:53:20 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Amini", "Erfan", ""], ["Naeeni", "Seyed Taghi Omid", ""], ["Ghaderi", "Pedram", ""], ["Amini", "Fereidoun", ""]]}, {"id": "1912.13430", "submitter": "Alberto Camacho", "authors": "Alberto Camacho, Sheila A. McIlraith", "title": "Towards Neural-Guided Program Synthesis for Linear Temporal Logic\n  Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL cs.GT cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing a program that realizes a logical specification is a classical\nproblem in computer science. We examine a particular type of program synthesis,\nwhere the objective is to synthesize a strategy that reacts to a potentially\nadversarial environment while ensuring that all executions satisfy a Linear\nTemporal Logic (LTL) specification. Unfortunately, exact methods to solve\nso-called LTL synthesis via logical inference do not scale. In this work, we\ncast LTL synthesis as an optimization problem. We employ a neural network to\nlearn a Q-function that is then used to guide search, and to construct programs\nthat are subsequently verified for correctness. Our method is unique in\ncombining search with deep learning to realize LTL synthesis. In our\nexperiments the learned Q-function provides effective guidance for synthesis\nproblems with relatively small specifications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:09:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Camacho", "Alberto", ""], ["McIlraith", "Sheila A.", ""]]}, {"id": "1912.13490", "submitter": "Gianluca Baldassarre PhD", "authors": "Gianluca Baldassarre and Giovanni Granato", "title": "Representation Internal-Manipulation (RIM): A Neuro-Inspired\n  Computational Theory of Consciousness", "comments": "16 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theories, based on neuroscientific and psychological empirical evidence\nand on computational concepts, have been elaborated to explain the emergence of\nconsciousness in the central nervous system. These theories propose key\nfundamental mechanisms to explain consciousness, but they only partially\nconnect such mechanisms to the possible functional and adaptive role of\nconsciousness. Recently, some cognitive and neuroscientific models try to solve\nthis gap by linking consciousness to various aspects of goal-directed\nbehaviour, the pivotal cognitive process that allows mammals to flexibly act in\nchallenging environments. Here we propose the Representation\nInternal-Manipulation (RIM) theory of consciousness, a theory that links the\nmain elements of consciousness theories to components and functions of\ngoal-directed behaviour, ascribing a central role for consciousness to the\ngoal-directed manipulation of internal representations. This manipulation\nrelies on four specific computational operations to perform the flexible\ninternal adaptation of all key elements of goal-directed computation, from the\nrepresentations of objects to those of goals, actions, and plans. Finally, we\npropose the concept of `manipulation agency' relating the sense of agency to\nthe internal manipulation of representations. This allows us to propose that\nthe subjective experience of consciousness is associated to the human capacity\nto generate and control a simulated internal reality that is vividly perceived\nand felt through the same perceptual and emotional mechanisms used to tackle\nthe external world.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:45:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Baldassarre", "Gianluca", ""], ["Granato", "Giovanni", ""]]}, {"id": "1912.13503", "submitter": "Alexander Sax", "authors": "Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra\n  Malik", "title": "Side-Tuning: A Baseline for Network Adaptation via Additive Side\n  Networks", "comments": "In ECCV 2020 (Spotlight). For more, see project website and code at\n  http://sidetuning.berkeley.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training a neural network for a desired task, one may prefer to adapt a\npre-trained network rather than starting from randomly initialized weights.\nAdaptation can be useful in cases when training data is scarce, when a single\nlearner needs to perform multiple tasks, or when one wishes to encode priors in\nthe network. The most commonly employed approaches for network adaptation are\nfine-tuning and using the pre-trained network as a fixed feature extractor,\namong others.\n  In this paper, we propose a straightforward alternative: side-tuning.\nSide-tuning adapts a pre-trained network by training a lightweight \"side\"\nnetwork that is fused with the (unchanged) pre-trained network via summation.\nThis simple method works as well as or better than existing solutions and it\nresolves some of the basic issues with fine-tuning, fixed features, and other\ncommon approaches. In particular, side-tuning is less prone to overfitting, is\nasymptotically consistent, and does not suffer from catastrophic forgetting in\nincremental learning. We demonstrate the performance of side-tuning under a\ndiverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy),\nreinforcement learning, imitation learning (visual navigation in Habitat), NLP\nquestion-answering (SQuAD v2), and single-task transfer learning (Taskonomy),\nwith consistently promising results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:52:32 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 00:02:34 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 22:36:58 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 00:44:06 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Zhang", "Jeffrey O", ""], ["Sax", "Alexander", ""], ["Zamir", "Amir", ""], ["Guibas", "Leonidas", ""], ["Malik", "Jitendra", ""]]}]