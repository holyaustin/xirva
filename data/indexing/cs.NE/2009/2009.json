[{"id": "2009.00110", "submitter": "Patrik Christen", "authors": "Olivier Del Fabbro and Patrik Christen", "title": "Adaptation and Control in the Allagmatic Method", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Control was from its very beginning an important concept in Cybernetics.\nLater on, with the works of W. Ross Ashby, for example, biological concepts\nsuch as adaptation were interpreted in the light of cybernetic systems theory.\nAdaptation is the process by which a system is capable of regulating or\ncontrolling itself in order to adapt to changes of its inner and outer\nenvironment in order to maintain a homeostatic state. In earlier works we have\ndeveloped a metamodel that on the one hand refers to cybernetic concepts such\nas structure, operation, and system, and on the other to the philosophy of\nindividuation of Gilbert Simondon. The result is the so-called allagmatic\nmethod that is capable of creating concrete models of systems such as\nartificial neural networks and cellular automata starting from abstract\nbuilding blocks. In this paper, we add to our already existing method the\ncybernetic concepts of control and especially adaptation. In regard to the\nmetamodel, we rely again on philosophical theories, this time the philosophy of\norganism of Alfred N. Whitehead. We show how these new meta-theoretical\nconcepts are described formally and how they are implemented in program code.\nWe also show what role they play in simple experiments. We conclude that\nphilosophical abstract concepts help to better understand the process of\ncreating computer models and their control and adaptation. In the outlook we\ndiscuss how the allagmatic method needs to be extended in order to cover the\nfield of complex systems and Norbert Wiener's ideas on control.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:21:16 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 22:08:28 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 21:49:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Del Fabbro", "Olivier", ""], ["Christen", "Patrik", ""]]}, {"id": "2009.00112", "submitter": "Francesco Caravelli", "authors": "Forrest C. Sheldon, Artemy Kolchinsky, Francesco Caravelli", "title": "The Computational Capacity of Memristor Reservoirs", "comments": "18 pages double columns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.stat-mech cs.ET nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a machine learning paradigm in which a\nhigh-dimensional dynamical system, or \\emph{reservoir}, is used to approximate\nand perform predictions on time series data. Its simple training procedure\nallows for very large reservoirs that can provide powerful computational\ncapabilities. The scale, speed and power-usage characteristics of reservoir\ncomputing could be enhanced by constructing reservoirs out of electronic\ncircuits, but this requires a precise understanding of how such circuits\nprocess and store information. We analyze the feasibility and optimal design of\nsuch reservoirs by considering the equations of motion of circuits that include\nboth linear elements (resistors, inductors, and capacitors) and nonlinear\nmemory elements (called memristors). This complements previous studies, which\nhave examined such systems through simulation and experiment. We provide\nanalytic results regarding the fundamental feasibility of such reservoirs, and\ngive a systematic characterization of their computational properties, examining\nthe types of input-output relationships that may be approximated. This allows\nus to design reservoirs with optimal properties in terms of their ability to\nreconstruct a certain signal (or functions thereof). In particular, by\nintroducing measures of the total linear and nonlinear computational capacities\nof the reservoir, we are able to design electronic circuits whose total\ncomputation capacity scales linearly with the system size. Comparison with\nconventional echo state reservoirs show that these electronic reservoirs can\nmatch or exceed their performance in a form that may be directly implemented in\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:24:45 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 17:30:12 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Sheldon", "Forrest C.", ""], ["Kolchinsky", "Artemy", ""], ["Caravelli", "Francesco", ""]]}, {"id": "2009.00457", "submitter": "Harideep Nair", "authors": "Harideep Nair, John Paul Shen, James E. Smith", "title": "Direct CMOS Implementation of Neuromorphic Temporal Neural Networks for\n  Sensory Processing", "comments": "Submission Under Review for an IEEE Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Neural Networks (TNNs) use time as a resource to represent and\nprocess information, mimicking the behavior of the mammalian neocortex. This\nwork focuses on implementing TNNs using off-the-shelf digital CMOS technology.\nA microarchitecture framework is introduced with a hierarchy of building blocks\nincluding: multi-neuron columns, multi-column layers, and multi-layer TNNs. We\npresent the direct CMOS gate-level implementation of the multi-neuron column\nmodel as the key building block for TNNs. Post-synthesis results are obtained\nusing Synopsys tools and the 45 nm CMOS standard cell library. The TNN\nmicroarchitecture framework is embodied in a set of characteristic equations\nfor assessing the total gate count, die area, compute time, and power\nconsumption for any TNN design. We develop a multi-layer TNN prototype of 32M\ngates. In 7 nm CMOS process, it consumes only 1.54 mm^2 die area and 7.26 mW\npower and can process 28x28 images at 107M FPS (9.34 ns per image). We evaluate\nthe prototype's performance and complexity relative to a recent\nstate-of-the-art TNN model.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 20:36:34 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Nair", "Harideep", ""], ["Shen", "John Paul", ""], ["Smith", "James E.", ""]]}, {"id": "2009.00581", "submitter": "Matthew Evanusa", "authors": "Matthew Evanusa and Cornelia Fermuller and Yiannis Aloimonos", "title": "A Deep 2-Dimensional Dynamical Spiking Neuronal Network for Temporal\n  Encoding trained with STDP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The brain is known to be a highly complex, asynchronous dynamical system that\nis highly tailored to encode temporal information. However, recent deep\nlearning approaches to not take advantage of this temporal coding. Spiking\nNeural Networks (SNNs) can be trained using biologically-realistic learning\nmechanisms, and can have neuronal activation rules that are biologically\nrelevant. This type of network is also structured fundamentally around\naccepting temporal information through a time-decaying voltage update, a kind\nof input that current rate-encoding networks have difficulty with. Here we show\nthat a large, deep layered SNN with dynamical, chaotic activity mimicking the\nmammalian cortex with biologically-inspired learning rules, such as STDP, is\ncapable of encoding information from temporal data. We argue that the\nrandomness inherent in the network weights allow the neurons to form groups\nthat encode the temporal data being inputted after self-organizing with STDP.\nWe aim to show that precise timing of input stimulus is critical in forming\nsynchronous neural groups in a layered network. We analyze the network in terms\nof network entropy as a metric of information transfer. We hope to tackle two\nproblems at once: the creation of artificial temporal neural systems for\nartificial intelligence, as well as solving coding mechanisms in the brain.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:12:18 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Evanusa", "Matthew", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2009.00612", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Moncef Gabbouj", "title": "Operational vs Convolutional Neural Networks for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently become a favored technique\nfor image denoising due to its adaptive learning ability, especially with a\ndeep configuration. However, their efficacy is inherently limited owing to\ntheir homogenous network formation with the unique use of linear convolution.\nIn this study, we propose a heterogeneous network model which allows greater\nflexibility for embedding additional non-linearity at the core of the data\ntransformation. To this end, we propose the idea of an operational neuron or\nOperational Neural Networks (ONN), which enables a flexible non-linear and\nheterogeneous configuration employing both inter and intra-layer neuronal\ndiversity. Furthermore, we propose a robust operator search strategy inspired\nby the Hebbian theory, called the Synaptic Plasticity Monitoring (SPM) which\ncan make data-driven choices for non-linearities in any architecture. An\nextensive set of comparative evaluations of ONNs and CNNs over two severe image\ndenoising problems yield conclusive evidence that ONNs enriched by non-linear\noperators can achieve a superior denoising performance against CNNs with both\nequivalent and well-known deep configurations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:15:28 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2009.00841", "submitter": "Waytehad Rose Moskolai", "authors": "Waytehad Moskola\\\"i, Wahabou Abdou (Le2i), Albert Dipanda (Le2i), Dina\n  Taiwe Kolyang (UMa)", "title": "Application of LSTM architectures for next frame forecasting in\n  Sentinel-1 images time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L'analyse pr{\\'e}dictive permet d'estimer les tendances des\n{\\'e}v{\\`e}nements futurs. De nos jours, les algorithmes Deep Learning\npermettent de faire de bonnes pr{\\'e}dictions. Cependant, pour chaque type de\nprobl{\\`e}me donn{\\'e}, il est n{\\'e}cessaire de choisir l'architecture\noptimale. Dans cet article, les mod{\\`e}les Stack-LSTM, CNN-LSTM et ConvLSTM\nsont appliqu{\\'e}s {\\`a} une s{\\'e}rie temporelle d'images radar sentinel-1, le\nbut {\\'e}tant de pr{\\'e}dire la prochaine occurrence dans une s{\\'e}quence. Les\nr{\\'e}sultats exp{\\'e}rimentaux {\\'e}valu{\\'e}s {\\`a} l'aide des indicateurs de\nperformance tels que le RMSE et le MAE, le temps de traitement et l'index de\nsimilarit{\\'e} SSIM, montrent que chacune des trois architectures peut produire\nde bons r{\\'e}sultats en fonction des param{\\`e}tres utilis{\\'e}s.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 06:44:16 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Moskola\u00ef", "Waytehad", "", "Le2i"], ["Abdou", "Wahabou", "", "Le2i"], ["Dipanda", "Albert", "", "Le2i"], ["Kolyang", "Dina Taiwe", "", "UMa"]]}, {"id": "2009.00845", "submitter": "Wouter Kouw", "authors": "Wouter M Kouw", "title": "Online system identification in a Duffing oscillator by free energy\n  minimisation", "comments": "10 pages, 5 figures. Accepted to the International Workshop on Active\n  Inference (final author version)", "journal-ref": null, "doi": "10.1007/978-3-030-64919-7_6", "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online system identification is the estimation of parameters of a dynamical\nsystem, such as mass or friction coefficients, for each measurement of the\ninput and output signals. Here, the nonlinear stochastic differential equation\nof a Duffing oscillator is cast to a generative model and dynamical parameters\nare inferred using variational message passing on a factor graph of the model.\nThe approach is validated with an experiment on data from an electronic\nimplementation of a Duffing oscillator. The proposed inference procedure\nperforms as well as offline prediction error minimisation in a state-of-the-art\nnonlinear model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 06:51:56 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kouw", "Wouter M", ""]]}, {"id": "2009.00945", "submitter": "Christos Koutlis", "authors": "Christos Koutlis, Symeon Papadopoulos, Manos Schinas, Ioannis\n  Kompatsiaris", "title": "LAVARNET: Neural Network Modeling of Causal Variable Relationships for\n  Multivariate Time Series Forecasting", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2020.106685", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series forecasting is of great importance to many\nscientific disciplines and industrial sectors. The evolution of a multivariate\ntime series depends on the dynamics of its variables and the connectivity\nnetwork of causal interrelationships among them. Most of the existing time\nseries models do not account for the causal effects among the system's\nvariables and even if they do they rely just on determining the\nbetween-variables causality network. Knowing the structure of such a complex\nnetwork and even more specifically knowing the exact lagged variables that\ncontribute to the underlying process is crucial for the task of multivariate\ntime series forecasting. The latter is a rather unexplored source of\ninformation to leverage. In this direction, here a novel neural network-based\narchitecture is proposed, termed LAgged VAriable Representation NETwork\n(LAVARNET), which intrinsically estimates the importance of lagged variables\nand combines high dimensional latent representations of them to predict future\nvalues of time series. Our model is compared with other baseline and state of\nthe art neural network architectures on one simulated data set and four real\ndata sets from meteorology, music, solar activity, and finance areas. The\nproposed architecture outperforms the competitive architectures in most of the\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:57:28 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Koutlis", "Christos", ""], ["Papadopoulos", "Symeon", ""], ["Schinas", "Manos", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2009.00990", "submitter": "Donya Yazdani", "authors": "D. Corus, P. S. Oliveto, D. Yazdani", "title": "Fast Immune System Inspired Hypermutation Operators for Combinatorial\n  Optimisation", "comments": "arXiv admin note: text overlap with arXiv:1806.00299", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various studies have shown that immune system inspired hypermutation\noperators can allow artificial immune systems (AIS) to be very efficient at\nescaping local optima of multimodal optimisation problems. However, this\nefficiency comes at the expense of considerably slower runtimes during the\nexploitation phase compared to standard evolutionary algorithms. We propose\nmodifications to the traditional `hypermutations with mutation potential' (HMP)\nthat allow them to be efficient at exploitation as well as maintaining their\neffective explorative characteristics. Rather than deterministically evaluating\nfitness after each bit-flip of a hypermutation, we sample the fitness function\nstochastically with a `parabolic' distribution which allows the `stop at first\nconstructive mutation' (FCM) variant of HMP to reduce the linear amount of\nwasted function evaluations when no improvement is found to a constant. The\nstochastic distribution also allows the removal of the FCM mechanism altogether\nas originally desired in the design of the HMP operators. We rigorously prove\nthe effectiveness of the proposed operators for all the benchmark functions\nwhere the performance of HMP is rigorously understood in the literature and\nvalidating the gained insights to show linear speed-ups for the identification\nof high quality approximate solutions to classical NP-Hard problems from\ncombinatorial optimisation. We then show the superiority of the HMP operators\nto the traditional ones in an analysis of the complete standard Opt-IA AIS,\nwhere the stochastic evaluation scheme allows HMP and ageing operators to work\nin harmony. Through a comparative performance study of other `fast mutation'\noperators from the literature, we conclude that a power-law distribution for\nthe parabolic evaluation scheme is the best compromise in black box scenarios\nwhere little problem knowledge is available.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:22:57 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Corus", "D.", ""], ["Oliveto", "P. S.", ""], ["Yazdani", "D.", ""]]}, {"id": "2009.01498", "submitter": "Kurt Mehlhorn", "authors": "Vincenzo Bonifaci and Enrico Facca and Frederic Folz and Andreas\n  Karrenbauer and Pavel Kolev and Kurt Mehlhorn and Giovanna Morigi and\n  Golnoosh Shahkarami and Quentin Vermande", "title": "Physarum Multi-Commodity Flow Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wet-lab experiments, the slime mold Physarum polycephalum has demonstrated\nits ability to solve shortest path problems and to design efficient networks.\nFor the shortest path problem, a mathematical model for the evolution of the\nslime is available and it has been shown in computer experiments and through\nmathematical analysis that the dynamics solves the shortest path problem. In\nthis paper, we introduce a dynamics for the network design problem. We\nformulate network design as the problem of constructing a network that\nefficiently supports a multi-commodity flow problem. We investigate the\ndynamics in computer simulations and analytically. The simulations show that\nthe dynamics is able to construct efficient and elegant networks. In the\ntheoretical part we show that the dynamics minimizes an objective combining the\ncost of the network and the cost of routing the demands through the network. We\nalso give alternative characterization of the optimum solution.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 07:48:48 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 15:17:07 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 11:36:33 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 21:05:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bonifaci", "Vincenzo", ""], ["Facca", "Enrico", ""], ["Folz", "Frederic", ""], ["Karrenbauer", "Andreas", ""], ["Kolev", "Pavel", ""], ["Mehlhorn", "Kurt", ""], ["Morigi", "Giovanna", ""], ["Shahkarami", "Golnoosh", ""], ["Vermande", "Quentin", ""]]}, {"id": "2009.01527", "submitter": "Nicolas Skatchkovsky", "authors": "Nicolas Skatchkovsky, Hyeryung Jang, Osvaldo Simeone", "title": "End-to-End Learning of Neuromorphic Wireless Systems for Low-Power Edge\n  Artificial Intelligence", "comments": "To be presented at Asilomar 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel \"all-spike\" low-power solution for remote\nwireless inference that is based on neuromorphic sensing, Impulse Radio (IR),\nand Spiking Neural Networks (SNNs). In the proposed system, event-driven\nneuromorphic sensors produce asynchronous time-encoded data streams that are\nencoded by an SNN, whose output spiking signals are pulse modulated via IR and\ntransmitted over general frequence-selective channels; while the receiver's\ninputs are obtained via hard detection of the received signals and fed to an\nSNN for classification. We introduce an end-to-end training procedure that\ntreats the cascade of encoder, channel, and decoder as a probabilistic\nSNN-based autoencoder that implements Joint Source-Channel Coding (JSCC). The\nproposed system, termed NeuroJSCC, is compared to conventional synchronous\nframe-based and uncoded transmissions in terms of latency and accuracy. The\nexperiments confirm that the proposed end-to-end neuromorphic edge architecture\nprovides a promising framework for efficient and low-latency remote sensing,\ncommunication, and inference.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:10:16 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Skatchkovsky", "Nicolas", ""], ["Jang", "Hyeryung", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2009.01573", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes, Lu\\'is A. Alexandre", "title": "Auto-Classifier: A Robust Defect Detector Based on an AutoML Head", "comments": "12 pages, 2 figures. Published in ICONIP2020, proceedings published\n  in the Springer's series of Lecture Notes in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach for surface defect detection is the use of hand-crafted\nfeature-based methods. However, this falls short when conditions vary that\naffect extracted images. So, in this paper, we sought to determine how well\nseveral state-of-the-art Convolutional Neural Networks perform in the task of\nsurface defect detection. Moreover, we propose two methods: CNN-Fusion, that\nfuses the prediction of all the networks into a final one, and Auto-Classifier,\nwhich is a novel proposal that improves a Convolutional Neural Network by\nmodifying its classification component using AutoML. We carried out experiments\nto evaluate the proposed methods in the task of surface defect detection using\ndifferent datasets from DAGM2007. We show that the use of Convolutional Neural\nNetworks achieves better results than traditional methods, and also, that\nAuto-Classifier out-performs all other methods, by achieving 100% accuracy and\n100% AUC results throughout all the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:39:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lopes", "Vasco", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "2009.01664", "submitter": "Kai Dresia", "authors": "Kai Dresia, Simon Jentzsch, G\\\"unther Waxenegger-Wilfing, Robson Hahn,\n  Jan Deeken, Michael Oschwald, Fabio Mota", "title": "Multidisciplinary Design Optimization of Reusable Launch Vehicles for\n  Different Propellants and Objectives", "comments": null, "journal-ref": null, "doi": "10.2514/1.A34944", "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the optimal design of a new launch vehicle is most important\nsince design decisions made in the early development phase limit the vehicles'\nlater performance and determines the associated costs. Reusing the first stage\nvia retro-propulsive landing increases the complexity even more. Therefore, we\ndevelop an optimization framework for partially reusable launch vehicles, which\nenables multidisciplinary design studies. The framework contains suitable mass\nestimates of all essential subsystems and a routine to calculate the needed\npropellant for the ascent and landing maneuvers. For design optimization, the\nframework can be coupled with a genetic algorithm. The overall goal is to\nreveal the implications of different propellant combinations and objective\nfunctions on the launcher's optimal design for various mission scenarios. The\nresults show that the optimization objective influences the most suitable\npropellant choice and the overall launcher design, concerning staging, weight,\nsize, and rocket engine parameters. In terms of gross lift-off weight, liquid\nhydrogen seems to be favorable. When optimizing for a minimum structural mass\nor an expandable structural mass, hydrocarbon-based solutions show better\nresults. Finally, launch vehicles using a hydrocarbon fuel in the first stage\nand liquid hydrogen in the upper stage are an appealing alternative, combining\nboth fuels' benefits.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 13:48:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Dresia", "Kai", ""], ["Jentzsch", "Simon", ""], ["Waxenegger-Wilfing", "G\u00fcnther", ""], ["Hahn", "Robson", ""], ["Deeken", "Jan", ""], ["Oschwald", "Michael", ""], ["Mota", "Fabio", ""]]}, {"id": "2009.01803", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai", "title": "Sparse Meta Networks for Sequential Adaptation and its Application to\n  Adaptive Language Modelling", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network requires a large amount of single-task data\nand involves a long time-consuming optimization phase. This is not scalable to\ncomplex, realistic environments with new unexpected changes. Humans can perform\nfast incremental learning on the fly and memory systems in the brain play a\ncritical role. We introduce Sparse Meta Networks -- a meta-learning approach to\nlearn online sequential adaptation algorithms for deep neural networks, by\nusing deep neural networks. We augment a deep neural network with a\nlayer-specific fast-weight memory. The fast-weights are generated sparsely at\neach time step and accumulated incrementally through time providing a useful\ninductive bias for online continual adaptation. We demonstrate strong\nperformance on a variety of sequential adaptation scenarios, from a simple\nonline reinforcement learning to a large scale adaptive language modelling.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:06:52 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""]]}, {"id": "2009.01827", "submitter": "Thibault Gauthier", "authors": "Thibault Gauthier", "title": "Tree Neural Networks in HOL4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of tree neural networks within the proof\nassistant HOL4. Their architecture makes them naturally suited for\napproximating functions whose domain is a set of formulas. We measure the\nperformance of our implementation and compare it with other machine learning\npredictors on the tasks of evaluating arithmetical expressions and estimating\nthe truth of propositional formulas.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:48:06 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gauthier", "Thibault", ""]]}, {"id": "2009.02174", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Laurent Rodriguez, Benoit Miramond", "title": "Improving Self-Organizing Maps with Unsupervised Feature Extraction", "comments": "Accepted for publication in the International Conference on Neural\n  Information Processing (ICONIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Self-Organizing Map (SOM) is a brain-inspired neural model that is very\npromising for unsupervised learning, especially in embedded applications.\nHowever, it is unable to learn efficient prototypes when dealing with complex\ndatasets. We propose in this work to improve the SOM performance by using\nextracted features instead of raw data. We conduct a comparative study on the\nSOM classification accuracy with unsupervised feature extraction using two\ndifferent approaches: a machine learning approach with Sparse Convolutional\nAuto-Encoders using gradient-based learning, and a neuroscience approach with\nSpiking Neural Networks using Spike Timing Dependant Plasticity learning. The\nSOM is trained on the extracted features, then very few labeled samples are\nused to label the neurons with their corresponding class. We investigate the\nimpact of the feature maps, the SOM size and the labeled subset size on the\nclassification accuracy using the different feature extraction methods. We\nimprove the SOM classification by +6.09\\% and reach state-of-the-art\nperformance on unsupervised image classification.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:19:24 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Khacef", "Lyes", ""], ["Rodriguez", "Laurent", ""], ["Miramond", "Benoit", ""]]}, {"id": "2009.02560", "submitter": "Basheer Qolomany", "authors": "Basheer Qolomany, Kashif Ahmad, Ala Al-Fuqaha, Junaid Qadir", "title": "Particle Swarm Optimized Federated Learning For Industrial IoT and Smart\n  City Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the research on Federated Learning (FL) has focused on analyzing\nglobal optimization, privacy, and communication, with limited attention\nfocusing on analyzing the critical matter of performing efficient local\ntraining and inference at the edge devices. One of the main challenges for\nsuccessful and efficient training and inference on edge devices is the careful\nselection of parameters to build local Machine Learning (ML) models. To this\naim, we propose a Particle Swarm Optimization (PSO)-based technique to optimize\nthe hyperparameter settings for the local ML models in an FL environment. We\nevaluate the performance of our proposed technique using two case studies.\nFirst, we consider smart city services and use an experimental transportation\ndataset for traffic prediction as a proxy for this setting. Second, we consider\nIndustrial IoT (IIoT) services and use the real-time telemetry dataset to\npredict the probability that a machine will fail shortly due to component\nfailures. Our experiments indicate that PSO provides an efficient approach for\ntuning the hyperparameters of deep Long short-term memory (LSTM) models when\ncompared to the grid search method. Our experiments illustrate that the number\nof clients-server communication rounds to explore the landscape of\nconfigurations to find the near-optimal parameters are greatly reduced (roughly\nby two orders of magnitude needing only 2%--4% of the rounds compared to state\nof the art non-PSO-based approaches). We also demonstrate that utilizing the\nproposed PSO-based technique to find the near-optimal configurations for FL and\ncentralized learning models does not adversely affect the accuracy of the\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 16:20:47 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Qolomany", "Basheer", ""], ["Ahmad", "Kashif", ""], ["Al-Fuqaha", "Ala", ""], ["Qadir", "Junaid", ""]]}, {"id": "2009.02732", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers, Oswin Krause", "title": "Convergence Analysis of the Hessian Estimation Evolution Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of algorithms called Hessian Estimation Evolution Strategies\n(HE-ESs) update the covariance matrix of their sampling distribution by\ndirectly estimating the curvature of the objective function. The approach is\npractically efficient, as attested by respectable performance on the BBOB\ntestbed, even on rather irregular functions.\n  In this paper we formally prove two strong guarantees for the (1+4)-HE-ES, a\nminimal elitist member of the family: stability of the covariance matrix\nupdate, and as a consequence, linear convergence on all convex quadratic\nproblems at a rate that is independent of the problem instance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 13:34:25 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 15:08:27 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Glasmachers", "Tobias", ""], ["Krause", "Oswin", ""]]}, {"id": "2009.03001", "submitter": "Alberto Gutierrez-Torre", "authors": "Alberto Gutierrez-Torre, Josep Ll. Berral, David Buchaca, Marc\n  Guevara, Albert Soret, David Carrera", "title": "Improving Maritime Traffic Emission Estimations on Missing Data with\n  CRBMs", "comments": "12 pages, 7 figures. Postprint accepted manuscript, find the full\n  version at Engineering Applications of Artificial Intelligence\n  (https://doi.org/10.1016/j.engappai.2020.103793)", "journal-ref": "Engineering Applications of Artificial Intelligence Volume 94,\n  September 2020, 103793", "doi": "10.1016/j.engappai.2020.103793", "report-no": null, "categories": "cs.CY cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maritime traffic emissions are a major concern to governments as they heavily\nimpact the Air Quality in coastal cities. Ships use the Automatic\nIdentification System (AIS) to continuously report position and speed among\nother features, and therefore this data is suitable to be used to estimate\nemissions, if it is combined with engine data. However, important ship features\nare often inaccurate or missing. State-of-the-art complex systems, like CALIOPE\nat the Barcelona Supercomputing Center, are used to model Air Quality. These\nsystems can benefit from AIS based emission models as they are very precise in\npositioning the pollution. Unfortunately, these models are sensitive to missing\nor corrupted data, and therefore they need data curation techniques to\nsignificantly improve the estimation accuracy. In this work, we propose a\nmethodology for treating ship data using Conditional Restricted Boltzmann\nMachines (CRBMs) plus machine learning methods to improve the quality of data\npassed to emission models. Results show that we can improve the default methods\nproposed to cover missing data. In our results, we observed that using our\nmethod the models boosted their accuracy to detect otherwise undetectable\nemissions. In particular, we used a real data-set of AIS data, provided by the\nSpanish Port Authority, to estimate that thanks to our method, the model was\nable to detect 45% of additional emissions, of additional emissions,\nrepresenting 152 tonnes of pollutants per week in Barcelona and propose new\nfeatures that may enhance emission modeling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:32:43 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 09:04:42 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Gutierrez-Torre", "Alberto", ""], ["Berral", "Josep Ll.", ""], ["Buchaca", "David", ""], ["Guevara", "Marc", ""], ["Soret", "Albert", ""], ["Carrera", "David", ""]]}, {"id": "2009.03473", "submitter": "Abhronil Sengupta", "authors": "Mehul Rastogi, Sen Lu, Nafiul Islam, Abhronil Sengupta", "title": "On the Self-Repair Role of Astrocytes in STDP Enabled Unsupervised SNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing is emerging to be a disruptive computational paradigm\nthat attempts to emulate various facets of the underlying structure and\nfunctionalities of the brain in the algorithm and hardware design of\nnext-generation machine learning platforms. This work goes beyond the focus of\ncurrent neuromorphic computing architectures on computational models for neuron\nand synapse to examine other computational units of the biological brain that\nmight contribute to cognition and especially self-repair. We draw inspiration\nand insights from computational neuroscience regarding functionalities of glial\ncells and explore their role in the fault-tolerant capacity of Spiking Neural\nNetworks (SNNs) trained in an unsupervised fashion using Spike-Timing Dependent\nPlasticity (STDP). We characterize the degree of self-repair that can be\nenabled in such networks with varying degree of faults ranging from 50% - 90%\nand evaluate our proposal on the MNIST and Fashion-MNIST datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 01:14:53 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 15:52:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Rastogi", "Mehul", ""], ["Lu", "Sen", ""], ["Islam", "Nafiul", ""], ["Sengupta", "Abhronil", ""]]}, {"id": "2009.03603", "submitter": "Peng Yang", "authors": "Hu Zhang, Peng Yang, Yanglong Yu, Mingjia Li, Ke Tang", "title": "Evolutionary Reinforcement Learning via Cooperative Coevolutionary\n  Negatively Correlated Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) have been successfully applied to optimize the\npolicies for Reinforcement Learning (RL) tasks due to their exploration\nability. The recently proposed Negatively Correlated Search (NCS) provides a\ndistinct parallel exploration search behavior and is expected to facilitate RL\nmore effectively. Considering that the commonly adopted neural policies usually\ninvolves millions of parameters to be optimized, the direct application of NCS\nto RL may face a great challenge of the large-scale search space. To address\nthis issue, this paper presents an NCS-friendly Cooperative Coevolution (CC)\nframework to scale-up NCS while largely preserving its parallel exploration\nsearch behavior. The issue of traditional CC that can deteriorate NCS is also\ndiscussed. Empirical studies on 10 popular Atari games show that the proposed\nmethod can significantly outperform three state-of-the-art deep RL methods with\n50% less computational time by effectively exploring a 1.7 million-dimensional\nsearch space.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 09:28:30 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Zhang", "Hu", ""], ["Yang", "Peng", ""], ["Yu", "Yanglong", ""], ["Li", "Mingjia", ""], ["Tang", "Ke", ""]]}, {"id": "2009.03622", "submitter": "Pablo Lanillos", "authors": "Otto van der Himst, Pablo Lanillos", "title": "Deep Active Inference for Partially Observable MDPs", "comments": "1st International Workshop on Active inference, European Conference\n  on Machine Learning (ECML/PCKDD 2020)", "journal-ref": null, "doi": "10.1007/978-3-030-64919-7_8", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep active inference has been proposed as a scalable approach to perception\nand action that deals with large policy and state spaces. However, current\nmodels are limited to fully observable domains. In this paper, we describe a\ndeep active inference model that can learn successful policies directly from\nhigh-dimensional sensory inputs. The deep learning architecture optimizes a\nvariant of the expected free energy and encodes the continuous state\nrepresentation by means of a variational autoencoder. We show, in the OpenAI\nbenchmark, that our approach has comparable or better performance than deep\nQ-learning, a state-of-the-art deep reinforcement learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 10:02:40 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["van der Himst", "Otto", ""], ["Lanillos", "Pablo", ""]]}, {"id": "2009.03665", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Vincent Gripon, Benoit Miramond", "title": "GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised\n  Learning", "comments": "Accepted for publication in the International Conference on Neural\n  Information Processing (ICONIP) 2020. arXiv admin note: text overlap with\n  arXiv:2009.02174", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification is a challenge in machine learning where the goal is\nto train a classifier using a very limited number of labeled examples. This\nscenario is likely to occur frequently in real life, for example when data\nacquisition or labeling is expensive. In this work, we consider the problem of\npost-labeled few-shot unsupervised learning, a classification task where\nrepresentations are learned in an unsupervised fashion, to be later labeled\nusing very few annotated examples. We argue that this problem is very likely to\noccur on the edge, when the embedded device directly acquires the data, and the\nexpert needed to perform labeling cannot be prompted often. To address this\nproblem, we consider an algorithm consisting of the concatenation of transfer\nlearning with clustering using Self-Organizing Maps (SOMs). We introduce a\nTensorFlow-based implementation to speed-up the process in multi-core CPUs and\nGPUs. Finally, we demonstrate the effectiveness of the method using standard\noff-the-shelf few-shot classification benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:22:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Khacef", "Lyes", ""], ["Gripon", "Vincent", ""], ["Miramond", "Benoit", ""]]}, {"id": "2009.03857", "submitter": "William Podlaski", "authors": "Michele Nardin, James W Phillips, William F Podlaski, Sander W Keemink", "title": "Nonlinear computations in spiking neural networks through multiplicative\n  synapses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain efficiently performs nonlinear computations through its intricate\nnetworks of spiking neurons, but how this is done remains elusive. While\nrecurrent spiking networks implementing linear computations can be directly\nderived and easily understood (e.g., in the spike coding network framework),\nthe connectivity required for nonlinear computations can be harder to\ninterpret, as they require additional non-linearities (e.g., dendritic or\nsynaptic) weighted through supervised training. Here we extend the spike coding\nframework to implement any polynomial dynamical system. This results in\nnetworks requiring multiplicative synapses, which we term the multiplicative\nspike coding network (mSCN). We demonstrate how the required connectivity for\nseveral nonlinear dynamical systems can be directly implemented in mSCNs,\nwithout training. We also show how to carry out higher-order polynomials with\ncoupled networks that use only pair-wise multiplicative synapses, and provide\nexpected numbers of connections for each synapse type. Overall, our work\nprovides an alternative method for implementing nonlinear computations in\nspiking neural networks, while keeping all the attractive features of standard\nSCNs (such as robustness, irregular and sparse firing, and interpretable\nconnectivity). Finally, we discuss the biological plausibility of our approach,\nand how the high accuracy and robustness of the approach may be of interest for\nneuromorphic computing.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:47:27 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:36:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Nardin", "Michele", ""], ["Phillips", "James W", ""], ["Podlaski", "William F", ""], ["Keemink", "Sander W", ""]]}, {"id": "2009.03863", "submitter": "Koushik Biswas", "authors": "Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey", "title": "TanhSoft -- a family of activation functions combining Tanh and Softplus", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning at its core, contains functions that are composition of a\nlinear transformation with a non-linear function known as activation function.\nIn past few years, there is an increasing interest in construction of novel\nactivation functions resulting in better learning. In this work, we propose a\nfamily of novel activation functions, namely TanhSoft, with four undetermined\nhyper-parameters of the form\ntanh({\\alpha}x+{\\beta}e^{{\\gamma}x})ln({\\delta}+e^x) and tune these\nhyper-parameters to obtain activation functions which are shown to outperform\nseveral well known activation functions. For instance, replacing ReLU with\nxtanh(0.6e^x)improves top-1 classification accuracy on CIFAR-10 by 0.46% for\nDenseNet-169 and 0.7% for Inception-v3 while with tanh(0.87x)ln(1 +e^x) top-1\nclassification accuracy on CIFAR-100 improves by 1.24% for DenseNet-169 and\n2.57% for SimpleNet model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:59:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Biswas", "Koushik", ""], ["Kumar", "Sandeep", ""], ["Banerjee", "Shilpak", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2009.03954", "submitter": "Yiding Hao", "authors": "Yiding Hao, Simon Mendelsohn, Rachel Sterneck, Randi Martinez, Robert\n  Frank", "title": "Probabilistic Predictions of People Perusing: Evaluating Metrics of\n  Language Model Performance for Psycholinguistic Modeling", "comments": "To appear in the proceedings of the Cognitive Modeling and\n  Computational Linguistics workshop (CMCL) at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By positing a relationship between naturalistic reading times and\ninformation-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008)\nprovides a natural interface between language models and psycholinguistic\nmodels. This paper re-evaluates a claim due to Goodkind and Bicknell (2018)\nthat a language model's ability to model reading times is a linear function of\nits perplexity. By extending Goodkind and Bicknell's analysis to modern neural\narchitectures, we show that the proposed relation does not always hold for Long\nShort-Term Memory networks, Transformers, and pre-trained models. We introduce\nan alternate measure of language modeling performance called predictability\nnorm correlation based on Cloze probabilities measured from human subjects. Our\nnew metric yields a more robust relationship between language model quality and\npsycholinguistic modeling performance that allows for comparison between models\nwith different training configurations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 19:12:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hao", "Yiding", ""], ["Mendelsohn", "Simon", ""], ["Sterneck", "Rachel", ""], ["Martinez", "Randi", ""], ["Frank", "Robert", ""]]}, {"id": "2009.04083", "submitter": "Chase Gaudet", "authors": "Chase J Gaudet and Anthony S Maida", "title": "Generalizing Complex/Hyper-complex Convolutions to Vector Map\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the core reasons that complex and hypercomplex valued neural\nnetworks offer improvements over their real-valued counterparts is the weight\nsharing mechanism and treating multidimensional data as a single entity. Their\nalgebra linearly combines the dimensions, making each dimension related to the\nothers. However, both are constrained to a set number of dimensions, two for\ncomplex and four for quaternions. Here we introduce novel vector map\nconvolutions which capture both of these properties provided by\ncomplex/hypercomplex convolutions, while dropping the unnatural dimensionality\nconstraints they impose. This is achieved by introducing a system that mimics\nthe unique linear combination of input dimensions, such as the Hamilton product\nfor quaternions. We perform three experiments to show that these novel vector\nmap convolutions seem to capture all the benefits of complex and hyper-complex\nnetworks, such as their ability to capture internal latent relations, while\navoiding the dimensionality restriction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 03:00:03 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Gaudet", "Chase J", ""], ["Maida", "Anthony S", ""]]}, {"id": "2009.04806", "submitter": "Alexander Wang", "authors": "Alexander Wang, Mengye Ren, Richard S. Zemel", "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch drawings capture the salient information of visual concepts. Previous\nwork has shown that neural networks are capable of producing sketches of\nnatural objects drawn from a small number of classes. While earlier approaches\nfocus on generation quality or retrieval, we explore properties of image\nrepresentations learned by training a model to produce sketches of images. We\nshow that this generative, class-agnostic model produces informative embeddings\nof images from novel examples, classes, and even novel datasets in a few-shot\nsetting. Additionally, we find that these learned representations exhibit\ninteresting structure and compositionality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:43:28 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 17:15:39 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 18:51:51 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 19:45:09 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Wang", "Alexander", ""], ["Ren", "Mengye", ""], ["Zemel", "Richard S.", ""]]}, {"id": "2009.05041", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou,\n  Antonio Torralba", "title": "Understanding the Role of Individual Units in a Deep Neural Network", "comments": "Proceedings of the National Academy of Sciences 2020. Code at\n  https://github.com/davidbau/dissect/ and website at\n  https://dissect.csail.mit.edu/", "journal-ref": null, "doi": "10.1073/pnas.1907375117", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel at finding hierarchical representations that solve\ncomplex tasks over large data sets. How can we humans understand these learned\nrepresentations? In this work, we present network dissection, an analytic\nframework to systematically identify the semantics of individual hidden units\nwithin image classification and image generation networks. First, we analyze a\nconvolutional neural network (CNN) trained on scene classification and discover\nunits that match a diverse set of object concepts. We find evidence that the\nnetwork has learned many object classes that play crucial roles in classifying\nscene classes. Second, we use a similar analytic method to analyze a generative\nadversarial network (GAN) model trained to generate scenes. By analyzing\nchanges made when small sets of units are activated or deactivated, we find\nthat objects can be added and removed from the output scenes while adapting to\nthe context. Finally, we apply our analytic framework to understanding\nadversarial attacks and to semantic image editing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:59:10 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 18:58:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Strobelt", "Hendrik", ""], ["Lapedriza", "Agata", ""], ["Zhou", "Bolei", ""], ["Torralba", "Antonio", ""]]}, {"id": "2009.05359", "submitter": "Beren Millidge Mr", "authors": "Beren Millidge, Alexander Tschantz, Anil K Seth, Christopher L Buckley", "title": "Activation Relaxation: A Local Dynamical Approximation to\n  Backpropagation in the Brain", "comments": "initial upload; revised version (updated abstract, related work)\n  28-09-20; 05/10/20: revised for ICLR submission; 10/10/20: minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation of error algorithm (backprop) has been instrumental in\nthe recent success of deep learning. However, a key question remains as to\nwhether backprop can be formulated in a manner suitable for implementation in\nneural circuitry. The primary challenge is to ensure that any candidate\nformulation uses only local information, rather than relying on global signals\nas in standard backprop. Recently several algorithms for approximating backprop\nusing only local signals have been proposed. However, these algorithms\ntypically impose other requirements which challenge biological plausibility:\nfor example, requiring complex and precise connectivity schemes, or multiple\nsequential backwards phases with information being stored across phases. Here,\nwe propose a novel algorithm, Activation Relaxation (AR), which is motivated by\nconstructing the backpropagation gradient as the equilibrium point of a\ndynamical system. Our algorithm converges rapidly and robustly to the correct\nbackpropagation gradients, requires only a single type of computational unit,\nutilises only a single parallel backwards relaxation phase, and can operate on\narbitrary computation graphs. We illustrate these properties by training deep\nneural networks on visual classification tasks, and describe simplifications to\nthe algorithm which remove further obstacles to neurobiological implementation\n(for example, the weight-transport problem, and the use of nonlinear\nderivatives), while preserving performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:56:34 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 10:37:05 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 21:19:22 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 18:21:01 GMT"}, {"version": "v5", "created": "Sat, 10 Oct 2020 14:16:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Millidge", "Beren", ""], ["Tschantz", "Alexander", ""], ["Seth", "Anil K", ""], ["Buckley", "Christopher L", ""]]}, {"id": "2009.05593", "submitter": "Ismael Balafrej", "authors": "Ismael Balafrej and Jean Rouat", "title": "P-CRITICAL: A Reservoir Autoregulation Plasticity Rule for Neuromorphic\n  Hardware", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation algorithms on recurrent artificial neural networks require an\nunfolding of accumulated states over time. These states must be kept in memory\nfor an undefined period of time which is task-dependent. This paper uses the\nreservoir computing paradigm where an untrained recurrent neural network layer\nis used as a preprocessor stage to learn temporal and limited data. These\nso-called reservoirs require either extensive fine-tuning or neuroplasticity\nwith unsupervised learning rules. We propose a new local plasticity rule named\nP-CRITICAL designed for automatic reservoir tuning that translates well to\nIntel's Loihi research chip, a recent neuromorphic processor. We compare our\napproach on well-known datasets from the machine learning community while using\na spiking neuronal architecture. We observe an improved performance on tasks\ncoming from various modalities without the need to tune parameters. Such\nalgorithms could be a key to end-to-end energy-efficient neuromorphic-based\nmachine learning on edge devices.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:13:03 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Balafrej", "Ismael", ""], ["Rouat", "Jean", ""]]}, {"id": "2009.05780", "submitter": "Xiaochen Fan", "authors": "Qianwen Ye, Xiaochen Fan, Gengfa Fang, Hongxia Bie, Chaocan Xiang,\n  Xudong Song and Xiangjian He", "title": "EdgeLoc: An Edge-IoT Framework for Robust Indoor Localization Using\n  Capsule Networks", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the unprecedented demand for location-based services in indoor\nscenarios, wireless indoor localization has become essential for mobile users.\nWhile GPS is not available at indoor spaces, WiFi RSS fingerprinting has become\npopular with its ubiquitous accessibility. However, it is challenging to\nachieve robust and efficient indoor localization with two major challenges.\nFirst, the localization accuracy can be degraded by the random signal\nfluctuations, which would influence conventional localization algorithms that\nsimply learn handcrafted features from raw fingerprint data. Second, mobile\nusers are sensitive to the localization delay, but conventional indoor\nlocalization algorithms are computation-intensive and time-consuming. In this\npaper, we propose EdgeLoc, an edge-IoT framework for efficient and robust\nindoor localization using capsule networks. We develop a deep learning model\nwith the CapsNet to efficiently extract hierarchical information from WiFi\nfingerprint data, thereby significantly improving the localization accuracy.\nMoreover, we implement an edge-computing prototype system to achieve a nearly\nreal-time localization process, by enabling mobile users with the deep-learning\nmodel that has been well-trained by the edge server. We conduct a real-world\nfield experimental study with over 33,600 data points and an extensive\nsynthetic experiment with the open dataset, and the experimental results\nvalidate the effectiveness of EdgeLoc. The best trade-off of the EdgeLoc system\nachieves 98.5% localization accuracy within an average positioning time of only\n2.31 ms in the field experiment.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 12:38:47 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ye", "Qianwen", ""], ["Fan", "Xiaochen", ""], ["Fang", "Gengfa", ""], ["Bie", "Hongxia", ""], ["Xiang", "Chaocan", ""], ["Song", "Xudong", ""], ["He", "Xiangjian", ""]]}, {"id": "2009.05800", "submitter": "Luc Libralesso", "authors": "Luc Libralesso, Pablo Andres Focke, Aur\\'elien Secardin, Vincent Jost", "title": "Iterative beam search algorithms for the permutation flowshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an iterative beam search algorithm for the permutation flowshop\n(makespan and flowtime minimization). This algorithm combines branching\nstrategies inspired by recent branch-and-bounds and a guidance strategy\ninspired by the LR heuristic. It obtains competitive results, reports many\nnew-best-so-far solutions on the VFR benchmark (makespan minimization) and the\nTaillard benchmark (flowtime minimization) without using any NEH-based\nbranching or iterative-greedy strategy. The source code is available at:\nhttps://gitlab.com/librallu/cats-pfsp.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 14:23:41 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Libralesso", "Luc", ""], ["Focke", "Pablo Andres", ""], ["Secardin", "Aur\u00e9lien", ""], ["Jost", "Vincent", ""]]}, {"id": "2009.06024", "submitter": "Gurpreet Singh", "authors": "Gurpreet Singh, Soumyajit Gupta, Matthew Lease", "title": "Extracting Optimal Solution Manifolds using Constrained Neural\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Optimization solution algorithms are restricted to point based\nsolutions. In practice, single or multiple objectives must be satisfied,\nwherein both the objective function and constraints can be non-convex resulting\nin multiple optimal solutions. Real world scenarios include intersecting\nsurfaces as Implicit Functions, Hyperspectral Unmixing and Pareto Optimal\nfronts. Local or global convexification is a common workaround when faced with\nnon-convex forms. However, such an approach is often restricted to a strict\nclass of functions, deviation from which results in sub-optimal solution to the\noriginal problem. We present neural solutions for extracting optimal sets as\napproximate manifolds, where unmodified, non-convex objectives and constraints\nare defined as modeler guided, domain-informed $L_2$ loss function. This\npromotes interpretability since modelers can confirm the results against known\nanalytical forms in their specific domains. We present synthetic and realistic\ncases to validate our approach and compare against known solvers for\nbench-marking in terms of accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 15:37:44 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 16:37:04 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 04:15:20 GMT"}, {"version": "v4", "created": "Sun, 3 Jan 2021 18:46:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Singh", "Gurpreet", ""], ["Gupta", "Soumyajit", ""], ["Lease", "Matthew", ""]]}, {"id": "2009.06037", "submitter": "Marco Virgolin", "authors": "Marco Virgolin", "title": "Genetic Programming is Naturally Suited to Evolve Bagging Ensembles", "comments": "Added interquartile range in tables 1, 2, and 3; improved Fig. 3 and\n  its analysis, improved experiment design of section 7.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning ensembles by bagging can substantially improve the generalization\nperformance of low-bias, high-variance estimators, including those evolved by\nGenetic Programming (GP). To be efficient, modern GP algorithms for evolving\n(bagging) ensembles typically rely on several (often inter-connected)\nmechanisms and respective hyper-parameters, ultimately compromising ease of\nuse. In this paper, we provide experimental evidence that such complexity might\nnot be warranted. We show that minor changes to fitness evaluation and\nselection are sufficient to make a simple and otherwise-traditional GP\nalgorithm evolve ensembles efficiently. The key to our proposal is to exploit\nthe way bagging works to compute, for each individual in the population,\nmultiple fitness values (instead of one) at a cost that is only marginally\nhigher than the one of a normal fitness evaluation. Experimental comparisons on\nclassification and regression tasks taken and reproduced from prior studies\nshow that our algorithm fares very well against state-of-the-art ensemble and\nnon-ensemble GP algorithms. We further provide insights into the proposed\napproach by (i) scaling the ensemble size, (ii) ablating the changes to\nselection, (iii) observing the evolvability induced by traditional subtree\nvariation. Code: https://github.com/marcovirgolin/2SEGP.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:28:11 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 06:22:15 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 07:07:50 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 14:30:30 GMT"}, {"version": "v5", "created": "Fri, 5 Feb 2021 13:15:42 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Virgolin", "Marco", ""]]}, {"id": "2009.06156", "submitter": "Oren Segal", "authors": "Philip Colangelo, Oren Segal, Alex Speicher, Martin Margala", "title": "AutoML for Multilayer Perceptron and FPGA Co-design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art Neural Network Architectures (NNAs) are challenging to\ndesign and implement efficiently in hardware. In the past couple of years, this\nhas led to an explosion in research and development of automatic Neural\nArchitecture Search (NAS) tools. AutomML tools are now used to achieve state of\nthe art NNA designs and attempt to optimize for hardware usage and design. Much\nof the recent research in the auto-design of NNAs has focused on convolution\nnetworks and image recognition, ignoring the fact that a significant part of\nthe workload in data centers is general-purpose deep neural networks. In this\nwork, we develop and test a general multilayer perceptron (MLP) flow that can\ntake arbitrary datasets as input and automatically produce optimized NNAs and\nhardware designs. We test the flow on six benchmarks. Our results show we\nexceed the performance of currently published MLP accuracy results and are\ncompetitive with non-MLP based results. We compare general and common GPU\narchitectures with our scalable FPGA design and show we can achieve higher\nefficiency and higher throughput (outputs per second) for the majority of\ndatasets. Further insights into the design space for both accurate networks and\nhigh performing hardware shows the power of co-design by correlating accuracy\nversus throughput, network size versus accuracy, and scaling to\nhigh-performance devices.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 02:37:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Colangelo", "Philip", ""], ["Segal", "Oren", ""], ["Speicher", "Alex", ""], ["Margala", "Martin", ""]]}, {"id": "2009.06202", "submitter": "Johannes Lederer", "authors": "Johannes Lederer", "title": "Risk Bounds for Robust Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed that certain loss functions can render deep-learning\npipelines robust against flaws in the data. In this paper, we support these\nempirical findings with statistical theory. We especially show that\nempirical-risk minimization with unbounded, Lipschitz-continuous loss\nfunctions, such as the least-absolute deviation loss, Huber loss, Cauchy loss,\nand Tukey's biweight loss, can provide efficient prediction under minimal\nassumptions on the data. More generally speaking, our paper provides\ntheoretical evidence for the benefits of robust loss functions in deep\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 05:06:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lederer", "Johannes", ""]]}, {"id": "2009.06342", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en and Alexander Schulz and Terrence C. Stewart and\n  Barbara Hammer", "title": "Reservoir Memory Machines as Neural Computers", "comments": "In print at the special issue 'New Frontiers in Extremely Efficient\n  Reservoir Computing' of IEEE TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2021.3094139", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable neural computers extend artificial neural networks with an\nexplicit memory without interference, thus enabling the model to perform\nclassic computation tasks such as graph traversal. However, such models are\ndifficult to train, requiring long training times and large datasets. In this\nwork, we achieve some of the computational capabilities of differentiable\nneural computers with a model that can be trained very efficiently, namely an\necho state network with an explicit memory without interference. This extension\nenables echo state networks to recognize all regular languages, including those\nthat contractive echo state networks provably can not recognize. Further, we\ndemonstrate experimentally that our model performs comparably to its\nfully-trained deep version on several typical benchmark tasks for\ndifferentiable neural computers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:01:30 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 08:57:42 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""], ["Schulz", "Alexander", ""], ["Stewart", "Terrence C.", ""], ["Hammer", "Barbara", ""]]}, {"id": "2009.06385", "submitter": "Faik Boray Tek", "authors": "F. Boray Tek, \\.Ilker \\c{C}am, Deniz Karl{\\i}", "title": "Adaptive Convolution Kernel for Artificial Neural Networks", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep neural networks are built by using stacked convolutional layers of\nfixed and single size (often 3$\\times$3) kernels. This paper describes a method\nfor training the size of convolutional kernels to provide varying size kernels\nin a single layer. The method utilizes a differentiable, and therefore\nbackpropagation-trainable Gaussian envelope which can grow or shrink in a base\ngrid. Our experiments compared the proposed adaptive layers to ordinary\nconvolution layers in a simple two-layer network, a deeper residual network,\nand a U-Net architecture. The results in the popular image classification\ndatasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and ``Faces in the\nWild'' showed that the adaptive kernels can provide statistically significant\nimprovements on ordinary convolution kernels. A segmentation experiment in the\nOxford-Pets dataset demonstrated that replacing a single ordinary convolution\nlayer in a U-shaped network with a single 7$\\times$7 adaptive layer can improve\nits learning performance and ability to generalize.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:36:50 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tek", "F. Boray", ""], ["\u00c7am", "\u0130lker", ""], ["Karl\u0131", "Deniz", ""]]}, {"id": "2009.06390", "submitter": "Yuxi Huan", "authors": "Yuxi Huan, Fan Wu, Michail Basios, Leslie Kanthan, Lingbo Li, Baowen\n  Xu", "title": "IEO: Intelligent Evolutionary Optimisation for Hyperparameter Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter optimisation is a crucial process in searching the optimal\nmachine learning model. The efficiency of finding the optimal hyperparameter\nsettings has been a big concern in recent researches since the optimisation\nprocess could be time-consuming, especially when the objective functions are\nhighly expensive to evaluate. In this paper, we introduce an intelligent\nevolutionary optimisation algorithm which applies machine learning technique to\nthe traditional evolutionary algorithm to accelerate the overall optimisation\nprocess of tuning machine learning models in classification problems. We\ndemonstrate our Intelligent Evolutionary Optimisation (IEO)in a series of\ncontrolled experiments, comparing with traditional evolutionary optimisation in\nhyperparameter tuning. The empirical study shows that our approach accelerates\nthe optimisation speed by 30.40% on average and up to 77.06% in the best\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 18:47:04 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Huan", "Yuxi", ""], ["Wu", "Fan", ""], ["Basios", "Michail", ""], ["Kanthan", "Leslie", ""], ["Li", "Lingbo", ""], ["Xu", "Baowen", ""]]}, {"id": "2009.06520", "submitter": "Kevin Moran P", "authors": "Cody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran and Denys\n  Poshyvanyk", "title": "A Systematic Literature Review on the Use of Deep Learning in Software\n  Engineering Research", "comments": "48 pages, Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasingly popular set of techniques adopted by software engineering\n(SE) researchers to automate development tasks are those rooted in the concept\nof Deep Learning (DL). The popularity of such techniques largely stems from\ntheir automated feature engineering capabilities, which aid in modeling\nsoftware artifacts. However, due to the rapid pace at which DL techniques have\nbeen adopted, it is difficult to distill the current successes, failures, and\nopportunities of the current research landscape. In an effort to bring clarity\nto this cross-cutting area of work, from its modern inception to the present,\nthis paper presents a systematic literature review of research at the\nintersection of SE & DL. The review canvases work appearing in the most\nprominent SE and DL conferences and journals and spans 84 papers across 22\nunique SE tasks. We center our analysis around the components of learning, a\nset of principles that govern the application of machine learning techniques\n(ML) to a given problem domain, discussing several aspects of the surveyed work\nat a granular level. The end result of our analysis is a research roadmap that\nboth delineates the foundations of DL techniques applied to SE research, and\nlikely areas of fertile exploration for the future.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:28:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Watson", "Cody", ""], ["Cooper", "Nathan", ""], ["Palacio", "David Nader", ""], ["Moran", "Kevin", ""], ["Poshyvanyk", "Denys", ""]]}, {"id": "2009.06734", "submitter": "Edward Frady", "authors": "E. Paxon Frady, Denis Kleyko, Friedrich T. Sommer", "title": "Variable Binding for Sparse Distributed Representations: Theory and\n  Applications", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic reasoning and neural networks are often considered incompatible\napproaches. Connectionist models known as Vector Symbolic Architectures (VSAs)\ncan potentially bridge this gap. However, classical VSAs and neural networks\nare still considered incompatible. VSAs encode symbols by dense pseudo-random\nvectors, where information is distributed throughout the entire neuron\npopulation. Neural networks encode features locally, often forming sparse\nvectors of neural activation. Following Rachkovskij (2001); Laiho et al.\n(2015), we explore symbolic reasoning with sparse distributed representations.\nThe core operations in VSAs are dyadic operations between vectors to express\nvariable binding and the representation of sets. Thus, algebraic manipulations\nenable VSAs to represent and process data structures in a vector space of fixed\ndimensionality. Using techniques from compressed sensing, we first show that\nvariable binding between dense vectors in VSAs is mathematically equivalent to\ntensor product binding between sparse vectors, an operation which increases\ndimensionality. This result implies that dimensionality-preserving binding for\ngeneral sparse vectors must include a reduction of the tensor matrix into a\nsingle sparse vector. Two options for sparsity-preserving variable binding are\ninvestigated. One binding method for general sparse vectors extends earlier\nproposals to reduce the tensor product into a vector, such as circular\nconvolution. The other method is only defined for sparse block-codes,\nblock-wise circular convolution. Our experiments reveal that variable binding\nfor block-codes has ideal properties, whereas binding for general sparse\nvectors also works, but is lossy, similar to previous proposals. We demonstrate\na VSA with sparse block-codes in example applications, cognitive reasoning and\nclassification, and discuss its relevance for neuroscience and neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 20:40:09 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Frady", "E. Paxon", ""], ["Kleyko", "Denis", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "2009.06762", "submitter": "Esteban Vilca", "authors": "Esteban Wilfredo Vilca Zu\\~niga", "title": "New complex network building methodology for High Level Classification\n  based on attribute-attribute interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-level classification algorithms focus on the interactions between\ninstances. These produce a new form to evaluate and classify data. In this\nprocess, the core is the complex network building methodology because it\ndetermines the metrics to be used for classification. The current methodologies\nuse variations of kNN to produce these graphs. However, this technique ignores\nsome hidden pattern between attributes and require normalization to be\naccurate. In this paper, we propose a new methodology for network building\nbased on attribute-attribute interactions that do not require normalization and\ncapture the hidden patterns of the attributes. The current results show us that\ncould be used to improve some current high-level techniques.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 21:58:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zu\u00f1iga", "Esteban Wilfredo Vilca", ""]]}, {"id": "2009.06808", "submitter": "Timoleon Moraitis", "authors": "Timoleon Moraitis, Abu Sebastian, Evangelos Eleftheriou (IBM Research\n  - Zurich)", "title": "Optimality of short-term synaptic plasticity in modelling certain\n  dynamic environments", "comments": "Main paper: 12 pages, 4 figures. Supplementary Information: 13 pages,\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neurons and their in-silico emulations for neuromorphic artificial\nintelligence (AI) use extraordinarily energy-efficient mechanisms, such as\nspike-based communication and local synaptic plasticity. It remains unclear\nwhether these neuronal mechanisms only offer efficiency or also underlie the\nsuperiority of biological intelligence. Here, we prove rigorously that, indeed,\nthe Bayes-optimal prediction and inference of randomly but continuously\ntransforming environments, a common natural setting, relies on short-term\nspike-timing-dependent plasticity, a hallmark of biological synapses. Further,\nthis dynamic Bayesian inference through plasticity enables circuits of the\ncerebral cortex in simulations to recognize previously unseen, highly distorted\ndynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,\nthe first to overcome multiple limitations of deep learning and outperform\nartificial neural networks in a visual task. The cortical-like network is\nspiking and event-based, trained only with unsupervised and local plasticity,\non a small, narrow, and static training dataset, but achieves recognition of\nunseen, transformed, and dynamic data better than deep neural networks with\ncontinuous activations, trained with supervised backpropagation on the\ntransforming data. These results link short-term plasticity to high-level\ncortical function, suggest optimality of natural intelligence for natural\nenvironments, and repurpose neuromorphic AI from mere efficiency to\ncomputational supremacy altogether.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 01:04:28 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 22:14:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Moraitis", "Timoleon", "", "IBM Research\n  - Zurich"], ["Sebastian", "Abu", "", "IBM Research\n  - Zurich"], ["Eleftheriou", "Evangelos", "", "IBM Research\n  - Zurich"]]}, {"id": "2009.06869", "submitter": "Aydogan Ozcan", "authors": "Md Sadman Sakib Rahman, Jingxi Li, Deniz Mengu, Yair Rivenson and\n  Aydogan Ozcan", "title": "Ensemble learning of diffractive optical networks", "comments": "22 Pages, 4 Figures, 1 Table", "journal-ref": "Light: Science & Applications (2021)", "doi": "10.1038/s41377-020-00446-w", "report-no": null, "categories": "cs.NE cs.CV cs.LG eess.IV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of research advances have emerged in the fields of optics and\nphotonics that benefit from harnessing the power of machine learning.\nSpecifically, there has been a revival of interest in optical computing\nhardware, due to its potential advantages for machine learning tasks in terms\nof parallelization, power efficiency and computation speed. Diffractive Deep\nNeural Networks (D2NNs) form such an optical computing framework, which\nbenefits from deep learning-based design of successive diffractive layers to\nall-optically process information as the input light diffracts through these\npassive layers. D2NNs have demonstrated success in various tasks, including\ne.g., object classification, spectral-encoding of information, optical pulse\nshaping and imaging, among others. Here, we significantly improve the inference\nperformance of diffractive optical networks using feature engineering and\nensemble learning. After independently training a total of 1252 D2NNs that were\ndiversely engineered with a variety of passive input filters, we applied a\npruning algorithm to select an optimized ensemble of D2NNs that collectively\nimprove their image classification accuracy. Through this pruning, we\nnumerically demonstrated that ensembles of N=14 and N=30 D2NNs achieve blind\ntesting accuracies of 61.14% and 62.13%, respectively, on the classification of\nCIFAR-10 test images, providing an inference improvement of >16% compared to\nthe average performance of the individual D2NNs within each ensemble. These\nresults constitute the highest inference accuracies achieved to date by any\ndiffractive optical neural network design on the same dataset and might provide\na significant leapfrog to extend the application space of diffractive optical\nimage classification and machine vision systems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 05:02:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Rahman", "Md Sadman Sakib", ""], ["Li", "Jingxi", ""], ["Mengu", "Deniz", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2009.07101", "submitter": "Kazuhisa Fujita Dr.", "authors": "Kazuhisa Fujita", "title": "Approximate spectral clustering using both reference vectors and\n  topology of the network generated by growing neural gas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering (SC) is one of the most popular clustering methods and\noften outperforms traditional clustering methods. SC uses the eigenvectors of a\nLaplacian matrix calculated from a similarity matrix of a dataset. SC has\nserious drawbacks: the significant increases in the time complexity derived\nfrom the computation of eigenvectors and the memory space complexity to store\nthe similarity matrix. To address the issues, I develop a new approximate\nspectral clustering using the network generated by growing neural gas (GNG),\ncalled ASC with GNG in this study. ASC with GNG uses not only reference vectors\nfor vector quantization but also the topology of the network for extraction of\nthe topological relationship between data points in a dataset. ASC with GNG\ncalculates the similarity matrix from both the reference vectors and the\ntopology of the network generated by GNG. Using the network generated from a\ndataset by GNG, ASC with GNG achieves to reduce the computational and space\ncomplexities and improve clustering quality. In this study, I demonstrate that\nASC with GNG effectively reduces the computational time. Moreover, this study\nshows that ASC with GNG provides equal to or better clustering performance than\nSC.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:49:24 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 13:55:19 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 02:23:16 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Fujita", "Kazuhisa", ""]]}, {"id": "2009.07430", "submitter": "Alex G. C.  de S\\'a", "authors": "M\\'arcio P. Basgalupp, Rodrigo C. Barros, Alex G. C. de S\\'a, Gisele\n  L. Pappa, Rafael G. Mantovani, Andr\\'e C. P. L. F. de Carvalho, Alex A.\n  Freitas", "title": "An Extensive Experimental Evaluation of Automated Machine Learning\n  Methods for Recommending Classification Algorithms (Extended Version)", "comments": "Accepted at Evolutionary Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an experimental comparison among four Automated Machine\nLearning (AutoML) methods for recommending the best classification algorithm\nfor a given input dataset. Three of these methods are based on Evolutionary\nAlgorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based\non the Combined Algorithm Selection and Hyper-parameter optimisation (CASH)\napproach. The EA-based methods build classification algorithms from a single\nmachine learning paradigm: either decision-tree induction, rule induction, or\nBayesian network classification. Auto-WEKA combines algorithm selection and\nhyper-parameter optimisation to recommend classification algorithms from\nmultiple paradigms. We performed controlled experiments where these four AutoML\nmethods were given the same runtime limit for different values of this limit.\nIn general, the difference in predictive accuracy of the three best AutoML\nmethods was not statistically significant. However, the EA evolving\ndecision-tree induction algorithms has the advantage of producing algorithms\nthat generate interpretable classification models and that are more scalable to\nlarge datasets, by comparison with many algorithms from other learning\nparadigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA\nhas shown meta-overfitting, a form of overfitting at the meta-learning level,\nrather than at the base-learning level.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:36:43 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Basgalupp", "M\u00e1rcio P.", ""], ["Barros", "Rodrigo C.", ""], ["de S\u00e1", "Alex G. C.", ""], ["Pappa", "Gisele L.", ""], ["Mantovani", "Rafael G.", ""], ["de Carvalho", "Andr\u00e9 C. P. L. F.", ""], ["Freitas", "Alex A.", ""]]}, {"id": "2009.07479", "submitter": "Anisleidy Gonz\\'alez-Mitjans", "authors": "A. Gonz\\'alez-Mitjans, D. Paz-Linares, A. Areces-Gonzalez, M. Li, Y.\n  Wang, ML. Bringas-Vega, and P.A Vald\\'es-Sosa", "title": "Neuroinformatic tool to study high dimensional dynamics with distributed\n  delays in Neural Mass Models", "comments": "12 pages, 6 figures, 2 tables and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuroscience has shown great progress in recent years. Several of the\ntheoretical bases have arisen from the examination of dynamic systems, using\nNeural Mass Models (NMMs). Due to the largescale brain dynamics of NMMs and the\ndifficulty of studying nonlinear systems, the local linearization approach to\ndiscretize the state equation was used via an algebraic formulation, as it\nintervenes favorably in the speed and efficiency of numerical integration. To\nstudy the spacetime organization of the brain and generate more complex\ndynamics, three structural levels (cortical unit, population and system) were\ndefined and assumed, in which the new assumed representation for conduction\ndelays and new ways of connecting were defined. This is a new time-delay NMM,\nwhich can simulate several types of EEG activities since kinetics information\nwas considered at three levels of complexity. Results obtained in this analysis\nprovide additional theoretical foundations and indicate specific\ncharacteristics for understanding neurodynamic.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 05:55:17 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 02:17:54 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 07:02:09 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 05:54:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gonz\u00e1lez-Mitjans", "A.", ""], ["Paz-Linares", "D.", ""], ["Areces-Gonzalez", "A.", ""], ["Li", "M.", ""], ["Wang", "Y.", ""], ["Bringas-Vega", "ML.", ""], ["Vald\u00e9s-Sosa", "P. A", ""]]}, {"id": "2009.07879", "submitter": "Qiong Liu", "authors": "Qiong Liu, Yanxia Zhang", "title": "Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data from IoT (Internet of Things) sensors become ubiquitous,\nstate-of-the-art machine learning algorithms face many challenges on directly\nusing sensor data. To overcome these challenges, methods must be designed to\nlearn directly from sensors without manual annotations. This paper introduces\nSensory Time-cue for Unsupervised Meta-learning (STUM). Different from\ntraditional learning approaches that either heavily depend on labels or on\ntime-independent feature extraction assumptions, such as Gaussian distribution\nfeatures, the STUM system uses time relation of inputs to guide the feature\nspace formation within and across modalities. The fact that STUM learns from a\nvariety of small tasks may put this method in the camp of Meta-Learning.\nDifferent from existing Meta-Learning approaches, STUM learning tasks are\ncomposed within and across multiple modalities based on time-cue co-exist with\nthe IoT streaming data. In an audiovisual learning example, because consecutive\nvisual frames usually comprise the same object, this approach provides a unique\nway to organize features from the same object together. The same method can\nalso organize visual object features with the object's spoken-name features\ntogether if the spoken name is presented with the object at about the same\ntime. This cross-modality feature organization may further help the\norganization of visual features that belong to similar objects but acquired at\ndifferent location and time. Promising results are achieved through\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:18:49 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Liu", "Qiong", ""], ["Zhang", "Yanxia", ""]]}, {"id": "2009.08092", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Yamini Bansal", "title": "Distributional Generalization: A New Kind of Generalization", "comments": "Co-first authors. V2: Intro shortened; no new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new notion of generalization -- Distributional Generalization\n-- which roughly states that outputs of a classifier at train and test time are\nclose *as distributions*, as opposed to close in just their average error. For\nexample, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then\na ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as\ncats on the *test set* as well, while leaving other classes unaffected. This\nbehavior is not captured by classical generalization, which would only consider\nthe average error and not the distribution of errors over the input domain. Our\nformal conjectures, which are much more general than this example, characterize\nthe form of distributional generalization that can be expected in terms of\nproblem parameters: model architecture, training procedure, number of samples,\nand data distribution. We give empirical evidence for these conjectures across\na variety of domains in machine learning, including neural networks, kernel\nmachines, and decision trees. Our results thus advance our empirical\nunderstanding of interpolating classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 06:26:17 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 02:41:52 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Bansal", "Yamini", ""]]}, {"id": "2009.08101", "submitter": "Robert Prentner", "authors": "Robert Prentner", "title": "Attracting Sets in Perceptual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document gives a specification for the model used in [1]. It presents a\nsimple way of optimizing mutual information between some input and the\nattractors of a (noisy) network, using a genetic algorithm. The nodes of this\nnetwork are modeled as simplified versions of the structures described in the\n\"interface theory of perception\" [2]. Accordingly, the system is referred to as\na \"perceptual network\".\n  The present paper is an edited version of technical parts of [1] and serves\nas accompanying text for the Python implementation PerceptualNetworks, freely\navailable under [3].\n  1. Prentner, R., and Fields, C.. Using AI methods to Evaluate a Minimal Model\nfor Perception. OpenPhilosophy 2019, 2, 503-524.\n  2. Hoffman, D. D., Prakash, C., and Singh, M.. The Interface Theory of\nPerception. Psychonomic Bulletin and Review 2015, 22, 1480-1506.\n  3. Prentner, R.. PerceptualNetworks.\nhttps://github.com/RobertPrentner/PerceptualNetworks. (accessed September 17\n2020)\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 06:46:44 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Prentner", "Robert", ""]]}, {"id": "2009.08378", "submitter": "Timo C. Wunderlich", "authors": "Timo C. Wunderlich, Christian Pehle", "title": "Event-Based Backpropagation can compute Exact Gradients for Spiking\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-021-91786-z", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks combine analog computation with event-based\ncommunication using discrete spikes. While the impressive advances of deep\nlearning are enabled by training non-spiking artificial neural networks using\nthe backpropagation algorithm, applying this algorithm to spiking networks was\npreviously hindered by the existence of discrete spike events and\ndiscontinuities. For the first time, this work derives the backpropagation\nalgorithm for a continuous-time spiking neural network and a general loss\nfunction by applying the adjoint method together with the proper partial\nderivative jumps, allowing for backpropagation through discrete spike events\nwithout approximations. This algorithm, EventProp, backpropagates errors at\nspike times in order to compute the exact gradient in an event-based,\ntemporally and spatially sparse fashion. We use gradients computed via\nEventProp to train networks on the Yin-Yang and MNIST datasets using either a\nspike time or voltage based loss function and report competitive performance.\nOur work supports the rigorous study of gradient-based learning algorithms in\nspiking neural networks and provides insights toward their implementation in\nnovel brain-inspired hardware.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:45:00 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 15:59:39 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 18:00:07 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wunderlich", "Timo C.", ""], ["Pehle", "Christian", ""]]}, {"id": "2009.08403", "submitter": "Roy Eliya", "authors": "Roy Eliya, J. Michael Herrmann", "title": "Evolutionary Selective Imitation: Interpretable Agents by Imitation\n  Learning Without a Demonstrator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for training an agent via an evolutionary strategy\n(ES), in which we iteratively improve a set of samples to imitate: Starting\nwith a random set, in every iteration we replace a subset of the samples with\nsamples from the best trajectories discovered so far. The evaluation procedure\nfor this set is to train, via supervised learning, a randomly initialised\nneural network (NN) to imitate the set and then execute the acquired policy\nagainst the environment. Our method is thus an ES based on a fitness function\nthat expresses the effectiveness of imitating an evolving data subset. This is\nin contrast to other ES techniques that iterate over the weights of the policy\ndirectly. By observing the samples that the agent selects for learning, it is\npossible to interpret and evaluate the evolving strategy of the agent more\nexplicitly than in NN learning. In our experiments, we trained an agent to\nsolve the OpenAI Gym environment Bipedalwalker-v3 by imitating an\nevolutionarily selected set of only 25 samples with a NN with only a few\nthousand parameters. We further test our method on the Procgen game Plunder and\nshow here as well that the proposed method is an interpretable, small, robust\nand effective alternative to other ES or policy gradient methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:25:31 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Eliya", "Roy", ""], ["Herrmann", "J. Michael", ""]]}, {"id": "2009.08497", "submitter": "Tarek Richard Besold", "authors": "Lorijn Zaadnoordijk, Tarek R. Besold, Rhodri Cusack", "title": "The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons\n  from Infant Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a surge in popularity of supervised Deep Learning, the desire to reduce\nthe dependence on curated, labelled data sets and to leverage the vast\nquantities of unlabelled data available recently triggered renewed interest in\nunsupervised learning algorithms. Despite a significantly improved performance\ndue to approaches such as the identification of disentangled latent\nrepresentations, contrastive learning, and clustering optimisations, the\nperformance of unsupervised machine learning still falls short of its\nhypothesised potential. Machine learning has previously taken inspiration from\nneuroscience and cognitive science with great success. However, this has mostly\nbeen based on adult learners with access to labels and a vast amount of prior\nknowledge. In order to push unsupervised machine learning forward, we argue\nthat developmental science of infant cognition might hold the key to unlocking\nthe next generation of unsupervised learning approaches. Conceptually, human\ninfant learning is the closest biological parallel to artificial unsupervised\nlearning, as infants too must learn useful representations from unlabelled\ndata. In contrast to machine learning, these new representations are learned\nrapidly and from relatively few examples. Moreover, infants learn robust\nrepresentations that can be used flexibly and efficiently in a number of\ndifferent tasks and contexts. We identify five crucial factors enabling\ninfants' quality and speed of learning, assess the extent to which these have\nalready been exploited in machine learning, and propose how further adoption of\nthese factors can give rise to previously unseen performance levels in\nunsupervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 18:47:06 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zaadnoordijk", "Lorijn", ""], ["Besold", "Tarek R.", ""], ["Cusack", "Rhodri", ""]]}, {"id": "2009.08576", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael\n  Carbin", "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?", "comments": "Published in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has explored the possibility of pruning neural networks at\ninitialization. We assess proposals for doing so: SNIP (Lee et al., 2019),\nGraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude\npruning. Although these methods surpass the trivial baseline of random pruning,\nthey remain below the accuracy of magnitude pruning after training, and we\nendeavor to understand why. We show that, unlike pruning after training,\nrandomly shuffling the weights these methods prune within each layer or\nsampling new initial values preserves or improves accuracy. As such, the\nper-weight pruning decisions made by these methods can be replaced by a\nper-layer choice of the fraction of weights to prune. This property suggests\nbroader challenges with the underlying pruning heuristics, the desire to prune\nat initialization, or both.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 01:13:38 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 21:38:32 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Frankle", "Jonathan", ""], ["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Carbin", "Michael", ""]]}, {"id": "2009.08698", "submitter": "Marc Ortiz", "authors": "Marc Ortiz, Florian Scheidegger, Marc Casas, Cristiano Malossi, Eduard\n  Ayguad\\'e", "title": "Generating Efficient DNN-Ensembles with Evolutionary Computation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we leverage ensemble learning as a tool for the creation of\nfaster, smaller, and more accurate deep learning models. We demonstrate that we\ncan jointly optimize for accuracy, inference time, and the number of parameters\nby combining DNN classifiers. To achieve this, we combine multiple ensemble\nstrategies: bagging, boosting, and an ordered chain of classifiers. To reduce\nthe number of DNN ensemble evaluations during the search, we propose EARN, an\nevolutionary approach that optimizes the ensemble according to three objectives\nregarding the constraints specified by the user. We run EARN on 10 image\nclassification datasets with an initial pool of 32 state-of-the-art DCNN on\nboth CPU and GPU platforms, and we generate models with speedups up to\n$7.60\\times$, reductions of parameters by $10\\times$, or increases in accuracy\nup to $6.01\\%$ regarding the best DNN in the pool. In addition, our method\ngenerates models that are $5.6\\times$ faster than the state-of-the-art methods\nfor automatic model generation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:14:56 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 12:59:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ortiz", "Marc", ""], ["Scheidegger", "Florian", ""], ["Casas", "Marc", ""], ["Malossi", "Cristiano", ""], ["Ayguad\u00e9", "Eduard", ""]]}, {"id": "2009.08841", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh, \\'Ad\\'am J. Berki", "title": "On the spatiotemporal behavior in biology-mimicking computing systems", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The payload performance of conventional computing systems, from single\nprocessors to supercomputers, reached its limits the nature enables. Both the\ngrowing demand to cope with \"big data\" (based on, or assisted by, artificial\nintelligence) and the interest in understanding the operation of our brain more\ncompletely, stimulated the efforts to build biology-mimicking computing systems\nfrom inexpensive conventional components and build different (\"neuromorphic\")\ncomputing systems. On one side, those systems require an unusually large number\nof processors, which introduces performance limitations and nonlinear scaling.\nOn the other side, the neuronal operation drastically differs from the\nconventional workloads. The conventional computing (including both its\nmathematical background and physical implementation) is based on assuming\ninstant interaction, while the biological neuronal systems have a\n\"spatiotemporal\" behavior. This difference alone makes imitating biological\nbehavior in technical implementation hard. Besides, the recent issues in\ncomputing called the attention to that the temporal behavior is a general\nfeature of computing systems, too. Some of their effects in both biological and\ntechnical systems were already noticed. Nevertheless, handling of those issues\nis incomplete/improper. Introducing temporal logic, based on the Minkowski\ntransform, gives quantitative insight into the operation of both kinds of\ncomputing systems, furthermore provides a natural explanation of decades-old\nempirical phenomena. Without considering their temporal behavior correctly,\nneither effective implementation nor a true imitation of biological neural\nsystems are possible.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:53:58 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 05:03:00 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 14:20:21 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""], ["Berki", "\u00c1d\u00e1m J.", ""]]}, {"id": "2009.08921", "submitter": "Yexin Yan", "authors": "Yexin Yan, Terrence C. Stewart, Xuan Choo, Bernhard Vogginger,\n  Johannes Partzsch, Sebastian Hoeppner, Florian Kelber, Chris Eliasmith, Steve\n  Furber, Christian Mayr", "title": "Low-Power Low-Latency Keyword Spotting and Adaptive Control with a\n  SpiNNaker 2 Prototype and Comparison with Loihi", "comments": null, "journal-ref": null, "doi": "10.1088/2634-4386/abf150", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implemented two neural network based benchmark tasks on a prototype chip\nof the second-generation SpiNNaker (SpiNNaker 2) neuromorphic system: keyword\nspotting and adaptive robotic control. Keyword spotting is commonly used in\nsmart speakers to listen for wake words, and adaptive control is used in\nrobotic applications to adapt to unknown dynamics in an online fashion. We\nhighlight the benefit of a multiply accumulate (MAC) array in the SpiNNaker 2\nprototype which is ordinarily used in rate-based machine learning networks when\nemployed in a neuromorphic, spiking context. In addition, the same benchmark\ntasks have been implemented on the Loihi neuromorphic chip, giving a\nside-by-side comparison regarding power consumption and computation time. While\nLoihi shows better efficiency when less complicated vector-matrix\nmultiplication is involved, with the MAC array, the SpiNNaker 2 prototype shows\nbetter efficiency when high dimensional vector-matrix multiplication is\ninvolved.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:38:55 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yan", "Yexin", ""], ["Stewart", "Terrence C.", ""], ["Choo", "Xuan", ""], ["Vogginger", "Bernhard", ""], ["Partzsch", "Johannes", ""], ["Hoeppner", "Sebastian", ""], ["Kelber", "Florian", ""], ["Eliasmith", "Chris", ""], ["Furber", "Steve", ""], ["Mayr", "Christian", ""]]}, {"id": "2009.08928", "submitter": "Keshav Ganapathy", "authors": "Keshav Ganapathy", "title": "A Study of Genetic Algorithms for Hyperparameter Optimization of Neural\n  Networks in Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With neural networks having demonstrated their versatility and benefits, the\nneed for their optimal performance is as prevalent as ever. A defining\ncharacteristic, hyperparameters, can greatly affect its performance. Thus\nengineers go through a process, tuning, to identify and implement optimal\nhyperparameters. That being said, excess amounts of manual effort are required\nfor tuning network architectures, training configurations, and preprocessing\nsettings such as Byte Pair Encoding (BPE). In this study, we propose an\nautomatic tuning method modeled after Darwin's Survival of the Fittest Theory\nvia a Genetic Algorithm (GA). Research results show that the proposed method, a\nGA, outperforms a random selection of hyperparameters.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 02:24:16 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Ganapathy", "Keshav", ""]]}, {"id": "2009.08929", "submitter": "Michal Przewozniczek", "authors": "Michal Witold Przewozniczek, Piotr Dziurzanski, Shuai Zhao, Leandro\n  Soares Indrusiak", "title": "Multi-Objective Parameter-less Population Pyramid for Solving Industrial\n  Process Planning Problems", "comments": "67 pages, 15 figures, submitted to Swarm and Evolutionary Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary methods are effective tools for obtaining high-quality results\nwhen solving hard practical problems. Linkage learning may increase their\neffectiveness. One of the state-of-the-art methods that employ linkage learning\nis the Parameter-less Population Pyramid (P3). P3 is dedicated to solving\nsingle-objective problems in discrete domains. Recent research shows that P3 is\nhighly competitive when addressing problems with so-called overlapping blocks,\nwhich are typical for practical problems. In this paper, we consider a\nmulti-objective industrial process planning problem that arises from practice\nand is NP-hard. To handle it, we propose a multi-objective version of P3. The\nextensive research shows that our proposition outperforms the competing methods\nfor the considered practical problem and typical multi-objective benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:40:57 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Przewozniczek", "Michal Witold", ""], ["Dziurzanski", "Piotr", ""], ["Zhao", "Shuai", ""], ["Indrusiak", "Leandro Soares", ""]]}, {"id": "2009.08931", "submitter": "Parth Mahendra", "authors": "Parth Mahendra", "title": "Spatio-Temporal Activation Function To Map Complex Dynamical Systems", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the real world is governed by complex and chaotic dynamical systems.\nAll of these dynamical systems pose a challenge in modelling them using neural\nnetworks. Currently, reservoir computing, which is a subset of recurrent neural\nnetworks, is actively used to simulate complex dynamical systems. In this work,\na two dimensional activation function is proposed which includes an additional\ntemporal term to impart dynamic behaviour on its output. The inclusion of a\ntemporal term alters the fundamental nature of an activation function, it\nprovides capability to capture the complex dynamics of time series data without\nrelying on recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 23:08:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Mahendra", "Parth", ""]]}, {"id": "2009.08932", "submitter": "Ajay Patrikar", "authors": "Ajay M. Patrikar", "title": "Multi-Activation Hidden Units for Neural Networks with Random Weights", "comments": "4 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:2008.10425", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single layer feedforward networks with random weights are successful in a\nvariety of classification and regression problems. These networks are known for\ntheir non-iterative and fast training algorithms. A major drawback of these\nnetworks is that they require a large number of hidden units. In this paper, we\npropose the use of multi-activation hidden units. Such units increase the\nnumber of tunable parameters and enable formation of complex decision surfaces,\nwithout increasing the number of hidden units. We experimentally show that\nmulti-activation hidden units can be used either to improve the classification\naccuracy, or to reduce computations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 13:17:33 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 20:54:20 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Patrikar", "Ajay M.", ""]]}, {"id": "2009.08934", "submitter": "Serkan Kiranyaz", "authors": "Serkan Kiranyaz, Junaid Malik, Habib Ben Abdallah, Turker Ince,\n  Alexandros Iosifidis, Moncef Gabbouj", "title": "Exploiting Heterogeneity in Operational Neural Networks by Synaptic\n  Plasticity", "comments": "15 pages, 19 figures, journal manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed network model, Operational Neural Networks (ONNs), can\ngeneralize the conventional Convolutional Neural Networks (CNNs) that are\nhomogenous only with a linear neuron model. As a heterogenous network model,\nONNs are based on a generalized neuron model that can encapsulate any set of\nnon-linear operators to boost diversity and to learn highly complex and\nmulti-modal functions or spaces with minimal network complexity and training\ndata. However, the default search method to find optimal operators in ONNs, the\nso-called Greedy Iterative Search (GIS) method, usually takes several training\nsessions to find a single operator set per layer. This is not only\ncomputationally demanding, also the network heterogeneity is limited since the\nsame set of operators will then be used for all neurons in each layer. To\naddress this deficiency and exploit a superior level of heterogeneity, in this\nstudy the focus is drawn on searching the best-possible operator set(s) for the\nhidden neurons of the network based on the Synaptic Plasticity paradigm that\nposes the essential learning theory in biological neurons. During training,\neach operator set in the library can be evaluated by their synaptic plasticity\nlevel, ranked from the worst to the best, and an elite ONN can then be\nconfigured using the top ranked operator sets found at each hidden layer.\nExperimental results over highly challenging problems demonstrate that the\nelite ONNs even with few neurons and layers can achieve a superior learning\nperformance than GIS-based ONNs and as a result the performance gap over the\nCNNs further widens.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 19:03:23 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kiranyaz", "Serkan", ""], ["Malik", "Junaid", ""], ["Abdallah", "Habib Ben", ""], ["Ince", "Turker", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2009.08935", "submitter": "Yong-Liang Xiao", "authors": "Yong-Liang Xiao", "title": "Unitary Learning for Deep Diffractive Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realization of deep learning with coherent diffraction has achieved\nremarkable development nowadays, which benefits on the fact that matrix\nmultiplication can be optically executed in parallel as well as with little\npower consumption. Coherent optical field propagated in the form of\ncomplex-value entity can be manipulated into a task-oriented output with\nstatistical inference. In this paper, we present a unitary learning protocol on\ndeep diffractive neural network, meeting the physical unitary prior in coherent\ndiffraction. Unitary learning is a backpropagation serving to unitary weights\nupdate through the gradient translation between Euclidean and Riemannian space.\nThe temporal-space evolution characteristic in unitary learning is formulated\nand elucidated. Particularly a compatible condition on how to select the\nnonlinear activations in complex space is unveiled, encapsulating the\nfundamental sigmoid, tanh and quasi-ReLu in complex space. As a preliminary\napplication, deep diffractive neural network with unitary learning is\ntentatively implemented on the 2D classification and verification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 07:16:09 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Xiao", "Yong-Liang", ""]]}, {"id": "2009.08936", "submitter": "Majdi Radaideh", "authors": "Majdi I. Radaideh, Koroush Shirvan", "title": "Improving Intelligence of Evolutionary Algorithms Using Experience Share\n  and Replay", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PESA, a novel approach combining Particle Swarm Optimisation\n(PSO), Evolution Strategy (ES), and Simulated Annealing (SA) in a hybrid\nAlgorithm, inspired from reinforcement learning. PESA hybridizes the three\nalgorithms by storing their solutions in a shared replay memory. Next, PESA\napplies prioritized replay to redistribute data between the three algorithms in\nfrequent form based on their fitness and priority values, which significantly\nenhances sample diversity and algorithm exploration. Additionally, greedy\nreplay is used implicitly within SA to improve PESA exploitation close to the\nend of evolution. The validation against 12 high-dimensional continuous\nbenchmark functions shows superior performance by PESA against standalone ES,\nPSO, and SA, under similar initial starting points, hyperparameters, and number\nof generations. PESA shows much better exploration behaviour, faster\nconvergence, and ability to find the global optima compared to its standalone\ncounterparts. Given the promising performance, PESA can offer an efficient\noptimisation option, especially after it goes through additional\nmultiprocessing improvements to handle complex and expensive fitness functions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:27:30 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Radaideh", "Majdi I.", ""], ["Shirvan", "Koroush", ""]]}, {"id": "2009.09081", "submitter": "Jingyue Zhao", "authors": "Jingyue Zhao, Nicoletta Risi, Marco Monforte, Chiara Bartolozzi,\n  Giacomo Indiveri, and Elisa Donati", "title": "Closed-loop spiking control on a neuromorphic processor implemented on\n  the iCub", "comments": null, "journal-ref": null, "doi": "10.1109/JETCAS.2020.3040390", "report-no": null, "categories": "cs.ET cs.NE cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite neuromorphic engineering promises the deployment of low latency,\nadaptive and low power systems that can lead to the design of truly autonomous\nartificial agents, the development of a fully neuromorphic artificial agent is\nstill missing. While neuromorphic sensing and perception, as well as\ndecision-making systems, are now mature, the control and actuation part is\nlagging behind. In this paper, we present a closed-loop motor controller\nimplemented on mixed-signal analog-digital neuromorphic hardware using a\nspiking neural network. The network performs a proportional control action by\nencoding target, feedback, and error signals using a spiking relational\nnetwork. It continuously calculates the error through a connectivity pattern,\nwhich relates the three variables by means of feed-forward connections.\nRecurrent connections within each population are used to speed up the\nconvergence, decrease the effect of mismatch and improve selectivity. The\nneuromorphic motor controller is interfaced with the iCub robot simulator. We\ntested our spiking P controller in a single joint control task, specifically\nfor the robot head yaw. The spiking controller sends the target positions,\nreads the motor state from its encoder, and sends back the motor commands to\nthe joint. The performance of the spiking controller is tested in a step\nresponse experiment and in a target pursuit task. In this work, we optimize the\nnetwork structure to make it more robust to noisy inputs and device mismatch,\nwhich leads to better control performances.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:17:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhao", "Jingyue", ""], ["Risi", "Nicoletta", ""], ["Monforte", "Marco", ""], ["Bartolozzi", "Chiara", ""], ["Indiveri", "Giacomo", ""], ["Donati", "Elisa", ""]]}, {"id": "2009.09149", "submitter": "Jekan Thangavelautham", "authors": "Jekan Thangavelautham and Yinan Xu", "title": "Co-Evolution of Multi-Robot Controllers and Task Cues for Off-World Open\n  Pit Mining", "comments": "8 pages, 10 figures, Accepted to International Symposium on\n  Artificial Intelligence, Robotics and Automation in Space, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are ideal for open-pit mining on the Moon as its a dull, dirty, and\ndangerous task. The challenge is to scale up productivity with an\never-increasing number of robots. This paper presents a novel method for\ndeveloping scalable controllers for use in multi-robot excavation and\nsite-preparation scenarios. The controller starts with a blank slate and does\nnot require human-authored operations scripts nor detailed modeling of the\nkinematics and dynamics of the excavator. The 'Artificial Neural Tissue' (ANT)\narchitecture is used as a control system for autonomous robot teams to perform\nresource gathering. This control architecture combines a variable-topology\nneural-network structure with a coarse-coding strategy that permits specialized\nareas to develop in the tissue. Our work in this field shows that fleets of\nautonomous decentralized robots have an optimal operating density. Too few\nrobots result in insufficient labor, while too many robots cause antagonism,\nwhere the robots undo each other's work and are stuck in gridlock. In this\npaper, we explore the use of templates and task cues to improve group\nperformance further and minimize antagonism. Our results show light beacons and\ntask cues are effective in sparking new and innovative solutions at improving\nrobot performance when placed under stressful situations such as severe\ntime-constraint.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 03:13:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Thangavelautham", "Jekan", ""], ["Xu", "Yinan", ""]]}, {"id": "2009.09232", "submitter": "Duo Wang", "authors": "Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja Jamnik,\n  Pietro Lio", "title": "Learned Low Precision Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Graph Neural Networks (GNNs) show promising performance on a range of\ngraph tasks, yet at present are costly to run and lack many of the\noptimisations applied to DNNs. We show, for the first time, how to\nsystematically quantise GNNs with minimal or no loss in performance using\nNetwork Architecture Search (NAS). We define the possible quantisation search\nspace of GNNs. The proposed novel NAS mechanism, named Low Precision Graph NAS\n(LPGNAS), constrains both architecture and quantisation choices to be\ndifferentiable. LPGNAS learns the optimal architecture coupled with the best\nquantisation strategy for different components in the GNN automatically using\nback-propagation in a single search round. On eight different datasets, solving\nthe task of classifying unseen nodes in a graph, LPGNAS generates quantised\nmodels with significant reductions in both model and buffer sizes but with\nsimilar accuracy to manually designed networks and other NAS results. In\nparticular, on the Pubmed dataset, LPGNAS shows a better size-accuracy Pareto\nfrontier compared to seven other manual and searched baselines, offering a 2.3\ntimes reduction in model size but a 0.4% increase in accuracy when compared to\nthe best NAS competitor. Finally, from our collected quantisation statistics on\na wide range of datasets, we suggest a W4A8 (4-bit weights, 8-bit activations)\nquantisation strategy might be the bottleneck for naive GNN quantisations.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 13:51:09 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Yiren", ""], ["Wang", "Duo", ""], ["Bates", "Daniel", ""], ["Mullins", "Robert", ""], ["Jamnik", "Mateja", ""], ["Lio", "Pietro", ""]]}, {"id": "2009.09298", "submitter": "Anup Das", "authors": "Adarsha Balaji, Shihao Song, Anup Das, Jeffrey Krichmar, Nikil Dutt,\n  James Shackleford, Nagarajan Kandasamy, Francky Catthoor", "title": "Enabling Resource-Aware Mapping of Spiking Neural Networks via Spatial\n  Decomposition", "comments": "Accepted for publication of IEEE Embedded Systems Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With growing model complexity, mapping Spiking Neural Network (SNN)-based\napplications to tile-based neuromorphic hardware is becoming increasingly\nchallenging. This is because the synaptic storage resources on a tile, viz. a\ncrossbar, can accommodate only a fixed number of pre-synaptic connections per\npost-synaptic neuron. For complex SNN models that have many pre-synaptic\nconnections per neuron, some connections may need to be pruned after training\nto fit onto the tile resources, leading to a loss in model quality, e.g.,\naccuracy. In this work, we propose a novel unrolling technique that decomposes\na neuron function with many pre-synaptic connections into a sequence of\nhomogeneous neural units, where each neural unit is a function computation\nnode, with two pre-synaptic connections. This spatial decomposition technique\nsignificantly improves crossbar utilization and retains all pre-synaptic\nconnections, resulting in no loss of the model quality derived from connection\npruning. We integrate the proposed technique within an existing SNN mapping\nframework and evaluate it using machine learning applications on the DYNAP-SE\nstate-of-the-art neuromorphic hardware. Our results demonstrate an average 60%\nlower crossbar requirement, 9x higher synapse utilization, 62% lower wasted\nenergy on the hardware, and between 0.8% and 4.6% increase in model quality.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:04:46 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Balaji", "Adarsha", ""], ["Song", "Shihao", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey", ""], ["Dutt", "Nikil", ""], ["Shackleford", "James", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2009.09346", "submitter": "Michael Poli", "authors": "Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama,\n  Jinkyoo Park", "title": "TorchDyn: A Neural Differential Equations Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-depth learning has recently emerged as a novel perspective on deep\nlearning, improving performance in tasks related to dynamical systems and\ndensity estimation. Core to these approaches is the neural differential\nequation, whose forward passes are the solutions of an initial value problem\nparametrized by a neural network. Unlocking the full potential of\ncontinuous-depth models requires a different set of software tools, due to\npeculiar differences compared to standard discrete neural networks, e.g\ninference must be carried out via numerical solvers. We introduce TorchDyn, a\nPyTorch library dedicated to continuous-depth learning, designed to elevate\nneural differential equations to be as accessible as regular plug-and-play deep\nlearning primitives. This objective is achieved by identifying and subdividing\ndifferent variants into common essential components, which can be combined and\nfreely repurposed to obtain complex compositional architectures. TorchDyn\nfurther offers step-by-step tutorials and benchmarks designed to guide\nresearchers and contributors.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 03:45:49 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Poli", "Michael", ""], ["Massaroli", "Stefano", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""], ["Park", "Jinkyoo", ""]]}, {"id": "2009.09439", "submitter": "Hlynur Dav{\\i}{\\dh} Hlynsson", "authors": "Hlynur Dav\\'i{\\dh} Hlynsson, Merlin Sch\\\"uler, Robin Schiewer, Tobias\n  Glasmachers, Laurenz Wiskott", "title": "Latent Representation Prediction Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deeply-learned planning methods are often based on learning representations\nthat are optimized for unrelated tasks. For example, they might be trained on\nreconstructing the environment. These representations are then combined with\npredictor functions for simulating rollouts to navigate the environment. We\nfind this principle of learning representations unsatisfying and propose to\nlearn them such that they are directly optimized for the task at hand: to be\nmaximally predictable for the predictor function. This results in\nrepresentations that are by design optimal for the downstream task of planning,\nwhere the learned predictor function is used as a forward model.\n  To this end, we propose a new way of jointly learning this representation\nalong with the prediction function, a system we dub Latent Representation\nPrediction Network (LARP). The prediction function is used as a forward model\nfor search on a graph in a viewpoint-matching task and the representation\nlearned to maximize predictability is found to outperform a pre-trained\nrepresentation. Our approach is shown to be more sample-efficient than standard\nreinforcement learning methods and our learned representation transfers\nsuccessfully to dissimilar objects.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:26:03 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 13:42:06 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hlynsson", "Hlynur Dav\u00ed\u00f0", ""], ["Sch\u00fcler", "Merlin", ""], ["Schiewer", "Robin", ""], ["Glasmachers", "Tobias", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "2009.09521", "submitter": "Yashesh Dhebar", "authors": "Yashesh Dhebar, Kalyanmoy Deb, Subramanya Nageshrao, Ling Zhu and\n  Dimitar Filev", "title": "Towards Interpretable-AI Policies Induction using Evolutionary Nonlinear\n  Decision Trees for Discrete Action Systems", "comments": "main paper: 12 pages (pages 1-12), Supplementary Document: 5 pages\n  (from pages 13-17). Video link: https://youtu.be/DByYWTQ6X3E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box AI induction methods such as deep reinforcement learning (DRL) are\nincreasingly being used to find optimal policies for a given control task.\nAlthough policies represented using a black-box AI are capable of efficiently\nexecuting the underlying control task and achieving optimal closed-loop\nperformance, the developed control rules are often complex and neither\ninterpretable nor explainable. In this paper, we use a recently proposed\nnonlinear decision-tree (NLDT) approach to find a hierarchical set of control\nrules in an attempt to maximize the open-loop performance for approximating and\nexplaining the pre-trained black-box DRL (oracle) agent using the labelled\nstate-action dataset. Recent advances in nonlinear optimization approaches\nusing evolutionary computation facilitates finding a hierarchical set of\nnonlinear control rules as a function of state variables using a\ncomputationally fast bilevel optimization procedure at each node of the\nproposed NLDT. Additionally, we propose a re-optimization procedure for\nenhancing closed-loop performance of an already derived NLDT. We evaluate our\nproposed methodologies (open and closed-loop NLDTs) on different control\nproblems having multiple discrete actions. In all these problems our proposed\napproach is able to find relatively simple and interpretable rules involving\none to four non-linear terms per rule, while simultaneously achieving on par\nclosed-loop performance when compared to a trained black-box DRL agent. A\npost-processing approach for simplifying the NLDT is also suggested. The\nobtained results are inspiring as they suggest the replacement of complicated\nblack-box DRL policies involving thousands of parameters (making them\nnon-interpretable) with relatively simple interpretable policies. Results are\nencouraging and motivating to pursue further applications of proposed approach\nin solving more complex control tasks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 20:41:57 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:28:51 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dhebar", "Yashesh", ""], ["Deb", "Kalyanmoy", ""], ["Nageshrao", "Subramanya", ""], ["Zhu", "Ling", ""], ["Filev", "Dimitar", ""]]}, {"id": "2009.09579", "submitter": "JaeSung Yoo", "authors": "Jaesung Yoo, Jeman Park, An Wang, David Mohaisen, and Joongheon Kim", "title": "On the Performance of Generative Adversarial Network (GAN) Variants: A\n  Clinical Data Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Network (GAN) is a useful type of Neural Networks in\nvarious types of applications including generative models and feature\nextraction. Various types of GANs are being researched with different insights,\nresulting in a diverse family of GANs with a better performance in each\ngeneration. This review focuses on various GANs categorized by their common\ntraits.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:18:58 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yoo", "Jaesung", ""], ["Park", "Jeman", ""], ["Wang", "An", ""], ["Mohaisen", "David", ""], ["Kim", "Joongheon", ""]]}, {"id": "2009.09644", "submitter": "Zimeng Lyu", "authors": "Zimeng Lyu, AbdElRahman ElSaid, Joshua Karns, Mohamed Mkaouer, Travis\n  Desell", "title": "An Experimental Study of Weight Initialization and Weight Inheritance\n  Effects on Neuroevolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight initialization is critical in being able to successfully train\nartificial neural networks (ANNs), and even more so for recurrent neural\nnetworks (RNNs) which can easily suffer from vanishing and exploding gradients.\nIn neuroevolution, where evolutionary algorithms are applied to neural\narchitecture search, weights typically need to be initialized at three\ndifferent times: when initial genomes (ANN architectures) are created at the\nbeginning of the search, when offspring genomes are generated by crossover, and\nwhen new nodes or edges are created during mutation. This work explores the\ndifference between using Xavier, Kaiming, and uniform random weight\ninitialization methods, as well as novel Lamarckian weight inheritance methods\nfor initializing new weights during crossover and mutation operations. These\nare examined using the Evolutionary eXploration of Augmenting Memory Models\n(EXAMM) neuroevolution algorithm, which is capable of evolving RNNs with a\nvariety of modern memory cells (e.g., LSTM, GRU, MGU, UGRNN and Delta-RNN\ncells) as well recurrent connections with varying time skips through a high\nperformance island based distributed evolutionary algorithm. Results show that\nwith statistical significance, utilizing the Lamarckian strategies outperforms\nKaiming, Xavier and uniform random weight initialization, and can speed\nneuroevolution by requiring less backpropagation epochs to be evaluated for\neach generated RNN.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:06:47 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 21:41:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Lyu", "Zimeng", ""], ["ElSaid", "AbdElRahman", ""], ["Karns", "Joshua", ""], ["Mkaouer", "Mohamed", ""], ["Desell", "Travis", ""]]}, {"id": "2009.09719", "submitter": "arXiv Admin", "authors": "Sagar Verma", "title": "A Survey on Machine Learning Applied to Dynamic Physical Systems", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text overlap with external source", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This survey is on recent advancements in the intersection of physical\nmodeling and machine learning. We focus on the modeling of nonlinear systems\nwhich are closer to electric motors. Survey on motor control and fault\ndetection in operation of electric motors has been done.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:41:54 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 13:27:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Verma", "Sagar", ""]]}, {"id": "2009.10199", "submitter": "Min Shi Mr.", "authors": "Min Shi, David A.Wilson, Xingquan Zhu, Yu Huang, Yuan Zhuang, Jianxun\n  Liu and Yufei Tang", "title": "Evolutionary Architecture Search for Graph Neural Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated machine learning (AutoML) has seen a resurgence in interest with\nthe boom of deep learning over the past decade. In particular, Neural\nArchitecture Search (NAS) has seen significant attention throughout the AutoML\nresearch community, and has pushed forward the state-of-the-art in a number of\nneural models to address grid-like data such as texts and images. However, very\nlitter work has been done about Graph Neural Networks (GNN) learning on\nunstructured network data. Given the huge number of choices and combinations of\ncomponents such as aggregator and activation function, determining the suitable\nGNN structure for a specific problem normally necessitates tremendous expert\nknowledge and laborious trails. In addition, the slight variation of hyper\nparameters such as learning rate and dropout rate could dramatically hurt the\nlearning capacity of GNN. In this paper, we propose a novel AutoML framework\nthrough the evolution of individual models in a large GNN architecture space\ninvolving both neural structures and learning parameters. Instead of optimizing\nonly the model structures with fixed parameter settings as existing work, an\nalternating evolution process is performed between GNN structures and learning\nparameters to dynamically find the best fit of each other. To the best of our\nknowledge, this is the first work to introduce and evaluate evolutionary\narchitecture search for GNN models. Experiments and validations demonstrate\nthat evolutionary NAS is capable of matching existing state-of-the-art\nreinforcement learning approaches for both the semi-supervised transductive and\ninductive node representation learning and classification.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 22:11:53 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Shi", "Min", ""], ["Wilson", "David A.", ""], ["Zhu", "Xingquan", ""], ["Huang", "Yu", ""], ["Zhuang", "Yuan", ""], ["Liu", "Jianxun", ""], ["Tang", "Yufei", ""]]}, {"id": "2009.10214", "submitter": "Prerit Terway", "authors": "Prerit Terway, Kenza Hamidouche, and Niraj K. Jha", "title": "DISPATCH: Design Space Exploration of Cyber-Physical Systems", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of cyber-physical systems (CPSs) is a challenging task that involves\nsearching over a large search space of various CPS configurations and possible\nvalues of components composing the system. Hence, there is a need for\nsample-efficient CPS design space exploration to select the system architecture\nand component values that meet the target system requirements. We address this\nchallenge by formulating CPS design as a multi-objective optimization problem\nand propose DISPATCH, a two-step methodology for sample-efficient search over\nthe design space. First, we use a genetic algorithm to search over discrete\nchoices of system component values for architecture search and component\nselection or only component selection and terminate the algorithm even before\nmeeting the system requirements, thus yielding a coarse design. In the second\nstep, we use an inverse design to search over a continuous space to fine-tune\nthe component values and meet the diverse set of system requirements. We use a\nneural network as a surrogate function for the inverse design of the system.\nThe neural network, converted into a mixed-integer linear program, is used for\nactive learning to sample component values efficiently in a continuous search\nspace. We illustrate the efficacy of DISPATCH on electrical circuit benchmarks:\ntwo-stage and three-stage transimpedence amplifiers. Simulation results show\nthat the proposed methodology improves sample efficiency by 5-14x compared to a\nprior synthesis method that relies on reinforcement learning. It also\nsynthesizes circuits with the best performance (highest bandwidth/lowest area)\ncompared to designs synthesized using reinforcement learning, Bayesian\noptimization, or humans.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 23:14:51 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 00:06:05 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Terway", "Prerit", ""], ["Hamidouche", "Kenza", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2009.10460", "submitter": "W B Langdon", "authors": "W. B. Langdon", "title": "Multi-threaded Memory Efficient Crossover in C++ for Generational\n  Genetic Programming", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C++ code snippets from a multi-core parallel memory-efficient crossover for\ngenetic programming are given. They may be adapted for separate generation\nevolutionary algorithms where large chromosomes or small RAM require no more\nthan M + (2 times nthreads) simultaneously active individuals.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:32:20 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Langdon", "W. B.", ""]]}, {"id": "2009.10520", "submitter": "Marijn van Knippenberg", "authors": "Marijn van Knippenberg, Mike Holenderski, Vlado Menkovski", "title": "Complex Vehicle Routing with Memory Augmented Neural Networks", "comments": "Presented at 3RD IEEE International Conference on Industrial\n  Cyber-Physical Systems, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex real-life routing challenges can be modeled as variations of\nwell-known combinatorial optimization problems. These routing problems have\nlong been studied and are difficult to solve at scale. The particular setting\nmay also make exact formulation difficult. Deep Learning offers an increasingly\nattractive alternative to traditional solutions, which mainly revolve around\nthe use of various heuristics. Deep Learning may provide solutions which are\nless time-consuming and of higher quality at large scales, as it generally does\nnot need to generate solutions in an iterative manner, and Deep Learning models\nhave shown a surprising capacity for solving complex tasks in recent years.\nHere we consider a particular variation of the Capacitated Vehicle Routing\n(CVRP) problem and investigate the use of Deep Learning models with explicit\nmemory components. Such memory components may help in gaining insight into the\nmodel's decisions as the memory and operations on it can be directly inspected\nat any time, and may assist in scaling the method to such a size that it\nbecomes viable for industry settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:18:06 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["van Knippenberg", "Marijn", ""], ["Holenderski", "Mike", ""], ["Menkovski", "Vlado", ""]]}, {"id": "2009.10685", "submitter": "Greg Yang", "authors": "Greg Yang", "title": "Tensor Programs III: Neural Matrix Laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a neural network (NN), *weight matrices* linearly transform inputs into\n*preactivations* that are then transformed nonlinearly into *activations*. A\ntypical NN interleaves multitudes of such linear and nonlinear transforms to\nexpress complex functions. Thus, the (pre-)activations depend on the weights in\nan intricate manner. We show that, surprisingly, (pre-)activations of a\nrandomly initialized NN become *independent* from the weights as the NN's\nwidths tend to infinity, in the sense of asymptotic freeness in random matrix\ntheory. We call this the Free Independence Principle (FIP), which has these\nconsequences: 1) It rigorously justifies the calculation of asymptotic Jacobian\nsingular value distribution of an NN in Pennington et al. [36,37], essential\nfor training ultra-deep NNs [48]. 2) It gives a new justification of gradient\nindependence assumption used for calculating the Neural Tangent Kernel of a\nneural network. FIP and these results hold for any neural architecture. We show\nFIP by proving a Master Theorem for any Tensor Program, as introduced in Yang\n[50,51], generalizing the Master Theorems proved in those works. As warmup\ndemonstrations of this new Master Theorem, we give new proofs of the semicircle\nand Marchenko-Pastur laws, which benchmarks our framework against these\nfundamental mathematical results.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:59:24 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 03:28:14 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 12:59:35 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Greg", ""]]}, {"id": "2009.10976", "submitter": "Dingqing Yang", "authors": "Dingqing Yang, Amin Ghasemazar, Xiaowei Ren, Maximilian Golub, Guy\n  Lemieux, Mieszko Lis", "title": "Procrustes: a Dataflow and Accelerator for Sparse Deep Neural Network\n  Training", "comments": "Appears in the Proceedings of the 53$^\\mathit{rd}$ IEEE/ACM\n  International Symposium on Microarchitecture (MICRO 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of DNN pruning has led to the development of energy-efficient\ninference accelerators that support pruned models with sparse weight and\nactivation tensors. Because the memory layouts and dataflows in these\narchitectures are optimized for the access patterns during\n$\\mathit{inference}$, however, they do not efficiently support the emerging\nsparse $\\mathit{training}$ techniques.\n  In this paper, we demonstrate (a) that accelerating sparse training requires\na co-design approach where algorithms are adapted to suit the constraints of\nhardware, and (b) that hardware for sparse DNN training must tackle constraints\nthat do not arise in inference accelerators. As proof of concept, we adapt a\nsparse training algorithm to be amenable to hardware acceleration; we then\ndevelop dataflow, data layout, and load-balancing techniques to accelerate it.\n  The resulting system is a sparse DNN training accelerator that produces\npruned models with the same accuracy as dense models without first training,\nthen pruning, and finally retraining, a dense model. Compared to training the\nequivalent unpruned models using a state-of-the-art DNN accelerator without\nsparse training support, Procrustes consumes up to 3.26$\\times$ less energy and\noffers up to 4$\\times$ speedup across a range of models, while pruning weights\nby an order of magnitude and maintaining unpruned accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:39:55 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yang", "Dingqing", ""], ["Ghasemazar", "Amin", ""], ["Ren", "Xiaowei", ""], ["Golub", "Maximilian", ""], ["Lemieux", "Guy", ""], ["Lis", "Mieszko", ""]]}, {"id": "2009.11182", "submitter": "Tarik A. Rashid", "authors": "Chnoor M. Rahman and Tarik A. Rashid", "title": "A new evolutionary algorithm: Learner performance based behavior\n  algorithm", "comments": "17 pages. Egyptian Informatics Journal, 2020", "journal-ref": null, "doi": "10.1016/j.eij.2020.08.003", "report-no": null, "categories": "cs.CY cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A novel evolutionary algorithm called learner performance based behavior\nalgorithm (LPB) is proposed in this article. The basic inspiration of LPB\noriginates from the process of accepting graduated learners from high school in\ndifferent departments at university. In addition, the changes those learners\nshould do in their studying behaviors to improve their study level at\nuniversity. The most important stages of optimization; exploitation and\nexploration are outlined by designing the process of accepting graduated\nlearners from high school to university and the procedure of improving the\nlearner's studying behavior at university to improve the level of their study.\nTo show the accuracy of the proposed algorithm, it is evaluated against a\nnumber of test functions, such as traditional benchmark functions, CEC-C06 2019\ntest functions, and a real-world case study problem. The results of the\nproposed algorithm are then compared to the DA, GA, and PSO. The proposed\nalgorithm produced superior results in most of the cases and comparative in\nsome others. It is proved that the algorithm has a great ability to deal with\nthe large optimization problems comparing to the DA, GA, and PSO. The overall\nresults proved the ability of LPB in improving the initial population and\nconverging towards the global optima. Moreover, the results of the proposed\nwork are proved statistically.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 04:17:35 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Rahman", "Chnoor M.", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "2009.11243", "submitter": "Luke Metz", "authors": "Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, Jascha\n  Sohl-Dickstein", "title": "Tasks, stability, architecture, and compute: Training more effective\n  learned optimizers, and using them to train themselves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much as replacing hand-designed features with learned functions has\nrevolutionized how we solve perceptual tasks, we believe learned algorithms\nwill transform how we train models. In this work we focus on general-purpose\nlearned optimizers capable of training a wide variety of problems with no\nuser-specified hyperparameters. We introduce a new, neural network\nparameterized, hierarchical optimizer with access to additional features such\nas validation loss to enable automatic regularization. Most learned optimizers\nhave been trained on only a single task, or a small number of tasks. We train\nour optimizers on thousands of tasks, making use of orders of magnitude more\ncompute, resulting in optimizers that generalize better to unseen tasks. The\nlearned optimizers not only perform well, but learn behaviors that are distinct\nfrom existing first order optimizers. For instance, they generate update steps\nthat have implicit regularization and adapt as the problem hyperparameters\n(e.g. batch size) or architecture (e.g. neural network width) change. Finally,\nthese learned optimizers show evidence of being useful for out of distribution\ntasks such as training themselves from scratch.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:35:09 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Metz", "Luke", ""], ["Maheswaranathan", "Niru", ""], ["Freeman", "C. Daniel", ""], ["Poole", "Ben", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2009.11245", "submitter": "Karla Burelo", "authors": "Mohammadali Sharifshazileh (1 and 2), Karla Burelo (1 and 2), Johannes\n  Sarnthein (2) and Giacomo Indiveri (1) ((1) Institute of Neuroinformatics,\n  University of Zurich and ETH Zurich, (2) Klinik f\\\"ur Neurochirurgie,\n  Universit\\\"atsSpital und Universit\\\"at Z\\\"urich)", "title": "An electronic neuromorphic system for real-time detection of High\n  Frequency Oscillations (HFOs) in intracranial EEG", "comments": "16 pages. A short video describing the rationale underlying the study\n  can be viewed on https://youtu.be/NuAA91fdmaM", "journal-ref": null, "doi": "10.1038/s41467-021-23342-2", "report-no": null, "categories": "eess.SP cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a neuromorphic system that combines for the first\ntime a neural recording headstage with a signal-to-spike conversion circuit and\na multi-core spiking neural network (SNN) architecture on the same die for\nrecording, processing, and detecting High Frequency Oscillations (HFO), which\nare biomarkers for the epileptogenic zone. The device was fabricated using a\nstandard 0.18$\\mu$m CMOS technology node and has a total area of 99mm$^{2}$. We\ndemonstrate its application to HFO detection in the iEEG recorded from 9\npatients with temporal lobe epilepsy who subsequently underwent epilepsy\nsurgery. The total average power consumption of the chip during the detection\ntask was 614.3$\\mu$W. We show how the neuromorphic system can reliably detect\nHFOs: the system predicts postsurgical seizure outcome with state-of-the-art\naccuracy, specificity and sensitivity (78%, 100%, and 33% respectively). This\nis the first feasibility study towards identifying relevant features in\nintracranial human data in real-time, on-chip, using event-based processors and\nspiking neural networks. By providing \"neuromorphic intelligence\" to neural\nrecording circuits the approach proposed will pave the way for the development\nof systems that can detect HFO areas directly in the operation room and improve\nthe seizure outcome of epilepsy surgery.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:40:44 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:22:30 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sharifshazileh", "Mohammadali", "", "1 and 2"], ["Burelo", "Karla", "", "1 and 2"], ["Sarnthein", "Johannes", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2009.11349", "submitter": "Gil Fidel", "authors": "Gil Fidel, Ron Bitton, Ziv Katzir, Asaf Shabtai", "title": "Adversarial robustness via stochastic regularization of neural\n  activation sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that the input domain of any machine learning\nclassifier is bound to contain adversarial examples. Thus we can no longer hope\nto immune classifiers against adversarial examples and instead can only aim to\nachieve the following two defense goals: 1) making adversarial examples harder\nto find, or 2) weakening their adversarial nature by pushing them further away\nfrom correctly classified data points. Most if not all the previously suggested\ndefense mechanisms attend to just one of those two goals, and as such, could be\nbypassed by adaptive attacks that take the defense mechanism into\nconsideration. In this work we suggest a novel defense mechanism that\nsimultaneously addresses both defense goals: We flatten the gradients of the\nloss surface, making adversarial examples harder to find, using a novel\nstochastic regularization term that explicitly decreases the sensitivity of\nindividual neurons to small input perturbations. In addition, we push the\ndecision boundary away from correctly classified inputs by leveraging Jacobian\nregularization. We present a solid theoretical basis and an empirical testing\nof our suggested approach, demonstrate its superiority over previously\nsuggested defense mechanisms, and show that it is effective against a wide\nrange of adaptive attacks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 19:31:55 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Fidel", "Gil", ""], ["Bitton", "Ron", ""], ["Katzir", "Ziv", ""], ["Shabtai", "Asaf", ""]]}, {"id": "2009.11368", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Evolution of Autopoiesis and Multicellularity in the Game of Life", "comments": null, "journal-ref": "Artificial Life, 27(1), 26-43 (2021)", "doi": "10.1162/artl_a_00334", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we introduced a model of symbiosis, Model-S, based on the evolution\nof seed patterns in Conway's Game of Life. In the model, the fitness of a seed\npattern is measured by one-on-one competitions in the Immigration Game, a\ntwo-player variation of the Game of Life. Our previous article showed that\nModel-S can serve as a highly abstract, simplified model of biological life:\n(1) The initial seed pattern is analogous to a genome. (2) The changes as the\ngame runs are analogous to the development of the phenome. (3) Tournament\nselection in Model-S is analogous to natural selection in biology. (4) The\nImmigration Game in Model-S is analogous to competition in biology. (5) The\nfirst three layers in Model-S are analogous to biological reproduction. (6) The\nfusion of seed patterns in Model-S is analogous to symbiosis. The current\narticle takes this analogy two steps further: (7) Autopoietic structures in the\nGame of Life (still lifes, oscillators, and spaceships -- collectively known as\nashes) are analogous to cells in biology. (8) The seed patterns in the Game of\nLife give rise to multiple, diverse, cooperating autopoietic structures,\nanalogous to multicellular biological life. We use the apgsearch software (Ash\nPattern Generator Search), developed by Adam Goucher for the study of ashes, to\nanalyze autopoiesis and multicellularity in Model-S. We find that the fitness\nof evolved seed patterns in Model-S is highly correlated with the diversity and\nquantity of multicellular autopoietic structures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 20:26:28 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 19:23:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "2009.11390", "submitter": "Valdimir Jos\\'e Ramon Pieter", "authors": "Valdimir Pieter", "title": "Parameters for the best convergence of an optimization algorithm\n  On-The-Fly", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What really sparked my interest was how certain parameters worked better at\nexecuting and optimization algorithm convergence even though the objective\nformula had no significant differences. Thus the research question stated:\n'Which parameters provides an upmost optimal convergence solution of an\nObjective formula using the on-the-fly method?' This research was done in an\nexperimental concept in which five different algorithms were tested with\ndifferent objective functions to discover which parameter would result well for\nthe best convergence. To find the correct parameter a method called\n'on-the-fly' was applied. I run the experiments with five different\noptimization algorithms. One of the test runs showed that each parameter has an\nincreasing or decreasing convergence accuracy towards the subjective function\ndepending on which specific optimization algorithm you choose. Each parameter\nhas an increasing or decreasing convergence accuracy toward the subjective\nfunction. One of the results in which evolutionary algorithm was applied with\nonly the recombination technique did well at finding the best optimization. As\nwell that some results have an increasing accuracy visualization by combing\nmutation or several parameters in one test performance. In conclusion, each\nalgorithm has its own set of the parameter that converge differently. Also\ndepending on the target formula that is used. This confirms that the fly method\na suitable approach at finding the best parameter. This means manipulations and\nobserve the effects in process to find the right parameter works as long as the\nlearning cost rate decreases over time.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 21:38:28 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Pieter", "Valdimir", ""]]}, {"id": "2009.11443", "submitter": "Thai Hung Le", "authors": "Hung Le and Svetha Venkatesh", "title": "Neurocoder: Learning General-Purpose Computation Using Stored Neural\n  Programs", "comments": "22 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks are uniquely adroit at machine learning by\nprocessing data through a network of artificial neurons. The inter-neuronal\nconnection weights represent the learnt Neural Program that instructs the\nnetwork on how to compute the data. However, without an external memory to\nstore Neural Programs, they are restricted to only one, overwriting learnt\nprograms when trained on new data. This is functionally equivalent to a\nspecial-purpose computer. Here we design Neurocoder, an entirely new class of\ngeneral-purpose conditional computational machines in which the neural network\n\"codes\" itself in a data-responsive way by composing relevant programs from a\nset of shareable, modular programs. This can be considered analogous to\nbuilding Lego structures from simple Lego bricks. Notably, our bricks change\ntheir shape through learning. External memory is used to create, store and\nretrieve modular programs. Like today's stored-program computers, Neurocoder\ncan now access diverse programs to process different data. Unlike manually\ncrafted computer programs, Neurocoder creates programs through training.\nIntegrating Neurocoder into current neural architectures, we demonstrate new\ncapacity to learn modular programs, handle severe pattern shifts and remember\nold programs as new ones are learnt, and show substantial performance\nimprovement in solving object recognition, playing video games and continual\nlearning tasks. Such integration with Neurocoder increases the computation\ncapability of any current neural network and endows it with entirely new\ncapacity to reuse simple programs to build complex ones. For the first time a\nNeural Program is treated as a datum in memory, paving the ways for modular,\nrecursive and procedural neural programming.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:39:16 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Le", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2009.11479", "submitter": "Yasushi Esaki", "authors": "Yasushi Esaki and Yuta Nakahara and Toshiyasu Matsushima", "title": "Theoretical Analysis of the Advantage of Deepening Neural Networks", "comments": "9 pages, 7 figures; accepted in 19th IEEE International Conference on\n  Machine Learning and Applications (IEEE ICMLA 2020)", "journal-ref": "2020 19th IEEE International Conference on Machine Learning and\n  Applications (ICMLA), pages 479-484", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new criteria to understand the advantage of deepening neural\nnetworks. It is important to know the expressivity of functions computable by\ndeep neural networks in order to understand the advantage of deepening neural\nnetworks. Unless deep neural networks have enough expressivity, they cannot\nhave good performance even though learning is successful. In this situation,\nthe proposed criteria contribute to understanding the advantage of deepening\nneural networks since they can evaluate the expressivity independently from the\nefficiency of learning. The first criterion shows the approximation accuracy of\ndeep neural networks to the target function. This criterion has the background\nthat the goal of deep learning is approximating the target function by deep\nneural networks. The second criterion shows the property of linear regions of\nfunctions computable by deep neural networks. This criterion has the background\nthat deep neural networks whose activation functions are piecewise linear are\nalso piecewise linear. Furthermore, by the two criteria, we show that to\nincrease layers is more effective than to increase units at each layer on\nimproving the expressivity of deep neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 04:10:50 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Esaki", "Yasushi", ""], ["Nakahara", "Yuta", ""], ["Matsushima", "Toshiyasu", ""]]}, {"id": "2009.11510", "submitter": "Junshan Wang", "authors": "Junshan Wang, Yilun Jin, Guojie Song, Xiaojun Ma", "title": "EPNE: Evolutionary Pattern Preserving Network Embedding", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information networks are ubiquitous and are ideal for modeling relational\ndata. Networks being sparse and irregular, network embedding algorithms have\ncaught the attention of many researchers, who came up with numerous embeddings\nalgorithms in static networks. Yet in real life, networks constantly evolve\nover time. Hence, evolutionary patterns, namely how nodes develop itself over\ntime, would serve as a powerful complement to static structures in embedding\nnetworks, on which relatively few works focus. In this paper, we propose EPNE,\na temporal network embedding model preserving evolutionary patterns of the\nlocal structure of nodes. In particular, we analyze evolutionary patterns with\nand without periodicity and design strategies correspondingly to model such\npatterns in time-frequency domains based on causal convolutions. In addition,\nwe propose a temporal objective function which is optimized simultaneously with\nproximity ones such that both temporal and structural information are\npreserved. With the adequate modeling of temporal information, our model is\nable to outperform other competitive methods in various prediction tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 06:31:14 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Wang", "Junshan", ""], ["Jin", "Yilun", ""], ["Song", "Guojie", ""], ["Ma", "Xiaojun", ""]]}, {"id": "2009.11990", "submitter": "Youngsoo Choi", "authors": "Youngkyu Kim, Youngsoo Choi, David Widemann, Tarek Zohdi", "title": "A fast and accurate physics-informed neural network reduced order model\n  with shallow masked autoencoder", "comments": "33 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional linear subspace reduced order models (LS-ROMs) are able to\naccelerate physical simulations, in which the intrinsic solution space falls\ninto a subspace with a small dimension, i.e., the solution space has a small\nKolmogorov n-width. However, for physical phenomena not of this type, e.g., any\nadvection-dominated flow phenomena, such as in traffic flow, atmospheric flows,\nand air flow over vehicles, a low-dimensional linear subspace poorly\napproximates the solution. To address cases such as these, we have developed a\nfast and accurate physics-informed neural network ROM, namely nonlinear\nmanifold ROM (NM-ROM), which can better approximate high-fidelity model\nsolutions with a smaller latent space dimension than the LS-ROMs. Our method\ntakes advantage of the existing numerical methods that are used to solve the\ncorresponding full order models. The efficiency is achieved by developing a\nhyper-reduction technique in the context of the NM-ROM. Numerical results show\nthat neural networks can learn a more efficient latent space representation on\nadvection-dominated data from 1D and 2D Burgers' equations. A speedup of up to\n2.6 for 1D Burgers' and a speedup of 11.7 for 2D Burgers' equations are\nachieved with an appropriate treatment of the nonlinear terms through a\nhyper-reduction technique. Finally, a posteriori error bounds for the NM-ROMs\nare derived that take account of the hyper-reduced operators.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 00:48:19 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 21:27:55 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Kim", "Youngkyu", ""], ["Choi", "Youngsoo", ""], ["Widemann", "David", ""], ["Zohdi", "Tarek", ""]]}, {"id": "2009.12216", "submitter": "Jon McCormack", "authors": "Jon McCormack and Andy Lomas", "title": "Deep Learning of Individual Aesthetics", "comments": "Author preprint of article for Neural Computing and Applications.\n  arXiv admin note: substantial text overlap with arXiv:2004.06874", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate evaluation of human aesthetic preferences represents a major\nchallenge for creative evolutionary and generative systems research. Prior work\nhas tended to focus on feature measures of the artefact, such as symmetry,\ncomplexity and coherence. However, research models from Psychology suggest that\nhuman aesthetic experiences encapsulate factors beyond the artefact, making\naccurate computational models very difficult to design. The interactive genetic\nalgorithm (IGA) circumvents the problem through human-in-the-loop, subjective\nevaluation of aesthetics, but is limited due to user fatigue and small\npopulation sizes. In this paper we look at how recent advances in deep learning\ncan assist in automating personal aesthetic judgement. Using a leading artist's\ncomputer art dataset, we investigate the relationship between image measures,\nsuch as complexity, and human aesthetic evaluation. We use dimension reduction\nmethods to visualise both genotype and phenotype space in order to support the\nexploration of new territory in a generative system. Convolutional Neural\nNetworks trained on the artist's prior aesthetic evaluations are used to\nsuggest new possibilities similar or between known high quality\ngenotype-phenotype mappings. We integrate this classification and discovery\nsystem into a software tool for evolving complex generative art and design.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 03:04:28 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["McCormack", "Jon", ""], ["Lomas", "Andy", ""]]}, {"id": "2009.12401", "submitter": "Fergal Stapleton", "authors": "Edgar Galv\\'an and Fergal Stapleton", "title": "Semantic-based Distance Approaches in Multi-objective Genetic\n  Programming", "comments": "8 pages, 6 tables, added additional reference, updated citation\n  format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantics in the context of Genetic Program (GP) can be understood as the\nbehaviour of a program given a set of inputs and has been well documented in\nimproving performance of GP for a range of diverse problems. There have been a\nwide variety of different methods which have incorporated semantics into\nsingle-objective GP. The study of semantics in Multi-objective (MO) GP,\nhowever, has been limited and this paper aims at tackling this issue. More\nspecifically, we conduct a comparison of three different forms of semantics in\nMOGP. One semantic-based method, (i) Semantic Similarity-based Crossover (SSC),\nis borrowed from single-objective GP, where the method has consistently being\nreported beneficial in evolutionary search. We also study two other methods,\ndubbed (ii) Semantic-based Distance as an additional criteriOn (SDO) and (iii)\nPivot Similarity SDO. We empirically and consistently show how by naturally\nhandling semantic distance as an additional criterion to be optimised in MOGP\nleads to better performance when compared to canonical methods and SSC. Both\nsemantic distance based approaches made use of a pivot, which is a reference\npoint from the sparsest region of the search space and it was found that\nindividuals which were both semantically similar and dissimilar to this pivot\nwere beneficial in promoting diversity. Moreover, we also show how the\nsemantics successfully promoted in single-objective optimisation does not\nnecessary lead to a better performance when adopted in MOGP.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:01:13 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 10:24:35 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2020 11:33:30 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 20:31:59 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Galv\u00e1n", "Edgar", ""], ["Stapleton", "Fergal", ""]]}, {"id": "2009.12531", "submitter": "Ryoji Tanabe Dr.", "authors": "Ryoji Tanabe", "title": "Analyzing Adaptive Parameter Landscapes in Parameter Adaptation Methods\n  for Differential Evolution", "comments": "This is an accepted version of a paper published in the proceedings\n  of GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3389820", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the scale factor and the crossover rate significantly influence the\nperformance of differential evolution (DE), parameter adaptation methods (PAMs)\nfor the two parameters have been well studied in the DE community. Although\nPAMs can sufficiently improve the effectiveness of DE, PAMs are poorly\nunderstood (e.g., the working principle of PAMs). One of the difficulties in\nunderstanding PAMs comes from the unclarity of the parameter space that\nconsists of the scale factor and the crossover rate. This paper addresses this\nissue by analyzing adaptive parameter landscapes in PAMs for DE. First, we\npropose a concept of an adaptive parameter landscape, which captures a moment\nin a parameter adaptation process. For each iteration, each individual in the\npopulation has its adaptive parameter landscape. Second, we propose a method of\nanalyzing adaptive parameter landscapes using a 1-step-lookahead greedy\nimprovement metric. Third, we examine adaptive parameter landscapes in PAMs by\nusing the proposed method. Results provide insightful information about PAMs in\nDE.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 08:12:59 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tanabe", "Ryoji", ""]]}, {"id": "2009.12672", "submitter": "Anup Das", "authors": "Twisha Titirsha and Anup Das", "title": "Reliability-Performance Trade-offs in Neuromorphic Computing", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic architectures built with Non-Volatile Memory (NVM) can\nsignificantly improve the energy efficiency of machine learning tasks designed\nwith Spiking Neural Networks (SNNs). A major source of voltage drop in a\ncrossbar of these architectures are the parasitic components on the crossbar's\nbitlines and wordlines, which are deliberately made longer to achieve lower\ncost-per-bit. We observe that the parasitic voltage drops create a significant\nasymmetry in programming speed and reliability of NVM cells in a crossbar.\nSpecifically, NVM cells that are on shorter current paths are faster to program\nbut have lower endurance than those on longer current paths, and vice versa.\nThis asymmetry in neuromorphic architectures create reliability-performance\ntrade-offs, which can be exploited efficiently using SNN mapping techniques. In\nthis work, we demonstrate such trade-offs using a previously-proposed SNN\nmapping technique with 10 workloads from contemporary machine learning tasks\nfor a state-of-the art neuromoorphic hardware.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 19:38:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Titirsha", "Twisha", ""], ["Das", "Anup", ""]]}, {"id": "2009.12745", "submitter": "Ho Ling Li", "authors": "Ho Ling Li", "title": "Faster Biological Gradient Descent Learning", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Back-propagation is a popular machine learning algorithm that uses gradient\ndescent in training neural networks for supervised learning, but can be very\nslow. A number of algorithms have been developed to speed up convergence and\nimprove robustness of the learning. However, they are complicated to implement\nbiologically as they require information from previous updates. Inspired by\nsynaptic competition in biology, we have come up with a simple and local\ngradient descent optimization algorithm that can reduce training time, with no\ndemand on past details. Our algorithm, named dynamic learning rate (DLR), works\nsimilarly to the traditional gradient descent used in back-propagation, except\nthat instead of having a uniform learning rate across all synapses, the\nlearning rate depends on the current neuronal connection weights. Our algorithm\nis found to speed up learning, particularly for small networks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 05:26:56 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Li", "Ho Ling", ""]]}, {"id": "2009.12788", "submitter": "Ryoji Tanabe Dr.", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "An Analysis of Quality Indicators Using Approximated Optimal\n  Distributions in a Three-dimensional Objective Space", "comments": "This is an accepted version of a paper published in the IEEE\n  Transactions on Evolutionary Computation", "journal-ref": null, "doi": "10.1109/TEVC.2020.2966014", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although quality indicators play a crucial role in benchmarking evolutionary\nmulti-objective optimization algorithms, their properties are still unclear.\nOne promising approach for understanding quality indicators is the use of the\noptimal distribution of objective vectors that optimizes each quality\nindicator. However, it is difficult to obtain the optimal distribution for each\nquality indicator, especially when its theoretical property is unknown. Thus,\noptimal distributions for most quality indicators have not been well\ninvestigated. To address these issues, first, we propose a problem formulation\nof finding the optimal distribution for each quality indicator on an arbitrary\nPareto front. Then, we approximate the optimal distributions for nine quality\nindicators using the proposed problem formulation. We analyze the nine quality\nindicators using their approximated optimal distributions on eight types of\nPareto fronts of three-objective problems. Our analysis demonstrates that\nuniformly-distributed objective vectors over the entire Pareto front are not\noptimal in many cases. Each quality indicator has its own optimal distribution\nfor each Pareto front. We also examine the consistency among the nine quality\nindicators.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 08:30:43 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2009.12867", "submitter": "Ryoji Tanabe Dr.", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "An Easy-to-use Real-world Multi-objective Optimization Problem Suite", "comments": "This is an accepted version of a paper published in Applied Soft\n  Computing", "journal-ref": null, "doi": "10.1016/j.asoc.2020.106078", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although synthetic test problems are widely used for the performance\nassessment of evolutionary multi-objective optimization algorithms, they are\nlikely to include unrealistic properties which may lead to\noverestimation/underestimation. To address this issue, we present a\nmulti-objective optimization problem suite consisting of 16 bound-constrained\nreal-world problems. The problem suite includes various problems in terms of\nthe number of objectives, the shape of the Pareto front, and the type of design\nvariables. 4 out of the 16 problems are multi-objective mixed-integer\noptimization problems. We provide Java, C, and Matlab source codes of the 16\nproblems so that they are available in an off-the-shelf manner. We examine an\napproximated Pareto front of each test problem. We also analyze the performance\nof six representative evolutionary multi-objective optimization algorithms on\nthe 16 problems. In addition to the 16 problems, we present 8 constrained\nmulti-objective real-world problems.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 15:11:08 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2009.13207", "submitter": "Jakub Fil", "authors": "Jakub Fil and Dominique Chu", "title": "A thermodynamically consistent chemical spiking neuron capable of\n  autonomous Hebbian learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully autonomous, thermodynamically consistent set of chemical\nreactions that implements a spiking neuron. This chemical neuron is able to\nlearn input patterns in a Hebbian fashion. The system is scalable to\narbitrarily many input channels. We demonstrate its performance in learning\nfrequency biases in the input as well as correlations between different input\nchannels. Efficient computation of time-correlations requires a highly\nnon-linear activation function. The resource requirements of a non-linear\nactivation function are discussed. In addition to the thermodynamically\nconsistent model of the CN, we also propose a biologically plausible version\nthat could be engineered in a synthetic biology context.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 10:43:13 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fil", "Jakub", ""], ["Chu", "Dominique", ""]]}, {"id": "2009.13266", "submitter": "Xinyue Zheng", "authors": "Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi", "title": "Disentangled Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search has shown its great potential in various areas\nrecently. However, existing methods rely heavily on a black-box controller to\nsearch architectures, which suffers from the serious problem of lacking\ninterpretability. In this paper, we propose disentangled neural architecture\nsearch (DNAS) which disentangles the hidden representation of the controller\ninto semantically meaningful concepts, making the neural architecture search\nprocess interpretable. Based on systematical study, we discover the correlation\nbetween network architecture and its performance, and propose a dense-sampling\nstrategy to conduct a targeted search in promising regions that may generate\nwell-performing architectures. We show that: 1) DNAS successfully disentangles\nthe architecture representations, including operation selection, skip\nconnections, and number of layers. 2) Benefiting from interpretability, DNAS\ncan find excellent architectures under different FLOPS restrictions flexibly.\n3) Dense-sampling leads to neural architecture search with higher efficiency\nand better performance. On the NASBench-101 dataset, DNAS achieves\nstate-of-the-art performance of 94.21% using less than 1/13 computational cost\nof baseline methods. On ImageNet dataset, DNAS discovers the competitive\narchitectures that achieves 22.7% test error. our method provides a new\nperspective of understanding neural architecture search.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 03:35:41 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zheng", "Xinyue", ""], ["Wang", "Peng", ""], ["Wang", "Qigang", ""], ["Shi", "Zhongchao", ""]]}, {"id": "2009.13347", "submitter": "Ryoji Tanabe Dr.", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "A Review of Evolutionary Multi-modal Multi-objective Optimization", "comments": "This is an accepted version of a paper published in the IEEE\n  Transactions on Evolutionary Computation", "journal-ref": null, "doi": "10.1109/TEVC.2019.2909744", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal multi-objective optimization aims to find all Pareto optimal\nsolutions including overlapping solutions in the objective space. Multi-modal\nmulti-objective optimization has been investigated in the evolutionary\ncomputation community since 2005. However, it is difficult to survey existing\nstudies in this field because they have been independently conducted and do not\nexplicitly use the term \"multi-modal multi-objective optimization\". To address\nthis issue, this paper reviews existing studies of evolutionary multi-modal\nmulti-objective optimization, including studies published under names that are\ndifferent from \"multi-modal multi-objective optimization\". Our review also\nclarifies open issues in this research area.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:14:36 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2009.13411", "submitter": "Deepak Alapatt", "authors": "Deepak Alapatt and Pietro Mascagni, Vinkle Srivastav, Nicolas Padoy", "title": "Artificial Intelligence in Surgery: Neural Networks and Deep Learning", "comments": null, "journal-ref": "In Hashimoto D.A. (Ed.) Artificial Intelligence in Surgery: A\n  Primer for Surgical Practice. New York: McGraw Hill. ISBN: 978-1260452730\n  (2020)", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks power most recent successes of artificial intelligence,\nspanning from self-driving cars to computer aided diagnosis in radiology and\npathology. The high-stake data intensive process of surgery could highly\nbenefit from such computational methods. However, surgeons and computer\nscientists should partner to develop and assess deep learning applications of\nvalue to patients and healthcare systems. This chapter and the accompanying\nhands-on material were designed for surgeons willing to understand the\nintuitions behind neural networks, become familiar with deep learning concepts\nand tasks, grasp what implementing a deep learning model in surgery means, and\nfinally appreciate the specific challenges and limitations of deep neural\nnetworks in surgery. For the associated hands-on material, please see\nhttps://github.com/CAMMA-public/ai4surgery.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:25:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Alapatt", "Deepak", ""], ["Mascagni", "Pietro", ""], ["Srivastav", "Vinkle", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2009.13501", "submitter": "Koushik Biswas", "authors": "Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey", "title": "EIS -- a family of activation functions combining Exponential, ISRU, and\n  Softplus", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions play a pivotal role in the function learning using\nneural networks. The non-linearity in the learned function is achieved by\nrepeated use of the activation function. Over the years, numerous activation\nfunctions have been proposed to improve accuracy in several tasks. Basic\nfunctions like ReLU, Sigmoid, Tanh, or Softplus have been favorite among the\ndeep learning community because of their simplicity. In recent years, several\nnovel activation functions arising from these basic functions have been\nproposed, which have improved accuracy in some challenging datasets. We propose\na five hyper-parameters family of activation functions, namely EIS, defined as,\n\\[ \\frac{x(\\ln(1+e^x))^\\alpha}{\\sqrt{\\beta+\\gamma x^2}+\\delta e^{-\\theta x}}.\n\\] We show examples of activation functions from the EIS family which\noutperform widely used activation functions on some well known datasets and\nmodels. For example, $\\frac{x\\ln(1+e^x)}{x+1.16e^{-x}}$ beats ReLU by 0.89\\% in\nDenseNet-169, 0.24\\% in Inception V3 in CIFAR100 dataset while 1.13\\% in\nInception V3, 0.13\\% in DenseNet-169, 0.94\\% in SimpleNet model in CIFAR10\ndataset. Also, $\\frac{x\\ln(1+e^x)}{\\sqrt{1+x^2}}$ beats ReLU by 1.68\\% in\nDenseNet-169, 0.30\\% in Inception V3 in CIFAR100 dataset while 1.0\\% in\nInception V3, 0.15\\% in DenseNet-169, 1.13\\% in SimpleNet model in CIFAR10\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:48:24 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 15:51:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Biswas", "Koushik", ""], ["Kumar", "Sandeep", ""], ["Banerjee", "Shilpak", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2009.14194", "submitter": "Emmanuel Dufourq Dr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "Deep Evolution for Facial Emotion Recognition", "comments": "Conference of the South African Institute of Computer Scientists and\n  Information Technologists 2020", "journal-ref": null, "doi": "10.1145/3410886.3410892", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep facial expression recognition faces two challenges that both stem from\nthe large number of trainable parameters: long training times and a lack of\ninterpretability. We propose a novel method based on evolutionary algorithms,\nthat deals with both challenges by massively reducing the number of trainable\nparameters, whilst simultaneously retaining classification performance, and in\nsome cases achieving superior performance. We are robustly able to reduce the\nnumber of parameters on average by 95% (e.g. from 2M to 100k parameters) with\nno loss in classification accuracy. The algorithm learns to choose small\npatches from the image, relative to the nose, which carry the most important\ninformation about emotion, and which coincide with typical human choices of\nimportant features. Our work implements a novel form attention and shows that\nevolutionary algorithms are a valuable addition to machine learning in the deep\nlearning era, both for reducing the number of parameters for facial expression\nrecognition and for providing interpretable features that can help reduce bias.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:58:09 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:21:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "2009.14260", "submitter": "Freddy Lecue", "authors": "Nicholas Halliwell, Freddy Lecue", "title": "Trustworthy Convolutional Neural Networks: A Gradient Penalized-based\n  Approach", "comments": "13pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are commonly used for image\nclassification. Saliency methods are examples of approaches that can be used to\ninterpret CNNs post hoc, identifying the most relevant pixels for a prediction\nfollowing the gradients flow. Even though CNNs can correctly classify images,\nthe underlying saliency maps could be erroneous in many cases. This can result\nin skepticism as to the validity of the model or its interpretation. We propose\na novel approach for training trustworthy CNNs by penalizing parameter choices\nthat result in inaccurate saliency maps generated during training. We add a\npenalty term for inaccurate saliency maps produced when the predicted label is\ncorrect, a penalty term for accurate saliency maps produced when the predicted\nlabel is incorrect, and a regularization term penalizing overly confident\nsaliency maps. Experiments show increased classification performance, user\nengagement, and trust.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:56:40 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Halliwell", "Nicholas", ""], ["Lecue", "Freddy", ""]]}, {"id": "2009.14456", "submitter": "Weihao Tan", "authors": "Weihao Tan, Devdhar Patel, Robert Kozma", "title": "Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven\n  Spiking Neural Networks", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) have great potential for energy-efficient\nimplementation of Deep Neural Networks (DNNs) on dedicated neuromorphic\nhardware. Recent studies demonstrated competitive performance of SNNs compared\nwith DNNs on image classification tasks, including CIFAR-10 and ImageNet data.\nThe present work focuses on using SNNs in combination with deep reinforcement\nlearning in ATARI games, which involves additional complexity as compared to\nimage classification. We review the theory of converting DNNs to SNNs and\nextending the conversion to Deep Q-Networks (DQNs). We propose a robust\nrepresentation of the firing rate to reduce the error during the conversion\nprocess. In addition, we introduce a new metric to evaluate the conversion\nprocess by comparing the decisions made by the DQN and SNN, respectively. We\nalso analyze how the simulation time and parameter normalization influence the\nperformance of converted SNNs. We achieve competitive scores on 17\ntop-performing Atari games. To the best of our knowledge, our work is the first\nto achieve state-of-the-art performance on multiple Atari games with SNNs. Our\nwork serves as a benchmark for the conversion of DQNs to SNNs and paves the way\nfor further research on solving reinforcement learning tasks with SNNs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 05:37:59 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 01:21:48 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Tan", "Weihao", ""], ["Patel", "Devdhar", ""], ["Kozma", "Robert", ""]]}, {"id": "2009.14477", "submitter": "Eneko Osaba", "authors": "Eneko Osaba, Esther Villar-Rodriguez, Javier Del Ser", "title": "A Coevolutionary Variable Neighborhood Search Algorithm for Discrete\n  Multitasking (CoVNS): Application to Community Detection over Graphs", "comments": "7 pages, paper accepted for presentation in the 2020 IEEE Symposium\n  Series on Computational Intelligence (IEEE SSCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of the multitasking optimization paradigm is to solve multiple\nand concurrent optimization tasks in a simultaneous way through a single search\nprocess. For attaining promising results, potential complementarities and\nsynergies between tasks are properly exploited, helping each other by virtue of\nthe exchange of genetic material. This paper is focused on Evolutionary\nMultitasking, which is a perspective for dealing with multitasking optimization\nscenarios by embracing concepts from Evolutionary Computation. This work\ncontributes to this field by presenting a new multitasking approach named as\nCoevolutionary Variable Neighborhood Search Algorithm, which finds its\ninspiration on both the Variable Neighborhood Search metaheuristic and\ncoevolutionary strategies. The second contribution of this paper is the\napplication field, which is the optimal partitioning of graph instances whose\nconnections among nodes are directed and weighted. This paper pioneers on the\nsimultaneous solving of this kind of tasks. Two different multitasking\nscenarios are considered, each comprising 11 graph instances. Results obtained\nby our method are compared to those issued by a parallel Variable Neighborhood\nSearch and independent executions of the basic Variable Neighborhood Search.\nThe discussion on such results support our hypothesis that the proposed method\nis a promising scheme for simultaneous solving community detection problems\nover graphs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 07:26:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Osaba", "Eneko", ""], ["Villar-Rodriguez", "Esther", ""], ["Del Ser", "Javier", ""]]}, {"id": "2009.14506", "submitter": "Carola Doerr", "authors": "Tome Eftimov, Gorjan Popovski, Quentin Renau, Peter Korosec, Carola\n  Doerr", "title": "Linear Matrix Factorization Embeddings for Single-objective Optimization\n  Landscapes", "comments": "To appear in Proc. of 2020 IEEE Symposium Series on Computational\n  Intelligence (SSCI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated per-instance algorithm selection and configuration have shown\npromising performances for a number of classic optimization problems, including\nsatisfiability, AI planning, and TSP. The techniques often rely on a set of\nfeatures that measure some characteristics of the problem instance at hand. In\nthe context of black-box optimization, these features have to be derived from a\nset of $(x,f(x))$ samples. A number of different features have been proposed in\nthe literature, measuring, for example, the modality, the separability, or the\nruggedness of the instance at hand. Several of the commonly used features,\nhowever, are highly correlated. While state-of-the-art machine learning\ntechniques can routinely filter such correlations, they hinder explainability\nof the derived algorithm design techniques.\n  We therefore propose in this work to pre-process the measured (raw) landscape\nfeatures through representation learning. More precisely, we show that a linear\ndimensionality reduction via matrix factorization significantly contributes\ntowards a better detection of correlation between different problem instances\n-- a key prerequisite for successful automated algorithm design.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 08:46:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Eftimov", "Tome", ""], ["Popovski", "Gorjan", ""], ["Renau", "Quentin", ""], ["Korosec", "Peter", ""], ["Doerr", "Carola", ""]]}, {"id": "2009.14670", "submitter": "Thanh Tung Khuat", "authors": "Thanh Tung Khuat and Bogdan Gabrys", "title": "An Online Learning Algorithm for a Neuro-Fuzzy Classifier with\n  Mixed-Attribute Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  General fuzzy min-max neural network (GFMMNN) is one of the efficient\nneuro-fuzzy systems for data classification. However, one of the downsides of\nits original learning algorithms is the inability to handle and learn from the\nmixed-attribute data. While categorical features encoding methods can be used\nwith the GFMMNN learning algorithms, they exhibit a lot of shortcomings. Other\napproaches proposed in the literature are not suitable for on-line learning as\nthey require entire training data available in the learning phase. With the\nrapid change in the volume and velocity of streaming data in many application\nareas, it is increasingly required that the constructed models can learn and\nadapt to the continuous data changes in real-time without the need for their\nfull retraining or access to the historical data. This paper proposes an\nextended online learning algorithm for the GFMMNN. The proposed method can\nhandle the datasets with both continuous and categorical features. The\nextensive experiments confirmed superior and stable classification performance\nof the proposed approach in comparison to other relevant learning algorithms\nfor the GFMM model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 13:45:36 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Khuat", "Thanh Tung", ""], ["Gabrys", "Bogdan", ""]]}, {"id": "2009.14695", "submitter": "Carlos Perales-Gonzalez", "authors": "Carlos Perales-Gonz\\'alez", "title": "Global convergence of Negative Correlation Extreme Learning Machine", "comments": "Jupyter Notebook associated in\n  https://github.com/cperales/pyridge/blob/ncelm/NCELM_convergence.ipynb", "journal-ref": "Neural Process Lett (2021)", "doi": "10.1007/s11063-021-10492-z", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensemble approaches introduced in the Extreme Learning Machine (ELM)\nliterature mainly come from methods that relies on data sampling procedures,\nunder the assumption that the training data are heterogeneously enough to set\nup diverse base learners. To overcome this assumption, it was proposed an ELM\nensemble method based on the Negative Correlation Learning (NCL) framework,\ncalled Negative Correlation Extreme Learning Machine (NCELM). This model works\nin two stages: i) different ELMs are generated as base learners with random\nweights in the hidden layer, and ii) a NCL penalty term with the information of\nthe ensemble prediction is introduced in each ELM minimization problem,\nupdating the base learners, iii) second step is iterated until the ensemble\nconverges.\n  Although this NCL ensemble method was validated by an experimental study with\nmultiple benchmark datasets, no information was given on the conditions about\nthis convergence. This paper mathematically presents the sufficient conditions\nto guarantee the global convergence of NCELM. The update of the ensemble in\neach iteration is defined as a contraction mapping function, and through Banach\ntheorem, global convergence of the ensemble is proved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:18:10 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 09:18:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Perales-Gonz\u00e1lez", "Carlos", ""]]}, {"id": "2009.14700", "submitter": "Ryoji Tanabe Dr.", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "A Framework to Handle Multi-modal Multi-objective Optimization in\n  Decomposition-based Evolutionary Algorithms", "comments": "This is an accepted version of a paper published in the IEEE\n  Transactions on Evolutionary Computation", "journal-ref": null, "doi": "10.1109/TEVC.2019.2949841", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal multi-objective optimization is to locate (almost) equivalent\nPareto optimal solutions as many as possible. While decomposition-based\nevolutionary algorithms have good performance for multi-objective optimization,\nthey are likely to perform poorly for multi-modal multi-objective optimization\ndue to the lack of mechanisms to maintain the solution space diversity. To\naddress this issue, this paper proposes a framework to improve the performance\nof decomposition-based evolutionary algorithms for multi-modal multi-objective\noptimization. Our framework is based on three operations: assignment, deletion,\nand addition operations. One or more individuals can be assigned to the same\nsubproblem to handle multiple equivalent solutions. In each iteration, a child\nis assigned to a subproblem based on its objective vector, i.e., its location\nin the objective space. The child is compared with its neighbors in the\nsolution space assigned to the same subproblem. The performance of improved\nversions of six decomposition-based evolutionary algorithms by our framework is\nevaluated on various test problems regarding the number of objectives, decision\nvariables, and equivalent Pareto optimal solution sets. Results show that the\nimproved versions perform clearly better than their original algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:32:57 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}, {"id": "2009.14702", "submitter": "Vincent Gripon", "authors": "Vincent Gripon, Matthias L\\\"owe, Franck Vermet", "title": "Some Remarks on Replicated Simulated Annealing", "comments": null, "journal-ref": null, "doi": "10.1007/s10955-021-02727-z", "report-no": null, "categories": "cs.LG cs.NE math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently authors have introduced the idea of training discrete weights neural\nnetworks using a mix between classical simulated annealing and a replica ansatz\nknown from the statistical physics literature. Among other points, they claim\ntheir method is able to find robust configurations. In this paper, we analyze\nthis so-called \"replicated simulated annealing\" algorithm. In particular, we\nexplicit criteria to guarantee its convergence, and study when it successfully\nsamples from configurations. We also perform experiments using synthetic and\nreal data bases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:33:53 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 18:41:03 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Gripon", "Vincent", ""], ["L\u00f6we", "Matthias", ""], ["Vermet", "Franck", ""]]}, {"id": "2009.14717", "submitter": "Ryoji Tanabe Dr.", "authors": "Ryoji Tanabe and Hisao Ishibuchi", "title": "Non-elitist Evolutionary Multi-objective Optimizers Revisited", "comments": "This is an accepted version of a paper published in the proceedings\n  of GECCO 2019", "journal-ref": null, "doi": "10.1145/3321707.3321754", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since around 2000, it has been considered that elitist evolutionary\nmulti-objective optimization algorithms (EMOAs) always outperform non-elitist\nEMOAs. This paper revisits the performance of non-elitist EMOAs for\nbi-objective continuous optimization when using an unbounded external archive.\nThis paper examines the performance of EMOAs with two elitist and one\nnon-elitist environmental selections. The performance of EMOAs is evaluated on\nthe bi-objective BBOB problem suite provided by the COCO platform. In contrast\nto conventional wisdom, results show that non-elitist EMOAs with particular\ncrossover methods perform significantly well on the bi-objective BBOB problems\nwith many decision variables when using the unbounded external archive. This\npaper also analyzes the properties of the non-elitist selection.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:52:58 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Tanabe", "Ryoji", ""], ["Ishibuchi", "Hisao", ""]]}]