[{"id": "2011.00111", "submitter": "Bhavin Shastri", "authors": "Bhavin J. Shastri, Alexander N. Tait, Thomas Ferreira de Lima, Wolfram\n  H. P. Pernice, Harish Bhaskaran, C. David Wright, Paul R. Prucnal", "title": "Photonics for artificial intelligence and neuromorphic computing", "comments": "21 pages, 6 figures", "journal-ref": "Nature Photonics 15, 102-114 (2021)", "doi": "10.1038/s41566-020-00754-y", "report-no": null, "categories": "physics.optics cs.NE physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in photonic computing has flourished due to the proliferation of\noptoelectronic components on photonic integration platforms. Photonic\nintegrated circuits have enabled ultrafast artificial neural networks,\nproviding a framework for a new class of information processing machines.\nAlgorithms running on such hardware have the potential to address the growing\ndemand for machine learning and artificial intelligence, in areas such as\nmedical diagnosis, telecommunications, and high-performance and scientific\ncomputing. In parallel, the development of neuromorphic electronics has\nhighlighted challenges in that domain, in particular, related to processor\nlatency. Neuromorphic photonics offers sub-nanosecond latencies, providing a\ncomplementary opportunity to extend the domain of artificial intelligence.\nHere, we review recent advances in integrated photonic neuromorphic systems,\ndiscuss current and future challenges, and outline the advances in science and\ntechnology needed to meet those challenges.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 21:41:44 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 21:06:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shastri", "Bhavin J.", ""], ["Tait", "Alexander N.", ""], ["de Lima", "Thomas Ferreira", ""], ["Pernice", "Wolfram H. P.", ""], ["Bhaskaran", "Harish", ""], ["Wright", "C. David", ""], ["Prucnal", "Paul R.", ""]]}, {"id": "2011.00241", "submitter": "Sunil Vadera", "authors": "Sunil Vadera and Salem Ameen", "title": "Methods for Pruning Deep Neural Networks", "comments": "Pre-Print version 1, prior to submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a survey of methods for pruning deep neural networks,\nfrom algorithms first proposed for fully connected networks in the 1990s to the\nrecent methods developed for reducing the size of convolutional neural\nnetworks. The paper begins by bringing together many different algorithms by\ncategorising them based on the underlying approach used. It then focuses on\nthree categories: methods that use magnitude-based pruning, methods that\nutilise clustering to identify redundancy, and methods that utilise sensitivity\nanalysis. Some of the key influencing studies within these categories are\npresented to illuminate the underlying approaches and results achieved.\n  Most studies on pruning present results from empirical evaluations, which are\ndistributed in the literature as new architectures, algorithms and data sets\nhave evolved with time. This paper brings together the reported results from\nsome key papers in one place by providing a resource that can be used to\nquickly compare reported results, and trace studies where specific methods,\ndata sets and architectures have been used.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 10:58:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Vadera", "Sunil", ""], ["Ameen", "Salem", ""]]}, {"id": "2011.00521", "submitter": "Hao Wang", "authors": "Bas van Stein and Hao Wang and Thomas B\\\"ack", "title": "Neural Network Design: Learning from Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) aims to optimize deep neural networks'\narchitecture for better accuracy or smaller computational cost and has recently\ngained more research interests. Despite various successful approaches proposed\nto solve the NAS task, the landscape of it, along with its properties, are\nrarely investigated. In this paper, we argue for the necessity of studying the\nlandscape property thereof and propose to use the so-called Exploratory\nLandscape Analysis (ELA) techniques for this goal. Taking a broad set of\ndesigns of the deep convolutional network, we conduct extensive experimentation\nto obtain their performance. Based on our analysis of the experimental results,\nwe observed high similarities between well-performing architecture designs,\nwhich is then used to significantly narrow the search space to improve the\nefficiency of any NAS algorithm. Moreover, we extract the ELA features over the\nNAS landscapes on three common image classification data sets, MNIST, Fashion,\nand CIFAR-10, which shows that the NAS landscape can be distinguished for those\nthree data sets. Also, when comparing to the ELA features of the well-known\nBlack-Box Optimization Benchmarking (BBOB) problem set, we found out that the\nNAS landscapes surprisingly form a new problem class on its own, which can be\nseparated from all $24$ BBOB problems. Given this interesting observation, we,\ntherefore, state the importance of further investigation on selecting an\nefficient optimizer for the NAS landscape as well as the necessity of\naugmenting the current benchmark problem set.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:02:02 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["van Stein", "Bas", ""], ["Wang", "Hao", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2011.00534", "submitter": "Julien Dupeyroux", "authors": "Julien Dupeyroux, Jesse Hagenaars, Federico Paredes-Vall\\'es, and\n  Guido de Croon", "title": "Neuromorphic control for optic-flow-based landings of MAVs using the\n  Loihi processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic processors like Loihi offer a promising alternative to\nconventional computing modules for endowing constrained systems like micro air\nvehicles (MAVs) with robust, efficient and autonomous skills such as take-off\nand landing, obstacle avoidance, and pursuit. However, a major challenge for\nusing such processors on robotic platforms is the reality gap between\nsimulation and the real world. In this study, we present for the very first\ntime a fully embedded application of the Loihi neuromorphic chip prototype in a\nflying robot. A spiking neural network (SNN) was evolved to compute the thrust\ncommand based on the divergence of the ventral optic flow field to perform\nautonomous landing. Evolution was performed in a Python-based simulator using\nthe PySNN library. The resulting network architecture consists of only 35\nneurons distributed among 3 layers. Quantitative analysis between simulation\nand Loihi reveals a root-mean-square error of the thrust setpoint as low as\n0.005 g, along with a 99.8% matching of the spike sequences in the hidden\nlayer, and 99.7% in the output layer. The proposed approach successfully\nbridges the reality gap, offering important insights for future neuromorphic\napplications in robotics. Supplementary material is available at\nhttps://mavlab.tudelft.nl/loihi/.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:25:04 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Dupeyroux", "Julien", ""], ["Hagenaars", "Jesse", ""], ["Paredes-Vall\u00e9s", "Federico", ""], ["de Croon", "Guido", ""]]}, {"id": "2011.00624", "submitter": "Joshua Mack", "authors": "Joshua Mack, Ruben Purdy, Kris Rockowitz, Michael Inouye, Edward\n  Richter, Spencer Valancius, Nirmal Kumbhare, Md Sahil Hassan, Kaitlin Fair,\n  John Mixter, Ali Akoglu", "title": "RANC: Reconfigurable Architecture for Neuromorphic Computing", "comments": "18 pages, 12 figures, accepted for publication in IEEE Transactions\n  on Computer-Aided Design of Integrated Circuits and Systems. For associated\n  source files see https://github.com/UA-RCL/RANC", "journal-ref": null, "doi": "10.1109/TCAD.2020.3038151", "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic architectures have been introduced as platforms for energy\nefficient spiking neural network execution. The massive parallelism offered by\nthese architectures has also triggered interest from non-machine learning\napplication domains. In order to lift the barriers to entry for hardware\ndesigners and application developers we present RANC: a Reconfigurable\nArchitecture for Neuromorphic Computing, an open-source highly flexible\necosystem that enables rapid experimentation with neuromorphic architectures in\nboth software via C++ simulation and hardware via FPGA emulation. We present\nthe utility of the RANC ecosystem by showing its ability to recreate behavior\nof the IBM's TrueNorth and validate with direct comparison to IBM's Compass\nsimulation environment and published literature. RANC allows optimizing\narchitectures based on application insights as well as prototyping future\nneuromorphic architectures that can support new classes of applications\nentirely. We demonstrate the highly parameterized and configurable nature of\nRANC by studying the impact of architectural changes on improving application\nmapping efficiency with quantitative analysis based on Alveo U250 FPGA. We\npresent post routing resource usage and throughput analysis across\nimplementations of Synthetic Aperture Radar classification and Vector Matrix\nMultiplication applications, and demonstrate a neuromorphic architecture that\nscales to emulating 259K distinct neurons and 73.3M distinct synapses.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 20:29:52 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mack", "Joshua", ""], ["Purdy", "Ruben", ""], ["Rockowitz", "Kris", ""], ["Inouye", "Michael", ""], ["Richter", "Edward", ""], ["Valancius", "Spencer", ""], ["Kumbhare", "Nirmal", ""], ["Hassan", "Md Sahil", ""], ["Fair", "Kaitlin", ""], ["Mixter", "John", ""], ["Akoglu", "Ali", ""]]}, {"id": "2011.00702", "submitter": "Rafael Pinto", "authors": "Rafael Pinto", "title": "Fast Reinforcement Learning with Incremental Gaussian Mixture Models", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel algorithm that integrates a data-efficient\nfunction approximator with reinforcement learning in continuous state spaces.\nAn online and incremental algorithm capable of learning from a single pass\nthrough data, called Incremental Gaussian Mixture Network (IGMN), was employed\nas a sample-efficient function approximator for the joint state and Q-values\nspace, all in a single model, resulting in a concise and data-efficient\nalgorithm, i.e., a reinforcement learning algorithm that learns from very few\ninteractions with the environment. Results are analyzed to explain the\nproperties of the obtained algorithm, and it is observed that the use of the\nIGMN function approximator brings some important advantages to reinforcement\nlearning in relation to conventional neural networks trained by gradient\ndescent methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 03:18:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pinto", "Rafael", ""]]}, {"id": "2011.00836", "submitter": "Koushik Kumaran", "authors": "Pranav Mani, ES Gopi, Koushik Kumaran, Hrishikesh Shekhar, Sharan\n  Chandra", "title": "Ant Colony Inspired Machine Learning Algorithm for Identifying and\n  Emulating Virtual Sensors", "comments": "9 Pages, 7 Figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible. All Authors are Co-First\n  Authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale of systems employed in industrial environments demands a large\nnumber of sensors to facilitate meticulous monitoring and functioning. These\nrequirements could potentially lead to inefficient system designs. The data\ncoming from various sensors are often correlated due to the underlying\nrelations in the system parameters that the sensors monitor. In theory, it\nshould be possible to emulate the output of certain sensors based on other\nsensors. Tapping into such possibilities holds tremendous advantages in terms\nof reducing system design complexity. In order to identify the subset of\nsensors whose readings can be emulated, the sensors must be grouped into\nclusters. Complex systems generally have a large quantity of sensors that\ncollect and store data over prolonged periods of time. This leads to the\naccumulation of massive amounts of data. In this paper we propose an end-to-end\nalgorithmic solution, to realise virtual sensors in such systems. This\nalgorithm splits the dataset into blocks and clusters each of them\nindividually. It then fuses these clustering solutions to obtain a global\nsolution using an Ant Colony inspired technique, FAC2T. Having grouped the\nsensors into clusters, we select representative sensors from each cluster.\nThese sensors are retained in the system while the other sensors readings are\nemulated by applying supervised learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:06:14 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 08:10:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mani", "Pranav", ""], ["Gopi", "ES", ""], ["Kumaran", "Koushik", ""], ["Shekhar", "Hrishikesh", ""], ["Chandra", "Sharan", ""]]}, {"id": "2011.00888", "submitter": "Fabio Schittler Neves", "authors": "Fabio Schittler Neves and Marc Timme", "title": "Controlled Perturbation-Induced Switching in Pulse-Coupled Oscillator\n  Networks", "comments": null, "journal-ref": "J. Phys. A 42, 345103 (2009)", "doi": "10.1088/1751-8113/42/34/345103", "report-no": null, "categories": "nlin.AO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulse-coupled systems such as spiking neural networks exhibit nontrivial\ninvariant sets in the form of attracting yet unstable saddle periodic orbits\nwhere units are synchronized into groups. Heteroclinic connections between such\norbits may in principle support switching processes in those networks and\nenable novel kinds of neural computations. For small networks of coupled\noscillators we here investigate under which conditions and how system symmetry\nenforces or forbids certain switching transitions that may be induced by\nperturbations. For networks of five oscillators we derive explicit transition\nrules that for two cluster symmetries deviate from those known from oscillators\ncoupled continuously in time. A third symmetry yields heteroclinic networks\nthat consist of sets of all unstable attractors with that symmetry and the\nconnections between them. Our results indicate that pulse-coupled systems can\nreliably generate well-defined sets of complex spatiotemporal patterns that\nconform to specific transition rules. We briefly discuss possible implications\nfor computation with spiking neural systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:49:56 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Neves", "Fabio Schittler", ""], ["Timme", "Marc", ""]]}, {"id": "2011.01447", "submitter": "C.-H. Huck Yang", "authors": "Hu Hu, Chao-Han Huck Yang, Xianjun Xia, Xue Bai, Xin Tang, Yajian\n  Wang, Shutong Niu, Li Chai, Juanjuan Li, Hongning Zhu, Feng Bao, Yuanjun\n  Zhao, Sabato Marco Siniscalchi, Yannan Wang, Jun Du, Chin-Hui Lee", "title": "A Two-Stage Approach to Device-Robust Acoustic Scene Classification", "comments": "Submitted to ICASSP 2021. Code available:\n  https://github.com/MihawkHu/DCASE2020_task1", "journal-ref": "ICASSP 2021-2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": null, "report-no": "845--849", "categories": "cs.SD cs.AI cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To improve device robustness, a highly desirable key feature of a competitive\ndata-driven acoustic scene classification (ASC) system, a novel two-stage\nsystem based on fully convolutional neural networks (CNNs) is proposed. Our\ntwo-stage system leverages on an ad-hoc score combination based on two CNN\nclassifiers: (i) the first CNN classifies acoustic inputs into one of three\nbroad classes, and (ii) the second CNN classifies the same inputs into one of\nten finer-grained classes. Three different CNN architectures are explored to\nimplement the two-stage classifiers, and a frequency sub-sampling scheme is\ninvestigated. Moreover, novel data augmentation schemes for ASC are also\ninvestigated. Evaluated on DCASE 2020 Task 1a, our results show that the\nproposed ASC system attains a state-of-the-art accuracy on the development set,\nwhere our best system, a two-stage fusion of CNN ensembles, delivers a 81.9%\naverage accuracy among multi-device test data, and it obtains a significant\nimprovement on unseen devices. Finally, neural saliency analysis with class\nactivation mapping (CAM) gives new insights on the patterns learnt by our\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:27:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hu", "Hu", ""], ["Yang", "Chao-Han Huck", ""], ["Xia", "Xianjun", ""], ["Bai", "Xue", ""], ["Tang", "Xin", ""], ["Wang", "Yajian", ""], ["Niu", "Shutong", ""], ["Chai", "Li", ""], ["Li", "Juanjuan", ""], ["Zhu", "Hongning", ""], ["Bao", "Feng", ""], ["Zhao", "Yuanjun", ""], ["Siniscalchi", "Sabato Marco", ""], ["Wang", "Yannan", ""], ["Du", "Jun", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2011.01613", "submitter": "Tomas Maul", "authors": "Chen Wen Kang, Chua Meng Hong, Tomas Maul", "title": "Towards a Universal Gating Network for Mixtures of Experts", "comments": "Appl Intell (2021)", "journal-ref": null, "doi": "10.1007/s10489-021-02301-w", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination and aggregation of knowledge from multiple neural networks\ncan be commonly seen in the form of mixtures of experts. However, such\ncombinations are usually done using networks trained on the same tasks, with\nlittle mention of the combination of heterogeneous pre-trained networks,\nespecially in the data-free regime. This paper proposes multiple data-free\nmethods for the combination of heterogeneous neural networks, ranging from the\nutilization of simple output logit statistics, to training specialized gating\nnetworks. The gating networks decide whether specific inputs belong to specific\nnetworks based on the nature of the expert activations generated. The\nexperiments revealed that the gating networks, including the universal gating\napproach, constituted the most accurate approach, and therefore represent a\npragmatic step towards applications with heterogeneous mixtures of experts in a\ndata-free regime. The code for this project is hosted on github at\nhttps://github.com/cwkang1998/network-merging.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:47:21 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kang", "Chen Wen", ""], ["Hong", "Chua Meng", ""], ["Maul", "Tomas", ""]]}, {"id": "2011.01926", "submitter": "Ke Li", "authors": "Shichong Peng and Ke Li", "title": "Generating Unobserved Alternatives", "comments": "Videos in the article are also available as ancillary files in the\n  previous version (arXiv:2011.01926v3). Website:\n  https://niopeng.github.io/HyperRIM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems where multiple predictions can be considered correct,\nbut only one of them is given as supervision. This setting differs from both\nthe regression and class-conditional generative modelling settings: in the\nformer, there is a unique observed output for each input, which is provided as\nsupervision; in the latter, there are many observed outputs for each input, and\nmany are provided as supervision. Applying either regression methods and\nconditional generative models to the present setting often results in a model\nthat can only make a single prediction for each input. We explore several\nproblems that have this property and develop an approach that can generate\nmultiple high-quality predictions given the same input. As a result, it can be\nused to generate high-quality outputs that are different from the observed\noutput.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:57:57 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 18:03:05 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 08:20:49 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 10:05:52 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peng", "Shichong", ""], ["Li", "Ke", ""]]}, {"id": "2011.02081", "submitter": "Matthew Aguirre", "authors": "Matthew Aguirre, Jan Sokol, Guhan Venkataraman, Alexander Ioannidis", "title": "A deep learning classifier for local ancestry inference", "comments": "Accepted to Learning Meaningful Representations of Life (LMRL),\n  Workshop at the 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local ancestry inference (LAI) identifies the ancestry of each segment of an\nindividual's genome and is an important step in medical and population genetic\nstudies of diverse cohorts. Several techniques have been used for LAI,\nincluding Hidden Markov Models and Random Forests. Here, we formulate the LAI\ntask as an image segmentation problem and develop a new LAI tool using a deep\nconvolutional neural network with an encoder-decoder architecture. We train our\nmodel using complete genome sequences from 982 unadmixed individuals from each\nof five continental ancestry groups, and we evaluate it using simulated admixed\ndata derived from an additional 279 individuals selected from the same\npopulations. We show that our model is able to learn admixture as a zero-shot\ntask, yielding ancestry assignments that are nearly as accurate as those from\nthe existing gold standard tool, RFMix.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:42:01 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Aguirre", "Matthew", ""], ["Sokol", "Jan", ""], ["Venkataraman", "Guhan", ""], ["Ioannidis", "Alexander", ""]]}, {"id": "2011.02159", "submitter": "Niru Maheswaranathan", "authors": "Niru Maheswaranathan, David Sussillo, Luke Metz, Ruoxi Sun, Jascha\n  Sohl-Dickstein", "title": "Reverse engineering learned optimizers reveals known and novel\n  mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned optimizers are algorithms that can themselves be trained to solve\noptimization problems. In contrast to baseline optimizers (such as momentum or\nAdam) that use simple update rules derived from theoretical principles, learned\noptimizers use flexible, high-dimensional, nonlinear parameterizations.\nAlthough this can lead to better performance in certain settings, their inner\nworkings remain a mystery. How is a learned optimizer able to outperform a well\ntuned baseline? Has it learned a sophisticated combination of existing\noptimization techniques, or is it implementing completely new behavior? In this\nwork, we address these questions by careful analysis and visualization of\nlearned optimizers. We study learned optimizers trained from scratch on three\ndisparate tasks, and discover that they have learned interpretable mechanisms,\nincluding: momentum, gradient clipping, learning rate schedules, and a new form\nof learning rate adaptation. Moreover, we show how the dynamics of learned\noptimizers enables these behaviors. Our results help elucidate the previously\nmurky understanding of how learned optimizers work, and establish tools for\ninterpreting future learned optimizers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 07:12:43 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Maheswaranathan", "Niru", ""], ["Sussillo", "David", ""], ["Metz", "Luke", ""], ["Sun", "Ruoxi", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2011.02188", "submitter": "Pawel Plawiak", "authors": "Filip Pa{\\l}ka, Wojciech Ksi\\k{a}\\.zek, Pawe{\\l} P{\\l}awiak, Micha{\\l}\n  Romaszewski, Kamil Ksi\\k{a}\\.zek", "title": "Hyperspectral classification of blood-like substances using machine\n  learning methods combined with genetic algorithms in transductive and\n  inductive scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is focused on applying genetic algorithms (GA) to model and band\nselection in hyperspectral image classification. We use a forensic-inspired\ndata set of seven hyperspectral images with blood and five visually similar\nsubstances to test GA-optimised classifiers in two scenarios: when the training\nand test data come from the same image and when they come from different\nimages, which is a more challenging task due to significant spectra\ndifferences. In our experiments we compare GA with a classic model optimisation\nthrough grid search. Our results show that GA-based model optimisation can\nreduce the number of bands and create an accurate classifier that outperforms\nthe GS-based reference models, provided that during model optimisation it has\naccess to examples similar to test data. We illustrate this with experiment\nhighlighting the importance of a validation set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 09:18:16 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Pa\u0142ka", "Filip", ""], ["Ksi\u0105\u017cek", "Wojciech", ""], ["P\u0142awiak", "Pawe\u0142", ""], ["Romaszewski", "Micha\u0142", ""], ["Ksi\u0105\u017cek", "Kamil", ""]]}, {"id": "2011.02291", "submitter": "Thibault Lechien", "authors": "Thibault Lechien, Jorik Jooken, Patrick De Causmaecker", "title": "Evolving test instances of the Hamiltonian completion problem", "comments": "12 pages, 12 figures, minor revisions in section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and comparing algorithm performance on graph instances is\nchallenging for multiple reasons. First, there is usually no standard set of\ninstances to benchmark performance. Second, using existing graph generators\nresults in a restricted spectrum of difficulty and the resulting graphs are\nusually not diverse enough to draw sound conclusions. That is why recent work\nproposes a new methodology to generate a diverse set of instances by using an\nevolutionary algorithm. We can then analyze the resulting graphs and get key\ninsights into which attributes are most related to algorithm performance. We\ncan also fill observed gaps in the instance space in order to generate graphs\nwith previously unseen combinations of features. This methodology is applied to\nthe instance space of the Hamiltonian completion problem using two different\nsolvers, namely the Concorde TSP Solver and a multi-start local search\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:04:58 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 19:28:04 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lechien", "Thibault", ""], ["Jooken", "Jorik", ""], ["De Causmaecker", "Patrick", ""]]}, {"id": "2011.02955", "submitter": "Khaled Koutini", "authors": "Khaled Koutini, Florian Henkel, Hamid Eghbal-zadeh, Gerhard Widmer", "title": "Low-Complexity Models for Acoustic Scene Classification Based on\n  Receptive Field Regularization and Frequency Damping", "comments": "Proceedings of the Detection and Classification of Acoustic Scenes\n  and Events 2020 Workshop (DCASE2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks are known to be very demanding in terms of computing and\nmemory requirements. Due to the ever increasing use of embedded systems and\nmobile devices with a limited resource budget, designing low-complexity models\nwithout sacrificing too much of their predictive performance gained great\nimportance. In this work, we investigate and compare several well-known methods\nto reduce the number of parameters in neural networks. We further put these\ninto the context of a recent study on the effect of the Receptive Field (RF) on\na model's performance, and empirically show that we can achieve high-performing\nlow-complexity models by applying specific restrictions on the RFs, in\ncombination with parameter reduction methods. Additionally, we propose a\nfilter-damping technique for regularizing the RF of models, without altering\ntheir architecture and changing their parameter counts. We will show that\nincorporating this technique improves the performance in various low-complexity\nsettings such as pruning and decomposed convolution. Using our proposed filter\ndamping, we achieved the 1st rank at the DCASE-2020 Challenge in the task of\nLow-Complexity Acoustic Scene Classification.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 16:34:11 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Koutini", "Khaled", ""], ["Henkel", "Florian", ""], ["Eghbal-zadeh", "Hamid", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2011.03155", "submitter": "Hock Hung Chieng", "authors": "Hock Hung Chieng, Noorhaniza Wahid and Pauline Ong", "title": "Parametric Flatten-T Swish: An Adaptive Non-linear Activation Function\n  For Deep Learning", "comments": "19 pages", "journal-ref": "Journal of Information and Communication Technology, 20(1), 21-39,\n  2021", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation function is a key component in deep learning that performs\nnon-linear mappings between the inputs and outputs. Rectified Linear Unit\n(ReLU) has been the most popular activation function across the deep learning\ncommunity. However, ReLU contains several shortcomings that can result in\ninefficient training of the deep neural networks, these are: 1) the negative\ncancellation property of ReLU tends to treat negative inputs as unimportant\ninformation for the learning, resulting in a performance degradation; 2) the\ninherent predefined nature of ReLU is unlikely to promote additional\nflexibility, expressivity, and robustness to the networks; 3) the mean\nactivation of ReLU is highly positive and leads to bias shift effect in network\nlayers; and 4) the multilinear structure of ReLU restricts the non-linear\napproximation power of the networks. To tackle these shortcomings, this paper\nintroduced Parametric Flatten-T Swish (PFTS) as an alternative to ReLU. By\ntaking ReLU as a baseline method, the experiments showed that PFTS improved\nclassification accuracy on SVHN dataset by 0.31%, 0.98%, 2.16%, 17.72%, 1.35%,\n0.97%, 39.99%, and 71.83% on DNN-3A, DNN-3B, DNN-4, DNN- 5A, DNN-5B, DNN-5C,\nDNN-6, and DNN-7, respectively. Besides, PFTS also achieved the highest mean\nrank among the comparison methods. The proposed PFTS manifested higher\nnon-linear approximation power during training and thereby improved the\npredictive performance of the networks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 01:50:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chieng", "Hock Hung", ""], ["Wahid", "Noorhaniza", ""], ["Ong", "Pauline", ""]]}, {"id": "2011.03424", "submitter": "Sara Latifi", "authors": "Sara Latifi, Noemi Mauro, Dietmar Jannach", "title": "Session-aware Recommendation: A Surprising Quest for the\n  State-of-the-art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are designed to help users in situations of information\noverload. In recent years, we observed increased interest in session-based\nrecommendation scenarios, where the problem is to make item suggestions to\nusers based only on interactions observed in an ongoing session. However, in\ncases where interactions from previous user sessions are available, the\nrecommendations can be personalized according to the users' long-term\npreferences, a process called session-aware recommendation. Today, research in\nthis area is scattered and many existing works only compare session-aware with\nsession-based models. This makes it challenging to understand what represents\nthe state-of-the-art. To close this research gap, we benchmarked recent\nsession-aware algorithms against each other and against a number of\nsession-based recommendation algorithms and trivial extensions thereof. Our\ncomparison, to some surprise, revealed that (i) item simple techniques based on\nnearest neighbors consistently outperform recent neural techniques and that\n(ii) session-aware models were mostly not better than approaches that do not\nuse long-term preference information. Our work therefore not only points to\npotential methodological issues where new methods are compared to weak\nbaselines, but also indicates that there remains a huge potential for more\nsophisticated session-aware recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:18:01 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Latifi", "Sara", ""], ["Mauro", "Noemi", ""], ["Jannach", "Dietmar", ""]]}, {"id": "2011.03459", "submitter": "Pasquale Minervini", "authors": "Erik Arakelyan, Daniel Daza, Pasquale Minervini, Michael Cochez", "title": "Complex Query Answering with Neural Link Predictors", "comments": "Proceedings of the Ninth International Conference on Learning\n  Representations (ICLR 2021, oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural link predictors are immensely useful for identifying missing edges in\nlarge scale Knowledge Graphs. However, it is still not clear how to use these\nmodels for answering more complex queries that arise in a number of domains,\nsuch as queries using logical conjunctions ($\\land$), disjunctions ($\\lor$) and\nexistential quantifiers ($\\exists$), while accounting for missing edges. In\nthis work, we propose a framework for efficiently answering complex queries on\nincomplete Knowledge Graphs. We translate each query into an end-to-end\ndifferentiable objective, where the truth value of each atom is computed by a\npre-trained neural link predictor. We then analyse two solutions to the\noptimisation problem, including gradient-based and combinatorial search. In our\nexperiments, the proposed approach produces more accurate results than\nstate-of-the-art methods -- black-box neural models trained on millions of\ngenerated queries -- without the need of training on a large and diverse set of\ncomplex queries. Using orders of magnitude less training data, we obtain\nrelative improvements ranging from 8% up to 40% in Hits@3 across different\nknowledge graphs containing factual information. Finally, we demonstrate that\nit is possible to explain the outcome of our model in terms of the intermediate\nsolutions identified for each of the complex query atoms. All our source code\nand datasets are available online, at https://github.com/uclnlp/cqd.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:20:49 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 21:19:37 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 18:08:30 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 09:42:49 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Arakelyan", "Erik", ""], ["Daza", "Daniel", ""], ["Minervini", "Pasquale", ""], ["Cochez", "Michael", ""]]}, {"id": "2011.03488", "submitter": "Gustav Sourek", "authors": "Gustav Sourek, Filip Zelezny, Ondrej Kuzelka", "title": "Learning with Molecules beyond Graph Neural Networks", "comments": "accepted to Machine Learning for Molecules Workshop @ NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a deep learning framework which is inherently based in the\nhighly expressive language of relational logic, enabling to, among other\nthings, capture arbitrarily complex graph structures. We show how Graph Neural\nNetworks and similar models can be easily covered in the framework by\nspecifying the underlying propagation rules in the relational logic. The\ndeclarative nature of the used language then allows to easily modify and extend\nthe propagation schemes into complex structures, such as the molecular rings\nwhich we choose for a short demonstration in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:42:42 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sourek", "Gustav", ""], ["Zelezny", "Filip", ""], ["Kuzelka", "Ondrej", ""]]}, {"id": "2011.03535", "submitter": "Simon Osindero", "authors": "Simon Osindero", "title": "Contrastive Topographic Models: Energy-based density models applied to\n  the understanding of sensory coding and cortical topography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of building theoretical models that help elucidate the\nfunction of the visual brain at computational/algorithmic and\nstructural/mechanistic levels. We seek to understand how the receptive fields\nand topographic maps found in visual cortical areas relate to underlying\ncomputational desiderata. We view the development of sensory systems from the\npopular perspective of probability density estimation; this is motivated by the\nnotion that an effective internal representational scheme is likely to reflect\nthe statistical structure of the environment in which an organism lives. We\napply biologically based constraints on elements of the model.\n  The thesis begins by surveying the relevant literature from the fields of\nneurobiology, theoretical neuroscience, and machine learning. After this review\nwe present our main theoretical and algorithmic developments: we propose a\nclass of probabilistic models, which we refer to as \"energy-based models\", and\nshow equivalences between this framework and various other types of\nprobabilistic model such as Markov random fields and factor graphs; we also\ndevelop and discuss approximate algorithms for performing maximum likelihood\nlearning and inference in our energy based models. The rest of the thesis is\nthen concerned with exploring specific instantiations of such models. By\nperforming constrained optimisation of model parameters to maximise the\nlikelihood of appropriate, naturalistic datasets we are able to qualitatively\nreproduce many of the receptive field and map properties found in vivo, whilst\nsimultaneously learning about statistical regularities in the data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 16:36:43 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Osindero", "Simon", ""]]}, {"id": "2011.03722", "submitter": "Md Faisal Mahbub Chowdhury", "authors": "Abhijit Mishra, Md Faisal Mahbub Chowdhury, Sagar Manohar, Dan\n  Gutfreund and Karthik Sankaranarayanan", "title": "Template Controllable keywords-to-text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel neural model for the understudied task of\ngenerating text from keywords. The model takes as input a set of un-ordered\nkeywords, and part-of-speech (POS) based template instructions. This makes it\nideal for surface realization in any NLG setup. The framework is based on the\nencode-attend-decode paradigm, where keywords and templates are encoded first,\nand the decoder judiciously attends over the contexts derived from the encoded\nkeywords and templates to generate the sentences. Training exploits weak\nsupervision, as the model trains on a large amount of labeled data with\nkeywords and POS based templates prepared through completely automatic means.\nQualitative and quantitative performance analyses on publicly available\ntest-data in various domains reveal our system's superiority over baselines,\nbuilt using state-of-the-art neural machine translation and controllable\ntransfer techniques. Our approach is indifferent to the order of input\nkeywords.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 08:05:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mishra", "Abhijit", ""], ["Chowdhury", "Md Faisal Mahbub", ""], ["Manohar", "Sagar", ""], ["Gutfreund", "Dan", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "2011.03842", "submitter": "Brosnan Yuen", "authors": "Brosnan Yuen, Minh Tu Hoang, Xiaodai Dong, and Tao Lu", "title": "Universal Activation Function For Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a Universal Activation Function (UAF) that achieves\nnear optimal performance in quantification, classification, and reinforcement\nlearning (RL) problems. For any given problem, the optimization algorithms are\nable to evolve the UAF to a suitable activation function by tuning the UAF's\nparameters. For the CIFAR-10 classification and VGG-8, the UAF converges to the\nMish like activation function, which has near optimal performance $F_{1} =\n0.9017\\pm0.0040$ when compared to other activation functions. For the\nquantification of simulated 9-gas mixtures in 30 dB signal-to-noise ratio (SNR)\nenvironments, the UAF converges to the identity function, which has near\noptimal root mean square error of $0.4888 \\pm 0.0032$ $\\mu M$. In the\nBipedalWalker-v2 RL dataset, the UAF achieves the 250 reward in $961 \\pm 193$\nepochs, which proves that the UAF converges in the lowest number of epochs.\nFurthermore, the UAF converges to a new activation function in the\nBipedalWalker-v2 RL dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 20:13:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yuen", "Brosnan", ""], ["Hoang", "Minh Tu", ""], ["Dong", "Xiaodai", ""], ["Lu", "Tao", ""]]}, {"id": "2011.04092", "submitter": "Koen Oostermeijer", "authors": "Koen Oostermeijer, Qing Wang and Jun Du", "title": "Frequency Gating: Improved Convolutional Neural Networks for Speech\n  Enhancement in the Time-Frequency Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the strengths of traditional convolutional neural networks (CNNs) is\ntheir inherent translational invariance. However, for the task of speech\nenhancement in the time-frequency domain, this property cannot be fully\nexploited due to a lack of invariance in the frequency direction. In this paper\nwe propose to remedy this inefficiency by introducing a method, which we call\nFrequency Gating, to compute multiplicative weights for the kernels of the CNN\nin order to make them frequency dependent. Several mechanisms are explored:\ntemporal gating, in which weights are dependent on prior time frames, local\ngating, whose weights are generated based on a single time frame and the ones\nadjacent to it, and frequency-wise gating, where each kernel is assigned a\nweight independent of the input data. Experiments with an autoencoder neural\nnetwork with skip connections show that both local and frequency-wise gating\noutperform the baseline and are therefore viable ways to improve CNN-based\nspeech enhancement neural networks. In addition, a loss function based on the\nextended short-time objective intelligibility score (ESTOI) is introduced,\nwhich we show to outperform the standard mean squared error (MSE) loss\nfunction.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 22:04:00 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Oostermeijer", "Koen", ""], ["Wang", "Qing", ""], ["Du", "Jun", ""]]}, {"id": "2011.04189", "submitter": "Naveed Tahir", "authors": "Naveed Tahir, Garrett E. Katz", "title": "Numerical Exploration of Training Loss Level-Sets in Deep Neural\n  Networks", "comments": "Code maintained at https://github.com/vanishinggrad/levelsets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational method for empirically characterizing the training\nloss level-sets of deep neural networks. Our method numerically constructs a\npath in parameter space that is constrained to a set with a fixed near-zero\ntraining loss. By measuring regularization functions and test loss at different\npoints within this path, we examine how different points in the parameter space\nwith the same fixed training loss compare in terms of generalization ability.\nWe also compare this method for finding regularized points with the more\ntypical method, that uses objective functions which are weighted sums of\ntraining loss and regularization terms. We apply dimensionality reduction to\nthe traversed paths in order to visualize the loss level sets in a\nwell-regularized region of parameter space. Our results provide new information\nabout the loss landscape of deep neural networks, as well as a new strategy for\nreducing test loss.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:49:33 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 01:22:39 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 17:57:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Tahir", "Naveed", ""], ["Katz", "Garrett E.", ""]]}, {"id": "2011.04275", "submitter": "Angelica Sofia Valeriani", "authors": "Angelica Sofia Valeriani", "title": "Runtime Performances Benchmark for Knowledge Graph Embedding Methods", "comments": "arXiv admin note: text overlap with arXiv:1903.11406,\n  arXiv:2002.00819 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper wants to focus on providing a characterization of the runtime\nperformances of state-of-the-art implementations of KGE alghoritms, in terms of\nmemory footprint and execution time. Despite the rapidly growing interest in\nKGE methods, so far little attention has been devoted to their comparison and\nevaluation; in particular, previous work mainly focused on performance in terms\nof accuracy in specific tasks, such as link prediction. To this extent, a\nframework is proposed for evaluating available KGE implementations against\ngraphs with different properties, with a particular focus on the effectiveness\nof the adopted optimization strategies. Graphs and models have been trained\nleveraging different architectures, in order to enlighten features and\nproperties of both models and the architectures they have been trained on. Some\nresults enlightened with experiments in this document are the fact that\nmultithreading is efficient, but benefit deacreases as the number of threads\ngrows in case of CPU. GPU proves to be the best architecture for the given\ntask, even if CPU with some vectorized instructions still behaves well.\nFinally, RAM utilization for the loading of the graph never changes between\ndifferent architectures and depends only on the type of graph, not on the\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:58:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Valeriani", "Angelica Sofia", ""]]}, {"id": "2011.04422", "submitter": "Alvin Lim", "authors": "Jiefeng Xu and Evren Gul and Alvin Lim", "title": "Maximizing Store Revenues using Tabu Search for Floor Space Optimization", "comments": null, "journal-ref": "International Journal of Revenue Management (IJRM), Vol. 12, No.\n  1/2, 2021", "doi": "10.1504/IJRM.2021.114969", "report-no": null, "categories": "cs.AI cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floor space optimization is a critical revenue management problem commonly\nencountered by retailers. It maximizes store revenue by optimally allocating\nfloor space to product categories which are assigned to their most appropriate\nplanograms. We formulate the problem as a connected multi-choice knapsack\nproblem with an additional global constraint and propose a tabu search based\nmeta-heuristic that exploits the multiple special neighborhood structures. We\nalso incorporate a mechanism to determine how to combine the multiple\nneighborhood moves. A candidate list strategy based on learning from prior\nsearch history is also employed to improve the search quality. The results of\ncomputational testing with a set of test problems show that our tabu search\nheuristic can solve all problems within a reasonable amount of time. Analyses\nof individual contributions of relevant components of the algorithm were\nconducted with computational experiments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:42:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Xu", "Jiefeng", ""], ["Gul", "Evren", ""], ["Lim", "Alvin", ""]]}, {"id": "2011.04438", "submitter": "Yahui Zhang", "authors": "Yahui Zhang, Joshua Robertson, Shuiying Xiang, Mat\\v{E}J Hejda,\n  Juli\\'An Bueno, and Antonio Hurtado", "title": "All-optical neuromorphic binary convolution with a spiking VCSEL neuron\n  for image gradient magnitudes", "comments": "jxxsy@126.com; antonio.hurtado@strath.ac.uk", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.NE physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-optical binary convolution with a photonic spiking vertical-cavity\nsurface-emitting laser (VCSEL) neuron is proposed and demonstrated\nexperimentally for the first time. Optical inputs, extracted from digital\nimages and temporally encoded using rectangular pulses, are injected in the\nVCSEL neuron which delivers the convolution result in the number of fast (<100\nps long) spikes fired. Experimental and numerical results show that binary\nconvolution is achieved successfully with a single spiking VCSEL neuron and\nthat all-optical binary convolution can be used to calculate image gradient\nmagnitudes to detect edge features and separate vertical and horizontal\ncomponents in source images. We also show that this all-optical spiking binary\nconvolution system is robust to noise and can operate with high-resolution\nimages. Additionally, the proposed system offers important advantages such as\nultrafast speed, high energy efficiency and simple hardware implementation,\nhighlighting the potentials of spiking photonic VCSEL neurons for high-speed\nneuromorphic image processing systems and future photonic spiking convolutional\nneural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:02:43 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhang", "Yahui", ""], ["Robertson", "Joshua", ""], ["Xiang", "Shuiying", ""], ["Hejda", "Mat\u011aJ", ""], ["Bueno", "Juli\u00c1n", ""], ["Hurtado", "Antonio", ""]]}, {"id": "2011.04441", "submitter": "Luka Ribar", "authors": "Luka Ribar, Rodolphe Sepulchre", "title": "Neuromorphic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.NE cs.SY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic engineering is a rapidly developing field that aims to take\ninspiration from the biological organization of neural systems to develop novel\ntechnology for computing, sensing and actuating. The unique properties of such\nsystems call for new signal processing and control paradigms. The article\nintroduces the mixed feedback organization of excitable neuronal systems,\nconsisting of interlocked positive and negative feedback loops acting in\ndistinct timescales. The principles of biological neuromodulation suggest a\nmethodology for designing and controlling mixed-feedback systems\nneuromorphically. The proposed design consists of a parallel interconnection of\nelementary circuit elements that mirrors the organization of biological neurons\nand utilizes the hardware components of neuromorphic electronic circuits. The\ninterconnection structure endows the neuromorphic systems with a simple control\nmethodology that reframes the neuronal control as an input-output shaping\nproblem. The potential of neuronal control is illustrated on simple network\nexamples that suggest the scalability of the mixed-feedback principles.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:06:06 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ribar", "Luka", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "2011.04720", "submitter": "Frithjof Gressmann", "authors": "Frithjof Gressmann, Zach Eaton-Rosen, Carlo Luschi", "title": "Improving Neural Network Training in Low Dimensional Random Bases", "comments": "Published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has proven to be remarkably effective in\noptimizing deep neural networks that employ ever-larger numbers of parameters.\nYet, improving the efficiency of large-scale optimization remains a vital and\nhighly active area of research. Recent work has shown that deep neural networks\ncan be optimized in randomly-projected subspaces of much smaller dimensionality\nthan their native parameter space. While such training is promising for more\nefficient and scalable optimization schemes, its practical application is\nlimited by inferior optimization performance. Here, we improve on recent random\nsubspace approaches as follows: Firstly, we show that keeping the random\nprojection fixed throughout training is detrimental to optimization. We propose\nre-drawing the random subspace at each step, which yields significantly better\nperformance. We realize further improvements by applying independent\nprojections to different parts of the network, making the approximation more\nefficient as network dimensionality grows. To implement these experiments, we\nleverage hardware-accelerated pseudo-random number generation to construct the\nrandom projections on-demand at every optimization step, allowing us to\ndistribute the computation of independent random directions across multiple\nworkers with shared random seeds. This yields significant reductions in memory\nand is up to 10 times faster for the workloads in question.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 19:50:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Gressmann", "Frithjof", ""], ["Eaton-Rosen", "Zach", ""], ["Luschi", "Carlo", ""]]}, {"id": "2011.04975", "submitter": "Michail-Antisthenis Tsompanas", "authors": "Michail-Antisthenis Tsompanas, Larry Bull, Andrew Adamatzky, Igor\n  Balaz", "title": "Evolving Nano Particle Cancer Treatments with Multiple Particle Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms have long been used for optimization problems where\nthe appropriate size of solutions is unclear a priori. The applicability of\nthis methodology is here investigated on the problem of designing a\nnano-particle (NP) based drug delivery system targeting cancer tumours.\nUtilizing a treatment comprising of multiple types of NPs is expected to be\nmore effective due to the higher complexity of the treatment. This paper begins\nby utilizing the well-known NK model to explore the effects of fitness\nlandscape ruggedness upon the evolution of genome length and, hence, solution\ncomplexity. The size of a novel sequence and the absence or presence of\nsequence deletion are also considered. Results show that whilst landscape\nruggedness can alter the dynamics of the process, it does not hinder the\nevolution of genome length. These findings are then explored within the\naforementioned real-world problem. In the first known instance, treatments with\nmultiple types of NPs are used simultaneously, via an agent-based open source\nphysics-based cell simulator. The results suggest that utilizing multiple types\nof NPs is more efficient when the solution space is explored with the\nevolutionary techniques under a predefined computational budget.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 08:46:30 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Tsompanas", "Michail-Antisthenis", ""], ["Bull", "Larry", ""], ["Adamatzky", "Andrew", ""], ["Balaz", "Igor", ""]]}, {"id": "2011.05081", "submitter": "Jonatas Chagas", "authors": "Jonatas B. C. Chagas, Markus Wagner", "title": "A weighted-sum method for solving the bi-objective traveling thief\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world optimization problems have multiple interacting components.\nEach of these can be NP-hard and they can be in conflict with each other, i.e.,\nthe optimal solution for one component does not necessarily represent an\noptimal solution for the other components. This can be a challenge for\nsingle-objective formulations, where the respective influence that each\ncomponent has on the overall solution quality can vary from instance to\ninstance. In this paper, we study a bi-objective formulation of the traveling\nthief problem, which has as components the traveling salesperson problem and\nthe knapsack problem. We present a weighted-sum method that makes use of\nrandomized versions of existing heuristics, and that would have won two recent\noptimization competitions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 13:11:55 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chagas", "Jonatas B. C.", ""], ["Wagner", "Markus", ""]]}, {"id": "2011.05127", "submitter": "Juan F. Hern\\'andez Albarrac\\'in", "authors": "Juan F. H. Albarrac\\'in, Rafael S. Oliveira, Marina Hirota, Jefersson\n  A. dos Santos, Ricardo da S. Torres", "title": "A Soft Computing Approach for Selecting and Combining Spectral Bands", "comments": "MDPI Remote Sensing - Special Issue \"Current Limits and New\n  Challenges and Opportunities in Soft Computing, Machine Learning and\n  Computational Intelligence for Remote Sensing\"", "journal-ref": "Remote Sens. 2020, 12(14), 2267", "doi": "10.3390/rs12142267", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a soft computing approach for automatically selecting and\ncombining indices from remote sensing multispectral images that can be used for\nclassification tasks. The proposed approach is based on a Genetic-Programming\n(GP) framework, a technique successfully used in a wide variety of optimization\nproblems. Through GP, it is possible to learn indices that maximize the\nseparability of samples from two different classes. Once the indices\nspecialized for all the pairs of classes are obtained, they are used in\npixelwise classification tasks. We used the GP-based solution to evaluate\ncomplex classification problems, such as those that are related to the\ndiscrimination of vegetation types within and between tropical biomes. Using\ntime series defined in terms of the learned spectral indices, we show that the\nGP framework leads to superior results than other indices that are used to\ndiscriminate and classify tropical biomes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:51:05 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Albarrac\u00edn", "Juan F. H.", ""], ["Oliveira", "Rafael S.", ""], ["Hirota", "Marina", ""], ["Santos", "Jefersson A. dos", ""], ["Torres", "Ricardo da S.", ""]]}, {"id": "2011.05277", "submitter": "Aymeric Vie", "authors": "Aymeric Vie", "title": "Qualities, challenges and future of genetic algorithms: a literature\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms, computer programs that simulate natural evolution, are\nincreasingly applied across many disciplines. They have been used to solve\nvarious optimisation problems from neural network architecture search to\nstrategic games, and to model phenomena of adaptation and learning. Expertise\non the qualities and drawbacks of this technique is largely scattered across\nthe literature or former, motivating an compilation of this knowledge at the\nlight of the most recent developments of the field. In this review, we present\ngenetic algorithms, their qualities, limitations and challenges, as well as\nsome future development perspectives. Genetic algorithms are capable of\nexploring large and complex spaces of possible solutions, to quickly locate\npromising elements, and provide an adequate modelling tool to describe\nevolutionary systems, from games to economies. They however suffer from high\ncomputation costs, difficult parameter configuration, and crucial\nrepresentation of the solutions. Recent developments such as GPU, parallel and\nquantum computing, conception of powerful parameter control methods, and novel\napproaches in representation strategies, may be keys to overcome those\nlimitations. This compiling review aims at informing practitioners and\nnewcomers in the field alike in their genetic algorithm research, and at\noutlining promising avenues for future research. It highlights the potential\nfor interdisciplinary research associating genetic algorithms to pulse original\ndiscoveries in social sciences, open ended evolution, artificial life and AI.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 17:53:33 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 13:10:16 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vie", "Aymeric", ""]]}, {"id": "2011.05279", "submitter": "Yu Qi", "authors": "Yu Qi, Zhaolan Zheng", "title": "A review of neural network algorithms and their applications in\n  supercritical extraction", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network realizes multi-parameter optimization and control by\nsimulating certain mechanisms of the human brain. It can be used in many fields\nsuch as signal processing, intelligent driving, optimal combination, vehicle\nabnormality detection, and chemical process optimization control. Supercritical\nextraction is a new type of high-efficiency chemical separation process, which\nis mainly used in the separation and purification of natural substances. There\nare many influencing factors. The neural network model can quickly optimize the\nprocess parameters and predict the experimental results under different process\nconditions. It is helpful to understand the inner law of the experiment and\ndetermine the optimal experimental conditions. This paper briefly describes the\nbasic concepts and research progress of neural networks and supercritical\nextraction, and summarizes the application of neural network algorithms in\nsupercritical extraction, aiming to provide reference for the development and\ninnovation of industry technology.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 01:51:02 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Qi", "Yu", ""], ["Zheng", "Zhaolan", ""]]}, {"id": "2011.05280", "submitter": "Hanle Zheng", "authors": "Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu and Guoqi Li", "title": "Going Deeper With Directly-Trained Larger Spiking Neural Networks", "comments": "12 pages, 6 figures, conference or other essential info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are promising in a bio-plausible coding for\nspatio-temporal information and event-driven signal processing, which is very\nsuited for energy-efficient implementation in neuromorphic hardware. However,\nthe unique working mode of SNNs makes them more difficult to train than\ntraditional networks. Currently, there are two main routes to explore the\ntraining of deep SNNs with high performance. The first is to convert a\npre-trained ANN model to its SNN version, which usually requires a long coding\nwindow for convergence and cannot exploit the spatio-temporal features during\ntraining for solving temporal tasks. The other is to directly train SNNs in the\nspatio-temporal domain. But due to the binary spike activity of the firing\nfunction and the problem of gradient vanishing or explosion, current methods\nare restricted to shallow architectures and thereby difficult in harnessing\nlarge-scale datasets (e.g. ImageNet). To this end, we propose a\nthreshold-dependent batch normalization (tdBN) method based on the emerging\nspatio-temporal backpropagation, termed \"STBP-tdBN\", enabling direct training\nof a very deep SNN and the efficient implementation of its inference on\nneuromorphic hardware. With the proposed method and elaborated shortcut\nconnection, we significantly extend directly-trained SNNs from a shallow\nstructure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we\ntheoretically analyze the effectiveness of our method based on \"Block Dynamical\nIsometry\" theory. Finally, we report superior accuracy results including 93.15\n% on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few\ntimesteps. To our best knowledge, it's the first time to explore the\ndirectly-trained deep SNNs with high performance on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 07:15:52 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 06:50:45 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Zheng", "Hanle", ""], ["Wu", "Yujie", ""], ["Deng", "Lei", ""], ["Hu", "Yifan", ""], ["Li", "Guoqi", ""]]}, {"id": "2011.05281", "submitter": "Sidhdharth Sikka", "authors": "Sidhdharth Sikka, Harshvardhan Sikka", "title": "A Genetic Algorithm Based Approach for Satellite Autonomy", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17934.18247/2", "report-no": null, "categories": "cs.NE cs.AI cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Autonomous spacecraft maneuver planning using an evolutionary algorithmic\napproach is investigated. Simulated spacecraft were placed into four different\ninitial orbits. Each was allowed a string of thirty delta-v impulse maneuvers\nin six cartesian directions, the positive and negative x, y and z directions.\nThe goal of the spacecraft maneuver string was to, starting from some non-polar\nstarting orbit, place the spacecraft into a polar, low eccentricity orbit. A\ngenetic algorithm was implemented, using a mating, fitness, mutation and\ncrossover scheme for impulse strings. The genetic algorithm was successfully\nable to produce this result for all the starting orbits. Performance and future\nwork is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:41:30 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 21:47:37 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sikka", "Sidhdharth", ""], ["Sikka", "Harshvardhan", ""]]}, {"id": "2011.05511", "submitter": "Yingtao Luo", "authors": "Yingtao Luo and Xuefeng Zhu", "title": "A Quantum-Inspired Probabilistic Model for the Inverse Design of\n  Meta-Structures", "comments": "Third Workshop on Machine Learning and the Physical Sciences (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE physics.app-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In quantum mechanics, a norm squared wave function can be interpreted as the\nprobability density that describes the likelihood of a particle to be measured\nin a given position or momentum. This statistical property is at the core of\nthe microcosmos. Meanwhile, machine learning inverse design of materials raised\nintensive attention, resulting in various intelligent systems for matter\nengineering. Here, inspired by quantum theory, we propose a probabilistic deep\nlearning paradigm for the inverse design of functional meta-structures. Our\nprobability-density-based neural network (PDN) can accurately capture all\nplausible meta-structures to meet the desired performances. Local maxima in\nprobability density distribution correspond to the most likely candidates. We\nverify this approach by designing multiple meta-structures for each targeted\ntransmission spectrum to enrich design choices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 02:09:10 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Luo", "Yingtao", ""], ["Zhu", "Xuefeng", ""]]}, {"id": "2011.05547", "submitter": "Yusuke Nojima", "authors": "Koen van der Blom, Timo M. Deist, Vanessa Volz, Mariapia Marchi,\n  Yusuke Nojima, Boris Naujoks, Akira Oyama, Tea Tu\\v{s}ar", "title": "Identifying Properties of Real-World Optimisation Problems through a\n  Questionnaire", "comments": "Book Chapter (Under review, revised version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimisation algorithms are commonly compared on benchmarks to get insight\ninto performance differences. However, it is not clear how closely benchmarks\nmatch the properties of real-world problems because these properties are\nlargely unknown. This work investigates the properties of real-world problems\nthrough a questionnaire to enable the design of future benchmark problems that\nmore closely resemble those found in the real world. The results, while not\nrepresentative as they are based on only 45 responses, indicate that many\nproblems possess at least one of the following properties: they are\nconstrained, deterministic, have only continuous variables, require substantial\ncomputation times for both the objectives and the constraints, or allow a\nlimited number of evaluations. Properties like known optimal solutions and\nanalytical gradients are rarely available, limiting the options in guiding the\noptimisation process. These are all important aspects to consider when\ndesigning realistic benchmark problems. At the same time, the design of\nrealistic benchmarks is difficult, because objective functions are often\nreported to be black-box and many problem properties are unknown. To further\nimprove the understanding of real-world problems, readers working on a\nreal-world optimisation problem are encouraged to fill out the questionnaire:\nhttps://tinyurl.com/opt-survey\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:09:01 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 10:03:44 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["van der Blom", "Koen", ""], ["Deist", "Timo M.", ""], ["Volz", "Vanessa", ""], ["Marchi", "Mariapia", ""], ["Nojima", "Yusuke", ""], ["Naujoks", "Boris", ""], ["Oyama", "Akira", ""], ["Tu\u0161ar", "Tea", ""]]}, {"id": "2011.05588", "submitter": "Sergey Yarushev A", "authors": "Alexey Averkin and Sergey Yarushev", "title": "Deep Neural Networks and Neuro-Fuzzy Networks for Intellectual Analysis\n  of Economic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In tis paper we consider approaches for time series forecasting based on deep\nneural networks and neuro-fuzzy nets. Also, we make short review of researches\nin forecasting based on various models of ANFIS models. Deep Learning has\nproven to be an effective method for making highly accurate predictions from\ncomplex data sources. Also, we propose our models of DL and Neuro-Fuzzy\nNetworks for this task. Finally, we show possibility of using these models for\ndata science tasks. This paper presents also an overview of approaches for\nincorporating rule-based methodology into deep learning neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 06:21:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Averkin", "Alexey", ""], ["Yarushev", "Sergey", ""]]}, {"id": "2011.05605", "submitter": "Tanmay Samak", "authors": "Sivanathan Kandhasamy, Vinayagam Babu Kuppusamy, Tanmay Vilas Samak,\n  Chinmay Vilas Samak", "title": "Decentralized Motion Planning for Multi-Robot Navigation using Deep\n  Reinforcement Learning", "comments": "Accepted at IEEE International Conference on Intelligent Sustainable\n  Systems (ICISS) 2020", "journal-ref": "2020 3rd International Conference on Intelligent Sustainable\n  Systems (ICISS), Thoothukudi, India, 2020, pp. 709-716", "doi": "10.1109/ICISS49785.2020.9316033", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a decentralized motion planning framework for addressing\nthe task of multi-robot navigation using deep reinforcement learning. A custom\nsimulator was developed in order to experimentally investigate the navigation\nproblem of 4 cooperative non-holonomic robots sharing limited state information\nwith each other in 3 different settings. The notion of decentralized motion\nplanning with common and shared policy learning was adopted, which allowed\nrobust training and testing of this approach in a stochastic environment since\nthe agents were mutually independent and exhibited asynchronous motion\nbehavior. The task was further aggravated by providing the agents with a sparse\nobservation space and requiring them to generate continuous action commands so\nas to efficiently, yet safely navigate to their respective goal locations,\nwhile avoiding collisions with other dynamic peers and static obstacles at all\ntimes. The experimental results are reported in terms of quantitative measures\nand qualitative remarks for both training and deployment phases.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:35:21 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 18:19:32 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kandhasamy", "Sivanathan", ""], ["Kuppusamy", "Vinayagam Babu", ""], ["Samak", "Tanmay Vilas", ""], ["Samak", "Chinmay Vilas", ""]]}, {"id": "2011.05623", "submitter": "Li Yuan", "authors": "Li Yuan, Will Xiao, Gabriel Kreiman, Francis E.H. Tay, Jiashi Feng,\n  Margaret S. Livingstone", "title": "Adversarial images for the primate brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks have been proposed as a model of primate\nvision. However, these networks are vulnerable to adversarial attacks, whereby\nintroducing minimal noise can fool networks into misclassifying images. Primate\nvision is thought to be robust to such adversarial images. We evaluated this\nassumption by designing adversarial images to fool primate vision. To do so, we\nfirst trained a model to predict responses of face-selective neurons in macaque\ninferior temporal cortex. Next, we modified images, such as human faces, to\nmatch their model-predicted neuronal responses to a target category, such as\nmonkey faces. These adversarial images elicited neuronal responses similar to\nthe target category. Remarkably, the same images fooled monkeys and humans at\nthe behavioral level. These results challenge fundamental assumptions about the\nsimilarity between computer and primate vision and show that a model of\nneuronal activity can selectively direct primate visual behavior.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:30:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Yuan", "Li", ""], ["Xiao", "Will", ""], ["Kreiman", "Gabriel", ""], ["Tay", "Francis E. H.", ""], ["Feng", "Jiashi", ""], ["Livingstone", "Margaret S.", ""]]}, {"id": "2011.05700", "submitter": "Farhad Pourpanah Dr.", "authors": "Farhad Pourpanah and Ran Wang and Chee Peng Lim and Danial Yazdani", "title": "A Review of the Family of Artificial Fish Swarm Algorithms: Recent\n  Advances and Applications", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Artificial Fish Swarm Algorithm (AFSA) is inspired by the ecological\nbehaviors of fish schooling in nature, viz., the preying, swarming, following\nand random behaviors. Owing to a number of salient properties, which include\nflexibility, fast convergence, and insensitivity to the initial parameter\nsettings, the family of AFSA has emerged as an effective Swarm Intelligence\n(SI) methodology that has been widely applied to solve real-world optimization\nproblems. Since its introduction in 2002, many improved and hybrid AFSA models\nhave been developed to tackle continuous, binary, and combinatorial\noptimization problems. This paper aims to present a concise review of the\nfamily of AFSA, encompassing the original ASFA and its improvements,\ncontinuous, binary, discrete, and hybrid models, as well as the associated\napplications. A comprehensive survey on the AFSA from its introduction to 2012\ncan be found in [1]. As such, we focus on a total of {\\color{blue}123} articles\npublished in high-quality journals since 2013. We also discuss possible AFSA\nenhancements and highlight future research directions for the family of\nAFSA-based models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:10:45 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Pourpanah", "Farhad", ""], ["Wang", "Ran", ""], ["Lim", "Chee Peng", ""], ["Yazdani", "Danial", ""]]}, {"id": "2011.05844", "submitter": "Arash Broumand", "authors": "Arash Broumand", "title": "Application of Compromising Evolution in Multi-objective Image Error\n  Concealment", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous multi-objective optimization problems encounter with a number of\nfitness functions to be simultaneously optimized of which their mutual\npreferences are not inherently known. Suffering from the lack of underlying\ngenerative models, the existing convex optimization approaches may fail to\nderive the Pareto optimal solution for those problems in complicated domains\nsuch as image enhancement. In order to obviate such shortcomings, the\nCompromising Evolution Method is proposed in this report to modify the Simple\nGenetic Algorithm by utilizing the notion of compromise. The simulation results\nshow the power of the proposed method solving multi-objective optimizations in\na case study of image error concealment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 15:22:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Broumand", "Arash", ""]]}, {"id": "2011.05847", "submitter": "Florent Forest", "authors": "Florent Forest, Mustapha Lebbah, Hanane Azzag, J\\'er\\^ome Lacaille", "title": "A Survey and Implementation of Performance Metrics for Self-Organized\n  Maps", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Self-Organizing Map algorithms have been used for almost 40 years across\nvarious application domains such as biology, geology, healthcare, industry and\nhumanities as an interpretable tool to explore, cluster and visualize\nhigh-dimensional data sets. In every application, practitioners need to know\nwhether they can \\textit{trust} the resulting mapping, and perform model\nselection to tune algorithm parameters (e.g. the map size). Quantitative\nevaluation of self-organizing maps (SOM) is a subset of clustering validation,\nwhich is a challenging problem as such. Clustering model selection is typically\nachieved by using clustering validity indices. While they also apply to\nself-organized clustering models, they ignore the topology of the map, only\nanswering the question: do the SOM code vectors approximate well the data\ndistribution? Evaluating SOM models brings in the additional challenge of\nassessing their topology: does the mapping preserve neighborhood relationships\nbetween the map and the original data? The problem of assessing the performance\nof SOM models has already been tackled quite thoroughly in literature, giving\nbirth to a family of quality indices incorporating neighborhood constraints,\ncalled \\textit{topographic} indices. Commonly used examples of such metrics are\nthe topographic error, neighborhood preservation or the topographic product.\nHowever, open-source implementations are almost impossible to find. This is the\nissue we try to solve in this work: after a survey of existing SOM performance\nmetrics, we implemented them in Python and widely used numerical libraries, and\nprovide them as an open-source library, SOMperf. This paper introduces each\nmetric available in our module along with usage examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 15:28:33 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Forest", "Florent", ""], ["Lebbah", "Mustapha", ""], ["Azzag", "Hanane", ""], ["Lacaille", "J\u00e9r\u00f4me", ""]]}, {"id": "2011.05987", "submitter": "Aaron Tuor", "authors": "Jan Drgona, Aaron R. Tuor, Vikas Chandan and Draguna L. Vrabie", "title": "Physics-constrained Deep Learning of Multi-zone Building Thermal\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a physics-constrained control-oriented deep learning method for\nmodeling building thermal dynamics. The proposed method is based on the\nsystematic encoding of physics-based prior knowledge into a structured\nrecurrent neural architecture. Specifically, our method incorporates structural\npriors from traditional physics-based building modeling into the neural network\nthermal dynamics model structure. Further, we leverage penalty methods to\nprovide inequality constraints, thereby bounding predictions within physically\nrealistic and safe operating ranges. Observing that stable eigenvalues\naccurately characterize the dissipativeness of the system, we additionally use\na constrained matrix parameterization based on the Perron-Frobenius theorem to\nbound the dominant eigenvalues of the building thermal model parameter\nmatrices. We demonstrate the proposed data-driven modeling approach's\neffectiveness and physical interpretability on a dataset obtained from a\nreal-world office building with 20 thermal zones. Using only 10 days'\nmeasurements for training, we demonstrate generalization over 20 consecutive\ndays, significantly improving the accuracy compared to prior state-of-the-art\nresults reported in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 06:39:14 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Drgona", "Jan", ""], ["Tuor", "Aaron R.", ""], ["Chandan", "Vikas", ""], ["Vrabie", "Draguna L.", ""]]}, {"id": "2011.06064", "submitter": "Nils M\\\"uller", "authors": "Nils M\\\"uller and Tobias Glasmachers", "title": "Non-local Optimization: Imposing Structure on Optimization Problems by\n  Relaxation", "comments": "19 pages, 1 figure, final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In stochastic optimization, particularly in evolutionary computation and\nreinforcement learning, the optimization of a function $f: \\Omega \\to\n\\mathbb{R}$ is often addressed through optimizing a so-called relaxation\n$\\theta \\in \\Theta \\mapsto \\mathbb{E}_\\theta(f)$ of $f$, where $\\Theta$\nresembles the parameters of a family of probability measures on $\\Omega$. We\ninvestigate the structure of such relaxations by means of measure theory and\nFourier analysis, enabling us to shed light on the success of many associated\nstochastic optimization methods. The main structural traits we derive and that\nallow fast and reliable optimization of relaxations are the consistency of\noptimal values of $f$, Lipschitzness of gradients, and convexity. We emphasize\nsettings where $f$ itself is not differentiable or convex, e.g., in the\npresence of (stochastic) disturbance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:45:47 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 15:27:50 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 13:20:12 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["M\u00fcller", "Nils", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "2011.06075", "submitter": "Joseph Friedman", "authors": "Wesley H. Brigner, Naimul Hassan, Xuan Hu, Christopher H. Bennett,\n  Felipe Garcia-Sanchez, Can Cui, Alvaro Velasquez, Matthew J. Marinella, Jean\n  Anne C. Incorvia, Joseph S. Friedman", "title": "Domain Wall Leaky Integrate-and-Fire Neurons with Shape-Based\n  Configurable Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.mes-hall cs.ET physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementary metal oxide semiconductor (CMOS) devices display volatile\ncharacteristics, and are not well suited for analog applications such as\nneuromorphic computing. Spintronic devices, on the other hand, exhibit both\nnon-volatile and analog features, which are well-suited to neuromorphic\ncomputing. Consequently, these novel devices are at the forefront of\nbeyond-CMOS artificial intelligence applications. However, a large quantity of\nthese artificial neuromorphic devices still require the use of CMOS, which\ndecreases the efficiency of the system. To resolve this, we have previously\nproposed a number of artificial neurons and synapses that do not require CMOS\nfor operation. Although these devices are a significant improvement over\nprevious renditions, their ability to enable neural network learning and\nrecognition is limited by their intrinsic activation functions. This work\nproposes modifications to these spintronic neurons that enable configuration of\nthe activation functions through control of the shape of a magnetic domain wall\ntrack. Linear and sigmoidal activation functions are demonstrated in this work,\nwhich can be extended through a similar approach to enable a wide variety of\nactivation functions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 21:07:02 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Brigner", "Wesley H.", ""], ["Hassan", "Naimul", ""], ["Hu", "Xuan", ""], ["Bennett", "Christopher H.", ""], ["Garcia-Sanchez", "Felipe", ""], ["Cui", "Can", ""], ["Velasquez", "Alvaro", ""], ["Marinella", "Matthew J.", ""], ["Incorvia", "Jean Anne C.", ""], ["Friedman", "Joseph S.", ""]]}, {"id": "2011.06188", "submitter": "Michal Lisicki", "authors": "Michal Lisicki, Arash Afkanpour, Graham W. Taylor", "title": "Evaluating Curriculum Learning Strategies in Neural Combinatorial\n  Optimization", "comments": "Presented at Workshop on Learning Meets Combinatorial Algorithms at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural combinatorial optimization (NCO) aims at designing problem-independent\nand efficient neural network-based strategies for solving combinatorial\nproblems. The field recently experienced growth by successfully adapting\narchitectures originally designed for machine translation. Even though the\nresults are promising, a large gap still exists between NCO models and classic\ndeterministic solvers, both in terms of accuracy and efficiency. One of the\ndrawbacks of current approaches is the inefficiency of training on multiple\nproblem sizes. Curriculum learning strategies have been shown helpful in\nincreasing performance in the multi-task setting. In this work, we focus on\ndesigning a curriculum learning-based training procedure that can help existing\narchitectures achieve competitive performance on a large range of problem sizes\nsimultaneously. We provide a systematic investigation of several training\nprocedures and use the insights gained to motivate application of a\npsychologically-inspired approach to improve upon the classic curriculum\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 04:21:04 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lisicki", "Michal", ""], ["Afkanpour", "Arash", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2011.06331", "submitter": "Shengcai Liu", "authors": "Shengcai Liu, Ke Tang and Xin Yao", "title": "Memetic Search for Vehicle Routing with Simultaneous Pickup-Delivery and\n  Time Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vehicle Routing Problem with Simultaneous Pickup-Delivery and Time\nWindows (VRPSPDTW) has attracted much research interest in the last decade, due\nto its wide application in modern logistics. Since VRPSPDTW is NP-hard and\nexact methods are only applicable to small-scale instances, heuristics and\nmeta-heuristics are commonly adopted. In this paper we propose a novel Memetic\nAlgorithm with efficient local search and extended neighborhood, dubbed MATE,\nto solve this problem. Compared to existing algorithms, the advantages of MATE\nlie in two aspects. First, it is capable of more effectively exploring the\nsearch space, due to its novel initialization procedure, crossover and\nlarge-step-size operators. Second, it is also more efficient in local\nexploitation, due to its sophisticated constant-time-complexity move evaluation\nmechanism. Experimental results on public benchmarks show that MATE outperforms\nall the state-of-the-art algorithms, and notably, finds new best-known\nsolutions on 12 instances (65 instances in total). Moreover, a comprehensive\nablation study is also conducted to show the effectiveness of the novel\ncomponents integrated in MATE. Finally, a new benchmark of large-scale\ninstances, derived from a real-world application of the JD logistics, is\nintroduced, which can serve as a new and more challenging test set for future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:06:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:01:13 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 02:30:10 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 13:22:04 GMT"}, {"version": "v5", "created": "Tue, 8 Jun 2021 08:24:45 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Shengcai", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""]]}, {"id": "2011.06388", "submitter": "Do Gyun Kim", "authors": "Do Gyun Kim, Jin Young Choi", "title": "An ensemble of Density based Geometric One-Class Classifier and Genetic\n  Algorithm", "comments": "This manuscript contains a wrong definitions and equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most rising issues in recent machine learning research is\nOne-Class Classification which considers data set composed of only one class\nand outliers. It is more reasonable than traditional Multi-Class Classification\nin dealing with some problematic data set or special cases. Generally,\nclassification accuracy and interpretability for user are considered as\ntrade-off in OCC methods. Classifier based on Hyper-Rectangle (H-RTGL) is a\nsort of classifier that can be a remedy for such trade-off and uses H-RTGL\nformulated by conjunction of geometric rules called interval. This interval can\nbe basis of interpretability since it can be easily understood by user.\nHowever, existing H-RTGL based OCC classifiers have limitations that (i) most\nof them cannot reflect density of target class and (ii) that considering\ndensity has primitive interval generation method, and (iii) there exists no\nsystematic procedure for hyperparameter of H-RTGL based OCC classifier, which\ninfluences classification performance of classifier. Based on these remarks, we\nsuggest One-Class Hyper-Rectangle Descriptor based on density (1-HRD_d) with\nmore elaborate interval generation method including parametric and\nnonparametric approaches. In addition, we designed Genetic Algorithm (GA) that\nconsists of chromosome structure and genetic operators for systematic\ngeneration of 1-HRD_d by optimization of hyperparameter. Our work is validated\nthrough a numerical experiment using actual data set with comparison of\nexisting OCC algorithms along with other H-RTGL based classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 04:22:03 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 07:29:19 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Kim", "Do Gyun", ""], ["Choi", "Jin Young", ""]]}, {"id": "2011.06427", "submitter": "Sadra Rahimi Kari", "authors": "S. Rahimi Kari", "title": "Realization of Stochastic Neural Networks and Its Potential Applications", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.NI eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successive Cancellation Decoders have come a long way since the\nimplementation of traditional SC decoders, but there still is a potential for\nimprovement. The main struggle over the years was to find an optimal algorithm\nto implement them. Most of the proposed algorithms are not practical enough to\nbe implemented in real-life. In this research, we aim to introduce the\nEfficiency of stochastic neural networks as an SC decoder and Find the possible\nways of improving its performance and practicality. In this paper, after a\nbrief introduction to stochastic neurons and SNNs, we introduce methods to\nrealize Stochastic NNs on both deterministic and stochastic platforms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:01:07 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kari", "S. Rahimi", ""]]}, {"id": "2011.06551", "submitter": "Massimiliano Di Ventra", "authors": "S.R.B. Bearden, Y.R. Pei, M. Di Ventra", "title": "Efficient Solution of Boolean Satisfiability Problems with Digital\n  MemComputing", "comments": null, "journal-ref": "Scientific Reports 10, 19741 (2020)", "doi": "10.1038/s41598-020-76666-2", "report-no": null, "categories": "cs.ET cond-mat.stat-mech cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean satisfiability is a propositional logic problem of interest in\nmultiple fields, e.g., physics, mathematics, and computer science. Beyond a\nfield of research, instances of the SAT problem, as it is known, require\nefficient solution methods in a variety of applications. It is the decision\nproblem of determining whether a Boolean formula has a satisfying assignment,\nbelieved to require exponentially growing time for an algorithm to solve for\nthe worst-case instances. Yet, the efficient solution of many classes of\nBoolean formulae eludes even the most successful algorithms, not only for the\nworst-case scenarios, but also for typical-case instances. Here, we introduce a\nmemory-assisted physical system (a digital memcomputing machine) that, when its\nnon-linear ordinary differential equations are integrated numerically, shows\nevidence for polynomially-bounded scalability while solving \"hard\"\nplanted-solution instances of SAT, known to require exponential time to solve\nin the typical case for both complete and incomplete algorithms. Furthermore,\nwe analytically demonstrate that the physical system can efficiently solve the\nSAT problem in continuous time, without the need to introduce chaos or an\nexponentially growing energy. The efficiency of the simulations is related to\nthe collective dynamical properties of the original physical system that\npersist in the numerical integration to robustly guide the solution search even\nin the presence of numerical errors. We anticipate our results to broaden\nresearch directions in physics-inspired computing paradigms ranging from theory\nto application, from simulation to hardware implementation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:13:46 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Bearden", "S. R. B.", ""], ["Pei", "Y. R.", ""], ["Di Ventra", "M.", ""]]}, {"id": "2011.06673", "submitter": "Maysum Panju", "authors": "Maysum Panju, Kourosh Parand, Ali Ghodsi", "title": "Symbolically Solving Partial Differential Equations using Deep Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a neural-based method for generating exact or approximate\nsolutions to differential equations in the form of mathematical expressions.\nUnlike other neural methods, our system returns symbolic expressions that can\nbe interpreted directly. Our method uses a neural architecture for learning\nmathematical expressions to optimize a customizable objective, and is scalable,\ncompact, and easily adaptable for a variety of tasks and configurations. The\nsystem has been shown to effectively find exact or approximate symbolic\nsolutions to various differential equations with applications in natural\nsciences. In this work, we highlight how our method applies to partial\ndifferential equations over multiple variables and more complex boundary and\ninitial value conditions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 22:16:03 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Panju", "Maysum", ""], ["Parand", "Kourosh", ""], ["Ghodsi", "Ali", ""]]}, {"id": "2011.06752", "submitter": "Jiajun Fan", "authors": "Jiajun Fan, He Ba, Xian Guo, Jianye Hao", "title": "Critic PI2: Master Continuous Planning via Policy Improvement with Path\n  Integrals and Deep Actor-Critic Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing agents with planning capabilities has long been one of the main\nchallenges in the pursuit of artificial intelligence. Tree-based planning\nmethods from AlphaGo to Muzero have enjoyed huge success in discrete domains,\nsuch as chess and Go. Unfortunately, in real-world applications like robot\ncontrol and inverted pendulum, whose action space is normally continuous, those\ntree-based planning techniques will be struggling. To address those\nlimitations, in this paper, we present a novel model-based reinforcement\nlearning frameworks called Critic PI2, which combines the benefits from\ntrajectory optimization, deep actor-critic learning, and model-based\nreinforcement learning. Our method is evaluated for inverted pendulum models\nwith applicability to many continuous control systems. Extensive experiments\ndemonstrate that Critic PI2 achieved a new state of the art in a range of\nchallenging continuous domains. Furthermore, we show that planning with a\ncritic significantly increases the sample efficiency and real-time performance.\nOur work opens a new direction toward learning the components of a model-based\nplanning system and how to use them.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 04:14:40 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fan", "Jiajun", ""], ["Ba", "He", ""], ["Guo", "Xian", ""], ["Hao", "Jianye", ""]]}, {"id": "2011.06811", "submitter": "Rasmus Berg Palm", "authors": "Rasmus Berg Palm, Elias Najarro, Sebastian Risi", "title": "Testing the Genomic Bottleneck Hypothesis in Hebbian Meta-Learning", "comments": "JMLR 148, NeurIPS pre-registration workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hebbian meta-learning has recently shown promise to solve hard reinforcement\nlearning problems, allowing agents to adapt to some degree to changes in the\nenvironment. However, because each synapse in these approaches can learn a very\nspecific learning rule, the ability to generalize to very different situations\nis likely reduced. We hypothesize that limiting the number of Hebbian learning\nrules through a \"genomic bottleneck\" can act as a regularizer leading to better\ngeneralization across changes to the environment. We test this hypothesis by\ndecoupling the number of Hebbian learning rules from the number of synapses and\nsystematically varying the number of Hebbian learning rules. The results in\nthis paper suggest that simultaneously learning the Hebbian learning rules and\ntheir assignment to synapses is a difficult optimization problem, leading to\npoor performance in the environments tested. However, parallel research to ours\nfinds that it is indeed possible to reduce the number of learning rules by\nclustering similar rules together. How to best implement a \"genomic bottleneck\"\nalgorithm is thus an important research direction that warrants further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 08:49:15 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:14:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Palm", "Rasmus Berg", ""], ["Najarro", "Elias", ""], ["Risi", "Sebastian", ""]]}, {"id": "2011.06913", "submitter": "Paul Dufoss\\'e", "authors": "Paul Dufoss\\'e and Cyrille Enderli", "title": "Finding optimal Pulse Repetion Intervals with Many-objective\n  Evolutionary Algorithms", "comments": "5 pages, 2 tables, 2 figures, submitted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of finding Pulse Repetition Intervals\nallowing the best compromises mitigating range and Doppler ambiguities in a\nPulsed-Doppler radar system. We revisit a problem that was proposed to the\nEvolutionary Computation community as a real-world case to test Many-objective\nOptimization algorithms. We use it as a baseline to compare several\nEvolutionary Algorithms for black-box optimization with different metrics.\nResulting data is aggregated to build a reference set of Pareto optimal points\nand is the starting point for further analysis and operational use by the radar\ndesigner.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 13:56:13 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:49:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Dufoss\u00e9", "Paul", ""], ["Enderli", "Cyrille", ""]]}, {"id": "2011.06923", "submitter": "Richard Schoonhoven", "authors": "Richard Schoonhoven, Allard A. Hendriksen, Dani\\\"el M. Pelt, K. Joost\n  Batenburg", "title": "LEAN: graph-based pruning for convolutional neural networks by\n  extracting longest chains", "comments": "8 pages + 2 pages references. Code will be made public via GitHub\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have proven to be highly successful at a\nrange of image-to-image tasks. CNNs can be computationally expensive, which can\nlimit their applicability in practice. Model pruning can improve computational\nefficiency by sparsifying trained networks. Common methods for pruning CNNs\ndetermine what convolutional filters to remove by ranking filters on an\nindividual basis. However, filters are not independent, as CNNs consist of\nchains of convolutions, which can result in sub-optimal filter selection.\n  We propose a novel pruning method, LongEst-chAiN (LEAN) pruning, which takes\nthe interdependency between the convolution operations into account. We propose\nto prune CNNs by using graph-based algorithms to select relevant chains of\nconvolutions. A CNN is interpreted as a graph, with the operator norm of each\nconvolution as distance metric for the edges. LEAN pruning iteratively extracts\nthe highest value path from the graph to keep. In our experiments, we test LEAN\npruning for several image-to-image tasks, including the well-known CamVid\ndataset. LEAN pruning enables us to keep just 0.5%-2% of the convolutions\nwithout significant loss of accuracy. When pruning CNNs with LEAN, we achieve a\nhigher accuracy than pruning filters individually, and different pruned\nsubstructures emerge.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:17:51 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Schoonhoven", "Richard", ""], ["Hendriksen", "Allard A.", ""], ["Pelt", "Dani\u00ebl M.", ""], ["Batenburg", "K. Joost", ""]]}, {"id": "2011.07248", "submitter": "Thomas Keller", "authors": "T. Anderson Keller, Jorn W.T. Peters, Priyank Jaini, Emiel Hoogeboom,\n  Patrick Forr\\'e, Max Welling", "title": "Self Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient gradient computation of the Jacobian determinant term is a core\nproblem in many machine learning settings, and especially so in the normalizing\nflow framework. Most proposed flow models therefore either restrict to a\nfunction class with easy evaluation of the Jacobian determinant, or an\nefficient estimator thereof. However, these restrictions limit the performance\nof such density models, frequently requiring significant depth to reach desired\nperformance levels. In this work, we propose Self Normalizing Flows, a flexible\nframework for training normalizing flows by replacing expensive terms in the\ngradient by learned approximate inverses at each layer. This reduces the\ncomputational complexity of each layer's exact update from $\\mathcal{O}(D^3)$\nto $\\mathcal{O}(D^2)$, allowing for the training of flow architectures which\nwere otherwise computationally infeasible, while also providing efficient\nsampling. We show experimentally that such models are remarkably stable and\noptimize to similar data likelihood values as their exact gradient\ncounterparts, while training more quickly and surpassing the performance of\nfunctionally constrained counterparts.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 09:51:51 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 12:14:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Keller", "T. Anderson", ""], ["Peters", "Jorn W. T.", ""], ["Jaini", "Priyank", ""], ["Hoogeboom", "Emiel", ""], ["Forr\u00e9", "Patrick", ""], ["Welling", "Max", ""]]}, {"id": "2011.07334", "submitter": "Eli Moore", "authors": "Eli Moore and Rishidev Chaudhuri", "title": "Using noise to probe recurrent neural network structure and prune\n  synapses", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many networks in the brain are sparsely connected, and the brain eliminates\nsynapses during development and learning. How could the brain decide which\nsynapses to prune? In a recurrent network, determining the importance of a\nsynapse between two neurons is a difficult computational problem, depending on\nthe role that both neurons play and on all possible pathways of information\nflow between them. Noise is ubiquitous in neural systems, and often considered\nan irritant to be overcome. Here we suggest that noise could play a functional\nrole in synaptic pruning, allowing the brain to probe network structure and\ndetermine which synapses are redundant. We construct a simple, local,\nunsupervised plasticity rule that either strengthens or prunes synapses using\nonly synaptic weight and the noise-driven covariance of the neighboring\nneurons. For a subset of linear and rectified-linear networks, we prove that\nthis rule preserves the spectrum of the original matrix and hence preserves\nnetwork dynamics even when the fraction of pruned synapses asymptotically\napproaches 1. The plasticity rule is biologically-plausible and may suggest a\nnew role for noise in neural computation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 16:51:05 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 18:07:01 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Moore", "Eli", ""], ["Chaudhuri", "Rishidev", ""]]}, {"id": "2011.07393", "submitter": "David Moss", "authors": "Xingyuan Xu, Mengxi Tan, Bill Corcoran, Jiayang Wu, Andreas Boes,\n  Thach G. Nguyen, Sai T. Chu, Brent E. Little, Damien G. Hicks, Roberto\n  Morandotti, Arnan Mitchell, and David J. Moss", "title": "11 TeraFLOPs per second photonic convolutional accelerator for deep\n  learning optical neural networks", "comments": "21 pages, 9 figures, 39 references", "journal-ref": "Nature, Volume 589 Issue 7840. pages 44-51 (2021)", "doi": "10.1038/s41586-020-03063-0", "report-no": null, "categories": "cs.NE physics.app-ph physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs), inspired by biological visual cortex\nsystems, are a powerful category of artificial neural networks that can extract\nthe hierarchical features of raw data to greatly reduce the network parametric\ncomplexity and enhance the predicting accuracy. They are of significant\ninterest for machine learning tasks such as computer vision, speech\nrecognition, playing board games and medical diagnosis. Optical neural networks\noffer the promise of dramatically accelerating computing speed to overcome the\ninherent bandwidth bottleneck of electronics. Here, we demonstrate a universal\noptical vector convolutional accelerator operating beyond 10 TeraFLOPS\n(floating point operations per second), generating convolutions of images of\n250,000 pixels with 8 bit resolution for 10 kernels simultaneously, enough for\nfacial image recognition. We then use the same hardware to sequentially form a\ndeep optical CNN with ten output neurons, achieving successful recognition of\nfull 10 digits with 900 pixel handwritten digit images with 88% accuracy. Our\nresults are based on simultaneously interleaving temporal, wavelength and\nspatial dimensions enabled by an integrated microcomb source. This approach is\nscalable and trainable to much more complex networks for demanding applications\nsuch as unmanned vehicle and real-time video recognition.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:24:01 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Xu", "Xingyuan", ""], ["Tan", "Mengxi", ""], ["Corcoran", "Bill", ""], ["Wu", "Jiayang", ""], ["Boes", "Andreas", ""], ["Nguyen", "Thach G.", ""], ["Chu", "Sai T.", ""], ["Little", "Brent E.", ""], ["Hicks", "Damien G.", ""], ["Morandotti", "Roberto", ""], ["Mitchell", "Arnan", ""], ["Moss", "David J.", ""]]}, {"id": "2011.07407", "submitter": "Daniel Lengyel", "authors": "Daniel Lengyel, Janith Petangoda, Isak Falk, Kate Highnam, Michalis\n  Lazarou, Arinbj\\\"orn Kolbeinsson, Marc Peter Deisenroth, Nicholas R. Jennings", "title": "GENNI: Visualising the Geometry of Equivalences for Neural Network\n  Identifiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm to visualise symmetries in neural networks.\nTypically, models are defined with respect to a parameter space, where\nnon-equal parameters can produce the same input-output map. Our proposed\nmethod, GENNI, allows us to efficiently identify parameters that are\nfunctionally equivalent and then visualise the subspace of the resulting\nequivalence class. By doing so, we are now able to better explore questions\nsurrounding identifiability, with applications to optimisation and\ngeneralizability, for commonly used or newly developed neural network\narchitectures.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 22:53:13 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Lengyel", "Daniel", ""], ["Petangoda", "Janith", ""], ["Falk", "Isak", ""], ["Highnam", "Kate", ""], ["Lazarou", "Michalis", ""], ["Kolbeinsson", "Arinbj\u00f6rn", ""], ["Deisenroth", "Marc Peter", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "2011.07464", "submitter": "Joseph Marino", "authors": "Joseph Marino", "title": "Predictive Coding, Variational Autoencoders, and Biological Connections", "comments": "NeurIPS NeuroAI Workshop, NAISys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper identifies connections between predictive coding, from theoretical\nneuroscience, and variational autoencoders, from machine learning. These\nconnections imply striking correspondences for biological neural circuits,\nsuggesting that pyramidal dendrites are functionally analogous to non-linear\ndeep networks and lateral inhibition is functionally analogous to normalizing\nflows. Connecting these areas provides new directions for further\ninvestigations.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 07:15:18 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Marino", "Joseph", ""]]}, {"id": "2011.07661", "submitter": "Luca Parisi", "authors": "Luca Parisi, Renfei Ma, Narrendar RaviChandran and Matteo Lanzillotta", "title": "hyper-sinh: An Accurate and Reliable Function from Shallow to Deep\n  Learning in TensorFlow and Keras", "comments": "19 pages, 6 listings/Python code snippets, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation\nfunction suitable for Deep Learning (DL)-based algorithms for supervised\nlearning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in\nthe open source Python libraries TensorFlow and Keras, is thus described and\nvalidated as an accurate and reliable activation function for both shallow and\ndeep neural networks. Improvements in accuracy and reliability in image and\ntext classification tasks on five (N = 5) benchmark data sets available from\nKeras are discussed. Experimental results demonstrate the overall competitive\nclassification performance of both shallow and deep neural networks, obtained\nvia this novel function. This function is evaluated with respect to gold\nstandard activation functions, demonstrating its overall competitive accuracy\nand reliability for both image and text classification.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:38:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Parisi", "Luca", ""], ["Ma", "Renfei", ""], ["RaviChandran", "Narrendar", ""], ["Lanzillotta", "Matteo", ""]]}, {"id": "2011.07727", "submitter": "Youngsoo Choi", "authors": "Youngkyu Kim and Youngsoo Choi and David Widemann and Tarek Zohdi", "title": "Efficient nonlinear manifold reduced order model", "comments": "10 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:2009.11990", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional linear subspace reduced order models (LS-ROMs) are able to\naccelerate physical simulations, in which the intrinsic solution space falls\ninto a subspace with a small dimension, i.e., the solution space has a small\nKolmogorov n-width. However, for physical phenomena not of this type, such as\nadvection-dominated flow phenomena, a low-dimensional linear subspace poorly\napproximates the solution. To address cases such as these, we have developed an\nefficient nonlinear manifold ROM (NM-ROM), which can better approximate\nhigh-fidelity model solutions with a smaller latent space dimension than the\nLS-ROMs. Our method takes advantage of the existing numerical methods that are\nused to solve the corresponding full order models (FOMs). The efficiency is\nachieved by developing a hyper-reduction technique in the context of the\nNM-ROM. Numerical results show that neural networks can learn a more efficient\nlatent space representation on advection-dominated data from 2D Burgers'\nequations with a high Reynolds number. A speed-up of up to 11.7 for 2D Burgers'\nequations is achieved with an appropriate treatment of the nonlinear terms\nthrough a hyper-reduction technique.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:46:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Youngkyu", ""], ["Choi", "Youngsoo", ""], ["Widemann", "David", ""], ["Zohdi", "Tarek", ""]]}, {"id": "2011.07831", "submitter": "Imanol Schlag", "authors": "Imanol Schlag, Tsendsuren Munkhdalai, J\\\"urgen Schmidhuber", "title": "Learning Associative Inference Using Fast Weight Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans can quickly associate stimuli to solve problems in novel contexts. Our\nnovel neural network model learns state representations of facts that can be\ncomposed to perform such associative inference. To this end, we augment the\nLSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through\ndifferentiable operations at every step of a given input sequence, the LSTM\nupdates and maintains compositional associations stored in the rapidly changing\nFWM weights. Our model is trained end-to-end by gradient descent and yields\nexcellent performance on compositional language reasoning problems,\nmeta-reinforcement-learning for POMDPs, and small-scale word-level language\nmodelling.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 10:01:23 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 17:00:19 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Schlag", "Imanol", ""], ["Munkhdalai", "Tsendsuren", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2011.08184", "submitter": "Jan Egger", "authors": "Jan Egger, Antonio Pepe, Christina Gsaxner, Jianning Li", "title": "Deep Learning -- A first Meta-Survey of selected Reviews across\n  Scientific Disciplines and their Research Impact", "comments": "39 pages, 5 tables, 80 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning belongs to the field of artificial intelligence, where machines\nperform tasks that typically require some kind of human intelligence. Deep\nlearning tries to achieve this by mimicking the learning of a human brain.\nSimilar to the basic structure of a brain, which consists of (billions of)\nneurons and connections between them, a deep learning algorithm consists of an\nartificial neural network, which resembles the biological brain structure.\nMimicking the learning process of humans with their senses, deep learning\nnetworks are fed with (sensory) data, like texts, images, videos or sounds.\nThese networks outperform the state-of-the-art methods in different tasks and,\nbecause of this, the whole field saw an exponential growth during the last\nyears. This growth resulted in way over 10 000 publications per year in the\nlast years. For example, the search engine PubMed alone, which covers only a\nsub-set of all publications in the medical field, provides over 11 000 results\nfor the search term $'$deep learning$'$ in Q3 2020, and ~90% of these results\nare from the last three years. Consequently, a complete overview over the field\nof deep learning is already impossible to obtain and, in the near future, it\nwill potentially become difficult to obtain an overview over a subfield.\nHowever, there are several review articles about deep learning, which are\nfocused on specific scientific fields or applications, for example deep\nlearning advances in computer vision or in specific tasks like object\ndetection. With these surveys as a foundation, the aim of this contribution is\nto provide a first high-level, categorized meta-analysis of selected reviews on\ndeep learning across different scientific disciplines and outline the research\nimpact that they already have during a short period of time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:14:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Egger", "Jan", ""], ["Pepe", "Antonio", ""], ["Gsaxner", "Christina", ""], ["Li", "Jianning", ""]]}, {"id": "2011.08712", "submitter": "Aria Khoshsirat", "authors": "Aria Khoshsirat", "title": "A Simple Framework to Quantify Different Types of Uncertainty in Deep\n  Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying uncertainty in a model's predictions is important as it enables\nthe safety of an AI system to be increased by acting on the model's output in\nan informed manner. This is crucial for applications where the cost of an error\nis high, such as in autonomous vehicle control, medical image analysis,\nfinancial estimations or legal fields. Deep Neural Networks are powerful\npredictors that have recently achieved state-of-the-art performance on a wide\nspectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging\nand yet on-going problem. In this paper we propose a complete framework to\ncapture and quantify three known types of uncertainty in DNNs for the task of\nimage classification. This framework includes an ensemble of CNNs for model\nuncertainty, a supervised reconstruction auto-encoder to capture distributional\nuncertainty and using the output of activation functions in the last layer of\nthe network, to capture data uncertainty. Finally we demonstrate the efficiency\nof our method on popular image datasets for classification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:36:42 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 07:35:42 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 17:44:35 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 20:03:34 GMT"}, {"version": "v5", "created": "Fri, 28 May 2021 15:33:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Khoshsirat", "Aria", ""]]}, {"id": "2011.08783", "submitter": "Karla Burelo", "authors": "Karla Burelo and Mohammadali Sharifshazileh and Niklaus Krayenb\\\"uhl\n  and Georgia Ramantani and Giacomo Indiveri and Johannes Sarnthein", "title": "A Spiking Neural Network (SNN) for detecting High Frequency Oscillations\n  (HFOs) in the intraoperative ECoG", "comments": "11 pages, 3 figures, 2 tables. The results of this publication were\n  obtained by simulating our hardware platform, built for online processing of\n  biological signals. This hardware combines neural recording headstages with a\n  multi-core neuromorphic processor arxiv.org/abs/2009.11245", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve seizure freedom, epilepsy surgery requires the complete resection\nof the epileptogenic brain tissue. In intraoperative ECoG recordings, high\nfrequency oscillations (HFOs) generated by epileptogenic tissue can be used to\ntailor the resection margin. However, automatic detection of HFOs in real-time\nremains an open challenge. Here we present a spiking neural network (SNN) for\nautomatic HFO detection that is optimally suited for neuromorphic hardware\nimplementation. We trained the SNN to detect HFO signals measured from\nintraoperative ECoG on-line, using an independently labeled dataset. We\ntargeted the detection of HFOs in the fast ripple frequency range (250-500 Hz)\nand compared the network results with the labeled HFO data. We endowed the SNN\nwith a novel artifact rejection mechanism to suppress sharp transients and\ndemonstrate its effectiveness on the ECoG dataset. The HFO rates (median 6.6\nHFO/min in pre-resection recordings) detected by this SNN are comparable to\nthose published in the dataset (58 min, 16 recordings). The postsurgical\nseizure outcome was \"predicted\" with 100% accuracy for all 8 patients. These\nresults provide a further step towards the construction of a real-time portable\nbattery-operated HFO detection system that can be used during epilepsy surgery\nto guide the resection of the epileptogenic zone.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:24:46 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Burelo", "Karla", ""], ["Sharifshazileh", "Mohammadali", ""], ["Krayenb\u00fchl", "Niklaus", ""], ["Ramantani", "Georgia", ""], ["Indiveri", "Giacomo", ""], ["Sarnthein", "Johannes", ""]]}, {"id": "2011.08895", "submitter": "Alex Lewandowski", "authors": "Varun Ranganathan, Alex Lewandowski", "title": "ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks", "comments": "To appear in \"Beyond Backpropagation - Novel Ideas for Training\n  Neural Architectures\" Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradient descent and backpropagation have enabled neural networks to achieve\nremarkable results in many real-world applications. Despite ongoing success,\ntraining a neural network with gradient descent can be a slow and strenuous\naffair. We present a simple yet faster training algorithm called Zeroth-Order\nRelaxed Backpropagation (ZORB). Instead of calculating gradients, ZORB uses the\npseudoinverse of targets to backpropagate information. ZORB is designed to\nreduce the time required to train deep neural networks without penalizing\nperformance. To illustrate the speed up, we trained a feed-forward neural\nnetwork with 11 layers on MNIST and observed that ZORB converged 300 times\nfaster than Adam while achieving a comparable error rate, without any\nhyperparameter tuning. We also broaden the scope of ZORB to convolutional\nneural networks, and apply it to subsamples of the CIFAR-10 dataset.\nExperiments on standard classification and regression benchmarks demonstrate\nZORB's advantage over traditional backpropagation with Gradient Descent.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:29:47 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ranganathan", "Varun", ""], ["Lewandowski", "Alex", ""]]}, {"id": "2011.09004", "submitter": "Rebecca Russell", "authors": "Aastha Acharya, Rebecca Russell, Nisar R. Ahmed", "title": "Explaining Conditions for Reinforcement Learning Behaviors from Real and\n  Imagined Data", "comments": "Accepted to the Workshop on Challenges of Real-World RL at NeurIPS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of reinforcement learning (RL) in the real world comes with\nchallenges in calibrating user trust and expectations. As a step toward\ndeveloping RL systems that are able to communicate their competencies, we\npresent a method of generating human-interpretable abstract behavior models\nthat identify the experiential conditions leading to different task execution\nstrategies and outcomes. Our approach consists of extracting experiential\nfeatures from state representations, abstracting strategy descriptors from\ntrajectories, and training an interpretable decision tree that identifies the\nconditions most predictive of different RL behaviors. We demonstrate our method\non trajectory data generated from interactions with the environment and on\nimagined trajectory data that comes from a trained probabilistic world model in\na model-based RL setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 23:40:47 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Acharya", "Aastha", ""], ["Russell", "Rebecca", ""], ["Ahmed", "Nisar R.", ""]]}, {"id": "2011.09067", "submitter": "M. Ali Vosoughi", "authors": "M. Ali Vosoughi", "title": "Distributed Injection-Locking in Analog Ising Machines to Solve\n  Combinatorial Optimizations", "comments": "5 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The oscillator-based Ising machine (OIM) is a network of coupled CMOS\noscillators that solves combinatorial optimization problems. In this paper, the\ndistribution of the injection-locking oscillations throughout the circuit is\nproposed to accelerate the phase-locking of the OIM. The implications of the\nproposed technique theoretically investigated and verified by extensive\nsimulations in EDA tools with a $130~nm$ PTM model. By distributing the\ninjective signal of the super-harmonic oscillator, the speed is increased by\n$219.8\\%$ with negligible increase in the power dissipation and phase-locking\nerror of the device due to the distributed technique.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:34:34 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 05:46:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Vosoughi", "M. Ali", ""]]}, {"id": "2011.09128", "submitter": "Moshe Eliasof", "authors": "Moshe Eliasof, Jonathan Ephrath, Lars Ruthotto, Eran Treister", "title": "Multigrid-in-Channels Neural Network Architectures", "comments": "This paper supersedes arXiv:2006.06799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multigrid-in-channels (MGIC) approach that tackles the quadratic\ngrowth of the number of parameters with respect to the number of channels in\nstandard convolutional neural networks (CNNs). It has been shown that there is\na redundancy in standard CNNs, as networks with light or sparse convolution\noperators yield similar performance to full networks. However, the number of\nparameters in the former networks also scales quadratically in width, while in\nthe latter case, the parameters typically have random sparsity patterns,\nhampering hardware efficiency. Our approach for building CNN architectures\nscales linearly with respect to the network's width while retaining full\ncoupling of the channels as in standard CNNs. To this end, we replace each\nconvolution block with its MGIC block utilizing a hierarchy of lightweight\nconvolutions. Our extensive experiments on image classification, segmentation,\nand point cloud classification show that applying this strategy to different\narchitectures like ResNet and MobileNetV3 considerably reduces the number of\nparameters while obtaining similar or better accuracy. For example, we obtain\n76.1% top-1 accuracy on ImageNet with a lightweight network with similar\nparameters and FLOPs to MobileNetV3.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 11:29:10 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 08:29:31 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Eliasof", "Moshe", ""], ["Ephrath", "Jonathan", ""], ["Ruthotto", "Lars", ""], ["Treister", "Eran", ""]]}, {"id": "2011.09236", "submitter": "Frank Glavin", "authors": "Preeti Jagdish Sajjan and Frank G. Glavin", "title": "A Multi-class Approach -- Building a Visual Classifier based on Textual\n  Descriptions using Zero-Shot Learning", "comments": "AICS 2020: Irish Conference on Artificial Intelligence and Cognitive\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) techniques for image classification routinely require\nmany labelled images for training the model and while testing, we ought to use\nimages belonging to the same domain as those used for training. In this paper,\nwe overcome the two main hurdles of ML, i.e. scarcity of data and constrained\nprediction of the classification model. We do this by introducing a visual\nclassifier which uses a concept of transfer learning, namely Zero-Shot Learning\n(ZSL), and standard Natural Language Processing techniques. We train a\nclassifier by mapping labelled images to their textual description instead of\ntraining it for specific classes. Transfer learning involves transferring\nknowledge across domains that are similar. ZSL intelligently applies the\nknowledge learned while training for future recognition tasks. ZSL\ndifferentiates classes as two types: seen and unseen classes. Seen classes are\nthe classes upon which we have trained our model and unseen classes are the\nclasses upon which we test our model. The examples from unseen classes have not\nbeen encountered in the training phase. Earlier research in this domain focused\non developing a binary classifier but, in this paper, we present a multi-class\nclassifier with a Zero-Shot Learning approach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:06:55 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sajjan", "Preeti Jagdish", ""], ["Glavin", "Frank G.", ""]]}, {"id": "2011.09380", "submitter": "Alireza Nadafian Neghabi", "authors": "Alireza Nadafian, Mohammad Ganjtabesh", "title": "Bio-plausible Unsupervised Delay Learning for Extracting Temporal\n  Features in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plasticity of the conduction delay between neurons plays a fundamental\nrole in learning. However, the exact underlying mechanisms in the brain for\nthis modulation is still an open problem. Understanding the precise adjustment\nof synaptic delays could help us in developing effective brain-inspired\ncomputational models in providing aligned insights with the experimental\nevidence. In this paper, we propose an unsupervised biologically plausible\nlearning rule for adjusting the synaptic delays in spiking neural networks.\nThen, we provided some mathematical proofs to show that our learning rule gives\na neuron the ability to learn repeating spatio-temporal patterns. Furthermore,\nthe experimental results of applying an STDP-based spiking neural network\nequipped with our proposed delay learning rule on Random Dot Kinematogram\nindicate the efficacy of the proposed delay learning rule in extracting\ntemporal features.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:25:32 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Nadafian", "Alireza", ""], ["Ganjtabesh", "Mohammad", ""]]}, {"id": "2011.09393", "submitter": "Nurislam Tursynbek", "authors": "Nurislam Tursynbek, Ilya Vilkoviskiy, Maria Sindeeva, Ivan Oseledets", "title": "Adversarial Turing Patterns from Cellular Automata", "comments": "Published as a conference paper at AAAI 2021 (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep classifiers are intriguingly vulnerable to universal\nadversarial perturbations: single disturbances of small magnitude that lead to\nmisclassification of most in-puts. This phenomena may potentially result in a\nserious security problem. Despite the extensive research in this area,there is\na lack of theoretical understanding of the structure of these perturbations. In\nimage domain, there is a certain visual similarity between patterns, that\nrepresent these perturbations, and classical Turing patterns, which appear as a\nsolution of non-linear partial differential equations and are underlying\nconcept of many processes in nature. In this paper,we provide a theoretical\nbridge between these two different theories, by mapping a simplified algorithm\nfor crafting universal perturbations to (inhomogeneous) cellular automata,the\nlatter is known to generate Turing patterns. Furthermore,we propose to use\nTuring patterns, generated by cellular automata, as universal perturbations,\nand experimentally show that they significantly degrade the performance of deep\nlearning models. We found this method to be a fast and efficient way to create\na data-agnostic quasi-imperceptible perturbation in the black-box scenario. The\nsource code is available at https://github.com/NurislamT/advTuring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:50:54 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 07:51:43 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 08:59:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Tursynbek", "Nurislam", ""], ["Vilkoviskiy", "Ilya", ""], ["Sindeeva", "Maria", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2011.09534", "submitter": "Nicolas Rougier", "authors": "Nicolas P. Rougier and Georgios Is. Detorakis", "title": "Randomized Self Organizing Map", "comments": "32 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a variation of the self organizing map algorithm by considering\nthe random placement of neurons on a two-dimensional manifold, following a blue\nnoise distribution from which various topologies can be derived. These\ntopologies possess random (but controllable) discontinuities that allow for a\nmore flexible self-organization, especially with high-dimensional data. The\nproposed algorithm is tested on one-, two- and three-dimensions tasks as well\nas on the MNIST handwritten digits dataset and validated using spectral\nanalysis and topological data analysis tools. We also demonstrate the ability\nof the randomized self-organizing map to gracefully reorganize itself in case\nof neural lesion and/or neurogenesis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:34:16 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Rougier", "Nicolas P.", ""], ["Detorakis", "Georgios Is.", ""]]}, {"id": "2011.09820", "submitter": "Yu Zhang", "authors": "Zhixiong Yue, Baijiong Lin, Xiaonan Huang, Yu Zhang", "title": "Effective, Efficient and Robust Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in adversarial attacks show the vulnerability of deep neural\nnetworks searched by Neural Architecture Search (NAS). Although NAS methods can\nfind network architectures with the state-of-the-art performance, the\nadversarial robustness and resource constraint are often ignored in NAS. To\nsolve this problem, we propose an Effective, Efficient, and Robust Neural\nArchitecture Search (E2RNAS) method to search a neural network architecture by\ntaking the performance, robustness, and resource constraint into consideration.\nThe objective function of the proposed E2RNAS method is formulated as a\nbi-level multi-objective optimization problem with the upper-level problem as a\nmulti-objective optimization problem, which is different from existing NAS\nmethods. To solve the proposed objective function, we integrate the\nmultiple-gradient descent algorithm, a widely studied gradient-based\nmulti-objective optimization algorithm, with the bi-level optimization.\nExperiments on benchmark datasets show that the proposed E2RNAS method can find\nadversarially robust architectures with optimized model size and comparable\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:46:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yue", "Zhixiong", ""], ["Lin", "Baijiong", ""], ["Huang", "Xiaonan", ""], ["Zhang", "Yu", ""]]}, {"id": "2011.09821", "submitter": "Jerry Swan", "authors": "Jerry Swan, Steven Adriaensen, Alexander E. I. Brownlee, Kevin\n  Hammond, Colin G. Johnson, Ahmed Kheiri, Faustyna Krawiec, J. J. Merelo,\n  Leandro L. Minku, Ender \\\"Ozcan, Gisele L. Pappa, Pablo Garc\\'ia-S\\'anchez,\n  Kenneth S\\\"orensen, Stefan Vo{\\ss}, Markus Wagner, David R. White", "title": "Metaheuristics \"In the Large\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Following decades of sustained improvement, metaheuristics are one of the\ngreat success stories of optimization research. However, in order for research\nin metaheuristics to avoid fragmentation and a lack of reproducibility, there\nis a pressing need for stronger scientific and computational infrastructure to\nsupport the development, analysis and comparison of new approaches. We argue\nthat, via principled choice of infrastructure support, the field can pursue a\nhigher level of scientific enquiry. We describe our vision and report on\nprogress, showing how the adoption of common protocols for all metaheuristics\ncan help liberate the potential of the field, easing the exploration of the\ndesign space of metaheuristics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:49:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:24:40 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 10:52:28 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 11:04:10 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Swan", "Jerry", ""], ["Adriaensen", "Steven", ""], ["Brownlee", "Alexander E. I.", ""], ["Hammond", "Kevin", ""], ["Johnson", "Colin G.", ""], ["Kheiri", "Ahmed", ""], ["Krawiec", "Faustyna", ""], ["Merelo", "J. J.", ""], ["Minku", "Leandro L.", ""], ["\u00d6zcan", "Ender", ""], ["Pappa", "Gisele L.", ""], ["Garc\u00eda-S\u00e1nchez", "Pablo", ""], ["S\u00f6rensen", "Kenneth", ""], ["Vo\u00df", "Stefan", ""], ["Wagner", "Markus", ""], ["White", "David R.", ""]]}, {"id": "2011.09839", "submitter": "Rajesh Ramachandran", "authors": "Rishabh Verma, R Rajesh and MS Easwaran", "title": "Modular Multi Target Tracking Using LSTM Networks", "comments": "Submitted to Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The process of association and tracking of sensor detections is a key element\nin providing situational awareness. When the targets in the scenario are dense\nand exhibit high maneuverability, Multi-Target Tracking (MTT) becomes a\nchallenging task. The conventional techniques to solve such NP-hard\ncombinatorial optimization problem involves multiple complex models and\nrequires tedious tuning of parameters, failing to provide an acceptable\nperformance within the computational constraints. This paper proposes a model\nfree end-to-end approach for airborne target tracking system using sensor\nmeasurements, integrating all the key elements of multi target tracking --\nassociation, prediction and filtering using deep learning with memory. The\nchallenging task of association is performed using the Bi-Directional Long\nshort-term memory (LSTM) whereas filtering and prediction are done using LSTM\nmodels. The proposed modular blocks can be independently trained and used in\nmultitude of tracking applications including non co-operative (e.g., radar) and\nco-operative sensors (e.g., AIS, IFF, ADS-B). Such modular blocks also enhances\nthe interpretability of the deep learning application. It is shown that\nperformance of the proposed technique outperforms conventional state of the art\ntechnique Joint Probabilistic Data Association with Interacting Multiple Model\n(JPDA-IMM) filter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:58:49 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Verma", "Rishabh", ""], ["Rajesh", "R", ""], ["Easwaran", "MS", ""]]}, {"id": "2011.09962", "submitter": "Elham Gholami Dr.", "authors": "Elham Gholami, Seyed Reza Kamel Tabbakh, Maryam Kheirabadi", "title": "Proposing method to Increase the detection accuracy of stomach cancer\n  based on colour and lint features of tongue using CNN and SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, gastric cancer is one of the diseases which affected many people's\nlife. Early detection and accuracy are the main and crucial challenges in\nfinding this kind of cancer. In this paper, a method to increase the accuracy\nof the diagnosis of detecting cancer using lint and colour features of tongue\nbased on deep convolutional neural networks and support vector machine is\nproposed. In the proposed method, the region of tongue is first separated from\nthe face image by {deep RCNN} \\color{black} Recursive Convolutional Neural\nNetwork (R-CNN) \\color{black}. After the necessary preprocessing, the images to\nthe convolutional neural network are provided and the training and test\noperations are triggered. The results show that the proposed method is\ncorrectly able to identify the area of the tongue as well as the patient's\nperson from the non-patient. Based on experiments, the DenseNet network has the\nhighest accuracy compared to other deep architectures. The experimental results\nshow that the accuracy of this network for gastric cancer detection reaches 91%\nwhich shows the superiority of method in comparison to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:06:29 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Gholami", "Elham", ""], ["Tabbakh", "Seyed Reza Kamel", ""], ["Kheirabadi", "Maryam", ""]]}, {"id": "2011.09964", "submitter": "Yukun Yang", "authors": "Yukun Yang", "title": "Temporal Surrogate Back-propagation for Spiking Neural Networks", "comments": "4 pases, 3 figures, 3 tables, 10 eqs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Spiking neural networks (SNN) are usually more energy-efficient as compared\nto Artificial neural networks (ANN), and the way they work has a great\nsimilarity with our brain. Back-propagation (BP) has shown its strong power in\ntraining ANN in recent years. However, since spike behavior is\nnon-differentiable, BP cannot be applied to SNN directly. Although prior works\ndemonstrated several ways to approximate the BP-gradient in both spatial and\ntemporal directions either through surrogate gradient or randomness, they\nomitted the temporal dependency introduced by the reset mechanism between each\nstep. In this article, we target on theoretical completion and investigate the\neffect of the missing term thoroughly. By adding the temporal dependency of the\nreset mechanism, the new algorithm is more robust to learning-rate adjustments\non a toy dataset but does not show much improvement on larger learning tasks\nlike CIFAR-10. Empirically speaking, the benefits of the missing term are not\nworth the additional computational overhead. In many cases, the missing term\ncan be ignored.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:22:47 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yang", "Yukun", ""]]}, {"id": "2011.09967", "submitter": "Bin Wang", "authors": "Wanshi Hong, Cong Zhang, Cy Chan, Bin Wang", "title": "Electric Vehicle Charging Infrastructure Planning: A Scalable\n  Computational Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal charging infrastructure planning problem over a large geospatial\narea is challenging due to the increasing network sizes of the transportation\nsystem and the electric grid. The coupling between the electric vehicle travel\nbehaviors and charging events is therefore complex. This paper focuses on the\ndemonstration of a scalable computational framework for the electric vehicle\ncharging infrastructure planning over the tightly integrated transportation and\nelectric grid networks. On the transportation side, a charging profile\ngeneration strategy is proposed leveraging the EV energy consumption model,\ntrip routing, and charger selection methods. On the grid side, a genetic\nalgorithm is utilized within the optimal power flow program to solve the\noptimal charger placement problem with integer variables by adaptively\nevaluating candidate solutions in the current iteration and generating new\nsolutions for the next iterations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:48:07 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Hong", "Wanshi", ""], ["Zhang", "Cong", ""], ["Chan", "Cy", ""], ["Wang", "Bin", ""]]}, {"id": "2011.09969", "submitter": "Huihui Wang", "authors": "Huihui Wang, Ruyang Mo", "title": "Neural network algorithm and its application in reactive distillation", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive distillation is a special distillation technology based on the\ncoupling of chemical reaction and distillation. It has the characteristics of\nlow energy consumption and high separation efficiency. However, because the\ncombination of reaction and separation produces highly nonlinear robust\nbehavior, the control and optimization of the reactive distillation process\ncannot use conventional methods, but must rely on neural network algorithms.\nThis paper briefly describes the characteristics and research progress of\nreactive distillation technology and neural network algorithms, and summarizes\nthe application of neural network algorithms in reactive distillation, aiming\nto provide reference for the development and innovation of industry technology.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:18:52 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wang", "Huihui", ""], ["Mo", "Ruyang", ""]]}, {"id": "2011.09970", "submitter": "Xingang Wang Professor", "authors": "Yali Guo, Han Zhang, Liang Wang, Huawei Fan, and Xingang Wang", "title": "Transfer learning of chaotic systems", "comments": "20 pages, 8 figures", "journal-ref": "Chaos 31, 011104 (2021)", "doi": "10.1063/5.0033870", "report-no": null, "categories": "cs.NE nlin.CD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Can a neural network trained by the time series of system A be used to\npredict the evolution of system B? This problem, knowing as transfer learning\nin a broad sense, is of great importance in machine learning and data mining,\nyet has not been addressed for chaotic systems. Here we investigate transfer\nlearning of chaotic systems from the perspective of synchronization-based state\ninference, in which a reservoir computer trained by chaotic system A is used to\ninfer the unmeasured variables of chaotic system B, while A is different from B\nin either parameter or dynamics. It is found that if systems A and B are\ndifferent in parameter, the reservoir computer can be well synchronized to\nsystem B. However, if systems A and B are different in dynamics, the reservoir\ncomputer fails to synchronize with system B in general. Knowledge transfer\nalong a chain of coupled reservoir computers is also studied, and it is found\nthat, although the reservoir computers are trained by different systems, the\nunmeasured variables of the driving system can be successfully inferred by the\nremote reservoir computer. Finally, by an experiment of chaotic pendulum, we\nshow that the knowledge learned from the modeling system can be used to predict\nthe evolution of the experimental system.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 04:09:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Guo", "Yali", ""], ["Zhang", "Han", ""], ["Wang", "Liang", ""], ["Fan", "Huawei", ""], ["Wang", "Xingang", ""]]}, {"id": "2011.10036", "submitter": "Haoye Lu Mr.", "authors": "Haoye Lu, Yongyi Mao, Amiya Nayak", "title": "On the Dynamics of Training Attention Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The attention mechanism has been widely used in deep neural networks as a\nmodel component. By now, it has become a critical building block in many\nstate-of-the-art natural language models. Despite its great success established\nempirically, the working mechanism of attention has not been investigated at a\nsufficient theoretical depth to date. In this paper, we set up a simple text\nclassification task and study the dynamics of training a simple attention-based\nclassification model using gradient descent. In this setting, we show that, for\nthe discriminative words that the model should attend to, a persisting identity\nexists relating its embedding and the inner product of its key and the query.\nThis allows us to prove that training must converge to attending to the\ndiscriminative words when the attention output is classified by a linear\nclassifier. Experiments are performed, which validate our theoretical analysis\nand provide further insights.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:55:30 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 03:51:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lu", "Haoye", ""], ["Mao", "Yongyi", ""], ["Nayak", "Amiya", ""]]}, {"id": "2011.10115", "submitter": "Florian Stelzer", "authors": "Florian Stelzer (1, 2 and 4), Andr\\'e R\\\"ohm (3), Raul Vicente (4),\n  Ingo Fischer (3), Serhiy Yanchuk (1) ((1) Institute of Mathematics,\n  Technische Universit\\\"at Berlin, Germany, (2) Department of Mathematics,\n  Humboldt-Universit\\\"at zu Berlin, Germany, (3) Instituto de F\\'isica\n  Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Spain, (4) Institute\n  of Computer Science, University of Tartu, Estonia)", "title": "Deep Neural Networks using a Single Neuron: Folded-in-Time Architecture\n  using Feedback-Modulated Delay Loops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are among the most widely applied machine learning tools\nshowing outstanding performance in a broad range of tasks. We present a method\nfor folding a deep neural network of arbitrary size into a single neuron with\nmultiple time-delayed feedback loops. This single-neuron deep neural network\ncomprises only a single nonlinearity and appropriately adjusted modulations of\nthe feedback signals. The network states emerge in time as a temporal unfolding\nof the neuron's dynamics. By adjusting the feedback-modulation within the\nloops, we adapt the network's connection weights. These connection weights are\ndetermined via a back-propagation algorithm, where both the delay-induced and\nlocal network connections must be taken into account. Our approach can fully\nrepresent standard Deep Neural Networks (DNN), encompasses sparse DNNs, and\nextends the DNN concept toward dynamical systems implementations. The new\nmethod, which we call Folded-in-time DNN (Fit-DNN), exhibits promising\nperformance in a set of benchmark tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:45:58 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 13:37:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Stelzer", "Florian", "", "1, 2 and 4"], ["R\u00f6hm", "Andr\u00e9", ""], ["Vicente", "Raul", ""], ["Fischer", "Ingo", ""], ["Yanchuk", "Serhiy", ""]]}, {"id": "2011.10208", "submitter": "Eric Nichols", "authors": "Eric Nichols and Leo Gao and Randy Gomez", "title": "Collaborative Storytelling with Large-scale Neural Language Models", "comments": "To appear in Proceedings of the 13th Annual ACM SIGGRAPH Conference\n  on Motion, Interaction and Games (MIG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Storytelling plays a central role in human socializing and entertainment.\nHowever, much of the research on automatic storytelling generation assumes that\nstories will be generated by an agent without any human interaction. In this\npaper, we introduce the task of collaborative storytelling, where an artificial\nintelligence agent and a person collaborate to create a unique story by taking\nturns adding to it. We present a collaborative storytelling system which works\nwith a human storyteller to create a story by generating new utterances based\non the story so far. We constructed the storytelling system by tuning a\npublicly-available large scale language model on a dataset of writing prompts\nand their accompanying fictional works. We identify generating sufficiently\nhuman-like utterances to be an important technical issue and propose a\nsample-and-rank approach to improve utterance quality. Quantitative evaluation\nshows that our approach outperforms a baseline, and we present qualitative\nevaluation of our system's capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:36:54 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Nichols", "Eric", ""], ["Gao", "Leo", ""], ["Gomez", "Randy", ""]]}, {"id": "2011.10283", "submitter": "Bryar Hassan Dr.", "authors": "Bryar A. Hassan", "title": "CSCF: a chaotic sine cosine firefly Algorithm for practical application\n  problems", "comments": "Neural Comput & Applic (2020)", "journal-ref": null, "doi": "10.1007/s00521-020-05474-6", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, numerous meta-heuristic based approaches are deliberated to reduce\nthe computational complexities of several existing approaches that include\ntricky derivations, very large memory space requirement, initial value\nsensitivity etc. However, several optimization algorithms namely firefly\nalgorithm, sine cosine algorithm, particle swarm optimization algorithm have\nfew drawbacks such as computational complexity, convergence speed etc. So to\novercome such shortcomings, this paper aims in developing a novel Chaotic Sine\nCosine Firefly (CSCF) algorithm with numerous variants to solve optimization\nproblems. Here, the chaotic form of two algorithms namely the sine cosine\nalgorithm (SCA) and the Firefly (FF) algorithms are integrated to improve the\nconvergence speed and efficiency thus minimizing several complexity issues.\nMoreover, the proposed CSCF approach is operated under various chaotic phases\nand the optimal chaotic variants containing the best chaotic mapping is\nselected. Then numerous chaotic benchmark functions are utilized to examine the\nsystem performance of the CSCF algorithm. Finally, the simulation results for\nthe problems based on engineering design are demonstrated to prove the\nefficiency, robustness and effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:54:28 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hassan", "Bryar A.", ""]]}, {"id": "2011.10389", "submitter": "Dominik Sisejkovic", "authors": "Dominik Sisejkovic, Farhad Merchant, Lennart M. Reimann, Harshit\n  Srivastava, Ahmed Hallawa and Rainer Leupers", "title": "Challenging the Security of Logic Locking Schemes in the Era of Deep\n  Learning: A Neuroevolutionary Approach", "comments": "25 pages, 17 figures, accepted at ACM JETC", "journal-ref": "ACM J. Emerg. Technol. Comput. Syst. 17, 3, Article 30 (May 2021),\n  26 pages", "doi": "10.1145/3431389", "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logic locking is a prominent technique to protect the integrity of hardware\ndesigns throughout the integrated circuit design and fabrication flow. However,\nin recent years, the security of locking schemes has been thoroughly challenged\nby the introduction of various deobfuscation attacks. As in most research\nbranches, deep learning is being introduced in the domain of logic locking as\nwell. Therefore, in this paper we present SnapShot: a novel attack on logic\nlocking that is the first of its kind to utilize artificial neural networks to\ndirectly predict a key bit value from a locked synthesized gate-level netlist\nwithout using a golden reference. Hereby, the attack uses a simpler yet more\nflexible learning model compared to existing work. Two different approaches are\nevaluated. The first approach is based on a simple feedforward fully connected\nneural network. The second approach utilizes genetic algorithms to evolve more\ncomplex convolutional neural network architectures specialized for the given\ntask. The attack flow offers a generic and customizable framework for attacking\nlocking schemes using machine learning techniques. We perform an extensive\nevaluation of SnapShot for two realistic attack scenarios, comprising both\nreference benchmark circuits as well as silicon-proven RISC-V core modules. The\nevaluation results show that SnapShot achieves an average key prediction\naccuracy of 82.60% for the selected attack scenario, with a significant\nperformance increase of 10.49 percentage points compared to the state of the\nart. Moreover, SnapShot outperforms the existing technique on all evaluated\nbenchmarks. The results indicate that the security foundation of common logic\nlocking schemes is build on questionable assumptions. The conclusions of the\nevaluation offer insights into the challenges of designing future logic locking\nschemes that are resilient to machine learning attacks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:03:19 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 08:49:19 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sisejkovic", "Dominik", ""], ["Merchant", "Farhad", ""], ["Reimann", "Lennart M.", ""], ["Srivastava", "Harshit", ""], ["Hallawa", "Ahmed", ""], ["Leupers", "Rainer", ""]]}, {"id": "2011.10520", "submitter": "Hugo Tessier", "authors": "Hugo Tessier, Vincent Gripon, Mathieu L\\'eonardon, Matthieu Arzel,\n  Thomas Hannagan, David Bertrand", "title": "Rethinking Weight Decay For Efficient Neural Network Pruning", "comments": "16 pages, 14 figures, submitted at ICML 2021, update : added new\n  results, rewrite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduced in the late 80's for generalization purposes, pruning has now\nbecome a staple to compress deep neural networks. Despite many innovations\nbrought in the last decades, pruning approaches still face core issues that\nhinder their performance or scalability. Drawing inspiration from early work in\nthe field, and especially the use of weight-decay to achieve sparsity, we\nintroduce Selective Weight Decay (SWD), which realizes efficient continuous\npruning throughout training. Our approach, theoretically-grounded on Lagrangian\nSmoothing, is versatile and can be applied to multiple tasks, networks and\npruning structures. We show that SWD compares favorably to state-of-the-art\napproaches in terms of performance/parameters ratio on the CIFAR-10, Cora and\nImageNet ILSVRC2012 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:25:53 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 15:30:19 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 08:28:53 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Tessier", "Hugo", ""], ["Gripon", "Vincent", ""], ["L\u00e9onardon", "Mathieu", ""], ["Arzel", "Matthieu", ""], ["Hannagan", "Thomas", ""], ["Bertrand", "David", ""]]}, {"id": "2011.10549", "submitter": "Ankith Mohan", "authors": "Ankith Mohan, Aiichiro Nakano, Emilio Ferrara", "title": "Graph Signal Recovery Using Restricted Boltzmann Machines", "comments": "Paper: 27 pages, 9 figures. Appendix: 5 pages, 12 figures. Submitted\n  to Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a model-agnostic pipeline to recover graph signals from an expert\nsystem by exploiting the content addressable memory property of restricted\nBoltzmann machine and the representational ability of a neural network. The\nproposed pipeline requires the deep neural network that is trained on a\ndownward machine learning task with clean data, data which is free from any\nform of corruption or incompletion. We show that denoising the representations\nlearned by the deep neural networks is usually more effective than denoising\nthe data itself. Although this pipeline can deal with noise in any dataset, it\nis particularly effective for graph-structured datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 18:43:53 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mohan", "Ankith", ""], ["Nakano", "Aiichiro", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2011.10568", "submitter": "Nishant Sinha", "authors": "Azhar Shaikh, Nishant Sinha", "title": "Learn to Bind and Grow Neural Structures", "comments": "Accepted to 8th ACM IKDD CODS and 26th COMAD (CODS-COMAD '21)\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-incremental learning involves the challenging problem of learning new\ntasks continually, without forgetting past knowledge. Many approaches address\nthe problem by expanding the structure of a shared neural network as tasks\narrive, but struggle to grow optimally, without losing past knowledge. We\npresent a new framework, Learn to Bind and Grow, which learns a neural\narchitecture for a new task incrementally, either by binding with layers of a\nsimilar task or by expanding layers which are more likely to conflict between\ntasks. Central to our approach is a novel, interpretable, parameterization of\nthe shared, multi-task architecture space, which then enables computing\nglobally optimal architectures using Bayesian optimization. Experiments on\ncontinual learning benchmarks show that our framework performs comparably with\nearlier expansion based approaches and is able to flexibly compute multiple\noptimal solutions with performance-size trade-offs.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 09:40:26 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shaikh", "Azhar", ""], ["Sinha", "Nishant", ""]]}, {"id": "2011.10760", "submitter": "Sukrit Mittal", "authors": "Sukrit Mittal and Dhish Kumar Saxena and Kalyanmoy Deb and Erik\n  Goodman", "title": "Enhanced Innovized Repair Operator for Evolutionary Multi- and\n  Many-objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": "COIN Lab Report: 2020020", "categories": "cs.NE cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Innovization\" is a task of learning common relationships among some or all\nof the Pareto-optimal (PO) solutions in multi- and many-objective optimization\nproblems. Recent studies have shown that a chronological sequence of\nnon-dominated solutions obtained in consecutive iterations during an\noptimization run also possess salient patterns that can be used to learn\nproblem features to help create new and improved solutions. In this paper, we\npropose a machine-learning- (ML-) assisted modelling approach that learns the\nmodifications in design variables needed to advance population members towards\nthe Pareto-optimal set. We then propose to use the resulting ML model as an\nadditional innovized repair (IR2) operator to be applied on offspring solutions\ncreated by the usual genetic operators, as a novel mean of improving their\nconvergence properties. In this paper, the well-known random forest (RF) method\nis used as the ML model and is integrated with various evolutionary multi- and\nmany-objective optimization algorithms, including NSGA-II, NSGA-III, and\nMOEA/D. On several test problems ranging from two to five objectives, we\ndemonstrate improvement in convergence behaviour using the proposed IR2-RF\noperator. Since the operator does not demand any additional solution\nevaluations, instead using the history of gradual and progressive improvements\nin solutions over generations, the proposed ML-based optimization opens up a\nnew direction of optimization algorithm development with advances in AI and ML\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 10:29:15 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mittal", "Sukrit", ""], ["Saxena", "Dhish Kumar", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""]]}, {"id": "2011.10831", "submitter": "AbdElRahman ElSaid", "authors": "AbdElRahman ElSaid, Joshua Karns, Zimeng Lyu, Alexander Ororbia,\n  Travis Desell", "title": "Continuous Ant-Based Neural Topology Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel, nature-inspired neural architecture search\n(NAS) algorithm based on ant colony optimization, Continuous Ant-based Neural\nTopology Search (CANTS), which utilizes synthetic ants that move over a\ncontinuous search space based on the density and distribution of pheromones, is\nstrongly inspired by how ants move in the real world. The paths taken by the\nant agents through the search space are utilized to construct artificial neural\nnetworks (ANNs). This continuous search space allows CANTS to automate the\ndesign of ANNs of any size, removing a key limitation inherent to many current\nNAS algorithms that must operate within structures with a size predetermined by\nthe user. CANTS employs a distributed asynchronous strategy which allows it to\nscale to large-scale high performance computing resources, works with a variety\nof recurrent memory cell structures, and makes use of a communal weight sharing\nstrategy to reduce training time. The proposed procedure is evaluated on three\nreal-world, time series prediction problems in the field of power systems and\ncompared to two state-of-the-art algorithms. Results show that CANTS is able to\nprovide improved or competitive results on all of these problems, while also\nbeing easier to use, requiring half the number of user-specified\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 17:49:44 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["ElSaid", "AbdElRahman", ""], ["Karns", "Joshua", ""], ["Lyu", "Zimeng", ""], ["Ororbia", "Alexander", ""], ["Desell", "Travis", ""]]}, {"id": "2011.10852", "submitter": "Melika Payvand", "authors": "Melika Payvand, Mohammed E. Fouda, Fadi Kurdahi, Ahmed M. Eltawil,\n  Emre O. Neftci", "title": "On-Chip Error-triggered Learning of Multi-layer Memristive Spiking\n  Neural Networks", "comments": "15 pages, 11 figures, Journal of Emerging Technology in Circuits and\n  Systems (JETCAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in neuromorphic computing show that local forms of\ngradient descent learning are compatible with Spiking Neural Networks (SNNs)\nand synaptic plasticity. Although SNNs can be scalably implemented using\nneuromorphic VLSI, an architecture that can learn using gradient-descent in\nsitu is still missing. In this paper, we propose a local, gradient-based,\nerror-triggered learning algorithm with online ternary weight updates. The\nproposed algorithm enables online training of multi-layer SNNs with memristive\nneuromorphic hardware showing a small loss in the performance compared with the\nstate of the art. We also propose a hardware architecture based on memristive\ncrossbar arrays to perform the required vector-matrix multiplications. The\nnecessary peripheral circuitry including pre-synaptic, post-synaptic and write\ncircuits required for online training, have been designed in the sub-threshold\nregime for power saving with a standard 180 nm CMOS process.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 19:44:19 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Payvand", "Melika", ""], ["Fouda", "Mohammed E.", ""], ["Kurdahi", "Fadi", ""], ["Eltawil", "Ahmed M.", ""], ["Neftci", "Emre O.", ""]]}, {"id": "2011.11058", "submitter": "Shashi Kant Gupta", "authors": "Shivi Gupta, Shashi Kant Gupta", "title": "Investigating Emotion-Color Association in Deep Neural Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been found that representations learned by Deep Neural Networks (DNNs)\ncorrelate very well to neural responses measured in primates' brains and\npsychological representations exhibited by human similarity judgment. On\nanother hand, past studies have shown that particular colors can be associated\nwith specific emotion arousal in humans. Do deep neural networks also learn\nthis behavior? In this study, we investigate if DNNs can learn implicit\nassociations in stimuli, particularly, an emotion-color association between\nimage stimuli. Our study was conducted in two parts. First, we collected human\nresponses on a forced-choice decision task in which subjects were asked to\nselect a color for a specified emotion-inducing image. Next, we modeled this\ndecision task on neural networks using the similarity between deep\nrepresentation (extracted using DNNs trained on object classification tasks) of\nthe images and images of colors used in the task. We found that our model\nshowed a fuzzy linear relationship between the two decision probabilities. This\nresults in two interesting findings, 1. The representations learned by deep\nneural networks can indeed show an emotion-color association 2. The\nemotion-color association is not just random but involves some cognitive\nphenomena. Finally, we also show that this method can help us in the emotion\nclassification task, specifically when there are very few examples to train the\nmodel. This analysis can be relevant to psychologists studying emotion-color\nassociations and artificial intelligence researchers modeling emotional\nintelligence in machines or studying representations learned by deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:48:02 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gupta", "Shivi", ""], ["Gupta", "Shashi Kant", ""]]}, {"id": "2011.11194", "submitter": "Xiang Fang", "authors": "Xiang Fang, Yuchong Hu, Pan Zhou, Dapeng Oliver Wu", "title": "V3H: View Variation and View Heredity for Incomplete Multi-view\n  Clustering", "comments": "Publisheded in IEEE Transactions on Artificial Intelligence", "journal-ref": "IEEE Transactions on Artificial Intelligence 2020", "doi": "10.1109/TAI.2021.3052425", "report-no": "vol. 1, no. 3, pp. 233-247, Dec. 2020", "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real data often appear in the form of multiple incomplete views. Incomplete\nmulti-view clustering is an effective method to integrate these incomplete\nviews. Previous methods only learn the consistent information between different\nviews and ignore the unique information of each view, which limits their\nclustering performance and generalizations. To overcome this limitation, we\npropose a novel View Variation and View Heredity approach (V3H). Inspired by\nthe variation and the heredity in genetics, V3H first decomposes each subspace\ninto a variation matrix for the corresponding view and a heredity matrix for\nall the views to represent the unique information and the consistent\ninformation respectively. Then, by aligning different views based on their\ncluster indicator matrices, V3H integrates the unique information from\ndifferent views to improve the clustering performance. Finally, with the help\nof the adjustable low-rank representation based on the heredity matrix, V3H\nrecovers the underlying true data structure to reduce the influence of the\nlarge incompleteness. More importantly, V3H presents possibly the first work to\nintroduce genetics to clustering algorithms for learning simultaneously the\nconsistent information and the unique information from incomplete multi-view\ndata. Extensive experimental results on fifteen benchmark datasets validate its\nsuperiority over other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:24:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:47:52 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 08:34:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Fang", "Xiang", ""], ["Hu", "Yuchong", ""], ["Zhou", "Pan", ""], ["Wu", "Dapeng Oliver", ""]]}, {"id": "2011.11293", "submitter": "Rasmus Berg Palm", "authors": "Thor V.A.N. Olesen, Dennis T.T. Nguyen, Rasmus Berg Palm, Sebastian\n  Risi", "title": "Evolutionary Planning in Latent Space", "comments": "Code to reproduce the experiments are available at\n  https://github.com/two2tee/WorldModelPlanning Video of driving performance is\n  available at https://youtu.be/3M39QgeF27U", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning is a powerful approach to reinforcement learning with several\ndesirable properties. However, it requires a model of the world, which is not\nreadily available in many real-life problems. In this paper, we propose to\nlearn a world model that enables Evolutionary Planning in Latent Space (EPLS).\nWe use a Variational Auto Encoder (VAE) to learn a compressed latent\nrepresentation of individual observations and extend a Mixture Density\nRecurrent Neural Network (MDRNN) to learn a stochastic, multi-modal forward\nmodel of the world that can be used for planning. We use the Random Mutation\nHill Climbing (RMHC) to find a sequence of actions that maximize expected\nreward in this learned model of the world. We demonstrate how to build a model\nof the world by bootstrapping it with rollouts from a random policy and\niteratively refining it with rollouts from an increasingly accurate planning\npolicy using the learned world model. After a few iterations of this\nrefinement, our planning agents are better than standard model-free\nreinforcement learning approaches demonstrating the viability of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 09:21:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Olesen", "Thor V. A. N.", ""], ["Nguyen", "Dennis T. T.", ""], ["Palm", "Rasmus Berg", ""], ["Risi", "Sebastian", ""]]}, {"id": "2011.11440", "submitter": "Paolo Pagliuca", "authors": "Paolo Pagliuca and Stefano Nolfi", "title": "The Dynamic of Body and Brain Co-Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method that permits to co-evolve the body and the control\nproperties of robots. It can be used to adapt the morphological traits of\nrobots with a hand-designed morphological bauplan or to evolve the\nmorphological bauplan as well. Our results indicate that robots with co-adapted\nbody and control traits outperform robots with fixed hand-designed\nmorphologies. Interestingly, the advantage is not due to the selection of\nbetter morphologies but rather to the mutual scaffolding process that results\nfrom the possibility to co-adapt the morphological traits to the control traits\nand vice versa. Our results also demonstrate that morphological variations do\nnot necessarily have destructive effects on robot skills.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 14:41:57 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pagliuca", "Paolo", ""], ["Nolfi", "Stefano", ""]]}, {"id": "2011.11705", "submitter": "Brian Hutchinson", "authors": "Alexandra Puchko, Robert Link, Brian Hutchinson, Ben Kravitz, Abigail\n  Snyder", "title": "DeepClimGAN: A High-Resolution Climate Data Generator", "comments": "Presented at NeurIPS 2019 Workshop Tackling Climate Change with\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth system models (ESMs), which simulate the physics and chemistry of the\nglobal atmosphere, land, and ocean, are often used to generate future\nprojections of climate change scenarios. These models are far too\ncomputationally intensive to run repeatedly, but limited sets of runs are\ninsufficient for some important applications, like adequately sampling\ndistribution tails to characterize extreme events. As a compromise, emulators\nare substantially less expensive but may not have all of the complexity of an\nESM. Here we demonstrate the use of a conditional generative adversarial\nnetwork (GAN) to act as an ESM emulator. In doing so, we gain the ability to\nproduce daily weather data that is consistent with what ESM might output over\nany chosen scenario. In particular, the GAN is aimed at representing a joint\nprobability distribution over space, time, and climate variables, enabling the\nstudy of correlated extreme events, such as floods, droughts, or heatwaves.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:13:37 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Puchko", "Alexandra", ""], ["Link", "Robert", ""], ["Hutchinson", "Brian", ""], ["Kravitz", "Ben", ""], ["Snyder", "Abigail", ""]]}, {"id": "2011.11710", "submitter": "Mihai Alexandru Petrovici", "authors": "Elena Kreutzer, Walter M. Senn, Mihai A. Petrovici", "title": "Natural-gradient learning for spiking neurons", "comments": "Joint senior authorship: Walter M. Senn and Mihai A. Petrovici", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE math.DG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many normative theories of synaptic plasticity, weight updates implicitly\ndepend on the chosen parametrization of the weights. This problem relates, for\nexample, to neuronal morphology: synapses which are functionally equivalent in\nterms of their impact on somatic firing can differ substantially in spine size\ndue to their different positions along the dendritic tree. Classical theories\nbased on Euclidean gradient descent can easily lead to inconsistencies due to\nsuch parametrization dependence. The issues are solved in the framework of\nRiemannian geometry, in which we propose that plasticity instead follows\nnatural gradient descent. Under this hypothesis, we derive a synaptic learning\nrule for spiking neurons that couples functional efficiency with the\nexplanation of several well-documented biological phenomena such as dendritic\ndemocracy, multiplicative scaling and heterosynaptic plasticity. We therefore\nsuggest that in its search for functional synaptic plasticity, evolution might\nhave come up with its own version of natural gradient descent.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:26:37 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kreutzer", "Elena", ""], ["Senn", "Walter M.", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "2011.11715", "submitter": "C.-H. Huck Yang", "authors": "Chao-Han Huck Yang, Linda Liu, Ankur Gandhe, Yile Gu, Anirudh Raju,\n  Denis Filimonov, Ivan Bulyko", "title": "Multi-task Language Modeling for Improving Speech Recognition of Rare\n  Words", "comments": "Preprint. Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR).\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:40:44 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 03:12:54 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 20:31:00 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yang", "Chao-Han Huck", ""], ["Liu", "Linda", ""], ["Gandhe", "Ankur", ""], ["Gu", "Yile", ""], ["Raju", "Anirudh", ""], ["Filimonov", "Denis", ""], ["Bulyko", "Ivan", ""]]}, {"id": "2011.11944", "submitter": "Yulai Zhang", "authors": "Yaru Li, Yulai Zhang", "title": "Hyper-parameter estimation method with particle swarm optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle swarm optimization (PSO) method cannot be directly used in the\nproblem of hyper-parameter estimation since the mathematical formulation of the\nmapping from hyper-parameters to loss function or generalization accuracy is\nunclear. Bayesian optimization (BO) framework is capable of converting the\noptimization of the hyper-parameters into the optimization of an acquisition\nfunction. The acquisition function is non-convex and multi-peak. So the problem\ncan be better solved by the PSO. The proposed method in this paper uses the\nparticle swarm method to optimize the acquisition function in the BO framework\nto get better hyper-parameters. The performances of proposed method in both of\nthe classification and regression models are evaluated and demonstrated. The\nresults on several benchmark problems are improved.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:51:51 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 04:16:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Yaru", ""], ["Zhang", "Yulai", ""]]}, {"id": "2011.12012", "submitter": "Shashi Kant Gupta", "authors": "Shashi Kant Gupta", "title": "A More Biologically Plausible Local Learning Rule for ANNs", "comments": "8 pages (4 main + 1 reference + 3 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation algorithm is often debated for its biological\nplausibility. However, various learning methods for neural architecture have\nbeen proposed in search of more biologically plausible learning. Most of them\nhave tried to solve the \"weight transport problem\" and try to propagate errors\nbackward in the architecture via some alternative methods. In this work, we\ninvestigated a slightly different approach that uses only the local information\nwhich captures spike timing information with no propagation of errors. The\nproposed learning rule is derived from the concepts of spike timing dependant\nplasticity and neuronal association. A preliminary evaluation done on the\nbinary classification of MNIST and IRIS datasets with two hidden layers shows\ncomparable performance with backpropagation. The model learned using this\nmethod also shows a possibility of better adversarial robustness against the\nFGSM attack compared to the model learned through backpropagation of\ncross-entropy loss. The local nature of learning gives a possibility of large\nscale distributed and parallel learning in the network. And finally, the\nproposed method is a more biologically sound method that can probably help in\nunderstanding how biological neurons learn different abstractions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 10:35:47 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gupta", "Shashi Kant", ""]]}, {"id": "2011.12043", "submitter": "Lukas Mauch", "authors": "Lukas Mauch, Stephen Tiedemann, Javier Alonso Garcia, Bac Nguyen Cong,\n  Kazuki Yoshiyama, Fabien Cardinaux, Thomas Kemp", "title": "Efficient Sampling for Predictor-Based Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, predictor-based algorithms emerged as a promising approach for\nneural architecture search (NAS). For NAS, we typically have to calculate the\nvalidation accuracy of a large number of Deep Neural Networks (DNNs), what is\ncomputationally complex. Predictor-based NAS algorithms address this problem.\nThey train a proxy model that can infer the validation accuracy of DNNs\ndirectly from their network structure. During optimization, the proxy can be\nused to narrow down the number of architectures for which the true validation\naccuracy must be computed, what makes predictor-based algorithms sample\nefficient. Usually, we compute the proxy for all DNNs in the network search\nspace and pick those that maximize the proxy as candidates for optimization.\nHowever, that is intractable in practice, because the search spaces are often\nvery large and contain billions of network architectures. The contributions of\nthis paper are threefold: 1) We define a sample efficiency gain to compare\ndifferent predictor-based NAS algorithms. 2) We conduct experiments on the\nNASBench-101 dataset and show that the sample efficiency of predictor-based\nalgorithms decreases dramatically if the proxy is only computed for a subset of\nthe search space. 3) We show that if we choose the subset of the search space\non which the proxy is evaluated in a smart way, the sample efficiency of the\noriginal predictor-based algorithm that has access to the full search space can\nbe regained. This is an important step to make predictor-based NAS algorithms\nuseful, in practice.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:36:36 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Mauch", "Lukas", ""], ["Tiedemann", "Stephen", ""], ["Garcia", "Javier Alonso", ""], ["Cong", "Bac Nguyen", ""], ["Yoshiyama", "Kazuki", ""], ["Cardinaux", "Fabien", ""], ["Kemp", "Thomas", ""]]}, {"id": "2011.12338", "submitter": "Carlo Michaelis", "authors": "Carlo Michaelis", "title": "PeleNet: A Reservoir Computing Framework for Loihi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-level frameworks for spiking neural networks are a key factor for fast\nprototyping and efficient development of complex algorithms. Such frameworks\nhave emerged in the last years for traditional computers, but programming\nneuromorphic hardware is still a challenge. Often low level programming with\nknowledge about the hardware of the neuromorphic chip is required. The PeleNet\nframework aims to simplify reservoir computing for the neuromorphic hardware\nLoihi. It is build on top of the NxSDK from Intel and is written in Python. The\nframework manages weight matrices, parameters and probes. In particular, it\nprovides an automatic and efficient distribution of networks over several cores\nand chips. With this, the user is not confronted with technical details and can\nconcentrate on experiments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 19:33:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Michaelis", "Carlo", ""]]}, {"id": "2011.12413", "submitter": "Leonardo Zepeda-N\\'u\\~nez", "authors": "Matthew Li and Laurent Demanet and Leonardo Zepeda-N\\'u\\~nez", "title": "Wide-band butterfly network: stable and efficient inversion via\n  multi-frequency neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an end-to-end deep learning architecture called the wide-band\nbutterfly network (WideBNet) for approximating the inverse scattering map from\nwide-band scattering data. This architecture incorporates tools from\ncomputational harmonic analysis, such as the butterfly factorization, and\ntraditional multi-scale methods, such as the Cooley-Tukey FFT algorithm, to\ndrastically reduce the number of trainable parameters to match the inherent\ncomplexity of the problem. As a result WideBNet is efficient: it requires fewer\ntraining points than off-the-shelf architectures, and has stable training\ndynamics, thus it can rely on standard weight initialization strategies. The\narchitecture automatically adapts to the dimensions of the data with only a few\nhyper-parameters that the user must specify. WideBNet is able to produce images\nthat are competitive with optimization-based approaches, but at a fraction of\nthe cost, and we also demonstrate numerically that it learns to super-resolve\nscatterers in the full aperture scattering setup.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:48:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Matthew", ""], ["Demanet", "Laurent", ""], ["Zepeda-N\u00fa\u00f1ez", "Leonardo", ""]]}, {"id": "2011.12428", "submitter": "Sebastian Goldt", "authors": "Maria Refinetti, St\\'ephane d'Ascoli, Ruben Ohana, Sebastian Goldt", "title": "Align, then memorise: the dynamics of learning with feedback alignment", "comments": "The accompanying code for this paper is available at\n  https://github.com/sdascoli/dfa-dynamics", "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning (ICML), PMLR 139, 2021", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct Feedback Alignment (DFA) is emerging as an efficient and biologically\nplausible alternative to the ubiquitous backpropagation algorithm for training\ndeep neural networks. Despite relying on random feedback weights for the\nbackward pass, DFA successfully trains state-of-the-art models such as\nTransformers. On the other hand, it notoriously fails to train convolutional\nnetworks. An understanding of the inner workings of DFA to explain these\ndiverging results remains elusive. Here, we propose a theory for the success of\nDFA. We first show that learning in shallow networks proceeds in two steps: an\nalignment phase, where the model adapts its weights to align the approximate\ngradient with the true gradient of the loss function, is followed by a\nmemorisation phase, where the model focuses on fitting the data. This two-step\nprocess has a degeneracy breaking effect: out of all the low-loss solutions in\nthe landscape, a network trained with DFA naturally converges to the solution\nwhich maximises gradient alignment. We also identify a key quantity underlying\nalignment in deep linear networks: the conditioning of the alignment matrices.\nThe latter enables a detailed understanding of the impact of data structure on\nalignment, and suggests a simple explanation for the well-known failure of DFA\nto train convolutional neural networks. Numerical experiments on MNIST and\nCIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and\nshow that the align-then-memorise process occurs sequentially from the bottom\nlayers of the network to the top.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:21:27 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 14:20:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Refinetti", "Maria", ""], ["d'Ascoli", "St\u00e9phane", ""], ["Ohana", "Ruben", ""], ["Goldt", "Sebastian", ""]]}, {"id": "2011.12448", "submitter": "Ziyi Gong", "authors": "Ziyi Gong and Paul Munro", "title": "Modeling the Evolution of Retina Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vital to primary visual processing, retinal circuitry shows many similar\nstructures across a very broad array of species, both vertebrate and\nnon-vertebrate, especially functional components such as lateral inhibition.\nThis surprisingly conservative pattern raises a question of how evolution leads\nto it, and whether there is any alternative that can also prompt helpful\npreprocessing. Here we design a method using genetic algorithm that, with many\ndegrees of freedom, leads to architectures whose functions are similar to\nbiological retina, as well as effective alternatives that are different in\nstructures and functions. We compare this model to natural evolution and\ndiscuss how our framework can come into goal-driven search and sustainable\nenhancement of neural network models in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:57:54 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 22:58:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gong", "Ziyi", ""], ["Munro", "Paul", ""]]}, {"id": "2011.12839", "submitter": "Nick Iliev", "authors": "Nick Iliev and Amit Ranjan Trivedi", "title": "Low Latency CMOS Hardware Acceleration for Fully Connected Layers in\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel low latency CMOS hardware accelerator for fully connected\n(FC) layers in deep neural networks (DNNs). The FC accelerator, FC-ACCL, is\nbased on 128 8x8 or 16x16 processing elements (PEs) for matrix-vector\nmultiplication, and 128 multiply-accumulate (MAC) units integrated with 128\nHigh Bandwidth Memory (HBM) units for storing the pretrained weights.\nMicro-architectural details for CMOS ASIC implementations are presented and\nsimulated performance is compared to recent hardware accelerators for DNNs for\nAlexNet and VGG 16. When comparing simulated processing latency for a 4096-1000\nFC8 layer, our FC-ACCL is able to achieve 48.4 GOPS (with a 100 MHz clock)\nwhich improves on a recent FC8 layer accelerator quoted at 28.8 GOPS with a 150\nMHz clock. We have achieved this considerable improvement by fully utilizing\nthe HBM units for storing and reading out column-specific FClayer weights in 1\ncycle with a novel colum-row-column schedule, and implementing a maximally\nparallel datapath for processing these weights with the corresponding MAC and\nPE units. When up-scaled to 128 16x16 PEs, for 16x16 tiles of weights, the\ndesign can reduce latency for the large FC6 layer by 60 % in AlexNet and by 3 %\nin VGG16 when compared to an alternative EIE solution which uses compression.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:49:38 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Iliev", "Nick", ""], ["Trivedi", "Amit Ranjan", ""]]}, {"id": "2011.12930", "submitter": "Anand Gopalakrishnan", "authors": "Anand Gopalakrishnan, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Unsupervised Object Keypoint Learning using Local Spatial Predictability", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose PermaKey, a novel approach to representation learning based on\nobject keypoints. It leverages the predictability of local image regions from\nspatial neighborhoods to identify salient regions that correspond to object\nparts, which are then converted to keypoints. Unlike prior approaches, it\nutilizes predictability to discover object keypoints, an intrinsic property of\nobjects. This ensures that it does not overly bias keypoints to focus on\ncharacteristics that are not unique to objects, such as movement, shape, colour\netc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints\ncorresponding to the most salient object parts and is robust to certain visual\ndistractors. Further, on downstream RL tasks in the Atari domain we demonstrate\nhow agents equipped with our keypoints outperform those using competing\nalternatives, even on challenging environments with moving backgrounds or\ndistractor objects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:27:05 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:10:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gopalakrishnan", "Anand", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2011.13130", "submitter": "Bhavana Burramukku Ms", "authors": "Bhavana Burramukku", "title": "Estimator Model for Prediction of Power Output of Wave Farms Using\n  Machine Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of power generated by a wave farm depends on the Wave Energy\nConverter (WEC) arrangement along with the usual wave conditions. Therefore,\nforming the appropriate arrangement of WECs in an array is an important factor\nin maximizing power absorption. Data collected from the test sites is used to\ndesign a neural model for predicting wave farm's power output generated. This\npaper focuses on developing a neural model for the prediction of wave energy\nbased on the data set derived from the four real wave scenarios from the\nsouthern coast of Australia. The applied converter model is a fully submerged\nthree-tether converter called CETO. A precise analysis of the WEC placement is\ninvestigated to reveal the amount of power generated by the wave farms on the\ntest site.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 05:05:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Burramukku", "Bhavana", ""]]}, {"id": "2011.13149", "submitter": "Shichong Peng", "authors": "Ke Li, Shichong Peng, Kailas Vodrahalli, Jitendra Malik", "title": "Better Knowledge Retention through Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continual learning, new categories may be introduced over time, and an\nideal learning system should perform well on both the original categories and\nthe new categories. While deep neural nets have achieved resounding success in\nthe classical supervised setting, they are known to forget about knowledge\nacquired in prior episodes of learning if the examples encountered in the\ncurrent episode of learning are drastically different from those encountered in\nprior episodes. In this paper, we propose a new method that can both leverage\nthe expressive power of deep neural nets and is resilient to forgetting when\nnew categories are introduced. We found the proposed method can reduce\nforgetting by 2.3x to 6.9x on CIFAR-10 compared to existing methods and by 1.8x\nto 2.7x on ImageNet compared to an oracle baseline.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 06:28:40 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Vodrahalli", "Kailas", ""], ["Malik", "Jitendra", ""]]}, {"id": "2011.13492", "submitter": "Aaron Tuor", "authors": "Jan Drgona, Elliott Skomski, Soumya Vasisht, Aaron Tuor, Draguna\n  Vrabie", "title": "Spectral Analysis and Stability of Deep Neural Dynamics", "comments": "Submitted to 3rd Annual Learning for Dynamics & Control Conference.\n  10 pages. 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our modern history of deep learning follows the arc of famous emergent\ndisciplines in engineering (e.g. aero- and fluid dynamics) when theory lagged\nbehind successful practical applications. Viewing neural networks from a\ndynamical systems perspective, in this work, we propose a novel\ncharacterization of deep neural networks as pointwise affine maps, making them\naccessible to a broader range of analysis methods to help close the gap between\ntheory and practice. We begin by showing the equivalence of neural networks\nwith parameter-varying affine maps parameterized by the state (feature) vector.\nAs the paper's main results, we provide necessary and sufficient conditions for\nthe global stability of generic deep feedforward neural networks. Further, we\nidentify links between the spectral properties of layer-wise weight\nparametrizations, different activation functions, and their effect on the\noverall network's eigenvalue spectra. We analyze a range of neural networks\nwith varying weight initializations, activation functions, bias terms, and\ndepths. Our view of neural networks as affine parameter varying maps allows us\nto \"crack open the black box\" of global neural network dynamical behavior\nthrough visualization of stationary points, regions of attraction, state-space\npartitioning, eigenvalue spectra, and stability properties. Our analysis covers\nneural networks both as an end-to-end function and component-wise without\nsimplifying assumptions or approximations. The methods we develop here provide\ntools to establish relationships between global neural dynamical properties and\ntheir constituent components which can aid in the principled design of neural\nnetworks for dynamics modeling and optimal control.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:13:16 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Drgona", "Jan", ""], ["Skomski", "Elliott", ""], ["Vasisht", "Soumya", ""], ["Tuor", "Aaron", ""], ["Vrabie", "Draguna", ""]]}, {"id": "2011.13497", "submitter": "Aaron Tuor", "authors": "Elliott Skomski, Jan Drgona, Aaron Tuor", "title": "Physics-Informed Neural State Space Models via Learning and Evolution", "comments": "Submitted to 3rd Annual Learning for Dynamics & Control Conference. 9\n  pages. 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works exploring deep learning application to dynamical systems\nmodeling have demonstrated that embedding physical priors into neural networks\ncan yield more effective, physically-realistic, and data-efficient models.\nHowever, in the absence of complete prior knowledge of a dynamical system's\nphysical characteristics, determining the optimal structure and optimization\nstrategy for these models can be difficult. In this work, we explore methods\nfor discovering neural state space dynamics models for system identification.\nStarting with a design space of block-oriented state space models and\nstructured linear maps with strong physical priors, we encode these components\ninto a model genome alongside network structure, penalty constraints, and\noptimization hyperparameters. Demonstrating the overall utility of the design\nspace, we employ an asynchronous genetic search algorithm that alternates\nbetween model selection and optimization and obtains accurate physically\nconsistent models of three physical systems: an aerodynamics body, a continuous\nstirred tank reactor, and a two tank interacting system.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:35:08 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Skomski", "Elliott", ""], ["Drgona", "Jan", ""], ["Tuor", "Aaron", ""]]}, {"id": "2011.13585", "submitter": "Debasis Mazumdar", "authors": "Debasis Mazumdar", "title": "Representation of 2D frame less visual space as a neural manifold and\n  its information geometric interpretation", "comments": "No. of pages 21, 6 figures and 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Representation of 2D frame less visual space as neural manifold and its\nmodelling in the frame work of information geometry is presented. Origin of\nhyperbolic nature of the visual space is investigated using evidences from\nneuroscience. Based on the results we propose that the processing of spatial\ninformation, particularly estimation of distance, perceiving geometrical curves\netc. in the human brain can be modeled in a parametric probability space\nendowed with Fisher-Rao metric. Compactness, convexity and differentiability of\nthe space is analysed and found that they obey the axioms of G space, proposed\nby Busemann. Further it is shown that it can be considered as a homogeneous\nRiemannian space of constant negative curvature. It is therefore ensured that\nthe space yields geodesics into it. Computer simulation of geodesics\nrepresenting a number of visual phenomena and advocating the hyperbolic\nstructure of visual space is carried out. Comparison of the simulated results\nwith the published experimental data is presented.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:21:43 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Mazumdar", "Debasis", ""]]}, {"id": "2011.13591", "submitter": "Shengran Hu", "authors": "Shengran Hu, Ran Cheng, Cheng He, Zhichao Lu", "title": "Multi-objective Neural Architecture Search with Almost No Training", "comments": "EMO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the recent past, neural architecture search (NAS) has attracted increasing\nattention from both academia and industries. Despite the steady stream of\nimpressive empirical results, most existing NAS algorithms are computationally\nprohibitive to execute due to the costly iterations of stochastic gradient\ndescent (SGD) training. In this work, we propose an effective alternative,\ndubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of\nnetwork architectures. By just training the last linear classification layer,\nRWE reduces the computational cost of evaluating an architecture from hours to\nseconds. When integrated within an evolutionary multi-objective algorithm, RWE\nobtains a set of efficient architectures with state-of-the-art performance on\nCIFAR-10 with less than two hours' searching on a single GPU card. Ablation\nstudies on rank-order correlations and transfer learning experiments to\nImageNet have further validated the effectiveness of RWE.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:39:17 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hu", "Shengran", ""], ["Cheng", "Ran", ""], ["He", "Cheng", ""], ["Lu", "Zhichao", ""]]}, {"id": "2011.13844", "submitter": "James Smith", "authors": "James E. Smith", "title": "A Temporal Neural Network Architecture for Online Learning", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing proposition is that by emulating the operation of the brain's\nneocortex, a spiking neural network (SNN) can achieve similar desirable\nfeatures: flexible learning, speed, and efficiency. Temporal neural networks\n(TNNs) are SNNs that communicate and process information encoded as relative\nspike times (in contrast to spike rates). A TNN architecture is proposed, and,\nas a proof-of-concept, TNN operation is demonstrated within the larger context\nof online supervised classification. First, through unsupervised learning, a\nTNN partitions input patterns into clusters based on similarity. The TNN then\npasses a cluster identifier to a simple online supervised decoder which\nfinishes the classification task. The TNN learning process adjusts synaptic\nweights by using only signals local to each synapse, and clustering behavior\nemerges globally. The system architecture is described at an abstraction level\nanalogous to the gate and register transfer levels in conventional digital\ndesign. Besides features of the overall architecture, several TNN components\nare new to this work. Although not addressed directly, the overall research\nobjective is a direct hardware implementation of TNNs. Consequently, all the\narchitecture elements are simple, and processing is done at very low precision.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:15:29 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 22:29:32 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Smith", "James E.", ""]]}, {"id": "2011.13965", "submitter": "Adarsha Balaji", "authors": "Adarsha Balaji and Anup Das", "title": "Compiling Spiking Neural Networks to Mitigate Neuromorphic Hardware\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking Neural Networks (SNNs) are efficient computation models to perform\nspatio-temporal pattern recognition on {resource}- and {power}-constrained\nplatforms. SNNs executed on neuromorphic hardware can further reduce energy\nconsumption of these platforms. With increasing model size and complexity,\nmapping SNN-based applications to tile-based neuromorphic hardware is becoming\nincreasingly challenging. This is attributed to the limitations of\nneuro-synaptic cores, viz. a crossbar, to accommodate only a fixed number of\npre-synaptic connections per post-synaptic neuron. For complex SNN-based models\nthat have many neurons and pre-synaptic connections per neuron, (1) connections\nmay need to be pruned after training to fit onto the crossbar resources,\nleading to a loss in model quality, e.g., accuracy, and (2) the neurons and\nsynapses need to be partitioned and placed on the neuro-sypatic cores of the\nhardware, which could lead to increased latency and energy consumption. In this\nwork, we propose (1) a novel unrolling technique that decomposes a neuron\nfunction with many pre-synaptic connections into a sequence of homogeneous\nneural units to significantly improve the crossbar utilization and retain all\npre-synaptic connections, and (2) SpiNeMap, a novel methodology to map SNNs on\nneuromorphic hardware with an aim to minimize energy consumption and spike\nlatency.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:10:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Balaji", "Adarsha", ""], ["Das", "Anup", ""]]}, {"id": "2011.14137", "submitter": "Abdul Wahab", "authors": "Abdul Wahab, Muhammad Anas Tahir, Naveed Iqbal, Faisal Shafait, Syed\n  Muhammad Raza Kazmi", "title": "Short-Term Load Forecasting using Bi-directional Sequential Models and\n  Feature Engineering for Small Datasets", "comments": "8 pages, 13 figures, 5 tables. Submitted to IEEE Transactions on\n  Power Systems, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electricity load forecasting enables the grid operators to optimally\nimplement the smart grid's most essential features such as demand response and\nenergy efficiency. Electricity demand profiles can vary drastically from one\nregion to another on diurnal, seasonal and yearly scale. Hence to devise a load\nforecasting technique that can yield the best estimates on diverse datasets,\nspecially when the training data is limited, is a big challenge. This paper\npresents a deep learning architecture for short-term load forecasting based on\nbidirectional sequential models in conjunction with feature engineering that\nextracts the hand-crafted derived features in order to aid the model for better\nlearning and predictions. In the proposed architecture, named as Deep Derived\nFeature Fusion (DeepDeFF), the raw input and hand-crafted features are trained\nat separate levels and then their respective outputs are combined to make the\nfinal prediction. The efficacy of the proposed methodology is evaluated on\ndatasets from five countries with completely different patterns. The results\ndemonstrate that the proposed technique is superior to the existing state of\nthe art.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 14:11:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wahab", "Abdul", ""], ["Tahir", "Muhammad Anas", ""], ["Iqbal", "Naveed", ""], ["Shafait", "Faisal", ""], ["Kazmi", "Syed Muhammad Raza", ""]]}, {"id": "2011.14395", "submitter": "Lennart Sch\\\"apermeier", "authors": "Lennart Sch\\\"apermeier, Christian Grimme, Pascal Kerschke", "title": "To Boldly Show What No One Has Seen Before: A Dashboard for Visualizing\n  Multi-objective Landscapes", "comments": "This version has been accepted for publication at the 11th\n  International Conference on Evolutionary Multi-Criterion Optimization (EMO\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously visualizing the decision and objective space of continuous\nmulti-objective optimization problems (MOPs) recently provided key\ncontributions in understanding the structure of their landscapes. For the sake\nof advancing these recent findings, we compiled all state-of-the-art\nvisualization methods in a single R-package (moPLOT). Moreover, we extended\nthese techniques to handle three-dimensional decision spaces and propose two\nsolutions for visualizing the resulting volume of data points. This enables -\nfor the first time - to illustrate the landscape structures of\nthree-dimensional MOPs.\n  However, creating these visualizations using the aforementioned framework\nstill lays behind a high barrier of entry for many people as it requires basic\nskills in R. To enable any user to create and explore MOP landscapes using\nmoPLOT, we additionally provide a dashboard that allows to compute the\nstate-of-the-art visualizations for a wide variety of common benchmark\nfunctions through an interactive (web-based) user interface.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 16:33:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sch\u00e4permeier", "Lennart", ""], ["Grimme", "Christian", ""], ["Kerschke", "Pascal", ""]]}, {"id": "2011.14439", "submitter": "Sam Greydanus", "authors": "Sam Greydanus", "title": "Scaling down Deep Learning", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though deep learning models have taken on commercial and political relevance,\nmany aspects of their training and operation remain poorly understood. This has\nsparked interest in \"science of deep learning\" projects, many of which are run\nat scale and require enormous amounts of time, money, and electricity. But how\nmuch of this research really needs to occur at scale? In this paper, we\nintroduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to\nclassic deep learning benchmarks. The training examples are 20 times smaller\nthan MNIST examples yet they differentiate more clearly between linear,\nnonlinear, and convolutional models which attain 32, 68, and 94% accuracy\nrespectively (these models obtain 94, 99+, and 99+% on MNIST). Then we present\nexample use cases which include measuring the spatial inductive biases of\nlottery tickets, observing deep double descent, and metalearning an activation\nfunction.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 20:08:37 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 22:09:02 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 20:09:44 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Greydanus", "Sam", ""]]}, {"id": "2011.14522", "submitter": "Greg Yang", "authors": "Greg Yang, Edward J. Hu", "title": "Feature Learning in Infinite-Width Neural Networks", "comments": "4th paper in the Tensor Programs series. Appearing in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As its width tends to infinity, a deep neural network's behavior under\ngradient descent can become simplified and predictable (e.g. given by the\nNeural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK\nparametrization). However, we show that the standard and NTK parametrizations\nof a neural network do not admit infinite-width limits that can learn features,\nwhich is crucial for pretraining and transfer learning such as with BERT. We\npropose simple modifications to the standard parametrization to allow for\nfeature learning in the limit. Using the *Tensor Programs* technique, we derive\nexplicit formulas for such limits. On Word2Vec and few-shot learning on\nOmniglot via MAML, two canonical tasks that rely crucially on feature learning,\nwe compute these limits exactly. We find that they outperform both NTK\nbaselines and finite-width networks, with the latter approaching the\ninfinite-width feature learning performance as width increases.\n  More generally, we classify a natural space of neural network\nparametrizations that generalizes standard, NTK, and Mean Field\nparametrizations. We show 1) any parametrization in this space either admits\nfeature learning or has an infinite-width training dynamics given by kernel\ngradient descent, but not both; 2) any such infinite-width limit can be\ncomputed using the Tensor Programs technique. Code for our experiments can be\nfound at github.com/edwardjhu/TP4.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 03:21:05 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 08:04:47 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Yang", "Greg", ""], ["Hu", "Edward J.", ""]]}, {"id": "2011.14709", "submitter": "James You Sian Tan", "authors": "James Y. S. Tan, Zengguang Cheng, Xuan Li, Nathan Youngblood, Utku E.\n  Ali, C. David Wright, Wolfram H. P. Pernice and Harish Bhaskaran", "title": "Monadic Pavlovian associative learning in a backpropagation-free\n  photonic network", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.NE physics.app-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over a century ago, Ivan P. Pavlov, in a classic experiment, demonstrated how\ndogs can learn to associate a ringing bell with food, thereby causing a ring to\nresult in salivation. Today, however, it is rare to find the use of Pavlovian\ntype associative learning for artificial intelligence (AI) applications.\nInstead, other biologically-inspired learning concepts, in particular\nartificial neural networks (ANNs) have flourished, yielding extensive impact on\na wide range of fields including finance, healthcare and transportation.\nHowever, learning in such \"conventional\" ANNs, in particular in the form of\nmodern deep neural networks (DNNs) are usually carried out using the\nbackpropagation method, is computationally and energy intensive. Here we report\nthe experimental demonstration of backpropagation-free learning, achieved using\na single (or monadic) associative hardware element. This is realized on an\nintegrated photonic platform using phase change materials combined with on-chip\ncascaded directional couplers. We link associative learning with supervised\nlearning, based on their common goal of associating certain inputs with\n\"correct\" outputs. We then expand the concept to develop larger-scale\nsupervised learning networks using our monadic Pavlovian photonic hardware,\ndeveloping a distinct machine-learning framework based on single-element\nassociations and, importantly, using backpropagation-free single-layer weight\narchitectures to approach general learning tasks. Our approach not only\nsignificantly reduces the computational burden imposed by learning in\nconventional neural network approaches, thereby increasing speed and decreasing\nenergy use during learning, but also offers higher bandwidth inherent to a\nphotonic implementation, paving the way for future deployment of fast photonic\nartificially intelligent machines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:41:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Tan", "James Y. S.", ""], ["Cheng", "Zengguang", ""], ["Li", "Xuan", ""], ["Youngblood", "Nathan", ""], ["Ali", "Utku E.", ""], ["Wright", "C. David", ""], ["Pernice", "Wolfram H. P.", ""], ["Bhaskaran", "Harish", ""]]}, {"id": "2011.14901", "submitter": "Annika Lindh", "authors": "Annika Lindh, Robert J. Ross, John D. Kelleher", "title": "Language-Driven Region Pointer Advancement for Controllable Image\n  Captioning", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controllable Image Captioning is a recent sub-field in the multi-modal task\nof Image Captioning wherein constraints are placed on which regions in an image\nshould be described in the generated natural language caption. This puts a\nstronger focus on producing more detailed descriptions, and opens the door for\nmore end-user control over results. A vital component of the Controllable Image\nCaptioning architecture is the mechanism that decides the timing of attending\nto each region through the advancement of a region pointer. In this paper, we\npropose a novel method for predicting the timing of region pointer advancement\nby treating the advancement step as a natural part of the language structure\nvia a NEXT-token, motivated by a strong correlation to the sentence structure\nin the training data. We find that our timing agrees with the ground-truth\ntiming in the Flickr30k Entities test data with a precision of 86.55% and a\nrecall of 97.92%. Our model implementing this technique improves the\nstate-of-the-art on standard captioning metrics while additionally\ndemonstrating a considerably larger effective vocabulary size.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:34:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lindh", "Annika", ""], ["Ross", "Robert J.", ""], ["Kelleher", "John D.", ""]]}, {"id": "2011.15027", "submitter": "Anatolii Mokshin", "authors": "Anatolii V. Mokshin, Vladimir V. Mokshin and Diana A. Mirziyarova", "title": "Formation of Regression Model for Analysis of Complex Systems Using\n  Methodology of Genetic Algorithms", "comments": "8 pages, 3 figures, 1 table", "journal-ref": "Nonlinear Phenomena in Complex Systems 23 (2020) 317-326", "doi": "10.33581/1561-4085-2020-23-3-317-326", "report-no": null, "categories": "physics.data-an cs.NE physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents the approach to analyzing the evolution of an arbitrary\ncomplex system whose behavior is characterized by a set of different\ntime-dependent factors. The key requirement for these factors is only that they\nmust contain an information about the system; it does not matter at all what\nthe nature (physical, biological, social, economic, etc.) of a complex system\nis. Within the framework of the presented theoretical approach, the problem of\nsearching for non-linear regression models that express the relationship\nbetween these factors for a complex system under study is solved. It will be\nshown that this problem can be solved using the methodology of \\emph{genetic\n(evolutionary)} algorithms. The resulting regression models make it possible to\npredict the most probable evolution of the considered system, as well as to\ndetermine the significance of some factors and, thereby, to formulate some\nrecommendations to drive by this system. It will be shown that the presented\ntheoretical approach can be used to analyze the data (information)\ncharacterizing the educational process in the discipline \"Physics\" in the\nsecondary school, and to develop the strategies for improving academic\nperformance in this discipline.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 11:02:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Mokshin", "Anatolii V.", ""], ["Mokshin", "Vladimir V.", ""], ["Mirziyarova", "Diana A.", ""]]}, {"id": "2011.15031", "submitter": "Siavash Golkar", "authors": "Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan M. Sengupta,\n  Dmitri B. Chklovskii", "title": "A biologically plausible neural network for local supervision in\n  cortical microcircuits", "comments": "Abstract presented at the NeurIPS 2020 workshop \"Beyond\n  Backpropagation\". arXiv admin note: text overlap with arXiv:2010.12660", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The backpropagation algorithm is an invaluable tool for training artificial\nneural networks; however, because of a weight sharing requirement, it does not\nprovide a plausible model of brain function. Here, in the context of a\ntwo-layer network, we derive an algorithm for training a neural network which\navoids this problem by not requiring explicit error computation and\nbackpropagation. Furthermore, our algorithm maps onto a neural network that\nbears a remarkable resemblance to the connectivity structure and learning rules\nof the cortex. We find that our algorithm empirically performs comparably to\nbackprop on a number of datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:35:22 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Golkar", "Siavash", ""], ["Lipshutz", "David", ""], ["Bahroun", "Yanis", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}]