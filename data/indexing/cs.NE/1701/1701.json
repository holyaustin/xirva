[{"id": "1701.00736", "submitter": "Hossein Hosseini", "authors": "S. Hossein Hosseini, Tohid Nouri, Afshin Ebrahimi, S. Ali Hosseini", "title": "Simulated Tornado Optimization", "comments": "6 pages, 15 figures, 1 table, IEEE International Conference on Signal\n  Processing and Intelligent System (ICSPIS16), Dec. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a swarm-based optimization algorithm inspired by air currents of a\ntornado. Two main air currents - spiral and updraft - are mimicked. Spiral\nmotion is designed for exploration of new search areas and updraft movements is\ndeployed for exploitation of a promising candidate solution. Assignment of just\none search direction to each particle at each iteration, leads to low\ncomputational complexity of the proposed algorithm respect to the conventional\nalgorithms. Regardless of the step size parameters, the only parameter of the\nproposed algorithm, called tornado diameter, can be efficiently adjusted by\nrandomization. Numerical results over six different benchmark cost functions\nindicate comparable and, in some cases, better performance of the proposed\nalgorithm respect to some other metaheuristics.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 11:28:23 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Hosseini", "S. Hossein", ""], ["Nouri", "Tohid", ""], ["Ebrahimi", "Afshin", ""], ["Hosseini", "S. Ali", ""]]}, {"id": "1701.00879", "submitter": "Xingyi Zhang", "authors": "Ye Tian, Ran Cheng, Xingyi Zhang and Yaochu Jin", "title": "PlatEMO: A MATLAB Platform for Evolutionary Multi-Objective Optimization", "comments": "20 pages, 12 figures, 4 tables", "journal-ref": "IEEE Computational Intelligence Magazine, 2017, 12(4): 73-87", "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last three decades, a large number of evolutionary algorithms have\nbeen developed for solving multiobjective optimization problems. However, there\nlacks an up-to-date and comprehensive software platform for researchers to\nproperly benchmark existing algorithms and for practitioners to apply selected\nalgorithms to solve their real-world problems. The demand of such a common tool\nbecomes even more urgent, when the source code of many proposed algorithms has\nnot been made publicly available. To address these issues, we have developed a\nMATLAB platform for evolutionary multi-objective optimization in this paper,\ncalled PlatEMO, which includes more than 50 multi-objective evolutionary\nalgorithms and more than 100 multi-objective test problems, along with several\nwidely used performance indicators. With a user-friendly graphical user\ninterface, PlatEMO enables users to easily compare several evolutionary\nalgorithms at one time and collect statistical results in Excel or LaTeX files.\nMore importantly, PlatEMO is completely open source, such that users are able\nto develop new algorithms on the basis of it. This paper introduces the main\nfeatures of PlatEMO and illustrates how to use it for performing comparative\nexperiments, embedding new algorithms, creating new test problems, and\ndeveloping performance indicators. Source code of PlatEMO is now available at:\nhttp://bimk.ahu.edu.cn/index.php?s=/Index/Software/index.html.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 00:52:49 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Tian", "Ye", ""], ["Cheng", "Ran", ""], ["Zhang", "Xingyi", ""], ["Jin", "Yaochu", ""]]}, {"id": "1701.01036", "submitter": "Yanghao Li", "authors": "Yanghao Li, Naiyan Wang, Jiaying Liu and Xiaodi Hou", "title": "Demystifying Neural Style Transfer", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Style Transfer has recently demonstrated very exciting results which\ncatches eyes in both academia and industry. Despite the amazing results, the\nprinciple of neural style transfer, especially why the Gram matrices could\nrepresent style remains unclear. In this paper, we propose a novel\ninterpretation of neural style transfer by treating it as a domain adaptation\nproblem. Specifically, we theoretically show that matching the Gram matrices of\nfeature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with\nthe second order polynomial kernel. Thus, we argue that the essence of neural\nstyle transfer is to match the feature distributions between the style images\nand the generated images. To further support our standpoint, we experiment with\nseveral other distribution alignment methods, and achieve appealing results. We\nbelieve this novel interpretation connects these two important research fields,\nand could enlighten future researches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 14:54:20 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 13:21:11 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Li", "Yanghao", ""], ["Wang", "Naiyan", ""], ["Liu", "Jiaying", ""], ["Hou", "Xiaodi", ""]]}, {"id": "1701.01214", "submitter": "Reza Yousefian", "authors": "Reza Yousefian and Sukumar Kamalasadan", "title": "A Review of Neural Network Based Machine Learning Approaches for Rotor\n  Angle Stability Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the current status and challenges of Neural Networks (NNs)\nbased machine learning approaches for modern power grid stability control\nincluding their design and implementation methodologies. NNs are widely\naccepted as Artificial Intelligence (AI) approaches offering an alternative way\nto control complex and ill-defined problems. In this paper various application\nof NNs for power system rotor angle stabilization and control problem is\ndiscussed. The main focus of this paper is on the use of Reinforcement Learning\n(RL) and Supervised Learning (SL) algorithms in power system wide-area control\n(WAC). Generally, these algorithms due to their capability in modeling\nnonlinearities and uncertainties are used for transient classification,\nneuro-control, wide-area monitoring and control, renewable energy management\nand control, and so on. The works of researchers in the field of conventional\nand renewable energy systems are reported and categorized. Paper concludes by\npresenting, comparing and evaluating various learning techniques and\ninfrastructure configurations based on efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 05:26:45 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Yousefian", "Reza", ""], ["Kamalasadan", "Sukumar", ""]]}, {"id": "1701.01271", "submitter": "Chengjun Li", "authors": "Chengjun Li and Jia Wu", "title": "Subpopulation Diversity Based Selecting Migration Moment in Distributed\n  Evolutionary Algorithms", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed evolutionary algorithms, migration interval is used to decide\nmigration moments. Nevertheless, migration moments predetermined by intervals\ncannot match the dynamic situation of evolution. In this paper, a scheme of\nsetting the success rate of migration based on subpopulation diversity at each\ninterval is proposed. With the scheme, migration still occurs at intervals, but\nthe probability of immigrants entering the target subpopulation will be\ndetermined by the diversity of this subpopulation according to a proposed\nformula. An analysis shows that the time consumption of our scheme is\nacceptable. In our experiments, the basement of parallelism is an evolutionary\nalgorithm for the traveling salesman problem. Under different value\ncombinations of parameters for the formula, outcomes for eight benchmark\ninstances of the distributed evolutionary algorithm with the proposed scheme\nare compared with those of a traditional one, respectively. Results show that\nthe distributed evolutionary algorithm based on our scheme has a significant\nadvantage on solutions especially for high difficulty instances. Moreover, it\ncan be seen that the algorithm with the scheme has the most outstanding\nperformance under three value combinations of above-mentioned parameters for\nthe formula.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 10:32:24 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Li", "Chengjun", ""], ["Wu", "Jia", ""]]}, {"id": "1701.01272", "submitter": "Weishan Dong", "authors": "Weishan Dong, Ting Yuan, Kai Yang, Changsheng Li, Shilei Zhang", "title": "Autoencoder Regularized Network For Driving Style Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study learning generalized driving style representations\nfrom automobile GPS trip data. We propose a novel Autoencoder Regularized deep\nneural Network (ARNet) and a trip encoding framework trip2vec to learn drivers'\ndriving styles directly from GPS records, by combining supervised and\nunsupervised feature learning in a unified architecture. Experiments on a\nchallenging driver number estimation problem and the driver identification\nproblem show that ARNet can learn a good generalized driving style\nrepresentation: It significantly outperforms existing methods and alternative\narchitectures by reaching the least estimation error on average (0.68, less\nthan one driver) and the highest identification accuracy (by at least 3%\nimprovement) compared with traditional supervised learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 10:38:07 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Dong", "Weishan", ""], ["Yuan", "Ting", ""], ["Yang", "Kai", ""], ["Li", "Changsheng", ""], ["Zhang", "Shilei", ""]]}, {"id": "1701.01329", "submitter": "Marwin Segler", "authors": "Marwin H.S. Segler, Thierry Kogej, Christian Tyrchan, Mark P. Waller", "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent\n  Neural Networks", "comments": "17 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In de novo drug design, computational strategies are used to generate novel\nmolecules with good affinity to the desired biological target. In this work, we\nshow that recurrent neural networks can be trained as generative models for\nmolecular structures, similar to statistical language models in natural\nlanguage processing. We demonstrate that the properties of the generated\nmolecules correlate very well with the properties of the molecules used to\ntrain the model. In order to enrich libraries with molecules active towards a\ngiven biological target, we propose to fine-tune the model with small sets of\nmolecules, which are known to be active against that target.\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\nmolecules that medicinal chemists designed, whereas against Plasmodium\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\nwith a scoring function, our model can perform the complete de novo drug design\ncycle to generate large sets of novel molecules for drug discovery.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 14:28:34 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Segler", "Marwin H. S.", ""], ["Kogej", "Thierry", ""], ["Tyrchan", "Christian", ""], ["Waller", "Mark P.", ""]]}, {"id": "1701.01495", "submitter": "Sadique Sheik", "authors": "Sadique Sheik, Somnath Paul, Charles Augustine, Gert Cauwenberghs", "title": "Membrane-Dependent Neuromorphic Learning Rule for Unsupervised Spike\n  Pattern Detection", "comments": "Published in IEEE BioCAS 2016 Proceedings, BioMedical Circuits and\n  Systems Conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several learning rules for synaptic plasticity, that depend on either spike\ntiming or internal state variables, have been proposed in the past imparting\nvarying computational capabilities to Spiking Neural Networks. Due to design\ncomplications these learning rules are typically not implemented on\nneuromorphic devices leaving the devices to be only capable of inference. In\nthis work we propose a unidirectional post-synaptic potential dependent\nlearning rule that is only triggered by pre-synaptic spikes, and easy to\nimplement on hardware. We demonstrate that such a learning rule is functionally\ncapable of replicating computational capabilities of pairwise STDP. Further\nmore, we demonstrate that this learning rule can be used to learn and classify\nspatio-temporal spike patterns in an unsupervised manner using individual\nneurons. We argue that this learning rule is computationally powerful and also\nideal for hardware implementations due to its unidirectional memory access.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 22:57:04 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Sheik", "Sadique", ""], ["Paul", "Somnath", ""], ["Augustine", "Charles", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1701.01791", "submitter": "Wei Wen", "authors": "Yandan Wang, Wei Wen, Linghao Song, and Hai Li", "title": "Classification Accuracy Improvement for Neuromorphic Computing Systems\n  with One-level Precision Synapses", "comments": "Best Paper Award of ASP-DAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain inspired neuromorphic computing has demonstrated remarkable advantages\nover traditional von Neumann architecture for its high energy efficiency and\nparallel data processing. However, the limited resolution of synaptic weights\ndegrades system accuracy and thus impedes the use of neuromorphic systems. In\nthis work, we propose three orthogonal methods to learn synapses with one-level\nprecision, namely, distribution-aware quantization, quantization regularization\nand bias tuning, to make image classification accuracy comparable to the\nstate-of-the-art. Experiments on both multi-layer perception and convolutional\nneural networks show that the accuracy drop can be well controlled within 0.19%\n(5.53%) for MNIST (CIFAR-10) database, compared to an ideal system without\nquantization.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 05:01:15 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Wang", "Yandan", ""], ["Wen", "Wei", ""], ["Song", "Linghao", ""], ["Li", "Hai", ""]]}, {"id": "1701.01811", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Alexandros Potamianos", "title": "Structural Attention Neural Networks for improved sentiment analysis", "comments": "Submitted to EACL2017 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a tree-structured attention neural network for sentences and\nsmall phrases and apply it to the problem of sentiment classification. Our\nmodel expands the current recursive models by incorporating structural\ninformation around a node of a syntactic tree using both bottom-up and top-down\ninformation propagation. Also, the model utilizes structural attention to\nidentify the most salient representations during the construction of the\nsyntactic tree. To our knowledge, the proposed models achieve state of the art\nperformance on the Stanford Sentiment Treebank dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 09:58:49 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1701.02810", "submitter": "Alexander M. Rush", "authors": "Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M.\n  Rush", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "comments": "Report for http://opennmt.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an open-source toolkit for neural machine translation (NMT). The\ntoolkit prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 23:32:43 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 15:54:27 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Klein", "Guillaume", ""], ["Kim", "Yoon", ""], ["Deng", "Yuntian", ""], ["Senellart", "Jean", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1701.03281", "submitter": "Tao Wei", "authors": "Tao Wei, Changhu Wang, Chang Wen Chen", "title": "Modularized Morphing of Neural Networks", "comments": "12 pages, 6 figures, Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the problem of network morphism, an effective learning\nscheme to morph a well-trained neural network to a new one with the network\nfunction completely preserved. Different from existing work where basic\nmorphing types on the layer level were addressed, we target at the central\nproblem of network morphism at a higher level, i.e., how a convolutional layer\ncan be morphed into an arbitrary module of a neural network. To simplify the\nrepresentation of a network, we abstract a module as a graph with blobs as\nvertices and convolutional layers as edges, based on which the morphing process\nis able to be formulated as a graph transformation problem. Two atomic morphing\noperations are introduced to compose the graphs, based on which modules are\nclassified into two families, i.e., simple morphable modules and complex\nmodules. We present practical morphing solutions for both of these two\nfamilies, and prove that any reasonable module can be morphed from a single\nconvolutional layer. Extensive experiments have been conducted based on the\nstate-of-the-art ResNet on benchmark datasets, and the effectiveness of the\nproposed solution has been verified.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 09:48:53 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Wei", "Tao", ""], ["Wang", "Changhu", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1701.03441", "submitter": "Fathi Salem", "authors": "Yuzhen Lu and Fathi M. Salem", "title": "Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural\n  Networks", "comments": "5 pages, 4 Figures, 3 Tables. arXiv admin note: substantial text\n  overlap with arXiv:1612.03707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard LSTM recurrent neural networks while very powerful in long-range\ndependency sequence applications have highly complex structure and relatively\nlarge (adaptive) parameters. In this work, we present empirical comparison\nbetween the standard LSTM recurrent neural network architecture and three new\nparameter-reduced variants obtained by eliminating combinations of the input\nsignal, bias, and hidden unit signals from individual gating signals. The\nexperiments on two sequence datasets show that the three new variants, called\nsimply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the\nstandard LSTM model with less (adaptive) parameters.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:12:05 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Lu", "Yuzhen", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1701.03452", "submitter": "Fathi Salem", "authors": "Joel Heck and Fathi M. Salem", "title": "Simplified Minimal Gated Unit Variations for Recurrent Neural Networks", "comments": "5 pages, 3 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks with various types of hidden units have been used\nto solve a diverse range of problems involving sequence data. Two of the most\nrecent proposals, gated recurrent units (GRU) and minimal gated units (MGU),\nhave shown comparable promising results on example public datasets. In this\npaper, we introduce three model variants of the minimal gated unit (MGU) which\nfurther simplify that design by reducing the number of parameters in the\nforget-gate dynamic equation. These three model variants, referred to simply as\nMGU1, MGU2, and MGU3, were tested on sequences generated from the MNIST dataset\nand from the Reuters Newswire Topics (RNT) dataset. The new models have shown\nsimilar accuracy to the MGU model while using fewer parameters and thus\nlowering training expense. One model variant, namely MGU2, performed better\nthan MGU on the datasets considered, and thus may be used as an alternate to\nMGU or GRU in recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:52:31 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Heck", "Joel", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1701.03682", "submitter": "Emrah Budur", "authors": "Priyank Mathur, Arkajyoti Misra, Emrah Budur", "title": "LIDE: Language Identification from Text Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in the use of microblogging came along with the rapid growth on\nshort linguistic data. On the other hand deep learning is considered to be the\nnew frontier to extract meaningful information out of large amount of raw data\nin an automated manner. In this study, we engaged these two emerging fields to\ncome up with a robust language identifier on demand, namely Language\nIdentification Engine (LIDE). As a result, we achieved 95.12% accuracy in\nDiscriminating between Similar Languages (DSL) Shared Task 2015 dataset, which\nis comparable to the maximum reported accuracy of 95.54% achieved so far.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 14:20:06 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Mathur", "Priyank", ""], ["Misra", "Arkajyoti", ""], ["Budur", "Emrah", ""]]}, {"id": "1701.03866", "submitter": "Steven Hansen", "authors": "Steven Stenberg Hansen", "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory", "comments": "Accepted into the NIPS 2016 workshop on Continual Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit assignment in traditional recurrent neural networks usually involves\nback-propagating through a long chain of tied weight matrices. The length of\nthis chain scales linearly with the number of time-steps as the same network is\nrun at each time-step. This creates many problems, such as vanishing gradients,\nthat have been well studied. In contrast, a NNEM's architecture recurrent\nactivity doesn't involve a long chain of activity (though some architectures\nsuch as the NTM do utilize a traditional recurrent architecture as a\ncontroller). Rather, the externally stored embedding vectors are used at each\ntime-step, but no messages are passed from previous time-steps. This means that\nvanishing gradients aren't a problem, as all of the necessary gradient paths\nare short. However, these paths are extremely numerous (one per embedding\nvector in memory) and reused for a very long time (until it leaves the memory).\nThus, the forward-pass information of each memory must be stored for the entire\nduration of the memory. This is problematic as this additional storage far\nsurpasses that of the actual memories, to the extent that large memories on\ninfeasible to back-propagate through in high dimensional settings. One way to\nget around the need to hold onto forward-pass information is to recalculate the\nforward-pass whenever gradient information is available. However, if the\nobservations are too large to store in the domain of interest, direct\nreinstatement of a forward pass cannot occur. Instead, we rely on a learned\nautoencoder to reinstate the observation, and then use the embedding network to\nrecalculate the forward-pass. Since the recalculated embedding vector is\nunlikely to perfectly match the one stored in memory, we try out 2\napproximations to utilize error gradient w.r.t. the vector in memory.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 01:47:54 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Hansen", "Steven Stenberg", ""]]}, {"id": "1701.04313", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Andrew Rosenberg, Abhinav Sethy, Bhuvana\n  Ramabhadran, Brian Kingsbury", "title": "End-to-End ASR-free Keyword Search from Speech", "comments": "Published in the IEEE 2017 International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2017), scheduled for 5-9 March 2017 in\n  New Orleans, Louisiana, USA", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2759726", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 15:05:39 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Rosenberg", "Andrew", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Kingsbury", "Brian", ""]]}, {"id": "1701.04383", "submitter": "Hasan Ali Aky\\\"urek", "authors": "Hasan Ali Aky\\\"urek, Erkan \\\"Ulker, Bar{\\i}\\c{s} Ko\\c{c}er", "title": "Automatic Knot Adjustment Using Dolphin Echolocation Algorithm for\n  B-Spline Curve Approximation", "comments": "The Journal of MacroTrends in Technology and Innovation, Vol 4. Issue\n  1. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a new approach to solve the cubic B-spline curve fitting\nproblem is presented based on a meta-heuristic algorithm called \" dolphin\necholocation \". The method minimizes the proximity error value of the selected\nnodes that measured using the least squares method and the Euclidean distance\nmethod of the new curve generated by the reverse engineering. The results of\nthe proposed method are compared with the genetic algorithm. As a result, this\nnew method seems to be successful.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:20:32 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Aky\u00fcrek", "Hasan Ali", ""], ["\u00dclker", "Erkan", ""], ["Ko\u00e7er", "Bar\u0131\u015f", ""]]}, {"id": "1701.04465", "submitter": "Aditya Sharma", "authors": "Aditya Sharma, Nikolas Wolfe, Bhiksha Raj", "title": "The Incredible Shrinking Neural Network: New Perspectives on Learning\n  Representations Through The Lens of Pruning", "comments": "30 pages, 36 figures, submission to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How much can pruning algorithms teach us about the fundamentals of learning\nrepresentations in neural networks? And how much can these fundamentals help\nwhile devising new pruning techniques? A lot, it turns out. Neural network\npruning has become a topic of great interest in recent years, and many\ndifferent techniques have been proposed to address this problem. The decision\nof what to prune and when to prune necessarily forces us to confront our\nassumptions about how neural networks actually learn to represent patterns in\ndata. In this work, we set out to test several long-held hypotheses about\nneural network learning representations, approaches to pruning and the\nrelevance of one in the context of the other. To accomplish this, we argue in\nfavor of pruning whole neurons as opposed to the traditional method of pruning\nweights from optimally trained networks. We first review the historical\nliterature, point out some common assumptions it makes, and propose methods to\ndemonstrate the inherent flaws in these assumptions. We then propose our novel\napproach to pruning and set about analyzing the quality of the decisions it\nmakes. Our analysis led us to question the validity of many widely-held\nassumptions behind pruning algorithms and the trade-offs we often make in the\ninterest of reducing computational complexity. We discovered that there is a\nstraightforward way, however expensive, to serially prune 40-70% of the neurons\nin a trained network with minimal effect on the learning representation and\nwithout any re-training. It is to be noted here that the motivation behind this\nwork is not to propose an algorithm that would outperform all existing methods,\nbut to shed light on what some inherent flaws in these methods can teach us\nabout learning representations and how this can lead us to superior pruning\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 21:49:47 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 09:15:28 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sharma", "Aditya", ""], ["Wolfe", "Nikolas", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1701.04949", "submitter": "Volodymyr Turchenko", "authors": "Volodymyr Turchenko, Eric Chalmers, Artur Luczak", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in\n  Caffe", "comments": "21 pages, 11 figures, 5 tables, 62 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of several models of a deep convolutional\nauto-encoder in the Caffe deep learning framework and their experimental\nevaluation on the example of MNIST dataset. We have created five models of a\nconvolutional auto-encoder which differ architecturally by the presence or\nabsence of pooling and unpooling layers in the auto-encoder's encoder and\ndecoder parts. Our results show that the developed models provide very good\nresults in dimensionality reduction and unsupervised clustering tasks, and\nsmall classification errors when we used the learned internal code as an input\nof a supervised linear classifier and multi-layer perceptron. The best results\nwere provided by a model where the encoder part contains convolutional and\npooling layers, followed by an analogous decoder part with deconvolution and\nunpooling layers without the use of switch variables in the decoder part. The\npaper also discusses practical details of the creation of a deep convolutional\nauto-encoder in the very popular Caffe deep learning framework. We believe that\nour approach and results presented in this paper could help other researchers\nto build efficient deep neural network architectures in the future.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 05:24:24 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Turchenko", "Volodymyr", ""], ["Chalmers", "Eric", ""], ["Luczak", "Artur", ""]]}, {"id": "1701.05004", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Thomas R. Shultz", "title": "Converting Cascade-Correlation Neural Nets into Probabilistic Generative\n  Models", "comments": null, "journal-ref": "Proceedings of the 39th Annual Conference of the Cognitive Science\n  Society (2017) (pp. 1029-1034). Austin, TX: Cognitive Science Society", "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are not only adept in recognizing what class an input instance belongs\nto (i.e., classification task), but perhaps more remarkably, they can imagine\n(i.e., generate) plausible instances of a desired class with ease, when\nprompted. Inspired by this, we propose a framework which allows transforming\nCascade-Correlation Neural Networks (CCNNs) into probabilistic generative\nmodels, thereby enabling CCNNs to generate samples from a category of interest.\nCCNNs are a well-known class of deterministic, discriminative NNs, which\nautonomously construct their topology, and have been successful in giving\naccounts for a variety of psychological phenomena. Our proposed framework is\nbased on a Markov Chain Monte Carlo (MCMC) method, called the\nMetropolis-adjusted Langevin algorithm, which capitalizes on the gradient\ninformation of the target distribution to direct its explorations towards\nregions of high probability, thereby achieving good mixing properties. Through\nextensive simulations, we demonstrate the efficacy of our proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 10:51:58 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Shultz", "Thomas R.", ""]]}, {"id": "1701.05121", "submitter": "Keyan Ghazi-Zahedi", "authors": "Keyan Ghazi-Zahedi", "title": "NMODE --- Neuro-MODule Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularisation, repetition, and symmetry are structural features shared by\nalmost all biological neural networks. These features are very unlikely to be\nfound by the means of structural evolution of artificial neural networks. This\npaper introduces NMODE, which is specifically designed to operate on\nneuro-modules. NMODE addresses a second problem in the context of evolutionary\nrobotics, which is incremental evolution of complex behaviours for complex\nmachines, by offering a way to interface neuro-modules. The scenario in mind is\na complex walking machine, for which a locomotion module is evolved first, that\nis then extended by other modules in later stages. We show that NMODE is able\nto evolve a locomotion behaviour for a standard six-legged walking machine in\napproximately 10 generations and show how it can be used for incremental\nevolution of a complex walking machine. The entire source code used in this\npaper is publicly available through GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 15:51:32 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Ghazi-Zahedi", "Keyan", ""]]}, {"id": "1701.05130", "submitter": "Rendani Mbuvha", "authors": "Ludvig Ericson, Rendani Mbuvha", "title": "On the Performance of Network Parallel Training in Artificial Neural\n  Networks", "comments": "4 Pages, 4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) have received increasing attention in\nrecent years with applications that span a wide range of disciplines including\nvital domains such as medicine, network security and autonomous transportation.\nHowever, neural network architectures are becoming increasingly complex and\nwith an increasing need to obtain real-time results from such models, it has\nbecome pivotal to use parallelization as a mechanism for speeding up network\ntraining and deployment. In this work we propose an implementation of Network\nParallel Training through Cannon's Algorithm for matrix multiplication. We show\nthat increasing the number of processes speeds up training until the point\nwhere process communication costs become prohibitive; this point varies by\nnetwork complexity. We also show through empirical efficiency calculations that\nthe speedup obtained is superlinear.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:17:35 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Ericson", "Ludvig", ""], ["Mbuvha", "Rendani", ""]]}, {"id": "1701.05159", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Michael Kampffmeyer, Enrico Maiorino and Robert\n  Jenssen", "title": "Temporal Overdrive Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel recurrent neural network architecture\ndesigned to model systems characterized by multiple characteristic timescales\nin their dynamics. The proposed network is composed by several recurrent groups\nof neurons that are trained to separately adapt to each timescale, in order to\nimprove the system identification process. We test our framework on time series\nprediction tasks and we show some promising, preliminary results achieved on\nsynthetic data. To evaluate the capabilities of our network, we compare the\nperformance with several state-of-the-art recurrent architectures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 17:37:35 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Kampffmeyer", "Michael", ""], ["Maiorino", "Enrico", ""], ["Jenssen", "Robert", ""]]}, {"id": "1701.05221", "submitter": "Nikolaos Fragoulis Dr", "authors": "I. Theodorakopoulos, V. Pothos, D. Kastaniotis and N. Fragoulis", "title": "Parsimonious Inference on Convolutional Neural Networks: Learning and\n  applying on-line kernel activation rules", "comments": "17 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new, radical CNN design approach is presented in this paper, considering\nthe reduction of the total computational load during inference. This is\nachieved by a new holistic intervention on both the CNN architecture and the\ntraining procedure, which targets to the parsimonious inference by learning to\nexploit or remove the redundant capacity of a CNN architecture. This is\naccomplished, by the introduction of a new structural element that can be\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\nor even improving its recognition accuracy. Our approach formulates a\nsystematic and data-driven method for developing CNNs that are trained to\neventually change size and form in real-time during inference, targeting to the\nsmaller possible computational footprint. Results are provided for the optimal\nimplementation on a few modern, high-end mobile computing platforms indicating\na significant speed-up of up to x3 times.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:03:12 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 06:43:02 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 08:57:29 GMT"}, {"version": "v4", "created": "Thu, 26 Jan 2017 08:58:52 GMT"}, {"version": "v5", "created": "Tue, 31 Jan 2017 12:15:43 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Theodorakopoulos", "I.", ""], ["Pothos", "V.", ""], ["Kastaniotis", "D.", ""], ["Fragoulis", "N.", ""]]}, {"id": "1701.05549", "submitter": "Krzysztof Cios", "authors": "Krzysztof J. Cios", "title": "Deep Neural Networks - A Brief History", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction to deep neural networks and their history.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 18:43:56 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Cios", "Krzysztof J.", ""]]}, {"id": "1701.05553", "submitter": "Casey Kneale", "authors": "Casey Kneale, Dominic Poerio, Karl S. Booksh", "title": "Optimized Spatial Partitioning via Minimal Swarm Intelligence", "comments": "To be submitted to the Journal of Technometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimized spatial partitioning algorithms are the corner stone of many\nsuccessful experimental designs and statistical methods. Of these algorithms,\nthe Centroidal Voronoi Tessellation (CVT) is the most widely utilized. CVT\nbased methods require global knowledge of spatial boundaries, do not readily\nallow for weighted regions, have challenging implementations, and are\ninefficiently extended to high dimensional spaces. We describe two simple\npartitioning schemes based on nearest and next nearest neighbor locations which\neasily incorporate these features at the slight expense of optimal placement.\nSeveral novel qualitative techniques which assess these partitioning schemes\nare also included. The feasibility of autonomous uninformed sensor networks\nutilizing these algorithms are considered. Some improvements in particle swarm\noptimizer results on multimodal test functions from partitioned initial\npositions in two space are also illustrated. Pseudo code for all of the novel\nalgorithms depicted here-in is available in the supplementary information of\nthis manuscript.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 18:59:04 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Kneale", "Casey", ""], ["Poerio", "Dominic", ""], ["Booksh", "Karl S.", ""]]}, {"id": "1701.05730", "submitter": "Michal Gregor", "authors": "Michal Gregor and Juraj Spalek", "title": "Using LLVM-based JIT Compilation in Genetic Programming", "comments": "Link to the IEEE published version:\n  http://ieeexplore.ieee.org/document/7512108/", "journal-ref": null, "doi": "10.1109/ELEKTRO.2016.7512108", "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes an approach to implementing genetic programming, which\nuses the LLVM library to just-in-time compile/interpret the evolved abstract\nsyntax trees. The solution is described in some detail, including a parser\n(based on FlexC++ and BisonC++) that can construct the trees from a simple toy\nlanguage with C-like syntax. The approach is compared with a previous\nimplementation (based on direct execution of trees using polymorphic functors)\nin terms of execution speed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 09:17:52 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Gregor", "Michal", ""], ["Spalek", "Juraj", ""]]}, {"id": "1701.05818", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (Palaiseau, OBELIX), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Fusion of Heterogeneous Data in Convolutional Networks for Urban\n  Semantic Labeling (Invited Paper)", "comments": "Joint Urban Remote Sensing Event (JURSE), Mar 2017, Dubai, United\n  Arab Emirates. Joint Urban Remote Sensing Event 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel module to perform fusion of heterogeneous\ndata using fully convolutional networks for semantic labeling. We introduce\nresidual correction as a way to learn how to fuse predictions coming out of a\ndual stream architecture. Especially, we perform fusion of DSM and IRRG optical\ndata on the ISPRS Vaihingen dataset over a urban area and obtain new\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 15:10:09 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Audebert", "Nicolas", "", "Palaiseau, OBELIX"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1701.05923", "submitter": "Fathi Salem", "authors": "Rahul Dey and Fathi M. Salem", "title": "Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks", "comments": "5 pages, 8 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in\nrecurrent neural networks (RNN) by reducing parameters in the update and reset\ngates. We evaluate the three variant GRU models on MNIST and IMDB datasets and\nshow that these GRU-RNN variant models perform as well as the original GRU RNN\nmodel while reducing the computational expense.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 20:53:51 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Dey", "Rahul", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1701.05935", "submitter": "Ke Li Kl", "authors": "Ke Li, Kalyanmoy Deb and Xin Yao", "title": "Integration of Preferences in Decomposition Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing studies on evolutionary multi-objective optimization focus on\napproximating the whole Pareto-optimal front. Nevertheless, rather than the\nwhole front, which demands for too many points (especially in a\nhigh-dimensional space), the decision maker might only interest in a partial\nregion, called the region of interest. In this case, solutions outside this\nregion can be noisy to the decision making procedure. Even worse, there is no\nguarantee that we can find the preferred solutions when tackling problems with\ncomplicated properties or a large number of objectives. In this paper, we\ndevelop a systematic way to incorporate the decision maker's preference\ninformation into the decomposition-based evolutionary multi-objective\noptimization methods. Generally speaking, our basic idea is a non-uniform\nmapping scheme by which the originally uniformly distributed reference points\non a canonical simplex can be mapped to the new positions close to the\naspiration level vector specified by the decision maker. By these means, we are\nable to steer the search process towards the region of interest either directly\nor in an interactive manner and also handle a large number of objectives. In\nthe meanwhile, the boundary solutions can be approximated given the decision\nmaker's requirements. Furthermore, the extent of the region of the interest is\nintuitively understandable and controllable in a closed form. Extensive\nexperiments, both proof-of-principle and on a variety of problems with 3 to 10\nobjectives, fully demonstrate the effectiveness of our proposed method for\napproximating the preferred solutions in the region of interest.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 22:05:09 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Li", "Ke", ""], ["Deb", "Kalyanmoy", ""], ["Yao", "Xin", ""]]}, {"id": "1701.06106", "submitter": "Sahil Garg", "authors": "Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano", "title": "Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\n  Changing World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 00:35:24 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 08:15:55 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Garg", "Sahil", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo", ""], ["Lozano", "Aurelie", ""]]}, {"id": "1701.06123", "submitter": "Mete Ozay", "authors": "Mete Ozay, Takayuki Okatani", "title": "Optimization on Product Submanifolds of Convolution Kernels", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 05:35:39 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:08:19 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1701.06538", "submitter": "Noam Shazeer", "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\n  Le, Geoffrey Hinton, Jeff Dean", "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:10:00 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Shazeer", "Noam", ""], ["Mirhoseini", "Azalia", ""], ["Maziarz", "Krzysztof", ""], ["Davis", "Andy", ""], ["Le", "Quoc", ""], ["Hinton", "Geoffrey", ""], ["Dean", "Jeff", ""]]}, {"id": "1701.06548", "submitter": "George Tucker", "authors": "Gabriel Pereyra, George Tucker, Jan Chorowski, {\\L}ukasz Kaiser,\n  Geoffrey Hinton", "title": "Regularizing Neural Networks by Penalizing Confident Output\n  Distributions", "comments": "Submitted to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically explore regularizing neural networks by penalizing low\nentropy output distributions. We show that penalizing low entropy output\ndistributions, which has been shown to improve exploration in reinforcement\nlearning, acts as a strong regularizer in supervised learning. Furthermore, we\nconnect a maximum entropy based confidence penalty to label smoothing through\nthe direction of the KL divergence. We exhaustively evaluate the proposed\nconfidence penalty and label smoothing on 6 common benchmarks: image\nclassification (MNIST and Cifar-10), language modeling (Penn Treebank), machine\ntranslation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ).\nWe find that both label smoothing and the confidence penalty improve\nstate-of-the-art models across benchmarks without modifying existing\nhyperparameters, suggesting the wide applicability of these regularizers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:35:28 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Pereyra", "Gabriel", ""], ["Tucker", "George", ""], ["Chorowski", "Jan", ""], ["Kaiser", "\u0141ukasz", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1701.06675", "submitter": "Melissa Aczon", "authors": "M Aczon, D Ledbetter, L Ho, A Gunny, A Flynn, J Williams, R Wetzel", "title": "Dynamic Mortality Risk Predictions in Pediatric Critical Care Using\n  Recurrent Neural Networks", "comments": "18 pages (5 of which in appendix), 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE math.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewing the trajectory of a patient as a dynamical system, a recurrent neural\nnetwork was developed to learn the course of patient encounters in the\nPediatric Intensive Care Unit (PICU) of a major tertiary care center. Data\nextracted from Electronic Medical Records (EMR) of about 12000 patients who\nwere admitted to the PICU over a period of more than 10 years were leveraged.\nThe RNN model ingests a sequence of measurements which include physiologic\nobservations, laboratory results, administered drugs and interventions, and\ngenerates temporally dynamic predictions for in-ICU mortality at user-specified\ntimes. The RNN's ICU mortality predictions offer significant improvements over\nthose from two clinically-used scores and static machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 23:32:10 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Aczon", "M", ""], ["Ledbetter", "D", ""], ["Ho", "L", ""], ["Gunny", "A", ""], ["Flynn", "A", ""], ["Williams", "J", ""], ["Wetzel", "R", ""]]}, {"id": "1701.07879", "submitter": "Gerard Rinkus", "authors": "Gerard Rinkus", "title": "A Radically New Theory of how the Brain Represents and Computes with\n  Probabilities", "comments": "33 pages, 10 figures - Sec. explaining single cell tuning fns as\n  artifacts of embedding SDRs in superposition removed (for future paper) -\n  Clarified that a given SDR code represents the whole likelihood distribution\n  over stored hypotheses at a coarsely-ranked level of fidelity (Submitted for\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is believed to implement probabilistic reasoning and to represent\ninformation via population, or distributed, coding. Most previous\npopulation-based probabilistic (PPC) theories share several basic properties:\n1) continuous-valued neurons; 2) fully(densely)-distributed codes, i.e.,\nall(most) units participate in every code; 3) graded synapses; 4) rate coding;\n5) units have innate unimodal tuning functions (TFs); 6) intrinsically noisy\nunits; and 7) noise/correlation is considered harmful. We present a radically\ndifferent theory that assumes: 1) binary units; 2) only a small subset of\nunits, i.e., a sparse distributed representation (SDR) (cell assembly),\ncomprises any individual code; 3) binary synapses; 4) signaling formally\nrequires only single (i.e., first) spikes; 5) units initially have completely\nflat TFs (all weights zero); 6) units are far less intrinsically noisy than\ntraditionally thought; rather 7) noise is a resource generated/used to cause\nsimilar inputs to map to similar codes, controlling a tradeoff between storage\ncapacity and embedding the input space statistics in the pattern of\nintersections over stored codes, epiphenomenally determining correlation\npatterns across neurons. The theory, Sparsey, was introduced 20+ years ago as a\ncanonical cortical circuit/algorithm model achieving efficient sequence\nlearning/recognition, but not elaborated as an alternative to PPC theories.\nHere, we show that: a) the active SDR simultaneously represents both the most\nsimilar/likely input and the entire (coarsely-ranked) similarity\nlikelihood/distribution over all stored inputs (hypotheses); and b) given an\ninput, the SDR code selection algorithm, which underlies both learning and\ninference, updates both the most likely hypothesis and the entire likelihood\ndistribution (cf. belief update) with a number of steps that remains constant\nas the number of stored items increases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 21:16:32 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:37:42 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 19:16:39 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 23:00:01 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Rinkus", "Gerard", ""]]}, {"id": "1701.07950", "submitter": "Vivek Nair", "authors": "Jianfeng Chen, Vivek Nair, Tim Menzies", "title": "Beyond Evolutionary Algorithms for Search-based Software Engineering", "comments": "17 pages, 10 figures, Information and Software Technology 2017", "journal-ref": null, "doi": "10.1016/j.infsof.2017.08.007", "report-no": null, "categories": "cs.SE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Evolutionary algorithms typically require a large number of\nevaluations (of solutions) to converge - which can be very slow and expensive\nto evaluate.Objective: To solve search-based software engineering (SE)\nproblems, using fewer evaluations than evolutionary methods.Method: Instead of\nmutating a small population, we build a very large initial population which is\nthen culled using a recursive bi-clustering chop approach. We evaluate this\napproach on multiple SE models, unconstrained as well as constrained, and\ncompare its performance with standard evolutionary algorithms. Results: Using\njust a few evaluations (under 100), we can obtain comparable results to\nstate-of-the-art evolutionary algorithms.Conclusion: Just because something\nworks, and is widespread use, does not necessarily mean that there is no value\nin seeking methods to improve that method. Before undertaking search-based SE\noptimization tasks using traditional EAs, it is recommended to try other\ntechniques, like those explored here, to obtain the same results with fewer\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 05:49:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 19:53:29 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 22:47:07 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Chen", "Jianfeng", ""], ["Nair", "Vivek", ""], ["Menzies", "Tim", ""]]}, {"id": "1701.07974", "submitter": "Haiping Huang", "authors": "Haiping Huang and Taro Toyoizumi", "title": "Reinforced stochastic gradient descent for deep neural network learning", "comments": "12 pages and 9 figures, nearly final version as a technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a standard optimization method to\nminimize a training error with respect to network parameters in modern neural\nnetwork learning. However, it typically suffers from proliferation of saddle\npoints in the high-dimensional parameter space. Therefore, it is highly\ndesirable to design an efficient algorithm to escape from these saddle points\nand reach a parameter region of better generalization capabilities. Here, we\npropose a simple extension of SGD, namely reinforced SGD, which simply adds\nprevious first-order gradients in a stochastic manner with a probability that\nincreases with learning time. As verified in a simple synthetic dataset, this\nmethod significantly accelerates learning compared with the original SGD.\nSurprisingly, it dramatically reduces over-fitting effects, even compared with\nstate-of-the-art adaptive learning algorithm---Adam. For a benchmark\nhandwritten digits dataset, the learning performance is comparable to Adam, yet\nwith an extra advantage of requiring one-fold less computer memory. The\nreinforced SGD is also compared with SGD with fixed or adaptive momentum\nparameter and Nesterov's momentum, which shows that the proposed framework is\nable to reach a similar generalization accuracy with less computational costs.\nOverall, our method introduces stochastic memory into gradients, which plays an\nimportant role in understanding how gradient-based training algorithms can work\nand its relationship with generalization abilities of deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 08:49:19 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 05:34:08 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 09:30:54 GMT"}, {"version": "v4", "created": "Tue, 19 Sep 2017 02:57:24 GMT"}, {"version": "v5", "created": "Wed, 22 Nov 2017 08:27:39 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Huang", "Haiping", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1701.08081", "submitter": "Nitin Malik Dr", "authors": "Naresh Kumari, Nitin Malik, A. N. Jha and Gaddam Mallesham", "title": "Design of PI Controller for Automatic Generation Control of Multi Area\n  Interconnected Power System using Bacterial Foraging Optimization", "comments": "SCOPUS indexed Singapore journal, ISSN (Print): 2319-8613, ISSN\n  (Online): 0975-4024, 8 pages, 11 Figures, 5 tables", "journal-ref": "Int. J. Engineering & Tech.,8(6),2779-2786,2016", "doi": "10.21817/ijet/2016/v8i6/160806236", "report-no": null, "categories": "cs.SY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The system comprises of three interconnected power system networks based on\nthermal, wind and hydro power generation. The load variation in any one of the\nnetwork results in frequency deviation in all the connected systems.The PI\ncontrollers have been connected separately with each system for the frequency\ncontrol and the gains (Kp and Ki) of all the controllers have been optimized\nalong with frequency bias (Bi) and speed regulation parameter (Ri). The\ncomputationally intelligent techniques like bacterial foraging optimization\n(BFO) and particle swarm optimization (PSO) have been applied for the tuning of\ncontroller gains along with variable parameters Bi and Ri. The gradient descent\n(GD) based conventional method has also been applied for optimizing the\nparameters Kp, Ki,Bi and Ri.The frequency responses are obtained with all the\nmethods. The performance index chosen is the integral square error (ISE). The\nsettling time, peak overshoot and peak undershoot of all the frequency\nresponses on applying three optimization techniques have been compared. It has\nbeen observed that the peak overshoot and peak undershoot significantly reduce\nwith BFO technique followed by the PSO and GD techniques. While obtaining such\noptimum response the settling time is increased marginally with bacterial\nforaging technique due to large number of mathematical equations used for the\ncomputation in BFO. The comparison of frequency response using three techniques\nshow the superiority of BFO over the PSO and GD techniques. The designing of\nthe system and tuning of the parameters with three techniques has been done in\nMATLAB/SIMULINK environment.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 17:45:41 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kumari", "Naresh", ""], ["Malik", "Nitin", ""], ["Jha", "A. N.", ""], ["Mallesham", "Gaddam", ""]]}, {"id": "1701.08259", "submitter": "Nitin Malik Dr", "authors": "Smriti Tikoo, Nitin Malik", "title": "Detection, Segmentation and Recognition of Face and its Features Using\n  Neural Network", "comments": "Google Scholar Indexed Journal, 5 pages, 10 figures, Journal of\n  Biosensors and Bioelectronics, vol. 7, no. 2, June-Sept 2016", "journal-ref": null, "doi": "10.4172/2155-6210.1000210", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection and recognition has been prevalent with research scholars and\ndiverse approaches have been incorporated till date to serve purpose. The\nrampant advent of biometric analysis systems, which may be full body scanners,\nor iris detection and recognition systems and the finger print recognition\nsystems, and surveillance systems deployed for safety and security purposes\nhave contributed to inclination towards same. Advances has been made with\nfrontal view, lateral view of the face or using facial expressions such as\nanger, happiness and gloominess, still images and video image to be used for\ndetection and recognition. This led to newer methods for face detection and\nrecognition to be introduced in achieving accurate results and economically\nfeasible and extremely secure. Techniques such as Principal Component analysis\n(PCA), Independent component analysis (ICA), Linear Discriminant Analysis\n(LDA), have been the predominant ones to be used. But with improvements needed\nin the previous approaches Neural Networks based recognition was like boon to\nthe industry. It not only enhanced the recognition but also the efficiency of\nthe process. Choosing Backpropagation as the learning method was clearly out of\nits efficiency to recognize nonlinear faces with an acceptance ratio of more\nthan 90% and execution time of only few seconds.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 06:44:31 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tikoo", "Smriti", ""], ["Malik", "Nitin", ""]]}, {"id": "1701.08431", "submitter": "Haiqiang Niu", "authors": "Haiqiang Niu, Emma Reeves, Peter Gerstoft", "title": "Source localization in an ocean waveguide using supervised machine\n  learning", "comments": "Submitted to The Journal of the Acoustical Society of America", "journal-ref": "The Journal of the Acoustical Society of America 142, 1176 (2017)", "doi": "10.1121/1.5000165", "report-no": null, "categories": "physics.ao-ph cs.NE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source localization in ocean acoustics is posed as a machine learning problem\nin which data-driven methods learn source ranges directly from observed\nacoustic data. The pressure received by a vertical linear array is preprocessed\nby constructing a normalized sample covariance matrix (SCM) and used as the\ninput. Three machine learning methods (feed-forward neural networks (FNN),\nsupport vector machines (SVM) and random forests (RF)) are investigated in this\npaper, with focus on the FNN. The range estimation problem is solved both as a\nclassification problem and as a regression problem by these three machine\nlearning algorithms. The results of range estimation for the Noise09 experiment\nare compared for FNN, SVM, RF and conventional matched-field processing and\ndemonstrate the potential of machine learning for underwater source\nlocalization..\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 21:18:22 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 00:58:01 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 17:34:00 GMT"}, {"version": "v4", "created": "Wed, 6 Sep 2017 19:31:51 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Niu", "Haiqiang", ""], ["Reeves", "Emma", ""], ["Gerstoft", "Peter", ""]]}, {"id": "1701.08718", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sarath Chandar, Yoshua Bengio", "title": "Memory Augmented Neural Networks with Wormhole Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical results on long-term dependency tasks have shown that neural\nnetworks augmented with an external memory can learn the long-term dependency\ntasks more easily and achieve better generalization than vanilla recurrent\nneural networks (RNN). We suggest that memory augmented neural networks can\nreduce the effects of vanishing gradients by creating shortcut (or wormhole)\nconnections. Based on this observation, we propose a novel memory augmented\nneural network model called TARDIS (Temporal Automatic Relation Discovery in\nSequences). The controller of TARDIS can store a selective set of embeddings of\nits own previous hidden states into an external memory and revisit them as and\nwhen needed. For TARDIS, memory acts as a storage for wormhole connections to\nthe past to propagate the gradients more effectively and it helps to learn the\ntemporal dependencies. The memory structure of TARDIS has similarities to both\nNeural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but\nboth read and write operations of TARDIS are simpler and more efficient. We use\ndiscrete addressing for read/write operations which helps to substantially to\nreduce the vanishing gradient problem with very long sequences. Read and write\noperations in TARDIS are tied with a heuristic once the memory becomes full,\nand this makes the learning problem simpler when compared to NTM or D-NTM type\nof architectures. We provide a detailed analysis on the gradient propagation in\ngeneral for MANNs. We evaluate our models on different long-term dependency\ntasks and report competitive results in all of them.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 17:34:51 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Chandar", "Sarath", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1701.08734", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols,\n  David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra", "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For artificial general intelligence (AGI) it would be efficient if multiple\nusers trained the same giant neural network, permitting parameter reuse,\nwithout catastrophic forgetting. PathNet is a first step in this direction. It\nis a neural network algorithm that uses agents embedded in the neural network\nwhose task is to discover which parts of the network to re-use for new tasks.\nAgents are pathways (views) through the network which determine the subset of\nparameters that are used and updated by the forwards and backwards passes of\nthe backpropogation algorithm. During learning, a tournament selection genetic\nalgorithm is used to select pathways through the neural network for replication\nand mutation. Pathway fitness is the performance of that pathway measured\naccording to a cost function. We demonstrate successful transfer learning;\nfixing the parameters along a path learned on task A and re-evolving a new\npopulation of paths for task B, allows task B to be learned faster than it\ncould be learned from scratch or after fine-tuning. Paths evolved on task B\nre-use parts of the optimal path evolved on task A. Positive transfer was\ndemonstrated for binary MNIST, CIFAR, and SVHN supervised learning\nclassification tasks, and a set of Atari and Labyrinth reinforcement learning\ntasks, suggesting PathNets have general applicability for neural network\ntraining. Finally, PathNet also significantly improves the robustness to\nhyperparameter choices of a parallel asynchronous reinforcement learning\nalgorithm (A3C).\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 18:06:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Banarse", "Dylan", ""], ["Blundell", "Charles", ""], ["Zwols", "Yori", ""], ["Ha", "David", ""], ["Rusu", "Andrei A.", ""], ["Pritzel", "Alexander", ""], ["Wierstra", "Daan", ""]]}, {"id": "1701.08893", "submitter": "Connelly Barnes", "authors": "Eric Risser, Pierre Wilmot, Connelly Barnes", "title": "Stable and Controllable Neural Texture Synthesis and Style Transfer\n  Using Histogram Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, methods have been proposed that perform texture synthesis and style\ntransfer by using convolutional neural networks (e.g. Gatys et al.\n[2015,2016]). These methods are exciting because they can in some cases create\nresults with state-of-the-art quality. However, in this paper, we show these\nmethods also have limitations in texture quality, stability, requisite\nparameter tuning, and lack of user controls. This paper presents a multiscale\nsynthesis pipeline based on convolutional neural networks that ameliorates\nthese issues. We first give a mathematical explanation of the source of\ninstabilities in many previous approaches. We then improve these instabilities\nby using histogram losses to synthesize textures that better statistically\nmatch the exemplar. We also show how to integrate localized style losses in our\nmultiscale framework. These losses can improve the quality of large features,\nimprove the separation of content and style, and offer artistic controls such\nas paint by numbers. We demonstrate that our approach offers improved quality,\nconvergence in fewer iterations, and more stability over the optimization.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 02:37:19 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 23:30:20 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Risser", "Eric", ""], ["Wilmot", "Pierre", ""], ["Barnes", "Connelly", ""]]}, {"id": "1701.08951", "submitter": "Nitin Malik Dr", "authors": "Sheila Mahapatra, Nitin Malik", "title": "A Hybrid Approach for Secured Optimal Power Flow and Voltage Stability\n  with TCSC Placement", "comments": "Google Scholar Indexed Australian journal, ISSN: 2005-4238, 8 pages,\n  2 figures, 1 table", "journal-ref": "International Journal of Advanced Science and Technology, vol.89,\n  pp.1-8, April 2016", "doi": "10.14257/ijast.2016.89.01", "report-no": null, "categories": "cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hybrid technique for secured optimal power flow coupled\nwith enhancing voltage stability with FACTS device installation. The hybrid\napproach of Improved Gravitational Search algorithm (IGSA) and Firefly\nalgorithm (FA) performance is analyzed by optimally placing TCSC controller.\nThe algorithm is implemented in MATLAB working platform and the power flow\nsecurity and voltage stability is evaluated with IEEE 30 bus transmission\nsystems. The optimal results generated are compared with those available in\nliterature and the superior performance of algorithm is depicted as minimum\ngeneration cost, reduced real power losses along with sustaining voltage\nstability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 09:09:47 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Mahapatra", "Sheila", ""], ["Malik", "Nitin", ""]]}, {"id": "1701.08978", "submitter": "Naveen Mellempudi", "authors": "Naveen Mellempudi, Abhisek Kundu, Dipankar Das, Dheevatsa Mudigere,\n  and Bharat Kaul", "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cluster-based quantization method to convert pre-trained full\nprecision weights into ternary weights with minimal impact on the accuracy. In\naddition, we also constrain the activations to 8-bits thus enabling sub 8-bit\nfull integer inference pipeline. Our method uses smaller clusters of N filters\nwith a common scaling factor to minimize the quantization loss, while also\nmaximizing the number of ternary operations. We show that with a cluster size\nof N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best\nfull precision results while replacing ~85% of all multiplications with 8-bit\naccumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1\naccuracy which within 2% of the full precision result. We also study the impact\nof the size of the cluster on both performance and accuracy, larger cluster\nsizes N=64 can replace ~98% of the multiplications with ternary operations but\nintroduces significant drop in accuracy which necessitates fine tuning the\nparameters with retraining the network at lower precision. To address this we\nhave also trained low-precision Resnet-50 with 8-bit activations and ternary\nweights by pre-initializing the network with full precision weights and achieve\n68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can\nrun on a full 8-bit compute pipeline, with a potential 16x improvement in\nperformance compared to baseline full-precision models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:28:37 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 04:09:31 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Mellempudi", "Naveen", ""], ["Kundu", "Abhisek", ""], ["Das", "Dipankar", ""], ["Mudigere", "Dheevatsa", ""], ["Kaul", "Bharat", ""]]}, {"id": "1701.09046", "submitter": "Andre Goncalves", "authors": "Andr\\'e R. Goncalves, Celso Cavelucci, Christiano Lyra Filho, Fernando\n  J. Von Zuben", "title": "An Extremal Optimization approach to parallel resonance constrained\n  capacitor placement problem", "comments": "Paper published in the 6th IEEE/PES Transmission and Distribution:\n  Latin America, 2012, Montevideo, Uruguay", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Installation of capacitors in distribution networks is one of the most used\nprocedure to compensate reactive power generated by loads and, consequently, to\nreduce technical losses. So, the problem consists in identifying the optimal\nplacement and sizing of capacitors. This problem is known in the literature as\noptimal capacitor placement problem. Neverthless, depending on the location and\nsize of the capacitor, it may become a harmonic source, allowing capacitor to\nenter into resonance with the distribution network, causing several undesired\nside effects. In this work we propose a parsimonious method to deal with the\ncapacitor placement problem that incorporates resonance constraints, ensuring\nthat every allocated capacitor will not act as a harmonic source. This proposed\nalgorithm is based upon a physical inspired metaheuristic known as Extremal\nOptimization. The results achieved showed that this proposal has reached\nsignificant gains when compared with other proposals that attempt repair, in a\npost-optimization stage, already obtained solutions which violate resonance\nconstraints.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 17:45:10 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Goncalves", "Andr\u00e9 R.", ""], ["Cavelucci", "Celso", ""], ["Filho", "Christiano Lyra", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1701.09175", "submitter": "Emin Orhan", "authors": "A. Emin Orhan, Xaq Pitkow", "title": "Skip Connections Eliminate Singularities", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip connections made the training of very deep networks possible and have\nbecome an indispensable component in a variety of neural architectures. A\ncompletely satisfactory explanation for their success remains elusive. Here, we\npresent a novel explanation for the benefits of skip connections in training\nvery deep networks. The difficulty of training deep networks is partly due to\nthe singularities caused by the non-identifiability of the model. Several such\nsingularities have been identified in previous works: (i) overlap singularities\ncaused by the permutation symmetry of nodes in a given layer, (ii) elimination\nsingularities corresponding to the elimination, i.e. consistent deactivation,\nof nodes, (iii) singularities generated by the linear dependence of the nodes.\nThese singularities cause degenerate manifolds in the loss landscape that slow\ndown learning. We argue that skip connections eliminate these singularities by\nbreaking the permutation symmetry of nodes, by reducing the possibility of node\nelimination and by making the nodes less linearly dependent. Moreover, for\ntypical initializations, skip connections move the network away from the\n\"ghosts\" of these singularities and sculpt the landscape around them to\nalleviate the learning slow-down. These hypotheses are supported by evidence\nfrom simplified models, as well as from experiments with deep networks trained\non real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 18:41:07 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 17:53:43 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 18:33:24 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2017 16:47:10 GMT"}, {"version": "v5", "created": "Mon, 22 May 2017 15:18:09 GMT"}, {"version": "v6", "created": "Wed, 24 May 2017 16:36:39 GMT"}, {"version": "v7", "created": "Tue, 20 Jun 2017 17:50:41 GMT"}, {"version": "v8", "created": "Sun, 4 Mar 2018 22:23:18 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Orhan", "A. Emin", ""], ["Pitkow", "Xaq", ""]]}]