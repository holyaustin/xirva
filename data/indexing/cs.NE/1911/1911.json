[{"id": "1911.00089", "submitter": "Michael Hauser", "authors": "Yiwei Fu, Samer Saab Jr, Asok Ray and Michael Hauser", "title": "A Dynamically Controlled Recurrent Neural Network for Modeling Dynamical\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel neural network architecture, called the\nDynamically Controlled Recurrent Neural Network (DCRNN), specifically designed\nto model dynamical systems that are governed by ordinary differential equations\n(ODEs). The current state vectors of these types of dynamical systems only\ndepend on their state-space models, along with the respective inputs and\ninitial conditions. Long Short-Term Memory (LSTM) networks, which have proven\nto be very effective for memory-based tasks, may fail to model physical\nprocesses as they tend to memorize, rather than learn how to capture the\ninformation on the underlying dynamics. The proposed DCRNN includes learnable\nskip-connections across previously hidden states, and introduces a\nregularization term in the loss function by relying on Lyapunov stability\ntheory. The regularizer enables the placement of eigenvalues of the transfer\nfunction induced by the DCRNN to desired values, thereby acting as an internal\ncontroller for the hidden state trajectory. The results show that, for\nforecasting a chaotic dynamical system, the DCRNN outperforms the LSTM in $100$\nout of $100$ randomized experiments by reducing the mean squared error of the\nLSTM's forecasting by $80.0\\% \\pm 3.0\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 20:22:38 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Fu", "Yiwei", ""], ["Saab", "Samer", "Jr"], ["Ray", "Asok", ""], ["Hauser", "Michael", ""]]}, {"id": "1911.00105", "submitter": "Weiwen Jiang", "authors": "Qing Lu, Weiwen Jiang, Xiaowei Xu, Yiyu Shi, Jingtong Hu", "title": "On Neural Architecture Search for Resource-Constrained Hardware\n  Platforms", "comments": "8 pages, ICCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, the success of Neural Architecture Search (NAS) has\nenabled researchers to broadly explore the design space using learning-based\nmethods. Apart from finding better neural network architectures, the idea of\nautomation has also inspired to improve their implementations on hardware.\nWhile some practices of hardware machine-learning automation have achieved\nremarkable performance, the traditional design concept is still followed: a\nnetwork architecture is first structured with excellent test accuracy, and then\ncompressed and optimized to fit into a target platform. Such a design flow will\neasily lead to inferior local-optimal solutions. To address this problem, we\npropose a new framework to jointly explore the space of neural architecture,\nhardware implementation, and quantization. Our objective is to find a quantized\narchitecture with the highest accuracy that is implementable on given hardware\nspecifications. We employ FPGAs to implement and test our designs with limited\nloop-up tables (LUTs) and required throughput. Compared to the separate\ndesign/searching methods, our framework has demonstrated much better\nperformance under strict specifications and generated designs of higher\naccuracy by 18\\% to 68\\% in the task of classifying CIFAR10 images. With 30,000\nLUTs, a light-weight design is found to achieve 82.98\\% accuracy and 1293\nimages/second throughput, compared to which, under the same constraints, the\ntraditional method even fails to find a valid solution.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 21:02:23 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Lu", "Qing", ""], ["Jiang", "Weiwen", ""], ["Xu", "Xiaowei", ""], ["Shi", "Yiyu", ""], ["Hu", "Jingtong", ""]]}, {"id": "1911.00139", "submitter": "Weiwen Jiang", "authors": "Weiwen Jiang, Qiuwen Lou, Zheyu Yan, Lei Yang, Jingtong Hu, Xiaobo\n  Sharon Hu, Yiyu Shi", "title": "Device-Circuit-Architecture Co-Exploration for Computing-in-Memory\n  Neural Accelerators", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-exploration of neural architectures and hardware design is promising to\nsimultaneously optimize network accuracy and hardware efficiency. However,\nstate-of-the-art neural architecture search algorithms for the co-exploration\nare dedicated for the conventional von-neumann computing architecture, whose\nperformance is heavily limited by the well-known memory wall. In this paper, we\nare the first to bring the computing-in-memory architecture, which can easily\ntranscend the memory wall, to interplay with the neural architecture search,\naiming to find the most efficient neural architectures with high network\naccuracy and maximized hardware efficiency. Such a novel combination makes\nopportunities to boost performance, but also brings a bunch of challenges. The\ndesign space spans across multiple layers from device type, circuit topology to\nneural architecture. In addition, the performance may degrade in the presence\nof device variation. To address these challenges, we propose a cross-layer\nexploration framework, namely NACIM, which jointly explores device, circuit and\narchitecture design space and takes device variation into consideration to find\nthe most robust neural architectures. Experimental results demonstrate that\nNACIM can find the robust neural network with 0.45% accuracy loss in the\npresence of device variation, compared with a 76.44% loss from the\nstate-of-the-art NAS without consideration of variation; in addition, NACIM\nachieves an energy efficiency up to 16.3 TOPs/W, 3.17X higher than the\nstate-of-the-art NAS.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 22:42:33 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 00:21:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Jiang", "Weiwen", ""], ["Lou", "Qiuwen", ""], ["Yan", "Zheyu", ""], ["Yang", "Lei", ""], ["Hu", "Jingtong", ""], ["Hu", "Xiaobo Sharon", ""], ["Shi", "Yiyu", ""]]}, {"id": "1911.00344", "submitter": "Lavanya Marla", "authors": "Lavanya Marla, Lav R. Varshney, Devavrat Shah, Nirmal A. Prakash and\n  Michael E. Gale", "title": "Short and Wide Network Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network flow is a powerful mathematical framework to systematically explore\nthe relationship between structure and function in biological, social, and\ntechnological networks. We introduce a new pipelining model of flow through\nnetworks where commodities must be transported over single paths rather than\nsplit over several paths and recombined. We show this notion of pipelined\nnetwork flow is optimized using network paths that are both short and wide, and\ndevelop efficient algorithms to compute such paths for given pairs of nodes and\nfor all-pairs. Short and wide paths are characterized for many real-world\nnetworks. To further demonstrate the utility of this network characterization,\nwe develop novel information-theoretic lower bounds on computation speed in\nnervous systems due to limitations from anatomical connectivity and physical\nnoise. For the nematode Caenorhabditis elegans, we find these bounds are\npredictive of biological timescales of behavior. Further, we find the\nparticular C. elegans connectome is globally less efficient for information\nflow than random networks, but the hub-and-spoke architecture of functional\nsubcircuits is optimal under constraint on number of synapses. This suggests\nfunctional subcircuits are a primary organizational principle of this small\ninvertebrate nervous system.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:43:07 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Marla", "Lavanya", ""], ["Varshney", "Lav R.", ""], ["Shah", "Devavrat", ""], ["Prakash", "Nirmal A.", ""], ["Gale", "Michael E.", ""]]}, {"id": "1911.00490", "submitter": "Alison Jenkins", "authors": "Alison Jenkins, Vinika Gupta, Alexis Myrick, Mary Lenoir", "title": "Variations of Genetic Algorithms", "comments": "genetic algorithm, elitism, generational, steady-state", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to develop the Genetic Algorithms (GA) for\nsolving the Schaffer F6 function in fewer than 4000 function evaluations on a\ntotal of 30 runs. Four types of Genetic Algorithms (GA) are presented -\nGenerational GA (GGA), Steady-State (mu+1)-GA (SSGA), Steady-Generational\n(mu,mu)-GA (SGGA), and (mu+mu)-GA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 17:57:03 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Jenkins", "Alison", ""], ["Gupta", "Vinika", ""], ["Myrick", "Alexis", ""], ["Lenoir", "Mary", ""]]}, {"id": "1911.00640", "submitter": "Xiang Zou", "authors": "Xiang Zou, Lie Yao, Donghua Zhao, Liang Chen, Ying Mao", "title": "The Intrinsic Properties of Brain Based on the Network Structure", "comments": "24 pages, 6 figures, 1 supplementary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Brain is a fantastic organ that helps creature adapting to the\nenvironment. Network is the most essential structure of brain, but the\ncapability of a simple network is still not very clear. In this study, we try\nto expound some brain functions only by the network property. Methods: Every\nnetwork can be equivalent to a simplified network, which is expressed by an\nequation set. The dynamic of the equation set can be described by some basic\nequations, which is based on the mathematical derivation. Results (1) In a\nclosed network, the stability is based on the excitatory/inhibitory synapse\nproportion. Spike probabilities in the assembly can meet the solution of a\nnonlinear equation set. (2) Network activity can spontaneously evolve into a\ncertain distribution under different stimulation, which is closely related to\ndecision making. (3) Short memory can be formed by coupling of network\nassemblies. Conclusion: The essential property of a network may contribute to\nsome important brain functions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 03:47:18 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zou", "Xiang", ""], ["Yao", "Lie", ""], ["Zhao", "Donghua", ""], ["Chen", "Liang", ""], ["Mao", "Ying", ""]]}, {"id": "1911.00809", "submitter": "Ruosong Wang", "authors": "Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan\n  Salakhutdinov, Sanjeev Arora", "title": "Enhanced Convolutional Neural Tangent Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that for training with $\\ell_2$ loss, convolutional\nneural networks (CNNs) whose width (number of channels in convolutional layers)\ngoes to infinity correspond to regression with respect to the CNN Gaussian\nProcess kernel (CNN-GP) if only the last layer is trained, and correspond to\nregression with respect to the Convolutional Neural Tangent Kernel (CNTK) if\nall layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019)\nyielded the finding that classification accuracy of CNTK on CIFAR-10 is within\n6-7% of that of that of the corresponding CNN architecture (best figure being\naround 78%) which is interesting performance for a fixed kernel. Here we show\nhow to significantly enhance the performance of these kernels using two ideas.\n(1) Modifying the kernel using a new operation called Local Average Pooling\n(LAP) which preserves efficient computability of the kernel and inherits the\nspirit of standard data augmentation using pixel shifts. Earlier papers were\nunable to incorporate naive data augmentation because of the quadratic training\ncost of kernel regression. This idea is inspired by Global Average Pooling\n(GAP), which we show for CNN-GP and CNTK is equivalent to full translation data\naugmentation. (2) Representing the input image using a pre-processing technique\nproposed by Coates et al. (2011), which uses a single convolutional layer\ncomposed of random image patches. On CIFAR-10, the resulting kernel, CNN-GP\nwith LAP and horizontal flip data augmentation, achieves 89% accuracy, matching\nthe performance of AlexNet (Krizhevsky et al., 2012). Note that this is the\nbest such result we know of for a classifier that is not a trained neural\nnetwork. Similar improvements are obtained for Fashion-MNIST.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 02:24:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Zhiyuan", ""], ["Wang", "Ruosong", ""], ["Yu", "Dingli", ""], ["Du", "Simon S.", ""], ["Hu", "Wei", ""], ["Salakhutdinov", "Ruslan", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1911.00822", "submitter": "Yujie Wu", "authors": "Lei Deng, Yujie Wu, Yifan Hu, Ling Liang, Guoqi Li, Xing Hu, Yufei\n  Ding, Peng Li, Yuan Xie", "title": "Comprehensive SNN Compression Using ADMM Optimization and Activity\n  Regularization", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As well known, the huge memory and compute costs of both artificial neural\nnetworks (ANNs) and spiking neural networks (SNNs) greatly hinder their\ndeployment on edge devices with high efficiency. Model compression has been\nproposed as a promising technique to improve the running efficiency via\nparameter and operation reduction. Whereas, this technique is mainly practiced\nin ANNs rather than SNNs. It is interesting to answer how much an SNN model can\nbe compressed without compromising its functionality, where two challenges\nshould be addressed: i) the accuracy of SNNs is usually sensitive to model\ncompression, which requires an accurate compression methodology; ii) the\ncomputation of SNNs is event-driven rather than static, which produces an extra\ncompression dimension on dynamic spikes. To this end, we realize a\ncomprehensive SNN compression through three steps. First, we formulate the\nconnection pruning and weight quantization as a constrained optimization\nproblem. Second, we combine spatio-temporal backpropagation (STBP) and\nalternating direction method of multipliers (ADMM) to solve the problem with\nminimum accuracy loss. Third, we further propose activity regularization to\nreduce the spike events for fewer active operations. These methods can be\napplied in either a single way for moderate compression or a joint way for\naggressive compression. We define several quantitative metrics to evaluation\nthe compression performance for SNNs. Our methodology is validated in pattern\nrecognition tasks over MNIST, N-MNIST, CIFAR10, and CIFAR100 datasets, where\nextensive comparisons, analyses, and insights are provided. To our best\nknowledge, this is the first work that studies SNN compression in a\ncomprehensive manner by exploiting all compressible components and achieves\nbetter results.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 04:07:23 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 01:22:43 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 06:42:05 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Deng", "Lei", ""], ["Wu", "Yujie", ""], ["Hu", "Yifan", ""], ["Liang", "Ling", ""], ["Li", "Guoqi", ""], ["Hu", "Xing", ""], ["Ding", "Yufei", ""], ["Li", "Peng", ""], ["Xie", "Yuan", ""]]}, {"id": "1911.00926", "submitter": "Daniel Tanneberg", "authors": "Daniel Tanneberg, Elmar Rueckert, Jan Peters", "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural\n  Computer Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key feature of intelligent behavior is the ability to learn abstract\nstrategies that transfer to unfamiliar problems. Therefore, we present a novel\narchitecture, based on memory-augmented networks, that is inspired by the von\nNeumann and Harvard architectures of modern computers. This architecture\nenables the learning of abstract algorithmic solutions via Evolution Strategies\nin a reinforcement learning setting. Applied to Sokoban, sliding block puzzle\nand robotic manipulation tasks, we show that the architecture can learn\nalgorithmic solutions with strong generalization and abstraction: scaling to\narbitrary task configurations and complexities, and being independent of both\nthe data representation and the task domain.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:02:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 11:21:39 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Tanneberg", "Daniel", ""], ["Rueckert", "Elmar", ""], ["Peters", "Jan", ""]]}, {"id": "1911.01072", "submitter": "Byung Hyung Kim", "authors": "Byung Hyung Kim and Sungho Jo", "title": "Wearable Affective Life-Log System for Understanding Emotion Dynamics in\n  Daily Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Past research on recognizing human affect has made use of a variety of\nphysiological sensors in many ways. Nonetheless, how affective dynamics are\ninfluenced in the context of human daily life has not yet been explored. In\nthis work, we present a wearable affective life-log system (ALIS), that is\nrobust as well as easy to use in daily life to detect emotional changes and\ndetermine their cause-and-effect relationship on users' lives. The proposed\nsystem records how a user feels in certain situations during long-term\nactivities with physiological sensors. Based on the long-term monitoring, the\nsystem analyzes how the contexts of the user's life affect his/her emotion\nchanges. Furthermore, real-world experimental results demonstrate that the\nproposed wearable life-log system enables us to build causal structures to find\neffective stress relievers suited to every stressful situation in school life.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 08:35:51 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:31:00 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kim", "Byung Hyung", ""], ["Jo", "Sungho", ""]]}, {"id": "1911.01102", "submitter": "Chung-Yi Li", "authors": "Chung-Yi Li, Pei-Chieh Yuan, Hung-Yi Lee", "title": "What does a network layer hear? Analyzing hidden representations of\n  end-to-end ASR through speech synthesis", "comments": "submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end speech recognition systems have achieved competitive results\ncompared to traditional systems. However, the complex transformations involved\nbetween layers given highly variable acoustic signals are hard to analyze. In\nthis paper, we present our ASR probing model, which synthesizes speech from\nhidden representations of end-to-end ASR to examine the information maintain\nafter each layer calculation. Listening to the synthesized speech, we observe\ngradual removal of speaker variability and noise as the layer goes deeper,\nwhich aligns with the previous studies on how deep network functions in speech\nrecognition. This paper is the first study analyzing the end-to-end speech\nrecognition model by demonstrating what each layer hears. Speaker verification\nand speech enhancement measurements on synthesized speech are also conducted to\nconfirm our observation further.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 10:07:13 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Chung-Yi", ""], ["Yuan", "Pei-Chieh", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1911.01141", "submitter": "Leendert Remmelzwaal", "authors": "Leendert A Remmelzwaal, Amit Mishra, George F R Ellis", "title": "Human eye inspired log-polar pre-processing for neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we draw inspiration from the human visual system, and present a\nbio-inspired pre-processing stage for neural networks. We implement this by\napplying a log-polar transformation as a pre-processing step, and to\ndemonstrate, we have used a naive convolutional neural network (CNN). We\ndemonstrate that a bio-inspired pre-processing stage can achieve rotation and\nscale robustness in CNNs. A key point in this paper is that the CNN does not\nneed to be trained to identify rotation or scaling permutations; rather it is\nthe log-polar pre-processing step that converts the image into a format that\nallows the CNN to handle rotation and scaling permutations. In addition we\ndemonstrate how adding a log-polar transformation as a pre-processing step can\nreduce the image size to ~20\\% of the Euclidean image size, without\nsignificantly compromising classification accuracy of the CNN. The\npre-processing stage presented in this paper is modelled after the retina and\ntherefore is only tested against an image dataset. Note: This paper has been\nsubmitted for SAUPEC/RobMech/PRASA 2020.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 11:45:13 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Remmelzwaal", "Leendert A", ""], ["Mishra", "Amit", ""], ["Ellis", "George F R", ""]]}, {"id": "1911.01158", "submitter": "Byung Hyung Kim", "authors": "Byung Hyung Kim and Sungho Jo", "title": "An Affective Situation Labeling System from Psychological Behaviors in\n  Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a computational framework for providing affective labels\nto real-life situations, called A-Situ. We first define an affective situation,\nas a specific arrangement of affective entities relevant to emotion elicitation\nin a situation. Then, the affective situation is represented as a set of labels\nin the valence-arousal emotion space. Based on physiological behaviors in\nresponse to a situation, the proposed framework quantifies the expected emotion\nevoked by the interaction with a stimulus event. The accumulated result in a\nspatiotemporal situation is represented as a polynomial curve called the\naffective curve, which bridges the semantic gap between cognitive and affective\nperception in real-world situations. We show the efficacy of the curve for\nreliable emotion labeling in real-world experiments, respectively concerning 1)\na comparison between the results from our system and existing explicit\nassessments for measuring emotion, 2) physiological distinctiveness in\nemotional states, and 3) physiological characteristics correlated to continuous\nlabels. The efficiency of affective curves to discriminate emotional states is\nevaluated through subject-dependent classification performance using\nbicoherence features to represent discrete affective states in the\nvalence-arousal space. Furthermore, electroencephalography-based statistical\nanalysis revealed the physiological correlates of the affective curves.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 12:32:10 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:26:04 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kim", "Byung Hyung", ""], ["Jo", "Sungho", ""]]}, {"id": "1911.01258", "submitter": "Reza Yazdani Aminabadi", "authors": "Reza Yazdani, Olatunji Ruwase, Minjia Zhang, Yuxiong He, Jose-Maria\n  Arnau, Antonio Gonzalez", "title": "LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long\n  Short-Term Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of LSTM neural networks for popular tasks such as Automatic\nSpeech Recognition has fostered an increasing interest in LSTM inference\nacceleration. Due to the recurrent nature and data dependencies of LSTM\ncomputations, designing a customized architecture specifically tailored to its\ncomputation pattern is crucial for efficiency. Since LSTMs are used for a\nvariety of tasks, generalizing this efficiency to diverse configurations, i.e.,\nadaptiveness, is another key feature of these accelerators. In this work, we\nfirst show the problem of low resource-utilization and adaptiveness for the\nstate-of-the-art LSTM implementations on GPU, FPGA and ASIC architectures. To\nsolve these issues, we propose an intelligent tiled-based dispatching mechanism\nthat efficiently handles the data dependencies and increases the adaptiveness\nof LSTM computation. To do so, we propose LSTM-Sharp as a hardware accelerator,\nwhich pipelines LSTM computation using an effective scheduling scheme to hide\nmost of the dependent serialization. Furthermore, LSTM-Sharp employs dynamic\nreconfigurable architecture to adapt to the model's characteristics. LSTM-Sharp\nachieves 1.5x, 2.86x, and 82x speedups on average over the state-of-the-art\nASIC, FPGA, and GPU implementations respectively, for different LSTM models and\nresource budgets. Furthermore, we provide significant energy-reduction with\nrespect to the previous solutions, due to the low power dissipation of\nLSTM-Sharp (383 GFLOPs/Watt).\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:51:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yazdani", "Reza", ""], ["Ruwase", "Olatunji", ""], ["Zhang", "Minjia", ""], ["He", "Yuxiong", ""], ["Arnau", "Jose-Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "1911.01310", "submitter": "Marco Gallieri", "authors": "Simone Pozzoli, Marco Gallieri, Riccardo Scattolini", "title": "Tustin neural networks: a class of recurrent nets for adaptive MPC of\n  mechanical systems", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of recurrent neural networks to represent the dynamics of unstable\nsystems is difficult due to the need to properly initialize their internal\nstates, which in most of the cases do not have any physical meaning, consequent\nto the non-smoothness of the optimization problem. For this reason, in this\npaper focus is placed on mechanical systems characterized by a number of\ndegrees of freedom, each one represented by two states, namely position and\nvelocity. For these systems, a new recurrent neural network is proposed:\nTustin-Net. Inspired by second-order dynamics, the network hidden states can be\nstraightforwardly estimated, as their differential relationships with the\nmeasured states are hardcoded in the forward pass. The proposed structure is\nused to model a double inverted pendulum and for model-based Reinforcement\nLearning, where an adaptive Model Predictive Controller scheme using the\nUnscented Kalman Filter is proposed to deal with parameter changes in the\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:21:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Pozzoli", "Simone", ""], ["Gallieri", "Marco", ""], ["Scattolini", "Riccardo", ""]]}, {"id": "1911.01545", "submitter": "Forough Arabshahi", "authors": "Forough Arabshahi, Zhichu Lu, Pranay Mundra, Sameer Singh, Animashree\n  Anandkumar", "title": "Compositional Generalization with Tree Stack Memory Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study compositional generalization, viz., the problem of zero-shot\ngeneralization to novel compositions of concepts in a domain. Standard neural\nnetworks fail to a large extent on compositional learning. We propose Tree\nStack Memory Units (Tree-SMU) to enable strong compositional generalization.\nTree-SMU is a recursive neural network with Stack Memory Units (\\SMU s), a\nnovel memory augmented neural network whose memory has a differentiable stack\nstructure. Each SMU in the tree architecture learns to read from its stack and\nto write to it by combining the stacks and states of its children through\ngating. The stack helps capture long-range dependencies in the problem domain,\nthereby enabling compositional generalization. Additionally, the stack also\npreserves the ordering of each node's descendants, thereby retaining locality\non the tree. We demonstrate strong empirical results on two mathematical\nreasoning benchmarks. We use four compositionality tests to assess the\ngeneralization performance of Tree-SMU and show that it enables accurate\ncompositional generalization compared to strong baselines such as Transformers\nand Tree-LSTMs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 00:27:03 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 16:17:52 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 04:19:53 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 16:50:58 GMT"}, {"version": "v5", "created": "Fri, 16 Oct 2020 00:24:06 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Arabshahi", "Forough", ""], ["Lu", "Zhichu", ""], ["Mundra", "Pranay", ""], ["Singh", "Sameer", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1911.01600", "submitter": "Hamada Nayel", "authors": "Hamada A. Nayel and Shashrekha H. L", "title": "Integrating Dictionary Feature into A Deep Learning Model for Disease\n  Named Entity Recognition", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, Deep Learning (DL) models are becoming important due to\ntheir demonstrated success at overcoming complex learning problems. DL models\nhave been applied effectively for different Natural Language Processing (NLP)\ntasks such as part-of-Speech (PoS) tagging and Machine Translation (MT).\nDisease Named Entity Recognition (Disease-NER) is a crucial task which aims at\nextracting disease Named Entities (NEs) from text. In this paper, a DL model\nfor Disease-NER using dictionary information is proposed and evaluated on\nNational Center for Biotechnology Information (NCBI) disease corpus and BC5CDR\ndataset. Word embeddings trained over general domain texts as well as\nbiomedical texts have been used to represent input to the proposed model. This\nstudy also compares two different Segment Representation (SR) schemes, namely\nIOB2 and IOBES for Disease-NER. The results illustrate that using dictionary\ninformation, pre-trained word embeddings, character embeddings and CRF with\nglobal score improves the performance of Disease-NER system.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 03:50:16 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Nayel", "Hamada A.", ""], ["L", "Shashrekha H.", ""]]}, {"id": "1911.01753", "submitter": "Hendry F Chame", "authors": "Hendry Ferreira Chame and Jun Tani", "title": "Cognitive and motor compliance in intentional human-robot interaction", "comments": "\"\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodiment and subjective experience in human-robot interaction are important\naspects to consider when studying both natural cognition and adaptive robotics\nto human environments. Although several researches have focused on nonverbal\ncommunication and collaboration, the study of autonomous physical interaction\nhas obtained less attention. From the perspective of neurorobotics, we\ninvestigate the relation between intentionality, motor compliance, cognitive\ncompliance, and behavior emergence. We propose a variational model inspired by\nthe principles of predictive coding and active inference to study\nintentionality and cognitive compliance, and an intermittent control concept\nfor motor deliberation and compliance based on torque feed-back. Our\nexperiments with the humanoid Torobo portrait interesting perspectives for the\nbio-inspired study of developmental and social processes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 13:03:10 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 09:44:27 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 12:07:28 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 12:55:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chame", "Hendry Ferreira", ""], ["Tani", "Jun", ""]]}, {"id": "1911.01831", "submitter": "Jonas Degrave", "authors": "Jonas Degrave, Abbas Abdolmaleki, Jost Tobias Springenberg, Nicolas\n  Heess, Martin Riedmiller", "title": "Quinoa: a Q-function You Infer Normalized Over Actions", "comments": "Deep RL Workshop/NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for learning an approximate action-value soft\nQ-function in the relative entropy regularised reinforcement learning setting,\nfor which an optimal improved policy can be recovered in closed form. We use\nrecent advances in normalising flows for parametrising the policy together with\na learned value-function; and show how this combination can be used to\nimplicitly represent Q-values of an arbitrary policy in continuous action\nspace. Using simple temporal difference learning on the Q-values then leads to\na unified objective for policy and value learning. We show how this approach\nconsiderably simplifies standard Actor-Critic off-policy algorithms, removing\nthe need for a policy optimisation step. We perform experiments on a range of\nestablished reinforcement learning benchmarks, demonstrating that our approach\nallows for complex, multimodal policy distributions in continuous action\nspaces, while keeping the process of sampling from the policy both fast and\nexact.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:51:06 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Degrave", "Jonas", ""], ["Abdolmaleki", "Abbas", ""], ["Springenberg", "Jost Tobias", ""], ["Heess", "Nicolas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1911.01952", "submitter": "Wei Huang", "authors": "Wei Huang, Youcheng Sun, Xingyu Zhao, James Sharp, Wenjie Ruan, Jie\n  Meng, and Xiaowei Huang", "title": "Coverage Guided Testing for Recurrent Neural Networks", "comments": "Accepted by IEEE Transactions on Reliability", "journal-ref": null, "doi": "10.1109/TR.2021.3080664", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recurrent neural networks (RNNs) have been applied to a broad range of\napplications, including natural language processing, drug discovery, and video\nrecognition. Their vulnerability to input perturbation is also known. Aligning\nwith a view from software defect detection, this paper aims to develop a\ncoverage guided testing approach to systematically exploit the internal\nbehaviour of RNNs, with the expectation that such testing can detect defects\nwith high possibility. Technically, the long short term memory network (LSTM),\na major class of RNNs, is thoroughly studied. A family of three test metrics\nare designed to quantify not only the values but also the temporal relations\n(including both step-wise and bounded-length) exhibited when LSTM processing\ninputs. A genetic algorithm is applied to efficiently generate test cases. The\ntest metrics and test case generation algorithm are implemented into a tool\nTestRNN, which is then evaluated on a set of LSTM benchmarks. Experiments\nconfirm that TestRNN has advantages over the state-of-art tool DeepStellar and\nattack-based defect detection methods, owing to its working with finer temporal\nsemantics and the consideration of the naturalness of input perturbation.\nFurthermore, TestRNN enables meaningful information to be collected and\nexhibited for users to understand the testing results, which is an important\nstep towards interpretable neural network testing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:21:30 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 12:55:28 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 14:20:55 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Huang", "Wei", ""], ["Sun", "Youcheng", ""], ["Zhao", "Xingyu", ""], ["Sharp", "James", ""], ["Ruan", "Wenjie", ""], ["Meng", "Jie", ""], ["Huang", "Xiaowei", ""]]}, {"id": "1911.01966", "submitter": "Mehdi El Krari", "authors": "Mehdi El Krari, Bela\\\"id Ahiod", "title": "A Memetic Algorithm Based on Breakout Local Search for the Generalized\n  Travelling Salesman Problem", "comments": null, "journal-ref": null, "doi": "10.1080/08839514.2020.1730629", "report-no": null, "categories": "cs.NE cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Travelling Salesman Problem (TSP) is one of the most popular\nCombinatorial Optimization Problem. It is well solicited for the large variety\nof applications that it can solve, but also for its difficulty to find optimal\nsolutions. One of the variants of the TSP is the Generalized TSP (GTSP), where\nthe TSP is considered as a special case which makes the GTSP harder to solve.\nWe propose in this paper a new memetic algorithm based on the well-known\nBreakout Local Search (BLS) metaheuristic to provide good solutions for GTSP\ninstances. Our approach is competitive compared to other recent memetic\nalgorithms proposed for the GTSP and gives at the same time some improvements\nto BLS to reduce its runtime.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 19:03:38 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Krari", "Mehdi El", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1911.01971", "submitter": "Elena Limonova", "authors": "Elena Limonova, Daniil Matveev, Dmitry Nikolaev, Vladimir V. Arlazarov", "title": "Bipolar Morphological Neural Networks: Convolution Without\n  Multiplication", "comments": "Submitted to International Conference on Machine Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we introduce a novel bipolar morphological neuron and bipolar\nmorphological layer models. The models use only such operations as addition,\nsubtraction and maximum inside the neuron and exponent and logarithm as\nactivation functions for the layer. The proposed models unlike previously\nintroduced morphological neural networks approximate the classical computations\nand show better recognition results. We also propose layer-by-layer approach to\ntrain the bipolar morphological networks, which can be further developed to an\nincremental approach for separate neurons to get higher accuracy. Both these\napproaches do not require special training algorithms and can use a variety of\ngradient descent methods. To demonstrate efficiency of the proposed model we\nconsider classical convolutional neural networks and convert the pre-trained\nconvolutional layers to the bipolar morphological layers. Seeing that the\nexperiments on recognition of MNIST and MRZ symbols show only moderate decrease\nof accuracy after conversion and training, bipolar neuron model can provide\nfaster inference and be very useful in mobile and embedded systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:57:35 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Limonova", "Elena", ""], ["Matveev", "Daniil", ""], ["Nikolaev", "Dmitry", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "1911.02150", "submitter": "Noam Shazeer", "authors": "Noam Shazeer", "title": "Fast Transformer Decoding: One Write-Head is All You Need", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-head attention layers, as used in the Transformer neural sequence\nmodel, are a powerful alternative to RNNs for moving information across and\nbetween sequences. While training these layers is generally fast and simple,\ndue to parallelizability across the length of the sequence, incremental\ninference (where such paralleization is impossible) is often slow, due to the\nmemory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\"\ntensors. We propose a variant called multi-query attention, where the keys and\nvalues are shared across all of the different attention \"heads\", greatly\nreducing the size of these tensors and hence the memory bandwidth requirements\nof incremental decoding. We verify experimentally that the resulting models can\nindeed be much faster to decode, and incur only minor quality degradation from\nthe baseline.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 00:19:05 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Shazeer", "Noam", ""]]}, {"id": "1911.02609", "submitter": "Fernando Najman", "authors": "Cecilia Romaro, Fernando Araujo Najman and Morgan Andr\\'e", "title": "A Numerical Study of the Time of Extinction in a Class of Systems of\n  Spiking Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a numerical study of a mathematical model of spiking\nneurons introduced by Ferrari et al. in an article entitled Phase transition\nforinfinite systems of spiking neurons. In this model we have a countable\nnumber of neurons linked together in a network, each of them having a membrane\npotential taking value in the integers, and each of them spiking over time at a\nrate which depends on the membrane potential through some rate function $\\phi$.\nBeside being affected by a spike each neuron can also be affected by leaking.\nAt each of these leak times, which occurs for a given neuron at a fixed rate\n$\\gamma$, the membrane potential of the neuron concerned is spontaneously reset\nto $0$. A wide variety of versions of this model can be considered by choosing\ndifferent graph structures for the network and different activation functions.\nIt was rigorously shown that when the graph structure of the network is the\none-dimensional lattice with a hard threshold for the activation function, this\nmodel presents a phase transition with respect to $\\gamma$, and that it also\npresents a metastable behavior. By the latter we mean that in the sub-critical\nregime the re-normalized time of extinction converges to an exponential random\nvariable of mean 1. It has also been proven that in the super-critical regime\nthe renormalized time of extinction converges in probability to 1. Here, we\ninvestigate numerically a richer class of graph structures and activation\nfunctions. Namely we investigate the case of the two dimensional and the three\ndimensional lattices, as well as the case of a linear function and a sigmoid\nfunction for the activation function. We present numerical evidence that the\nresult of metastability in the sub-critical regime holds for these graphs and\nactivation functions as well as the convergence in probability to $1$ in the\nsuper-critical regime.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 20:02:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Romaro", "Cecilia", ""], ["Najman", "Fernando Araujo", ""], ["Andr\u00e9", "Morgan", ""]]}, {"id": "1911.02624", "submitter": "Nathana\\\"el Fijalkow", "authors": "Judith Clymo, Haik Manukian, Nathana\\\"el Fijalkow, Adri\\`a Gasc\\'on,\n  Brooks Paige", "title": "Data Generation for Neural Programming by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming by example is the problem of synthesizing a program from a small\nset of input / output pairs. Recent works applying machine learning methods to\nthis task show promise, but are typically reliant on generating synthetic\nexamples for training. A particular challenge lies in generating meaningful\nsets of inputs and outputs, which well-characterize a given program and\naccurately demonstrate its behavior. Where examples used for testing are\ngenerated by the same method as training data then the performance of a model\nmay be partly reliant on this similarity. In this paper we introduce a novel\napproach using an SMT solver to synthesize inputs which cover a diverse set of\nbehaviors for a given program. We carry out a case study comparing this method\nto existing synthetic data generation procedures in the literature, and find\nthat data generated using our approach improves both the discriminatory power\nof example sets and the ability of trained machine learning models to\ngeneralize to unfamiliar data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 20:57:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Clymo", "Judith", ""], ["Manukian", "Haik", ""], ["Fijalkow", "Nathana\u00ebl", ""], ["Gasc\u00f3n", "Adri\u00e0", ""], ["Paige", "Brooks", ""]]}, {"id": "1911.03010", "submitter": "Mehdi Ghatee Dr.", "authors": "Mohammad Mahdi Bejani and Mehdi Ghatee", "title": "Regularized Deep Networks in Intelligent Transportation Systems: A\n  Taxonomy and a Case Study", "comments": "A review paper with 8 pages, 2 figures, and 2 tables, submitted to\n  18th International Conference on Traffic & Transportation Engineering,\n  Tehran, February 25-27, 2020. Artificial Intelligence Review (2021)", "journal-ref": null, "doi": "10.1007/s10462-021-09975-1", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent Transportation Systems (ITS) are much correlated with data\nscience mechanisms. Among the different correlation branches, this paper\nfocuses on the neural network learning models. Some of the considered models\nare shallow and they get some user-defined features and learn the relationship,\nwhile deep models extract the necessary features before learning by themselves.\nBoth of these paradigms are utilized in the recent intelligent transportation\nsystems (ITS) to support decision-making by the aid of different operations\nsuch as frequent patterns mining, regression, clustering, and classification.\nWhen these learners cannot generalize the results and just memorize the\ntraining samples, they fail to support the necessities. In these cases, the\ntesting error is bigger than the training error. This phenomenon is addressed\nas overfitting in the literature. Because, this issue decreases the reliability\nof learning systems, in ITS applications, we cannot use such over-fitted\nmachine learning models for different tasks such as traffic prediction, the\nsignal controlling, safety applications, emergency responses, mode detection,\ndriving evaluation, etc. Besides, deep learning models use a great number of\nhyper-parameters, the overfitting in deep models is more attention. To solve\nthis problem, the regularized learning models can be followed. The aim of this\npaper is to review the approaches presented to regularize the overfitting in\ndifferent categories of ITS studies. Then, we give a case study on driving\nsafety that uses a regularized version of the convolutional neural network\n(CNN).\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:03:11 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bejani", "Mohammad Mahdi", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "1911.03329", "submitter": "Stuart Shieber", "authors": "Mirac Suzgun and Sebastian Gehrmann and Yonatan Belinkov and Stuart M.\n  Shieber", "title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce three memory-augmented Recurrent Neural Networks (MARNNs) and\nexplore their capabilities on a series of simple language modeling tasks whose\nsolutions require stack-based mechanisms. We provide the first demonstration of\nneural networks recognizing the generalized Dyck languages, which express the\ncore of what it means to be a language with hierarchical structure. Our\nmemory-augmented architectures are easy to train in an end-to-end fashion and\ncan learn the Dyck languages over as many as six parenthesis-pairs, in addition\nto two deterministic palindrome languages and the string-reversal transduction\ntask, by emulating pushdown automata. Our experiments highlight the increased\nmodeling capacity of memory-augmented models over simple RNNs, while inflecting\nour understanding of the limitations of these models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:33:51 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Suzgun", "Mirac", ""], ["Gehrmann", "Sebastian", ""], ["Belinkov", "Yonatan", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1911.03332", "submitter": "Victor Gimenez-Abalos", "authors": "Victor Gimenez-Abalos, Armand Vilalta, Dario Garcia-Gasulla, Jesus\n  Labarta and Eduard Ayguad\\'e", "title": "Feature discriminativity estimation in CNNs for transfer learning", "comments": "Presented in the 22nd International Conference of the Catalan\n  Association for Artificial Intelligence (CCIA 19)", "journal-ref": "Volume 319: Artificial Intelligence Research and Development 2019", "doi": "10.3233/FAIA190109", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of feature extraction on convolutional neural networks is to\nreuse deep representations learnt for a pre-trained model to solve a new,\npotentially unrelated problem. However, raw feature extraction from all layers\nis unfeasible given the massive size of these networks. Recently, a supervised\nmethod using complexity reduction was proposed, resulting in significant\nimprovements in performance for transfer learning tasks. This approach first\ncomputes the discriminative power of features, and then discretises them using\nthresholds computed for the task. In this paper, we analyse the behaviour of\nthese thresholds, with the purpose of finding a methodology for their\nestimation. After a comprehensive study, we find a very strong correlation\nbetween problem size and threshold value, with coefficient of determination\nabove 90%. These results allow us to propose a unified model for threshold\nestimation, with potential application to transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:37:25 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Gimenez-Abalos", "Victor", ""], ["Vilalta", "Armand", ""], ["Garcia-Gasulla", "Dario", ""], ["Labarta", "Jesus", ""], ["Ayguad\u00e9", "Eduard", ""]]}, {"id": "1911.03404", "submitter": "Grzegorz Brus D.Sc.", "authors": "Szymon Buchaniec, Marek Gnatowski, Grzegorz Brus", "title": "An Analysis of an Integrated Mathematical Modeling -- Artificial Neural\n  Network Approach for the Problems with a Limited Learning Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common and universal problems in science is to investigate a\nfunction. The prediction can be made by an Artificial Neural Network (ANN) or a\nmathematical model. Both approaches have their advantages and disadvantages.\nMathematical models were sought as more trustworthy as their prediction is\nbased on the laws of physics expressed in the form of mathematical equations.\nHowever, the majority of existing mathematical models include different\nempirical parameters, and both approaches inherit inevitable experimental\nerrors. At the same time, the approximation of neural networks can reproduce\nthe solution extremely well if fed with a sufficient amount of data. The\ndifference is that an ANN requires big data to build its accurate approximation\nwhereas a typical mathematical model needs just several data points to estimate\nan empirical constant. Therefore, the common problem that developer meet is the\ninaccuracy of mathematical models and artificial neural network. An another\ncommon challenge is the computational complexity of the mathematical models, or\nlack of data for a sufficient precision of the Artificial Neural Networks. In\nthe presented paper those problems are addressed using the integration of a\nmathematical model with an artificial neural network. In the presented\nanalysis, an ANN predicts just a part of the mathematical model and its weights\nand biases are adjusted based on the output of the mathematical model. The\nperformance of Integrated Mathematical modeling - Artificial Neural Network\n(IMANN) is compared to a Dense Neural Network (DNN) with the use of the\nbenchmarking functions. The obtained calculation results indicate that such an\napproach could lead to an increase of precision as well as limiting the\ndata-set required for learning.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 17:38:58 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:54:52 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Buchaniec", "Szymon", ""], ["Gnatowski", "Marek", ""], ["Brus", "Grzegorz", ""]]}, {"id": "1911.03409", "submitter": "Parthe Pandit", "authors": "Parthe Pandit, Mojtaba Sahraee-Ardakan, Sundeep Rangan, Philip\n  Schniter, Alyson K. Fletcher", "title": "Inference with Deep Generative Priors in High Dimensions", "comments": "50 pages, double-spaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NE eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative priors offer powerful models for complex-structured data,\nsuch as images, audio, and text. Using these priors in inverse problems\ntypically requires estimating the input and/or hidden signals in a multi-layer\ndeep neural network from observation of its output. While these approaches have\nbeen successful in practice, rigorous performance analysis is complicated by\nthe non-convex nature of the underlying optimization problems. This paper\npresents a novel algorithm, Multi-Layer Vector Approximate Message Passing\n(ML-VAMP), for inference in multi-layer stochastic neural networks. ML-VAMP can\nbe configured to compute maximum a priori (MAP) or approximate minimum\nmean-squared error (MMSE) estimates for these networks. We show that the\nperformance of ML-VAMP can be exactly predicted in a certain high-dimensional\nrandom limit. Furthermore, under certain conditions, ML-VAMP yields estimates\nthat achieve the minimum (i.e., Bayes-optimal) MSE as predicted by the replica\nmethod. In this way, ML-VAMP provides a computationally efficient method for\nmulti-layer inference with an exact performance characterization and testable\nconditions for optimality in the large-system limit.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 17:54:10 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Pandit", "Parthe", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Rangan", "Sundeep", ""], ["Schniter", "Philip", ""], ["Fletcher", "Alyson K.", ""]]}, {"id": "1911.03439", "submitter": "Amir Dehsarvi", "authors": "Amir Dehsarvi, Jennifer Kay South Palomares, Stephen Leslie Smith", "title": "Towards Monitoring Parkinson's Disease Following Drug Treatment: CGP\n  Classification of rs-MRI Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.05378", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: It is commonly accepted that accurate monitoring of\nneurodegenerative diseases is crucial for effective disease management and\ndelivery of medication and treatment. This research develops automatic clinical\nmonitoring techniques for PD, following treatment, using the novel application\nof EAs. Specifically, the research question addressed was: Can accurate\nmonitoring of PD be achieved using EAs on rs-fMRI data for patients prescribed\nModafinil (typically prescribed for PD patients to relieve physical fatigue)?\nMethods: This research develops novel clinical monitoring tools using data from\na controlled experiment where participants were administered Modafinil versus\nplacebo, examining the novel application of EAs to both map and predict the\nfunctional connectivity in participants using rs-fMRI data. Specifically, CGP\nwas used to classify DCM analysis and timeseries data. Results were validated\nwith two other commonly used classification methods (ANN and SVM) and via\nk-fold cross-validation. Results: Findings revealed a maximum accuracy of\n74.57% for CGP. Furthermore, CGP provided comparable performance accuracy\nrelative to ANN and SVM. Nevertheless, EAs enable us to decode the classifier,\nin terms of understanding the data inputs that are used, more easily than in\nANN and SVM. Conclusions: These findings underscore the applicability of both\nDCM analyses for classification and CGP as a novel classification technique for\nbrain imaging data with medical implications for medication monitoring.\nFurthermore, classification of fMRI data for research typically involves\nstatistical modelling techniques being often hypothesis driven, whereas EAs use\ndata-driven explanatory modelling methods resulting in numerous benefits. DCM\nanalysis is novel for classification and advantageous as it provides\ninformation on the causal links between different brain regions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:46:50 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Dehsarvi", "Amir", ""], ["Palomares", "Jennifer Kay South", ""], ["Smith", "Stephen Leslie", ""]]}, {"id": "1911.03540", "submitter": "Marko Angjelichinoski", "authors": "Marko Angjelichinoski, John Choi, Taposh Banerjee, Bijan Pesaran and\n  Vahid Tarokh", "title": "Cross-subject Decoding of Eye Movement Goals from Local Field Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. We consider the cross-subject decoding problem from local field\npotential (LFP) signals, where training data collected from the prefrontal\ncortex (PFC) of a source subject is used to decode intended motor actions in a\ndestination subject. Approach. We propose a novel supervised transfer learning\ntechnique, referred to as data centering, which is used to adapt the feature\nspace of the source to the feature space of the destination. The key\ningredients of data centering are the transfer functions used to model the\ndeterministic component of the relationship between the source and destination\nfeature spaces. We propose an efficient data-driven estimation approach for\nlinear transfer functions that uses the first and second order moments of the\nclass-conditional distributions. Main result. We apply our data centering\ntechnique with linear transfer functions for cross-subject decoding of eye\nmovement intentions in an experiment where two macaque monkeys perform\nmemory-guided visual saccades to one of eight target locations. The results\nshow peak cross-subject decoding performance of $80\\%$, which marks a\nsubstantial improvement over random choice decoder. In addition to this, data\ncentering also outperforms standard sampling-based methods in setups with\nimbalanced training data. Significance. The analyses presented herein\ndemonstrate that the proposed data centering is a viable novel technique for\nreliable LFP-based cross-subject brain-computer interfacing and neural\nprostheses.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 21:12:19 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 20:34:55 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 20:47:21 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Angjelichinoski", "Marko", ""], ["Choi", "John", ""], ["Banerjee", "Taposh", ""], ["Pesaran", "Bijan", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1911.03614", "submitter": "Yiming Cui", "authors": "Ziqing Yang, Yiming Cui, Wanxiang Che, Ting Liu, Shijin Wang, Guoping\n  Hu", "title": "Improving Machine Reading Comprehension via Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) as a regularization method has proved its\neffectiveness in various tasks, such as image classification and text\nclassification. Though there are successful applications of AT in many tasks of\nnatural language processing (NLP), the mechanism behind it is still unclear. In\nthis paper, we aim to apply AT on machine reading comprehension (MRC) and study\nits effects from multiple perspectives. We experiment with three different\nkinds of RC tasks: span-based RC, span-based RC with unanswerable questions and\nmulti-choice RC. The experimental results show that the proposed method can\nimprove the performance significantly and universally on SQuAD1.1, SQuAD2.0 and\nRACE. With virtual adversarial training (VAT), we explore the possibility of\nimproving the RC models with semi-supervised learning and prove that examples\nfrom a different task are also beneficial. We also find that AT helps little in\ndefending against artificial adversarial examples, but AT helps the model to\nlearn better on examples that contain more low-frequency words.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 05:31:40 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Yang", "Ziqing", ""], ["Cui", "Yiming", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1911.03630", "submitter": "Saeed Reza Kheradpisheh", "authors": "Aref Moqadam Mehr, Saeed Reza Kheradpisheh, Hadi Farahani", "title": "Action Recognition Using Supervised Spiking Neural Networks", "comments": "We found a bug in our implementations and we should admit that our\n  reported results were wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neurons use spikes to process and learn temporally dynamic inputs\nin an energy and computationally efficient way. However, applying the\nstate-of-the-art gradient-based supervised algorithms to spiking neural\nnetworks (SNN) is a challenge due to the non-differentiability of the\nactivation function of spiking neurons. Employing surrogate gradients is one of\nthe main solutions to overcome this challenge. Although SNNs naturally work in\nthe temporal domain, recent studies have focused on developing SNNs to solve\nstatic image categorization tasks. In this paper, we employ a surrogate\ngradient descent learning algorithm to recognize twelve human hand gestures\nrecorded by dynamic vision sensor (DVS) cameras. The proposed SNN could reach\n97.2% recognition accuracy on test data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 07:16:10 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 11:07:11 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mehr", "Aref Moqadam", ""], ["Kheradpisheh", "Saeed Reza", ""], ["Farahani", "Hadi", ""]]}, {"id": "1911.03710", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Evolution of Cooperation for Multiple Mutant Configurations on All\n  Regular Graphs with $N \\leq 14$ players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NE math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the emergence of cooperation in structured populations with any\narrangement of cooperators and defectors on the evolutionary graph. Using\nstructure coefficients defined for configurations describing such arrangements\nof any number of mutants, we provide results for weak selection to favor\ncooperation over defection on any regular graph with $N \\leq 14$ vertices.\nFurthermore, the properties of graphs that particularly promote cooperation are\nanalyzed. It is shown that the number of graph cycles of certain length is a\ngood predictor for the values of the structure coefficient, and thus a tendency\nto favor cooperation. Another property of particularly cooperation-promoting\nregular graphs with a low degree is that they are structured to have blocks\nwith clusters of mutants that are connected by cut vertices and/or hinge\nvertices.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 15:07:22 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "1911.03738", "submitter": "Marc Tanti", "authors": "Marc Tanti and Albert Gatt and Kenneth P. Camilleri", "title": "On Architectures for Including Visual Information in Neural Language\n  Models for Image Description", "comments": "145 pages, 41 figures, 15 tables, Doctoral thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neural language model can be conditioned into generating descriptions for\nimages by providing visual information apart from the sentence prefix. This\nvisual information can be included into the language model through different\npoints of entry resulting in different neural architectures. We identify four\nmain architectures which we call init-inject, pre-inject, par-inject, and\nmerge.\n  We analyse these four architectures and conclude that the best performing one\nis init-inject, which is when the visual information is injected into the\ninitial state of the recurrent neural network. We confirm this using both\nautomatic evaluation measures and human annotation.\n  We then analyse how much influence the images have on each architecture. This\nis done by measuring how different the output probabilities of a model are when\na partial sentence is combined with a completely different image from the one\nit is meant to be combined with. We find that init-inject tends to quickly\nbecome less influenced by the image as more words are generated. A different\narchitecture called merge, which is when the visual information is merged with\nthe recurrent neural network's hidden state vector prior to output, loses\nvisual influence much more slowly, suggesting that it would work better for\ngenerating longer sentences.\n  We also observe that the merge architecture can have its recurrent neural\nnetwork pre-trained in a text-only language model (transfer learning) rather\nthan be initialised randomly as usual. This results in even better performance\nthan the other architectures, provided that the source language model is not\ntoo good at language modelling or it will overspecialise and be less effective\nat image description generation.\n  Our work opens up new avenues of research in neural architectures,\nexplainable AI, and transfer learning.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 17:07:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Camilleri", "Kenneth P.", ""]]}, {"id": "1911.03769", "submitter": "Jorge Gomez Robles", "authors": "J. Gomez Robles, J. Vanschoren", "title": "Learning to reinforcement learn for Neural Architecture Search", "comments": "32 pages, 21 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) is a goal-oriented learning solution that has\nproven to be successful for Neural Architecture Search (NAS) on the CIFAR and\nImageNet datasets. However, a limitation of this approach is its high\ncomputational cost, making it unfeasible to replay it on other datasets.\nThrough meta-learning, we could bring this cost down by adapting previously\nlearned policies instead of learning them from scratch. In this work, we\npropose a deep meta-RL algorithm that learns an adaptive policy over a set of\nenvironments, making it possible to transfer it to previously unseen tasks. The\nalgorithm was applied to various proof-of-concept environments in the past, but\nwe adapt it to the NAS problem. We empirically investigate the agent's behavior\nduring training when challenged to design chain-structured neural architectures\nfor three datasets with increasing levels of hardness, to later fix the policy\nand evaluate it on two unseen datasets of different difficulty. Our results\nshow that, under resource constraints, the agent effectively adapts its\nstrategy during training to design better architectures than the ones designed\nby a standard RL algorithm, and can design good architectures during the\nevaluation on previously unseen environments. We also provide guidelines on the\napplicability of our framework in a more complex NAS setting by studying the\nprogress of the agent when challenged to design multi-branch architectures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 20:13:00 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 22:22:35 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Robles", "J. Gomez", ""], ["Vanschoren", "J.", ""]]}, {"id": "1911.04021", "submitter": "Abdelrahman Hosny", "authors": "Abdelrahman Hosny, Soheil Hashemi, Mohamed Shalan and Sherief Reda", "title": "DRiLLS: Deep Reinforcement Learning for Logic Synthesis", "comments": "ASPDAC'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic synthesis requires extensive tuning of the synthesis optimization flow\nwhere the quality of results (QoR) depends on the sequence of optimizations\nused. Efficient design space exploration is challenging due to the exponential\nnumber of possible optimization permutations. Therefore, automating the\noptimization process is necessary. In this work, we propose a novel\nreinforcement learning-based methodology that navigates the optimization space\nwithout human intervention. We demonstrate the training of an Advantage Actor\nCritic (A2C) agent that seeks to minimize area subject to a timing constraint.\nUsing the proposed methodology, designs can be optimized autonomously with\nno-humans in-loop. Evaluation on the comprehensive EPFL benchmark suite shows\nthat the agent outperforms existing exploration methodologies and improves QoRs\nby an average of 13%.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 00:38:39 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 04:07:24 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Hosny", "Abdelrahman", ""], ["Hashemi", "Soheil", ""], ["Shalan", "Mohamed", ""], ["Reda", "Sherief", ""]]}, {"id": "1911.04477", "submitter": "Xianda Xu", "authors": "Xianda Xu, Marco Pedersoli", "title": "A Computing Kernel for Network Binarization on PyTorch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have now achieved state-of-the-art results in a wide\nrange of tasks including image classification, object detection and so on.\nHowever, they are both computation consuming and memory intensive, making them\ndifficult to deploy on low-power devices. Network binarization is one of the\nexisting effective techniques for model compression and acceleration, but there\nis no computing kernel yet to support it on PyTorch. In this paper we developed\na computing kernel supporting 1-bit xnor and bitcount computation on PyTorch.\nExperimental results show that our kernel could accelerate the inference of the\nbinarized neural network by 3 times in GPU and by 4.5 times in CPU compared\nwith the control group.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:26:04 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Xu", "Xianda", ""], ["Pedersoli", "Marco", ""]]}, {"id": "1911.04658", "submitter": "Jialong Shi", "authors": "Jialong Shi, Jianyong Sun, Qingfu Zhang", "title": "Multi-objectivization Inspired Metaheuristics for the Sum-of-the-Parts\n  Combinatorial Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objectivization is a term used to describe strategies developed for\noptimizing single-objective problems by multi-objective algorithms. This paper\nfocuses on multi-objectivizing the sum-of-the-parts combinatorial optimization\nproblems, which include the traveling salesman problem, the unconstrained\nbinary quadratic programming and other well-known combinatorial optimization\nproblem. For a sum-of-the-parts combinatorial optimization problem, we propose\nto decompose its original objective into two sub-objectives with controllable\ncorrelation. Based on the decomposition method, two new multi-objectivization\ninspired single-objective optimization techniques called non-dominance search\nand non-dominance exploitation are developed, respectively. Non-dominance\nsearch is combined with two metaheuristics, namely iterated local search and\niterated tabu search, while non-dominance exploitation is embedded within the\niterated Lin-Kernighan metaheuristic. The resultant metaheuristics are called\nILS+NDS, ITS+NDS and ILK+NDE, respectively. Empirical studies on some TSP and\nUBQP instances show that with appropriate correlation between the\nsub-objectives, there are more chances to escape from local optima when new\nstarting solution is selected from the non-dominated solutions defined by the\ndecomposed sub-objectives. Experimental results also show that ILS+NDS, ITS+NDS\nand ILK+NDE all significantly outperform their counterparts on most of the test\ninstances.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:46:51 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 11:59:07 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 11:04:21 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Shi", "Jialong", ""], ["Sun", "Jianyong", ""], ["Zhang", "Qingfu", ""]]}, {"id": "1911.05020", "submitter": "Jianjun Hu", "authors": "Yabo Dan, Yong Zhao, Xiang Li, Shaobo Li, Ming Hu and Jianjun Hu", "title": "Generative adversarial networks (GAN) based efficient sampling of\n  chemical space for inverse design of inorganic materials", "comments": "15 pages", "journal-ref": "npj Comput Mater 6, 84 (2020)", "doi": "10.1038/s41524-020-00352-0", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A major challenge in materials design is how to efficiently search the vast\nchemical design space to find the materials with desired properties. One\neffective strategy is to develop sampling algorithms that can exploit both\nexplicit chemical knowledge and implicit composition rules embodied in the\nlarge materials database. Here, we propose a generative machine learning model\n(MatGAN) based on a generative adversarial network (GAN) for efficient\ngeneration of new hypothetical inorganic materials. Trained with materials from\nthe ICSD database, our GAN model can generate hypothetical materials not\nexisting in the training dataset, reaching a novelty of 92.53% when generating\n2 million samples. The percentage of chemically valid (charge neutral and\nelectronegativity balanced) samples out of all generated ones reaches 84.5% by\nour GAN when trained with materials from ICSD even though no such chemical\nrules are explicitly enforced in our GAN model, indicating its capability to\nlearn implicit chemical composition rules. Our algorithm could be used to speed\nup inverse design or computational screening of inorganic materials.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:31:37 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Dan", "Yabo", ""], ["Zhao", "Yong", ""], ["Li", "Xiang", ""], ["Li", "Shaobo", ""], ["Hu", "Ming", ""], ["Hu", "Jianjun", ""]]}, {"id": "1911.05266", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Akshay Chawla, Marios Savvides", "title": "Learning Non-Parametric Invariances from Data with Permanent Random\n  Connectomes", "comments": "Preprint (accepted at NeurIPS SVRHM 2019 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in supervised classification and in machine\nlearning in general, is the modelling of non-parametric invariances that exist\nin data. Most prior art has focused on enforcing priors in the form of\ninvariances to parametric nuisance transformations that are expected to be\npresent in data. Learning non-parametric invariances directly from data remains\nan important open problem. In this paper, we introduce a new architectural\nlayer for convolutional networks which is capable of learning general\ninvariances from data itself. This layer can learn invariance to non-parametric\ntransformations and interestingly, motivates and incorporates permanent random\nconnectomes, thereby being called Permanent Random Connectome Non-Parametric\nTransformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with\nrandom connections (not just weights) which are a small subset of the\nconnections in a fully connected convolution layer. Importantly, these\nconnections in PRC-NPTNs once initialized remain permanent throughout training\nand testing. Permanent random connectomes make these architectures loosely more\nbiologically plausible than many other mainstream network architectures which\nrequire highly ordered structures. We motivate randomly initialized connections\nas a simple method to learn invariance from data itself while invoking\ninvariance towards multiple nuisance transformations simultaneously. We find\nthat these randomly initialized permanent connections have positive effects on\ngeneralization, outperform much larger ConvNet baselines and the recently\nproposed Non-Parametric Transformation Network (NPTN) on benchmarks that\nenforce learning invariances from the data itself.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:03:48 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 15:32:41 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 00:54:18 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Pal", "Dipan K.", ""], ["Chawla", "Akshay", ""], ["Savvides", "Marios", ""]]}, {"id": "1911.05294", "submitter": "Madyan Alsenwi", "authors": "Madyan Alsenwi, Kitae Kim, and Choong Seon Hong", "title": "Radio Resource Allocation in 5G New Radio: A Neural Networks Based\n  Approach)", "comments": null, "journal-ref": "Journal of KIISE, Vol. 46, No. 9, pp.961-967, 2019", "doi": "10.5626/JOK.2019.46.9.961", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum frequency-time unit that can be allocated to User Equipments\n(UEs) in the fifth generation (5G) cellular networks is a Resource Block (RB).\nA RB is a channel composed of a set of OFDM subcarriers for a given time slot\nduration. 5G New Radio (NR) allows for a large number of block shapes ranging\nfrom 15 kHz to 480 kHz. In this paper, we address the problem of RBs allocation\nto UEs. The RBs are allocated at the beginning of each time slot based on the\nchannel state of each UE. The problem is formulated based on the Generalized\nProportional Fair (GPF) scheduling. Then, we model the problem as a 2-Dimension\nHopfield Neural Networks (2D-HNN). Finally, in an attempt to solve the problem,\nthe energy function of 2D-HNN is investigated. Simulation results show the\nefficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 05:01:52 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Alsenwi", "Madyan", ""], ["Kim", "Kitae", ""], ["Hong", "Choong Seon", ""]]}, {"id": "1911.05371", "submitter": "Yuki Asano", "authors": "Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi", "title": "Self-labelling via simultaneous clustering and representation learning", "comments": "Accepted paper at the International Conference on Learning\n  Representations (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining clustering and representation learning is one of the most promising\napproaches for unsupervised learning of deep neural networks. However, doing so\nnaively leads to ill posed learning problems with degenerate solutions. In this\npaper, we propose a novel and principled learning formulation that addresses\nthese issues. The method is obtained by maximizing the information between\nlabels and input data indices. We show that this criterion extends standard\ncrossentropy minimization to an optimal transport problem, which we solve\nefficiently for millions of input images and thousands of labels using a fast\nvariant of the Sinkhorn-Knopp algorithm. The resulting method is able to\nself-label visual data so as to train highly competitive image representations\nwithout manual labels. Our method achieves state of the art representation\nlearning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and\nImageNet and yields the first self-supervised AlexNet that outperforms the\nsupervised Pascal VOC detection baseline. Code and models are available.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 09:47:49 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 13:22:38 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 18:03:39 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Asano", "Yuki Markus", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1911.05479", "submitter": "Asim Iqbal", "authors": "Asim Iqbal, Phil Dong, Christopher M Kim, Heeun Jang", "title": "Decoding Neural Responses in Mouse Visual Cortex through a Deep Neural\n  Network", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN).\n  IEEE, 2019", "doi": "10.1109/IJCNN.2019.8852121", "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a code to unravel the population of neural responses that leads to a\ndistinct animal behavior has been a long-standing question in the field of\nneuroscience. With the recent advances in machine learning, it is shown that\nthe hierarchically Deep Neural Networks (DNNs) perform optimally in decoding\nunique features out of complex datasets. In this study, we utilize the power of\na DNN to explore the computational principles in the mammalian brain by\nexploiting the Neuropixel data from Allen Brain Institute. We decode the neural\nresponses from mouse visual cortex to predict the presented stimuli to the\nanimal for natural (bear, trees, cheetah, etc.) and artificial (drifted\ngratings, orientated bars, etc.) classes. Our results indicate that neurons in\nmouse visual cortex encode the features of natural and artificial objects in a\ndistinct manner, and such neural code is consistent across animals. We\ninvestigate this by applying transfer learning to train a DNN on the neural\nresponses of a single animal and test its generalized performance across\nmultiple animals. Within a single animal, DNN is able to decode the neural\nresponses with as much as 100% classification accuracy. Across animals, this\naccuracy is reduced to 91%. This study demonstrates the potential of utilizing\nthe DNN models as a computational framework to understand the neural coding\nprinciples in the mammalian brain.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:02:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Iqbal", "Asim", ""], ["Dong", "Phil", ""], ["Kim", "Christopher M", ""], ["Jang", "Heeun", ""]]}, {"id": "1911.05484", "submitter": "Mohammad Amin", "authors": "M. Amin, F. Safaei, N. S. Ghaderian", "title": "Extracting a Discriminative Structural Sub-Network for ASD Screening\n  using the Evolutionary Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autism spectrum disorder (ASD) is one of the most significant neurological\ndisorders that disrupt a person's social communication skills. The progression\nand development of neuroimaging technologies has made structural network\nconstruction of brain regions possible. In this paper, after finding the\ndiscriminative sub-network using the evolutionary algorithm, the simple\nfeatures of the sub-network lead us to diagnose autism in various subjects with\nplausible accuracy (76% on average). This method yields substantially better\nresults compared to previous researches. Thus, this method may be used as an\naccurate assistance in autism screening\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 12:21:58 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Amin", "M.", ""], ["Safaei", "F.", ""], ["Ghaderian", "N. S.", ""]]}, {"id": "1911.05521", "submitter": "Felix Christian Bauer", "authors": "Felix Christian Bauer, Dylan Richard Muir, Giacomo Indiveri", "title": "Real-time ultra-low power ECG anomaly detection using an event-driven\n  neuromorphic processor", "comments": null, "journal-ref": null, "doi": "10.1109/TBCAS.2019.2953001", "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of pathological conditions in human subjects can be\nachieved through off-line analysis of recorded biological signals such as\nelectrocardiograms (ECGs). However, human diagnosis is time-consuming and\nexpensive, as it requires the time of medical professionals. This is especially\ninefficient when indicative patterns in the biological signals are infrequent.\nMoreover, patients with suspected pathologies are often monitored for extended\nperiods, requiring the storage and examination of large amounts of\nnon-pathological data, and entailing a difficult visual search task for\ndiagnosing professionals.\n  In this work we propose a compact and sub-mW low power neural processing\nsystem that can be used to perform on-line and real-time preliminary diagnosis\nof pathological conditions, to raise warnings for the existence of possible\npathological conditions, or to trigger an off-line data recording system for\nfurther analysis by a medical professional. We apply the system to real-time\nclassification of ECG data for distinguishing between healthy heartbeats and\npathological rhythms.\n  Multi-channel analog ECG traces are encoded as asynchronous streams of binary\nevents and processed using a spiking recurrent neural network operated in a\nreservoir computing paradigm. An event-driven neuron output layer is then\ntrained to recognize one of several pathologies. Finally, the filtered activity\nof this output layer is used to generate a binary trigger signal indicating the\npresence or absence of a pathological pattern.\n  We validate the approach proposed using a Dynamic Neuromorphic Asynchronous\nProcessor (DYNAP) chip, implemented using a standard 180 nm CMOS VLSI process,\nand present experimental results measured from the chip.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 14:56:36 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Bauer", "Felix Christian", ""], ["Muir", "Dylan Richard", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1911.05640", "submitter": "F{\\i}rat Tuna", "authors": "Firat Tuna", "title": "Neural Network Processing Neural Networks: An efficient way to learn\n  higher order functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functions are rich in meaning and can be interpreted in a variety of ways.\nNeural networks were proven to be capable of approximating a large class of\nfunctions[1]. In this paper, we propose a new class of neural networks called\n\"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural\nnetworks and numerical values, instead of just numerical values. Thus enabling\nneural networks to represent and process rich structures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 19:15:34 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 23:11:27 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Tuna", "Firat", ""]]}, {"id": "1911.05916", "submitter": "Ziang Yan", "authors": "Ziang Yan, Yiwen Guo, Changshui Zhang", "title": "Adversarial Margin Maximization Networks", "comments": "11 pages + 1 page appendix, accepted by T-PAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2948348", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous recent success of deep neural networks (DNNs) has sparked a\nsurge of interest in understanding their predictive ability. Unlike the human\nvisual system which is able to generalize robustly and learn with little\nsupervision, DNNs normally require a massive amount of data to learn new\nconcepts. In addition, research works also show that DNNs are vulnerable to\nadversarial examples-maliciously generated images which seem perceptually\nsimilar to the natural ones but are actually formed to fool learning models,\nwhich means the models have problem generalizing to unseen data with certain\ntype of distortions. In this paper, we analyze the generalization ability of\nDNNs comprehensively and attempt to improve it from a geometric point of view.\nWe propose adversarial margin maximization (AMM), a learning-based\nregularization which exploits an adversarial perturbation as a proxy. It\nencourages a large margin in the input space, just like the support vector\nmachines. With a differentiable formulation of the perturbation, we train the\nregularized DNNs simply through back-propagation in an end-to-end manner.\nExperimental results on various datasets (including MNIST, CIFAR-10/100, SVHN\nand ImageNet) and different DNN architectures demonstrate the superiority of\nour method over previous state-of-the-arts. Code and models for reproducing our\nresults will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 03:13:17 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Yan", "Ziang", ""], ["Guo", "Yiwen", ""], ["Zhang", "Changshui", ""]]}, {"id": "1911.06105", "submitter": "Aaron Vose", "authors": "Aaron D. Vose, Jacob Balma, Damon Farnsworth, Kaylie Anderson, and\n  Yuri K. Peterson", "title": "PharML.Bind: Pharmacologic Machine Learning for Protein-Ligand\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it feasible to create an analysis paradigm that can analyze and then\naccurately and quickly predict known drugs from experimental data? PharML.Bind\nis a machine learning toolkit which is able to accomplish this feat. Utilizing\ndeep neural networks and big data, PharML.Bind correlates\nexperimentally-derived drug affinities and protein-ligand X-ray structures to\ncreate novel predictions. The utility of PharML.Bind is in its application as a\nrapid, accurate, and robust prediction platform for discovery and personalized\nmedicine. This paper demonstrates that graph neural networks (GNNs) can be\ntrained to screen hundreds of thousands of compounds against thousands of\ntargets in minutes, a vastly shorter time than previous approaches. This\nmanuscript presents results from training and testing using the entirety of\nBindingDB after cleaning; this includes a test set with 19,708 X-ray structures\nand 247,633 drugs, leading to 2,708,151 unique protein-ligand pairings.\nPharML.Bind achieves a prodigious 98.3% accuracy on this test set in under 25\nminutes. PharML.Bind is premised on the following key principles: 1) speed and\na high enrichment factor per unit compute time, provided by high-quality\ntraining data combined with a novel GNN architecture and use of\nhigh-performance computing resources, 2) the ability to generalize to proteins\nand drugs outside of the training set, including those with unknown active\nsites, through the use of an active-site-agnostic GNN mapping, and 3) the\nability to be easily integrated as a component of increasingly-complex\nprediction and analysis pipelines. PharML.Bind represents a timely and\npractical approach to leverage the power of machine learning to efficiently\nanalyze and predict drug action on any practical scale and will provide utility\nin a variety of discovery and medical applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 23:32:44 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Vose", "Aaron D.", ""], ["Balma", "Jacob", ""], ["Farnsworth", "Damon", ""], ["Anderson", "Kaylie", ""], ["Peterson", "Yuri K.", ""]]}, {"id": "1911.06276", "submitter": "Federico Bertoni", "authors": "Federico Bertoni, Giovanna Citti, Alessandro Sarti", "title": "LGN-CNN: a biologically inspired CNN architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a biologically inspired Convolutional Neural\nNetwork (CNN) architecture called LGN-CNN that has a first convolutional layer\ncomposed by a single filter that mimics the role of the Lateral Geniculate\nNucleus (LGN). The first layer of the neural network shows a rotationally\nsymmetric pattern justified by the structure of the net itself that turns up to\nbe an approximation of a Laplacian of Gaussian (LoG). The latter function is in\nturn a good approximation of the receptive profiles (RPs) of the cells in the\nLGN. The analogy with respect to the visual system structure is established,\nemerging directly from the architecture of the neural network. A proof of\nrotation invariance of the first layer is given on a fixed LGN-CNN architecture\nand the computational results are shown. Thus, contrast invariance capability\nof the LGN-CNN is investigated and a comparison between the Retinex effects of\nthe first layer of LGN-CNN and the Retinex effects of a LoG is provided on\ndifferent images. A statistical study is done on the filters of the second\nconvolutional layer with respect to biological data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:00:14 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 15:15:43 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bertoni", "Federico", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1911.06322", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "Auto-encoding a Knowledge Graph Using a Deep Belief Network: A Random\n  Fields Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We started with a knowledge graph of connected entities and descriptive\nproperties of those entities, from which, a hierarchical representation of the\nknowledge graph is derived. Using a graphical, energy-based neural network, we\nare able to show that the structure of the hierarchy can be internally captured\nby the neural network, which allows for efficient output of the underlying\nequilibrium distribution from which the data are drawn.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 22:41:21 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:48:56 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 19:09:52 GMT"}, {"version": "v4", "created": "Thu, 26 Dec 2019 02:17:47 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1911.06471", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi and Mohammad Samragh and Tara Javidi and Farinaz\n  Koushanfar", "title": "ASCAI: Adaptive Sampling for acquiring Compact AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ASCAI, a novel adaptive sampling methodology that can\nlearn how to effectively compress Deep Neural Networks (DNNs) for accelerated\ninference on resource-constrained platforms. Modern DNN compression techniques\ncomprise various hyperparameters that require per-layer customization to ensure\nhigh accuracy. Choosing such hyperparameters is cumbersome as the pertinent\nsearch space grows exponentially with the number of model layers. To\neffectively traverse this large space, we devise an intelligent sampling\nmechanism that adapts the sampling strategy using customized operations\ninspired by genetic algorithms. As a special case, we consider the space of\nmodel compression as a vector space. The adaptively selected samples enable\nASCAI to automatically learn how to tune per-layer compression hyperparameters\nto optimize the accuracy/model-size trade-off. Our extensive evaluations show\nthat ASCAI outperforms rule-based and reinforcement learning methods in terms\nof compression rate and/or accuracy\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 04:13:55 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Samragh", "Mohammad", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1911.06556", "submitter": "Marco Gallieri", "authors": "Marco Gallieri and Seyed Sina Mirrazavi Salehian and Nihat Engin Toklu\n  and Alessio Quaglino and Jonathan Masci and Jan Koutn\\'ik and Faustino Gomez", "title": "Safe Interactive Model-Based Learning", "comments": "NeurIPS 2019 workshop on Safety and Robustness in Decision-Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control applications present hard operational constraints. A violation of\nthese can result in unsafe behavior. This paper introduces Safe Interactive\nModel Based Learning (SiMBL), a framework to refine an existing controller and\na system model while operating on the real environment. SiMBL is composed of\nthe following trainable components: a Lyapunov function, which determines a\nsafe set; a safe control policy; and a Bayesian RNN forward model. A min-max\ncontrol framework, based on alternate minimisation and backpropagation through\nthe forward model, is used for the offline computation of the controller and\nthe safe set. Safety is formally verified a-posteriori with a probabilistic\nmethod that utilizes the Noise Contrastive Priors (NPC) idea to build a\nBayesian RNN forward model with an additive state uncertainty estimate which is\nlarge outside the training data distribution. Iterative refinement of the model\nand the safe set is achieved thanks to a novel loss that conditions the\nuncertainty estimates of the new model to be close to the current one. The\nlearned safe set and model can also be used for safe exploration, i.e., to\ncollect data within the safe invariant set, for which a simple one-step MPC is\nproposed. The single components are tested on the simulation of an inverted\npendulum with limited torque and stability region, showing that iteratively\nadding more data can improve the model, the controller and the size of the safe\nregion.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 10:34:45 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 09:54:23 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Gallieri", "Marco", ""], ["Salehian", "Seyed Sina Mirrazavi", ""], ["Toklu", "Nihat Engin", ""], ["Quaglino", "Alessio", ""], ["Masci", "Jonathan", ""], ["Koutn\u00edk", "Jan", ""], ["Gomez", "Faustino", ""]]}, {"id": "1911.06602", "submitter": "Toby St Clere Smithe", "authors": "Toby B. St Clere Smithe", "title": "Radically Compositional Cognitive Concepts", "comments": "6 pages, 2 figures; NeurIPS 2019 Context and Compositionality\n  workshop. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite ample evidence that our concepts, our cognitive architecture, and\nmathematics itself are all deeply compositional, few models take advantage of\nthis structure. We therefore propose a radically compositional approach to\ncomputational neuroscience, drawing on the methods of applied category theory.\nWe describe how these tools grant us a means to overcome complexity and improve\ninterpretability, and supply a rigorous common language for scientific\nmodelling, analogous to the type theories of computer science. As a case study,\nwe sketch how to translate from compositional narrative concepts to neural\ncircuits and back again.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:20:36 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Smithe", "Toby B. St Clere", ""]]}, {"id": "1911.06641", "submitter": "Zhiwei Liang", "authors": "Zhiyue Liu, Jiahai Wang, Zhiwei Liang", "title": "CatGAN: Category-aware Generative Adversarial Networks with Hierarchical\n  Evolutionary Learning for Category Text Generation", "comments": "15 pages, 4 figures. Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating multiple categories of texts is a challenging task and draws more\nand more attention. Since generative adversarial nets (GANs) have shown\ncompetitive results on general text generation, they are extended for category\ntext generation in some previous works. However, the complicated model\nstructures and learning strategies limit their performance and exacerbate the\ntraining instability. This paper proposes a category-aware GAN (CatGAN) which\nconsists of an efficient category-aware model for category text generation and\na hierarchical evolutionary learning algorithm for training our model. The\ncategory-aware model directly measures the gap between real samples and\ngenerated samples on each category, then reducing this gap will guide the model\nto generate high-quality category samples. The Gumbel-Softmax relaxation\nfurther frees our model from complicated learning strategies for updating\nCatGAN on discrete data. Moreover, only focusing on the sample quality normally\nleads the mode collapse problem, thus a hierarchical evolutionary learning\nalgorithm is introduced to stabilize the training procedure and obtain the\ntrade-off between quality and diversity while training CatGAN. Experimental\nresults demonstrate that CatGAN outperforms most of the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 14:03:30 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 09:16:28 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Liu", "Zhiyue", ""], ["Wang", "Jiahai", ""], ["Liang", "Zhiwei", ""]]}, {"id": "1911.06704", "submitter": "Tirtharaj Dash", "authors": "Rohit Kaushik, Shikhar Jain, Siddhant Jain, Tirtharaj Dash", "title": "Performance evaluation of deep neural networks for forecasting\n  time-series with multiple structural breaks and high volatility", "comments": "Preprint (18 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of automatic and accurate forecasting of time-series data has\nalways been an interesting challenge for the machine learning and forecasting\ncommunity. A majority of the real-world time-series problems have\nnon-stationary characteristics that make the understanding of trend and\nseasonality difficult. Our interest in this paper is to study the applicability\nof the popular deep neural networks (DNN) as function approximators for\nnon-stationary TSF. We evaluate the following DNN models: Multi-layer\nPerceptron (MLP), Convolutional Neural Network (CNN), and RNN with Long-Short\nTerm Memory (LSTM-RNN) and RNN with Gated-Recurrent Unit (GRU-RNN). These DNN\nmethods have been evaluated over 10 popular Indian financial stocks data.\nFurther, the performance evaluation of these DNNs has been carried out in\nmultiple independent runs for two settings of forecasting: (1) single-step\nforecasting, and (2) multi-step forecasting. These DNN methods show convincing\nperformance for single-step forecasting (one-day ahead forecast). For the\nmulti-step forecasting (multiple days ahead forecast), we have evaluated the\nmethods for different forecast periods. The performance of these methods\ndemonstrates that long forecast periods have an adverse effect on performance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 16:44:23 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 13:26:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kaushik", "Rohit", ""], ["Jain", "Shikhar", ""], ["Jain", "Siddhant", ""], ["Dash", "Tirtharaj", ""]]}, {"id": "1911.06832", "submitter": "Kevin Sebastian Luck", "authors": "Kevin Sebastian Luck, Heni Ben Amor, Roberto Calandra", "title": "Data-efficient Co-Adaptation of Morphology and Behaviour with Deep\n  Reinforcement Learning", "comments": "Accepted for the Conference on Robot Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals are capable of quickly learning new behaviours to solve\nnew tasks. Yet, we often forget that they also rely on a highly specialized\nmorphology that co-adapted with motor control throughout thousands of years.\nAlthough compelling, the idea of co-adapting morphology and behaviours in\nrobots is often unfeasible because of the long manufacturing times, and the\nneed to re-design an appropriate controller for each morphology. In this paper,\nwe propose a novel approach to automatically and efficiently co-adapt a robot\nmorphology and its controller. Our approach is based on recent advances in deep\nreinforcement learning, and specifically the soft actor critic algorithm. Key\nto our approach is the possibility of leveraging previously tested morphologies\nand behaviors to estimate the performance of new candidate morphologies. As\nsuch, we can make full use of the information available for making more\ninformed decisions, with the ultimate goal of achieving a more data-efficient\nco-adaptation (i.e., reducing the number of morphologies and behaviors tested).\nSimulated experiments show that our approach requires drastically less design\nprototypes to find good morphology-behaviour combinations, making this method\nparticularly suitable for future co-adaptation of robot designs in the real\nworld.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 19:01:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Luck", "Kevin Sebastian", ""], ["Amor", "Heni Ben", ""], ["Calandra", "Roberto", ""]]}, {"id": "1911.06859", "submitter": "Minsoo Rhu", "authors": "Bongjoon Hyun, Youngeun Kwon, Yujeong Choi, John Kim, Minsoo Rhu", "title": "NeuMMU: Architectural Support for Efficient Address Translations in\n  Neural Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To satisfy the compute and memory demands of deep neural networks, neural\nprocessing units (NPUs) are widely being utilized for accelerating deep\nlearning algorithms. Similar to how GPUs have evolved from a slave device into\na mainstream processor architecture, it is likely that NPUs will become first\nclass citizens in this fast-evolving heterogeneous architecture space. This\npaper makes a case for enabling address translation in NPUs to decouple the\nvirtual and physical memory address space. Through a careful data-driven\napplication characterization study, we root-cause several limitations of prior\nGPU-centric address translation schemes and propose a memory management unit\n(MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our\nproposal incurs only an average 0.06% performance overhead.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:10:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hyun", "Bongjoon", ""], ["Kwon", "Youngeun", ""], ["Choi", "Yujeong", ""], ["Kim", "John", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1911.06918", "submitter": "Jung-Woo Chang", "authors": "Jung-Woo Chang, Saehyun Ahn, Keon-Woo Kang, Suk-Ju Kang", "title": "Towards Design Methodology of Efficient Fast Algorithms for Accelerating\n  Generative Adversarial Networks on FPGAs", "comments": "Proceedings of the 25th Asia and South Pacific Design Automation\n  Conference (ASP-DAC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown excellent performance in\nimage and speech applications. GANs create impressive data primarily through a\nnew type of operator called deconvolution (DeConv) or transposed convolution\n(Conv). To implement the DeConv layer in hardware, the state-of-the-art\naccelerator reduces the high computational complexity via the DeConv-to-Conv\nconversion and achieves the same results. However, there is a problem that the\nnumber of filters increases due to this conversion. Recently, Winograd minimal\nfiltering has been recognized as an effective solution to improve the\narithmetic complexity and resource efficiency of the Conv layer. In this paper,\nwe propose an efficient Winograd DeConv accelerator that combines these two\northogonal approaches on FPGAs. Firstly, we introduce a new class of fast\nalgorithm for DeConv layers using Winograd minimal filtering. Since there are\nregular sparse patterns in Winograd filters, we further amortize the\ncomputational complexity by skipping zero weights. Secondly, we propose a new\ndataflow to prevent resource underutilization by reorganizing the filter layout\nin the Winograd domain. Finally, we propose an efficient architecture for\nimplementing Winograd DeConv by designing the line buffer and exploring the\ndesign space. Experimental results on various GANs show that our accelerator\nachieves up to 1.78x~8.38x speedup over the state-of-the-art DeConv\naccelerators.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:49:38 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chang", "Jung-Woo", ""], ["Ahn", "Saehyun", ""], ["Kang", "Keon-Woo", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "1911.07004", "submitter": "Guo-Jun Qi", "authors": "Feng Lin, Haohang Xu, Houqiang Li, Hongkai Xiong, Guo-Jun Qi", "title": "AETv2: AutoEncoding Transformations for Self-Supervised Representation\n  Learning by Minimizing Geodesic Distances in Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning by predicting transformations has demonstrated\noutstanding performances in both unsupervised and (semi-)supervised tasks.\nAmong the state-of-the-art methods is the AutoEncoding Transformations (AET) by\ndecoding transformations from the learned representations of original and\ntransformed images. Both deterministic and probabilistic AETs rely on the\nEuclidean distance to measure the deviation of estimated transformations from\ntheir groundtruth counterparts. However, this assumption is questionable as a\ngroup of transformations often reside on a curved manifold rather staying in a\nflat Euclidean space. For this reason, we should use the geodesic to\ncharacterize how an image transform along the manifold of a transformation\ngroup, and adopt its length to measure the deviation between transformations.\nParticularly, we present to autoencode a Lie group of homography\ntransformations PG(2) to learn image representations. For this, we make an\nestimate of the intractable Riemannian logarithm by projecting PG(2) to a\nsubgroup of rotation transformations SO(3) that allows the closed-form\nexpression of geodesic distances. Experiments demonstrate the proposed AETv2\nmodel outperforms the previous version as well as the other state-of-the-art\nself-supervised models in multiple tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 09:58:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Lin", "Feng", ""], ["Xu", "Haohang", ""], ["Li", "Houqiang", ""], ["Xiong", "Hongkai", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1911.07112", "submitter": "Alison Jenkins", "authors": "Alison Jenkins, Vinika Gupta, Alexis Myrick, Mary Lenoir", "title": "Particle Swarm and EDAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Particle Swarm Optimization (PSO) algorithm is developed for solving the\nSchaffer F6 function in fewer than 4000 function evaluations on a total of 30\nruns. Four variations of the Full Model of Particle Swarm Optimization (PSO)\nalgorithms are presented which consist of combinations of Ring and Star\ntopologies with Synchronous and Asynchronous updates. The Full Model with\ncombinations of Ring and Star topologies in combination with Synchronous and\nAsynchronous Particle Updates is explored.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 23:08:13 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jenkins", "Alison", ""], ["Gupta", "Vinika", ""], ["Myrick", "Alexis", ""], ["Lenoir", "Mary", ""]]}, {"id": "1911.07115", "submitter": "Alison Jenkins", "authors": "Alison Jenkins, Vinika Gupta, Mary Lenoir", "title": "General Regression Neural Networks, Radial Basis Function Neural\n  Networks, Support Vector Machines, and Feedforward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.NE cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this project is to develop a code to discover the optimal sigma\nvalue that maximum the F1 score and the optimal sigma value that maximizes the\naccuracy and to find out if they are the same. Four algorithms which can be\nused to solve this problem are: Genetic Regression Neural Networks (GRNNs),\nRadial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines\n(SVMs) and Feedforward Neural Network (FFNNs).\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 23:31:26 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jenkins", "Alison", ""], ["Gupta", "Vinika", ""], ["Lenoir", "Mary", ""]]}, {"id": "1911.07228", "submitter": "Kiet Nguyen Van", "authors": "Binh An Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen", "title": "Error Analysis for Vietnamese Named Entity Recognition on Deep Neural\n  Network Models", "comments": "19th International Conference on Computational Linguistics and\n  Intelligent Text Processing (CICLING 2018)", "journal-ref": "19th International Conference on Computational Linguistics and\n  Intelligent Text Processing (CICLING 2018)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Vietnamese Named Entity Recognition (NER) systems have had a\ngreat breakthrough when using Deep Neural Network methods. This paper describes\nthe primary errors of the state-of-the-art NER systems on Vietnamese language.\nAfter conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with\ndifferent word embeddings on the Vietnamese NER dataset. This dataset is\nprovided by VLSP in 2016 and used to evaluate most of the current Vietnamese\nNER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we\nanalyze the errors on this model in detail. Our error-analysis results provide\nus thorough insights in order to increase the performance of NER for the\nVietnamese language and improve the quality of the corpus in the future works.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 13:03:07 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 13:08:38 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nguyen", "Binh An", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "1911.07247", "submitter": "Jonathan Baxter", "authors": "Peter L. Bartlett and Jonathan Baxter", "title": "Hebbian Synaptic Modifications in Spiking Neurons that Learn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a new model of synaptic plasticity, based on recent\nalgorithms for reinforcement learning (in which an agent attempts to learn\nappropriate actions to maximize its long-term average reward). We show that\nthese direct reinforcement learning algorithms also give locally optimal\nperformance for the problem of reinforcement learning with multiple agents,\nwithout any explicit communication between agents. By considering a network of\nspiking neurons as a collection of agents attempting to maximize the long-term\naverage of a reward signal, we derive a synaptic update rule that is\nqualitatively similar to Hebb's postulate. This rule requires only simple\ncomputations, such as addition and leaky integration, and involves only\nquantities that are available in the vicinity of the synapse. Furthermore, it\nleads to synaptic connection strengths that give locally optimal values of the\nlong term average reward. The reinforcement learning paradigm is sufficiently\nbroad to encompass many learning problems that are solved by the brain. We\nillustrate, with simulations, that the approach is effective for simple pattern\nclassification and motor learning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 14:36:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Baxter", "Jonathan", ""]]}, {"id": "1911.07302", "submitter": "Michail-Antisthenis Tsompanas", "authors": "Michail-Antisthenis Tsompanas, Larry Bull, Andrew Adamatzky, Igor\n  Balaz", "title": "Haploid-Diploid Evolution: Nature's Memetic Algorithm", "comments": "arXiv admin note: text overlap with arXiv:1903.11598", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a recent explanation for the fundamental haploid-diploid\nlifecycle of eukaryotic organisms to present a new memetic algorithm that\ndiffers from all previous known work using diploid representations. A form of\nthe Baldwin effect has been identified as inherent to the evolutionary\nmechanisms of eukaryotes and a simplified version is presented here which\nmaintains such behaviour. Using a well-known abstract tuneable model, it is\nshown that varying fitness landscape ruggedness varies the benefit of\nhaploid-diploid algorithms. Moreover, the methodology is applied to optimise\nthe targeted delivery of a therapeutic compound utilizing nano-particles to\ncancerous tumour cells with the multicellular simulator PhysiCell.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 11:39:05 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Tsompanas", "Michail-Antisthenis", ""], ["Bull", "Larry", ""], ["Adamatzky", "Andrew", ""], ["Balaz", "Igor", ""]]}, {"id": "1911.07446", "submitter": "Cong Hao", "authors": "Cong Hao, Yao Chen, Xinheng Liu, Atif Sarwari, Daryl Sew, Ashutosh\n  Dhar, Bryan Wu, Dongdong Fu, Jinjun Xiong, Wen-mei Hwu, Junli Gu, Deming Chen", "title": "NAIS: Neural Architecture and Implementation Search and its Applications\n  in Autonomous Driving", "comments": "8 pages, ICCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing demands for powerful AI algorithms in many application\ndomains have motivated massive investment in both high-quality deep neural\nnetwork (DNN) models and high-efficiency implementations. In this position\npaper, we argue that a simultaneous DNN/implementation co-design methodology,\nnamed Neural Architecture and Implementation Search (NAIS), deserves more\nresearch attention to boost the development productivity and efficiency of both\nDNN models and implementation optimization. We propose a stylized design\nmethodology that can drastically cut down the search cost while preserving the\nquality of the end solution.As an illustration, we discuss this\nDNN/implementation methodology in the context of both FPGAs and GPUs. We take\nautonomous driving as a key use case as it is one of the most demanding areas\nfor high quality AI algorithms and accelerators. We discuss how such a\nco-design methodology can impact the autonomous driving industry significantly.\nWe identify several research opportunities in this exciting domain.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:17:14 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hao", "Cong", ""], ["Chen", "Yao", ""], ["Liu", "Xinheng", ""], ["Sarwari", "Atif", ""], ["Sew", "Daryl", ""], ["Dhar", "Ashutosh", ""], ["Wu", "Bryan", ""], ["Fu", "Dongdong", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Gu", "Junli", ""], ["Chen", "Deming", ""]]}, {"id": "1911.07509", "submitter": "Anis Koubaa", "authors": "Marwa Ben Jabra, Adel Ammar, Anis Koubaa, Omar Cheikhrouhou, Habib\n  Hamam", "title": "AI-based Pilgrim Detection using Convolutional Neural Networks", "comments": "Accepted in ATSIP'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pilgrimage represents the most important Islamic religious gathering in the\nworld where millions of pilgrims visit the holy places of Makkah and Madinah to\nperform their rituals. The safety and security of pilgrims is the highest\npriority for the authorities. In Makkah, 5000 cameras are spread around the\nholy for monitoring pilgrims, but it is almost impossible to track all events\nby humans considering the huge number of images collected every second. To\naddress this issue, we propose to use artificial intelligence technique based\non deep learning and convolution neural networks to detect and identify\nPilgrims and their features. For this purpose, we built a comprehensive dataset\nfor the detection of pilgrims and their genders. Then, we develop two\nconvolutional neural networks based on YOLOv3 and Faster-RCNN for the detection\nof Pilgrims. Experiments results show that Faster RCNN with Inception v2\nfeature extractor provides the best mean average precision over all classes of\n51%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 09:46:54 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 19:06:10 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Jabra", "Marwa Ben", ""], ["Ammar", "Adel", ""], ["Koubaa", "Anis", ""], ["Cheikhrouhou", "Omar", ""], ["Hamam", "Habib", ""]]}, {"id": "1911.07662", "submitter": "Haiping Huang", "authors": "Haiping Huang", "title": "Variational mean-field theory for training restricted Boltzmann machines\n  with binary synapses", "comments": "9 pages, 2 figures, a mean-field framework proposed for unsupervised\n  learning in RBM with discrete synapses, which was previously out of reach", "journal-ref": "Phys. Rev. E 102, 030301 (2020)", "doi": "10.1103/PhysRevE.102.030301", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning requiring only raw data is not only a fundamental\nfunction of the cerebral cortex, but also a foundation for a next generation of\nartificial neural networks. However, a unified theoretical framework to treat\nsensory inputs, synapses and neural activity together is still lacking. The\ncomputational obstacle originates from the discrete nature of synapses, and\ncomplex interactions among these three essential elements of learning. Here, we\npropose a variational mean-field theory in which the distribution of synaptic\nweights is considered. The unsupervised learning can then be decomposed into\ntwo intertwined steps: a maximization step is carried out as a gradient ascent\nof the lower-bound on the data log-likelihood, in which the synaptic weight\ndistribution is determined by updating variational parameters, and an\nexpectation step is carried out as a message passing procedure on an equivalent\nor dual neural network whose parameter is specified by the variational\nparameters of the weight distribution. Therefore, our framework provides\ninsights on how data (or sensory inputs), synapses and neural activities\ninteract with each other to achieve the goal of extracting statistical\nregularities in sensory inputs. This variational framework is verified in\nrestricted Boltzmann machines with planted synaptic weights and\nhandwritten-digits learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 02:12:08 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 03:58:24 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Huang", "Haiping", ""]]}, {"id": "1911.07698", "submitter": "Maurizio Ferrari Dacrema", "authors": "Maurizio Ferrari Dacrema and Simone Boglio and Paolo Cremonesi and\n  Dietmar Jannach", "title": "A Troubling Analysis of Reproducibility and Progress in Recommender\n  Systems Research", "comments": "Source code and full results available at:\n  https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation", "journal-ref": "ACM Transactions on Information Systems 39, 2, Article 20 (January\n  2021), 49 pages", "doi": "10.1145/3434185", "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of algorithms that generate personalized ranked item lists is a\ncentral topic of research in the field of recommender systems. In the past few\nyears, in particular, approaches based on deep learning (neural) techniques\nhave become dominant in the literature. For all of them, substantial progress\nover the state-of-the-art is claimed. However, indications exist of certain\nproblems in today's research practice, e.g., with respect to the choice and\noptimization of the baselines used for comparison, raising questions about the\npublished claims. In order to obtain a better understanding of the actual\nprogress, we have tried to reproduce recent results in the area of neural\nrecommendation approaches based on collaborative filtering. The worrying\noutcome of the analysis of these recent works-all were published at prestigious\nscientific conferences between 2015 and 2018-is that 11 out of the 12\nreproducible neural approaches can be outperformed by conceptually simple\nmethods, e.g., based on the nearest-neighbor heuristics. None of the\ncomputationally complex neural methods was actually consistently better than\nalready existing learning-based techniques, e.g., using matrix factorization or\nlinear models. In our analysis, we discuss common issues in today's research\npractice, which, despite the many papers that are published on the topic, have\napparently led the field to a certain level of stagnation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:27:09 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 10:18:34 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 14:09:01 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Dacrema", "Maurizio Ferrari", ""], ["Boglio", "Simone", ""], ["Cremonesi", "Paolo", ""], ["Jannach", "Dietmar", ""]]}, {"id": "1911.07729", "submitter": "Luc Frachon", "authors": "Luc Frachon, Wei Pang, George M. Coghill", "title": "ImmuNeCS: Neural Committee Search by an Artificial Immune System", "comments": "16 pages including references, 6 figures, 3 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current Neural Architecture Search techniques can suffer from a few\nshortcomings, including high computational cost, excessive bias from the search\nspace, conceptual complexity or uncertain empirical benefits over random\nsearch. In this paper, we present ImmuNeCS, an attempt at addressing these\nissues with a method that offers a simple, flexible, and efficient way of\nbuilding deep learning models automatically, and we demonstrate its\neffectiveness in the context of convolutional neural networks. Instead of\nsearching for the 1-best architecture for a given task, we focus on building a\npopulation of neural networks that are then ensembled into a neural network\ncommittee, an approach we dub 'Neural Committee Search'. To ensure sufficient\nperformance from the committee, our search algorithm is based on an artificial\nimmune system that balances individual performance with population diversity.\nThis allows us to stop the search when accuracy starts to plateau, and to\nbridge the performance gap through ensembling. In order to justify our method,\nwe first verify that the chosen search space exhibits the locality property. To\nfurther improve efficiency, we also combine partial evaluation, weight\ninheritance, and progressive search. First, experiments are run to verify the\nvalidity of these techniques. Then, preliminary experimental results on two\npopular computer vision benchmarks show that our method consistently\noutperforms random search and yields promising results within reasonable GPU\nbudgets. An additional experiment also shows that ImmuNeCS's solutions transfer\neffectively to a more difficult task, where they achieve results comparable to\na direct search on the new task. We believe these findings can open the way for\nnew, accessible alternatives to traditional NAS.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:00:28 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 15:39:03 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 11:06:38 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 20:23:51 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Frachon", "Luc", ""], ["Pang", "Wei", ""], ["Coghill", "George M.", ""]]}, {"id": "1911.07805", "submitter": "Shokooh Taghian", "authors": "Shokooh Taghian, Mohammad H. Nadimi-Shahraki", "title": "Binary Sine Cosine Algorithms for Feature Selection from Medical Data", "comments": null, "journal-ref": null, "doi": "10.5121/acij.2019.10501", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-constructed classification model highly depends on input feature\nsubsets from a dataset, which may contain redundant, irrelevant, or noisy\nfeatures. This challenge can be worse while dealing with medical datasets. The\nmain aim of feature selection as a pre-processing task is to eliminate these\nfeatures and select the most effective ones. In the literature, metaheuristic\nalgorithms show a successful performance to find optimal feature subsets. In\nthis paper, two binary metaheuristic algorithms named S-shaped binary Sine\nCosine Algorithm (SBSCA) and V-shaped binary Sine Cosine Algorithm (VBSCA) are\nproposed for feature selection from the medical data. In these algorithms, the\nsearch space remains continuous, while a binary position vector is generated by\ntwo transfer functions S-shaped and V-shaped for each solution. The proposed\nalgorithms are compared with four latest binary optimization algorithms over\nfive medical datasets from the UCI repository. The experimental results confirm\nthat using both bSCA variants enhance the accuracy of classification on these\nmedical datasets compared to four other algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 12:57:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Taghian", "Shokooh", ""], ["Nadimi-Shahraki", "Mohammad H.", ""]]}, {"id": "1911.07925", "submitter": "Tianfu Li", "authors": "Tianfu Li, Zhibin Zhao, Chuang Sun, Li Cheng, Xuefeng Chen, Ruqiang\n  Yan, Robert X. Gao", "title": "WaveletKernelNet: An Interpretable Deep Neural Network for Industrial\n  Intelligent Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN), with ability of feature learning and\nnonlinear mapping, has demonstrated its effectiveness in prognostics and health\nmanagement (PHM). However, explanation on the physical meaning of a CNN\narchitecture has rarely been studied. In this paper, a novel wavelet driven\ndeep neural network termed as WaveletKernelNet (WKN) is presented, where a\ncontinuous wavelet convolutional (CWConv) layer is designed to replace the\nfirst convolutional layer of the standard CNN. This enables the first CWConv\nlayer to discover more meaningful filters. Furthermore, only the scale\nparameter and translation parameter are directly learned from raw data at this\nCWConv layer. This provides a very effective way to obtain a customized filter\nbank, specifically tuned for extracting defect-related impact component\nembedded in the vibration signal. In addition, three experimental verification\nusing data from laboratory environment are carried out to verify effectiveness\nof the proposed method for mechanical fault diagnosis. The results show the\nimportance of the designed CWConv layer and the output of CWConv layer is\ninterpretable. Besides, it is found that WKN has fewer parameters, higher fault\nclassification accuracy and faster convergence speed than standard CNN.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 07:22:56 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 12:47:17 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 04:37:47 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Tianfu", ""], ["Zhao", "Zhibin", ""], ["Sun", "Chuang", ""], ["Cheng", "Li", ""], ["Chen", "Xuefeng", ""], ["Yan", "Ruqiang", ""], ["Gao", "Robert X.", ""]]}, {"id": "1911.08020", "submitter": "Ao Ren", "authors": "Ao Ren, Tao Zhang, Yuhao Wang, Sheng Lin, Peiyan Dong, Yen-kuang Chen,\n  Yuan Xie, Yanzhi Wang", "title": "DARB: A Density-Aware Regular-Block Pruning for Deep Neural Networks", "comments": "This paper has been accepted by AAAI'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing parameter volume of deep neural networks (DNNs) hinders\nthe artificial intelligence applications on resource constrained devices, such\nas mobile and wearable devices. Neural network pruning, as one of the\nmainstream model compression techniques, is under extensive study to reduce the\nnumber of parameters and computations. In contrast to irregular pruning that\nincurs high index storage and decoding overhead, structured pruning techniques\nhave been proposed as the promising solutions. However, prior studies on\nstructured pruning tackle the problem mainly from the perspective of\nfacilitating hardware implementation, without analyzing the characteristics of\nsparse neural networks. The neglect on the study of sparse neural networks\ncauses inefficient trade-off between regularity and pruning ratio.\nConsequently, the potential of structurally pruning neural networks is not\nsufficiently mined.\n  In this work, we examine the structural characteristics of the irregularly\npruned weight matrices, such as the diverse redundancy of different rows, the\nsensitivity of different rows to pruning, and the positional characteristics of\nretained weights. By leveraging the gained insights as a guidance, we first\npropose the novel block-max weight masking (BMWM) method, which can effectively\nretain the salient weights while imposing high regularity to the weight matrix.\nAs a further optimization, we propose a density-adaptive regular-block (DARB)\npruning that outperforms prior structured pruning work with high pruning ratio\nand decoding efficiency. Our experimental results show that DARB can achieve\n13$\\times$ to 25$\\times$ pruning ratio, which are 2.8$\\times$ to 4.3$\\times$\nimprovements than the state-of-the-art counterparts on multiple neural network\nmodels and tasks. Moreover, DARB can achieve 14.3$\\times$ decoding efficiency\nthan block pruning with higher pruning ratio.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 00:46:05 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 05:33:11 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ren", "Ao", ""], ["Zhang", "Tao", ""], ["Wang", "Yuhao", ""], ["Lin", "Sheng", ""], ["Dong", "Peiyan", ""], ["Chen", "Yen-kuang", ""], ["Xie", "Yuan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1911.08030", "submitter": "Abenezer Girma Mr", "authors": "Abenezer Girma, Xuyang Yan, Abdollah Homaifar", "title": "Driver Identification Based on Vehicle Telematics Data using\n  LSTM-Recurrent Neural Network", "comments": "IEEE ICTAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advancements in vehicle security systems, over the last decade,\nauto-theft rates have increased, and cyber-security attacks on\ninternet-connected and autonomous vehicles are becoming a new threat. In this\npaper, a deep learning model is proposed, which can identify drivers from their\ndriving behaviors based on vehicle telematics data. The proposed\nLong-Short-Term-Memory (LSTM) model predicts the identity of the driver based\non the individual's unique driving patterns learned from the vehicle telematics\ndata. Given the telematics is time-series data, the problem is formulated as a\ntime series prediction task to exploit the embedded sequential information. The\nperformance of the proposed approach is evaluated on three naturalistic driving\ndatasets, which gives high accuracy prediction results. The robustness of the\nmodel on noisy and anomalous data that is usually caused by sensor defects or\nenvironmental factors is also investigated. Results show that the proposed\nmodel prediction accuracy remains satisfactory and outperforms the other\napproaches despite the extent of anomalies and noise-induced in the data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:15:20 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Girma", "Abenezer", ""], ["Yan", "Xuyang", ""], ["Homaifar", "Abdollah", ""]]}, {"id": "1911.08074", "submitter": "Liang Li", "authors": "Yu-Xuan Li, Jin-Yuan Liu, Liang Li and Xiang Guan", "title": "Thick-Net: Parallel Network Structure for Sequential Modeling", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have been widely used in sequence learning tasks.\nIn previous studies, the performance of the model has always been improved by\neither wider or deeper structures. However, the former becomes more prone to\noverfitting, while the latter is difficult to optimize. In this paper, we\npropose a simple new model named Thick-Net, by expanding the network from\nanother dimension: thickness. Multiple parallel values are obtained via more\nsets of parameters in each hidden state, and the maximum value is selected as\nthe final output among parallel intermediate outputs. Notably, Thick-Net can\nefficiently avoid overfitting, and is easier to optimize than the vanilla\nstructures due to the large dropout affiliated with it. Our model is evaluated\non four sequential tasks including adding problem, permuted sequential MNIST,\ntext classification and language modeling. The results of these tasks\ndemonstrate that our model can not only improve accuracy with faster\nconvergence but also facilitate a better generalization ability.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 03:18:30 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Li", "Yu-Xuan", ""], ["Liu", "Jin-Yuan", ""], ["Li", "Liang", ""], ["Guan", "Xiang", ""]]}, {"id": "1911.08261", "submitter": "Qianhui Liu", "authors": "Qianhui Liu, Gang Pan, Haibo Ruan, Dong Xing, Qi Xu, and Huajin Tang", "title": "Unsupervised AER Object Recognition Based on Multiscale Spatio-Temporal\n  Features and Spiking Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an unsupervised address event representation (AER) object\nrecognition approach. The proposed approach consists of a novel multiscale\nspatio-temporal feature (MuST) representation of input AER events and a spiking\nneural network (SNN) using spike-timing-dependent plasticity (STDP) for object\nrecognition with MuST. MuST extracts the features contained in both the spatial\nand temporal information of AER event flow, and meanwhile forms an informative\nand compact feature spike representation. We show not only how MuST exploits\nspikes to convey information more effectively, but also how it benefits the\nrecognition using SNN. The recognition process is performed in an unsupervised\nmanner, which does not need to specify the desired status of every single\nneuron of SNN, and thus can be flexibly applied in real-world recognition\ntasks. The experiments are performed on five AER datasets including a new one\nnamed GESTURE-DVS. Extensive experimental results show the effectiveness and\nadvantages of this proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 13:47:47 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Liu", "Qianhui", ""], ["Pan", "Gang", ""], ["Ruan", "Haibo", ""], ["Xing", "Dong", ""], ["Xu", "Qi", ""], ["Tang", "Huajin", ""]]}, {"id": "1911.08264", "submitter": "Elina Thibeau-Sutre", "authors": "Elina Thibeau Sutre (ARAMIS), Olivier Colliot (ARAMIS), Didier Dormont\n  (ARAMIS), Ninon Burgos (ARAMIS)", "title": "Visualization approach to assess the robustness of neural networks for\n  medical image classification", "comments": null, "journal-ref": "SPIE Medical Imaging 2020, Feb 2020, Houston, United States", "doi": null, "report-no": null, "categories": "eess.IV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of neural networks for diagnosis classification is becoming more and\nmore prevalent in the medical imaging community. However, deep learning method\noutputs remain hard to explain. Another difficulty is to choose among the large\nnumber of techniques developed to analyze how networks learn, as all present\ndifferent limitations. In this paper, we extended the framework of Fong and\nVedaldi [IEEE International Conference on Computer Vision (ICCV), 2017] to\nvisualize the training of convolutional neural networks (CNNs) on 3D\nquantitative neuroimaging data. Our application focuses on the detection of\nAlzheimer's disease with gray matter probability maps extracted from structural\nMRI. We first assessed the robustness of the visualization method by studying\nthe coherence of the longitudinal patterns and regions identified by the\nnetwork. We then studied the stability of the CNN training by computing\nvisualization-based similarity indexes between different re-runs of the CNN. We\ndemonstrated that the areas identified by the CNN were consistent with what is\nknown of Alzheimer's disease and that the visualization approach extract\ncoherent longitudinal patterns. We also showed that the CNN training is not\nstable and that the areas identified mainly depend on the initialization and\nthe training process. This issue may exist in many other medical studies using\ndeep learning methods on datasets in which the number of samples is too small\nand the data dimension is high. This means that it may not be possible to rely\non deep learning to detect stable regions of interest in this field yet.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 13:57:57 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 15:39:17 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 16:16:34 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Sutre", "Elina Thibeau", "", "ARAMIS"], ["Colliot", "Olivier", "", "ARAMIS"], ["Dormont", "Didier", "", "ARAMIS"], ["Burgos", "Ninon", "", "ARAMIS"]]}, {"id": "1911.08373", "submitter": "Jibin Wu", "authors": "Jibin Wu, Emre Yilmaz, Malu Zhang, Haizhou Li and Kay Chen Tan", "title": "Deep Spiking Neural Networks for Large Vocabulary Automatic Speech\n  Recognition", "comments": "Submitted to Frontier of Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANN) have become the mainstream acoustic modeling\ntechnique for large vocabulary automatic speech recognition (ASR). A\nconventional ANN features a multi-layer architecture that requires massive\namounts of computation. The brain-inspired spiking neural networks (SNN)\nclosely mimic the biological neural networks and can operate on low-power\nneuromorphic hardware with spike-based computation. Motivated by their\nunprecedented energyefficiency and rapid information processing capability, we\nexplore the use of SNNs for speech recognition. In this work, we use SNNs for\nacoustic modeling and evaluate their performance on several large vocabulary\nrecognition scenarios. The experimental results demonstrate competitive ASR\naccuracies to their ANN counterparts, while require significantly reduced\ncomputational cost and inference time. Integrating the algorithmic power of\ndeep SNNs with energy-efficient neuromorphic hardware, therefore, offer an\nattractive solution for ASR applications running locally on mobile and embedded\ndevices.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:09:02 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wu", "Jibin", ""], ["Yilmaz", "Emre", ""], ["Zhang", "Malu", ""], ["Li", "Haizhou", ""], ["Tan", "Kay Chen", ""]]}, {"id": "1911.08518", "submitter": "Shamma Nasrin", "authors": "Shamma Nasrin, Srikanth Ramakrishna, Theja Tulabandhula, Amit Ranjan\n  Trivedi", "title": "Supported-BinaryNet: Bitcell Array-based Weight Supports for Dynamic\n  Accuracy-Latency Trade-offs in SRAM-based Binarized Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce bitcell array-based support parameters to improve\nthe prediction accuracy of SRAM-based binarized neural network (SRAM-BNN). Our\napproach enhances the training weight space of SRAM-BNN while requiring minimal\noverheads to a typical design. More flexibility of the weight space leads to\nhigher prediction accuracy in our design. We adapt row digital-to-analog (DAC)\nconverter, and computing flow in SRAM-BNN for bitcell array-based weight\nsupports. Using the discussed interventions, our scheme also allows a dynamic\ntrade-off of accuracy against latency to address dynamic latency constraints in\ntypical real-time applications. We specifically discuss results on two training\ncases: (i) learning of support parameters on a pre-trained BNN and (ii)\nsimultaneous learning of supports and weight binarization. In the former case,\nour approach reduces classification error in MNIST by 35.71% (error rate\ndecreases from 1.4% to 0.91%). In the latter case, the error is reduced by\n27.65% (error rate decreases from 1.4% to 1.13%). To reduce the power\noverheads, we propose a dynamic drop out a part of the support parameters. Our\narchitecture can drop out 52% of the bitcell array-based support parameters\nwithout losing accuracy. We also characterize our design under varying degrees\nof process variability in the transistors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:27:06 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 17:34:40 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nasrin", "Shamma", ""], ["Ramakrishna", "Srikanth", ""], ["Tulabandhula", "Theja", ""], ["Trivedi", "Amit Ranjan", ""]]}, {"id": "1911.08555", "submitter": "Abhronil Sengupta", "authors": "Akul Malhotra, Sen Lu, Kezhou Yang, Abhronil Sengupta", "title": "Exploiting Oxide Based Resistive RAM Variability for Bayesian Neural\n  Network Hardware Design", "comments": null, "journal-ref": null, "doi": "10.1109/TNANO.2020.2982819", "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty plays a key role in real-time machine learning. As a significant\nshift from standard deep networks, which does not consider any uncertainty\nformulation during its training or inference, Bayesian deep networks are being\ncurrently investigated where the network is envisaged as an ensemble of\nplausible models learnt by the Bayes' formulation in response to uncertainties\nin sensory data. Bayesian deep networks consider each synaptic weight as a\nsample drawn from a probability distribution with learnt mean and variance.\nThis paper elaborates on a hardware design that exploits cycle-to-cycle\nvariability of oxide based Resistive Random Access Memories (RRAMs) as a means\nto realize such a probabilistic sampling function, instead of viewing it as a\ndisadvantage.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 04:06:08 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 17:55:20 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 06:43:43 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 00:02:48 GMT"}, {"version": "v5", "created": "Fri, 20 Mar 2020 16:33:18 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Malhotra", "Akul", ""], ["Lu", "Sen", ""], ["Yang", "Kezhou", ""], ["Sengupta", "Abhronil", ""]]}, {"id": "1911.08584", "submitter": "Eilif Muller", "authors": "Eilif B. Muller, Philippe Beaudoin", "title": "Neocortical plasticity: an unsupervised cake but no free lunch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fields of artificial intelligence and neuroscience have a long history of\nfertile bi-directional interactions. On the one hand, important inspiration for\nthe development of artificial intelligence systems has come from the study of\nnatural systems of intelligence, the mammalian neocortex in particular. On the\nother, important inspiration for models and theories of the brain have emerged\nfrom artificial intelligence research. A central question at the intersection\nof these two areas is concerned with the processes by which neocortex learns,\nand the extent to which they are analogous to the back-propagation training\nalgorithm of deep networks. Matching the data efficiency, transfer and\ngeneralization properties of neocortical learning remains an area of active\nresearch in the field of deep learning. Recent advances in our understanding of\nneuronal, synaptic and dendritic physiology of the neocortex suggest new\napproaches for unsupervised representation learning, perhaps through a new\nclass of objective functions, which could act alongside or in lieu of\nback-propagation. Such local learning rules have implicit rather than explicit\nobjectives with respect to the training data, facilitating domain adaptation\nand generalization. Incorporating them into deep networks for representation\nlearning could better leverage unlabelled datasets to offer significant\nimprovements in data efficiency of downstream supervised readout learning, and\nreduce susceptibility to adversarial perturbations, at the cost of a more\nrestricted domain of applicability.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:32:42 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Muller", "Eilif B.", ""], ["Beaudoin", "Philippe", ""]]}, {"id": "1911.08589", "submitter": "Jason Platt", "authors": "Jason A. Platt and Anna Miller and Lawson Fuller and Henry D. I.\n  Abarbanel", "title": "Machine Learning Classification Informed by a Functional Biophysical\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel machine learning architecture for classification suggested\nby experiments on olfactory systems. The network separates input stimuli,\nrepresented as spatially distinct currents, via winnerless competition---a\nprocess based on the intrinsic sequential dynamics of the neural system---then\nuses a support vector machine (SVM) to provide precision to the space-time\nseparation of the output. The combined network uses biophysical models of\nneurons and shows high discrimination among inputs and robustness to noise.\nWhile using the SVM alone does not permit determination of the components of\nmixtures of classified inputs, the combined network is able to tell the precise\nconcentrations of the constituent parts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:09:11 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 20:23:51 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Platt", "Jason A.", ""], ["Miller", "Anna", ""], ["Fuller", "Lawson", ""], ["Abarbanel", "Henry D. I.", ""]]}, {"id": "1911.08600", "submitter": "Artem Kaznatcheev", "authors": "David A. Cohen, Martin C. Cooper, Artem Kaznatcheev, Mark Wallace", "title": "Steepest ascent can be exponential in bounded treewidth problems", "comments": "8 pages main text, 4 pages appendix, 1 page references; fixed error\n  in f(a,b) to match code", "journal-ref": "Operations Research Letters 48 (2020) 217-224", "doi": "10.1016/j.orl.2020.02.010", "report-no": null, "categories": "cs.DM cs.DS cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of local search based on steepest ascent. We\nshow that even when all variables have domains of size two and the underlying\nconstraint graph of variable interactions has bounded treewidth (in our\nconstruction, treewidth 7), there are fitness landscapes for which an\nexponential number of steps may be required to reach a local optimum. This is\nan improvement on prior recursive constructions of long steepest ascents, which\nwe prove to need constraint graphs of unbounded treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:42:08 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 14:42:33 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Cohen", "David A.", ""], ["Cooper", "Martin C.", ""], ["Kaznatcheev", "Artem", ""], ["Wallace", "Mark", ""]]}, {"id": "1911.08650", "submitter": "Jordan MacLachlan", "authors": "Jordan MacLachlan, Yi Mei, Juergen Branke, Mengjie Zhang", "title": "Genetic Programming Hyper-Heuristics with Vehicle Collaboration for\n  Uncertain Capacitated Arc Routing Problems", "comments": null, "journal-ref": null, "doi": "10.1162/evco_a_00267", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its direct relevance to post-disaster operations, meter reading and\ncivil refuse collection, the Uncertain Capacitated Arc Routing Problem (UCARP)\nis an important optimisation problem. Stochastic models are critical to study\nas they more accurately represent the real-world than their deterministic\ncounterparts. Although there have been extensive studies in solving routing\nproblems under uncertainty, very few have considered UCARP, and none consider\ncollaboration between vehicles to handle the negative effects of uncertainty.\nThis paper proposes a novel Solution Construction Procedure (SCP) that\ngenerates solutions to UCARP within a collaborative, multi-vehicle framework.\nIt consists of two types of collaborative activities: one when a vehicle\nunexpectedly expends capacity (\\emph{route failure}), and the other during the\nrefill process. Then, we propose a Genetic Programming Hyper-Heuristic (GPHH)\nalgorithm to evolve the routing policy used within the collaborative framework.\nThe experimental studies show that the new heuristic with vehicle collaboration\nand GP-evolved routing policy significantly outperforms the compared\nstate-of-the-art algorithms on commonly studied test problems. This is shown to\nbe especially true on instances with larger numbers of tasks and vehicles. This\nclearly shows the advantage of vehicle collaboration in handling the uncertain\nenvironment, and the effectiveness of the newly proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 00:55:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["MacLachlan", "Jordan", ""], ["Mei", "Yi", ""], ["Branke", "Juergen", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1911.09230", "submitter": "Atsushi Masumori", "authors": "Atsushi Masumori, Lana Sinapayen, Takashi Ikegami", "title": "Predictive Coding as Stimulus Avoidance in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding can be regarded as a function which reduces the error\nbetween an input signal and a top-down prediction. If reducing the error is\nequivalent to reducing the influence of stimuli from the environment,\npredictive coding can be regarded as stimulation avoidance by prediction. Our\nprevious studies showed that action and selection for stimulation avoidance\nemerge in spiking neural networks through spike-timing dependent plasticity\n(STDP). In this study, we demonstrate that spiking neural networks with random\nstructure spontaneously learn to predict temporal sequences of stimuli based\nsolely on STDP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 00:54:55 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Masumori", "Atsushi", ""], ["Sinapayen", "Lana", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1911.09257", "submitter": "Alexander Wong", "authors": "Andrew Hryniowski and Alexander Wong", "title": "DeepLABNet: End-to-end Learning of Deep Radial Basis Networks with Fully\n  Learnable Basis Functions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From fully connected neural networks to convolutional neural networks, the\nlearned parameters within a neural network have been primarily relegated to the\nlinear parameters (e.g., convolutional filters). The non-linear functions\n(e.g., activation functions) have largely remained, with few exceptions in\nrecent years, parameter-less, static throughout training, and seen limited\nvariation in design. Largely ignored by the deep learning community, radial\nbasis function (RBF) networks provide an interesting mechanism for learning\nmore complex non-linear activation functions in addition to the linear\nparameters in a network. However, the interest in RBF networks has waned over\ntime due to the difficulty of integrating RBFs into more complex deep neural\nnetwork architectures in a tractable and stable manner. In this work, we\npresent a novel approach that enables end-to-end learning of deep RBF networks\nwith fully learnable activation basis functions in an automatic and tractable\nmanner. We demonstrate that our approach for enabling the use of learnable\nactivation basis functions in deep neural networks, which we will refer to as\nDeepLABNet, is an effective tool for automated activation function learning\nwithin complex network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:06:15 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Hryniowski", "Andrew", ""], ["Wong", "Alexander", ""]]}, {"id": "1911.09309", "submitter": "Zhijie Chen", "authors": "Zhijie Chen, Junchi Yan, Longyuan Li and Xiaokang Yang", "title": "Decoding Spiking Mechanism with Dynamic Learning on Neuron Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main concern in cognitive neuroscience is to decode the overt neural spike\ntrain observations and infer latent representations under neural circuits.\nHowever, traditional methods entail strong prior on network structure and\nhardly meet the demand for real spike data. Here we propose a novel neural\nnetwork approach called Neuron Activation Network that extracts neural\ninformation explicitly from single trial neuron population spike trains. Our\nproposed method consists of a spatiotemporal learning procedure on sensory\nenvironment and a message passing mechanism on population graph, followed by a\nneuron activation process in a recursive fashion. Our model is aimed to\nreconstruct neuron information while inferring representations of neuron\nspiking states. We apply our model to retinal ganglion cells and the\nexperimental results suggest that our model holds a more potent capability in\ngenerating neural spike sequences with high fidelity than the state-of-the-art\nmethods, as well as being more expressive and having potential to disclose\nlatent spiking mechanism. The source code will be released with the final\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 06:56:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chen", "Zhijie", ""], ["Yan", "Junchi", ""], ["Li", "Longyuan", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1911.09560", "submitter": "Kai Arulkumaran", "authors": "Andrea Agostinelli, Kai Arulkumaran, Marta Sarrico, Pierre Richemond,\n  Anil Anthony Bharath", "title": "Memory-Efficient Episodic Control Reinforcement Learning with Dynamic\n  Online k-means", "comments": "Workshop on Biological and Artificial Reinforcement Learning, NeurIPS\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neuro-inspired episodic control (EC) methods have been developed to\novercome the data-inefficiency of standard deep reinforcement learning\napproaches. Using non-/semi-parametric models to estimate the value function,\nthey learn rapidly, retrieving cached values from similar past states. In\nrealistic scenarios, with limited resources and noisy data, maintaining\nmeaningful representations in memory is essential to speed up the learning and\navoid catastrophic forgetting. Unfortunately, EC methods have a large space and\ntime complexity. We investigate different solutions to these problems based on\nprioritising and ranking stored states, as well as online clustering\ntechniques. We also propose a new dynamic online k-means algorithm that is both\ncomputationally-efficient and yields significantly better performance at\nsmaller memory sizes; we validate this approach on classic reinforcement\nlearning environments and Atari games.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 15:54:49 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Agostinelli", "Andrea", ""], ["Arulkumaran", "Kai", ""], ["Sarrico", "Marta", ""], ["Richemond", "Pierre", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1911.09615", "submitter": "Kai Arulkumaran", "authors": "Marta Sarrico, Kai Arulkumaran, Andrea Agostinelli, Pierre Richemond,\n  Anil Anthony Bharath", "title": "Sample-Efficient Reinforcement Learning with Maximum Entropy Mellowmax\n  Episodic Control", "comments": "Workshop on Biological and Artificial Reinforcement Learning, NeurIPS\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have enabled reinforcement learning to scale to more complex\nand challenging domains, but these methods typically require large quantities\nof training data. An alternative is to use sample-efficient episodic control\nmethods: neuro-inspired algorithms which use non-/semi-parametric models that\npredict values based on storing and retrieving previously experienced\ntransitions. One way to further improve the sample efficiency of these\napproaches is to use more principled exploration strategies. In this work, we\ntherefore propose maximum entropy mellowmax episodic control (MEMEC), which\nsamples actions according to a Boltzmann policy with a state-dependent\ntemperature. We demonstrate that MEMEC outperforms other uncertainty- and\nsoftmax-based exploration methods on classic reinforcement learning\nenvironments and Atari games, achieving both more rapid learning and higher\nfinal rewards.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:19:36 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Sarrico", "Marta", ""], ["Arulkumaran", "Kai", ""], ["Agostinelli", "Andrea", ""], ["Richemond", "Pierre", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1911.09948", "submitter": "Dominique Beroule", "authors": "Dominique B\\'eroule (LIMSI), Pascale Gisquet-Verrier (Neuro-PSI)", "title": "Decision Making guided by Emotion A computational architecture", "comments": null, "journal-ref": "WCCI 2012 IEEE world congress on computational intelligence, Jun\n  2012, Brisbane, France", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational architecture is presented, in which \"swift and fuzzy\"\nemotional channels guide a \"slow and precise\" decision-making channel. Reported\nneurobiological studies first provide hints on the representation of both\nemotional and cognitive dimensions across brain structures, mediated by the\nneuromodulation system. The related model is based on Guided Propagation\nNetworks, the inner flows of which can be guided through modulation. A\nkey-channel of this model grows from a few emotional cues, and is aimed at\nanticipating the consequences of ongoing possible actions. Current experimental\nresults of a computer simulation show the integrated contribution of several\nemotional influences, as well as issues of accidental all-out emotions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:49:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["B\u00e9roule", "Dominique", "", "LIMSI"], ["Gisquet-Verrier", "Pascale", "", "Neuro-PSI"]]}, {"id": "1911.09977", "submitter": "Eirini Troullinou", "authors": "Eirini Troullinou, Grigorios Tsagkatakis, Spyridon Chavlis, Gergely\n  Turi, Wen-Ke Li, Attila Losonczy, Panagiotis Tsakalides, Panayiota Poirazi", "title": "Artificial neural networks in action for an automated cell-type\n  classification of biological neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of different neuronal cell types is critical for understanding\ntheir contribution to brain functions. Yet, automated and reliable\nclassification of neurons remains a challenge, primarily because of their\nbiological complexity. Typical approaches include laborious and expensive\nimmunohistochemical analysis while feature extraction algorithms based on\ncellular characteristics have recently been proposed. The former rely on\nmolecular markers, which are often expressed in many cell types, while the\nlatter suffer from similar issues: finding features that are distinctive for\neach class has proven to be equally challenging. Moreover, both approaches are\ntime consuming and demand a lot of human intervention. In this work we\nestablish the first, automated cell-type classification method that relies on\nneuronal activity rather than molecular or cellular features. We test our\nmethod on a real-world dataset comprising of raw calcium activity signals for\nfour neuronal types. We compare the performance of three different deep\nlearning models and demonstrate that our method can achieve automated\nclassification of neuronal cell types with unprecedented accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:19:25 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 11:41:40 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 12:11:25 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Troullinou", "Eirini", ""], ["Tsagkatakis", "Grigorios", ""], ["Chavlis", "Spyridon", ""], ["Turi", "Gergely", ""], ["Li", "Wen-Ke", ""], ["Losonczy", "Attila", ""], ["Tsakalides", "Panagiotis", ""], ["Poirazi", "Panayiota", ""]]}, {"id": "1911.10113", "submitter": "Mohammed K Alzaylaee Dr", "authors": "Mohammed K. Alzaylaee, Suleiman Y. Yerima and Sakir Sezer", "title": "DL-Droid: Deep learning based android malware detection using real\n  devices", "comments": null, "journal-ref": null, "doi": "10.1016/j.cose.2019.101663", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Android operating system has been the most popular for smartphones and\ntablets since 2012. This popularity has led to a rapid raise of Android malware\nin recent years. The sophistication of Android malware obfuscation and\ndetection avoidance methods have significantly improved, making many\ntraditional malware detection methods obsolete. In this paper, we propose\nDL-Droid, a deep learning system to detect malicious Android applications\nthrough dynamic analysis using stateful input generation. Experiments performed\nwith over 30,000 applications (benign and malware) on real devices are\npresented. Furthermore, experiments were also conducted to compare the\ndetection performance and code coverage of the stateful input generation method\nwith the commonly used stateless approach using the deep learning system. Our\nstudy reveals that DL-Droid can achieve up to 97.8% detection rate (with\ndynamic features only) and 99.6% detection rate (with dynamic + static\nfeatures) respectively which outperforms traditional machine learning\ntechniques. Furthermore, the results highlight the significance of enhanced\ninput generation for dynamic analysis as DL-Droid with the state-based input\ngeneration is shown to outperform the existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:16:15 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Alzaylaee", "Mohammed K.", ""], ["Yerima", "Suleiman Y.", ""], ["Sezer", "Sakir", ""]]}, {"id": "1911.10124", "submitter": "Timoth\\'ee Masquelier Dr", "authors": "Romain Zimmer, Thomas Pellegrini, Srisht Fateh Singh, Timoth\\'ee\n  Masquelier", "title": "Technical report: supervised training of convolutional spiking neural\n  networks with PyTorch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been shown that spiking neural networks (SNNs) can be\ntrained efficiently, in a supervised manner, using backpropagation through\ntime. Indeed, the most commonly used spiking neuron model, the leaky\nintegrate-and-fire neuron, obeys a differential equation which can be\napproximated using discrete time steps, leading to a recurrent relation for the\npotential. The firing threshold causes optimization issues, but they can be\novercome using a surrogate gradient. Here, we extend previous approaches in two\nways. Firstly, we show that the approach can be used to train convolutional\nlayers. Convolutions can be done in space, time (which simulates conduction\ndelays), or both. Secondly, we include fast horizontal connections \\`a la\nDen\\`eve: when a neuron N fires, we subtract to the potentials of all the\nneurons with the same receptive the dot product between their weight vectors\nand the one of neuron N. As Den\\`eve et al. showed, this is useful to represent\na dynamic multidimensional analog signal in a population of spiking neurons.\nHere we demonstrate that, in addition, such connections also allow implementing\na multidimensional send-on-delta coding scheme. We validate our approach on one\nspeech classification benchmarks: the Google speech command dataset. We managed\nto reach nearly state-of-the-art accuracy (94%) while maintaining low firing\nrates (about 5Hz). Our code is based on PyTorch and is available in open source\nat http://github.com/romainzimmer/s2net\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:24:38 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zimmer", "Romain", ""], ["Pellegrini", "Thomas", ""], ["Singh", "Srisht Fateh", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1911.10351", "submitter": "Andrei Velichko", "authors": "Andrei Velichko, Petr Boriskov", "title": "Oscillator Circuit for Spike Neural Network with Sigmoid Like Activation\n  Function and Firing Rate Coding", "comments": "9 pages, 8 figures. in IEEE Transactions on Circuits and Systems II:\n  Express Briefs", "journal-ref": null, "doi": "10.1109/TCSII.2020.2997117", "report-no": null, "categories": "cs.ET cs.NE physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study presents an oscillator circuit for a spike neural network with the\npossibility of firing rate coding and sigmoid-like activation function. The\ncircuit contains a switching element with an S-shaped current-voltage\ncharacteristic and two capacitors; one of the capacitors is shunted by a\ncontrol resistor. The circuit is characterised by a strong dependence of the\nfrequency of relaxation oscillations on the magnitude of the control resistor.\nThe dependence has a sigmoid-like form and we present an analytical method for\ndependence calculation. Finally, we describe the concept of the spike neural\nnetwork architecture with firing rate coding based on the presented circuit for\ncreating neuromorphic devices and artificial intelligence.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 11:36:08 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Velichko", "Andrei", ""], ["Boriskov", "Petr", ""]]}, {"id": "1911.10442", "submitter": "Eli (Omid) David", "authors": "Ido Faran, Nathan S. Netanyahu, Eli David, Maxim Shoshany, Fadi Kizel,\n  Jisung Geba Chang, Ronit Rud", "title": "Ground Truth Simulation for Deep Learning Classification of\n  Mid-Resolution Venus Images Via Unmixing of High-Resolution Hyperspectral\n  Fenix Data", "comments": null, "journal-ref": "IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS), pages 807-810, Yokohama, Japan, July 2019", "doi": "10.1109/IGARSS.2019.8900186", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network for classification constitutes a major problem\nin remote sensing due to the lack of adequate field data. Acquiring\nhigh-resolution ground truth (GT) by human interpretation is both\ncost-ineffective and inconsistent. We propose, instead, to utilize\nhigh-resolution, hyperspectral images for solving this problem, by unmixing\nthese images to obtain reliable GT for training a deep network. Specifically,\nwe simulate GT from high-resolution, hyperspectral FENIX images, and use it for\ntraining a convolutional neural network (CNN) for pixel-based classification.\nWe show how the model can be transferred successfully to classify new\nmid-resolution VENuS imagery.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 01:31:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Faran", "Ido", ""], ["Netanyahu", "Nathan S.", ""], ["David", "Eli", ""], ["Shoshany", "Maxim", ""], ["Kizel", "Fadi", ""], ["Chang", "Jisung Geba", ""], ["Rud", "Ronit", ""]]}, {"id": "1911.10519", "submitter": "Priyansh Saxena", "authors": "Priyansh Saxena, Shivani Tayal, Raahat Gupta, Akshat Maheshwari,\n  Gaurav Kaushal, Ritu Tiwari", "title": "Three Dimensional Route Planning for Multiple Unmanned Aerial Vehicles\n  using Salp Swarm Algorithm", "comments": "An author was mistakenly added in the paper dur to some human error.\n  Moreover the research is still under progress and we would like to upload the\n  paper with the final results once the work is complete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Route planning for multiple Unmanned Aerial Vehicles (UAVs) is a series of\ntranslation and rotational steps from a given start location to the destination\ngoal location. The goal of the route planning problem is to determine the most\noptimal route avoiding any collisions with the obstacles present in the\nenvironment. Route planning is an NP-hard optimization problem. In this paper,\na newly proposed Salp Swarm Algorithm (SSA) is used, and its performance is\ncompared with deterministic and other Nature-Inspired Algorithms (NIAs). The\nresults illustrate that SSA outperforms all the other meta-heuristic algorithms\nin route planning for multiple UAVs in a 3D environment. The proposed approach\nimproves the average cost and overall time by 1.25% and 6.035% respectively\nwhen compared to recently reported data. Route planning is involved in many\nreal-life applications like robot navigation, self-driving car, autonomous UAV\nfor search and rescue operations in dangerous ground-zero situations, civilian\nsurveillance, military combat and even commercial services like package\ndelivery by drones.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 12:36:18 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 11:31:55 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 15:14:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Saxena", "Priyansh", ""], ["Tayal", "Shivani", ""], ["Gupta", "Raahat", ""], ["Maheshwari", "Akshat", ""], ["Kaushal", "Gaurav", ""], ["Tiwari", "Ritu", ""]]}, {"id": "1911.10735", "submitter": "Julien Girard-Satabin", "authors": "Julien Girard-Satabin (TAU, LIST), Guillaume Charpiat (LRI, TAU),\n  Zakaria Chihani (LIST), Marc Schoenauer (TAU)", "title": "CAMUS: A Framework to Build Formal Specifications for Deep Perception\n  Systems Using Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of provable deep neural network robustness has raised considerable\ninterest in recent years. Most research has focused on adversarial robustness,\nwhich studies the robustness of perceptive models in the neighbourhood of\nparticular samples. However, other works have proved global properties of\nsmaller neural networks. Yet, formally verifying perception remains uncharted.\nThis is due notably to the lack of relevant properties to verify, as the\ndistribution of possible inputs cannot be formally specified. We propose to\ntake advantage of the simulators often used either to train machine learning\nmodels or to check them with statistical tests, a growing trend in industry.\nOur formulation allows us to formally express and verify safety properties on\nperception units, covering all cases that could ever be generated by the\nsimulator, to the difference of statistical tests which cover only seen\nexamples. Along with this theoretical formulation , we provide a tool to\ntranslate deep learning models into standard logical formulae. As a proof of\nconcept, we train a toy example mimicking an autonomous car perceptive unit,\nand we formally verify that it will never fail to capture the relevant\ninformation in the provided inputs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:28:45 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Girard-Satabin", "Julien", "", "TAU, LIST"], ["Charpiat", "Guillaume", "", "LRI, TAU"], ["Chihani", "Zakaria", "", "LIST"], ["Schoenauer", "Marc", "", "TAU"]]}, {"id": "1911.10741", "submitter": "Jun Zhou", "authors": "Bo Wang, Jun Zhou, Weng-Fai Wong, and Li-Shiuan Peh", "title": "Shenjing: A low power reconfigurable neuromorphic accelerator with\n  partial-sum and spike networks-on-chip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next wave of on-device AI will likely require energy-efficient deep\nneural networks. Brain-inspired spiking neural networks (SNN) has been\nidentified to be a promising candidate. Doing away with the need for\nmultipliers significantly reduces energy. For on-device applications, besides\ncomputation, communication also incurs a significant amount of energy and time.\nIn this paper, we propose Shenjing, a configurable SNN architecture which fully\nexposes all on-chip communications to software, enabling software mapping of\nSNN models with high accuracy at low power. Unlike prior SNN architectures like\nTrueNorth, Shenjing does not require any model modification and retraining for\nthe mapping. We show that conventional artificial neural networks (ANN) such as\nmultilayer perceptron, convolutional neural networks, as well as the latest\nresidual neural networks can be mapped successfully onto Shenjing, realizing\nANNs with SNN's energy efficiency. For the MNIST inference problem using a\nmultilayer perceptron, we were able to achieve an accuracy of 96% while\nconsuming just 1.26mW using 10 Shenjing cores.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:32:24 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Wang", "Bo", ""], ["Zhou", "Jun", ""], ["Wong", "Weng-Fai", ""], ["Peh", "Li-Shiuan", ""]]}, {"id": "1911.10943", "submitter": "Thiparat Chotibut", "authors": "Zuozhu Liu, Thiparat Chotibut, Christopher Hillar, Shaowei Lin", "title": "Biologically Plausible Sequence Learning with Spiking Neural Networks", "comments": "Accepted for publication in the Proceedings of the 34th AAAI\n  Conference on Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the celebrated discrete-time model of nervous activity outlined\nby McCulloch and Pitts in 1943, we propose a novel continuous-time model, the\nMcCulloch-Pitts network (MPN), for sequence learning in spiking neural\nnetworks. Our model has a local learning rule, such that the synaptic weight\nupdates depend only on the information directly accessible by the synapse. By\nexploiting asymmetry in the connections between binary neurons, we show that\nMPN can be trained to robustly memorize multiple spatiotemporal patterns of\nbinary vectors, generalizing the ability of the symmetric Hopfield network to\nmemorize static spatial patterns. In addition, we demonstrate that the model\ncan efficiently learn sequences of binary pictures as well as generative models\nfor experimental neural spike-train data. Our learning rule is consistent with\nspike-timing-dependent plasticity (STDP), thus providing a theoretical ground\nfor the systematic design of biologically inspired networks with large and\nrobust long-range sequence storage capacity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:11:07 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Zuozhu", ""], ["Chotibut", "Thiparat", ""], ["Hillar", "Christopher", ""], ["Lin", "Shaowei", ""]]}, {"id": "1911.10988", "submitter": "Richard Gerum", "authors": "Richard C. Gerum, Andr\\'e Erpenbeck, Patrick Krauss, Achim Schilling", "title": "Sparsity through evolutionary pruning prevents neuronal networks from\n  overfitting", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2020.05.007", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Machine learning techniques take advantage of the exponentially rising\ncalculation power in new generation processor units. Thus, the number of\nparameters which are trained to resolve complex tasks was highly increased over\nthe last decades. However, still the networks fail - in contrast to our brain -\nto develop general intelligence in the sense of being able to solve several\ncomplex tasks with only one network architecture. This could be the case\nbecause the brain is not a randomly initialized neural network, which has to be\ntrained by simply investing a lot of calculation power, but has from birth some\nfixed hierarchical structure. To make progress in decoding the structural basis\nof biological neural networks we here chose a bottom-up approach, where we\nevolutionarily trained small neural networks in performing a maze task. This\nsimple maze task requires dynamical decision making with delayed rewards. We\nwere able to show that during the evolutionary optimization random severance of\nconnections lead to better generalization performance of the networks compared\nto fully connected networks. We conclude that sparsity is a central property of\nneural networks and should be considered for modern Machine learning\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 14:13:09 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 15:45:19 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Gerum", "Richard C.", ""], ["Erpenbeck", "Andr\u00e9", ""], ["Krauss", "Patrick", ""], ["Schilling", "Achim", ""]]}, {"id": "1911.10995", "submitter": "Guoxiang Tong", "authors": "Abel Sa\\^A J. R Malano, Guanjun Du, Guoxiang Tong, and Naixue Xiong", "title": "DE/RM-MEDA: A New Hybrid Multi-Objective Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the condition of Karush-Kuhn-Tucker, the Pareto Set (PS) in the\ndecision area of an m-objective optimization problem is a piecewise continuous\n(m-1)-D manifold. For illustrate the degree of convergence of the population,\nwe employed the ratio of the sum of the first (m-1) largest eigenvalue of the\npopulation's covariance matrix of the sum of all eigenvalue. Based on this\nproperty, this paper proposes a new algorithm, called DE/RM-MEDA, which mix\ndifferential evolutionary (DE) and the estimation of distribution algorithm\n(EDA) to generate and adaptively adjusts the number of new solutions by the\nratio. The proposed algorithm is experimented on nine tec09 problems. The\ncomparison results between DE/RM-MEDA and the others algorithms, called\nNSGA-II-DE and RM-MEDA, show that the proposed algorithm perform better in\nterms of convergence and diversity metric.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 13:59:19 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Malano", "Abel Sa\u00c2 J. R", ""], ["Du", "Guanjun", ""], ["Tong", "Guoxiang", ""], ["Xiong", "Naixue", ""]]}, {"id": "1911.11285", "submitter": "Pierre Richemond", "authors": "Pierre H. Richemond, Arinbj\\\"orn Kolbeinsson, Yike Guo", "title": "Biologically inspired architectures for sample-efficient deep\n  reinforcement learning", "comments": "Deep Reinforcement Learning Workshop, NeurIPS 2019, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning requires a heavy price in terms of sample\nefficiency and overparameterization in the neural networks used for function\napproximation. In this work, we use tensor factorization in order to learn more\ncompact representation for reinforcement learning policies. We show empirically\nthat in the low-data regime, it is possible to learn online policies with 2 to\n10 times less total coefficients, with little to no loss of performance. We\nalso leverage progress in second order optimization, and use the theory of\nwavelet scattering to further reduce the number of learned coefficients, by\nforegoing learning the topmost convolutional layer filters altogether. We\nevaluate our results on the Atari suite against recent baseline algorithms that\nrepresent the state-of-the-art in data efficiency, and get comparable results\nwith an order of magnitude gain in weight parsimony.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 23:59:22 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Richemond", "Pierre H.", ""], ["Kolbeinsson", "Arinbj\u00f6rn", ""], ["Guo", "Yike", ""]]}, {"id": "1911.11326", "submitter": "Doo Seok Jeong", "authors": "Vladimir Kornijcuk, Dohun Kim, Guhyun Kim, Doo Seok Jeong", "title": "Simplified calcium signaling cascade for synaptic plasticity", "comments": "42 pages, 7 figures, Accepted by Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for synaptic plasticity based on a calcium signaling\ncascade. The model simplifies the full signaling pathways from a calcium influx\nto the phosphorylation (potentiation) and dephosphorylation (depression) of\nglutamate receptors that are gated by fictive C1 and C2 catalysts,\nrespectively. This model is based on tangible chemical reactions, including\nfictive catalysts, for long-term plasticity rather than the conceptual theories\ncommonplace in various models, such as preset thresholds of calcium\nconcentration. Our simplified model successfully reproduced the experimental\nsynaptic plasticity induced by different protocols such as (i) a synchronous\npairing protocol and (ii) correlated presynaptic and postsynaptic action\npotentials (APs). Further, the ocular dominance plasticity (or the experimental\nverification of the celebrated Bienenstock--Cooper--Munro theory) was\nreproduced by two model synapses that compete by means of back-propagating APs\n(bAPs). The key to this competition is synapse-specific bAPs with reference to\nbAP-boosting on the physiological grounds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 04:02:34 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Kornijcuk", "Vladimir", ""], ["Kim", "Dohun", ""], ["Kim", "Guhyun", ""], ["Jeong", "Doo Seok", ""]]}, {"id": "1911.11423", "submitter": "Stephen Merity", "authors": "Stephen Merity", "title": "Single Headed Attention RNN: Stop Thinking With Your Head", "comments": "Addition of citations and contextual results (no attention head,\n  single attention head, attention per layer), removal of wordpiece\n  WikiText-103 numbers due to normalization issues, fix of SHA attention figure\n  Q arrow, other minor fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leading approaches in language modeling are all obsessed with TV shows of\nmy youth - namely Transformers and Sesame Street. Transformers this,\nTransformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer\nscale silicon. We opt for the lazy path of old and proven techniques with a\nfancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The\nauthor's lone goal is to show that the entire field might have evolved a\ndifferent direction if we had instead been obsessed with a slightly different\nacronym and slightly different result. We take a previously strong language\nmodel based only on boring LSTMs and get it to within a stone's throw of a\nstone's throw of state-of-the-art byte level language model results on enwik8.\nThis work has undergone no intensive hyperparameter optimization and lived\nentirely on a commodity desktop machine that made the author's small studio\napartment far too warm in the midst of a San Franciscan summer. The final\nresults are achievable in plus or minus 24 hours on a single GPU as the author\nis impatient. The attention mechanism is also readily extended to large\ncontexts with minimal computation. Take that Sesame Street.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 09:45:33 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 12:00:15 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Merity", "Stephen", ""]]}, {"id": "1911.11691", "submitter": "Siavash Golkar", "authors": "Siavash Golkar", "title": "Emergent Structures and Lifetime Structure Evolution in Artificial\n  Neural Networks", "comments": "Proceedings of NeurIPS workshop on Real Neurons & Hidden Units. 5\n  Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the flexibility of biological neural networks whose connectivity\nstructure changes significantly during their lifetime, we introduce the\nUnstructured Recursive Network (URN) and demonstrate that it can exhibit\nsimilar flexibility during training via gradient descent. We show empirically\nthat many of the different neural network structures commonly used in practice\ntoday (including fully connected, locally connected and residual networks of\ndifferent depths and widths) can emerge dynamically from the same URN. These\ndifferent structures can be derived using gradient descent on a single general\nloss function where the structure of the data and the relative strengths of\nvarious regulator terms determine the structure of the emergent network. We\nshow that this loss function and the regulators arise naturally when\nconsidering the symmetries of the network as well as the geometric properties\nof the input data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:51:37 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Golkar", "Siavash", ""]]}, {"id": "1911.12352", "submitter": "Necati Uysal", "authors": "Baogang Zhang, Necati Uysal, Deliang Fan, Rickard Ewetz", "title": "Representable Matrices: Enabling High Accuracy Analog Computation for\n  Inference of DNNs using Memristors", "comments": "6 pages, ASPDAC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analog computing based on memristor technology is a promising solution to\naccelerating the inference phase of deep neural networks (DNNs). A fundamental\nproblem is to map an arbitrary matrix to a memristor crossbar array (MCA) while\nmaximizing the resulting computational accuracy. The state-of-the-art mapping\ntechnique is based on a heuristic that only guarantees to produce the correct\noutput for two input vectors. In this paper, a technique that aims to produce\nthe correct output for every input vector is proposed, which involves\nspecifying the memristor conductance values and a scaling factor realized by\nthe peripheral circuitry. The key insight of the paper is that the conductance\nmatrix realized by an MCA is only required to be proportional to the target\nmatrix. The selection of the scaling factor between the two regulates the\nutilization of the programmable memristor conductance range and the\nrepresentability of the target matrix. Consequently, the scaling factor is set\nto balance precision and value range errors. Moreover, a technique of\nconverting conductance values into state variables and vice versa is proposed\nto handle memristors with non-ideal device characteristics. Compared with the\nstate-of-the-art technique, the proposed mapping results in 4X-9X smaller\nerrors. The improvements translate into that the classification accuracy of a\nseven-layer convolutional neural network (CNN) on CIFAR-10 is improved from\n20.5% to 71.8%.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:53:18 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zhang", "Baogang", ""], ["Uysal", "Necati", ""], ["Fan", "Deliang", ""], ["Ewetz", "Rickard", ""]]}, {"id": "1911.12446", "submitter": "Samuel Bosch", "authors": "Samuel Bosch, Alexander Sanchez de la Cerda, Mohsen Imani, Tajana\n  Simunic Rosing and Giovanni De Micheli", "title": "QubitHD: A Stochastic Acceleration Method for HD Computing-Based Machine\n  Learning", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine Learning algorithms based on Brain-inspired Hyperdimensional (HD)\ncomputing imitate cognition by exploiting statistical properties of\nhigh-dimensional vector spaces. It is a promising solution for achieving high\nenergy-efficiency in different machine learning tasks, such as classification,\nsemi-supervised learning and clustering. A weakness of existing HD\ncomputing-based ML algorithms is the fact that they have to be binarized for\nachieving very high energy-efficiency. At the same time, binarized models reach\nlower classification accuracies. To solve the problem of the trade-off between\nenergy-efficiency and classification accuracy, we propose the QubitHD\nalgorithm. It stochastically binarizes HD-based algorithms, while maintaining\ncomparable classification accuracies to their non-binarized counterparts. The\nFPGA implementation of QubitHD provides a 65% improvement in terms of\nenergy-efficiency, and a 95% improvement in terms of the training time, as\ncompared to state-of-the-art HD-based ML algorithms. It also outperforms\nstate-of-the-art low-cost classifiers (like Binarized Neural Networks) in terms\nof speed and energy-efficiency by an order of magnitude during training and\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 22:20:00 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 13:12:54 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bosch", "Samuel", ""], ["de la Cerda", "Alexander Sanchez", ""], ["Imani", "Mohsen", ""], ["Rosing", "Tajana Simunic", ""], ["De Micheli", "Giovanni", ""]]}, {"id": "1911.12457", "submitter": "Hadis Mohseni", "authors": "Sina Hojjatinia, Sajad Hamzenejadi, and Hadis Mohseni", "title": "Android Botnet Detection using Convolutional Neural Networks", "comments": "submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, Android devices are able to provide various services. They support\napplications for different purposes such as entertainment, business, health,\neducation, and banking services. Because of the functionality and popularity of\nAndroid devices as well as the open-source policy of Android OS, they have\nbecome a suitable target for attackers. Android Botnet is one of the most\ndangerous malwares because an attacker called Botmaster can control that\nremotely to perform destructive attacks. A number of researchers have used\ndifferent well-known Machine Learning (ML) methods to recognize Android Botnets\nfrom benign applications. However, these conventional methods are not able to\ndetect new sophisticated Android Botnets. In this paper, we propose a novel\nmethod based on Android permissions and Convolutional Neural Networks (CNNs) to\nclassify Botnets and benign Android applications. Being the first developed\nmethod that uses CNNs for this aim, we also proposed a novel method to\nrepresent each application as an image which is constructed based on the\nco-occurrence of used permissions in that application. The proposed CNN is a\nbinary classifier that is trained using these images. Evaluating the proposed\nmethod on 5450 Android applications consist of Botnet and benign samples, the\nobtained results show the accuracy of 97.2% and recall of 96% which is a\npromising result just using Android permissions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 23:03:49 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hojjatinia", "Sina", ""], ["Hamzenejadi", "Sajad", ""], ["Mohseni", "Hadis", ""]]}, {"id": "1911.12675", "submitter": "Xu Shen", "authors": "Xu Shen, Xinmei Tian, Tongliang Liu, Fang Xu and Dacheng Tao", "title": "Continuous Dropout", "comments": "Accepted by TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout has been proven to be an effective algorithm for training robust deep\nnetworks because of its ability to prevent overfitting by avoiding the\nco-adaptation of feature detectors. Current explanations of dropout include\nbagging, naive Bayes, regularization, and sex in evolution. According to the\nactivation patterns of neurons in the human brain, when faced with different\nsituations, the firing rates of neurons are random and continuous, not binary\nas current dropout does. Inspired by this phenomenon, we extend the traditional\nbinary dropout to continuous dropout. On the one hand, continuous dropout is\nconsiderably closer to the activation characteristics of neurons in the human\nbrain than traditional binary dropout. On the other hand, we demonstrate that\ncontinuous dropout has the property of avoiding the co-adaptation of feature\ndetectors, which suggests that we can extract more independent feature\ndetectors for model averaging in the test stage. We introduce the proposed\ncontinuous dropout to a feedforward neural network and comprehensively compare\nit with binary dropout, adaptive dropout, and DropConnect on MNIST, CIFAR-10,\nSVHN, NORB, and ILSVRC-12. Thorough experiments demonstrate that our method\nperforms better in preventing the co-adaptation of feature detectors and\nimproves test performance. The code is available at:\nhttps://github.com/jasonustc/caffe-multigpu/tree/dropout.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:37:48 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["Liu", "Tongliang", ""], ["Xu", "Fang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1911.12809", "submitter": "George De Ath", "authors": "George De Ath, Richard M. Everson, Alma A. M. Rahat and Jonathan E.\n  Fieldsend", "title": "Greed is Good: Exploration and Exploitation Trade-offs in Bayesian\n  Optimisation", "comments": "Published in ACM Transactions on Evolutionary Learning and\n  Optimization (TELO). 22 pages (main paper) + 27 pages (supplementary\n  material)", "journal-ref": null, "doi": "10.1145/3425501", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of acquisition functions for Bayesian optimisation to locate\nthe global optimum of continuous functions is investigated in terms of the\nPareto front between exploration and exploitation. We show that Expected\nImprovement (EI) and the Upper Confidence Bound (UCB) always select solutions\nto be expensively evaluated on the Pareto front, but Probability of Improvement\nis not guaranteed to do so and Weighted Expected Improvement does so only for a\nrestricted range of weights.\n  We introduce two novel $\\epsilon$-greedy acquisition functions. Extensive\nempirical evaluation of these together with random search, purely exploratory,\nand purely exploitative search on 10 benchmark problems in 1 to 10 dimensions\nshows that $\\epsilon$-greedy algorithms are generally at least as effective as\nconventional acquisition functions (e.g., EI and UCB), particularly with a\nlimited budget. In higher dimensions $\\epsilon$-greedy approaches are shown to\nhave improved performance over conventional approaches. These results are borne\nout on a real world computational fluid dynamics optimisation problem and a\nrobotics active learning problem. Our analysis and experiments suggest that the\nmost effective strategy, particularly in higher dimensions, is to be mostly\ngreedy, occasionally selecting a random exploratory solution.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 17:52:06 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 12:58:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["De Ath", "George", ""], ["Everson", "Richard M.", ""], ["Rahat", "Alma A. M.", ""], ["Fieldsend", "Jonathan E.", ""]]}, {"id": "1911.13011", "submitter": "Tarik A. Rashid", "authors": "Bryar A. Hassan, Tarik A. Rashid", "title": "Operational Framework for Recent Advances in Backtracking Search\n  Optimisation Algorithm: A Systematic Review and Performance Evaluation", "comments": "72 pages, 12 figures", "journal-ref": "Applied Mathematics and Computation,124919, 29 November 2019", "doi": "10.1016/j.amc.2019.124919", "report-no": null, "categories": "cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The experiments conducted in previous studies demonstrated the successful\nperformance of BSA and its non-sensitivity toward the several types of\noptimisation problems. This success of BSA motivated researchers to work on\nexpanding it, e.g., developing its improved versions or employing it for\ndifferent applications and problem domains. However, there is a lack of\nliterature review on BSA; therefore, reviewing the aforementioned modifications\nand applications systematically will aid further development of the algorithm.\nThis paper provides a systematic review and meta-analysis that emphasise on\nreviewing the related studies and recent developments on BSA. Hence, the\nobjectives of this work are two-fold: (i) First, two frameworks for depicting\nthe main extensions and the uses of BSA are proposed. The first framework is a\ngeneral framework to depict the main extensions of BSA, whereas the second is\nan operational framework to present the expansion procedures of BSA to guide\nthe researchers who are working on improving it. (ii) Second, the experiments\nconducted in this study fairly compare the analytical performance of BSA with\nfour other competitive algorithms: differential evolution (DE), particle swarm\noptimisation (PSO), artificial bee colony (ABC), and firefly (FF) on 16\ndifferent hardness scores of the benchmark functions with different initial\ncontrol parameters such as problem dimensions and search space. The\nexperimental results indicate that BSA is statistically superior than the\naforementioned algorithms in solving different cohorts of numerical\noptimisation problems such as problems with different levels of hardness score,\nproblem dimensions, and search spaces.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:26:41 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 06:11:53 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Hassan", "Bryar A.", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "1911.13053", "submitter": "Changlin Li", "authors": "Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang,\n  Liang Lin, Xiaojun Chang", "title": "Blockwisely Supervised Neural Architecture Search with Knowledge\n  Distillation", "comments": "To be appear in CVPR 2020. We achieve a state-of-the-art 78.4% top-1\n  accuracy on ImageNet in a mobile setting, which is about a 2.1% gain over\n  EfficientNet-B0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS), aiming at automatically designing network\narchitectures by machines, is hoped and expected to bring about a new\nrevolution in machine learning. Despite these high expectation, the\neffectiveness and efficiency of existing NAS solutions are unclear, with some\nrecent works going so far as to suggest that many existing NAS solutions are no\nbetter than random architecture selection. The inefficiency of NAS solutions\nmay be attributed to inaccurate architecture evaluation. Specifically, to speed\nup NAS, recent works have proposed under-training different candidate\narchitectures in a large search space concurrently by using shared network\nparameters; however, this has resulted in incorrect architecture ratings and\nfurthered the ineffectiveness of NAS.\n  In this work, we propose to modularize the large search space of NAS into\nblocks to ensure that the potential candidate architectures are fully trained;\nthis reduces the representation shift caused by the shared parameters and leads\nto the correct rating of the candidates. Thanks to the block-wise search, we\ncan also evaluate all of the candidate architectures within a block. Moreover,\nwe find that the knowledge of a network model lies not only in the network\nparameters but also in the network architecture. Therefore, we propose to\ndistill the neural architecture (DNA) knowledge from a teacher model as the\nsupervision to guide our block-wise architecture search, which significantly\nimproves the effectiveness of NAS. Remarkably, the capacity of our searched\narchitecture has exceeded the teacher model, demonstrating the practicability\nand scalability of our method. Finally, our method achieves a state-of-the-art\n78.4\\% top-1 accuracy on ImageNet in a mobile setting, which is about a 2.1\\%\ngain over EfficientNet-B0. All of our searched models along with the evaluation\ncode are available online.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 11:00:30 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 06:08:31 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Li", "Changlin", ""], ["Peng", "Jiefeng", ""], ["Yuan", "Liuchun", ""], ["Wang", "Guangrun", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""], ["Chang", "Xiaojun", ""]]}, {"id": "1911.13097", "submitter": "Johan Kwisthout", "authors": "Abdullahi Ali and Johan Kwisthout", "title": "A spiking neural algorithm for the Network Flow problem", "comments": "Supported by Intel Corporation in the Intel Neuromorphic Research\n  Community", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is currently not clear what the potential is of neuromorphic hardware\nbeyond machine learning and neuroscience. In this project, a problem is\ninvestigated that is inherently difficult to fully implement in neuromorphic\nhardware by introducing a new machine model in which a conventional Turing\nmachine and neuromorphic oracle work together to solve such types of problems.\nWe show that the P-complete Max Network Flow problem is intractable in models\nwhere the oracle may be consulted only once (`create-and-run' model) but\nbecomes tractable using an interactive (`neuromorphic co-processor') model of\ncomputation. More in specific we show that a logspace-constrained Turing\nmachine with access to an interactive neuromorphic oracle with linear space,\ntime, and energy constraints can solve Max Network Flow. A modified variant of\nthis algorithm is implemented on the Intel Loihi chip; a neuromorphic manycore\nprocessor developed by Intel Labs. We show that by off-loading the search for\naugmenting paths to the neuromorphic processor we can get energy efficiency\ngains, while not sacrificing runtime resources. This result demonstrates how\nP-complete problems can be mapped on neuromorphic architectures in a\ntheoretically and potentially practically efficient manner.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:19:50 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ali", "Abdullahi", ""], ["Kwisthout", "Johan", ""]]}, {"id": "1911.13135", "submitter": "Gabriel Turinici", "authors": "Gabriel Turinici (CEREMADE, Universit\\'e Paris Dauphine - PSL)", "title": "Radon Sobolev Variational Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The quality of generative models (such as Generative adversarial networks and\nVariational Auto-Encoders) depends heavily on the choice of a good probability\ndistance. However some popular metrics like the Wasserstein or the Sliced\nWasserstein distances, the Jensen-Shannon divergence, the Kullback-Leibler\ndivergence, lack convenient properties such as (geodesic) convexity, fast\nevaluation and so on. To address these shortcomings, we introduce a class of\ndistances that have built-in convexity. We investigate the relationship with\nsome known paradigms (sliced distances - a synonym for Radon distances -,\nreproducing kernel Hilbert spaces, energy distances). The distances are shown\nto possess fast implementations and are included in an adapted Variational\nAuto-Encoder termed Radon Sobolev Variational Auto-Encoder (RS-VAE) which\nproduces high quality results on standard generative datasets.\n  Keywords: Variational Auto-Encoder; Generative model; Sobolev spaces; Radon\nSobolev Variational Auto-Encoder;\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:02:28 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 17:00:16 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 18:08:35 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Turinici", "Gabriel", "", "CEREMADE, Universit\u00e9 Paris Dauphine - PSL"]]}, {"id": "1911.13214", "submitter": "Lionel Eyraud-Dubois", "authors": "Julien Herrmann (UB, LaBRI, TADAAM), Olivier Beaumont (HiePACS, UB,\n  LaBRI), Lionel Eyraud-Dubois (HiePACS, UB, LaBRI), Julien Hermann, Alexis\n  Joly (ZENITH, LIRMM, UM), Alena Shilova (HiePACS, UB, LaBRI)", "title": "Optimal checkpointing for heterogeneous chains: how to train deep neural\n  networks with limited memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new activation checkpointing method which allows to\nsignificantly decrease memory usage when training Deep Neural Networks with the\nback-propagation algorithm. Similarly to checkpoint-ing techniques coming from\nthe literature on Automatic Differentiation, it consists in dynamically\nselecting the forward activations that are saved during the training phase, and\nthen automatically recomputing missing activations from those previously\nrecorded. We propose an original computation model that combines two types of\nactivation savings: either only storing the layer inputs, or recording the\ncomplete history of operations that produced the outputs (this uses more\nmemory, but requires fewer recomputations in the backward phase), and we\nprovide an algorithm to compute the optimal computation sequence for this\nmodel. This paper also describes a PyTorch implementation that processes the\nentire chain, dealing with any sequential DNN whose internal layers may be\narbitrarily complex and automatically executing it according to the optimal\ncheckpointing strategy computed given a memory limit. Through extensive\nexperiments, we show that our implementation consistently outperforms existing\ncheckpoint-ing approaches for a large class of networks, image sizes and batch\nsizes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 13:05:11 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Herrmann", "Julien", "", "UB, LaBRI, TADAAM"], ["Beaumont", "Olivier", "", "HiePACS, UB,\n  LaBRI"], ["Eyraud-Dubois", "Lionel", "", "HiePACS, UB, LaBRI"], ["Hermann", "Julien", "", "ZENITH, LIRMM, UM"], ["Joly", "Alexis", "", "ZENITH, LIRMM, UM"], ["Shilova", "Alena", "", "HiePACS, UB, LaBRI"]]}]