[{"id": "1503.00022", "submitter": "Soham De", "authors": "Soham De, Indradyumna Roy, Tarunima Prabhakar, Kriti Suneja, Sourish\n  Chaudhuri, Rita Singh, Bhiksha Raj", "title": "Plagiarism Detection in Polyphonic Music using Monaural Signal\n  Separation", "comments": "Preprint version", "journal-ref": "INTERSPEECH-2012, 1744-1747 (2012)", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the large number of new musical tracks released each year, automated\napproaches to plagiarism detection are essential to help us track potential\nviolations of copyright. Most current approaches to plagiarism detection are\nbased on musical similarity measures, which typically ignore the issue of\npolyphony in music. We present a novel feature space for audio derived from\ncompositional modelling techniques, commonly used in signal separation, that\nprovides a mechanism to account for polyphony without incurring an inordinate\namount of computational overhead. We employ this feature representation in\nconjunction with traditional audio feature representations in a classification\nframework which uses an ensemble of distance features to characterize pairs of\nsongs as being plagiarized or not. Our experiments on a database of about 3000\nmusical track pairs show that the new feature space characterization produces\nsignificant improvements over standard baselines.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 21:57:16 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["De", "Soham", ""], ["Roy", "Indradyumna", ""], ["Prabhakar", "Tarunima", ""], ["Suneja", "Kriti", ""], ["Chaudhuri", "Sourish", ""], ["Singh", "Rita", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1503.00081", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang", "title": "Activity Recognition Using A Combination of Category Components And\n  Local Models for Video Surveillance", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 18,\n  pp. 1128-1139, 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatic recognition of human\nactivities for video surveillance applications. We propose to represent an\nactivity by a combination of category components, and demonstrate that this\napproach offers flexibility to add new activities to the system and an ability\nto deal with the problem of building models for activities lacking training\ndata. For improving the recognition accuracy, a Confident-Frame- based\nRecognition algorithm is also proposed, where the video frames with high\nconfidence for recognizing an activity are used as a specialized local model to\nhelp classify the remainder of the video frames. Experimental results show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:49:33 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Poovendran", "Radha", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "1503.00082", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang", "title": "Group Event Detection with a Varying Number of Group Members for Video\n  Surveillance", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 20,\n  no. 8, pp. 1057-1067, 2010", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatic recognition of group\nactivities for video surveillance applications. We propose to use a group\nrepresentative to handle the recognition with a varying number of group\nmembers, and use an Asynchronous Hidden Markov Model (AHMM) to model the\nrelationship between people. Furthermore, we propose a group activity detection\nalgorithm which can handle both symmetric and asymmetric group activities, and\ndemonstrate that this approach enables the detection of hierarchical\ninteractions between people. Experimental results show the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:51:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Poovendran", "Radha", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "1503.00083", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Krit Panusopone, David M. Baylon, Ming-Ting Sun", "title": "A Computation Control Motion Estimation Method for Complexity-Scalable\n  Video Coding", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 20,\n  no. 11, pp. 1533-1543, 2010", "doi": "10.1109/TCSVT.2010.2077773", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new Computation-Control Motion Estimation (CCME) method is\nproposed which can perform Motion Estimation (ME) adaptively under different\ncomputation or power budgets while keeping high coding performance. We first\npropose a new class-based method to measure the Macroblock (MB) importance\nwhere MBs are classified into different classes and their importance is\nmeasured by combining their class information as well as their initial matching\ncost information. Based on the new MB importance measure, a complete CCME\nframework is then proposed to allocate computation for ME. The proposed method\nperforms ME in a one-pass flow. Experimental results demonstrate that the\nproposed method can allocate computation more accurately than previous methods\nand thus has better performance under the same computation budget.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:59:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Weiyao", ""], ["Panusopone", "Krit", ""], ["Baylon", "David M.", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1503.00085", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Krit Panusopone, David M. Baylon, Ming-Ting Sun, Zhenzhong\n  Chen, Hongxiang Li", "title": "A Fast Sub-Pixel Motion Estimation Algorithm for H.264/AVC Video Coding", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 21,\n  no. 2, pp. 237-242, 2011", "doi": "10.1109/TCSVT.2011.2106290", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion Estimation (ME) is one of the most time-consuming parts in video\ncoding. The use of multiple partition sizes in H.264/AVC makes it even more\ncomplicated when compared to ME in conventional video coding standards. It is\nimportant to develop fast and effective sub-pixel ME algorithms since (a) The\ncomputation overhead by sub-pixel ME has become relatively significant while\nthe complexity of integer-pixel search has been greatly reduced by fast\nalgorithms, and (b) Reducing sub-pixel search points can greatly save the\ncomputation for sub-pixel interpolation. In this paper, a novel fast sub-pixel\nME algorithm is proposed which performs a 'rough' sub-pixel search before the\npartition selection, and performs a 'precise' sub-pixel search for the best\npartition. By reducing the searching load for the large number of non-best\npartitions, the computation complexity for sub-pixel search can be greatly\ndecreased. Experimental results show that our method can reduce the sub-pixel\nsearch points by more than 50% compared to existing fast sub-pixel ME methods\nwith negligible quality degradation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:08:03 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Weiyao", ""], ["Panusopone", "Krit", ""], ["Baylon", "David M.", ""], ["Sun", "Ming-Ting", ""], ["Chen", "Zhenzhong", ""], ["Li", "Hongxiang", ""]]}, {"id": "1503.00087", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Hongxiang Li, Zhenzhong Chen, Wei Li, Bing\n  Zhou", "title": "Macroblock Classification Method for Video Applications Involving\n  Motions", "comments": "This manuscript is the accepted version for TB (IEEE Transactions on\n  Broadcasting)", "journal-ref": "IEEE Trans. Broadcasting, vol. 58, no. 1, pp. 34-46, 2012", "doi": "10.1109/TBC.2011.2170611", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a macroblock classification method is proposed for various\nvideo processing applications involving motions. Based on the analysis of the\nMotion Vector field in the compressed video, we propose to classify Macroblocks\nof each video frame into different classes and use this class information to\ndescribe the frame content. We demonstrate that this low-computation-complexity\nmethod can efficiently catch the characteristics of the frame. Based on the\nproposed macroblock classification, we further propose algorithms for different\nvideo processing applications, including shot change detection, motion\ndiscontinuity detection, and outlier rejection for global motion estimation.\nExperimental results demonstrate that the methods based on the proposed\napproach can work effectively on these applications.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:14:01 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Li", "Hongxiang", ""], ["Chen", "Zhenzhong", ""], ["Li", "Wei", ""], ["Zhou", "Bing", ""]]}, {"id": "1503.00088", "submitter": "Weiyao Lin", "authors": "Yihao Zhang, Weiyao Lin, Bing Zhou, Zhenzhong Chen, Bin Sheng, Jianxin\n  Wu, Wenjun Zhang", "title": "Facial Expression Cloning with Elastic and Muscle Models", "comments": "This manuscript is the accepted version for JVCI (Journal of Visual\n  Communication and Image Representation)", "journal-ref": "Journal of Visual Communication and Image Representation, vol. 25,\n  no. 5, pp. 916-927, 2014", "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expression cloning plays an important role in facial expression synthesis. In\nthis paper, a novel algorithm is proposed for facial expression cloning. The\nproposed algorithm first introduces a new elastic model to balance the global\nand local warping effects, such that the impacts from facial feature diversity\namong people can be minimized, and thus more effective geometric warping\nresults can be achieved. Furthermore, a muscle-distribution-based (MD) model is\nproposed, which utilizes the muscle distribution of the human face and results\nin more accurate facial illumination details. In addition, we also propose a\nnew distance-based metric to automatically select the optimal parameters such\nthat the global and local warping effects in the elastic model can be suitably\nbalanced. Experimental results show that our proposed algorithm outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:23:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Zhang", "Yihao", ""], ["Lin", "Weiyao", ""], ["Zhou", "Bing", ""], ["Chen", "Zhenzhong", ""], ["Sheng", "Bin", ""], ["Wu", "Jianxin", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1503.00118", "submitter": "Weiyao Lin", "authors": "Mingliang Chen, Weiyao Lin, Xiaozhen Zheng", "title": "An Efficient Coding Method for Coding Region-of-Interest Locations in\n  AVS2", "comments": "This manuscript is the accepted version for ICMEW (IEEE Intl. Conf.\n  Multimedia & Expo Workshop), IEEE Intl. Conf. Multimedia & Expo Workshop\n  (ICME), 2014", "journal-ref": null, "doi": "10.1109/ICMEW.2014.6890688", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-of-Interest (ROI) location information in videos has many practical\nusages in video coding field, such as video content analysis and user\nexperience improvement. Although ROI-based coding has been studied widely by\nmany researchers to improve coding efficiency for video contents, the ROI\nlocation information itself is seldom coded in video bitstream. In this paper,\nwe will introduce our proposed ROI location coding tool which has been adopted\nin surveillance profile of AVS2 video coding standard (surveillance profile).\nOur tool includes three schemes: direct-coding scheme, differential- coding\nscheme, and reconstructed-coding scheme. We will illustrate the details of\nthese schemes, and perform analysis of their advantages and disadvantages,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 11:42:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chen", "Mingliang", ""], ["Lin", "Weiyao", ""], ["Zheng", "Xiaozhen", ""]]}, {"id": "1503.00121", "submitter": "Weiyao Lin", "authors": "Hai-Miao Hu, Bo Li, Weiyao Lin, Wei Li, Ming-Ting Sun", "title": "Region-Based Rate-Control for H.264/AVC for Low Bit-Rate Applications", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 22,\n  no. 11, pp. 1564-1576, 2012", "doi": "10.1109/TCSVT.2012.2199398", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate-control plays an important role in video coding. However, in the\nconventional rate-control algorithms, the number and position of Macroblocks\n(MBs) inside one basic unit for rate-control is inflexible and predetermined.\nThe different characteristics of the MBs are not fully considered. Also, there\nis no overall optimization of the coding of basic units. This paper proposes a\nnew region-based rate-control scheme for H.264/AVC to improve the coding\nefficiency. The inter-frame information is explored to objectively divide one\nframe into multiple regions based on their rate-distortion behaviors. The MBs\nwith the similar characteristics are classified into the same region, and the\nentire region instead of a single MB or a group of contiguous MBs is treated as\na basic unit for rate-control. A linear rate-quantization stepsize model and a\nlinear distortion-quantization stepsize model are proposed to accurately\ndescribe the rate-distortion characteristics for the region-based basic units.\nMoreover, based on the above linear models, an overall optimization model is\nproposed to obtain suitable Quantization Parameters (QPs) for the region-based\nbasic units. Experimental results demonstrate that the proposed region-based\nrate-control approach can achieve both better subjective and objective quality\nby performing the rate-control adaptively with the content, compared to the\nconventional rate-control approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 11:57:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Hu", "Hai-Miao", ""], ["Li", "Bo", ""], ["Lin", "Weiyao", ""], ["Li", "Wei", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1503.00245", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "Novel Metaknowledge-based Processing Technique for Multimedia Big Data\n  clustering challenges", "comments": "IEEE Multimedia Big Data (BigMM 2015)", "journal-ref": null, "doi": "10.1109/BigMM.2015.78", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past research has challenged us with the task of showing relational patterns\nbetween text-based data and then clustering for predictive analysis using Golay\nCode technique. We focus on a novel approach to extract metaknowledge in\nmultimedia datasets. Our collaboration has been an on-going task of studying\nthe relational patterns between datapoints based on metafeatures extracted from\nmetaknowledge in multimedia datasets. Those selected are significant to suit\nthe mining technique we applied, Golay Code algorithm. In this research paper\nwe summarize findings in optimization of metaknowledge representation for\n23-bit representation of structured and unstructured multimedia data in order\nto\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:53:15 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00388", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Jamil Ahmad, Haleem Farman, Muhammad Zubair", "title": "A Novel Image Steganographic Approach for Hiding Text in Color Images\n  using HSI Color Model", "comments": "An easy to follow paper of 11 pages. arXiv admin note: text overlap\n  with arXiv:1502.07808", "journal-ref": "Middle-East Journal of Scientific Research 22.5 (2014): 647-654", "doi": "10.5829/idosi.mejsr.2014.22.05.21946", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Steganography is the process of embedding text in images such that its\nexistence cannot be detected by Human Visual System (HVS) and is known only to\nsender and receiver. This paper presents a novel approach for image\nsteganography using Hue-Saturation-Intensity (HSI) color space based on Least\nSignificant Bit (LSB). The proposed method transforms the image from RGB color\nspace to Hue-Saturation-Intensity (HSI) color space and then embeds secret data\ninside the Intensity Plane (I-Plane) and transforms it back to RGB color model\nafter embedding. The said technique is evaluated by both subjective and\nObjective Analysis. Experimentally it is found that the proposed method have\nlarger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and\nmultiple security levels which shows its superiority as compared to several\nexisting methods\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 01:41:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Muhammad", "Khan", ""], ["Ahmad", "Jamil", ""], ["Farman", "Haleem", ""], ["Zubair", "Muhammad", ""]]}, {"id": "1503.00843", "submitter": "H.R.  Chennamma", "authors": "Sowmya K.N. and H.R. Chennamma", "title": "A Survey On Video Forgery Detection", "comments": "11 pages, 3 figures, International Journal of Computer Engineering\n  and Applications, Volume IX, Issue II, February 2015", "journal-ref": "International Journal of Computer Engineering and Applications,\n  Volume IX, Issue II, pp. 17-27, February 2015", "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Digital Forgeries though not visibly identifiable to human perception it\nmay alter or meddle with underlying natural statistics of digital content.\nTampering involves fiddling with video content in order to cause damage or make\nunauthorized alteration/modification. Tampering detection in video is\ncumbersome compared to image when considering the properties of the video.\nTampering impacts need to be studied and the applied technique/method is used\nto establish the factual information for legal course in judiciary. In this\npaper we give an overview of the prior literature and challenges involved in\nvideo forgery detection where passive approach is found.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 07:17:46 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["N.", "Sowmya K.", ""], ["Chennamma", "H. R.", ""]]}, {"id": "1503.01620", "submitter": "Mohsen Abdoli", "authors": "Mohsen Abdoli, Hossein Sarikhani, Mohammad Ghanbari, and Patrice\n  Brault", "title": "Gaussian Mixture Model Based Contrast Enhancement", "comments": null, "journal-ref": "Image Processing, IET, Vol. 9, No. 7, pp. 569-577, 2015", "doi": "10.1049/iet-ipr.2014.0583", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method for enhancing low contrast images is proposed. This\nmethod, called Gaussian Mixture Model based Contrast Enhancement (GMMCE),\nbrings into play the Gaussian mixture modeling of histograms to model the\ncontent of the images. Based on the fact that each homogeneous area in natural\nimages has a Gaussian-shaped histogram, it decomposes the narrow histogram of\nlow contrast images into a set of scaled and shifted Gaussians. The individual\nhistograms are then stretched by increasing their variance parameters, and are\ndiffused on the entire histogram by scattering their mean parameters, to build\na broad version of the histogram. The number of Gaussians as well as their\nparameters are optimized to set up a GMM with lowest approximation error and\nhighest similarity to the original histogram. Compared to the existing\nhistogram-based methods, the experimental results show that the quality of\nGMMCE enhanced pictures are mostly consistent and outperform other benchmark\nmethods. Additionally, the computational complexity analysis show that GMMCE is\na low complexity method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 12:31:36 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 08:12:27 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Abdoli", "Mohsen", ""], ["Sarikhani", "Hossein", ""], ["Ghanbari", "Mohammad", ""], ["Brault", "Patrice", ""]]}, {"id": "1503.01817", "submitter": "Bart Thomee", "authors": "Bart Thomee and David A. Shamma and Gerald Friedland and Benjamin\n  Elizalde and Karl Ni and Douglas Poland and Damian Borth and Li-Jia Li", "title": "YFCC100M: The New Data in Multimedia Research", "comments": null, "journal-ref": "Communications of the ACM, 59(2), pp. 64-73, 2016", "doi": "10.1145/2812802", "report-no": null, "categories": "cs.MM cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M),\nthe largest public multimedia collection that has ever been released. The\ndataset contains a total of 100 million media objects, of which approximately\n99.2 million are photos and 0.8 million are videos, all of which carry a\nCreative Commons license. Each media object in the dataset is represented by\nseveral pieces of metadata, e.g. Flickr identifier, owner name, camera, title,\ntags, geo, media source. The collection provides a comprehensive snapshot of\nhow photos and videos were taken, described, and shared over the years, from\nthe inception of Flickr in 2004 until early 2014. In this article we explain\nthe rationale behind its creation, as well as the implications the dataset has\nfor science, research, engineering, and development. We further present several\nnew challenges in multimedia research that can now be expanded upon with our\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 23:43:42 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 20:10:14 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Thomee", "Bart", ""], ["Shamma", "David A.", ""], ["Friedland", "Gerald", ""], ["Elizalde", "Benjamin", ""], ["Ni", "Karl", ""], ["Poland", "Douglas", ""], ["Borth", "Damian", ""], ["Li", "Li-Jia", ""]]}, {"id": "1503.01934", "submitter": "Siddharth Arora Dr.", "authors": "Subhayan Roy Moulick, Siddharth Arora, Chirag Jain, Prasanta K.\n  Panigrahi", "title": "Reliable SVD based Semi-blind and Invisible Watermarking Schemes", "comments": "11 Pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-blind watermarking scheme is presented based on Singular Value\nDecomposition (SVD), which makes essential use of the fact that, the SVD\nsubspace preserves significant amount of information of an image and is a one\nway decomposition. The principal components are used, along with the\ncorresponding singular vectors of the watermark image to watermark the target\nimage. For further security, the semi-blind scheme is extended to an invisible\nhash based watermarking scheme. The hash based scheme commits a watermark with\na key such that, it is incoherent with the actual watermark, and can only be\nextracted using the key. Its security is analyzed in the random oracle model\nand shown to be unforgeable, invisible and satisfying the property of\nnon-repudiation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 12:42:32 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Moulick", "Subhayan Roy", ""], ["Arora", "Siddharth", ""], ["Jain", "Chirag", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1503.02955", "submitter": "Konstantin Miller", "authors": "Konstantin Miller, Abdel-Karim Al-Tamimi, Adam Wolisz", "title": "Low-Delay Adaptive Video Streaming Based on Short-Term TCP Throughput\n  Prediction", "comments": "Technical Report TKN-15-001, Telecommunication Networks Group,\n  Technische Universitaet Berlin. Updated by TR TKN-16-001, available at\n  http://arxiv.org/abs/1603.00859", "journal-ref": null, "doi": null, "report-no": "TKN-15-001", "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, HTTP-Based Adaptive Streaming has become the de facto standard for\nvideo streaming over the Internet. It allows the client to adapt media\ncharacteristics to varying network conditions in order to maximize Quality of\nExperience (QoE). In the case of live streaming this task becomes particularly\nchallenging. An important factor than might help improving performance is the\ncapability to correctly predict network throughput dynamics on short to medium\ntimescales. It becomes notably difficult in wireless networks that are often\nsubject to continuous throughput fluctuations.\n  In the present work, we develop an adaptation algorithm for HTTP-Based\nAdaptive Live Streaming that, for each adaptation decision, maximizes a\nQoE-based utility function depending on the probability of playback\ninterruptions, average video quality, and the amount of video quality\nfluctuations. To compute the utility function the algorithm leverages\nthroughput predictions, and dynamically estimated prediction accuracy.\n  We are trying to close the gap created by the lack of studies analyzing TCP\nthroughput on short to medium timescales. We study several time series\nprediction methods and their error distributions. We observe that Simple Moving\nAverage performs best in most cases. We also observe that the relative\nunderestimation error is best represented by a truncated normal distribution,\nwhile the relative overestimation error is best represented by a Lomax\ndistribution. Moreover, underestimations and overestimations exhibit a temporal\ncorrelation that we use to further improve prediction accuracy.\n  We compare the proposed algorithm with a baseline approach that uses a fixed\nmargin between past throughput and selected media bit rate, and an oracle-based\napproach that has perfect knowledge over future throughput for a certain time\nhorizon.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 15:40:41 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 13:56:36 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 15:43:04 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Miller", "Konstantin", ""], ["Al-Tamimi", "Abdel-Karim", ""], ["Wolisz", "Adam", ""]]}, {"id": "1503.03674", "submitter": "Manjula G R", "authors": "G. R. Manjula, Ajit Danti", "title": "A novel hash based least significant bit (2-3-3) image steganography in\n  spatial domain", "comments": "10 pages International journal of security privacy and trust\n  management Feb 2015 issue", "journal-ref": null, "doi": "10.5121/ijsptm.2015.4102", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel 2-3-3 LSB insertion method. The image\nsteganography takes the advantage of human eye limitation. It uses color image\nas cover media for embedding secret message.The important quality of a\nsteganographic system is to be less distortive while increasing the size of the\nsecret message. In this paper a method is proposed to embed a color secret\nimage into a color cover image. A 2-3-3 LSB insertion method has been used for\nimage steganography. Experimental results show an improvement in the Mean\nsquared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the\nproposed technique over the base technique of hash based 3-3-2 LSB insertion.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 11:16:18 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Manjula", "G. R.", ""], ["Danti", "Ajit", ""]]}, {"id": "1503.03920", "submitter": "Samar Alqhtani Mrs", "authors": "Samar M. Alqhtani, Suhuai Luo and Brian Regan", "title": "Fusing Text and Image for Event Detection in Twitter", "comments": "9 Pages, 4 figuers", "journal-ref": null, "doi": "10.5121/ijma.2015.7103", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we develop an accurate and effective event detection\nmethod to detect events from a Twitter stream, which uses visual and textual\ninformation to improve the performance of the mining process. The method\nmonitors a Twitter stream to pick up tweets having texts and images and stores\nthem into a database. This is followed by applying a mining algorithm to detect\nan event. The procedure starts with detecting events based on text only by\nusing the feature of the bag-of-words which is calculated using the term\nfrequency-inverse document frequency (TF-IDF) method. Then it detects the event\nbased on image only by using visual features including histogram of oriented\ngradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color\nhistogram. K nearest neighbours (Knn) classification is used in the detection.\nThe final decision of the event detection is made based on the reliabilities of\ntext only detection and image only detection. The experiment result showed that\nthe proposed method achieved high accuracy of 0.94, comparing with 0.89 with\ntexts only, and 0.86 with images only.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 01:11:08 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Alqhtani", "Samar M.", ""], ["Luo", "Suhuai", ""], ["Regan", "Brian", ""]]}, {"id": "1503.04250", "submitter": "Julia Bernd", "authors": "Julia Bernd, Damian Borth, Benjamin Elizalde, Gerald Friedland,\n  Heather Gallagher, Luke Gottlieb, Adam Janin, Sara Karabashlieva, Jocelyn\n  Takahashi, Jennifer Won", "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans", "comments": "47 pages; 3 figures; 25 tables. Also published as ICSI Technical\n  Report TR-15-001", "journal-ref": null, "doi": null, "report-no": "TR-15-001", "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The YLI Multimedia Event Detection corpus is a public-domain index of videos\nwith annotations and computed features, specialized for research in multimedia\nevent detection (MED), i.e., automatically identifying what's happening in a\nvideo by analyzing the audio and visual content. The videos indexed in the\nYLI-MED corpus are a subset of the larger YLI feature corpus, which is being\ndeveloped by the International Computer Science Institute and Lawrence\nLivermore National Laboratory based on the Yahoo Flickr Creative Commons 100\nMillion (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting\none of ten target events, or no target event, and are annotated for additional\nattributes like language spoken and whether the video has a musical score. The\nannotations also include degree of annotator agreement and average annotator\nconfidence scores for the event categorization of each video. Version 1.0 of\nYLI-MED includes 1823 \"positive\" videos that depict the target events and\n48,138 \"negative\" videos, as well as 177 supplementary videos that are similar\nto event videos but are not positive examples. Our goal in producing YLI-MED is\nto be as open about our data and procedures as possible. This report describes\nthe procedures used to collect the corpus; gives detailed descriptive\nstatistics about the corpus makeup (and how video attributes affected\nannotators' judgments); discusses possible biases in the corpus introduced by\nour procedural choices and compares it with the most similar existing dataset,\nTRECVID MED's HAVIC corpus; and gives an overview of our future plans for\nexpanding the annotation effort.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 23:36:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bernd", "Julia", ""], ["Borth", "Damian", ""], ["Elizalde", "Benjamin", ""], ["Friedland", "Gerald", ""], ["Gallagher", "Heather", ""], ["Gottlieb", "Luke", ""], ["Janin", "Adam", ""], ["Karabashlieva", "Sara", ""], ["Takahashi", "Jocelyn", ""], ["Won", "Jennifer", ""]]}, {"id": "1503.04263", "submitter": "Seung Hyun Jeon", "authors": "Seung Hyun Jeon, Sanghong An, Changwoo Yoon, Hyun-woo Lee, and Junkyun\n  Choi", "title": "User Centric Content Management System for Open IPTV Over SNS (ICTC2012)", "comments": "10 pages, 17 figures, An earlier version of this paper was awarded as\n  best paper at the IEEE International Conference on ICT Convergence (ICTC),\n  Jeju, Korea, October 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coupled schemes between service-oriented architecture (SOA) and Web 2.0 have\nrecently been researched. Web-based content providers and telecommunications\ncompany (Telecom) based Internet protocol television (IPTV) providers have\nstruggled against each other to accommodate more three-screen service\nsubscribers. Since the advent of Web 2.0, more abundant reproduced content can\nbe circulated. However, because according to increasing device's resolution and\ncontent formats IPTV providers transcode content in advance, network bandwidth,\nstorage and operation costs for content management systems (CMSs) are wasted.\nIn this paper, we present a user centric CMS for open IPTV, which integrates\nSOA and Web 2.0. Considering content popularity based on a Zipf-like\ndistribution to solve these problems, we analyze the performance between the\nuser centric CMS and the conventional Web syndication system for normalized\ncosts. Based on the user centric CMS, we implement a social Web TV with\ndevice-aware function, which can aggregate, transcode, and deploy content over\nsocial networking service (SNS) independently.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 03:56:25 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jeon", "Seung Hyun", ""], ["An", "Sanghong", ""], ["Yoon", "Changwoo", ""], ["Lee", "Hyun-woo", ""], ["Choi", "Junkyun", ""]]}, {"id": "1503.04624", "submitter": "Morgan Barbier", "authors": "Morgan Barbier, Jean-Marie Le Bars, Christophe Rosenberger", "title": "Image Watermaking With Biometric Data For Copyright Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the proof of ownership or legitimate usage of a\ndigital content, such as an image, in order to tackle the illegitimate copy.\nThe proposed scheme based on the combination of the watermark-ing and\ncancelable biometrics does not require a trusted third party, all the exchanges\nare between the provider and the customer. The use of cancelable biometrics\npermits to provide a privacy compliant proof of identity. We illustrate the\nrobustness of this method against intentional and unintentional attacks of the\nwatermarked content.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 12:31:45 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Barbier", "Morgan", ""], ["Bars", "Jean-Marie Le", ""], ["Rosenberger", "Christophe", ""]]}, {"id": "1503.04718", "submitter": "Haodong Li", "authors": "Haodong Li and Weiqi Luo and Xiaoqing Qiu and Jiwu Huang", "title": "Identification of Image Operations Based on Steganalytic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image forensics have attracted wide attention during the past decade. Though\nmany forensic methods have been proposed to identify image forgeries, most of\nthem are targeted ones, since their proposed features are highly dependent on\nthe image operation under investigation. The performance of the well-designed\nfeatures for detecting the targeted operation usually degrades significantly\nfor other operations. On the other hand, a wise attacker can perform\nanti-forensics to fool the existing forensic methods, making countering\nanti-forensics become an urgent need. In this paper, we try to find a universal\nfeature to detect various image processing and anti-forensic operations. Based\non our extensive experiments and analysis, we find that any image\nprocessing/anti-forensic operations would inevitably modify many image pixels.\nThis would change some inherent statistics within original images, which is\nsimilar to the case of steganography. Therefore, we model image\nprocessing/anti-forensic operations as steganography problems, and propose a\ndetection strategy by applying steganalytic features. With some advanced\nsteganalytic features, we are able to detect various image operations and\nfurther identify their types. In our experiments, we have tested several\nsteganalytic features on 11 different kinds of typical image processing\noperations and 4 kinds of anti-forensic operations. The experimental results\nhave shown that the proposed strategy significantly outperforms the existing\nforensic methods in both effectiveness and universality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 16:34:48 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 01:21:34 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Li", "Haodong", ""], ["Luo", "Weiqi", ""], ["Qiu", "Xiaoqing", ""], ["Huang", "Jiwu", ""]]}, {"id": "1503.04958", "submitter": "Valery Gorbachev", "authors": "V.N. Gorbachev, E.M. Kaynarova, I.K. Metelev, O.V. Pavlovskaya", "title": "The blind detection for palette image watermarking without changing the\n  color", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To hide a binary pattern in the palette image a steganographic scheme with\nblind detection is considered. The embedding algorithm uses the Lehmer code by\npalette color permutations for which the cover image palette is generally\nrequired. The found transformation between the palette and RGB images allows to\nextract the hidden data without any cover work.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 09:09:48 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Gorbachev", "V. N.", ""], ["Kaynarova", "E. M.", ""], ["Metelev", "I. K.", ""], ["Pavlovskaya", "O. V.", ""]]}, {"id": "1503.05528", "submitter": "Andres Almansa", "authors": "Alasdair Newson, Andr\\'es Almansa (LTCI), Matthieu Fradet, Yann\n  Gousseau, Patrick P\\'erez", "title": "Video Inpainting of Complex Scenes", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, Society for Industrial and\n  Applied Mathematics, 2014, 7 (4), pp.1993-2019", "doi": "10.1137/140954933", "report-no": null, "categories": "cs.CV cs.MM eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic video inpainting algorithm which relies on the\noptimisation of a global, patch-based functional. Our algorithm is able to deal\nwith a variety of challenging situations which naturally arise in video\ninpainting, such as the correct reconstruction of dynamic textures, multiple\nmoving objects and moving background. Furthermore, we achieve this in an order\nof magnitude less execution time with respect to the state-of-the-art. We are\nalso able to achieve good quality results on high definition videos. Finally,\nwe provide specific algorithmic details to make implementation of our algorithm\nas easy as possible. The resulting algorithm requires no segmentation or manual\ninput other than the definition of the inpainting mask, and can deal with a\nwider variety of situations than is handled by previous work. 1. Introduction.\nAdvanced image and video editing techniques are increasingly common in the\nimage processing and computer vision world, and are also starting to be used in\nmedia entertainment. One common and difficult task closely linked to the world\nof video editing is image and video \" inpainting \". Generally speaking, this is\nthe task of replacing the content of an image or video with some other content\nwhich is visually pleasing. This subject has been extensively studied in the\ncase of images, to such an extent that commercial image inpainting products\ndestined for the general public are available, such as Photoshop's \" Content\nAware fill \" [1]. However, while some impressive results have been obtained in\nthe case of videos, the subject has been studied far less extensively than\nimage inpainting. This relative lack of research can largely be attributed to\nhigh time complexity due to the added temporal dimension. Indeed, it has only\nvery recently become possible to produce good quality inpainting results on\nhigh definition videos, and this only in a semi-automatic manner. Nevertheless,\nhigh-quality video inpainting has many important and useful applications such\nas film restoration, professional post-production in cinema and video editing\nfor personal use. For this reason, we believe that an automatic, generic video\ninpainting algorithm would be extremely useful for both academic and\nprofessional communities.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:35:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 06:43:01 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Newson", "Alasdair", "", "LTCI"], ["Almansa", "Andr\u00e9s", "", "LTCI"], ["Fradet", "Matthieu", ""], ["Gousseau", "Yann", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1503.06959", "submitter": "Luca Baroffio", "authors": "Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi", "title": "Fast keypoint detection in video sequences", "comments": "submitted to IEEE International Conference on Image Processing 2015", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7471895", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of computer vision tasks exploit a succinct representation of the\nvisual content in the form of sets of local features. Given an input image,\nfeature extraction algorithms identify a set of keypoints and assign to each of\nthem a description vector, based on the characteristics of the visual content\nsurrounding the interest point. Several tasks might require local features to\nbe extracted from a video sequence, on a frame-by-frame basis. Although\ntemporal downsampling has been proven to be an effective solution for mobile\naugmented reality and visual search, high temporal resolution is a key\nrequirement for time-critical applications such as object tracking, event\nrecognition, pedestrian detection, surveillance. In recent years, more and more\ncomputationally efficient visual feature detectors and decriptors have been\nproposed. Nonetheless, such approaches are tailored to still images. In this\npaper we propose a fast keypoint detection algorithm for video sequences, that\nexploits the temporal coherence of the sequence of keypoints. According to the\nproposed method, each frame is preprocessed so as to identify the parts of the\ninput frame for which keypoint detection and description need to be performed.\nOur experiments show that it is possible to achieve a reduction in\ncomputational time of up to 40%, without significantly affecting the task\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 09:28:28 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Baroffio", "Luca", ""], ["Cesana", "Matteo", ""], ["Redondi", "Alessandro", ""], ["Tagliasacchi", "Marco", ""]]}, {"id": "1503.07551", "submitter": "Helio M. de Oliveira", "authors": "P. Carrion, H.M. de Oliveira and R.M. Campello de Souza", "title": "A Low-throughput Wavelet-based Steganography Audio Scheme", "comments": "2 pages, 1 figure, conference: 8th Brazilian Symposium on Information\n  and Computer System Security, 2008, Gramado, RS, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the preliminary of a novel scheme of steganography, and\nintroduces the idea of combining two secret keys in the operation. The first\nsecret key encrypts the text using a standard cryptographic scheme (e.g. IDEA,\nSAFER+, etc.) prior to the wavelet audio decomposition. The way in which the\ncipher text is embedded in the file requires another key, namely a stego-key,\nwhich is associated with features of the audio wavelet analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 03:15:25 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Carrion", "P.", ""], ["de Oliveira", "H. M.", ""], ["de Souza", "R. M. Campello", ""]]}, {"id": "1503.08248", "submitter": "Xirong Li", "authors": "Xirong Li and Tiberio Uricchio and Lamberto Ballan and Marco Bertini\n  and Cees G. M. Snoek and Alberto Del Bimbo", "title": "Socializing the Semantic Gap: A Comparative Survey on Image Tag\n  Assignment, Refinement and Retrieval", "comments": "to appear in ACM Computing Surveys", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, 14:1-14:39, June 2016", "doi": "10.1145/2906152", "report-no": null, "categories": "cs.IR cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where previous reviews on content-based image retrieval emphasize on what can\nbe seen in an image to bridge the semantic gap, this survey considers what\npeople tag about an image. A comprehensive treatise of three closely linked\nproblems, i.e., image tag assignment, refinement, and tag-based image retrieval\nis presented. While existing works vary in terms of their targeted tasks and\nmethodology, they rely on the key functionality of tag relevance, i.e.\nestimating the relevance of a specific tag with respect to the visual content\nof a given image and its social context. By analyzing what information a\nspecific method exploits to construct its tag relevance function and how such\ninformation is exploited, this paper introduces a taxonomy to structure the\ngrowing literature, understand the ingredients of the main works, clarify their\nconnections and difference, and recognize their merits and limitations. For a\nhead-to-head comparison between the state-of-the-art, a new experimental\nprotocol is presented, with training sets containing 10k, 100k and 1m images\nand an evaluation on three test sets, contributed by various research groups.\nEleven representative works are implemented and evaluated. Putting all this\ntogether, the survey aims to provide an overview of the past and foster\nprogress for the near future.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 00:10:16 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 05:33:21 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 05:45:31 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Li", "Xirong", ""], ["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Bertini", "Marco", ""], ["Snoek", "Cees G. M.", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1503.08726", "submitter": "Jitang Lee", "authors": "Chi-Heng Lin, De-Nian Yang, Ji-Tang Lee and Wanjiun Liao", "title": "Error-Resilient Multicasting for Multi-View 3D Videos in Wireless\n  Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1409.8352", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of naked-eye 3D mobile devices, mobile 3D video services\nare becoming increasingly important for video service providers, such as\nYoutube and Netflix, while multi-view 3D videos have the potential to inspire a\nvariety of innovative applications. However, enabling multi-view 3D video\nservices may overwhelm WiFi networks when every view of a video are\nmulticasted. In this paper, therefore, we propose to incorporate\ndepth-image-based rendering (DIBR), which allows each mobile client to\nsynthesize the desired view from nearby left and right views, in order to\neffectively reduce the bandwidth consumption. Moreover, when each client\nsuffers from packet losses, retransmissions incur additional bandwidth\nconsumption and excess delay, which in turn undermines the quality of\nexperience in video applications. To address the above issue, we first discover\nthe merit of view protection via DIBR for multi-view video multicast using a\nmathematical analysis and then design a new protocol, named Multi-View Group\nManagement Protocol (MVGMP), to support the dynamic join and leave of users and\nthe change of desired views. The simulation results demonstrate that our\nprotocol effectively reduces bandwidth consumption and increases the\nprobability for each client to successfully playback the desired views in a\nmulti-view 3D video.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 16:06:39 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 09:30:15 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2015 07:24:52 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2015 19:51:12 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Lin", "Chi-Heng", ""], ["Yang", "De-Nian", ""], ["Lee", "Ji-Tang", ""], ["Liao", "Wanjiun", ""]]}]