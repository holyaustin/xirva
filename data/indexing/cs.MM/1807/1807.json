[{"id": "1807.00681", "submitter": "Haiqiang Wang", "authors": "Haiqiang Wang, Xinfeng Zhang, Chao Yang and C.-C. Jay Kuo", "title": "Analysis and prediction of JND-based video quality model", "comments": "PCS 2018. arXiv admin note: text overlap with arXiv:1710.11090", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The just-noticeable-difference (JND) visual perception property has received\nmuch attention in characterizing human subjective viewing experience of\ncompressed video. In this work, we quantify the JND-based video quality\nassessment model using the satisfied user ratio (SUR) curve, and show that the\nSUR model can be greatly simplified since the JND points of multiple subjects\nfor the same content in the VideoSet can be well modeled by the normal\ndistribution. Then, we design an SUR prediction method with video quality\ndegradation features and masking features and use them to predict the first,\nsecond and the third JND points and their corresponding SUR curves. Finally, we\nverify the performance of the proposed SUR prediction method with different\nconfigurations on the VideoSet. The experimental results demonstrate that the\nproposed SUR prediction method achieves good performance in various resolutions\nwith the mean absolute error (MAE) of the SUR smaller than 0.05 on average.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 22:34:37 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Wang", "Haiqiang", ""], ["Zhang", "Xinfeng", ""], ["Yang", "Chao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1807.00920", "submitter": "Haiqiang Wang", "authors": "Haiqiang Wang, Xinfeng Zhang, Chao Yang and C.-C. Jay Kuo", "title": "A JND-based Video Quality Assessment Model and Its Application", "comments": "v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the Just-Noticeable-Difference (JND) criterion, a subjective video\nquality assessment (VQA) dataset, called the VideoSet, was constructed\nrecently. In this work, we propose a JND-based VQA model using a probabilistic\nframework to analyze and clean collected subjective test data. While most\ntraditional VQA models focus on content variability, our proposed VQA model\ntakes both subject and content variabilities into account. The model parameters\nused to describe subject and content variabilities are jointly optimized by\nsolving a maximum likelihood estimation (MLE) problem. As an application, the\nnew subjective VQA model is used to filter out unreliable video quality scores\ncollected in the VideoSet. Experiments are conducted to demonstrate the\neffectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 23:17:07 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Haiqiang", ""], ["Zhang", "Xinfeng", ""], ["Yang", "Chao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1807.01106", "submitter": "Rodrigo Martin", "authors": "Rodrigo Mart\\'in, Michael Weinmann, Matthias B. Hullin", "title": "A Study of Material Sonification in Touchscreen Devices", "comments": "9 pages", "journal-ref": "Proc. ACM ISS 2018, 305-310", "doi": "10.1145/3279778.3281455", "report-no": null, "categories": "cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even in the digital age, designers largely rely on physical material samples\nto illustrate their products, as existing visual representations fail to\nsufficiently reproduce the look and feel of real world materials. Here, we\ninvestigate the use of interactive material sonification as an additional\nsensory modality for communicating well-established material qualities like\nsoftness, pleasantness or value. We developed a custom application for\ntouchscreen devices that receives tactile input and translate it into material\nrubbing sound using granular synthesis. We used this system to perform a\npsychophysical study, in which the ability of the user to rate subjective\nmaterial qualities is evaluated, with the actual material samples serving as\nreference stimulus. Our experimental results indicate that the considered audio\ncues do not significantly contribute to the perception of material qualities\nbut are able to increase the level of immersion when interacting with digital\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:05:31 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 09:29:27 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 14:48:22 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mart\u00edn", "Rodrigo", ""], ["Weinmann", "Michael", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1807.01147", "submitter": "Vaneet Aggarwal", "authors": "Abubakr Alabbasi and Vaneet Aggarwal and Tian Lan and Yu Xiang and\n  Moo-Ryong Ra and Yih-Farn R. Chen", "title": "FastTrack: Minimizing Stalls for CDN-based Over-the-top Video Streaming\n  Systems", "comments": "18 pages. arXiv admin note: text overlap with arXiv:1806.09466,\n  arXiv:1703.08348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic for internet video streaming has been rapidly increasing and is\nfurther expected to increase with the higher definition videos and IoT\napplications, such as 360 degree videos and augmented virtual reality\napplications. While efficient management of heterogeneous cloud resources to\noptimize the quality of experience is important, existing work in this problem\nspace often left out important factors. In this paper, we present a model for\ndescribing a today's representative system architecture for video streaming\napplications, typically composed of a centralized origin server and several CDN\nsites. Our model comprehensively considers the following factors: limited\ncaching spaces at the CDN sites, allocation of CDN for a video request, choice\nof different ports from the CDN, and the central storage and bandwidth\nallocation. With the model, we focus on minimizing a performance metric, stall\nduration tail probability (SDTP), and present a novel, yet efficient, algorithm\nto solve the formulated optimization problem. The theoretical bounds with\nrespect to the SDTP metric are also analyzed and presented. Our extensive\nsimulation results demonstrate that the proposed algorithms can significantly\nimprove the SDTP metric, compared to the baseline strategies. Small-scale video\nstreaming system implementation in a real cloud environment further validates\nour results.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 10:24:28 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Alabbasi", "Abubakr", ""], ["Aggarwal", "Vaneet", ""], ["Lan", "Tian", ""], ["Xiang", "Yu", ""], ["Ra", "Moo-Ryong", ""], ["Chen", "Yih-Farn R.", ""]]}, {"id": "1807.02548", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura, Thomas Rarris, Deniz Gunduz, Ozgur Ercetin", "title": "Delay-Aware Coded Caching for Mobile Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM eess.IV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the trade-off between the cache capacity and the user\ndelay for a cooperative Small Base Station (SBS) coded caching system with\nmobile users. First, a delay-aware coded caching policy, which takes into\naccount the popularity of the files and the maximum re-buffering delay to\nminimize the average rebuffering delay of a mobile user under a given cache\ncapacity constraint is introduced. Subsequently, we address a scenario where\nsome files are served by the macro-cell base station (MBS) when the cache\ncapacity of the SBSs is not sufficient to store all the files in the library.\nFor this scenario, we develop a coded caching policy that minimizes the average\namount of data served by the MBS under an average re-buffering delay\nconstraint.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 19:06:47 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ozfatura", "Emre", ""], ["Rarris", "Thomas", ""], ["Gunduz", "Deniz", ""], ["Ercetin", "Ozgur", ""]]}, {"id": "1807.02895", "submitter": "Chengyuan Zhang", "authors": "Jun Long, Qunfeng Liu, Xinpan Yuan, Chengyuan Zhang, Junfeng Liu", "title": "A Filter of Minhash for Image Similarity Measures", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image similarity measures play an important role in nearest neighbor search\nand duplicate detection for large-scale image datasets. Recently, Minwise\nHashing (or Minhash) and its related hashing algorithms have achieved great\nperformances in large-scale image retrieval systems. However, there are a large\nnumber of comparisons for image pairs in these applications, which may spend a\nlot of computation time and affect the performance. In order to quickly obtain\nthe pairwise images that theirs similarities are higher than the specific\nthreshold T (e.g., 0.5), we propose a dynamic threshold filter of Minwise\nHashing for image similarity measures. It greatly reduces the calculation time\nby terminating the unnecessary comparisons in advance. We also find that the\nfilter can be extended to other hashing algorithms, on when the estimator\nsatisfies the binomial distribution, such as b-Bit Minwise Hashing, One\nPermutation Hashing, etc. In this pager, we use the Bag-of-Visual-Words (BoVW)\nmodel based on the Scale Invariant Feature Transform (SIFT) to represent the\nimage features. We have proved that the filter is correct and effective through\nthe experiment on real image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 23:27:12 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Long", "Jun", ""], ["Liu", "Qunfeng", ""], ["Yuan", "Xinpan", ""], ["Zhang", "Chengyuan", ""], ["Liu", "Junfeng", ""]]}, {"id": "1807.03046", "submitter": "Emilia Gomez", "authors": "Emilia G\\'omez and Merlijn Blaauw and Jordi Bonada and Pritish Chandna\n  and Helena Cuesta", "title": "Deep Learning for Singing Processing: Achievements, Challenges and\n  Impact on Singers and Listeners", "comments": "Keynote speech, 2018 Joint Workshop on Machine Learning for Music.\n  The Federated Artificial Intelligence Meeting (FAIM), a joint workshop\n  program of ICML, IJCAI/ECAI, and AAMAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes some recent advances on a set of tasks related to the\nprocessing of singing using state-of-the-art deep learning techniques. We\ndiscuss their achievements in terms of accuracy and sound quality, and the\ncurrent challenges, such as availability of data and computing resources. We\nalso discuss the impact that these advances do and will have on listeners and\nsingers when they are integrated in commercial applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 11:19:42 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["G\u00f3mez", "Emilia", ""], ["Blaauw", "Merlijn", ""], ["Bonada", "Jordi", ""], ["Chandna", "Pritish", ""], ["Cuesta", "Helena", ""]]}, {"id": "1807.03094", "submitter": "Di Hu", "authors": "Di Hu, Feiping Nie, Xuelong Li", "title": "Deep Multimodal Clustering for Unsupervised Audiovisual Learning", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seen birds twitter, the running cars accompany with noise, etc. These\nnaturally audiovisual correspondences provide the possibilities to explore and\nunderstand the outside world. However, the mixed multiple objects and sounds\nmake it intractable to perform efficient matching in the unconstrained\nenvironment. To settle this problem, we propose to adequately excavate audio\nand visual components and perform elaborate correspondence learning among them.\nConcretely, a novel unsupervised audiovisual learning model is proposed, named\nas \\Deep Multimodal Clustering (DMC), that synchronously performs sets of\nclustering with multimodal vectors of convolutional maps in different shared\nspaces for capturing multiple audiovisual correspondences. And such integrated\nmultimodal clustering network can be effectively trained with max-margin loss\nin the end-to-end fashion. Amounts of experiments in feature evaluation and\naudiovisual tasks are performed. The results demonstrate that DMC can learn\neffective unimodal representation, with which the classifier can even\noutperform human performance. Further, DMC shows noticeable performance in\nsound localization, multisource detection, and audiovisual understanding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:13:10 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 13:16:52 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 07:36:17 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hu", "Di", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1807.03773", "submitter": "Jinyao Xia Mrs", "authors": "J.Y. Xia, B.J. Xiao, Fei Yang and Dan Li", "title": "EAST Real-Time VOD System Based on MDSplus", "comments": "21st IEEE Real Time Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with EAST (Experimental Advanced Superconducting Tokamak) experimental\ndata analyzed by more and more collaborators, the experimental videos which\ndirectly reflect the real status of vacuum attract more and more researchers'\nattention. The real time VOD (Video On Demand) system based on MDSplus allows\nusers reading the video frames in real time as same as the signal data which is\nalso stored in the MDSplus database. User can display the plasma discharge\nvideos and analyze videos frame by frame through jScope or our VOD web station.\nThe system mainly includes the frames storing and frames displaying. The frames\nstoring application accepts shot information by using socket TCP communication\nfirstly, then reads video frames through disk mapping, finally stores them into\nMDSplus. The displaying process is implemented through B/S (Browser/Server)\nframework, it uses PHP and JavaScript to realize VOD function and read frames\ninformation from MDSplus. The system offers a unit way to access and backup\nexperimental data and video during the EAST experiment, which is of great\nbenefit to EAST experimenter than the formal VOD system in VOD function and\nreal time performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 02:06:22 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Xia", "J. Y.", ""], ["Xiao", "B. J.", ""], ["Yang", "Fei", ""], ["Li", "Dan", ""]]}, {"id": "1807.04465", "submitter": "Cheng Kang Hsieh", "authors": "Miguel Campo, Cheng-Kang Hsieh, Matt Nickens, JJ Espinoza, Abhinav\n  Taliyan, Julie Rieger, Jean Ho, Bettina Sherick", "title": "Competitive Analysis System for Theatrical Movie Releases Based on Movie\n  Trailer Deep Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audience discovery is an important activity at major movie studios. Deep\nmodels that use convolutional networks to extract frame-by-frame features of a\nmovie trailer and represent it in a form that is suitable for prediction are\nnow possible thanks to the availability of pre-built feature extractors trained\non large image datasets. Using these pre-built feature extractors, we are able\nto process hundreds of publicly available movie trailers, extract\nframe-by-frame low level features (e.g., a face, an object, etc) and create\nvideo-level representations. We use the video-level representations to train a\nhybrid Collaborative Filtering model that combines video features with\nhistorical movie attendance records. The trained model not only makes accurate\nattendance and audience prediction for existing movies, but also successfully\nprofiles new movies six to eight months prior to their release.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 08:24:56 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Campo", "Miguel", ""], ["Hsieh", "Cheng-Kang", ""], ["Nickens", "Matt", ""], ["Espinoza", "JJ", ""], ["Taliyan", "Abhinav", ""], ["Rieger", "Julie", ""], ["Ho", "Jean", ""], ["Sherick", "Bettina", ""]]}, {"id": "1807.05323", "submitter": "Bichuan Guo", "authors": "Bichuan Guo, Xinyao Chen, Jiawen Gu, Yuxing Han, Jiangtao Wen", "title": "A Bayesian Approach to Block Structure Inference in AV1-based Multi-rate\n  Video Encoding", "comments": "published in IEEE Data Compression Conference, 2018", "journal-ref": null, "doi": "10.1109/DCC.2018.00047", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to differences in frame structure, existing multi-rate video encoding\nalgorithms cannot be directly adapted to encoders utilizing special reference\nframes such as AV1 without introducing substantial rate-distortion loss. To\ntackle this problem, we propose a novel bayesian block structure inference\nmodel inspired by a modification to an HEVC-based algorithm. It estimates the\nposterior probabilistic distributions of block partitioning, and adapts early\nterminations in the RDO procedure accordingly. Experimental results show that\nthe proposed method provides flexibility for controlling the tradeoff between\nspeed and coding efficiency, and can achieve an average time saving of 36.1%\n(up to 50.6%) with negligible bitrate cost.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 02:32:29 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Guo", "Bichuan", ""], ["Chen", "Xinyao", ""], ["Gu", "Jiawen", ""], ["Han", "Yuxing", ""], ["Wen", "Jiangtao", ""]]}, {"id": "1807.05364", "submitter": "Bichuan Guo", "authors": "Bichuan Guo, Yuxing Han, Jiangtao Wen", "title": "Convex Optimization Based Bit Allocation for Light Field Compression\n  under Weighting and Consistency Constraints", "comments": "published in IEEE Data Compression Conference, 2018", "journal-ref": null, "doi": "10.1109/DCC.2018.00019", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with conventional image and video, light field images introduce the\nweight channel, as well as the visual consistency of rendered view, information\nthat has to be taken into account when compressing the pseudo-temporal-sequence\n(PTS) created from light field images. In this paper, we propose a novel frame\nlevel bit allocation framework for PTS coding. A joint model that measures\nweighted distortion and visual consistency, combined with an iterative encoding\nsystem, yields the optimal bit allocation for each frame by solving a convex\noptimization problem. Experimental results show that the proposed framework is\neffective in producing desired distortion distribution based on weights, and\nachieves up to 24.7% BD-rate reduction comparing to the default rate control\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 09:19:49 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Guo", "Bichuan", ""], ["Han", "Yuxing", ""], ["Wen", "Jiangtao", ""]]}, {"id": "1807.05365", "submitter": "Bichuan Guo", "authors": "Bichuan Guo, Yuxing Han, Jiangtao Wen", "title": "Fast Block Structure Determination in AV1-based Multiple Resolutions\n  Video Encoding", "comments": "published in IEEE International Conference on Multimedia and Expo,\n  2018", "journal-ref": null, "doi": "10.1109/ICME.2018.8486492", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely used adaptive HTTP streaming requires an efficient algorithm to\nencode the same video to different resolutions. In this paper, we propose a\nfast block structure determination algorithm based on the AV1 codec that\naccelerates high resolution encoding, which is the bottle-neck of multiple\nresolutions encoding. The block structure similarity across resolutions is\nmodeled by the fineness of frame detail and scale of object motions, this\nenables us to accelerate high resolution encoding based on low resolution\nencoding results. The average depth of a block's co-located neighborhood is\nused to decide early termination in the RDO process. Encoding results show that\nour proposed algorithm reduces encoding time by 30.1%-36.8%, while keeping\nBD-rate low at 0.71%-1.04%. Comparing to the state-of-the-art, our method\nhalves performance loss without sacrificing time savings.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 09:29:57 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Guo", "Bichuan", ""], ["Han", "Yuxing", ""], ["Wen", "Jiangtao", ""]]}, {"id": "1807.06160", "submitter": "Homanga Bharadhwaj", "authors": "Homanga Bharadhwaj", "title": "Layer-wise Relevance Propagation for Explainable Recommendations", "comments": "Accepted in Proceedings of the EARS Workshop at SIGIR 2018", "journal-ref": "Homanga Bharadhwaj. 2018. Layer-wise Relevance Propagation for\n  Explainable Recommendations. In Proceedings of SIGIR 2018 Workshop on\n  ExplainAble Recommendation and Search (EARS'18). ACM, New York, NY, USA", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of explanations in a deep-learning based\nmodel for recommendations by leveraging the technique of layer-wise relevance\npropagation. We use a Deep Convolutional Neural Network to extract relevant\nfeatures from the input images before identifying similarity between the images\nin feature space. Relationships between the images are identified by the model\nand layer-wise relevance propagation is used to infer pixel-level details of\nthe images that may have significantly informed the model's choice. We evaluate\nour method on an Amazon products dataset and demonstrate the efficacy of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 00:38:31 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Bharadhwaj", "Homanga", ""]]}, {"id": "1807.06196", "submitter": "Mireille Boutin", "authors": "Christian Tendyck and Andrew Haddad and Mireille Boutin", "title": "Photo-unrealistic Image Enhancement for Subject Placement in Outdoor\n  Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera display reflections are an issue in bright light situations, as they\nmay prevent users from correctly positioning the subject in the picture. We\npropose a software solution to this problem, which consists in modifying the\nimage in the viewer, in real time. In our solution, the user is seeing a\nposterized image which roughly represents the contour of the objects. Five\nenhancement methods are compared in a user study. Our results indicate that the\nproblem considered is a valid one, as users had problems locating landmarks\nnearly 37% of the time under sunny conditions, and that our proposed\nenhancement method using contrasting colors is a practical solution to that\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 03:24:14 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Tendyck", "Christian", ""], ["Haddad", "Andrew", ""], ["Boutin", "Mireille", ""]]}, {"id": "1807.06786", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Kyungyun Lee, Jiyoung Park, Jangyeon Park, Juhan Nam", "title": "Deep Content-User Embedding Model for Music Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning based recommendation systems have been actively\nexplored to solve the cold-start problem using a hybrid approach. However, the\nmajority of previous studies proposed a hybrid model where collaborative\nfiltering and content-based filtering modules are independently trained. The\nend-to-end approach that takes different modality data as input and jointly\ntrains the model can provide better optimization but it has not been fully\nexplored yet. In this work, we propose deep content-user embedding model, a\nsimple and intuitive architecture that combines the user-item interaction and\nmusic audio content. We evaluate the model on music recommendation and music\nauto-tagging tasks. The results show that the proposed model significantly\noutperforms the previous work. We also discuss various directions to improve\nthe proposed model further.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 06:13:15 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Lee", "Jongpil", ""], ["Lee", "Kyungyun", ""], ["Park", "Jiyoung", ""], ["Park", "Jangyeon", ""], ["Nam", "Juhan", ""]]}, {"id": "1807.07126", "submitter": "Nagabhushan Eswara", "authors": "Nagabhushan Eswara, S Ashique, Anand Panchbhai, Soumen Chakraborty,\n  Hemanth P. Sethuram, Kiran Kuchi, Abhinav Kumar, and Sumohana S. Channappayya", "title": "Streaming Video QoE Modeling and Prediction: A Long Short-Term Memory\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP based adaptive video streaming has become a popular choice of streaming\ndue to the reliable transmission and the flexibility offered to adapt to\nvarying network conditions. However, due to rate adaptation in adaptive\nstreaming, the quality of the videos at the client keeps varying with time\ndepending on the end-to-end network conditions. Further, varying network\nconditions can lead to the video client running out of playback content\nresulting in rebuffering events. These factors affect the user satisfaction and\ncause degradation of the user quality of experience (QoE). It is important to\nquantify the perceptual QoE of the streaming video users and monitor the same\nin a continuous manner so that the QoE degradation can be minimized. However,\nthe continuous evaluation of QoE is challenging as it is determined by complex\ndynamic interactions among the QoE influencing factors. Towards this end, we\npresent LSTM-QoE, a recurrent neural network based QoE prediction model using a\nLong Short-Term Memory (LSTM) network. The LSTM-QoE is a network of cascaded\nLSTM blocks to capture the nonlinearities and the complex temporal dependencies\ninvolved in the time varying QoE. Based on an evaluation over several publicly\navailable continuous QoE databases, we demonstrate that the LSTM-QoE has the\ncapability to model the QoE dynamics effectively. We compare the proposed model\nwith the state-of-the-art QoE prediction models and show that it provides\nsuperior performance across these databases. Further, we discuss the state\nspace perspective for the LSTM-QoE and show the efficacy of the state space\nmodeling approaches for QoE prediction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 20:06:19 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Eswara", "Nagabhushan", ""], ["Ashique", "S", ""], ["Panchbhai", "Anand", ""], ["Chakraborty", "Soumen", ""], ["Sethuram", "Hemanth P.", ""], ["Kuchi", "Kiran", ""], ["Kumar", "Abhinav", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "1807.07149", "submitter": "Mireille Boutin", "authors": "Albert Parra and Andrew W. Haddad and Mireille Boutin and Edward J.\n  Delp", "title": "A Hand-Held Multimedia Translation and Interpretation System with\n  Application to Diet Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network independent, hand-held system to translate and\ndisambiguate foreign restaurant menu items in real-time. The system is based on\nthe use of a portable multimedia device, such as a smartphones or a PDA. An\naccurate and fast translation is obtained using a Machine Translation engine\nand a context-specific corpora to which we apply two pre-processing steps,\ncalled translation standardization and $n$-gram consolidation. The phrase-table\ngenerated is orders of magnitude lighter than the ones commonly used in market\napplications, thus making translations computationally less expensive, and\ndecreasing the battery usage. Translation ambiguities are mitigated using\nmultimedia information including images of dishes and ingredients, along with\ningredient lists. We implemented a prototype of our system on an iPod Touch\nSecond Generation for English speakers traveling in Spain. Our tests indicate\nthat our translation method yields higher accuracy than translation engines\nsuch as Google Translate, and does so almost instantaneously. The memory\nrequirements of the application, including the database of images, are also\nwell within the limits of the device. By combining it with a database of\nnutritional information, our proposed system can be used to help individuals\nwho follow a medical diet maintain this diet while traveling.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 03:52:26 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Parra", "Albert", ""], ["Haddad", "Andrew W.", ""], ["Boutin", "Mireille", ""], ["Delp", "Edward J.", ""]]}, {"id": "1807.07203", "submitter": "Nakamasa Inoue", "authors": "Nakamasa Inoue, Koichi Shinoda", "title": "Few-Shot Adaptation for Multimedia Semantic Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a few-shot adaptation framework, which bridges zero-shot learning\nand supervised many-shot learning, for semantic indexing of image and video\ndata. Few-shot adaptation provides robust parameter estimation with few\ntraining examples, by optimizing the parameters of zero-shot learning and\nsupervised many-shot learning simultaneously. In this method, first we build a\nzero-shot detector, and then update it by using the few examples. Our\nexperiments show the effectiveness of the proposed framework on three datasets:\nTRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we\nshow that our method outperforms recent few-shot learning methods. On the\nTRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision\nunder the zero-shot condition and the supervised condition, respectively. To\nthe best of our knowledge, these are the best results on this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 00:58:33 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Inoue", "Nakamasa", ""], ["Shinoda", "Koichi", ""]]}, {"id": "1807.07278", "submitter": "Andreas Arzt", "authors": "Andreas Arzt and Stefan Lattner", "title": "Audio-to-Score Alignment using Transposition-invariant Features", "comments": "19th International Society for Music Information Retrieval\n  Conference, Paris, France, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-to-score alignment is an important pre-processing step for in-depth\nanalysis of classical music. In this paper, we apply novel\ntransposition-invariant audio features to this task. These low-dimensional\nfeatures represent local pitch intervals and are learned in an unsupervised\nfashion by a gated autoencoder. Our results show that the proposed features are\nindeed fully transposition-invariant and enable accurate alignments between\ntransposed scores and performances. Furthermore, they can even outperform\nwidely used features for audio-to-score alignment on `untransposed data', and\nthus are a viable and more flexible alternative to well-established features\nfor music alignment and matching.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 08:13:34 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Arzt", "Andreas", ""], ["Lattner", "Stefan", ""]]}, {"id": "1807.07617", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer and Alexis Ringot and Florian Taurer", "title": "SoniControl - A Mobile Ultrasonic Firewall", "comments": "To appear in proceedings of 2018 ACM Multimedia Conference October\n  22--26, 2018, Seoul, Republic of Korea", "journal-ref": null, "doi": "10.1145/3240508.3241393", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exchange of data between mobile devices in the near-ultrasonic frequency\nband is a new promising technology for near field communication (NFC) but also\nraises a number of privacy concerns. We present the first ultrasonic firewall\nthat reliably detects ultrasonic communication and provides the user with\neffective means to prevent hidden data exchange. This demonstration showcases a\nnew media-based communication technology (\"data over audio\") together with its\nrelated privacy concerns. It enables users to (i) interactively test out and\nexperience ultrasonic information exchange and (ii) shows how to protect\noneself against unwanted tracking.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 19:18:51 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Ringot", "Alexis", ""], ["Taurer", "Florian", ""]]}, {"id": "1807.07961", "submitter": "Jianbo Yuan", "authors": "Yuxiao Chen, Jianbo Yuan, Quanzeng You, Jiebo Luo", "title": "Twitter Sentiment Analysis via Bi-sense Emoji Embedding and\n  Attention-based LSTM", "comments": null, "journal-ref": null, "doi": "10.1145/3240508.3240533", "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis on large-scale social media data is important to bridge\nthe gaps between social media contents and real world activities including\npolitical election prediction, individual and public emotional status\nmonitoring and analysis, and so on. Although textual sentiment analysis has\nbeen well studied based on platforms such as Twitter and Instagram, analysis of\nthe role of extensive emoji uses in sentiment analysis remains light. In this\npaper, we propose a novel scheme for Twitter sentiment analysis with extra\nattention on emojis. We first learn bi-sense emoji embeddings under positive\nand negative sentimental tweets individually, and then train a sentiment\nclassifier by attending on these bi-sense emoji embeddings with an\nattention-based long short-term memory network (LSTM). Our experiments show\nthat the bi-sense embedding is effective for extracting sentiment-aware\nembeddings of emojis and outperforms the state-of-the-art models. We also\nvisualize the attentions to show that the bi-sense emoji embedding provides\nbetter guidance on the attention mechanism to obtain a more robust\nunderstanding of the semantics and sentiments.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 04:09:08 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 01:25:05 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Chen", "Yuxiao", ""], ["Yuan", "Jianbo", ""], ["You", "Quanzeng", ""], ["Luo", "Jiebo", ""]]}, {"id": "1807.08176", "submitter": "Bo Fu", "authors": "Bo Fu, Xiao-Yang Zhao, Yi Li, Xiang-Hai Wang and Yong-Gong Ren", "title": "A Convolutional Neural Networks Denoising Approach for Salt and Pepper\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The salt and pepper noise, especially the one with extremely high percentage\nof impulses, brings a significant challenge to image denoising. In this paper,\nwe propose a non-local switching filter convolutional neural network denoising\nalgorithm, named NLSF-CNN, for salt and pepper noise. As its name suggested,\nour NLSF-CNN consists of two steps, i.e., a NLSF processing step and a CNN\ntraining step. First, we develop a NLSF pre-processing step for noisy images\nusing non-local information. Then, the pre-processed images are divided into\npatches and used for CNN training, leading to a CNN denoising model for future\nnoisy images. We conduct a number of experiments to evaluate the effectiveness\nof NLSF-CNN. Experimental results show that NLSF-CNN outperforms the\nstate-of-the-art denoising algorithms with a few training images.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 16:49:25 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Fu", "Bo", ""], ["Zhao", "Xiao-Yang", ""], ["Li", "Yi", ""], ["Wang", "Xiang-Hai", ""], ["Ren", "Yong-Gong", ""]]}, {"id": "1807.08430", "submitter": "Kang Dang Mr", "authors": "Kang Dang, Chunluan Zhou, Zhigang Tu, Michael Hoy, Justin Dauwels,\n  Junsong Yuan", "title": "Actor-Action Semantic Segmentation with Region Masks", "comments": "Accepted by BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the actor-action semantic segmentation problem, which\nrequires joint labeling of both actor and action categories in video frames.\nOne major challenge for this task is that when an actor performs an action,\ndifferent body parts of the actor provide different types of cues for the\naction category and may receive inconsistent action labeling when they are\nlabeled independently. To address this issue, we propose an end-to-end\nregion-based actor-action segmentation approach which relies on region masks\nfrom an instance segmentation algorithm. Our main novelty is to avoid labeling\npixels in a region mask independently - instead we assign a single action label\nto these pixels to achieve consistent action labeling. When a pixel belongs to\nmultiple region masks, max pooling is applied to resolve labeling conflicts.\nOur approach uses a two-stream network as the front-end (which learns features\ncapturing both appearance and motion information), and uses two region-based\nsegmentation networks as the back-end (which takes the fused features from the\ntwo-stream network as the input and predicts actor-action labeling).\nExperiments on the A2D dataset demonstrate that both the region-based\nsegmentation strategy and the fused features from the two-stream network\ncontribute to the performance improvements. The proposed approach outperforms\nthe state-of-the-art results by more than 8% in mean class accuracy, and more\nthan 5% in mean class IOU, which validates its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 05:11:23 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Dang", "Kang", ""], ["Zhou", "Chunluan", ""], ["Tu", "Zhigang", ""], ["Hoy", "Michael", ""], ["Dauwels", "Justin", ""], ["Yuan", "Junsong", ""]]}, {"id": "1807.08571", "submitter": "Shiqi Dong", "authors": "Ru Zhang, Shiqi Dong, Jianyi Liu", "title": "Invisible Steganography via Generative Adversarial Networks", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there are plenty of works introducing convolutional neural networks\n(CNNs) to the steganalysis and exceeding conventional steganalysis algorithms.\nThese works have shown the improving potential of deep learning in information\nhiding domain. There are also several works based on deep learning to do image\nsteganography, but these works still have problems in capacity, invisibility\nand security. In this paper, we propose a novel CNN architecture named as\n\\isgan to conceal a secret gray image into a color cover image on the sender\nside and exactly extract the secret image out on the receiver side. There are\nthree contributions in our work: (i) we improve the invisibility by hiding the\nsecret image only in the Y channel of the cover image; (ii) We introduce the\ngenerative adversarial networks to strengthen the security by minimizing the\ndivergence between the empirical probability distributions of stego images and\nnatural images. (iii) In order to associate with the human visual system\nbetter, we construct a mixed loss function which is more appropriate for\nsteganography to generate more realistic stego images and reveal out more\nbetter secret images. Experiment results show that ISGAN can achieve\nstart-of-art performances on LFW, Pascal VOC2012 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 12:53:29 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 15:52:21 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 15:02:45 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Zhang", "Ru", ""], ["Dong", "Shiqi", ""], ["Liu", "Jianyi", ""]]}, {"id": "1807.09074", "submitter": "Donlaporn Srifar", "authors": "Donlaporn Srifar", "title": "360 virtual reality travel media for elderly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The objectives of this qualitative research were to study the model of\n360-degree virtual reality travel media, to compare appropriateness of moving\n360-degree virtual reality travel media for elderly with both still and moving\ncameras, and to study satisfaction of elderly in 360-degree virtual reality\ntravel media. The informants are 10 elders with age above and equal to 60 years\nold who live in Bangkok regardless of genders. Data were collected through\ndocuments, detailed interview, and non-participant observation of elders to\n360-degree virtual reality travel media with data triangulation. 1. From the\nliterature review 1. The creation must primarily consider the target consumers\non their physics 2. must have fluidity on changing the view of the camera by\ncalibrating with the target consumers 3. The image displayed must not move too\nfast to prevent dizziness and improve the comfort of the target consumers. It\nis also highly recommended to implement a function to customize the movement\nrate for the customer. 2. From the in-depth interview with the target\nconsumers, the results found that 1. They are worried and not used to the\nequipment 2. They have no idea where to look 3. They feel excited 5. They are\ninterested in what is more to see 6. They feel like they did actually travel\nthere 7. They can hear the sound clearly 8. They do not like when the camera is\nmoving and find still camera more comfortable. 3. From the non-participant\nobservation and found that they are always excited, laughed, and smiled when\nwatching the media. They always asked where this is and why they cannot see\nanything when turning around.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:51:25 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Srifar", "Donlaporn", ""]]}, {"id": "1807.09418", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli", "title": "Video Storytelling: Textual Summaries for Events", "comments": "Published in IEEE Transactions on Multimedia", "journal-ref": "J. Li, Y. Wong, Q. Zhao and M. S. Kankanhalli, \"Video\n  Storytelling: Textual Summaries for Events,\" in IEEE Transactions on\n  Multimedia, 2019", "doi": "10.1109/TMM.2019.2930041", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bridging vision and natural language is a longstanding goal in computer\nvision and multimedia research. While earlier works focus on generating a\nsingle-sentence description for visual content, recent works have studied\nparagraph generation. In this work, we introduce the problem of video\nstorytelling, which aims at generating coherent and succinct stories for long\nvideos. Video storytelling introduces new challenges, mainly due to the\ndiversity of the story and the length and complexity of the video. We propose\nnovel methods to address the challenges. First, we propose a context-aware\nframework for multimodal embedding learning, where we design a Residual\nBidirectional Recurrent Neural Network to leverage contextual information from\npast and future. Second, we propose a Narrator model to discover the underlying\nstoryline. The Narrator is formulated as a reinforcement learning agent which\nis trained by directly optimizing the textual metric of the generated story. We\nevaluate our method on the Video Story dataset, a new dataset that we have\ncollected to enable the study. We compare our method with multiple\nstate-of-the-art baselines, and show that our method achieves better\nperformance, in terms of quantitative measures and user study.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 02:43:19 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 10:21:46 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 12:39:48 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan S.", ""]]}, {"id": "1807.09560", "submitter": "Michele Svanera", "authors": "Michele Svanera, Mattia Savardi, Alberto Signoroni, Andr\\'as B\\'alint\n  Kov\\'acs, Sergio Benini", "title": "Who is the director of this movie? Automatic style recognition based on\n  shot features", "comments": null, "journal-ref": null, "doi": "10.1109/MMUL.2019.2940004", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how low-level formal features, such as shot duration, meant as length\nof camera takes, and shot scale, i.e. the distance between the camera and the\nsubject, are distinctive of a director's style in art movies. So far such\nfeatures were thought of not having enough varieties to become distinctive of\nan author. However our investigation on the full filmographies of six different\nauthors (Scorsese, Godard, Tarr, Fellini, Antonioni, and Bergman) for a total\nnumber of 120 movies analysed second by second, confirms that these\nshot-related features do not appear as random patterns in movies from the same\ndirector. For feature extraction we adopt methods based on both conventional\nand deep learning techniques. Our findings suggest that feature sequential\npatterns, i.e. how features evolve in time, are at least as important as the\nrelated feature distributions. To the best of our knowledge this is the first\nstudy dealing with automatic attribution of movie authorship, which opens up\ninteresting lines of cross-disciplinary research on the impact of style on the\naesthetic and emotional effects on the viewers.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 12:55:59 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Svanera", "Michele", ""], ["Savardi", "Mattia", ""], ["Signoroni", "Alberto", ""], ["Kov\u00e1cs", "Andr\u00e1s B\u00e1lint", ""], ["Benini", "Sergio", ""]]}, {"id": "1807.10204", "submitter": "Rafael Valle", "authors": "Rafael Valle", "title": "Visual Display and Retrieval of Music Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes computational methods for the visual display and\nanalysis of music information. We provide a concise description of software,\nmusic descriptors and data visualization techniques commonly used in music\ninformation retrieval. Finally, we provide use cases where the described\nsoftware, descriptors and visualizations are showcased.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 15:41:35 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Valle", "Rafael", ""]]}, {"id": "1807.10569", "submitter": "Gerald Friedland", "authors": "Gerald Friedland, Jingkang Wang, Ruoxi Jia, Bo Li", "title": "The Helmholtz Method: Using Perceptual Compression to Reduce Machine\n  Learning Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fundamental answer to a frequently asked question in\nmultimedia computing and machine learning: Do artifacts from perceptual\ncompression contribute to error in the machine learning process and if so, how\nmuch? Our approach to the problem is a reinterpretation of the Helmholtz Free\nEnergy formula from physics to explain the relationship between content and\nnoise when using sensors (such as cameras or microphones) to capture multimedia\ndata. The reinterpretation allows a bit-measurement of the noise contained in\nimages, audio, and video by combining a classifier with perceptual compression,\nsuch as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer's\nIDMT-SMT-Audio-Effects dataset indicate that, at the right quality level,\nperceptual compression is actually not harmful but contributes to a significant\nreduction of complexity of the machine learning process. That is, our noise\nquantification method can be used to speed up the training of deep learning\nclassifiers significantly while maintaining, or sometimes even improving,\noverall classification accuracy. Moreover, our results provide insights into\nthe reasons for the success of deep learning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 01:49:50 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Friedland", "Gerald", ""], ["Wang", "Jingkang", ""], ["Jia", "Ruoxi", ""], ["Li", "Bo", ""]]}, {"id": "1807.10894", "submitter": "Haiqiang Wang", "authors": "Haiqiang Wang, Ioannis Katsavounidis, Xinfeng Zhang, Chao Yang, C.-C.\n  Jay Kuo", "title": "A user model for JND-based video quality assessment: theory and\n  applications", "comments": "To appear at SPIE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video quality assessment (VQA) technology has attracted a lot of\nattention in recent years due to an increasing demand of video streaming\nservices. Existing VQA methods are designed to predict video quality in terms\nof the mean opinion score (MOS) calibrated by humans in subjective experiments.\nHowever, they cannot predict the satisfied user ratio (SUR) of an aggregated\nviewer group. Furthermore, they provide little guidance to video coding\nparameter selection, e.g. the Quantization Parameter (QP) of a set of\nconsecutive frames, in practical video streaming services. To overcome these\nshortcomings, the just-noticeable-difference (JND) based VQA methodology has\nbeen proposed as an alternative. It is observed experimentally that the JND\nlocation is a normally distributed random variable. In this work, we explain\nthis distribution by proposing a user model that takes both subject\nvariabilities and content variabilities into account. This model is built upon\nuser's capability to discern the quality difference between video clips encoded\nwith different QPs. Moreover, it analyzes video content characteristics to\naccount for inter-content variability. The proposed user model is validated on\nthe data collected in the VideoSet. It is demonstrated that the model is\nflexible to predict SUR distribution of a specific user group.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 05:32:29 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wang", "Haiqiang", ""], ["Katsavounidis", "Ioannis", ""], ["Zhang", "Xinfeng", ""], ["Yang", "Chao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1807.11014", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xinwei Sun, Zhiyong Yang, Xiaochun Cao,\n  Qingming Huang, and Yuan Yao", "title": "A Margin-based MLE for Crowdsourced Partial Ranking", "comments": "9 pages, Accepted by ACM Multimedia 2018 as a full paper", "journal-ref": null, "doi": "10.1145/3240508.3240597", "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A preference order or ranking aggregated from pairwise comparison data is\ncommonly understood as a strict total order. However, in real-world scenarios,\nsome items are intrinsically ambiguous in comparisons, which may very well be\nan inherent uncertainty of the data. In this case, the conventional total order\nranking can not capture such uncertainty with mere global ranking or utility\nscores. In this paper, we are specifically interested in the recent surge in\ncrowdsourcing applications to predict partial but more accurate (i.e., making\nless incorrect statements) orders rather than complete ones. To do so, we\npropose a novel framework to learn some probabilistic models of partial orders\nas a \\emph{margin-based Maximum Likelihood Estimate} (MLE) method. We prove\nthat the induced MLE is a joint convex optimization problem with respect to all\nthe parameters, including the global ranking scores and margin parameter.\nMoreover, three kinds of generalized linear models are studied, including the\nbasic uniform model, Bradley-Terry model, and Thurstone-Mosteller model,\nequipped with some theoretical analysis on FDR and Power control for the\nproposed methods. The validity of these models are supported by experiments\nwith both simulated and real-world datasets, which shows that the proposed\nmodels exhibit improvements compared with traditional state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 07:09:00 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Sun", "Xinwei", ""], ["Yang", "Zhiyong", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1807.11428", "submitter": "Feng Zhu", "authors": "Ru Zhang, Feng Zhu, Jianyi Liu and Gongshen Liu", "title": "Efficient feature learning and multi-size image steganalysis based on\n  CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For steganalysis, many studies showed that convolutional neural network has\nbetter performances than the two-part structure of traditional machine learning\nmethods. However, there are still two problems to be resolved: cutting down\nsignal to noise ratio of the steganalysis feature map and steganalyzing images\nof arbitrary size. Some algorithms required fixed size images as the input and\nhad low accuracy due to the underutilization of the noise residuals obtained by\nvarious types of filters. In this paper, we focus on designing an improved\nnetwork structure based on CNN to resolve the above problems. First, we use 3x3\nkernels instead of the traditional 5x5 kernels and optimize convolution kernels\nin the preprocessing layer. The smaller convolution kernels are used to reduce\nthe number of parameters and model the features in a small local region. Next,\nwe use separable convolutions to utilize channel correlation of the residuals,\ncompress the image content and increase the signal-to-noise ratio (between the\nstego signal and the image signal). Then, we use spatial pyramid pooling (SPP)\nto aggregate the local features, enhance the representation ability of\nfeatures, and steganalyze arbitrary size image. Finally, data augmentation is\nadopted to further improve network performance. The experimental results show\nthat the proposed CNN structure is significantly better than other four methods\nsuch as SRM, Ye-Net, Xu-Net, and Yedroudj-Net, when it is used to detect two\nspatial algorithms such as WOW and S-UNIWARAD with a wide variety of datasets\nand payloads.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 16:36:50 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Zhang", "Ru", ""], ["Zhu", "Feng", ""], ["Liu", "Jianyi", ""], ["Liu", "Gongshen", ""]]}]