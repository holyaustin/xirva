[{"id": "1608.00486", "submitter": "Conrad Sanderson", "authors": "ZongYuan Ge, Chris McCool, Conrad Sanderson, Peng Wang, Lingqiao Liu,\n  Ian Reid, Peter Corke", "title": "Exploiting Temporal Information for DCNN-based Fine-Grained Object\n  Classification", "comments": "International Conference on Digital Image Computing: Techniques and\n  Applications, 2016", "journal-ref": null, "doi": "10.1109/DICTA.2016.7797039", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification is a relatively new field that has concentrated\non using information from a single image, while ignoring the enormous potential\nof using video data to improve classification. In this work we present the\nnovel task of video-based fine-grained object classification, propose a\ncorresponding new video dataset, and perform a systematic study of several\nrecent deep convolutional neural network (DCNN) based approaches, which we\nspecifically adapt to the task. We evaluate three-dimensional DCNNs, two-stream\nDCNNs, and bilinear DCNNs. Two forms of the two-stream approach are used, where\nspatial and temporal data from two independent DCNNs are fused either via early\nfusion (combination of the fully-connected layers) and late fusion\n(concatenation of the softmax outputs of the DCNNs). For bilinear DCNNs,\ninformation from the convolutional layers of the spatial and temporal DCNNs is\ncombined via local co-occurrences. We then fuse the bilinear DCNN and early\nfusion of the two-stream approach to combine the spatial and temporal\ninformation at the local and global level (Spatio-Temporal Co-occurrence).\nUsing the new and challenging video dataset of birds, classification\nperformance is improved from 23.1% (using single images) to 41.1% when using\nthe Spatio-Temporal Co-occurrence system. Incorporating automatically detected\nbounding box location further improves the classification accuracy to 53.6%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 16:34:16 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 01:23:47 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 06:40:02 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Ge", "ZongYuan", ""], ["McCool", "Chris", ""], ["Sanderson", "Conrad", ""], ["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""], ["Corke", "Peter", ""]]}, {"id": "1608.00613", "submitter": "Roman Starosolski", "authors": "Roman Starosolski", "title": "Skipping Selected Steps of DWT Computation in Lossless JPEG 2000 for\n  Improved Bitrates", "comments": "Keywords: discrete wavelet transform, lifting technique, JPEG 2000,\n  reversible denoising and lifting step, lossless image compression, image\n  coding standards", "journal-ref": "PLOS ONE 11(12):e0168704 (2016)", "doi": "10.1371/journal.pone.0168704", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to improve bitrates of lossless JPEG 2000, we propose to modify the\ndiscrete wavelet transform (DWT) by skipping selected steps of its computation.\nWe employ a heuristic to construct the skipped steps DWT (SS-DWT) in an\nimage-adaptive way and define fixed SS-DWT variants. For a large and diverse\nset of images, we find that SS-DWT significantly improves bitrates of\nnon-photographic images. From a practical standpoint, the most interesting\nresults are obtained by applying entropy estimation of coding effects for\nselecting among the fixed SS-DWT variants. This way we get the compression\nscheme that, as opposed to the general SS-DWT case, is compliant with the JPEG\n2000 part 2 standard. It provides average bitrate improvement of roughly 5% for\nthe entire test-set, whereas the overall compression time becomes only 3%\ngreater than that of the unmodified JPEG 2000. Bitrates of photographic and\nnon-photographic images are improved by roughly 0.5% and 14%, respectively. At\na significantly increased cost of exploiting a heuristic, selecting the steps\nto be skipped based on the actual bitrate instead of an estimated one, and by\napplying reversible denoising and lifting steps to SS-DWT, we have attained\ngreater bitrate improvements of up to about 17.5% for non-photographic images.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 20:59:30 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 10:40:20 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Starosolski", "Roman", ""]]}, {"id": "1608.00905", "submitter": "Sonal Goel", "authors": "Sonal Goel, Niharika Sachdeva, Ponnurangam Kumaraguru, A V Subramanyam\n  and Divam Gupta", "title": "PicHunt: Social Media Image Retrieval for Improved Law Enforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  First responders are increasingly using social media to identify and reduce\ncrime for well-being and safety of the society. Images shared on social media\nhurting religious, political, communal and other sentiments of people, often\ninstigate violence and create law & order situations in society. This results\nin the need for first responders to inspect the spread of such images and users\npropagating them on social media. In this paper, we present a comparison\nbetween different hand-crafted features and a Convolutional Neural Network\n(CNN) model to retrieve similar images, which outperforms state-of-art\nhand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time\nimage search system, robust to retrieve modified images that allows first\nresponders to analyze the current spread of images, sentiments floating and\ndetails of users propagating such content. The system also aids officials to\nsave time of manually analyzing the content by reducing the search space on an\naverage by 67%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 17:09:19 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 11:16:36 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Goel", "Sonal", ""], ["Sachdeva", "Niharika", ""], ["Kumaraguru", "Ponnurangam", ""], ["Subramanyam", "A V", ""], ["Gupta", "Divam", ""]]}, {"id": "1608.00925", "submitter": "Francesco Renna", "authors": "Francesco Renna and Joseph Doyle and Vasileios Giotsas and Yiannis\n  Andreopoulos", "title": "Media Query Processing For The Internet-of-Things: Coupling Of Device\n  Energy Consumption And Cloud Infrastructure Billing", "comments": "15 pages, 6 figures, 4 tables. Accepted for publication in the IEEE\n  Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio/visual recognition and retrieval applications have recently garnered\nsignificant attention within Internet-of-Things (IoT) oriented services, given\nthat video cameras and audio processing chipsets are now ubiquitous even in\nlow-end embedded systems. In the most typical scenario for such services, each\ndevice extracts audio/visual features and compacts them into feature\ndescriptors, which comprise media queries. These queries are uploaded to a\nremote cloud computing service that performs content matching for\nclassification or retrieval applications. Two of the most crucial aspects for\nsuch services are: (i) controlling the device energy consumption when using the\nservice; (ii) reducing the billing cost incurred from the cloud infrastructure\nprovider. In this paper we derive analytic conditions for the optimal coupling\nbetween the device energy consumption and the incurred cloud infrastructure\nbilling. Our framework encapsulates: the energy consumption to produce and\ntransmit audio/visual queries, the billing rates of the cloud infrastructure,\nthe number of devices concurrently connected to the same cloud server, {the\nquery volume constraint of each cluster of devices,} and the statistics of the\nquery data production volume per device. Our analytic results are validated via\na deployment with: (i) the device side comprising compact image descriptors\n(queries) computed on Beaglebone Linux embedded platforms and transmitted to\nAmazon Web Services (AWS) Simple Storage Service; (ii) the cloud side carrying\nout image similarity detection via AWS Elastic Compute Cloud (EC2) instances,\nwith the AWS Auto Scaling being used to control the number of instances\naccording to the demand.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 18:40:03 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Renna", "Francesco", ""], ["Doyle", "Joseph", ""], ["Giotsas", "Vasileios", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1608.01947", "submitter": "Jean-Marc Valin", "authors": "Jean-Marc Valin, Timothy B. Terriberry, Nathan E. Egge, Thomas Daede,\n  Yushin Cho, Christopher Montgomery, Michael Bebenita", "title": "Daala: Building A Next-Generation Video Codec From Unconventional\n  Technology", "comments": "6 pages, accepted for multimedia signal processing (MMSP) workshop,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daala is a new royalty-free video codec that attempts to compete with\nstate-of-the-art royalty-bearing codecs. To do so, it must achieve good\ncompression while avoiding all of their patented techniques. We use technology\nthat is as different as possible from traditional approaches to achieve this.\nThis paper describes the technology behind Daala and discusses where it fits in\nthe newly created AV1 codec from the Alliance for Open Media. We show that\nDaala is approaching the performance level of more mature, state-of-the art\nvideo codecs and can contribute to improving AV1.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 17:36:51 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Valin", "Jean-Marc", ""], ["Terriberry", "Timothy B.", ""], ["Egge", "Nathan E.", ""], ["Daede", "Thomas", ""], ["Cho", "Yushin", ""], ["Montgomery", "Christopher", ""], ["Bebenita", "Michael", ""]]}, {"id": "1608.02289", "submitter": "Rossano Schifanella", "authors": "Rossano Schifanella, Paloma de Juan, Joel Tetreault, Liangliang Cao", "title": "Detecting Sarcasm in Multimodal Social Platforms", "comments": "10 pages, 3 figures, final version published in the Proceedings of\n  ACM Multimedia 2016", "journal-ref": null, "doi": "10.1145/2964284.2964321", "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm is a peculiar form of sentiment expression, where the surface\nsentiment differs from the implied sentiment. The detection of sarcasm in\nsocial media platforms has been applied in the past mainly to textual\nutterances where lexical indicators (such as interjections and intensifiers),\nlinguistic markers, and contextual information (such as user profiles, or past\nconversations) were used to detect the sarcastic tone. However, modern social\nmedia platforms allow to create multimodal messages where audiovisual content\nis integrated with the text, making the analysis of a mode in isolation\npartial. In our work, we first study the relationship between the textual and\nvisual aspects in multimodal posts from three major social media platforms,\ni.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to\nquantify the extent to which images are perceived as necessary by human\nannotators. Moreover, we propose two different computational frameworks to\ndetect sarcasm that integrate the textual and visual modalities. The first\napproach exploits visual semantics trained on an external dataset, and\nconcatenates the semantics features with state-of-the-art textual features. The\nsecond method adapts a visual neural network initialized with parameters\ntrained on ImageNet to multimodal sarcastic posts. Results show the positive\neffect of combining modalities for the detection of sarcasm across platforms\nand methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 00:59:03 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Schifanella", "Rossano", ""], ["de Juan", "Paloma", ""], ["Tetreault", "Joel", ""], ["Cao", "Liangliang", ""]]}, {"id": "1608.02291", "submitter": "Guillermo Morales-Luna", "authors": "Aleksey Zhuvikin and Valery Korzhik and Guillermo Morales-Luna", "title": "Semi-Fragile Image Authentication based on CFD and 3-Bit Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great adventure of watermarking usage in the context of\nconventional authentication since it does not require additional storage space\nfor supplementary metadata. However JPEG compression, being a conventional\nmethod to compress images, leads to exact authentication breaking. We discuss a\nsemi-fragile watermarking system for digital images tolerant to JPEG/JPEG2000\ncompression. Recently we have published a selective authentication method based\non Zernike moments. But unfortunately it has large computational complexity and\nnot sufficiently good detection of small image modifications. In the current\npaper it is proposed (in contrast to Zernike moments approach) the usage of\nimage finite differences and 3-bit quantization as the main technique. In order\nto embed a watermark (WM) into the image, some areas of the Haar wavelet\ntransform coefficients are used. Simulation results show a good resistance of\nthis method to JPEG compression with $\\mbox{\\rm CR}\\leq 30\\%$ (Compression\nRatio), high probability of small image modification recognition, image quality\nassessments $\\mbox{\\rm PSNR}\\geq 40$ (Peak signal-to-noise ratio) dB and\n$\\mbox{\\rm SSIM}\\geq 0.98$ (Structural Similarity Index Measure) after\nembedding and lower computation complexity of WM embedding and extraction. All\nthese properties qualify this approach as effective.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 01:10:11 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Zhuvikin", "Aleksey", ""], ["Korzhik", "Valery", ""], ["Morales-Luna", "Guillermo", ""]]}, {"id": "1608.02988", "submitter": "Krzysztof Szczypiorski", "authors": "Krzysztof Szczypiorski", "title": "StegIbiza: New Method for Information Hiding in Club Music", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new method for information hiding in club music is\nintroduced. The method called StegIbiza is based on using the music tempo as a\ncarrier. The tempo is modulated by hidden messages with a 3-value coding\nscheme, which is an adoption of Morse code for StegIbiza. The evaluation of the\nsystem was performed for several music samples (with and without StegIbiza\nenabled) on a selected group of testers who had a music background. Finally,\nfor the worst case scenario, none of them could identify any differences in the\naudio with a 1% margin of changed tempo.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:04:30 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Szczypiorski", "Krzysztof", ""]]}, {"id": "1608.03016", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, LiangLiang Cao, Jiang Zhu, Jiebo Luo", "title": "Mining Fashion Outfit Composition Using An End-to-End Deep Learning\n  Approach on Set Data", "comments": "IEEE TMM", "journal-ref": null, "doi": "10.1109/TMM.2017.2690144", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composing fashion outfits involves deep understanding of fashion standards\nwhile incorporating creativity for choosing multiple fashion items (e.g.,\nJewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality\nfashion outfits are usually designed by fashion experts and followed by large\naudiences. In this paper, we propose a machine learning system to compose\nfashion outfits automatically. The core of the proposed automatic composition\nsystem is to score fashion outfit candidates based on the appearances and\nmeta-data. We propose to leverage outfit popularity on fashion oriented\nwebsites to supervise the scoring component. The scoring component is a\nmulti-modal multi-instance deep learning system that evaluates instance\naesthetics and set compatibility simultaneously. In order to train and evaluate\nthe proposed composition system, we have collected a large scale fashion outfit\ndataset with 195K outfits and 368K fashion items from Polyvore. Although the\nfashion outfit scoring and composition is rather challenging, we have achieved\nan AUC of 85% for the scoring component, and an accuracy of 77% for a\nconstrained composition task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:11:32 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 05:26:23 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Li", "Yuncheng", ""], ["Cao", "LiangLiang", ""], ["Zhu", "Jiang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1608.03462", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan and Ozgur Yilmaz", "title": "Multi-View Product Image Search Using Deep ConvNets Representations", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view product image queries can improve retrieval performance over\nsingle view queries significantly. In this paper, we investigated the\nperformance of deep convolutional neural networks (ConvNets) on multi-view\nproduct image search. First, we trained a VGG-like network to learn deep\nConvNets representations of product images. Then, we computed the deep ConvNets\nrepresentations of database and query images and performed single view queries,\nand multi-view queries using several early and late fusion approaches.\n  We performed extensive experiments on the publicly available Multi-View\nObject Image Dataset (MVOD 5K) with both clean background queries from the\nInternet and cluttered background queries from a mobile phone. We compared the\nperformance of ConvNets to the classical bag-of-visual-words (BoWs). We\nconcluded that (1) multi-view queries with deep ConvNets representations\nperform significantly better than single view queries, (2) ConvNets perform\nmuch better than BoWs and have room for further improvement, (3) pre-training\nof ConvNets on a different image dataset with background clutter is needed to\nobtain good performance on cluttered product image queries obtained with a\nmobile phone.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 13:50:07 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 08:08:28 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Bastan", "Muhammet", ""], ["Yilmaz", "Ozgur", ""]]}, {"id": "1608.04267", "submitter": "Zihan Zhou", "authors": "Zihan Zhou, Farshid Farhat, James Z. Wang", "title": "Detecting Dominant Vanishing Points in Natural Scenes with Application\n  to Composition-Sensitive Image Retrieval", "comments": "15 pages, 18 figures, to appear in IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear perspective is widely used in landscape photography to create the\nimpression of depth on a 2D photo. Automated understanding of linear\nperspective in landscape photography has several real-world applications,\nincluding aesthetics assessment, image retrieval, and on-site feedback for\nphoto composition, yet adequate automated understanding has been elusive. We\naddress this problem by detecting the dominant vanishing point and the\nassociated line structures in a photo. However, natural landscape scenes pose\ngreat technical challenges because often the inadequate number of strong edges\nconverging to the dominant vanishing point is inadequate. To overcome this\ndifficulty, we propose a novel vanishing point detection method that exploits\nglobal structures in the scene via contour detection. We show that our method\nsignificantly outperforms state-of-the-art methods on a public ground truth\nlandscape image dataset that we have created. Based on the detection results,\nwe further demonstrate how our approach to linear perspective understanding\nprovides on-site guidance to amateur photographers on their work through a\nnovel viewpoint-specific image retrieval system.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 13:48:22 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 14:58:05 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhou", "Zihan", ""], ["Farhat", "Farshid", ""], ["Wang", "James Z.", ""]]}, {"id": "1608.04868", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi and George Fazekas and Brian McFee and Kyunghyun Cho and\n  Mark Sandler", "title": "Towards Music Captioning: Generating Music Playlist Descriptions", "comments": "2 pages, ISMIR 2016 Late-breaking/session extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Descriptions are often provided along with recommendations to help users'\ndiscovery. Recommending automatically generated music playlists (e.g.\npersonalised playlists) introduces the problem of generating descriptions. In\nthis paper, we propose a method for generating music playlist descriptions,\nwhich is called as music captioning. In the proposed method, audio content\nanalysis and natural language processing are adopted to utilise the information\nof each track.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 06:24:46 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 05:23:51 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["McFee", "Brian", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1608.04902", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Jiarui Sun, Siwei Ma, Zhouchen Lin, Jian Zhang, Shiqi\n  Wang, Wen Gao", "title": "Globally Variance-Constrained Sparse Representation and Its Application\n  in Image Set Coding", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 27 (2018) 3753-3765", "doi": "10.1109/TIP.2018.2823546", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation leads to an efficient way to approximately recover a\nsignal by the linear composition of a few bases from a learnt dictionary, based\non which various successful applications have been achieved. However, in the\nscenario of data compression, its efficiency and popularity are hindered. It is\nbecause of the fact that encoding sparsely distributed coefficients may consume\nmore bits for representing the index of nonzero coefficients. Therefore,\nintroducing an accurate rate-constraint in sparse coding and dictionary\nlearning becomes meaningful, which has not been fully exploited in the context\nof sparse representation. According to the Shannon entropy inequality, the\nvariance of a Gaussian distributed data bounds its entropy, indicating the\nactual bitrate can be well estimated by its variance. Hence, a Globally\nVariance-Constrained Sparse Representation (GVCSR) model is proposed in this\nwork, where a variance-constrained rate term is introduced to the optimization\nprocess. Specifically, we employ the Alternating Direction Method of\nMultipliers (ADMM) to solve the non-convex optimization problem for sparse\ncoding and dictionary learning, both of them have shown the state-of-the-art\nrate-distortion performance for image representation. Furthermore, we\ninvestigate the potential of applying the GVCSR algorithm in the practical\nimage set compression, where the optimized dictionary is trained to efficiently\nrepresent the images captured in similar scenarios by implicitly utilizing\ninter-image correlations. Experimental results have demonstrated superior\nrate-distortion performance against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 09:34:51 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 03:23:28 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhang", "Xiang", ""], ["Sun", "Jiarui", ""], ["Ma", "Siwei", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Jian", ""], ["Wang", "Shiqi", ""], ["Gao", "Wen", ""]]}, {"id": "1608.05001", "submitter": "Fei Hu", "authors": "Fei Hu, Changjiu Pu, Haowei Gao, Mengzi Tang and Li Li", "title": "An image compression and encryption scheme based on deep learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for\nunsupervised learning. Which has multi layers that project the vector\nrepresentation of input data into a lower vector space. These projection\nvectors are dense representations of the input data. As a result, SAE can be\nused for image compression. Using chaotic logistic map, the compression ones\ncan further be encrypted. In this study, an application of image compression\nand encryption is suggested using SAE and chaotic logistic map. Experiments\nshow that this application is feasible and effective. It can be used for image\ntransmission and image protection on internet simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:51:25 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 02:27:20 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Hu", "Fei", ""], ["Pu", "Changjiu", ""], ["Gao", "Haowei", ""], ["Tang", "Mengzi", ""], ["Li", "Li", ""]]}, {"id": "1608.05054", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan and Hilal Kandemir and Busra Canturk", "title": "MT3S: Mobile Turkish Scene Text-to-Speech System for the Visually\n  Impaired", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading text is one of the essential needs of the visually impaired people.\nWe developed a mobile system that can read Turkish scene and book text, using a\nfast gradient-based multi-scale text detection algorithm for real-time\noperation and Tesseract OCR engine for character recognition. We evaluated the\nOCR accuracy and running time of our system on a new, publicly available mobile\nTurkish scene text dataset we constructed and also compared with\nstate-of-the-art systems. Our system proved to be much faster, able to run on a\nmobile device, with OCR accuracy comparable to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 19:24:23 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Bastan", "Muhammet", ""], ["Kandemir", "Hilal", ""], ["Canturk", "Busra", ""]]}, {"id": "1608.05850", "submitter": "Christophe Guyeux", "authors": "Yousra A. Fadil and Jean-Fran\\c{c}ois Couchot and Rapha\\\"el Couturier\n  and Christophe Guyeux", "title": "Steganalyzer performances in operational contexts", "comments": "Proceedings of IIH-MSP 2015, The Eleventh International Conference on\n  Intelligent Information Hiding and Multimedia Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography and steganalysis are two important branches of the information\nhiding field of research. Steganography methods consist in hiding information\nin such a way that the secret message is undetectable for the uninitiated.\nSteganalyzis encompasses all the techniques that attempt to detect the presence\nof such hidden information. This latter is usually designed by making\nclassifiers able to separate innocent images from steganographied ones\naccording to their differences on well-selected features. We wonder, in this\narticle whether it is possible to construct a kind of universal steganalyzer\nwithout any knowledge regarding the steganographier side. The effects on the\nclassification score of a modification of either parameters or methods between\nthe learning and testing stages are then evaluated, while the possibility to\nimprove the separation score by merging many methods during learning stage is\ndeeper investigated.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 17:26:15 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Fadil", "Yousra A.", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Couturier", "Rapha\u00ebl", ""], ["Guyeux", "Christophe", ""]]}, {"id": "1608.06690", "submitter": "Dong Liu", "authors": "Yuanying Dai, Dong Liu, Feng Wu", "title": "A Convolutional Neural Network Approach for Post-Processing in HEVC\n  Intra Coding", "comments": "MMM 2017", "journal-ref": null, "doi": "10.1007/978-3-319-51811-4_3", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image and video compression algorithms yield visually annoying\nartifacts including blocking, blurring, and ringing, especially at low\nbit-rates. To reduce these artifacts, post-processing techniques have been\nextensively studied. Recently, inspired by the great success of convolutional\nneural network (CNN) in computer vision, some researches were performed on\nadopting CNN in post-processing, mostly for JPEG compressed images. In this\npaper, we present a CNN-based post-processing algorithm for High Efficiency\nVideo Coding (HEVC), the state-of-the-art video coding standard. We redesign a\nVariable-filter-size Residue-learning CNN (VRCNN) to improve the performance\nand to accelerate network training. Experimental results show that using our\nVRCNN as post-processing leads to on average 4.6% bit-rate reduction compared\nto HEVC baseline. The VRCNN outperforms previously studied networks in\nachieving higher bit-rate reduction, lower memory cost, and multiplied\ncomputational speedup.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 02:15:06 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 11:23:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Dai", "Yuanying", ""], ["Liu", "Dong", ""], ["Wu", "Feng", ""]]}, {"id": "1608.06770", "submitter": "Emanuele Sansone", "authors": "E. Sansone, K. Apostolidis, N. Conci, G. Boato, V. Mezaris, F.G.B. De\n  Natale", "title": "Automatic Synchronization of Multi-User Photo Galleries", "comments": "ACCEPTED to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the issue of photo galleries synchronization, where\npictures related to the same event are collected by different users. Existing\nsolutions to address the problem are usually based on unrealistic assumptions,\nlike time consistency across photo galleries, and often heavily rely on\nheuristics, limiting therefore the applicability to real-world scenarios. We\npropose a solution that achieves better generalization performance for the\nsynchronization task compared to the available literature. The method is\ncharacterized by three stages: at first, deep convolutional neural network\nfeatures are used to assess the visual similarity among the photos; then, pairs\nof similar photos are detected across different galleries and used to construct\na graph; eventually, a probabilistic graphical model is used to estimate the\ntemporal offset of each pair of galleries, by traversing the minimum spanning\ntree extracted from this graph. The experimental evaluation is conducted on\nfour publicly available datasets covering different types of events,\ndemonstrating the strength of our proposed method. A thorough discussion of the\nobtained results is provided for a critical assessment of the quality in\nsynchronization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 10:17:16 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 11:19:49 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Sansone", "E.", ""], ["Apostolidis", "K.", ""], ["Conci", "N.", ""], ["Boato", "G.", ""], ["Mezaris", "V.", ""], ["De Natale", "F. G. B.", ""]]}, {"id": "1608.07068", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun", "title": "Title Generation for User Generated Videos", "comments": "14 pages, 4 figures, ECCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great video title describes the most salient event compactly and captures\nthe viewer's attention. In contrast, video captioning tends to generate\nsentences that describe the video as a whole. Although generating a video title\nautomatically is a very useful task, it is much less addressed than video\ncaptioning. We address video title generation for the first time by proposing\ntwo methods that extend state-of-the-art video captioners to this new task.\nFirst, we make video captioners highlight sensitive by priming them with a\nhighlight detector. Our framework allows for jointly training a model for title\ngeneration and video highlight localization. Second, we induce high sentence\ndiversity in video captioners, so that the generated titles are also diverse\nand catchy. This means that a large number of sentences might be required to\nlearn the sentence structure of titles. Hence, we propose a novel sentence\naugmentation method to train a captioner with additional sentence-only examples\nthat come without corresponding videos. We collected a large-scale Video Titles\nin the Wild (VTW) dataset of 18100 automatically crawled user-generated videos\nand titles. On VTW, our methods consistently improve title prediction accuracy,\nand achieve the best performance in both automatic and human evaluation.\nFinally, our sentence augmentation method also outperforms the baselines on the\nM-VAD dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 09:49:23 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 17:36:13 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chen", "Tseng-Hung", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1608.07337", "submitter": "Wojciech Mazurczyk", "authors": "Wojciech Mazurczyk, Maciej Karas, Krzysztof Szczypiorski, Artur\n  Janicki", "title": "YouSkyde: Information Hiding for Skype Video Traffic", "comments": "16 pages, 10 figures", "journal-ref": "Multimedia Tools and Applications, 2015", "doi": "10.1007/s11042-015-2740-0", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new information hiding method for Skype videoconference calls\n- YouSkyde - is introduced. A Skype traffic analysis revealed that introducing\nintentional losses into the Skype video traffic stream to provide the means for\nclandestine communication is the most favourable solution. A YouSkyde\nproof-of-concept implementation was carried out and its experimental evaluation\nis presented. The results obtained prove that the proposed method is feasible\nand offer a steganographic bandwidth as high as 0.93 kbps, while introducing\nnegligible distortions into transmission quality and providing high\nundetectability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 23:28:33 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Mazurczyk", "Wojciech", ""], ["Karas", "Maciej", ""], ["Szczypiorski", "Krzysztof", ""], ["Janicki", "Artur", ""]]}, {"id": "1608.07373", "submitter": "Jen-Yu Liu", "authors": "Jen-Yu Liu, Shyh-Kang Jeng, Yi-Hsuan Yang", "title": "Applying Topological Persistence in Convolutional Neural Network for\n  Music Audio Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an increased interest in the application of\npersistent homology, a topological tool for data analysis, to machine learning\nproblems. Persistent homology is known for its ability to numerically\ncharacterize the shapes of spaces induced by features or functions. On the\nother hand, deep neural networks have been shown effective in various tasks. To\nour best knowledge, however, existing neural network models seldom exploit\nshape information. In this paper, we investigate a way to use persistent\nhomology in the framework of deep neural networks. Specifically, we propose to\nembed the so-called \"persistence landscape,\" a rather new topological summary\nfor data, into a convolutional neural network (CNN) for dealing with audio\nsignals. Our evaluation on automatic music tagging, a multi-label\nclassification task, shows that the resulting persistent convolutional neural\nnetwork (PCNN) model can perform significantly better than state-of-the-art\nmodels in prediction accuracy. We also discuss the intuition behind the design\nof the proposed model, and offer insights into the features that it learns.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 07:14:37 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Liu", "Jen-Yu", ""], ["Jeng", "Shyh-Kang", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1608.07876", "submitter": "Hirokatsu Kataoka", "authors": "Yun He, Soma Shirakabe, Yutaka Satoh, Hirokatsu Kataoka", "title": "Human Action Recognition without Human", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to evaluate \"human action recognition without\nhuman\". Motion representation is frequently discussed in human action\nrecognition. We have examined several sophisticated options, such as dense\ntrajectories (DT) and the two-stream convolutional neural network (CNN).\nHowever, some features from the background could be too strong, as shown in\nsome recent studies on human action recognition. Therefore, we considered\nwhether a background sequence alone can classify human actions in current\nlarge-scale action datasets (e.g., UCF101).\n  In this paper, we propose a novel concept for human action analysis that is\nnamed \"human action recognition without human\". An experiment clearly shows the\neffect of a background sequence for understanding an action label.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:22:38 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["He", "Yun", ""], ["Shirakabe", "Soma", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1608.08469", "submitter": "Xiaoqi Yin", "authors": "Xiaoqi Yin, Mihovil Bartulovi\\'c, Vyas Sekar, Bruno Sinopoli", "title": "On the Efficiency and Fairness of Multiplayer HTTP-based Adaptive Video\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-perceived quality-of-experience (QoE) is critical in internet video\ndelivery systems. Extensive prior work has studied the design of client-side\nbitrate adaptation algorithms to maximize single-player QoE. However,\nmultiplayer QoE fairness becomes critical as the growth of video traffic makes\nit more likely that multiple players share a bottleneck in the network. Despite\nseveral recent proposals, there is still a series of open questions. In this\npaper, we bring the problem space to light from a control theory perspective by\nformalizing the multiplayer QoE fairness problem and addressing two key\nquestions in the broader problem space. First, we derive the sufficient\nconditions of convergence to steady state QoE fairness under TCP-based\nbandwidth sharing scheme. Based on the insight from this analysis that\nin-network active bandwidth allocation is needed, we propose a non-linear\nMPC-based, router-assisted bandwidth allocation algorithm that regards each\nplayer as closed-loop systems. We use trace-driven simulation to show the\nimprovement over existing approaches. We identify several research directions\nenabled by the control theoretic modeling and envision that control theory can\nplay an important role on guiding real system design in adaptive video\nstreaming.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 05:02:32 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Yin", "Xiaoqi", ""], ["Bartulovi\u0107", "Mihovil", ""], ["Sekar", "Vyas", ""], ["Sinopoli", "Bruno", ""]]}]