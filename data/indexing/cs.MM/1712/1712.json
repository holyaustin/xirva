[{"id": "1712.00043", "submitter": "Navaneeth Kamballur Kottayil", "authors": "Navaneeth K. Kottayil, Irene Cheng, Frederic Dufaux and Anup Basu", "title": "A Color Intensity Invariant Low Level Feature Optimization Framework for\n  Image Quality Assessment", "comments": null, "journal-ref": "Signal, Image and Video Processing 10.6 (2016):1169-1176", "doi": "10.1007/s11760-016-0873-x", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Quality Assessment (IQA) algorithms evaluate the perceptual quality of\nan image using evaluation scores that assess the similarity or difference\nbetween two images. We propose a new low-level feature based IQA technique,\nwhich applies filter-bank decomposition and center-surround methodology.\nDiffering from existing methods, our model incorporates color intensity\nadaptation and frequency scaling optimization at each filter-bank level and\nspatial orientation to extract and enhance perceptually significant features.\nOur computational model exploits the concept of object detection and\nencapsulates characteristics proposed in other IQA algorithms in a unified\narchitecture. We also propose a systematic approach to review the evolution of\nIQA algorithms using unbiased test datasets, instead of looking at individual\nscores in isolation. Experimental results demonstrate the feasibility of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:38:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Kottayil", "Navaneeth K.", ""], ["Cheng", "Irene", ""], ["Dufaux", "Frederic", ""], ["Basu", "Anup", ""]]}, {"id": "1712.00334", "submitter": "Fabio Paolizzo", "authors": "Fabio Paolizzo", "title": "Enabling Embodied Analogies in Intelligent Music Systems", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present methodology is aimed at cross-modal machine learning and uses\nmultidisciplinary tools and methods drawn from a broad range of areas and\ndisciplines, including music, systematic musicology, dance, motion capture,\nhuman-computer interaction, computational linguistics and audio signal\nprocessing. Main tasks include: (1) adapting wisdom-of-the-crowd approaches to\nembodiment in music and dance performance to create a dataset of music and\nmusic lyrics that covers a variety of emotions, (2) applying\naudio/language-informed machine learning techniques to that dataset to identify\nautomatically the emotional content of the music and the lyrics, and (3)\nintegrating motion capture data from a Vicon system and dancers performing on\nthat music.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 08:27:08 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Paolizzo", "Fabio", ""]]}, {"id": "1712.00866", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Taejun Kim, Jiyoung Park, Juhan Nam", "title": "Raw Waveform-based Audio Classification Using Sample-level CNN\n  Architectures", "comments": "NIPS, Machine Learning for Audio Signal Processing Workshop\n  (ML4Audio), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music, speech, and acoustic scene sound are often handled separately in the\naudio domain because of their different signal characteristics. However, as the\nimage domain grows rapidly by versatile image classification models, it is\nnecessary to study extensible classification models in the audio domain as\nwell. In this study, we approach this problem using two types of sample-level\ndeep convolutional neural networks that take raw waveforms as input and uses\nfilters with small granularity. One is a basic model that consists of\nconvolution and pooling layers. The other is an improved model that\nadditionally has residual connections, squeeze-and-excitation modules and\nmulti-level concatenation. We show that the sample-level models reach\nstate-of-the-art performance levels for the three different categories of\nsound. Also, we visualize the filters along layers and compare the\ncharacteristics of learned filters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 00:58:58 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Lee", "Jongpil", ""], ["Kim", "Taejun", ""], ["Park", "Jiyoung", ""], ["Nam", "Juhan", ""]]}, {"id": "1712.01456", "submitter": "Zhiqian Chen", "authors": "Zhiqian Chen, Chih-Wei Wu, Yen-Cheng Lu, Alexander Lerch and\n  Chang-Tien Lu", "title": "Learning to Fuse Music Genres with Generative Adversarial Dual Learning", "comments": "International Conference on Data Mining - New Orleans, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FusionGAN is a novel genre fusion framework for music generation that\nintegrates the strengths of generative adversarial networks and dual learning.\nIn particular, the proposed method offers a dual learning extension that can\neffectively integrate the styles of the given domains. To efficiently quantify\nthe difference among diverse domains and avoid the vanishing gradient issue,\nFusionGAN provides a Wasserstein based metric to approximate the distance\nbetween the target domain and the existing domains. Adopting the Wasserstein\ndistance, a new domain is created by combining the patterns of the existing\ndomains using adversarial learning. Experimental results on public music\ndatasets demonstrated that our approach could effectively merge two genres.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 02:53:27 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Chen", "Zhiqian", ""], ["Wu", "Chih-Wei", ""], ["Lu", "Yen-Cheng", ""], ["Lerch", "Alexander", ""], ["Lu", "Chang-Tien", ""]]}, {"id": "1712.02225", "submitter": "Xuelin Qian", "authors": "Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu,\n  Yu-Gang Jiang, Xiangyang Xue", "title": "Pose-Normalized Image Generation for Person Re-identification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (re-id) faces two major challenges: the lack of\ncross-view paired training data and learning discriminative identity-sensitive\nand view-invariant features in the presence of large pose variations. In this\nwork, we address both problems by proposing a novel deep person image\ngeneration model for synthesizing realistic person images conditional on the\npose. The model is based on a generative adversarial network (GAN) designed\nspecifically for pose normalization in re-id, thus termed pose-normalization\nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep\nre-id feature free of the influence of pose variations. We show that this\nfeature is strong on its own and complementary to features learned with the\noriginal images. Importantly, under the transfer learning setting, we show that\nour model generalizes well to any new re-id dataset without the need for\ncollecting any training data for model fine-tuning. The model thus has the\npotential to make re-id model truly scalable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 15:18:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 04:55:01 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 00:28:00 GMT"}, {"version": "v4", "created": "Fri, 2 Feb 2018 06:59:45 GMT"}, {"version": "v5", "created": "Tue, 13 Feb 2018 06:22:12 GMT"}, {"version": "v6", "created": "Wed, 25 Apr 2018 05:57:05 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Qian", "Xuelin", ""], ["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Wang", "Wenxuan", ""], ["Qiu", "Jie", ""], ["Wu", "Yang", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1712.02313", "submitter": "Nitin Khanna Dr.", "authors": "Vinay Verma, Nikita Agarwal, and Nitin Khanna", "title": "DCT-domain Deep Convolutional Neural Networks for Multiple JPEG\n  Compression Classification", "comments": "12 pages", "journal-ref": null, "doi": "10.1016/j.image.2018.04.014", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advancements in digital imaging systems and networking,\nlow-cost hand-held image capture devices equipped with network connectivity are\nbecoming ubiquitous. This ease of digital image capture and sharing is also\naccompanied by widespread usage of user-friendly image editing software. Thus,\nwe are in an era where digital images can be very easily used for the massive\nspread of false information and their integrity need to be seriously\nquestioned. Application of multiple lossy compressions on images is an\nessential part of any image editing pipeline involving lossy compressed images.\nThis paper aims to address the problem of classifying images based on the\nnumber of JPEG compressions they have undergone, by utilizing deep\nconvolutional neural networks in DCT domain. The proposed system incorporates a\nwell designed pre-processing step before feeding the image data to CNN to\ncapture essential characteristics of compression artifacts and make the system\nimage content independent. Detailed experiments are performed to optimize\ndifferent aspects of the system, such as depth of CNN, number of DCT\nfrequencies, and execution time. Results on the standard UCID dataset\ndemonstrate that the proposed system outperforms existing systems for multiple\nJPEG compression detection and is capable of classifying more number of\nre-compression cycles then existing systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:30:31 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Verma", "Vinay", ""], ["Agarwal", "Nikita", ""], ["Khanna", "Nitin", ""]]}, {"id": "1712.02438", "submitter": "Cristian Ionita", "authors": "Cristian Ionita, Alexandru Barbulescu", "title": "Real-time Video Processing in Web Applications", "comments": "12th Romanian Human-Computer Interaction Conference, RoCHI 2015,\n  Bucharest, Romania, September 24-25, 2015,\n  http://rochi.utcluj.ro/articole/3/RoCHI-2015-Ionita.pdf", "journal-ref": "Cristian Ionita, Alexandru Barbulescu, Realtime Video Processing\n  in Web Applications, 12th Romanian Human-Computer Interaction Conference,\n  RoCHI 2015, Bucharest, Romania, September 24-25, 2015, p129-132", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OpenGL ES standard is implemented in modern desktop and mobile browsers\nthrough the WebGL API. This paper explores the potential for using OpenGL ES\nhardware acceleration for real time video processing in standard HTML5\napplications. It analyses the WebGL performance across device types and\ncompares it with the standard JavaScript and canvas performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 23:17:00 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Ionita", "Cristian", ""], ["Barbulescu", "Alexandru", ""]]}, {"id": "1712.02912", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e", "title": "Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search", "comments": "PhD Thesis, 123 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 02:14:17 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Andr\u00e9", "Fabien", ""]]}, {"id": "1712.02926", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Tracy Xiao Liu, Chenhao Tan, Jie Tang", "title": "Online Red Packets: A Large-scale Empirical Study of Gift Giving on\n  WeChat", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC cs.MM econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gift giving is a ubiquitous social phenomenon, and red packets have been used\nas monetary gifts in Asian countries for thousands of years. In recent years,\nonline red packets have become widespread in China through the WeChat platform.\nExploiting a unique dataset consisting of 61 million group red packets and\nseven million users, we conduct a large-scale, data-driven study to understand\nthe spread of red packets and the effect of red packets on group activity. We\nfind that the cash flows between provinces are largely consistent with\nprovincial GDP rankings, e.g., red packets are sent from users in the south to\nthose in the north. By distinguishing spontaneous from reciprocal red packets,\nwe reveal the behavioral patterns in sending red packets: males, seniors, and\npeople with more in-group friends are more inclined to spontaneously send red\npackets, while red packets from females, youths, and people with less in-group\nfriends are more reciprocal. Furthermore, we use propensity score matching to\nstudy the external effects of red packets on group dynamics. We show that red\npackets increase group participation and strengthen in-group relationships,\nwhich partly explain the benefits and motivations for sending red packets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 03:15:58 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yuan", "Yuan", ""], ["Liu", "Tracy Xiao", ""], ["Tan", "Chenhao", ""], ["Tang", "Jie", ""]]}, {"id": "1712.03621", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu, Wei Wang, Jing Dong, Yiliang Xiong and Hongxia Wang", "title": "A Graph-theoretic Model to Steganography on Social Networks", "comments": "Manuscript name is updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography aims to conceal the very fact that the communication takes\nplace, by embedding a message into a digit object such as image without\nintroducing noticeable artifacts. A number of steganographic systems have been\ndeveloped in past years, most of which, however, are confined to the laboratory\nconditions where the real-world use of steganography are rarely concerned. In\nthis paper, we introduce an alternative perspective to steganography. A\ngraph-theoretic model to steganography on social networks is presented to\nanalyze real-world steganographic scenarios. In the graph, steganographic\nparticipants are corresponding to the vertices with meaningless unique\nidentifiers. Each edge allows the two vertices to communicate with each other\nby any steganographic algorithm. Meanwhile, the edges are associated with\nweights to quantize the corresponding communication risk (or say cost). The\noptimization task is to minimize the overall risk, which is modeled as additive\nover the social network. We analyze different scenarios on a social network,\nand provide the suited solutions to the corresponding optimization tasks. We\nprove that a multiplicative probabilistic graph is equivalent to an additive\nweighted graph. From the viewpoint of an attacker, he may hope to detect\nsuspicious communication channels, the data encoder(s) and the data decoder(s).\nWe present limited detection analysis to steganographic communication on a\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 01:13:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:08:57 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 06:48:32 GMT"}, {"version": "v4", "created": "Tue, 6 Mar 2018 17:39:51 GMT"}, {"version": "v5", "created": "Sun, 1 Apr 2018 13:47:03 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wu", "Hanzhou", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Xiong", "Yiliang", ""], ["Wang", "Hongxia", ""]]}, {"id": "1712.05274", "submitter": "Jian Wu", "authors": "Jian Wu and Changran Hu and Yulong Wang and Xiaolin Hu and Jun Zhu", "title": "A Hierarchical Recurrent Neural Network for Symbolic Melody Generation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural networks have been used to generate symbolic\nmelodies. However, the long-term structure in the melody has posed great\ndifficulty for designing a good model. In this paper, we present a hierarchical\nrecurrent neural network for melody generation, which consists of three\nLong-Short-Term-Memory (LSTM) subnetworks working in a coarse-to-fine manner\nalong time. Specifically, the three subnetworks generate bar profiles, beat\nprofiles and notes in turn, and the output of the high-level subnetworks are\nfed into the low-level subnetworks, serving as guidance for generating the\nfiner time-scale melody components in low-level subnetworks. Two human behavior\nexperiments demonstrate the advantage of this structure over the single-layer\nLSTM which attempts to learn all hidden structures in melodies. Compared with\nthe state-of-the-art models MidiNet and MusicVAE, the hierarchical recurrent\nneural network produces better melodies evaluated by humans.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 15:11:09 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 03:25:40 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Wu", "Jian", ""], ["Hu", "Changran", ""], ["Wang", "Yulong", ""], ["Hu", "Xiaolin", ""], ["Zhu", "Jun", ""]]}, {"id": "1712.05785", "submitter": "Xingyou Song", "authors": "Jordan Prosky, Xingyou Song, Andrew Tan, Michael Zhao", "title": "Sentiment Predictability for Stocks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present our findings and experiments for stock-market\nprediction using various textual sentiment analysis tools, such as mood\nanalysis and event extraction, as well as prediction models, such as LSTMs and\nspecific convolutional architectures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:41:53 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 20:24:40 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Prosky", "Jordan", ""], ["Song", "Xingyou", ""], ["Tan", "Andrew", ""], ["Zhao", "Michael", ""]]}, {"id": "1712.05901", "submitter": "Jung Woo Ha", "authors": "Jung-Woo Ha, Adrian Kim, Chanju Kim, Jangyeon Park, Sunghun Kim", "title": "Automatic Music Highlight Extraction using Convolutional Recurrent\n  Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music highlights are valuable contents for music services. Most methods\nfocused on low-level signal features. We propose a method for extracting\nhighlights using high-level features from convolutional recurrent attention\nnetworks (CRAN). CRAN utilizes convolution and recurrent layers for sequential\nlearning with an attention mechanism. The attention allows CRAN to capture\nsignificant snippets for distinguishing between genres, thus being used as a\nhigh-level feature. CRAN was evaluated on over 32,000 popular tracks in Korea\nfor two months. Experimental results show our method outperforms three baseline\nmethods through quantitative and qualitative evaluations. Also, we analyze the\neffects of attention and sequence information on performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 04:27:36 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Ha", "Jung-Woo", ""], ["Kim", "Adrian", ""], ["Kim", "Chanju", ""], ["Park", "Jangyeon", ""], ["Kim", "Sunghun", ""]]}, {"id": "1712.06204", "submitter": "Yuting Chen", "authors": "Yuting Chen, Joseph Wang, Yannan Bai, Gregory Casta\\~n\\'on, and\n  Venkatesh Saligrama", "title": "Probabilistic Semantic Retrieval for Surveillance Videos with Activity\n  Graphs", "comments": "1520-9210 (c) 2018 IEEE. This paper has been accepted by IEEE\n  Transactions on Multimedia. Print ISSN: 1520-9210. Online ISSN: 1941-0077.\n  Preprint link is https://ieeexplore.ieee.org/document/8438958/", "journal-ref": null, "doi": "10.1109/TMM.2018.2865860", "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for finding complex activities matching\nuser-described queries in cluttered surveillance videos. The wide diversity of\nqueries coupled with unavailability of annotated activity data limits our\nability to train activity models. To bridge the semantic gap we propose to let\nusers describe an activity as a semantic graph with object attributes and\ninter-object relationships associated with nodes and edges, respectively. We\nlearn node/edge-level visual predictors during training and, at test-time,\npropose to retrieve activity by identifying likely locations that match the\nsemantic graph. We formulate a novel CRF based probabilistic activity\nlocalization objective that accounts for mis-detections, mis-classifications\nand track-losses, and outputs a likelihood score for a candidate grounded\nlocation of the query in the video. We seek groundings that maximize overall\nprecision and recall. To handle the combinatorial search over all\nhigh-probability groundings, we propose a highest precision subgraph matching\nalgorithm. Our method outperforms existing retrieval methods on benchmarked\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 23:11:28 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 02:03:27 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Chen", "Yuting", ""], ["Wang", "Joseph", ""], ["Bai", "Yannan", ""], ["Casta\u00f1\u00f3n", "Gregory", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1712.06240", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu", "title": "Minimizing Embedding Distortion with Weighted Bigraph Matching in\n  Reversible Data Hiding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a required payload, the existing reversible data hiding (RDH) methods\nalways expect to reduce the embedding distortion as much as possible, such as\nby utilizing a well-designed predictor, taking into account the carrier-content\ncharacteristics, and/or improving modification efficiency etc. However, due to\nthe diversity of natural images, it is actually very hard to accurately model\nthe statistical characteristics of natural images, which has limited the\npractical use of traditional RDH methods that rely heavily on the content\ncharacteristics. Based on this perspective, instead of directly exploiting the\ncontent characteristics, in this paper, we model the embedding operation on a\nweighted bipartite graph to reduce the introduced distortion due to data\nembedding, which is proved to be equivalent to a graph problem called as\n\\emph{minimum weight maximum matching (MWMM)}. By solving the MWMM problem, we\ncan find the optimal histogram shifting strategy under the given condition.\nSince the proposed method is essentially a general embedding model for the RDH,\nit can be utilized for designing an RDH scheme. In our experiments, we\nincorporate the proposed method into some related works, and, our experimental\nresults have shown that the proposed method can significantly improve the\npayload-distortion performance, indicating that the proposed method could be\ndesirable and promising for practical use and the design of RDH schemes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 03:37:36 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Wu", "Hanzhou", ""]]}, {"id": "1712.06651", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "Objects that Sound", "comments": "Appears in: European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper our objectives are, first, networks that can embed audio and\nvisual inputs into a common space that is suitable for cross-modal retrieval;\nand second, a network that can localize the object that sounds in an image,\ngiven the audio signal. We achieve both these objectives by training from\nunlabelled video using only audio-visual correspondence (AVC) as the objective\nfunction. This is a form of cross-modal self-supervision from video.\n  To this end, we design new network architectures that can be trained for\ncross-modal retrieval and localizing the sound source in an image, by using the\nAVC task. We make the following contributions: (i) show that audio and visual\nembeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and\nbetween-mode retrieval; (ii) explore various architectures for the AVC task,\nincluding those for the visual stream that ingest a single image, or multiple\nimages, or a single image and multi-frame optical flow; (iii) show that the\nsemantic object that sounds within an image can be localized (using only the\nsound, no motion or flow information); and (iv) give a cautionary tale on how\nto avoid undesirable shortcuts in the data preparation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 19:52:53 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 16:26:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1712.06951", "submitter": "Yan Ke", "authors": "Ming-ming Liu, Min-qing Zhang, Jia Liu, Ying-nan Zhang, Yan Ke", "title": "Coverless Information Hiding Based on Generative adversarial networks", "comments": "arXiv admin note: text overlap with arXiv:1703.05502 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image steganography modifies the content of the image more or\nless, it is hard to resist the detection of image steganalysis tools. To\naddress this problem, a novel method named generative coverless information\nhiding method based on generative adversarial networks is proposed in this\npaper. The main idea of the method is that the class label of generative\nadversarial networks is replaced with the secret information as a driver to\ngenerate hidden image directly, and then extract the secret information from\nthe hidden image through the discriminator. It's the first time that the\ncoverless information hiding is achieved by generative adversarial networks.\nCompared with the traditional image steganography, this method does not modify\nthe content of the original image. therefore, this method can resist image\nsteganalysis tools effectively. In terms of steganographic capacity,\nanti-steganalysis, safety and reliability, the experimen shows that this hidden\nalgorithm performs well.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 06:00:57 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Liu", "Ming-ming", ""], ["Zhang", "Min-qing", ""], ["Liu", "Jia", ""], ["Zhang", "Ying-nan", ""], ["Ke", "Yan", ""]]}, {"id": "1712.07269", "submitter": "Navaneeth Kamballur Kottayil", "authors": "Navaneeth Kamballur Kottayil, Giuseppe Valenzise, Frederic Dufaux,\n  Irene Cheng", "title": "Blind High Dynamic Range Quality estimation by disentangling perceptual\n  and noise features in images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2778570", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the visual quality of High Dynamic Range (HDR) images is an\nunexplored and an interesting research topic that has become relevant with the\ncurrent boom in HDR technology. We propose a new convolutional neural network\nbased model for No reference image quality assessment(NR-IQA) on HDR data. This\nmodel predicts the amount and location of noise, perceptual influence of image\npixels on the noise, and the perceived quality, of a distorted image without\nany reference image. The proposed model extracts numerical values corresponding\nto the noise present in any given distorted image, and the perceptual effects\nexhibited by a human eye when presented with the same. These two measures are\nextracted separately yet sequentially and combined in a mixing function to\ncompute the quality of the distorted image perceived by a human eye. Our\ntraining process derives the the component that computes perceptual effects\nfrom a real world image quality dataset, rather than using results of\npsycovisual experiments. With the proposed model, we demonstrate state of the\nart performance for HDR NR-IQA and our results show performance similar to HDR\nFull Reference Image Quality Assessment algorithms (FR-IQA).\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 00:00:57 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Kottayil", "Navaneeth Kamballur", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""], ["Cheng", "Irene", ""]]}, {"id": "1712.07540", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Image Registration Techniques: A Survey", "comments": null, "journal-ref": null, "doi": "10.17605/OSF.IO/RV65C", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Registration is the process of aligning two or more images of the same\nscene with reference to a particular image. The images are captured from\nvarious sensors at different times and at multiple view-points. Thus to get a\nbetter picture of any change of a scene or object over a considerable period of\ntime image registration is important. Image registration finds application in\nmedical sciences, remote sensing and in computer vision. This paper presents a\ndetailed review of several approaches which are classified accordingly along\nwith their contributions and drawbacks. The main steps of an image registration\nprocedure are also discussed. Different performance measures are presented that\ndetermine the registration quality and accuracy. The scope for the future\nresearch are presented as well.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:44:28 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1712.08273", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Recurrent Pixel Embedding for Instance Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a differentiable, end-to-end trainable framework for solving\npixel-level grouping problems such as instance segmentation consisting of two\nnovel components. First, we regress pixels into a hyper-spherical embedding\nspace so that pixels from the same group have high cosine similarity while\nthose from different groups have similarity below a specified margin. We\nanalyze the choice of embedding dimension and margin, relating them to\ntheoretical results on the problem of distributing points uniformly on the\nsphere. Second, to group instances, we utilize a variant of mean-shift\nclustering, implemented as a recurrent neural network parameterized by kernel\nbandwidth. This recurrent grouping module is differentiable, enjoys convergent\ndynamics and probabilistic interpretability. Backpropagating the group-weighted\nloss through this module allows learning to focus on only correcting embedding\nerrors that won't be resolved during subsequent clustering. Our framework,\nwhile conceptually simple and theoretically abundant, is also practically\neffective and computationally efficient. We demonstrate substantial\nimprovements over state-of-the-art instance segmentation for object proposal\ngeneration, as well as demonstrating the benefits of grouping loss for\nclassification tasks such as boundary detection and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 01:48:53 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1712.08714", "submitter": "Anurag Ghosh", "authors": "Anurag Ghosh, Suriya Singh, C.V. Jawahar", "title": "Towards Structured Analysis of Broadcast Badminton Videos", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports video data is recorded for nearly every major tournament but remains\narchived and inaccessible to large scale data mining and analytics. It can only\nbe viewed sequentially or manually tagged with higher-level labels which is\ntime consuming and prone to errors. In this work, we propose an end-to-end\nframework for automatic attributes tagging and analysis of sport videos. We use\ncommonly available broadcast videos of matches and, unlike previous approaches,\ndoes not rely on special camera setups or additional sensors.\n  Our focus is on Badminton as the sport of interest. We propose a method to\nanalyze a large corpus of badminton broadcast videos by segmenting the points\nplayed, tracking and recognizing the players in each point and annotating their\nrespective badminton strokes. We evaluate the performance on 10 Olympic matches\nwith 20 players and achieved 95.44% point segmentation accuracy, 97.38% player\ndetection score (mAP@0.5), 97.98% player identification accuracy, and stroke\nsegmentation edit scores of 80.48%. We further show that the automatically\nannotated videos alone could enable the gameplay analysis and inference by\ncomputing understandable metrics such as player's reaction time, speed, and\nfootwork around the court, etc.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 04:51:31 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ghosh", "Anurag", ""], ["Singh", "Suriya", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1712.09480", "submitter": "Yifan Wang", "authors": "Xiyao Liu, Yifang Wang, Ziqiang Sun, Lei Wang, Rongchang Zhao,\n  Yuesheng Zhu and Beiji Zou", "title": "Robust and discriminative zero-watermark scheme based on invariant\n  feature and similarity-based retrieval for protecting large-scale DIBR 3D\n  videos", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital rights management (DRM) of depth-image-based rendering (DIBR) 3D\nvideo is an emerging area of research. Existing schemes for DIBR 3D video cause\nvideo distortions, are vulnerable to severe signal and geometric attacks,\ncannot protect 2D frame and depth map independently or can hardly deal with\nlarge-scale videos. To address these issues, a novel zero-watermark scheme\nbased on invariant feature and similarity-based retrieval for protecting DIBR\n3D video (RZW-SR3D) is proposed in this study. In RZW-SR3D, invariant features\nare extracted to generate master and ownership shares for providing\ndistortion-free, robust and discriminative copyright identification under\nvarious attacks. Different from traditional zero-watermark schemes, features\nand ownership shares are stored correlatively, and a similarity-based retrieval\nphase is designed to provide effective solutions for large-scale videos. In\naddition, flexible mechanisms based on attention-based fusion are designed to\nprotect 2D frame and depth map independently and simultaneously. Experimental\nresults demonstrate that RZW-SR3D have superior DRM performances than existing\nschemes. First, RZW-SR3D can extracted the ownership shares relevant to a\nparticular 3D video precisely and reliably for effective copyright\nidentification of large-scale videos. Second, RZW-SR3D ensures lossless,\nprecise, reliable and flexible copyright identification for 2D frame and depth\nmap of 3D videos.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 02:48:53 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 13:30:29 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Liu", "Xiyao", ""], ["Wang", "Yifang", ""], ["Sun", "Ziqiang", ""], ["Wang", "Lei", ""], ["Zhao", "Rongchang", ""], ["Zhu", "Yuesheng", ""], ["Zou", "Beiji", ""]]}, {"id": "1712.09915", "submitter": "Mario Michael Krell", "authors": "Mario Michael Krell, Julia Bernd, Yifan Li, Daniel Ma, Jaeyoung Choi,\n  Michael Ellsworth, Damian Borth, Gerald Friedland", "title": "Field Studies with Multimedia Big Data: Opportunities and Challenges\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-17-002", "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social multimedia users are increasingly sharing all kinds of data about the\nworld. They do this for their own reasons, not to provide data for field\nstudies-but the trend presents a great opportunity for scientists. The Yahoo\nFlickr Creative Commons 100 Million (YFCC100M) dataset comprises 99 million\nimages and nearly 800 thousand videos from Flickr, all shared under Creative\nCommons licenses. To enable scientists to leverage these media records for\nfield studies, we propose a new framework that extracts targeted subcorpora\nfrom the YFCC100M, in a format usable by researchers who are not experts in big\ndata retrieval and processing.\n  This paper discusses a number of examples from the literature-as well as some\nentirely new ideas-of natural and social science field studies that could be\npiloted, supplemented, replicated, or conducted using YFCC100M data. These\nexamples illustrate the need for a general new open-source framework for\nMultimedia Big Data Field Studies. There is currently a gap between the\nseparate aspects of what multimedia researchers have shown to be possible with\nconsumer-produced big data and the follow-through of creating a comprehensive\nfield study framework that supports scientists across other disciplines.\n  To bridge this gap, we must meet several challenges. For example, the\nframework must handle unlabeled and noisily labeled data to produce a filtered\ndataset for a scientist-who naturally wants it to be both as large and as clean\nas possible. This requires an iterative approach that provides access to\nstatistical summaries and refines the search by constructing new classifiers.\nThe first phase of our framework is available as Multimedia Commons Search, an\nintuitive interface that enables complex search queries at a large scale...\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:17:43 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Krell", "Mario Michael", ""], ["Bernd", "Julia", ""], ["Li", "Yifan", ""], ["Ma", "Daniel", ""], ["Choi", "Jaeyoung", ""], ["Ellsworth", "Michael", ""], ["Borth", "Damian", ""], ["Friedland", "Gerald", ""]]}]