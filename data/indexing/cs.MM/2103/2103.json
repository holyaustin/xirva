[{"id": "2103.00097", "submitter": "Parth Sane", "authors": "Parth Sane", "title": "A Brief Survey of Current Software Engineering Practices in Continuous\n  Integration and Automated Accessibility Testing", "comments": "IEEE Conference WiSPNET 2021 Accepted Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's long been accepted that continuous integration (CI) in software\nengineering increases the code quality of enterprise projects when adhered to\nby it's practitioners. But is any of that effort to increase code quality and\nvelocity directed towards improving software accessibility accommodations? What\nare the potential benefits quoted in literature? Does it fit with the modern\nagile way that teams operate in most enterprises? This paper attempts to map\nthe current scene of the software engineering effort spent on improving\naccessibility via continuous integration and it's hurdles to adoption as quoted\nby researchers. We also try to explore steps that agile teams may take to train\nmembers on how to implement accessibility testing and introduce key diagrams to\nvisualize processes to implement CI based accessibility testing procedures in\nthe software development lifecycle.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 01:13:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sane", "Parth", ""]]}, {"id": "2103.00981", "submitter": "Sarthak Chakraborty", "authors": "Lovish Chopra, Sarthak Chakraborty, Abhijit Mondal and Sandip\n  Chakraborty", "title": "PARIMA: Viewport Adaptive 360-Degree Video Streaming", "comments": "12 pages", "journal-ref": "Proceedings of the Web Conference 2021 (2021) 2379-2391", "doi": "10.1145/3442381.3450070", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing advancements in technologies for capturing 360{\\deg} videos,\nadvances in streaming such videos have become a popular research topic.\nHowever, streaming 360{\\deg} videos require high bandwidth, thus escalating the\nneed for developing optimized streaming algorithms. Researchers have proposed\nvarious methods to tackle the problem, considering the network bandwidth or\nattempt to predict future viewports in advance. However, most of the existing\nworks either (1) do not consider video contents to predict user viewport, or\n(2) do not adapt to user preferences dynamically, or (3) require a lot of\ntraining data for new videos, thus making them potentially unfit for video\nstreaming purposes. We develop PARIMA, a fast and efficient online viewport\nprediction model that uses past viewports of users along with the trajectories\nof prime objects as a representative of video content to predict future\nviewports. We claim that the head movement of a user majorly depends upon the\ntrajectories of the prime objects in the video. We employ a pyramid-based\nbitrate allocation scheme and perform a comprehensive evaluation of the\nperformance of PARIMA. In our evaluation, we show that PARIMA outperforms\nstate-of-the-art approaches, improving the Quality of Experience by over 30\\%\nwhile maintaining a short response time.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:16:36 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 20:04:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chopra", "Lovish", ""], ["Chakraborty", "Sarthak", ""], ["Mondal", "Abhijit", ""], ["Chakraborty", "Sandip", ""]]}, {"id": "2103.01607", "submitter": "Chenguo Lin", "authors": "Chaoning Zhang, Chenguo Lin, Philipp Benz, Kejiang Chen, Weiming Zhang\n  and In So Kweon", "title": "A Brief Survey on Deep Learning Based Data Hiding, Steganography and\n  Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data hiding is the art of concealing messages with limited perceptual\nchanges. Recently, deep learning has provided enriching perspectives for it and\nmade significant progress. In this work, we conduct a brief yet comprehensive\nreview of existing literature and outline three meta-architectures. Based on\nthis, we summarize specific strategies for various applications of deep hiding,\nincluding steganography, light field messaging and watermarking. Finally,\nfurther insight into deep hiding is provided through incorporating the\nperspective of adversarial attack.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:01:03 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhang", "Chaoning", ""], ["Lin", "Chenguo", ""], ["Benz", "Philipp", ""], ["Chen", "Kejiang", ""], ["Zhang", "Weiming", ""], ["Kweon", "In So", ""]]}, {"id": "2103.01760", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Ankitesh K. Singh, Muhammed Coban, Marta Karczewicz,\n  Yinhao Zhu, Yang Yang, Amir Said, Taco S. Cohen", "title": "Transform Network Architectures for Deep Learning based End-to-End\n  Image/Video Coding in Subsampled Color Spaces", "comments": "10 pages, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning based end-to-end image/video coding (DLEC)\narchitectures are designed for non-subsampled RGB color format. However, in\norder to achieve a superior coding performance, many state-of-the-art\nblock-based compression standards such as High Efficiency Video Coding\n(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for\nYUV 4:2:0 format, where U and V components are subsampled by considering the\nhuman visual system. This paper investigates various DLEC designs to support\nYUV 4:2:0 format by comparing their performance against the main profiles of\nHEVC and VVC standards under a common evaluation framework. Moreover, a new\ntransform network architecture is proposed to improve the efficiency of coding\nYUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the\nproposed architecture significantly outperforms naive extensions of existing\narchitectures designed for RGB format and achieves about 10% average BD-rate\nimprovement over the intra-frame coding in HEVC.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:47:27 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Singh", "Ankitesh K.", ""], ["Coban", "Muhammed", ""], ["Karczewicz", "Marta", ""], ["Zhu", "Yinhao", ""], ["Yang", "Yang", ""], ["Said", "Amir", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2103.02189", "submitter": "Nabajeet Barman Dr", "authors": "Nabajeet Barman and Maria G Martini", "title": "User Generated HDR Gaming Video Streaming: Dataset, Codec Comparison and\n  Challenges", "comments": "14 pages, 8 figures, submitted to IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gaming video streaming services have grown tremendously in the past few\nyears, with higher resolutions, higher frame rates and HDR gaming videos\ngetting increasingly adopted among the gaming community. Since gaming content\nas such is different from non-gaming content, it is imperative to evaluate the\nperformance of the existing encoders to help understand the bandwidth\nrequirements of such services, as well as further improve the compression\nefficiency of such encoders. Towards this end, we present in this paper\nGamingHDRVideoSET, a dataset consisting of eighteen 10-bit UHD-HDR gaming\nvideos and encoded video sequences using four different codecs, together with\ntheir objective evaluation results. The dataset is available online at [to be\nadded after paper acceptance]. Additionally, the paper discusses the codec\ncompression efficiency of most widely used practical encoders, i.e., x264\n(H.264/AVC), x265 (H.265/HEVC) and libvpx (VP9), as well the recently proposed\nencoder libaom (AV1), on 10-bit, UHD-HDR content gaming content. Our results\nshow that the latest compression standard AV1 results in the best compression\nefficiency, followed by HEVC, H.264, and VP9.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:34:06 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Barman", "Nabajeet", ""], ["Martini", "Maria G", ""]]}, {"id": "2103.02453", "submitter": "Shoko Imaizumi", "authors": "Minagi Ueda and Shoko Imaizumi", "title": "Reversible Data Hiding Associated with Digital Halftoning That Allows\n  Printing with Special Color Ink by Using Single Color Layer", "comments": "2 pages", "journal-ref": null, "doi": "10.1541/ieejeiss.141.163", "report-no": "IEEJ Trans. Electr. Inf. & Syst., vol.141, no.2, pp.163-164,\n  February 2021", "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient framework of reversible data hiding to preserve\ncompatibility between normal printing and printing with a special color ink by\nusing a single common image. The special color layer is converted to a binary\nimage by digital halftoning and losslessly compressed using JBIG2. Then, the\ncompressed information of the binarized special color layer is reversibly\nembedded into the general color layer without significant distortion. Our\nexperimental results show the availability of the proposed method in terms of\nthe marked image quality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:59:58 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ueda", "Minagi", ""], ["Imaizumi", "Shoko", ""]]}, {"id": "2103.02550", "submitter": "Marta Orduna", "authors": "Marta Orduna, Pablo P\\'erez, Jes\\'us Guti\\'errez and Narciso Garc\\'ia", "title": "Methodology to Assess Quality, Presence, Empathy, Attitude, and\n  Attention in Social VR: International Experiences Use Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper analyzes the joint assessment of quality, spatial and social\npresence, empathy, attitude, and attention in three conditions: (A)visualizing\nand rating the quality of contents in a Head-Mounted Display (HMD),\n(B)visualizing the contents in an HMD,and (C)visualizing the contents in an HMD\nwhere participants can see their hands and take notes. The experiment simulates\nan immersive communication where participants attend conversations of different\ngenres and from different acquisition perspectives in the context of\ninternational experiences. Video quality is evaluated with Single-Stimulus\nDiscrete Quality Evaluation (SSDQE) methodology. Spatial and social presence\nare evaluated with questionnaires adapted from the literature. Initial empathy\nis assessed with Interpersonal Reactivity Index(IRI) and a questionnaire is\ndesigned to evaluate attitude. Attention is evaluated with 3 questions that had\npass/fail answers. 54 participants were evenly distributed among A, B, and C\nconditions taking into account their international experience backgrounds,\nobtaining a diverse sample of participants. The results from the subjective\ntest validate the proposed methodology in VR communications, showing that video\nquality experiments can be adapted to conditions imposed by experiments focused\non the evaluation of socioemotional features in terms of contents of\nlong-duration, actor and observer acquisition perspectives, and genre. In\naddition, the positive results related to the sense of presence imply that\ntechnology can be relevant in the analyzed use case. The acquisition\nperspective greatly influences social presence and all the contents have a\npositive impact on all participants on their attitude towards international\nexperiences. The annotated dataset, Student Experiences Around the World\ndataset (SEAW-dataset), obtained from the experiment is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:43:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Orduna", "Marta", ""], ["P\u00e9rez", "Pablo", ""], ["Guti\u00e9rrez", "Jes\u00fas", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "2103.02777", "submitter": "Shoko Imaizumi", "authors": "Kotoko Hiraoka, Kensuke Fukumoto, Takashi Yamazoe, Norimichi Tsumura,\n  Satoshi Kaneko, Wataru Arai, Shoko Imaizumi", "title": "Application of Reversible Data Hiding for Printing with Special Color\n  Inks to Preserve Compatibility with Normal Printing", "comments": "8 pages", "journal-ref": "IEEJ Trans. Electr. Inf. & Syst., vol.141, no.2, pp.155-162,\n  February 2021", "doi": "10.1541/ieejeiss.141.155", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient framework with compatibility between normal printing\nand printing with special color inks in this paper. Special color inks can be\nused for printing to represent some particular colors and specific optical\nproperties, which are difficult to express using only CMYK inks. Special color\nlayers are required in addition to the general color layer for printing with\nspecial color inks. We introduce a reversible data hiding (RDH) method to embed\nthe special color layers into the general color layer without visible\nartifacts. The proposed method can realize both normal printing and printing\nwith special color inks by using a single layer. Our experimental results show\nthat the quality of the marked image is virtually identical to that of the\noriginal image, i.e., the general color layer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 00:58:53 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Hiraoka", "Kotoko", ""], ["Fukumoto", "Kensuke", ""], ["Yamazoe", "Takashi", ""], ["Tsumura", "Norimichi", ""], ["Kaneko", "Satoshi", ""], ["Arai", "Wataru", ""], ["Imaizumi", "Shoko", ""]]}, {"id": "2103.03023", "submitter": "Bi-Cheng Yan", "authors": "Bi-Cheng Yan and Berlin Chen", "title": "End-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms", "comments": "Preprint. Under review 5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mispronunciation detection and diagnosis (MDD) is designed to identify\npronunciation errors and provide instructive feedback to guide non-native\nlanguage learners, which is a core component in computer-assisted pronunciation\ntraining (CAPT) systems. However, MDD often suffers from the data-sparsity\nproblem due to that collecting non-native data and the associated annotations\nis time-consuming and labor-intensive. To address this issue, we explore a\nfully end-to-end (E2E) neural model for MDD, which processes learners' speech\ndirectly based on raw waveforms. Compared to conventional hand-crafted acoustic\nfeatures, raw waveforms retain more acoustic phenomena and potentially can help\nneural networks discover better and more customized representations. To this\nend, our MDD model adopts a co-called SincNet module to take input a raw\nwaveform and covert it to a suitable vector representation sequence. SincNet\nemploys the cardinal sine (sinc) function to implement learnable bandpass\nfilters, drawing inspiration from the convolutional neural network (CNN). By\ncomparison to CNN, SincNet has fewer parameters and is more amenable to human\ninterpretation. Extensive experiments are conducted on the L2-ARCTIC dataset,\nwhich is a publicly-available non-native English speech corpus compiled for\nresearch on CAPT. We find that the sinc filters of SincNet can be adapted\nquickly for non-native language learners of different nationalities.\nFurthermore, our model can achieve comparable mispronunciation detection\nperformance in relation to state-of-the-art E2E MDD models that take input the\nstandard handcrafted acoustic features. Besides that, our model also provides\nconsiderable improvements on phone error rate (PER) and diagnosis accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:33:50 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:38:42 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 01:13:09 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 07:10:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yan", "Bi-Cheng", ""], ["Chen", "Berlin", ""]]}, {"id": "2103.03070", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Esin Guldogan, Moncef\n  Gabbouj", "title": "Convolutional versus Self-Organized Operational Neural Networks for\n  Real-World Blind Image Denoising", "comments": "Submitted for review in IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Real-world blind denoising poses a unique image restoration challenge due to\nthe non-deterministic nature of the underlying noise distribution. Prevalent\ndiscriminative networks trained on synthetic noise models have been shown to\ngeneralize poorly to real-world noisy images. While curating real-world noisy\nimages and improving ground truth estimation procedures remain key points of\ninterest, a potential research direction is to explore extensions to the widely\nused convolutional neuron model to enable better generalization with fewer data\nand lower network complexity, as opposed to simply using deeper Convolutional\nNeural Networks (CNNs). Operational Neural Networks (ONNs) and their recent\nvariant, Self-organized ONNs (Self-ONNs), propose to embed enhanced\nnon-linearity into the neuron model and have been shown to outperform CNNs\nacross a variety of regression tasks. However, all such comparisons have been\nmade for compact networks and the efficacy of deploying operational layers as a\ndrop-in replacement for convolutional layers in contemporary deep architectures\nremains to be seen. In this work, we tackle the real-world blind image\ndenoising problem by employing, for the first time, a deep Self-ONN. Extensive\nquantitative and qualitative evaluations spanning multiple metrics and four\nhigh-resolution real-world noisy image datasets against the state-of-the-art\ndeep CNN network, DnCNN, reveal that deep Self-ONNs consistently achieve\nsuperior results with performance gains of up to 1.76dB in PSNR. Furthermore,\nSelf-ONNs with half and even quarter the number of layers that require only a\nfraction of computational resources as that of DnCNN can still achieve similar\nor better results compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:49:17 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 20:21:56 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Yamac", "Mehmet", ""], ["Guldogan", "Esin", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2103.03496", "submitter": "Markku Suomalainen", "authors": "Katherine J. Mimnaugh, Markku Suomalainen, Israel Becerra, Eliezer\n  Lozano, Rafael Murrieta-Cid, and Steven M. LaValle", "title": "Analysis of User Preferences for Robot Motions in Immersive Telepresence", "comments": "Accepted for publication in IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers how the motions of a telepresence robot moving\nautonomously affect a person immersed in the robot through a head-mounted\ndisplay. In particular, we explore the preference, comfort, and naturalness of\nelements of piecewise linear paths compared to the same elements on a smooth\npath. In a user study, thirty-six subjects watched panoramic videos of three\ndifferent paths through a simulated museum in virtual reality and responded to\nquestionnaires regarding each path. Preference for a particular path was\ninfluenced the most by comfort, forward speed, and characteristics of the\nturns. Preference was also strongly associated with the users' perceived\nnaturalness, which was primarily determined by the ability to see salient\nobjects, the distance to the walls and objects, as well as the turns.\nParticipants favored the paths that had a one meter per second forward speed\nand rated the path with the least amount of turns as the most comfortable\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 06:50:48 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 07:35:51 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Mimnaugh", "Katherine J.", ""], ["Suomalainen", "Markku", ""], ["Becerra", "Israel", ""], ["Lozano", "Eliezer", ""], ["Murrieta-Cid", "Rafael", ""], ["LaValle", "Steven M.", ""]]}, {"id": "2103.03539", "submitter": "Pengfei Qu", "authors": "Xintian Wu, Pengfei Qu, Shaofei Wang, Lin Xie and Jie Dong", "title": "Extend the FFmpeg Framework to Analyze Media Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new set of video analytics plugins developed for the\nFFmpeg framework. Multimedia applications that increasingly utilize the FFmpeg\nmedia features for its comprehensive media encoding, decoding, muxing, and\ndemuxing capabilities can now additionally analyze the video content based on\nAI models. The plugins are thread optimized for best performance overcoming\ncertain FFmpeg threading limitations. The plugins utilize the Intel OpenVINO\nToolkit inference engine as the backend. The analytics workloads are\naccelerated on different platforms such as CPU, GPU, FPGA or specialized\nanalytics accelerators. With our reference implementation, the feature of\nOpenVINO as inference backend has been pushed into FFmpeg mainstream\nrepository. We plan to submit more patches later.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:39:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wu", "Xintian", ""], ["Qu", "Pengfei", ""], ["Wang", "Shaofei", ""], ["Xie", "Lin", ""], ["Dong", "Jie", ""]]}, {"id": "2103.03612", "submitter": "Yiming Li", "authors": "Yiming Li, Shan Liu, Yu Chen, Yushan Zheng, Sijia Chen, Bin Zhu, Jian\n  Lou", "title": "An Optimized H.266/VVC Software Decoder On Mobile Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the successor of H.265/HEVC, the new versatile video coding standard\n(H.266/VVC) can provide up to 50% bitrate saving with the same subjective\nquality, at the cost of increased decoding complexity. To accelerate the\napplication of the new coding standard, a real-time H.266/VVC software decoder\nthat can support various platforms is implemented, where SIMD technologies,\nparallelism optimization, and the acceleration strategies based on the\ncharacteristics of each coding tool are applied. As the mobile devices have\nbecome an essential carrier for video services nowadays, the mentioned\noptimization efforts are not only implemented for the x86 platform, but more\nimportantly utilized to highly optimize the decoding performance on the ARM\nplatform in this work. The experimental results show that when running on the\nApple A14 SoC (iPhone 12pro), the average single-thread decoding speed of the\npresent implementation can achieve 53fps (RA and LB) for full HD (1080p)\nbitstreams generated by VTM-11.0 reference software using 8bit Common Test\nConditions (CTC). When multi-threading is enabled, an average of 32 fps (RA)\ncan be achieved when decoding the 4K bitstreams.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:33:17 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Li", "Yiming", ""], ["Liu", "Shan", ""], ["Chen", "Yu", ""], ["Zheng", "Yushan", ""], ["Chen", "Sijia", ""], ["Zhu", "Bin", ""], ["Lou", "Jian", ""]]}, {"id": "2103.03756", "submitter": "Andrew Tristan", "authors": "Andrew Tristan, Vinicius Woloszyn and Ben Kaden", "title": "BOPI: A Programming Interface For Reuse Of Research Data Available On\n  DSpace Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent study showed that more than 70% of researchers fail to reproduce\ntheir peers's experiments and more than half fail to reproduce their own\nexperiments. Obviously, from a perspective of scientific quality this is a more\nthan unsatisfying numbers. One approach to mitigate this flaw lies in the\ntransparent provision of relevant research data to increase the base of\navailable material to evaluate and possibly reconduct experiments. However,\nsuch data needs to be presented and accessed in a findable and purposefully\nusable way. In this work, we report the development of a programming interface\nto enhance findability and accessibility of research data (available in DSpace\nsystems) and hence reproducibility of scientific experiments with data. This\ninterface allows researchers to (i) find research data in multiples languages\ntrough automatic translation of metadata; (ii) display a preview of data\nwithout download it beforehand; (iii) provide a detailed statistics of the data\nwith interactive graphs for quality assessment; (iv) automatic download of data\ndirectly from Python-based experiments. Usability tests revealed that this\ninterface improves the effectiveness, efficiency and satisfaction during the\nreuse of research data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:32:51 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:02:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Tristan", "Andrew", ""], ["Woloszyn", "Vinicius", ""], ["Kaden", "Ben", ""]]}, {"id": "2103.04203", "submitter": "Wassim Hamidouche", "authors": "Guillaume Gautier, Mousa FarajAllah, Wassim Hamidouche, Olivier\n  D\\'eforges and Safwan El Assad", "title": "Selective Encryption of the Versatile Video Coding Standard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Versatile video coding (VVC) is the next generation video coding standard\ndeveloped by the joint video experts team (JVET) and released in July 2020. VVC\nintroduces several new coding tools providing a significant coding gain over\nthe high efficiency video coding (HEVC) standard. It is well known that\nincreasing the coding efficiency adds more dependencies in the video bitstream\nmaking format-compliant encryption with the standard more challenging. In this\npaper we tackle the problem of selective encryption of the VVC standard in\nformat-compliant and constant bitrate. These two constraints ensure that the\nencrypted bitstream can be decoded by any VVC decoder while the bitrate remains\nunchanged by the encryption. The selective encryption of all possible VVC\nsyntax elements is investigated. A new algorithm is proposed to encrypt in\nformat-compliant and constant bitrate the transform coefficients (TCs) together\nwith other syntax elements at the level of the entropy encoder. The proposed\nsolution was integrated and assessed under the VVC reference software model\nversion 6.0. Experimental results showed that the encryption drastically\ndecreases the video quality while the encryption is robust against several\ntypes of attacks. The encryption space is estimated in the range of 15% to 26%\nof the bitstream size resulting in a lightweight encryption process. The web\npage of this work is available at https://gugautie.github.io/sevvc/.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 22:27:30 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gautier", "Guillaume", ""], ["FarajAllah", "Mousa", ""], ["Hamidouche", "Wassim", ""], ["D\u00e9forges", "Olivier", ""], ["Assad", "Safwan El", ""]]}, {"id": "2103.04609", "submitter": "Mattia Lecci", "authors": "Mattia Lecci, Andrea Zanella, Michele Zorzi", "title": "An ns-3 Implementation of a Bursty Traffic Framework for Virtual Reality\n  Sources", "comments": "9 pages, 6 figures. Please cite it as: M. Lecci, A. Zanella, M.\n  Zorzi, \"An ns-3 Implementation of a Bursty Traffic Framework for Virtual\n  Reality Sources,\" in Workshop on ns-3 (WNS3), Jun. 2021, Virtual Event, US", "journal-ref": null, "doi": "10.1145/3460797.3460807", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation wireless communication technologies will allow users to\nobtain unprecedented performance, paving the way to new and immersive\napplications. A prominent application requiring high data rates and low\ncommunication delay is Virtual Reality (VR), whose presence will become\nincreasingly stronger in the years to come. To the best of our knowledge, we\npropose the first traffic model for VR applications based on traffic traces\nacquired from a commercial VR streaming software, allowing the community to\nfurther study and improve the technology to manage this type of traffic. This\nwork implements ns-3 applications able to generate and process large bursts of\npackets, enabling the possibility of analyzing APP-level end-to-end metrics,\nmaking the source code as well as the acquired VR traffic traces publicly\navailable and open-source.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 08:59:58 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 10:03:37 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lecci", "Mattia", ""], ["Zanella", "Andrea", ""], ["Zorzi", "Michele", ""]]}, {"id": "2103.04692", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala and John A. Bateman", "title": "Semiotically-grounded distant viewing of diagrams: insights from two\n  multimodal corpora", "comments": "22 pages, 11 figures. Under review at Digital Scholarship in the\n  Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we bring together theories of multimodal communication and\ncomputational methods to study how primary school science diagrams combine\nmultiple expressive resources. We position our work within the field of digital\nhumanities, and show how annotations informed by multimodality research, which\ntarget expressive resources and discourse structure, allow imposing structure\non the output of computational methods. We illustrate our approach by analysing\ntwo multimodal diagram corpora: the first corpus is intended to support\nresearch on automatic diagram processing, whereas the second is oriented\ntowards studying diagrams as a mode of communication. Our results show that\nmultimodally-informed annotations can bring out structural patterns in the\ndiagrams, which also extend across diagrams that deal with different topics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:04:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hiippala", "Tuomo", ""], ["Bateman", "John A.", ""]]}, {"id": "2103.05302", "submitter": "Hailong Ning", "authors": "Hailong Ning, Bin Zhao, and Yuan Yuan", "title": "Semantics-Consistent Representation Learning for Remote Sensing\n  Image-Voice Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of earth observation technology, massive amounts of\nremote sensing (RS) images are acquired. To find useful information from these\nimages, cross-modal RS image-voice retrieval provides a new insight. This paper\naims to study the task of RS image-voice retrieval so as to search effective\ninformation from massive amounts of RS data. Existing methods for RS\nimage-voice retrieval rely primarily on the pairwise relationship to narrow the\nheterogeneous semantic gap between images and voices. However, apart from the\npairwise relationship included in the datasets, the intra-modality and\nnon-paired inter-modality relationships should also be taken into account\nsimultaneously, since the semantic consistency among non-paired representations\nplays an important role in the RS image-voice retrieval task. Inspired by this,\na semantics-consistent representation learning (SCRL) method is proposed for RS\nimage-voice retrieval. The main novelty is that the proposed method takes the\npairwise, intra-modality, and non-paired inter-modality relationships into\naccount simultaneously, thereby improving the semantic consistency of the\nlearned representations for the RS image-voice retrieval. The proposed SCRL\nmethod consists of two main steps: 1) semantics encoding and 2)\nsemantics-consistent representation learning. Firstly, an image encoding\nnetwork is adopted to extract high-level image features with a transfer\nlearning strategy, and a voice encoding network with dilated convolution is\ndevised to obtain high-level voice features. Secondly, a consistent\nrepresentation space is conducted by modeling the three kinds of relationships\nto narrow the heterogeneous semantic gap and learn semantics-consistent\nrepresentations across two modalities. Extensive experimental results on three\nchallenging RS image-voice datasets show the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:59:47 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ning", "Hailong", ""], ["Zhao", "Bin", ""], ["Yuan", "Yuan", ""]]}, {"id": "2103.05842", "submitter": "Jisheng Li", "authors": "Jisheng Li, Yuze He, Yubin Hu, Yuxing Han, Jiangtao Wen", "title": "Learning to compose 6-DoF omnidirectional videos using multi-sphere\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Omnidirectional video is an essential component of Virtual Reality. Although\nvarious methods have been proposed to generate content that can be viewed with\nsix degrees of freedom (6-DoF), existing systems usually involve complex depth\nestimation, image in-painting or stitching pre-processing. In this paper, we\npropose a system that uses a 3D ConvNet to generate a multi-sphere images (MSI)\nrepresentation that can be experienced in 6-DoF VR. The system utilizes\nconventional omnidirectional VR camera footage directly without the need for a\ndepth map or segmentation mask, thereby significantly simplifying the overall\ncomplexity of the 6-DoF omnidirectional video composition. By using a newly\ndesigned weighted sphere sweep volume (WSSV) fusing technique, our approach is\ncompatible with most panoramic VR camera setups. A ground truth generation\napproach for high-quality artifact-free 6-DoF contents is proposed and can be\nused by the research and development community for 6-DoF content generation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:09:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["He", "Yuze", ""], ["Hu", "Yubin", ""], ["Han", "Yuxing", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05843", "submitter": "Jisheng Li", "authors": "Jisheng Li, Qi Dai, Jiangtao Wen", "title": "Learning to Estimate Kernel Scale and Orientation of Defocus Blur with\n  Asymmetric Coded Aperture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consistent in-focus input imagery is an essential precondition for machine\nvision systems to perceive the dynamic environment. A defocus blur severely\ndegrades the performance of vision systems. To tackle this problem, we propose\na deep-learning-based framework estimating the kernel scale and orientation of\nthe defocus blur to adjust lens focus rapidly. Our pipeline utilizes 3D ConvNet\nfor a variable number of input hypotheses to select the optimal slice from the\ninput stack. We use random shuffle and Gumbel-softmax to improve network\nperformance. We also propose to generate synthetic defocused images with\nvarious asymmetric coded apertures to facilitate training. Experiments are\nconducted to demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:12:15 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["Dai", "Qi", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05858", "submitter": "Jisheng Li", "authors": "Jisheng Li, Ziyu Wen, Sihan Li, Yikai Zhao, Bichuan Guo, Jiangtao Wen", "title": "Novel tile segmentation scheme for omnidirectional video", "comments": "Published in 2016 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532381", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regular omnidirectional video encoding technics use map projection to flatten\na scene from a spherical shape into one or several 2D shapes. Common projection\nmethods including equirectangular and cubic projection have varying levels of\ninterpolation that create a large number of non-information-carrying pixels\nthat lead to wasted bitrate. In this paper, we propose a tile based\nomnidirectional video segmentation scheme which can save up to 28% of pixel\narea and 20% of BD-rate averagely compared to the traditional equirectangular\nprojection based approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:49:18 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["Wen", "Ziyu", ""], ["Li", "Sihan", ""], ["Zhao", "Yikai", ""], ["Guo", "Bichuan", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05905", "submitter": "Yibing Song", "authors": "Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu", "title": "VideoMoCo: Contrastive Video Representation Learning with Temporally\n  Adversarial Examples", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MoCo is effective for unsupervised image representation learning. In this\npaper, we propose VideoMoCo for unsupervised video representation learning.\nGiven a video sequence as an input sample, we improve the temporal feature\nrepresentations of MoCo from two perspectives. First, we introduce a generator\nto drop out several frames from this sample temporally. The discriminator is\nthen learned to encode similar feature representations regardless of frame\nremovals. By adaptively dropping out different frames during training\niterations of adversarial learning, we augment this input sample to train a\ntemporally robust encoder. Second, we use temporal decay to model key\nattenuation in the memory queue when computing the contrastive loss. As the\nmomentum encoder updates after keys enqueue, the representation ability of\nthese keys degrades when we use the current input sample for contrastive\nlearning. This degradation is reflected via temporal decay to attend the input\nsample to recent keys in the queue. As a result, we adapt MoCo to learn video\nrepresentations without empirically designing pretext tasks. By empowering the\ntemporal robustness of the encoder and modeling the temporal decay of the keys,\nour VideoMoCo improves MoCo temporally based on contrastive learning.\nExperiments on benchmark datasets including UCF101 and HMDB51 show that\nVideoMoCo stands as a state-of-the-art video representation learning method.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:22:21 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 02:45:50 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pan", "Tian", ""], ["Song", "Yibing", ""], ["Yang", "Tianyu", ""], ["Jiang", "Wenhao", ""], ["Liu", "Wei", ""]]}, {"id": "2103.06032", "submitter": "Chunbin Gu", "authors": "Chunbin Gu, Jiajun Bu, Xixi Zhou, Chengwei Yao, Dongfang Ma, Zhi Yu,\n  Xifeng Yan", "title": "Cross-modal Image Retrieval with Deep Mutual Information Maximization", "comments": "35 pages,7 figures, Submitted to Neuralcomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the cross-modal image retrieval, where the inputs\ncontain a source image plus some text that describes certain modifications to\nthis image and the desired image. Prior work usually uses a three-stage\nstrategy to tackle this task: 1) extract the features of the inputs; 2) fuse\nthe feature of the source image and its modified text to obtain fusion feature;\n3) learn a similarity metric between the desired image and the source image +\nmodified text by using deep metric learning. Since classical image/text\nencoders can learn the useful representation and common pair-based loss\nfunctions of distance metric learning are enough for cross-modal retrieval,\npeople usually improve retrieval accuracy by designing new fusion networks.\nHowever, these methods do not successfully handle the modality gap caused by\nthe inconsistent distribution and representation of the features of different\nmodalities, which greatly influences the feature fusion and similarity\nlearning. To alleviate this problem, we adopt the contrastive self-supervised\nlearning method Deep InforMax (DIM) to our approach to bridge this gap by\nenhancing the dependence between the text, the image, and their fusion.\nSpecifically, our method narrows the modality gap between the text modality and\nthe image modality by maximizing mutual information between their not exactly\nsemantically identical representation. Moreover, we seek an effective common\nsubspace for the semantically same fusion feature and desired image's feature\nby utilizing Deep InforMax between the low-level layer of the image encoder and\nthe high-level layer of the fusion network. Extensive experiments on three\nlarge-scale benchmark datasets show that we have bridged the modality gap\nbetween different modalities and achieve state-of-the-art retrieval\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:08:09 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Gu", "Chunbin", ""], ["Bu", "Jiajun", ""], ["Zhou", "Xixi", ""], ["Yao", "Chengwei", ""], ["Ma", "Dongfang", ""], ["Yu", "Zhi", ""], ["Yan", "Xifeng", ""]]}, {"id": "2103.06116", "submitter": "Li Yang", "authors": "Li Yang, Mai Xu, Deng Xin and Bo Feng", "title": "Spatial Attention-based Non-reference Perceptual Quality Prediction\n  Network for Omnidirectional Images", "comments": "Accepted by IEEE ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the strong correlation between visual attention and perceptual\nquality, many methods attempt to use human saliency information for image\nquality assessment. Although this mechanism can get good performance, the\nnetworks require human saliency labels, which is not easily accessible for\nomnidirectional images (ODI). To alleviate this issue, we propose a spatial\nattention-based perceptual quality prediction network for non-reference quality\nassessment on ODIs (SAP-net). To drive our SAP-net, we establish a large-scale\nIQA dataset of ODIs (IQA-ODI), which is composed of subjective scores of 200\nsubjects on 1,080 ODIs. In IQA-ODI, there are 120 high quality ODIs as\nreference, and 960 ODIs with impairments in both JPEG compression and map\nprojection. Without any human saliency labels, our network can adaptively\nestimate human perceptual quality on impaired ODIs through a self-attention\nmanner, which significantly promotes the prediction performance of quality\nscores. Moreover, our method greatly reduces the computational complexity in\nquality assessment task on ODIs. Extensive experiments validate that our\nnetwork outperforms 9 state-of-the-art methods for quality assessment on ODIs.\nThe dataset and code have been available on \\url{\nhttps://github.com/yanglixiaoshen/SAP-Net}.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:14:37 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Li", ""], ["Xu", "Mai", ""], ["Xin", "Deng", ""], ["Feng", "Bo", ""]]}, {"id": "2103.06342", "submitter": "Umberto Michieli", "authors": "Umberto Michieli and Pietro Zanuttigh", "title": "Continual Semantic Segmentation via Repulsion-Attraction of Sparse and\n  Disentangled Latent Representations", "comments": "CVPR 2021. 22 pages, 10 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks suffer from the major limitation of catastrophic\nforgetting old tasks when learning new ones. In this paper we focus on class\nincremental continual learning in semantic segmentation, where new categories\nare made available over time while previous training data is not retained. The\nproposed continual learning scheme shapes the latent space to reduce forgetting\nwhilst improving the recognition of novel classes. Our framework is driven by\nthree novel components which we also combine on top of existing techniques\neffortlessly. First, prototypes matching enforces latent space consistency on\nold classes, constraining the encoder to produce similar latent representation\nfor previously seen classes in the subsequent steps. Second, features\nsparsification allows to make room in the latent space to accommodate novel\nclasses. Finally, contrastive learning is employed to cluster features\naccording to their semantics while tearing apart those of different classes.\nExtensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the\neffectiveness of our approach, significantly outperforming state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:02:05 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:58:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2103.06541", "submitter": "Trisha Mittal", "authors": "Trisha Mittal, Puneet Mathur, Aniket Bera, Dinesh Manocha", "title": "Affect2MM: Affective Analysis of Multimedia Content Using Emotion\n  Causality", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Affect2MM, a learning method for time-series emotion prediction\nfor multimedia content. Our goal is to automatically capture the varying\nemotions depicted by characters in real-life human-centric situations and\nbehaviors. We use the ideas from emotion causation theories to computationally\nmodel and determine the emotional state evoked in clips of movies. Affect2MM\nexplicitly models the temporal causality using attention-based methods and\nGranger causality. We use a variety of components like facial features of\nactors involved, scene understanding, visual aesthetics, action/situation\ndescription, and movie script to obtain an affective-rich representation to\nunderstand and perceive the scene. We use an LSTM-based learning model for\nemotion perception. To evaluate our method, we analyze and compare our\nperformance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE\ndataset, and observe an average of 10-15% increase in the performance over SOTA\nmethods for all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:07:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Mittal", "Trisha", ""], ["Mathur", "Puneet", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2103.06675", "submitter": "Robert Skupin", "authors": "Robert Skupin, Christian Bartnik, Adam Wieckowski, Yago Sanchez,\n  Benjamin Bross, Cornelius Hellge, Thomas Schierl", "title": "Open GOP Resolution Switching in HTTP Adaptive Streaming with VVC", "comments": "Accepted at IEEE Picture Coding Symposium 2021, 5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The user experience in adaptive HTTP streaming relies on offering bitrate\nladders with suitable operation points for all users and typically involves\nmultiple resolutions. While open GOP coding structures are generally known to\nprovide substantial coding efficiency benefit, their use in HTTP streaming has\nbeen precluded through lacking support of reference picture resampling (RPR) in\nAVC and HEVC. The newly emerging Versatile Video Coding (VVC) standard supports\nRPR, but only conversational scenarios were primarily investigated during the\ndesign of VVC. This paper aims at enabling usage of RPR in HTTP streaming\nscenarios through analysing the drift potential of VVC coding tools and\npresenting a constrained encoding method that avoids severe drift artefacts in\nresolution switching with open GOP coding in VVC. In typical live streaming\nconfigurations, the presented method achieves -8.57% BD-rate reduction compared\nto closed GOP coding while in a typical Video on Demand configuration, -1.89%\nBD-rate reduction is reported. The constraints penalty compared to regular open\nGOP coding is 0.65% BD-rate in the worst case. The presented method was\nintegrated into the publicly available open source VVC encoder VVenC v0.3.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:10:29 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:10:25 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Skupin", "Robert", ""], ["Bartnik", "Christian", ""], ["Wieckowski", "Adam", ""], ["Sanchez", "Yago", ""], ["Bross", "Benjamin", ""], ["Hellge", "Cornelius", ""], ["Schierl", "Thomas", ""]]}, {"id": "2103.07197", "submitter": "Cumhur Erkut", "authors": "Juan Alonso and Cumhur Erkut", "title": "Latent Space Explorations of Singing Voice Synthesis using DDSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning based singing voice models require large datasets and\nlengthy training times. In this work we present a lightweight architecture,\nbased on the Differentiable Digital Signal Processing (DDSP) library, that is\nable to output song-like utterances conditioned only on pitch and amplitude,\nafter twelve hours of training using small datasets of unprocessed audio. The\nresults are promising, as both the melody and the singer's voice are\nrecognizable. In addition, we present two zero-configuration tools to train new\nmodels and experiment with them. Currently we are exploring the latent space\nrepresentation, which is included in the DDSP library, but not in the original\nDDSP examples. Our results indicate that the latent space improves both the\nidentification of the singer as well as the comprehension of the lyrics. Our\ncode is available at https://github.com/juanalonso/DDSP-singing-experiments\nwith links to the zero-configuration notebooks, and our sound examples are at\nhttps://juanalonso.github.io/DDSP-singing-experiments/ .\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:38:29 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Alonso", "Juan", ""], ["Erkut", "Cumhur", ""]]}, {"id": "2103.07390", "submitter": "Chitralekha Gupta", "authors": "Chitralekha Gupta, Purnima Kamath, Lonce Wyse", "title": "Signal Representations for Synthesizing Audio Textures with Generative\n  Adversarial Networks", "comments": "Submitted to Sound and Music Computing Conference (SMC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) currently achieve the state-of-the-art\nsound synthesis quality for pitched musical instruments using a 2-channel\nspectrogram representation consisting of log magnitude and instantaneous\nfrequency (the \"IFSpectrogram\"). Many other synthesis systems use\nrepresentations derived from the magnitude spectra, and then depend on a\nbackend component to invert the output magnitude spectrograms that generally\nresult in audible artefacts associated with the inversion process. However, for\nsignals that have closely-spaced frequency components such as non-pitched and\nother noisy sounds, training the GAN on the 2-channel IFSpectrogram\nrepresentation offers no advantage over the magnitude spectra based\nrepresentations. In this paper, we propose that training GANs on single-channel\nmagnitude spectra, and using the Phase Gradient Heap Integration (PGHI)\ninversion algorithm is a better comprehensive approach for audio synthesis\nmodeling of diverse signals that include pitched, non-pitched, and dynamically\ncomplex sounds. We show that this method produces higher-quality output for\nwideband and noisy sounds, such as pops and chirps, compared to using the\nIFSpectrogram. Furthermore, the sound quality for pitched sounds is comparable\nto using the IFSpectrogram, even while using a simpler representation with half\nthe memory requirements.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:31:20 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Gupta", "Chitralekha", ""], ["Kamath", "Purnima", ""], ["Wyse", "Lonce", ""]]}, {"id": "2103.07659", "submitter": "Donghong Gu", "authors": "Jiaqian Wang, Donghong Gu, Chi Yang, Yun Xue, Zhengxin Song, Haoliang\n  Zhao, Luwei Xiao", "title": "Targeted aspect based multimodal sentiment analysis:an attention capsule\n  extraction and multi-head fusion network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multimodal sentiment analysis has currently identified its significance in a\nvariety of domains. For the purpose of sentiment analysis, different aspects of\ndistinguishing modalities, which correspond to one target, are processed and\nanalyzed. In this work, we propose the targeted aspect-based multimodal\nsentiment analysis (TABMSA) for the first time. Furthermore, an attention\ncapsule extraction and multi-head fusion network (EF-Net) on the task of TABMSA\nis devised. The multi-head attention (MHA) based network and the ResNet-152 are\nemployed to deal with texts and images, respectively. The integration of MHA\nand capsule network aims to capture the interaction among the multimodal\ninputs. In addition to the targeted aspect, the information from the context\nand the image is also incorporated for sentiment delivered. We evaluate the\nproposed model on two manually annotated datasets. the experimental results\ndemonstrate the effectiveness of our proposed model for this new task.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 09:11:24 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Jiaqian", ""], ["Gu", "Donghong", ""], ["Yang", "Chi", ""], ["Xue", "Yun", ""], ["Song", "Zhengxin", ""], ["Zhao", "Haoliang", ""], ["Xiao", "Luwei", ""]]}, {"id": "2103.07666", "submitter": "Simeng Sun", "authors": "Simeng Sun, Tao Yu, Jiahua Xu, Jianxin Lin, Wei Zhou and Zhibo Chen", "title": "GraphIQA:Learning Distortion Graph Representations for Blind Image\n  Quality Assessment", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based blind image quality assessment (BIQA) methods have recently\ndrawn much attention for their superior performance compared to traditional\nmethods. However, most of them do not effectively leverage the relationship\nbetween distortion-related factors, showing limited feature representation\ncapacity. In this paper, we show that human perceptual quality is highly\ncorrelated with distortion type and degree, and is biased by image content in\nmost IQA systems. Based on this observation, we propose a Distortion Graph\nRepresentation (DGR) learning framework for IQA, called GraphIQA. In GraphIQA,\neach distortion is represented as a graph i.e. DGR. One can distinguish\ndistortion types by comparing different DGRs, and predict image quality by\nlearning the relationship between different distortion degrees in DGR.\nSpecifically, we develop two sub-networks to learn the DGRs: a) Type\nDiscrimination Network (TDN) that embeds DGR into a compact code for better\ndiscriminating distortion types; b) Fuzzy Prediction Network (FPN) that\nextracts the distributional characteristics of the samples in DGR and predicts\nfuzzy degrees based on a Gaussian prior. Experiments show that our GraphIQA\nachieves the state-of-the-art performance on many benchmark datasets of both\nsynthetic and authentic distortion.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 09:33:24 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sun", "Simeng", ""], ["Yu", "Tao", ""], ["Xu", "Jiahua", ""], ["Lin", "Jianxin", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2103.07883", "submitter": "Fabio Poiesi", "authors": "M. Bortolon, L. Bazzanella, F. Poiesi", "title": "Multi-view data capture for dynamic object reconstruction using handheld\n  augmented reality mobiles", "comments": "Accepted in Journal of Real-Time Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a system to capture nearly-synchronous frame streams from multiple\nand moving handheld mobiles that is suitable for dynamic object 3D\nreconstruction. Each mobile executes Simultaneous Localisation and Mapping\non-board to estimate its pose, and uses a wireless communication channel to\nsend or receive synchronisation triggers. Our system can harvest frames and\nmobile poses in real time using a decentralised triggering strategy and a\ndata-relay architecture that can be deployed either at the Edge or in the\nCloud. We show the effectiveness of our system by employing it for 3D skeleton\nand volumetric reconstructions. Our triggering strategy achieves equal\nperformance to that of an NTP-based synchronisation approach, but offers higher\nflexibility, as it can be adjusted online based on application needs. We\ncreated a challenging new dataset, namely 4DM, that involves six handheld\naugmented reality mobiles recording an actor performing sports actions\noutdoors. We validate our system on 4DM, analyse its strengths and limitations,\nand compare its modules with alternative ones.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 10:26:50 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 06:08:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bortolon", "M.", ""], ["Bazzanella", "L.", ""], ["Poiesi", "F.", ""]]}, {"id": "2103.09477", "submitter": "Uttam Kumar Mondal", "authors": "Uttam Kr. Mondal, Shamayita Pal, AmitRanjan Dutta, J.K. Mandal", "title": "A New Approach to Enhance Security of Visual Cryptography Using\n  Steganography (VisUS)", "comments": "National Conference on Next Generation Computing & Information\n  Security (NCNGCIS-2011), 25-26 March,2011,IMS,Noida", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Steganography is a process that hides secrete message or secrete hologram or\nsecrete video or secrete image whose mere presence within the source data\nshould be undetectable and use for transmitting secret information over public\nmedia. Visual cryptography is a cryptographic technique in which no\ncryptographic computation is needed at the decryption end and the decryption is\nperformed by the human visual system (HVS). In this paper, both Steganography\nand visual cryptography have been selected to provide more secure data\ntransmission over the public media with less hazard of computation. This\ntechnique generates shares with less space overhead as well as without\nincreasing the computational complexity compared to existing techniques and may\nprovide better security. It is also easy to implement like other techniques of\nvisual cryptography. Finally, experimental results are given to establish the\nsecurity criteria.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:15:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Mondal", "Uttam Kr.", ""], ["Pal", "Shamayita", ""], ["Dutta", "AmitRanjan", ""], ["Mandal", "J. K.", ""]]}, {"id": "2103.10018", "submitter": "Hailong Ning", "authors": "Hailong Ning, Xiangtao Zheng, Yuan Yuan, Xiaoqiang Lu", "title": "Audio Description from Image by Modal Translation Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.10.053", "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio is the main form for the visually impaired to obtain information. In\nreality, all kinds of visual data always exist, but audio data does not exist\nin many cases. In order to help the visually impaired people to better perceive\nthe information around them, an image-to-audio-description (I2AD) task is\nproposed to generate audio descriptions from images in this paper. To complete\nthis totally new task, a modal translation network (MT-Net) from visual to\nauditory sense is proposed. The proposed MT-Net includes three progressive\nsub-networks: 1) feature learning, 2) cross-modal mapping, and 3) audio\ngeneration. First, the feature learning sub-network aims to learn semantic\nfeatures from image and audio, including image feature learning and audio\nfeature learning. Second, the cross-modal mapping sub-network transforms the\nimage feature into a cross-modal representation with the same semantic concept\nas the audio feature. In this way, the correlation of inter-modal data is\neffectively mined for easing the heterogeneous gap between image and audio.\nFinally, the audio generation sub-network is designed to generate the audio\nwaveform from the cross-modal representation. The generated audio waveform is\ninterpolated to obtain the corresponding audio file according to the sample\nfrequency. Being the first attempt to explore the I2AD task, three large-scale\ndatasets with plenty of manual audio descriptions are built. Experiments on the\ndatasets verify the feasibility of generating intelligible audio from an image\ndirectly and the effectiveness of proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 04:48:29 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ning", "Hailong", ""], ["Zheng", "Xiangtao", ""], ["Yuan", "Yuan", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "2103.10043", "submitter": "Palash Goyal", "authors": "Saurabh Sahu and Palash Goyal", "title": "Enhancing Transformer for Video Understanding Using Gated Multi-Level\n  Attention and Temporal Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of Transformer model has led to tremendous advancements in\nsequence modeling, especially in text domain. However, the use of\nattention-based models for video understanding is still relatively unexplored.\nIn this paper, we introduce Gated Adversarial Transformer (GAT) to enhance the\napplicability of attention-based models to videos. GAT uses a multi-level\nattention gate to model the relevance of a frame based on local and global\ncontexts. This enables the model to understand the video at various\ngranularities. Further, GAT uses adversarial training to improve model\ngeneralization. We propose temporal attention regularization scheme to improve\nthe robustness of attention modules to adversarial examples. We illustrate the\nperformance of GAT on the large-scale YoutTube-8M data set on the task of video\ncategorization. We further show ablation studies along with quantitative and\nqualitative analysis to showcase the improvement.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:39:09 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Sahu", "Saurabh", ""], ["Goyal", "Palash", ""]]}, {"id": "2103.10572", "submitter": "Qiuchi Li", "authors": "Qiuchi Li, Dimitris Gkoumas, Christina Lioma, Massimo Melucci", "title": "Quantum-inspired Multimodal Fusion for Video Sentiment Analysis", "comments": "Post-print accepted by Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the crucial challenge of fusing different modalities of features\nfor multimodal sentiment analysis. Mainly based on neural networks, existing\napproaches largely model multimodal interactions in an implicit and\nhard-to-understand manner. We address this limitation with inspirations from\nquantum theory, which contains principled methods for modeling complicated\ninteractions and correlations. In our quantum-inspired framework, the word\ninteraction within a single modality and the interaction across modalities are\nformulated with superposition and entanglement respectively at different\nstages. The complex-valued neural network implementation of the framework\nachieves comparable results to state-of-the-art systems on two benchmarking\nvideo sentiment analysis datasets. In the meantime, we produce the unimodal and\nbimodal sentiment directly from the model to interpret the entangled decision.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:59:43 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 06:55:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Qiuchi", ""], ["Gkoumas", "Dimitris", ""], ["Lioma", "Christina", ""], ["Melucci", "Massimo", ""]]}, {"id": "2103.10798", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Quanwei Huang, Youbao Tang, Xingxu Yao, Jufeng Yang,\n  Guiguang Ding, Bj\\\"orn W. Schuller", "title": "Computational Emotion Analysis From Images: Recent Advances and Future\n  Directions", "comments": "Accepted chapter in the book \"Human Perception of Visual Information\n  Psychological and Computational Perspective\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are usually evoked in humans by images. Recently, extensive research\nefforts have been dedicated to understanding the emotions of images. In this\nchapter, we aim to introduce image emotion analysis (IEA) from a computational\nperspective with the focus on summarizing recent advances and suggesting future\ndirections. We begin with commonly used emotion representation models from\npsychology. We then define the key computational problems that the researchers\nhave been trying to solve and provide supervised frameworks that are generally\nused for different IEA tasks. After the introduction of major challenges in\nIEA, we present some representative methods on emotion feature extraction,\nsupervised classifier learning, and domain adaptation. Furthermore, we\nintroduce available datasets for evaluation and summarize some main results.\nFinally, we discuss some open questions and future directions that researchers\ncan pursue.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:33:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhao", "Sicheng", ""], ["Huang", "Quanwei", ""], ["Tang", "Youbao", ""], ["Yao", "Xingxu", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2103.12204", "submitter": "Long Chen", "authors": "Long Chen, Zhihong Jiang, Jun Xiao, Wei Liu", "title": "Human-like Controllable Image Captioning with Verb-specific Semantic\n  Roles", "comments": "Accepted by CVPR 2021. The code is available at:\n  https://github.com/mad-red/VSR-guided-CIC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllable Image Captioning (CIC) -- generating image descriptions\nfollowing designated control signals -- has received unprecedented attention\nover the last few years. To emulate the human ability in controlling caption\ngeneration, current CIC studies focus exclusively on control signals concerning\nobjective properties, such as contents of interest or descriptive patterns.\nHowever, we argue that almost all existing objective control signals have\noverlooked two indispensable characteristics of an ideal control signal: 1)\nEvent-compatible: all visual contents referred to in a single sentence should\nbe compatible with the described activity. 2) Sample-suitable: the control\nsignals should be suitable for a specific image sample. To this end, we propose\na new control signal for CIC: Verb-specific Semantic Roles (VSR). VSR consists\nof a verb and some semantic roles, which represents a targeted activity and the\nroles of entities involved in this activity. Given a designated VSR, we first\ntrain a grounded semantic role labeling (GSRL) model to identify and ground all\nentities for each role. Then, we propose a semantic structure planner (SSP) to\nlearn human-like descriptive semantic structures. Lastly, we use a role-shift\ncaptioning model to generate the captions. Extensive experiments and ablations\ndemonstrate that our framework can achieve better controllability than several\nstrong baselines on two challenging CIC benchmarks. Besides, we can generate\nmulti-level diverse captions easily. The code is available at:\nhttps://github.com/mad-red/VSR-guided-CIC.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:17:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Long", ""], ["Jiang", "Zhihong", ""], ["Xiao", "Jun", ""], ["Liu", "Wei", ""]]}, {"id": "2103.12456", "submitter": "Xuan Ma", "authors": "Xuan Ma, Xiaoshan Yang, Junyu Gao, and Changsheng Xu", "title": "Health Status Prediction with Local-Global Heterogeneous Behavior Graph", "comments": null, "journal-ref": null, "doi": "10.1145/3457893", "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health management is getting increasing attention all over the world.\nHowever, existing health management mainly relies on hospital examination and\ntreatment, which are complicated and untimely. The emerging of mobile devices\nprovides the possibility to manage people's health status in a convenient and\ninstant way. Estimation of health status can be achieved with various kinds of\ndata streams continuously collected from wearable sensors. However, these data\nstreams are multi-source and heterogeneous, containing complex temporal\nstructures with local contextual and global temporal aspects, which makes the\nfeature learning and data joint utilization challenging. We propose to model\nthe behavior-related multi-source data streams with a local-global graph, which\ncontains multiple local context sub-graphs to learn short term local context\ninformation with heterogeneous graph neural networks and a global temporal\nsub-graph to learn long term dependency with self-attention networks. Then\nhealth status is predicted based on the structure-aware representation learned\nfrom the local-global behavior graph. We take experiments on StudentLife\ndataset, and extensive results demonstrate the effectiveness of our proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:10:04 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ma", "Xuan", ""], ["Yang", "Xiaoshan", ""], ["Gao", "Junyu", ""], ["Xu", "Changsheng", ""]]}, {"id": "2103.12516", "submitter": "Zhidu Li", "authors": "Dapeng Wu, Ruili Bao, Zhidu Li, Honggang Wang, Ruyan Wang", "title": "Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid\n  Human-Artificial Intelligence Scheme", "comments": "This paper has been submitted to IEEE Transactions on Multimedia for\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a video service enhancement strategy is investigated under an\nedge-cloud collaboration framework, where video caching and delivery decisions\nare made in the cloud and edge respectively. We aim to guarantee the user\nfairness in terms of video coding rate under statistical delay constraint and\nedge caching capacity constraint. A hybrid human-artificial intelligence\napproach is developed to improve the user hit rate for video caching.\nSpecifically, individual user interest is first characterized by merging\nfactorization machine (FM) model and multi-layer perceptron (MLP) model, where\nboth low-order and high-order features can be well learned simultaneously.\nThereafter, a social aware similarity model is constructed to transferred\nindividual user interest to group interest, based on which, videos can be\nselected to cache. Furthermore, a double bisection exploration scheme is\nproposed to optimize wireless resource allocation and video coding rate. The\neffectiveness of the proposed video caching scheme and video delivery scheme is\nfinally validated by extensive experiments with a real-world data set.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 13:34:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wu", "Dapeng", ""], ["Bao", "Ruili", ""], ["Li", "Zhidu", ""], ["Wang", "Honggang", ""], ["Wang", "Ruyan", ""]]}, {"id": "2103.12541", "submitter": "Preslav Nakov", "authors": "Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri,\n  Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed Firooz,\n  Preslav Nakov", "title": "A Survey on Multimodal Disinformation Detection", "comments": "disinformation, misinformation, factuality, harmfulness, fake news,\n  propaganda, multimodality, text, images, videos, network structure,\n  temporality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CL cs.CR cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the proliferation of fake news, propaganda,\nmisinformation, and disinformation online. While initially this was mostly\nabout textual content, over time images and videos gained popularity, as they\nare much easier to consume, attract much more attention, and spread further\nthan simple text. As a result, researchers started targeting different\nmodalities and combinations thereof. As different modalities are studied in\ndifferent research communities, with insufficient interaction, here we offer a\nsurvey that explores the state-of-the-art on multimodal disinformation\ndetection covering various combinations of modalities: text, images, audio,\nvideo, network structure, and temporal information. Moreover, while some\nstudies focused on factuality, others investigated how harmful the content is.\nWhile these two components in the definition of disinformation -- (i)\nfactuality and (ii) harmfulness, are equally important, they are typically\nstudied in isolation. Thus, we argue for the need to tackle disinformation\ndetection by taking into account multiple modalities as well as both factuality\nand harmfulness, in the same framework. Finally, we discuss current challenges\nand future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:04:17 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Alam", "Firoj", ""], ["Cresci", "Stefano", ""], ["Chakraborty", "Tanmoy", ""], ["Silvestri", "Fabrizio", ""], ["Dimitrov", "Dimiter", ""], ["Martino", "Giovanni Da San", ""], ["Shaar", "Shaden", ""], ["Firooz", "Hamed", ""], ["Nakov", "Preslav", ""]]}, {"id": "2103.13477", "submitter": "Zijian Kuang", "authors": "Zijian Kuang and Xinran Tie", "title": "A Survey of Multimedia Technologies and Robust Algorithms", "comments": "arXiv admin note: text overlap with arXiv:2010.12968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia technologies are now more practical and deployable in real life,\nand the algorithms are widely used in various researching areas such as deep\nlearning, signal processing, haptics, computer vision, robotics, and medical\nmultimedia processing. This survey provides an overview of multimedia\ntechnologies and robust algorithms in multimedia data processing, medical\nmultimedia processing, human facial expression tracking and pose recognition,\nand multimedia in education and training. This survey will also analyze and\npropose a future research direction based on the overview of current robust\nalgorithms and multimedia technologies. We want to thank the research and\nprevious work done by the Multimedia Research Centre (MRC), the University of\nAlberta, which is the inspiration and starting point for future research.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 20:52:23 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 02:49:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kuang", "Zijian", ""], ["Tie", "Xinran", ""]]}, {"id": "2103.13674", "submitter": "Seung-Hun Nam", "authors": "Minseok Yoon, Seung-Hun Nam, In-Jae Yu, Wonhyuk Ahn, Myung-Joon Kwon,\n  Heung-Kyu Lee", "title": "Frame-rate Up-conversion Detection Based on Convolutional Neural Network\n  for Learning Spatiotemporal Features", "comments": "preprint; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance in user-friendly and powerful video editing tools, anyone\ncan easily manipulate videos without leaving prominent visual traces.\nFrame-rate up-conversion (FRUC), a representative temporal-domain operation,\nincreases the motion continuity of videos with a lower frame-rate and is used\nby malicious counterfeiters in video tampering such as generating fake\nframe-rate video without improving the quality or mixing temporally spliced\nvideos. FRUC is based on frame interpolation schemes and subtle artifacts that\nremain in interpolated frames are often difficult to distinguish. Hence,\ndetecting such forgery traces is a critical issue in video forensics. This\npaper proposes a frame-rate conversion detection network (FCDNet) that learns\nforensic features caused by FRUC in an end-to-end fashion. The proposed network\nuses a stack of consecutive frames as the input and effectively learns\ninterpolation artifacts using network blocks to learn spatiotemporal features.\nThis study is the first attempt to apply a neural network to the detection of\nFRUC. Moreover, it can cover the following three types of frame interpolation\nschemes: nearest neighbor interpolation, bilinear interpolation, and\nmotion-compensated interpolation. In contrast to existing methods that exploit\nall frames to verify integrity, the proposed approach achieves a high detection\nspeed because it observes only six frames to test its authenticity. Extensive\nexperiments were conducted with conventional forensic methods and neural\nnetworks for video forensic tasks to validate our research. The proposed\nnetwork achieved state-of-the-art performance in terms of detecting the\ninterpolated artifacts of FRUC. The experimental results also demonstrate that\nour trained model is robust for an unseen dataset, unlearned frame-rate, and\nunlearned quality factor.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:47:46 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yoon", "Minseok", ""], ["Nam", "Seung-Hun", ""], ["Yu", "In-Jae", ""], ["Ahn", "Wonhyuk", ""], ["Kwon", "Myung-Joon", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2103.13689", "submitter": "Shunquan Tan", "authors": "Xianbo Mo and Shunquan Tan and Bin Li and Jiwu Huang", "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning\n  Framework for Universal Non-additive Steganography", "comments": "submitted to TIFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:12:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Mo", "Xianbo", ""], ["Tan", "Shunquan", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "2103.14247", "submitter": "Dewang Hou", "authors": "Dewang Hou, Yang Zhao, Yuyao Ye, Jiayu Yang, Jian Zhang, Ronggang Wang", "title": "Super-Resolving Compressed Video in Coding Chain", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling and lossy coding are widely used in video transmission and storage.\nPrevious methods for enhancing the resolution of such videos often ignore the\ninherent interference between resolution loss and compression artifacts, which\ncompromises perceptual video quality. To address this problem, we present a\nmixed-resolution coding framework, which cooperates with a reference-based\nDCNN. In this novel coding chain, the reference-based DCNN learns the direct\nmapping from low-resolution (LR) compressed video to their high-resolution (HR)\nclean version at the decoder side. We further improve reconstruction quality by\ndevising an efficient deformable alignment module with receptive field block to\nhandle various motion distances and introducing a disentangled loss that helps\nnetworks distinguish the artifact patterns from texture. Extensive experiments\ndemonstrate the effectiveness of proposed innovations by comparing with\nstate-of-the-art single image, video and reference-based restoration methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:39:54 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hou", "Dewang", ""], ["Zhao", "Yang", ""], ["Ye", "Yuyao", ""], ["Yang", "Jiayu", ""], ["Zhang", "Jian", ""], ["Wang", "Ronggang", ""]]}, {"id": "2103.14431", "submitter": "Zihui Xue", "authors": "Zihui Xue, Sucheng Ren, Zhengqi Gao and Hang Zhao", "title": "Multimodal Knowledge Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of multimodal sensors and the accessibility of the Internet\nhave brought us a massive amount of unlabeled multimodal data. Since existing\ndatasets and well-trained models are primarily unimodal, the modality gap\nbetween a unimodal network and unlabeled multimodal data poses an interesting\nproblem: how to transfer a pre-trained unimodal network to perform the same\ntask on unlabeled multimodal data? In this work, we propose multimodal\nknowledge expansion (MKE), a knowledge distillation-based framework to\neffectively utilize multimodal data without requiring labels. Opposite to\ntraditional knowledge distillation, where the student is designed to be\nlightweight and inferior to the teacher, we observe that a multimodal student\nmodel consistently denoises pseudo labels and generalizes better than its\nteacher. Extensive experiments on four tasks and different modalities verify\nthis finding. Furthermore, we connect the mechanism of MKE to semi-supervised\nlearning and offer both empirical and theoretical explanations to understand\nthe denoising capability of a multimodal student.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:32:07 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:10:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xue", "Zihui", ""], ["Ren", "Sucheng", ""], ["Gao", "Zhengqi", ""], ["Zhao", "Hang", ""]]}, {"id": "2103.14853", "submitter": "Gavin Buckingham", "authors": "Gavin Buckingham", "title": "Hand tracking for immersive virtual reality: opportunities and\n  challenges", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand tracking has become an integral feature of recent generations of\nimmersive virtual reality head-mounted displays. With the widespread adoption\nof this feature, hardware engineers and software developers are faced with an\nexciting array of opportunities and a number of challenges, mostly in relation\nto the human user. In this article, I outline what I see as the main\npossibilities for hand tracking to add value to immersive virtual reality as\nwell as some of the potential challenges in the context of the psychology and\nneuroscience of the human user. It is hoped that this paper serves as a roadmap\nfor the development of best practices in the field for the development of\nsubsequent generations of hand tracking and virtual reality technologies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:28:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Buckingham", "Gavin", ""]]}, {"id": "2103.15602", "submitter": "Pan Wang", "authors": "Pan Wang, Zhifeng Gong, Shuo Wang, Hao Dong, Jialu Fan, Ling Li, Peter\n  Childs and Yike Guo", "title": "Product semantics translation from brain activity via adversarial\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A small change of design semantics may affect a user's satisfaction with a\nproduct. To modify a design semantic of a given product from personalised brain\nactivity via adversarial learning, in this work, we propose a deep generative\ntransformation model to modify product semantics from the brain signal. We\nattempt to accomplish such synthesis: 1) synthesising the product image with\nnew features corresponding to EEG signal; 2) maintaining the other image\nfeatures that irrelevant to EEG signal. We leverage the idea of StarGAN and the\nmodel is designed to synthesise products with preferred design semantics\n(colour & shape) via adversarial learning from brain activity, and is applied\nwith a case study to generate shoes with different design semantics from\nrecorded EEG signals. To verify our proposed cognitive transformation model, a\ncase study has been presented. The results work as a proof-of-concept that our\nframework has the potential to synthesis product semantic from brain activity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:27:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Pan", ""], ["Gong", "Zhifeng", ""], ["Wang", "Shuo", ""], ["Dong", "Hao", ""], ["Fan", "Jialu", ""], ["Li", "Ling", ""], ["Childs", "Peter", ""], ["Guo", "Yike", ""]]}, {"id": "2103.15686", "submitter": "Kecheng Zheng", "authors": "Rui Zhao, Kecheng Zheng, Zheng-Jun Zha, Hongtao Xie and Jiebo Luo", "title": "Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal video-text retrieval, a challenging task in the field of vision\nand language, aims at retrieving corresponding instance giving sample from\neither modality. Existing approaches for this task all focus on how to design\nencoding model through a hard negative ranking loss, leaving two key problems\nunaddressed during this procedure. First, in the training stage, only a\nmini-batch of instance pairs is available in each iteration. Therefore, this\nkind of hard negatives is locally mined inside a mini-batch while ignoring the\nglobal negative samples among the dataset. Second, there are many text\ndescriptions for one video and each text only describes certain local features\nof a video. Previous works for this task did not consider to fuse the multiply\ntexts corresponding to a video during the training. In this paper, to solve the\nabove two problems, we propose a novel memory enhanced embedding learning\n(MEEL) method for videotext retrieval. To be specific, we construct two kinds\nof memory banks respectively: cross-modal memory module and text center memory\nmodule. The cross-modal memory module is employed to record the instance\nembeddings of all the datasets for global negative mining. To avoid the fast\nevolving of the embedding in the memory bank during training, we utilize a\nmomentum encoder to update the features by a moving-averaging strategy. The\ntext center memory module is designed to record the center information of the\nmultiple textual instances corresponding to a video, and aims at bridging these\ntextual instances together. Extensive experimental results on two challenging\nbenchmarks, i.e., MSR-VTT and VATEX, demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:15:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Rui", ""], ["Zheng", "Kecheng", ""], ["Zha", "Zheng-Jun", ""], ["Xie", "Hongtao", ""], ["Luo", "Jiebo", ""]]}, {"id": "2103.16079", "submitter": "Weiping Zheng", "authors": "Weiping Zheng, Dacan Jiang, Gansen Zhao", "title": "Environmental sound analysis with mixup based multitask learning and\n  cross-task fusion", "comments": "5 pages, 1 figue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Environmental sound analysis is currently getting more and more attentions.\nIn the domain, acoustic scene classification and acoustic event classification\nare two closely related tasks. In this letter, a two-stage method is proposed\nfor the above tasks. In the first stage, a mixup based MTL solution is proposed\nto classify both tasks in one single convolutional neural network. Artificial\nmulti-label samples are used in the training of the MTL model, which are mixed\nup using existing single-task datasets. The multi-task model obtained can\neffectively recognize both the acoustic scenes and events. Compared with other\nmethods such as re-annotation or synthesis, the mixup based MTL is low-cost,\nflexible and effective. In the second stage, the MTL model is modified into a\nsingle-task model which is fine-tuned using the original dataset corresponding\nto the specific task. By controlling the frozen layers carefully, the\ntask-specific high level features are fused and the performance of the single\nclassification task is further improved. The proposed method has confirmed the\ncomplementary characteristics of acoustic scene and acoustic event\nclassifications. Finally, enhanced by ensemble learning, a satisfactory\naccuracy of 84.5 percent on TUT acoustic scene 2017 dataset and an accuracy of\n77.5 percent on ESC-50 dataset are achieved respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:11:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zheng", "Weiping", ""], ["Jiang", "Dacan", ""], ["Zhao", "Gansen", ""]]}, {"id": "2103.16510", "submitter": "Cagatay Basdogan", "authors": "Senem Ezgi Emgin, Amirreza Aghakhani, T. Metin Sezgin, and Cagatay\n  Basdogan", "title": "HapTable: An Interactive Tabletop Providing Online Haptic Feedback for\n  Touch Gestures", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019,\n  Vol. 25, No. 9, pp. 2749-2762", "doi": "10.1109/TVCG.2018.2855154", "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HapTable; a multimodal interactive tabletop that allows users to\ninteract with digital images and objects through natural touch gestures, and\nreceive visual and haptic feedback accordingly. In our system, hand pose is\nregistered by an infrared camera and hand gestures are classified using a\nSupport Vector Machine (SVM) classifier. To display a rich set of haptic\neffects for both static and dynamic gestures, we integrated electromechanical\nand electrostatic actuation techniques effectively on tabletop surface of\nHapTable, which is a surface capacitive touch screen. We attached four piezo\npatches to the edges of tabletop to display vibrotactile feedback for static\ngestures. For this purpose, the vibration response of the tabletop, in the form\nof frequency response functions (FRFs), was obtained by a laser Doppler\nvibrometer for 84 grid points on its surface. Using these FRFs, it is possible\nto display localized vibrotactile feedback on the surface for static gestures.\nFor dynamic gestures, we utilize the electrostatic actuation technique to\nmodulate the frictional forces between finger skin and tabletop surface by\napplying voltage to its conductive layer. Here, we present two examples of such\napplications, one for static and one for dynamic gestures, along with detailed\nuser studies. In the first one, user detects the direction of a virtual flow,\nsuch as that of wind or water, by putting their hand on the tabletop surface\nand feeling a vibrotactile stimulus traveling underneath it. In the second\nexample, user rotates a virtual knob on the tabletop surface to select an item\nfrom a menu while feeling the knob's detents and resistance to rotation in the\nform of frictional haptic feedback.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:12:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Emgin", "Senem Ezgi", ""], ["Aghakhani", "Amirreza", ""], ["Sezgin", "T. Metin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16518", "submitter": "Cagatay Basdogan", "authors": "Bushra Sadia, Senem Ezgi Emgin, T. Metin Sezgin, Cagatay Basdogan", "title": "Data-Driven Vibrotactile Rendering of Digital Buttons on Touchscreens", "comments": null, "journal-ref": "International Journal of Human-Computer Studies, 2020, Vol. 135,\n  102363", "doi": "10.1016/j.ijhcs.2019.09.005", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although mobile phones incorporate low-cost vibration motors to enhance\ntouch-based interactions, it is not possible to generate complex tactile\neffects on their touchscreens. It is also difficult to relate the limited\nvibrotactile feedback generated by these motors to different types of physical\nbuttons. In this study, we focus on creating vibrotactile feedback on a\ntouchscreen that simulates the feeling of physical buttons using piezo\nactuators attached to it. We first recorded and analyzed the force,\nacceleration, and voltage data from twelve participants interacting with three\ndifferent physical buttons: latch, toggle, and push buttons. Then, a\nbutton-specific vibrotactile stimulus was generated for each button based on\nthe recorded data. Finally, we conducted a threealternative forced choice\n(3AFC) experiment with twenty participants to explore whether the resultant\nstimulus is distinct and realistic. In our experiment, participants were able\nto match the three digital buttons with their physical counterparts with a\nsuccess rate of 83%. In addition, we harvested seven adjective pairs from the\nparticipants expressing their perceptual feeling of pressing the physical\nbuttons. All twenty participants rated the degree of their subjective feelings\nassociated with each adjective for all the physical and digital buttons\ninvestigated in this study. Our statistical analysis showed that there exist at\nleast three adjective pairs for which participants have rated two out of three\ndigital buttons similar to their physical counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:21:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sadia", "Bushra", ""], ["Emgin", "Senem Ezgi", ""], ["Sezgin", "T. Metin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16828", "submitter": "Wing Yin Yu", "authors": "Wing-Yin Yu, Lai-Man Po, Yuzhi Zhao, Jingjing Xiong, Kin-Wai Lau", "title": "Spatial Content Alignment For Pose Transfer", "comments": "IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to unreliable geometric matching and content misalignment, most\nconventional pose transfer algorithms fail to generate fine-trained person\nimages. In this paper, we propose a novel framework Spatial Content Alignment\nGAN (SCAGAN) which aims to enhance the content consistency of garment textures\nand the details of human characteristics. We first alleviate the spatial\nmisalignment by transferring the edge content to the target pose in advance.\nSecondly, we introduce a new Content-Style DeBlk which can progressively\nsynthesize photo-realistic person images based on the appearance features of\nthe source image, the target pose heatmap and the prior transferred content in\nedge domain. We compare the proposed framework with several state-of-the-art\nmethods to show its superiority in quantitative and qualitative analysis.\nMoreover, detailed ablation study results demonstrate the efficacy of our\ncontributions. Codes are publicly available at\ngithub.com/rocketappslab/SCA-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:10:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Wing-Yin", ""], ["Po", "Lai-Man", ""], ["Zhao", "Yuzhi", ""], ["Xiong", "Jingjing", ""], ["Lau", "Kin-Wai", ""]]}, {"id": "2103.16932", "submitter": "Weng-Tai Su", "authors": "Weng-tai Su, Ta-Hsuan Chao, Shang-Hua Yang and Chia-Wen Lin", "title": "Seeing through a Black Box: Toward High-Quality Terahertz\n  TomographicImaging via Multi-Scale Spatio-Spectral Image Fusion", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Terahertz tomographic imaging has recently arisen significant attention due\nto its non-invasive, non-destructive, non-ionizing, material-classification,\nand ultrafast-frame-rate nature for object exploration and inspection. However,\nits strong water absorption nature and low noise tolerance lead to undesired\nblurring and distortion of reconstructed terahertz images. Research groups aim\nto deal with this issue through the use of synthetic data in the training\nphase, but still, their performances are highly constrained by the\ndiffraction-limited terahertz signals. In this paper, we propose a novel\nmulti-scale spatio-spectral fusion Unet (MS3-Unet) that extracts multi-scale\nfeatures from the different spectral of terahertz image data for restoration.\nMS3-Unet utilizes multi-scale branches to extract spatio-spectral features\nwhich are then processed by element-wise adaptive filters, and then fused to\nachieve high-quality terahertz image restoration. Here, we experimentally\nconstruct ultra-high-speed terahertz time-domain spectroscopy system covering a\nbroad frequency range from 0.1 THz to 4 THz for building up\ntemporal/spectral/spatial/phase/material terahertz database of hidden 3-D\nobjects. Complementary to a quantitative evaluation, we demonstrate the\neffectiveness of the proposed MS3-Unet image restoration approach on 3-D\nterahertz tomographic reconstruction applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:29:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Su", "Weng-tai", ""], ["Chao", "Ta-Hsuan", ""], ["Yang", "Shang-Hua", ""], ["Lin", "Chia-Wen", ""]]}]