[{"id": "1908.00375", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang and Ming Jiang", "title": "Quality Assessment of In-the-Wild Videos", "comments": "9 pages, 7 figures, 4 tables. ACM Multimedia 2019 camera ready. ->\n  Update alignment formatting of Table 1", "journal-ref": null, "doi": "10.1145/3343031.3351028", "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment of in-the-wild videos is a challenging problem because of\nthe absence of reference videos and shooting distortions. Knowledge of the\nhuman visual system can help establish methods for objective quality assessment\nof in-the-wild videos. In this work, we show two eminent effects of the human\nvisual system, namely, content-dependency and temporal-memory effects, could be\nused for this purpose. We propose an objective no-reference video quality\nassessment method by integrating both effects into a deep neural network. For\ncontent-dependency, we extract features from a pre-trained image classification\nneural network for its inherent content-aware property. For temporal-memory\neffects, long-term dependencies, especially the temporal hysteresis, are\nintegrated into the network with a gated recurrent unit and a\nsubjectively-inspired temporal pooling layer. To validate the performance of\nour method, experiments are conducted on three publicly available in-the-wild\nvideo quality assessment databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm,\nrespectively. Experimental results demonstrate that our proposed method\noutperforms five state-of-the-art methods by a large margin, specifically,\n12.39%, 15.71%, 15.45%, and 18.09% overall performance improvements over the\nsecond-best method VBLIINDS, in terms of SROCC, KROCC, PLCC and RMSE,\nrespectively. Moreover, the ablation study verifies the crucial role of both\nthe content-aware features and the modeling of temporal-memory effects. The\nPyTorch implementation of our method is released at\nhttps://github.com/lidq92/VSFA.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:08:04 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 07:16:53 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 14:31:25 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "1908.00812", "submitter": "Eirina Bourtsoulatze", "authors": "Eirina Bourtsoulatze, Aaron Chadha, Ilya Fadeev, Vasileios Giotsas,\n  Yiannis Andreopoulos", "title": "Deep Video Precoding", "comments": "16 pages, 14 figures, 11 tables, to appear in IEEE Trans. Circ. Syst.\n  for Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several groups are currently investigating how deep learning may advance the\nstate-of-the-art in image and video coding. An open question is how to make\ndeep neural networks work in conjunction with existing (and upcoming) video\ncodecs, such as MPEG AVC, HEVC, VVC, Google VP9 and AOM AV1, as well as\nexisting container and transport formats, without imposing any changes at the\nclient side. Such compatibility is a crucial aspect when it comes to practical\ndeployment, especially due to the fact that the video content industry and\nhardware manufacturers are expected to remain committed to these standards for\nthe foreseeable future. We propose to use deep neural networks as precoders for\ncurrent and future video codecs and adaptive video streaming systems. In our\ncurrent design, the core precoding component comprises a cascaded structure of\ndownscaling neural networks that operates during video encoding, prior to\ntransmission. This is coupled with a precoding mode selection algorithm for\neach independently-decodable stream segment, which adjusts the downscaling\nfactor according to scene characteristics, the utilized encoder, and the\ndesired bitrate and encoding configuration. Our framework is compatible with\nall current and future codec and transport standards, as our deep precoding\nnetwork structure is trained in conjunction with linear upscaling filters\n(e.g., the bilinear filter), which are supported by all web video players.\nResults with FHD and UHD content and widely-used AVC, HEVC and VP9 encoders\nshow that coupling such standards with the proposed deep video precoding allows\nfor 15% to 45% rate reduction under encoding configurations and bitrates\nsuitable for video-on-demand adaptive streaming systems. The use of precoding\ncan also lead to encoding complexity reduction, which is essential for\ncost-effective cloud deployment of complex encoders like H.265/HEVC and VP9.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 11:44:14 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 20:10:34 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bourtsoulatze", "Eirina", ""], ["Chadha", "Aaron", ""], ["Fadeev", "Ilya", ""], ["Giotsas", "Vasileios", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1908.00902", "submitter": "Flip Phillips", "authors": "J. Farley Norman, James T. Todd, Flip Phillips", "title": "Effects of Illumination on the Categorization of Shiny Materials", "comments": "v2, 20 pages, 15 figures, 26 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present research was designed to examine how patterns of illumination\ninfluence the perceptual categorization of metal, shiny black, and shiny white\nmaterials. The stimuli depicted three possible objects that were illuminated by\nfive possible HDRI light maps, which varied in their overall distributions of\nilluminant directions and intensities. The surfaces included a low roughness\nchrome material, a shiny black material, and a shiny white material with both\ndiffuse and specular components. Observers rated each stimulus by adjusting\nfour sliders to indicate their confidence that the depicted material was metal,\nshiny black, shiny white or something else, and these adjustments were\nconstrained so that the sum of all four settings was always 100%. The results\nrevealed that the metal and shiny black categories are easily confused. For\nexample, metal materials with low intensity light maps or a narrow range of\nilluminant directions are often judged as shiny black, whereas shiny black\nmaterials with high intensity light maps or a wide range of illuminant\ndirections are often judged as metal. A spherical harmonic analysis was\nperformed on the different light maps in an effort to quantitatively predict\nhow they would bias observers' judgments of metal and shiny black surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 15:19:54 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 16:22:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Norman", "J. Farley", ""], ["Todd", "James T.", ""], ["Phillips", "Flip", ""]]}, {"id": "1908.00941", "submitter": "Qiuqiang Kong", "authors": "Jie Jiang, Qiuqiang Kong, Mark Plumbley, Nigel Gilbert", "title": "Deep Learning Based Energy Disaggregation and On/Off Detection of\n  Household Appliances", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation, a.k.a. Non-Intrusive Load Monitoring, aims to separate\nthe energy consumption of individual appliances from the readings of a mains\npower meter measuring the total energy consumption of, e.g. a whole house.\nEnergy consumption of individual appliances can be useful in many applications,\ne.g., providing appliance-level feedback to the end users to help them\nunderstand their energy consumption and ultimately save energy. Recently, with\nthe availability of large-scale energy consumption datasets, various neural\nnetwork models such as convolutional neural networks and recurrent neural\nnetworks have been investigated to solve the energy disaggregation problem.\nNeural network models can learn complex patterns from large amounts of data and\nhave been shown to outperform the traditional machine learning methods such as\nvariants of hidden Markov models. However, current neural network methods for\nenergy disaggregation are either computational expensive or are not capable of\nhandling long-term dependencies. In this paper, we investigate the application\nof the recently developed WaveNet models for the task of energy disaggregation.\nBased on a real-world energy dataset collected from 20 households over two\nyears, we show that WaveNet models outperforms the state-of-the-art deep\nlearning methods proposed in the literature for energy disaggregation in terms\nof both error measures and computational cost. On the basis of energy\ndisaggregation, we then investigate the performance of two deep-learning based\nframeworks for the task of on/off detection which aims at estimating whether an\nappliance is in operation or not. Based on the same dataset, we show that for\nthe task of on/off detection the second framework, i.e., directly training a\nbinary classifier, achieves better performance in terms of F1 score.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 02:23:35 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 09:20:42 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Jiang", "Jie", ""], ["Kong", "Qiuqiang", ""], ["Plumbley", "Mark", ""], ["Gilbert", "Nigel", ""]]}, {"id": "1908.01277", "submitter": "Yalong Yang", "authors": "Linda Woodburn, Yalong Yang and Kim Marriott", "title": "Interactive Visualisation of Hierarchical Quantitative Data: An\n  Evaluation", "comments": "Presented at IEEE VIS 2019 in Vancouver, Canada and included in the\n  VIS 2019 conference proceedings. Improved the image quality in the paper", "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933545", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have compared three common visualisations for hierarchical quantitative\ndata, treemaps, icicle plots and sunburst charts as well as a semicircular\nvariant of sunburst charts we call the sundown chart. In a pilot study, we\nfound that the sunburst chart was least preferred. In a controlled study with\n12 participants, we compared treemaps, icicle plots and sundown charts. Treemap\nwas the least preferred and had a slower performance on a basic navigation task\nand slower performance and accuracy in hierarchy understanding tasks. The\nicicle plot and sundown chart had similar performance with slight user\npreference for the icicle plot.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 06:16:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 19:33:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 22:10:55 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Woodburn", "Linda", ""], ["Yang", "Yalong", ""], ["Marriott", "Kim", ""]]}, {"id": "1908.01483", "submitter": "Wenkang Su", "authors": "Wenkang Su, Jiangqun Ni, Yuanfeng Pan, Xianglei Hu and Yun-Qing Shi", "title": "Image Steganography using Gaussian Markov Random Field Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on adaptive steganography show that the performance of image\nsteganographic communication can be improved by incorporating the non-additive\nmodels that capture the dependences among adjacent pixels. In this paper, a\nGaussian Markov Random Field model (GMRF) with four-element cross neighborhood\nis proposed to characterize the interactions among local elements of cover\nimages, and the problem of secure image steganography is formulated as the one\nof minimization of KL-divergence in terms of a series of low-dimensional clique\nstructures associated with GMRF by taking advantages of the conditional\nindependence of GMRF. The adoption of the proposed GMRF tessellates the cover\nimage into two disjoint subimages, and an alternating iterative optimization\nscheme is developed to effectively embed the given payload while minimizing the\ntotal KL-divergence between cover and stego, i.e., the statistical\ndetectability. Experimental results demonstrate that the proposed GMRF\noutperforms the prior arts of model based schemes, e.g., MiPOD, and rivals the\nstate-of-the-art HiLL for practical steganography, where the selection channel\nknowledges are unavailable to steganalyzers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 06:22:39 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Su", "Wenkang", ""], ["Ni", "Jiangqun", ""], ["Pan", "Yuanfeng", ""], ["Hu", "Xianglei", ""], ["Shi", "Yun-Qing", ""]]}, {"id": "1908.01947", "submitter": "Wenkang Su", "authors": "Wenkang Su, Jiangqun Ni, Xianglei Hu, Jiwu Huang", "title": "New Design Paradigm of Distortion Cost Function for Efficient JPEG\n  Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the introduction of JPEG phase-aware steganalysis features,\ne.g., GFR, the design of JPEG steganographic distortion cost function turns to\nmaintain not only the statistical undetectability in DCT domain but also in\nspatial domain. To tackle this issue, this paper presents a novel paradigm for\nthe design of JPEG steganographic distortion cost function, which calculates\nthe distortion cost via a generalized Distortion Cost Domain Transformation\n(DCDT) function. The proposed function comprises the decompressed pixel block\nembedding changes and their corresponding embedding distortion costs for unit\nchange, where the pixel embedding distortion costs are represented in a more\ngeneral exponential model, aiming to flexibly allocate the embedding data. In\nthis way, the JPEG steganography could be formulated as the optimization\nproblem of minimizing the overall distortion cost in its decompressed spatial\ndomain, which is equivalent to maximizing its statistical undetectability\nagainst JPEG phase-aware steganalysis features. Experimental results show that\nthe proposed DCDT equipped with HiLL (a spatial steganographic distortion cost\nfunction) is superior to other state-of-the-art JPEG steganographic schemes,\ne.g., UERD, J-UNIWARD, and GUED in resisting the detection of JPEG phase-aware\nfeature-based steganalyzers GFR and SCA-GFR, and rivals BET-HiLL with one order\nof magnitude lower computational complexity, along with the possibility of\nbeing further improved by considering the mutually dependent embedding\ninteractions. In addition, the proposed DCDT is also verified to be effective\nfor different image databases and quality factors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 04:06:29 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 11:14:34 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 04:50:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Su", "Wenkang", ""], ["Ni", "Jiangqun", ""], ["Hu", "Xianglei", ""], ["Huang", "Jiwu", ""]]}, {"id": "1908.01970", "submitter": "Yiqun Xu", "authors": "Yiqun Xu, Wei Hu, Shanshe Wang, Xinfeng Zhang, Shiqi Wang, Siwei Ma,\n  Zongming Guo, Wen Gao", "title": "Predictive Generalized Graph Fourier Transform for Attribute Compression\n  of Dynamic Point Clouds", "comments": "14 pages, 12 figures, accepted to IEEE Transactions on Circuits and\n  Systems for Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D scanning devices and depth sensors advance, dynamic point clouds have\nattracted increasing attention as a format for 3D objects in motion, with\napplications in various fields such as immersive telepresence, navigation for\nautonomous driving and gaming. Nevertheless, the tremendous amount of data in\ndynamic point clouds significantly burden transmission and storage. To this\nend, we propose a complete compression framework for attributes of 3D dynamic\npoint clouds, focusing on optimal inter-coding. Firstly, we derive the optimal\ninter-prediction and predictive transform coding assuming the Gaussian Markov\nRandom Field model with respect to a spatio-temporal graph underlying the\nattributes of dynamic point clouds. The optimal predictive transform proves to\nbe the Generalized Graph Fourier Transform in terms of spatio-temporal\ndecorrelation. Secondly, we propose refined motion estimation via efficient\nregistration prior to inter-prediction, which searches the temporal\ncorrespondence between adjacent frames of irregular point clouds. Finally, we\npresent a complete framework based on the optimal inter-coding and our\npreviously proposed intra-coding, where we determine the optimal coding mode\nfrom rate-distortion optimization with the proposed offline-trained $\\lambda$-Q\nmodel. Experimental results show that we achieve around 17% bit rate reduction\non average over competitive dynamic point cloud compression methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:59:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 13:31:36 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 03:41:09 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2020 09:04:50 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xu", "Yiqun", ""], ["Hu", "Wei", ""], ["Wang", "Shanshe", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shiqi", ""], ["Ma", "Siwei", ""], ["Guo", "Zongming", ""], ["Gao", "Wen", ""]]}, {"id": "1908.02039", "submitter": "Romain Artru", "authors": "Romain Artru, Alexandre Gouaillard, Touradj Ebrahimi", "title": "Digital Watermarking of video streams: Review of the State-Of-The-Art", "comments": "33 pages, 11 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Watermarking is an extremely wide aspect of information security,\neither by its applications, by its properties, or by its designs. In\nparticular, a lot of research has been made about video watermarking and it can\nmake it quite difficult to put into perspective the various schemes possible in\norder to implement a watermarking process for a given application. This paper\npresents an in-depth overview of the current video watermarking technologies\nand how they each respond to certain criteria that may be imposed by the aimed\napplication. The goal being in first place to be able to define the desired\nequilibrium point between invisibility, robustness and efficiency for an\napplication. Then, given this balance, being able to deduce the best location\nof the information embedding as well as the method used to embed it. The\nequilibrium point is to be found using the needed properties of the watermark\nand by studying the threat model that the scheme will have to face. The\nlocation describes whether the extra information should be added to the\nmetadata of the video, to its frames or to specific regions of its frames.\nFinally, the method to embed the watermark refers to the insertion domain and\nits coefficients to be altered in order to insert the wanted information.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 09:43:32 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 09:16:26 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Artru", "Romain", ""], ["Gouaillard", "Alexandre", ""], ["Ebrahimi", "Touradj", ""]]}, {"id": "1908.02052", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Sarah Goodwin and Kim Marriott", "title": "Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2016). Awarded Best Paper Honorable Mention. Part of PhD thesis\n  arXiv:1908.00662", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 23:1\n  (2017) 411-420", "doi": "10.1109/TVCG.2016.2598885", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Showing flows of people and resources between multiple geographic locations\nis a challenging visualisation problem. We conducted two quantitative user\nstudies to evaluate different visual representations for such dense\nmany-to-many flows. In our first study we compared a bundled node-link flow map\nrepresentation and OD Maps [37] with a new visualisation we call MapTrix. Like\nOD Maps, MapTrix overcomes the clutter associated with a traditional flow map\nwhile providing geographic embedding that is missing in standard OD matrix\nrepresentations. We found that OD Maps and MapTrix had similar performance\nwhile bundled node-link flow map representations did not scale at all well. Our\nsecond study compared participant performance with OD Maps and MapTrix on\nlarger data sets. Again performance was remarkably similar.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:18:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Goodwin", "Sarah", ""], ["Marriott", "Kim", ""]]}, {"id": "1908.02088", "submitter": "Yalong Yang", "authors": "Yalong Yang, Bernhard Jenny, Tim Dwyer, Kim Marriott, Haohui Chen and\n  Maxime Cordeil", "title": "Maps and Globes in Virtual Reality", "comments": "Presented at 20th EG/VGTC Conference on Visualization (EuroVis 2018).\n  Part of PhD thesis arXiv:1908.00662", "journal-ref": "Computer Graphics Forum 37:3 (2018) 427-438", "doi": "10.1111/cgf.13431", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores different ways to render world-wide geographic maps in\nvirtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's\nviewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)\nan egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved\nmap, created by projecting the map onto a section of a sphere which curves\naround the user. In all four visualisations the geographic centre can be\nsmoothly adjusted with a standard handheld VR controller and the user, through\na head-tracked headset, can physically move around the visualisation. For\ndistance comparison, exocentric globe is more accurate than egocentric globe\nand flat map. For area comparison, more time is required with exocentric and\negocentric globes than with flat and curved maps. For direction estimation, the\nexocentric globe is more accurate and faster than the other visual\npresentations. Our study participants had a weak preference for the exocentric\nglobe. Generally, the curved map had benefits over the flat map. In almost all\ncases the egocentric globe was found to be the least effective visualisation.\nOverall, our results provide support for the use of exocentric globes for\ngeographic visualisation in mixed-reality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:45:51 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Jenny", "Bernhard", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Chen", "Haohui", ""], ["Cordeil", "Maxime", ""]]}, {"id": "1908.02089", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Bernhard Jenny, Kim Marriott, Maxime Cordeil\n  and Haohui Chen", "title": "Origin-Destination Flow Maps in Immersive Environments", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2018). Part of PhD thesis arXiv:1908.00662", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 25:1\n  (2019) 693-703", "doi": "10.1109/TVCG.2018.2865192", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive virtual- and augmented-reality headsets can overlay a flat image\nagainst any surface or hang virtual objects in the space around the user. The\ntechnology is rapidly improving and may, in the long term, replace traditional\nflat panel displays in many situations. When displays are no longer\nintrinsically flat, how should we use the space around the user for abstract\ndata visualisation? In this paper, we ask this question with respect to\norigin-destination flow data in a global geographic context. We report on the\nfindings of three studies exploring different spatial encodings for flow maps.\nThe first experiment focuses on different 2D and 3D encodings for flows on flat\nmaps. We find that participants are significantly more accurate with raised\nflow paths whose height is proportional to flow distance but fastest with\ntraditional straight line 2D flows. In our second and third experiment, we\ncompared flat maps, 3D globes and a novel interactive design we call MapsLink,\ninvolving a pair of linked flat maps. We find that participants took\nsignificantly more time with MapsLink than other flow maps while the 3D globe\nwith raised flows was the fastest, most accurate, and most preferred method.\nOur work suggests that careful use of the third spatial dimension can resolve\nvisual clutter in complex flow maps.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:57:41 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Jenny", "Bernhard", ""], ["Marriott", "Kim", ""], ["Cordeil", "Maxime", ""], ["Chen", "Haohui", ""]]}, {"id": "1908.02127", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Jinhui Tang, Jiangwei Li, Wei Luo, Hanqing Lu", "title": "Aligning Linguistic Words and Visual Semantic Units for Image Captioning", "comments": "8 pages, 5 figures. Accepted by ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3350943", "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning attempts to generate a sentence composed of several\nlinguistic words, which are used to describe objects, attributes, and\ninteractions in an image, denoted as visual semantic units in this paper. Based\non this view, we propose to explicitly model the object interactions in\nsemantics and geometry based on Graph Convolutional Networks (GCNs), and fully\nexploit the alignment between linguistic words and visual semantic units for\nimage captioning. Particularly, we construct a semantic graph and a geometry\ngraph, where each node corresponds to a visual semantic unit, i.e., an object,\nan attribute, or a semantic (geometrical) interaction between two objects.\nAccordingly, the semantic (geometrical) context-aware embeddings for each unit\nare obtained through the corresponding GCN learning processers. At each time\nstep, a context gated attention module takes as inputs the embeddings of the\nvisual semantic units and hierarchically align the current word with these\nunits by first deciding which type of visual semantic unit (object, attribute,\nor interaction) the current word is about, and then finding the most correlated\nvisual semantic units under this type. Extensive experiments are conducted on\nthe challenging MS-COCO image captioning dataset, and superior results are\nreported when comparing to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:19:24 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Tang", "Jinhui", ""], ["Li", "Jiangwei", ""], ["Luo", "Wei", ""], ["Lu", "Hanqing", ""]]}, {"id": "1908.02270", "submitter": "Tianchi Huang", "authors": "Tianchi Huang, Chao Zhou, Rui-Xiao Zhang, Chenglei Wu, Xin Yao, Lifeng\n  Sun", "title": "Comyco: Quality-Aware Adaptive Video Streaming via Imitation Learning", "comments": "ACM Multimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3351014", "report-no": null, "categories": "cs.MM cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based Adaptive Bit Rate~(ABR) method, aiming to learn outstanding\nstrategies without any presumptions, has become one of the research hotspots\nfor adaptive streaming. However, it typically suffers from several issues,\ni.e., low sample efficiency and lack of awareness of the video quality\ninformation. In this paper, we propose Comyco, a video quality-aware ABR\napproach that enormously improves the learning-based methods by tackling the\nabove issues. Comyco trains the policy via imitating expert trajectories given\nby the instant solver, which can not only avoid redundant exploration but also\nmake better use of the collected samples. Meanwhile, Comyco attempts to pick\nthe chunk with higher perceptual video qualities rather than video bitrates. To\nachieve this, we construct Comyco's neural network architecture, video datasets\nand QoE metrics with video quality features. Using trace-driven and real-world\nexperiments, we demonstrate significant improvements of Comyco's sample\nefficiency in comparison to prior work, with 1700x improvements in terms of the\nnumber of samples required and 16x improvements on training time required.\nMoreover, results illustrate that Comyco outperforms previously proposed\nmethods, with the improvements on average QoE of 7.5% - 16.79%. Especially,\nComyco also surpasses state-of-the-art approach Pensieve by 7.37% on average\nvideo quality under the same rebuffering time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 17:48:46 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 07:54:23 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Huang", "Tianchi", ""], ["Zhou", "Chao", ""], ["Zhang", "Rui-Xiao", ""], ["Wu", "Chenglei", ""], ["Yao", "Xin", ""], ["Sun", "Lifeng", ""]]}, {"id": "1908.02308", "submitter": "Shih-Fu Chang", "authors": "Shih-Fu Chang, Alex Hauptmann, Louis-Philippe Morency, Sameer Antani,\n  Dick Bulterman, Carlos Busso, Joyce Chai, Julia Hirschberg, Ramesh Jain,\n  Ketan Mayer-Patel, Reuven Meth, Raymond Mooney, Klara Nahrstedt, Shri\n  Narayanan, Prem Natarajan, Sharon Oviatt, Balakrishnan Prabhakaran, Arnold\n  Smeulders, Hari Sundaram, Zhengyou Zhang, Michelle Zhou", "title": "Report of 2017 NSF Workshop on Multimedia Challenges, Opportunities and\n  Research Roadmaps", "comments": "Long Report of NSF Workshop on Multimedia Challenges, Opportunities\n  and Research Roadmaps, held in March 2017, Washington DC. Short report\n  available separately", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the transformative technologies and the rapidly changing global R&D\nlandscape, the multimedia and multimodal community is now faced with many new\nopportunities and uncertainties. With the open source dissemination platform\nand pervasive computing resources, new research results are being discovered at\nan unprecedented pace. In addition, the rapid exchange and influence of ideas\nacross traditional discipline boundaries have made the emphasis on multimedia\nmultimodal research even more important than before. To seize these\nopportunities and respond to the challenges, we have organized a workshop to\nspecifically address and brainstorm the challenges, opportunities, and research\nroadmaps for MM research. The two-day workshop, held on March 30 and 31, 2017\nin Washington DC, was sponsored by the Information and Intelligent Systems\nDivision of the National Science Foundation of the United States. Twenty-three\n(23) invited participants were asked to review and identify research areas in\nthe MM field that are most important over the next 10-15 year timeframe.\nImportant topics were selected through discussion and consensus, and then\ndiscussed in depth in breakout groups. Breakout groups reported initial\ndiscussion results to the whole group, who continued with further extensive\ndeliberation. For each identified topic, a summary was produced after the\nworkshop to describe the main findings, including the state of the art,\nchallenges, and research roadmaps planned for the next 5, 10, and 15 years in\nthe identified area.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 18:14:57 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Chang", "Shih-Fu", ""], ["Hauptmann", "Alex", ""], ["Morency", "Louis-Philippe", ""], ["Antani", "Sameer", ""], ["Bulterman", "Dick", ""], ["Busso", "Carlos", ""], ["Chai", "Joyce", ""], ["Hirschberg", "Julia", ""], ["Jain", "Ramesh", ""], ["Mayer-Patel", "Ketan", ""], ["Meth", "Reuven", ""], ["Mooney", "Raymond", ""], ["Nahrstedt", "Klara", ""], ["Narayanan", "Shri", ""], ["Natarajan", "Prem", ""], ["Oviatt", "Sharon", ""], ["Prabhakaran", "Balakrishnan", ""], ["Smeulders", "Arnold", ""], ["Sundaram", "Hari", ""], ["Zhang", "Zhengyou", ""], ["Zhou", "Michelle", ""]]}, {"id": "1908.02446", "submitter": "Jun Chen", "authors": "Jun Chen, Ryosuke Watanabe, Keisuke Nonaka, Tomoaki Konno, Hiroshi\n  Sankoh, Sei Naito", "title": "A Robust Billboard-based Free-viewpoint Video Synthesizing Algorithm for\n  Sports Scenes", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a billboard-based free-viewpoint video synthesizing algorithm for\nsports scenes that can robustly reconstruct and render a high-fidelity\nbillboard model for each object, including an occluded one, in each camera. Its\ncontributions are (1) applicable to a challenging shooting condition where a\nhigh precision 3D model cannot be built because a small number of cameras\nfeaturing wide-baseline are equipped; (2) capable of reproducing appearances of\nocclusions, that is one of the most significant issues for billboard-based\napproaches due to the ineffective detection of overlaps. To achieve\ncontributions above, the proposed method does not attempt to find a\nhigh-quality 3D model but utilizes a raw 3D model that is obtained directly\nfrom space carving. Although the model is insufficiently accurate for producing\nan impressive visual effect, precise objects segmentation and occlusions\ndetection can be performed by back-projecting it onto each camera plane. The\nbillboard model of each object in each camera is rendered according to whether\nit is occluded or not, and its location in the virtual stadium is determined\nconsidering the location of its 3D model. We synthesized free-viewpoint videos\nof two soccer sequences recorded by five cameras with the proposed and\nstate-of-art methods to demonstrate its performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 05:24:27 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 07:39:16 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Chen", "Jun", ""], ["Watanabe", "Ryosuke", ""], ["Nonaka", "Keisuke", ""], ["Konno", "Tomoaki", ""], ["Sankoh", "Hiroshi", ""], ["Naito", "Sei", ""]]}, {"id": "1908.02473", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Na Xu and Feng Wang", "title": "Separable Reversible Data Hiding Based on Integer Mapping and Multi-MSB\n  Prediction for Encrypted 3D Mesh Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible data hiding in encrypted domain (RDH-ED) has received tremendous\nattention from the research community because data can be embedded into cover\nmedia without exposing it to the third party data hider and the cover media can\nbe losslessly recovered after the extraction of the embedded data. Although, in\nrecent years, extensive studies have been carried out about images based\nRDH-ED, little attention is paid to RDH-ED in 3D meshes due to its complex data\nstructure and irregular geometry. In this paper, we propose a separable RDH-ED\nmethod for 3D meshes based on integer mapping and Multi-MSB (multiplication\nmost significant bit) prediction. The proposed method divides all the vertices\nof the mesh into the \"embedded\" set and \"reference\" set, and maps decimals of\nthe vertex into integers. Then, we calculate the Multi-MSB prediction errors\nfor the vertices of the \"embedded\" set and a bit-stream encryption technique\nwill be executed. Finally, additional data is embedded by replacing the\nMulti-MSB of the encrypted vertex coordinates. According to different\npermissions, recipient can obtain the original plaintext meshes, additional\ndata or both. Experimental results show that the proposed method has higher\nembedding capacity and higher quality of the recovered meshes compared to the\nstate-of-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 07:41:03 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 16:47:25 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Xu", "Na", ""], ["Wang", "Feng", ""]]}, {"id": "1908.02726", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Yu Wu, Hehe Fan, Chenggang Yan, Yi Yang", "title": "Cascaded Revision Network for Novel Object Captioning", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2020.2965966", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning, a challenging task where the machine automatically\ndescribes an image by sentences, has drawn significant attention in recent\nyears. Despite the remarkable improvements of recent approaches, however, these\nmethods are built upon a large set of training image-sentence pairs. The\nexpensive labor efforts hence limit the captioning model to describe the wider\nworld. In this paper, we present a novel network structure, Cascaded Revision\nNetwork, which aims at relieving the problem by equipping the model with\nout-of-domain knowledge. CRN first tries its best to describe an image using\nthe existing vocabulary from in-domain knowledge. Due to the lack of\nout-of-domain knowledge, the caption may be inaccurate or include ambiguous\nwords for the image with unknown (novel) objects. We propose to re-edit the\nprimary captioning sentence by a series of cascaded operations. We introduce a\nperplexity predictor to find out which words are most likely to be inaccurate\ngiven the input image. Thereafter, we utilize external knowledge from a\npre-trained object detection model and select more accurate words from\ndetection results by the visual matching module. In the last step, we design a\nsemantic matching module to ensure that the novel object is fit in the right\nposition. By this novel cascaded captioning-revising mechanism, CRN can\naccurately describe images with unseen objects. We validate the proposed method\nwith state-of-the-art performance on the held-out MSCOCO dataset as well as\nscale to ImageNet, demonstrating the effectiveness of this method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 01:36:31 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Feng", "Qianyu", ""], ["Wu", "Yu", ""], ["Fan", "Hehe", ""], ["Yan", "Chenggang", ""], ["Yang", "Yi", ""]]}, {"id": "1908.03361", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Kai Schr\\\"oter, Moritz M\\\"unch, Bin Yang, Andrea Unger,\n  Doris Dransch, Joachim Denzler", "title": "Enhancing Flood Impact Analysis using Interactive Retrieval of Social\n  Media Images", "comments": null, "journal-ref": "Archives of Data Science, Series A, 5.1, 2018", "doi": "10.5445/KSP/1000087327/06", "report-no": null, "categories": "cs.IR cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The analysis of natural disasters such as floods in a timely manner often\nsuffers from limited data due to a coarse distribution of sensors or sensor\nfailures. This limitation could be alleviated by leveraging information\ncontained in images of the event posted on social media platforms, so-called\n\"Volunteered Geographic Information (VGI)\". To save the analyst from the need\nto inspect all images posted online manually, we propose to use content-based\nimage retrieval with the possibility of relevance feedback for retrieving only\nrelevant images of the event to be analyzed. To evaluate this approach, we\nintroduce a new dataset of 3,710 flood images, annotated by domain experts\nregarding their relevance with respect to three tasks (determining the flooded\narea, inundation depth, water pollution). We compare several image features and\nrelevance feedback methods on that dataset, mixed with 97,085 distractor\nimages, and are able to improve the precision among the top 100 retrieval\nresults from 55% with the baseline retrieval to 87% after 5 rounds of feedback.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:29:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Schr\u00f6ter", "Kai", ""], ["M\u00fcnch", "Moritz", ""], ["Yang", "Bin", ""], ["Unger", "Andrea", ""], ["Dransch", "Doris", ""], ["Denzler", "Joachim", ""]]}, {"id": "1908.03451", "submitter": "Wenmian Yang", "authors": "Wenmian Yang, Weijia Jia, Wenyuan Gao, Xiaojie Zhou, Yutao Luo", "title": "Interactive Variance Attention based Online Spoiler Detection for\n  Time-Sync Comments", "comments": "Accepted by CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3357872", "report-no": null, "categories": "cs.IR cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, time-sync comment (TSC), a new form of interactive comments, has\nbecome increasingly popular in Chinese video websites. By posting TSCs, people\ncan easily express their feelings and exchange their opinions with others when\nwatching online videos. However, some spoilers appear among the TSCs. These\nspoilers reveal crucial plots in videos that ruin people's surprise when they\nfirst watch the video. In this paper, we proposed a novel Similarity-Based\nNetwork with Interactive Variance Attention (SBN-IVA) to classify comments as\nspoilers or not. In this framework, we firstly extract textual features of TSCs\nthrough the word-level attentive encoder. We design Similarity-Based Network\n(SBN) to acquire neighbor and keyframe similarity according to semantic\nsimilarity and timestamps of TSCs. Then, we implement Interactive Variance\nAttention (IVA) to eliminate the impact of noise comments. Finally, we obtain\nthe likelihood of spoiler based on the difference between the neighbor and\nkeyframe similarity. Experiments show SBN-IVA is on average 11.2\\% higher than\nthe state-of-the-art method on F1-score in baselines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 13:24:21 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 06:22:08 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Yang", "Wenmian", ""], ["Jia", "Weijia", ""], ["Gao", "Wenyuan", ""], ["Zhou", "Xiaojie", ""], ["Luo", "Yutao", ""]]}, {"id": "1908.03505", "submitter": "David Semedo", "authors": "Gon\\c{c}alo Marcelino, David Semedo, Andr\\'e Mour\\~ao, Saverio Blasi,\n  Marta Mrak, Jo\\~ao Magalh\\~aes", "title": "A Benchmark of Visual Storytelling in Social Media", "comments": "To appear in ACM ICMR 2019", "journal-ref": null, "doi": "10.1145/3323873.3325047", "report-no": null, "categories": "cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media editors in the newsroom are constantly pressed to provide a \"like-being\nthere\" coverage of live events. Social media provides a disorganised collection\nof images and videos that media professionals need to grasp before publishing\ntheir latest news updated. Automated news visual storyline editing with social\nmedia content can be very challenging, as it not only entails the task of\nfinding the right content but also making sure that news content evolves\ncoherently over time. To tackle these issues, this paper proposes a benchmark\nfor assessing social media visual storylines. The SocialStories benchmark,\ncomprised by total of 40 curated stories covering sports and cultural events,\nprovides the experimental setup and introduces novel quantitative metrics to\nperform a rigorous evaluation of visual storytelling with social media data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:51:33 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Marcelino", "Gon\u00e7alo", ""], ["Semedo", "David", ""], ["Mour\u00e3o", "Andr\u00e9", ""], ["Blasi", "Saverio", ""], ["Mrak", "Marta", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}, {"id": "1908.03673", "submitter": "Xiongwei Wu", "authors": "Xiongwei Wu, Doyen Sahoo, Steven C.H. Hoi", "title": "Recent Advances in Deep Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental visual recognition problem in computer\nvision and has been widely studied in the past decades. Visual object detection\naims to find objects of certain target classes with precise localization in a\ngiven image and assign each object instance a corresponding class label. Due to\nthe tremendous successes of deep learning based image classification, object\ndetection techniques using deep learning have been actively studied in recent\nyears. In this paper, we give a comprehensive survey of recent advances in\nvisual object detection with deep learning. By reviewing a large body of recent\nrelated work in literature, we systematically analyze the existing object\ndetection frameworks and organize the survey into three major parts: (i)\ndetection components, (ii) learning strategies, and (iii) applications &\nbenchmarks. In the survey, we cover a variety of factors affecting the\ndetection performance in detail, such as detector architectures, feature\nlearning, proposal generation, sampling strategies, etc. Finally, we discuss\nseveral future directions to facilitate and spur future research for visual\nobject detection with deep learning. Keywords: Object Detection, Deep Learning,\nDeep Convolutional Neural Networks\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 02:54:17 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wu", "Xiongwei", ""], ["Sahoo", "Doyen", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1908.03737", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual\n  Cross-modal Retrieval", "comments": "21 pages,11 figures", "journal-ref": "ACM TOMM 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to retrieve data in one modality by a query in\nanother modality, which has been a very interesting research issue in the field\nof multimedia, information retrieval, and computer vision, and database. Most\nexisting works focus on cross-modal retrieval between text-image, text-video,\nand lyrics-audio.Little research addresses cross-modal retrieval between audio\nand video due to limited audio-video paired datasets and semantic information.\nThe main challenge of audio-visual cross-modal retrieval task focuses on\nlearning joint embeddings from a shared subspace for computing the similarity\nacross different modalities, where generating new representations is to\nmaximize the correlation between audio and visual modalities space. In this\nwork, we propose a novel deep triplet neural network with cluster canonical\ncorrelation analysis(TNN-C-CCA), which is an end-to-end supervised learning\narchitecture with audio branch and video branch.We not only consider the\nmatching pairs in the common space but also compute the mismatching pairs when\nmaximizing the correlation. In particular, two significant contributions are\nmade: i) a better representation by constructing deep triplet neural network\nwith triplet loss for optimal projections can be generated to maximize\ncorrelation in the shared subspace. ii) positive examples and negative examples\nare used in the learning stage to improve the capability of embedding learning\nbetween audio and video. Our experiment is run over 5-fold cross-validation,\nwhere average performance is applied to demonstrate the performance of\naudio-video cross-modal retrieval. The experimental results achieved on two\ndifferent audio-visual datasets show the proposed learning architecture with\ntwo branches outperforms existing six CCA-based methods and four\nstate-of-the-art based cross-modal retrieval methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 12:03:48 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 13:06:50 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 17:07:43 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "1908.03738", "submitter": "Donghuo Zeng", "authors": "Haoting Liang, Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Personalized Music Recommendation with Triplet Network", "comments": "1 figure; 1 table", "journal-ref": "DEIM 2019", "doi": null, "report-no": "SU-4240-720", "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many online music services emerged in recent years so that effective\nmusic recommendation systems are desirable. Some common problems in\nrecommendation system like feature representations, distance measure and cold\nstart problems are also challenges for music recommendation. In this paper, I\nproposed a triplet neural network, exploiting both positive and negative\nsamples to learn the representation and distance measure between users and\nitems, to solve the recommendation task.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 12:03:55 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Liang", "Haoting", ""], ["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "1908.03744", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Audio-Visual Embedding for Cross-Modal MusicVideo Retrieval through\n  Supervised Deep CCA", "comments": "8 pages, 9 figures. Accepted by ISM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has successfully shown excellent performance in learning joint\nrepresentations between different data modalities. Unfortunately, little\nresearch focuses on cross-modal correlation learning where temporal structures\nof different data modalities, such as audio and video, should be taken into\naccount. Music video retrieval by given musical audio is a natural way to\nsearch and interact with music contents. In this work, we study cross-modal\nmusic video retrieval in terms of emotion similarity. Particularly, audio of an\narbitrary length is used to retrieve a longer or full-length music video. To\nthis end, we propose a novel audio-visual embedding algorithm by Supervised\nDeep CanonicalCorrelation Analysis (S-DCCA) that projects audio and video into\na shared space to bridge the semantic gap between audio and video. This also\npreserves the similarity between audio and visual contents from different\nvideos with the same class label and the temporal structure. The contribution\nof our approach is mainly manifested in the two aspects: i) We propose to\nselect top k audio chunks by attention-based Long Short-Term Memory\n(LSTM)model, which can represent good audio summarization with local\nproperties. ii) We propose an end-to-end deep model for cross-modal\naudio-visual learning where S-DCCA is trained to learn the semantic correlation\nbetween audio and visual modalities. Due to the lack of music video dataset, we\nconstruct 10K music video dataset from YouTube 8M dataset. Some promising\nresults such as MAP and precision-recall show that our proposed model can be\napplied to music video retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 12:29:05 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "1908.03846", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Jinsong Su, Jiebo Luo", "title": "Exploiting Temporal Relationships in Video Moment Localization with\n  Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of video moment localization with natural language,\ni.e. localizing a video segment described by a natural language sentence. While\nmost prior work focuses on grounding the query as a whole, temporal\ndependencies and reasoning between events within the text are not fully\nconsidered. In this paper, we propose a novel Temporal Compositional Modular\nNetwork (TCMN) where a tree attention network first automatically decomposes a\nsentence into three descriptions with respect to the main event, context event\nand temporal signal. Two modules are then utilized to measure the visual\nsimilarity and location similarity between each segment and the decomposed\ndescriptions. Moreover, since the main event and context event may rely on\ndifferent modalities (RGB or optical flow), we use late fusion to form an\nensemble of four models, where each model is independently trained by one\ncombination of the visual input. Experiments show that our model outperforms\nthe state-of-the-art methods on the TEMPO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 03:59:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhang", "Songyang", ""], ["Su", "Jinsong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.03904", "submitter": "Rizwan Sadiq", "authors": "Rizwan Sadiq, Sasan AsadiAbadi, Engin Erzin", "title": "Emotion Dependent Facial Animation from Affective Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM cs.SD", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In human-to-computer interaction, facial animation in synchrony with\naffective speech can deliver more naturalistic conversational agents. In this\npaper, we present a two-stage deep learning approach for affective speech\ndriven facial shape animation. In the first stage, we classify affective speech\ninto seven emotion categories. In the second stage, we train separate deep\nestimators within each emotion category to synthesize facial shape from the\naffective speech. Objective and subjective evaluations are performed over the\nSAVEE dataset. The proposed emotion dependent facial shape model performs\nbetter in terms of the Mean Squared Error (MSE) loss and in generating the\nlandmark animations, as compared to training a universal model regardless of\nthe emotion.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 13:15:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Sadiq", "Rizwan", ""], ["AsadiAbadi", "Sasan", ""], ["Erzin", "Engin", ""]]}, {"id": "1908.04045", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Lizi Liao, Tat-Seng Chua", "title": "Automatic Fashion Knowledge Extraction from Social Media", "comments": "2 pages, 4 figures, ACMMM 2019 Demo", "journal-ref": null, "doi": "10.1145/3343031.3350607", "report-no": null, "categories": "cs.IR cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion knowledge plays a pivotal role in helping people in their dressing.\nIn this paper, we present a novel system to automatically harvest fashion\nknowledge from social media. It unifies three tasks of occasion, person and\nclothing discovery from multiple modalities of images, texts and metadata. A\ncontextualized fashion concept learning model is applied to leverage the rich\ncontextual information for improving the fashion concept learning performance.\nAt the same time, to counter the label noise within training data, we employ a\nweak label modeling method to further boost the performance. We build a website\nto demonstrate the quality of fashion knowledge extracted by our system.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:15:27 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ma", "Yunshan", ""], ["Liao", "Lizi", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1908.04047", "submitter": "Ahmadreza Montazerolghaem", "authors": "Ahmadreza Montazerolghaem", "title": "SIP Server Load Balancing Based on SDN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session Initiation Protocol (SIP) grows for VoIP applications, and faces\nchallenges including security and overload. On the other hand, the new concept\nof Software-defined Networking (SDN) has made great changes in the networked\nworld. SDN is the idea of separating the control plane from the network\ninfrastructure that can bring several benefits. We used this idea to provide a\nnew architecture for SIP networks. Moreover, for the load distribution\nchallenge in these networks, a framework based on SDN was offered, in which the\nload balancing and network management can be easily done by a central\ncontroller considering the network status. Unlike the traditional methods, in\nthis framework, there is no need to change the infrastructures like SIP servers\nor SIP load balancer to implement the distribution method. Also, several types\nof load distribution algorithms can be performed as software in the controller.\nWe were able to achieve the desired results by simulating the three methods\nbased on the proposed framework in Mininet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:18:45 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Montazerolghaem", "Ahmadreza", ""]]}, {"id": "1908.04297", "submitter": "Aakanksha Rana", "authors": "Cagri Ozcinar, Aakanksha Rana, and Aljosa Smolic", "title": "Super-resolution of Omnidirectional Images Using Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An omnidirectional image (ODI) enables viewers to look in every direction\nfrom a fixed point through a head-mounted display providing an immersive\nexperience compared to that of a standard image. Designing immersive virtual\nreality systems with ODIs is challenging as they require high resolution\ncontent. In this paper, we study super-resolution for ODIs and propose an\nimproved generative adversarial network based model which is optimized to\nhandle the artifacts obtained in the spherical observational space.\nSpecifically, we propose to use a fast PatchGAN discriminator, as it needs\nfewer parameters and improves the super-resolution at a fine scale. We also\nexplore the generative models with adversarial learning by introducing a\nspherical-content specific loss function, called 360-SS. To train and test the\nperformance of our proposed model we prepare a dataset of 4500 ODIs. Our\nresults demonstrate the efficacy of the proposed method and identify new\nchallenges in ODI super-resolution for future investigations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:05:59 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ozcinar", "Cagri", ""], ["Rana", "Aakanksha", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.04472", "submitter": "Peng Qi", "authors": "Peng Qi, Juan Cao, Tianyun Yang, Junbo Guo, and Jintao Li", "title": "Exploiting Multi-domain Visual Information for Fake News Detection", "comments": "10 pages, 9 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social media promotes the proliferation of fake\nnews. With the development of multimedia technology, fake news attempts to\nutilize multimedia contents with images or videos to attract and mislead\nreaders for rapid dissemination, which makes visual contents an important part\nof fake news. Fake-news images, images attached in fake news posts,include not\nonly fake images which are maliciously tampered but also real images which are\nwrongly used to represent irrelevant events. Hence, how to fully exploit the\ninherent characteristics of fake-news images is an important but challenging\nproblem for fake news detection. In the real world, fake-news images may have\nsignificantly different characteristics from real-news images at both physical\nand semantic levels, which can be clearly reflected in the frequency and pixel\ndomain, respectively. Therefore, we propose a novel framework Multi-domain\nVisual Neural Network (MVNN) to fuse the visual information of frequency and\npixel domains for detecting fake news. Specifically, we design a CNN-based\nnetwork to automatically capture the complex patterns of fake-news images in\nthe frequency domain; and utilize a multi-branch CNN-RNN model to extract\nvisual features from different semantic levels in the pixel domain. An\nattention mechanism is utilized to fuse the feature representations of\nfrequency and pixel domains dynamically. Extensive experiments conducted on a\nreal-world dataset demonstrate that MVNN outperforms existing methods with at\nleast 9.2% in accuracy, and can help improve the performance of multimodal fake\nnews detection by over 5.2%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 03:19:46 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Qi", "Peng", ""], ["Cao", "Juan", ""], ["Yang", "Tianyun", ""], ["Guo", "Junbo", ""], ["Li", "Jintao", ""]]}, {"id": "1908.05858", "submitter": "Ting Yao", "authors": "Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, Tao Mei", "title": "daBNN: A Super Fast Inference Framework for Binary Neural Networks on\n  ARM devices", "comments": "Accepted by 2019 ACMMM Open Source Software Competition. Source code:\n  https://github.com/JDAI-CV/dabnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is always well believed that Binary Neural Networks (BNNs) could\ndrastically accelerate the inference efficiency by replacing the arithmetic\noperations in float-valued Deep Neural Networks (DNNs) with bit-wise\noperations. Nevertheless, there has not been open-source implementation in\nsupport of this idea on low-end ARM devices (e.g., mobile phones and embedded\ndevices). In this work, we propose daBNN --- a super fast inference framework\nthat implements BNNs on ARM devices. Several speed-up and memory refinement\nstrategies for bit-packing, binarized convolution, and memory layout are\nuniquely devised to enhance inference efficiency. Compared to the recent\nopen-source BNN inference framework, BMXNet, our daBNN is\n$7\\times$$\\sim$$23\\times$ faster on a single binary convolution, and about\n$6\\times$ faster on Bi-Real Net 18 (a BNN variant of ResNet-18). The daBNN is a\nBSD-licensed inference framework, and its source code, sample projects and\npre-trained models are available on-line: https://github.com/JDAI-CV/dabnn.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:07:57 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Zhang", "Jianhao", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Zhao", "He", ""], ["Mei", "Tao", ""]]}, {"id": "1908.05913", "submitter": "Jiyoung Lee", "authors": "Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, Kwanghoon Sohn", "title": "Context-Aware Emotion Recognition Networks", "comments": "International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional techniques for emotion recognition have focused on the facial\nexpression analysis only, thus providing limited ability to encode context that\ncomprehensively represents the emotional responses. We present deep networks\nfor context-aware emotion recognition, called CAER-Net, that exploit not only\nhuman facial expression but also context information in a joint and boosting\nmanner. The key idea is to hide human faces in a visual scene and seek other\ncontexts based on an attention mechanism. Our networks consist of two\nsub-networks, including two-stream encoding networks to seperately extract the\nfeatures of face and context regions, and adaptive fusion networks to fuse such\nfeatures in an adaptive fashion. We also introduce a novel benchmark for\ncontext-aware emotion recognition, called CAER, that is more appropriate than\nexisting benchmarks both qualitatively and quantitatively. On several\nbenchmarks, CAER-Net proves the effect of context for emotion recognition. Our\ndataset is available at http://caer-dataset.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:59:15 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Lee", "Jiyoung", ""], ["Kim", "Seungryong", ""], ["Kim", "Sunok", ""], ["Park", "Jungin", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1908.05965", "submitter": "Erdun Gao", "authors": "Erdun Gao and Zhibin Pan and Xinyi Gao", "title": "Adaptive Embedding Pattern for Grayscale-Invariance Reversible Data\n  Hiding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional reversible data hiding (RDH) methods, researchers pay\nattention to enlarge the embedding capacity (EC) and to reduce the embedding\ndistortion (ED). Recently, a completely novel RDH algorithm was developed to\nembed secret data into color image without changing the corresponding grayscale\n[1], which largely expands the applications of RDH. In [1], for color image,\nchannel R and channel B are exploited to carry secret information, channel G is\nadjusted for balancing the modifications of channel R and channel B to keep the\ninvariance of grayscale. However, we found that the embedding performance (EP)\nof that method is still unsatisfied and could be further enhanced. To improve\nthe EP, an adaptive embedding pattern is introduced to enhance the competence\nof algorithm for selectively embedding different bits of secret data into\npixels according to context information. Moreover, a novel two-level predictor\nis designed by uniting two normal predictors for reducing the ED for embedding\nmore bits. Experimental results demonstrate that, compared to the previous\nmethod, our scheme could significantly enhance the image fidelity while keeping\nthe grayscale invariant.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 13:23:02 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Gao", "Erdun", ""], ["Pan", "Zhibin", ""], ["Gao", "Xinyi", ""]]}, {"id": "1908.06148", "submitter": "Govind Mittal", "authors": "Govind Mittal, Pawel Korus, Nasir Memon", "title": "FiFTy: Large-scale File Fragment Type Identification using Neural\n  Networks", "comments": "Paper accepted for publication in the IEEE Transactions on\n  Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FiFTy, a modern file type identification tool for memory forensics\nand data carving. In contrast to previous approaches based on hand-crafted\nfeatures, we design a compact neural network architecture, which uses a\ntrainable embedding space, akin to successful natural language processing\nmodels. Our approach dispenses with explicit feature extraction which is a\nbottleneck in legacy systems. We evaluate the proposed method on a novel\ndataset with 75 file types - the most diverse and balanced dataset reported to\ndate. FiFTy consistently outperforms all baselines in terms of speed, accuracy\nand individual misclassification rates. We achieved an average accuracy of\n77.5% with processing speed of approx 38 sec/GB, which is better and more than\nan order of magnitude faster than the previous state-of-the-art tool - Sceadan\n(69% at 9 min/GB). Our tool and the corresponding dataset are available\npublicly online.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 19:53:46 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 05:13:26 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Mittal", "Govind", ""], ["Korus", "Pawel", ""], ["Memon", "Nasir", ""]]}, {"id": "1908.06239", "submitter": "Tran Huyen Thi Thanh", "authors": "Huyen T. T. Tran, Duc V. Nguyen, Nam Pham Ngoc, Trang H. Hoang, Truong\n  Thu Huong, and Truong Cong Thang", "title": "Impacts of Retina-related Zones on Quality Perception of Omnidirectional\n  Image", "comments": "IEEE Access, 2019", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2953983", "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR), which brings immersive experiences to viewers, has been\ngaining popularity in recent years. A key feature in VR systems is the use of\nomnidirectional content, which provides 360-degree views of scenes. In this\nwork, we study the human quality perception of omnidirectional images, focusing\non different zones surrounding the foveation point. For that purpose, an\nextensive subjective experiment is carried out to assess the perceptual quality\nof omnidirectional images with non-uniform quality. Through experimental\nresults, the impacts of different zones are analyzed. Moreover, nineteen\nobjective quality metrics, including foveal quality metrics, are evaluated\nusing our database. It is quantitatively shown that the zones corresponding to\nthe fovea and parafovea of human eyes are extremely important for quality\nperception, while the impacts of the other zones corresponding to the perifovea\nand periphery are small. Besides, the investigated metrics are found to be not\neffective enough to reflect the quality perceived by viewers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 04:43:18 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Tran", "Huyen T. T.", ""], ["Nguyen", "Duc V.", ""], ["Ngoc", "Nam Pham", ""], ["Hoang", "Trang H.", ""], ["Huong", "Truong Thu", ""], ["Thang", "Truong Cong", ""]]}, {"id": "1908.06280", "submitter": "Wei Zhou", "authors": "Likun Shi, Wei Zhou, Zhibo Chen", "title": "No-Reference Light Field Image Quality Assessment Based on\n  Spatial-Angular Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field image quality assessment (LFI-QA) is a significant and\nchallenging research problem. It helps to better guide light field acquisition,\nprocessing and applications. However, only a few objective models have been\nproposed and none of them completely consider intrinsic factors affecting the\nLFI quality. In this paper, we propose a No-Reference Light Field image Quality\nAssessment (NR-LFQA) scheme, where the main idea is to quantify the LFI quality\ndegradation through evaluating the spatial quality and angular consistency. We\nfirst measure the spatial quality deterioration by capturing the naturalness\ndistribution of the light field cyclopean image array, which is formed when\nhuman observes the LFI. Then, as a transformed representation of LFI, the\nEpipolar Plane Image (EPI) contains the slopes of lines and involves the\nangular information. Therefore, EPI is utilized to extract the global and local\nfeatures from LFI to measure angular consistency degradation. Specifically, the\ndistribution of gradient direction map of EPI is proposed to measure the global\nangular consistency distortion in the LFI. We further propose the weighted\nlocal binary pattern to capture the characteristics of local angular\nconsistency degradation. Extensive experimental results on four publicly\navailable LFI quality datasets demonstrate that the proposed method outperforms\nstate-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:56:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Shi", "Likun", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "1908.06752", "submitter": "Aakanksha Rana", "authors": "Aakanksha Rana, Cagri Ozcinar, Aljoscha Smolic", "title": "Towards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality", "comments": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683318", "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambisonics i.e., a full-sphere surround sound, is quintessential with\n360-degree visual content to provide a realistic virtual reality (VR)\nexperience. While 360-degree visual content capture gained a tremendous boost\nrecently, the estimation of corresponding spatial sound is still challenging\ndue to the required sound-field microphones or information about the\nsound-source locations. In this paper, we introduce a novel problem of\ngenerating Ambisonics in 360-degree videos using the audio-visual cue. With\nthis aim, firstly, a novel 360-degree audio-visual video dataset of 265 videos\nis introduced with annotated sound-source locations. Secondly, a pipeline is\ndesigned for an automatic Ambisonic estimation problem. Benefiting from the\ndeep learning-based audio-visual feature-embedding and prediction modules, our\npipeline estimates the 3D sound-source locations and further use such locations\nto encode to the B-format. To benchmark our dataset and pipeline, we\nadditionally propose evaluation criteria to investigate the performance using\ndifferent 360-degree input representations. Our results demonstrate the\nefficacy of the proposed pipeline and open up a new area of research in\n360-degree audio-visual analysis for future investigations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:49:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rana", "Aakanksha", ""], ["Ozcinar", "Cagri", ""], ["Smolic", "Aljoscha", ""]]}, {"id": "1908.07226", "submitter": "Alp \\\"Oktem", "authors": "Alp \\\"Oktem and Mireia Farr\\'us and Antonio Bonafonte", "title": "Prosodic Phrase Alignment for Machine Dubbing", "comments": "Interspeech 2019 pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dubbing is a type of audiovisual translation where dialogues are translated\nand enacted so that they give the impression that the media is in the target\nlanguage. It requires a careful alignment of dubbed recordings with the lip\nmovements of performers in order to achieve visual coherence. In this paper, we\ndeal with the specific problem of prosodic phrase synchronization within the\nframework of machine dubbing. Our methodology exploits the attention mechanism\noutput in neural machine translation to find plausible phrasing for the\ntranslated dialogue lines and then uses them to condition their synthesis. Our\ninitial work in this field records comparable speech rate ratio to professional\ndubbing translation, and improvement in terms of lip-syncing of long dialogue\nlines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 08:52:52 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["\u00d6ktem", "Alp", ""], ["Farr\u00fas", "Mireia", ""], ["Bonafonte", "Antonio", ""]]}, {"id": "1908.07673", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng", "title": "Learning Joint Embedding for Cross-Modal Retrieval", "comments": "3 pages, 1 figure, Submitted to ICDM2019 Ph.D. Forum session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cross-modal retrieval process is to use a query in one modality to obtain\nrelevant data in another modality. The challenging issue of cross-modal\nretrieval lies in bridging the heterogeneous gap for similarity computation,\nwhich has been broadly discussed in image-text, audio-text, and video-text\ncross-modal multimedia data mining and retrieval. However, the gap in temporal\nstructures of different data modalities is not well addressed due to the lack\nof alignment relationship between temporal cross-modal structures. Our research\nfocuses on learning the correlation between different modalities for the task\nof cross-modal retrieval. We have proposed an architecture: Supervised-Deep\nCanonical Correlation Analysis (S-DCCA), for cross-modal retrieval. In this\nforum paper, we will talk about how to exploit triplet neural networks (TNN) to\nenhance the correlation learning for cross-modal retrieval. The experimental\nresult shows the proposed TNN-based supervised correlation learning\narchitecture can get the best result when the data representation extracted by\nsupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 02:04:18 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zeng", "Donghuo", ""]]}, {"id": "1908.07683", "submitter": "Kwanyong Park", "authors": "Kwanyong Park, Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon", "title": "Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video\n  Translation", "comments": "Accepted by ACM Multimedia(ACM MM) 2019", "journal-ref": null, "doi": "10.1145/3343031.3350864", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of unpaired video-to-video\ntranslation. Given a video in the source domain, we aim to learn the\nconditional distribution of the corresponding video in the target domain,\nwithout seeing any pairs of corresponding videos. While significant progress\nhas been made in the unpaired translation of images, directly applying these\nmethods to an input video leads to low visual quality due to the additional\ntime dimension. In particular, previous methods suffer from semantic\ninconsistency (i.e., semantic label flipping) and temporal flickering\nartifacts. To alleviate these issues, we propose a new framework that is\ncomposed of carefully-designed generators and discriminators, coupled with two\ncore objective functions: 1) content preserving loss and 2) temporal\nconsistency loss. Extensive qualitative and quantitative evaluations\ndemonstrate the superior performance of the proposed method against previous\napproaches. We further apply our framework to a domain adaptation task and\nachieve favorable results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 02:54:21 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Park", "Kwanyong", ""], ["Woo", "Sanghyun", ""], ["Kim", "Dahun", ""], ["Cho", "Donghyeon", ""], ["Kweon", "In So", ""]]}, {"id": "1908.07738", "submitter": "Fan Liu", "authors": "Fan Liu, Zhiyong Cheng, Changchang Sun, Yinglong Wang, Liqiang Nie,\n  Mohan Kankanhalli", "title": "User Diverse Preference Modeling by Multimodal Attentive Metric Learning", "comments": "Accepted by ACM Multimedia 2019 as a full paper", "journal-ref": null, "doi": "10.1145/3343031.3350953", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing recommender systems represent a user's preference with a\nfeature vector, which is assumed to be fixed when predicting this user's\npreferences for different items. However, the same vector cannot accurately\ncapture a user's varying preferences on all items, especially when considering\nthe diverse characteristics of various items. To tackle this problem, in this\npaper, we propose a novel Multimodal Attentive Metric Learning (MAML) method to\nmodel user diverse preferences for various items. In particular, for each\nuser-item pair, we propose an attention neural network, which exploits the\nitem's multimodal features to estimate the user's special attention to\ndifferent aspects of this item. The obtained attention is then integrated into\na metric-based learning method to predict the user preference on this item. The\nadvantage of metric learning is that it can naturally overcome the problem of\ndot product similarity, which is adopted by matrix factorization (MF) based\nrecommendation models but does not satisfy the triangle inequality property. In\naddition, it is worth mentioning that the attention mechanism cannot only help\nmodel user's diverse preferences towards different items, but also overcome the\ngeometrically restrictive problem caused by collaborative metric learning.\nExtensive experiments on large-scale real-world datasets show that our model\ncan substantially outperform the state-of-the-art baselines, demonstrating the\npotential of modeling user diverse preference for recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:52:39 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Liu", "Fan", ""], ["Cheng", "Zhiyong", ""], ["Sun", "Changchang", ""], ["Wang", "Yinglong", ""], ["Nie", "Liqiang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1908.07810", "submitter": "Shiwan Zhao Mr", "authors": "Yike Wu, Shiwan Zhao, Jia Chen, Ying Zhang, Xiaojie Yuan, Zhong Su", "title": "Improving Captioning for Low-Resource Languages by Cycle Consistency", "comments": "Published in ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the captioning performance on low-resource languages by leveraging\nEnglish caption datasets has received increasing research interest in recent\nyears. Existing works mainly fall into two categories: translation-based and\nalignment-based approaches. In this paper, we propose to combine the merits of\nboth approaches in one unified architecture. Specifically, we use a pre-trained\nEnglish caption model to generate high-quality English captions, and then take\nboth the image and generated English captions to generate low-resource language\ncaptions. We improve the captioning performance by adding the cycle consistency\nconstraint on the cycle of image regions, English words, and low-resource\nlanguage words. Moreover, our architecture has a flexible design which enables\nit to benefit from large monolingual English caption datasets. Experimental\nresults demonstrate that our approach outperforms the state-of-the-art methods\non common evaluation metrics. The attention visualization also shows that the\nproposed approach really improves the fine-grained alignment between words and\nimage regions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 12:15:35 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Wu", "Yike", ""], ["Zhao", "Shiwan", ""], ["Chen", "Jia", ""], ["Zhang", "Ying", ""], ["Yuan", "Xiaojie", ""], ["Su", "Zhong", ""]]}, {"id": "1908.08505", "submitter": "Aakanksha Rana", "authors": "Emin Zerman, Aakanksha Rana, Aljosa Smolic", "title": "ColorNet -- Estimating Colorfulness in Natural Images", "comments": "Accepted to IEEE International Conference on Image Processing (ICIP)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the colorfulness of a natural or virtual scene is critical for many\napplications in image processing field ranging from capturing to display. In\nthis paper, we propose the first deep learning-based colorfulness estimation\nmetric. For this purpose, we develop a color rating model which simultaneously\nlearns to extracts the pertinent characteristic color features and the mapping\nfrom feature space to the ideal colorfulness scores for a variety of natural\ncolored images. Additionally, we propose to overcome the lack of adequate\nannotated dataset problem by combining/aligning two publicly available\ncolorfulness databases using the results of a new subjective test which employs\na common subset of both databases. Using the obtained subjectively annotated\ndataset with 180 colored images, we finally demonstrate the efficacy of our\nproposed model over the traditional methods, both quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:24:37 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zerman", "Emin", ""], ["Rana", "Aakanksha", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.08985", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Xun Yang, Lizi Liao, Yixin Cao, Tat-Seng Chua", "title": "Who, Where, and What to Wear? Extracting Fashion Knowledge from Social\n  Media", "comments": "9 pages, 8 figures, ACMMM 2019", "journal-ref": null, "doi": "10.1145/3343031.3350889", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion knowledge helps people to dress properly and addresses not only\nphysiological needs of users, but also the demands of social activities and\nconventions. It usually involves three mutually related aspects of: occasion,\nperson and clothing. However, there are few works focusing on extracting such\nknowledge, which will greatly benefit many downstream applications, such as\nfashion recommendation. In this paper, we propose a novel method to\nautomatically harvest fashion knowledge from social media. We unify three tasks\nof occasion, person and clothing discovery from multiple modalities of images,\ntexts and metadata. For person detection and analysis, we use the off-the-shelf\ntools due to their flexibility and satisfactory performance. For clothing\nrecognition and occasion prediction, we unify the two tasks by using a\ncontextualized fashion concept learning module, which captures the dependencies\nand correlations among different fashion concepts. To alleviate the heavy\nburden of human annotations, we introduce a weak label modeling module which\ncan effectively exploit machine-labeled data, a complementary of clean data. In\nexperiments, we contribute a benchmark dataset and conduct extensive\nexperiments from both quantitative and qualitative perspectives. The results\ndemonstrate the effectiveness of our model in fashion concept prediction, and\nthe usefulness of extracted knowledge with comprehensive analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:05:59 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ma", "Yunshan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Cao", "Yixin", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1908.09082", "submitter": "Felix Hamza-Lup", "authors": "Felix Hamza-Lup", "title": "Kinesthetic Learning -- Haptic User Interfaces for Gyroscopic Precession\n  Simulation", "comments": null, "journal-ref": "Journal of Human Computer Interaction (RO-CHI) Vol. 11(3) (2018)\n  185-204", "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some forces in nature are difficult to comprehend due to their non-intuitive\nand abstract nature. Forces driving gyroscopic precession are invisible, yet\ntheir effect is very important in a variety of applications, from space\nnavigation to motion tracking. Current technological advancements in haptic\ninterfaces, enables development of revolutionary user interfaces, combining\nmultiple modalities: tactile, visual and auditory. Tactile augmented user\ninterfaces have been deployed in a variety of areas, from surgical training to\nelementary education. This research provides an overview of haptic user\ninterfaces in higher education, and presents the development and assessment of\na haptic-user interface that supports the learner's understanding of gyroscopic\nprecession forces. The visual-haptic simulator proposed, is one module from a\nseries of simulators targeted at complex concept representation, using\nmulti-modal user interfaces. Various higher education domains, from classical\nphysics to mechanical engineering, will benefit from the mainstream adoption of\nmulti-modal interfaces for hands-on training and content delivery. Experimental\nresults are promising, and underline the valuable impact that haptic user\ninterfaces have on enabling abstract concepts understanding, through\nkinesthetic learning and hands-on practice.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 03:24:14 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hamza-Lup", "Felix", ""]]}, {"id": "1908.09514", "submitter": "Ting Yao", "authors": "Yang Chen and Yingwei Pan and Ting Yao and Xinmei Tian and Tao Mei", "title": "Mocycle-GAN: Unpaired Video-to-Video Translation", "comments": "Accepted as a full paper for ACMMM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is the task of translating an image\nfrom one domain to another in the absence of any paired training examples and\ntends to be more applicable to practical applications. Nevertheless, the\nextension of such synthesis from image-to-image to video-to-video is not\ntrivial especially when capturing spatio-temporal structures in videos. The\ndifficulty originates from the aspect that not only the visual appearance in\neach frame but also motion between consecutive frames should be realistic and\nconsistent across transformation. This motivates us to explore both appearance\nstructure and temporal continuity in video synthesis. In this paper, we present\na new Motion-guided Cycle GAN, dubbed as Mocycle-GAN, that novelly integrates\nmotion estimation into unpaired video translator. Technically, Mocycle-GAN\ncapitalizes on three types of constrains: adversarial constraint discriminating\nbetween synthetic and real frame, cycle consistency encouraging an inverse\ntranslation on both frame and motion, and motion translation validating the\ntransfer of motion between consecutive frames. Extensive experiments are\nconducted on video-to-labels and labels-to-video translation, and superior\nresults are reported when comparing to state-of-the-art methods. More\nremarkably, we qualitatively demonstrate our Mocycle-GAN for both\nflower-to-flower and ambient condition transfer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 07:51:17 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Chen", "Yang", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Tian", "Xinmei", ""], ["Mei", "Tao", ""]]}, {"id": "1908.09822", "submitter": "Zhiding Yu", "authors": "Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V. K. Vijaya Kumar, Jinsong\n  Wang", "title": "Confidence Regularized Self-Training", "comments": "Accepted to ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in domain adaptation show that deep self-training presents a\npowerful means for unsupervised domain adaptation. These methods often involve\nan iterative process of predicting on target domain and then taking the\nconfident predictions as pseudo-labels for retraining. However, since\npseudo-labels can be noisy, self-training can put overconfident label belief on\nwrong classes, leading to deviated solutions with propagated errors. To address\nthe problem, we propose a confidence regularized self-training (CRST)\nframework, formulated as regularized self-training. Our method treats\npseudo-labels as continuous latent variables jointly optimized via alternating\noptimization. We propose two types of confidence regularization: label\nregularization (LR) and model regularization (MR). CRST-LR generates soft\npseudo-labels while CRST-MR encourages the smoothness on network output.\nExtensive experiments on image classification and semantic segmentation show\nthat CRSTs outperform their non-regularized counterpart with state-of-the-art\nperformance. The code and models of this work are available at\nhttps://github.com/yzou2/CRST.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 17:56:13 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 05:26:12 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 10:57:38 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zou", "Yang", ""], ["Yu", "Zhiding", ""], ["Liu", "Xiaofeng", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Wang", "Jinsong", ""]]}, {"id": "1908.09987", "submitter": "Yinwei Wei", "authors": "Yinwei Wei, Zhiyong Cheng, Xuzheng Yu, Zhou Zhao, Lei Zhu, Liqiang Nie", "title": "Personalized Hashtag Recommendation for Micro-videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized hashtag recommendation methods aim to suggest users hashtags to\nannotate, categorize, and describe their posts. The hashtags, that a user\nprovides to a post (e.g., a micro-video), are the ones which in her mind can\nwell describe the post content where she is interested in. It means that we\nshould consider both users' preferences on the post contents and their personal\nunderstanding on the hashtags. Most existing methods rely on modeling either\nthe interactions between hashtags and posts or the interactions between users\nand hashtags for hashtag recommendation. These methods have not well explored\nthe complicated interactions among users, hashtags, and micro-videos. In this\npaper, towards the personalized micro-video hashtag recommendation, we propose\na Graph Convolution Network based Personalized Hashtag Recommendation (GCN-PHR)\nmodel, which leverages recently advanced GCN techniques to model the complicate\ninteractions among <users, hashtags, micro-videos> and learn their\nrepresentations. In our model, the users, hashtags, and micro-videos are three\ntypes of nodes in a graph and they are linked based on their direct\nassociations. In particular, the message-passing strategy is used to learn the\nrepresentation of a node (e.g., user) by aggregating the message passed from\nthe directly linked other types of nodes (e.g., hashtag and micro-video).\nBecause a user is often only interested in certain parts of a micro-video and a\nhashtag is typically used to describe the part (of a micro-video) that the user\nis interested in, we leverage the attention mechanism to filter the message\npassed from micro-videos to users and hashtags, which can significantly improve\nthe representation capability. Extensive experiments have been conducted on two\nreal-world micro-video datasets and demonstrate that our model outperforms the\nstate-of-the-art approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:14:52 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wei", "Yinwei", ""], ["Cheng", "Zhiyong", ""], ["Yu", "Xuzheng", ""], ["Zhao", "Zhou", ""], ["Zhu", "Lei", ""], ["Nie", "Liqiang", ""]]}, {"id": "1908.10078", "submitter": "Wieslaw Kopec", "authors": "Kinga Skorupska, Manuel N\\'u\\~nez, Wies{\\l}aw Kope\\'c, Rados{\\l}aw\n  Nielek", "title": "A Comparative Study of Younger and Older Adults' Interaction with a\n  Crowdsourcing Android TV App for Detecting Errors in TEDx Video Subtitles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-29387-1_25", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report the results of a pilot study comparing the older and\nyounger adults' interaction with an Android TV application which enables users\nto detect errors in video subtitles. Overall, the interaction with the\nTV-mediated crowdsourcing system relying on language profficiency was seen as\nintuitive, fun and accessible, but also cognitively demanding; more so for\nyounger adults who focused on the task of detecting errors, than for older\nadults who concentrated more on the meaning and edutainment aspect of the\nvideos. We also discuss participants' motivations and preliminary\nrecommendations for the design of TV-enabled crowdsourcing tasks and subtitle\nQA systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 08:37:16 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Skorupska", "Kinga", ""], ["N\u00fa\u00f1ez", "Manuel", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""]]}, {"id": "1908.10079", "submitter": "Jiahua Xu", "authors": "Jiahua Xu, Ziyuan Luo, Wei Zhou, Wenyuan Zhang and Zhibo Chen", "title": "Quality Assessment of Stereoscopic 360-degree Images from\n  Multi-viewports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective quality assessment of stereoscopic panoramic images becomes a\nchallenging problem owing to the rapid growth of 360-degree contents. Different\nfrom traditional 2D image quality assessment (IQA), more complex aspects are\ninvolved in 3D omnidirectional IQA, especially unlimited field of view (FoV)\nand extra depth perception, which brings difficulty to evaluate the quality of\nexperience (QoE) of 3D omnidirectional images. In this paper, we propose a\nmulti-viewport based fullreference stereo 360 IQA model. Due to the freely\nchangeable viewports when browsing in the head-mounted display (HMD), our\nproposed approach processes the image inside FoV rather than the projected one\nsuch as equirectangular projection (ERP). In addition, since overall QoE\ndepends on both image quality and depth perception, we utilize the features\nestimated by the difference map between left and right views which can reflect\ndisparity. The depth perception features along with binocular image qualities\nare employed to further predict the overall QoE of 3D 360 images. The\nexperimental results on our public Stereoscopic OmnidirectionaL Image quality\nassessment Database (SOLID) show that the proposed method achieves a\nsignificant improvement over some well-known IQA metrics and can accurately\nreflect the overall QoE of perceived images.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 08:46:55 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:27:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Xu", "Jiahua", ""], ["Luo", "Ziyuan", ""], ["Zhou", "Wei", ""], ["Zhang", "Wenyuan", ""], ["Chen", "Zhibo", ""]]}, {"id": "1908.10087", "submitter": "Ziyuan Luo", "authors": "Ziyuan Luo, Wei Zhou, Likun Shi, and Zhibo Chen", "title": "No-Reference Light Field Image Quality Assessment Based on Micro-Lens\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field image quality assessment (LF-IQA) plays a significant role due to\nits guidance to Light Field (LF) contents acquisition, processing and\napplication. The LF can be represented as 4-D signal, and its quality depends\non both angular consistency and spatial quality. However, few existing LF-IQA\nmethods concentrate on effects caused by angular inconsistency. Especially,\nno-reference methods lack effective utilization of 2-D angular information. In\nthis paper, we focus on measuring the 2-D angular consistency for LF-IQA. The\nMicro-Lens Image (MLI) refers to the angular domain of the LF image, which can\nsimultaneously record the angular information in both horizontal and vertical\ndirections. Since the MLI contains 2-D angular information, we propose a\nNo-Reference Light Field image Quality assessment model based on MLI (LF-QMLI).\nSpecifically, we first utilize Global Entropy Distribution (GED) and Uniform\nLocal Binary Pattern descriptor (ULBP) to extract features from the MLI, and\nthen pool them together to measure angular consistency. In addition, the\ninformation entropy of Sub-Aperture Image (SAI) is adopted to measure spatial\nquality. Extensive experimental results show that LF-QMLI achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 08:53:20 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Luo", "Ziyuan", ""], ["Zhou", "Wei", ""], ["Shi", "Likun", ""], ["Chen", "Zhibo", ""]]}, {"id": "1908.10818", "submitter": "Peng Qi", "authors": "Juan Cao, Qiang Sheng, Peng Qi, Lei Zhong, Yanyan Wang, Xueyao Zhang", "title": "False News Detection on Social Media", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has become a major information platform where people consume and\nshare news. However, it has also enabled the wide dissemination of false news,\ni.e., news posts published on social media that are verifiably false, causing\nsignificant negative effects on society. In order to help prevent further\npropagation of false news on social media, we set up this competition to\nmotivate the development of automated real-time false news detection\napproaches. Specifically, this competition includes three sub-tasks: false-news\ntext detection, false-news image detection and false-news multi-modal\ndetetcion, which aims to motivate participants to further explore the\nefficiency of multiple modalities in detecting false news and reasonable fusion\napproaches of multi-modal contents. To better support this competition, we also\nconstruct and publicize a multi-modal data repository about False News on Weibo\nSocial platform(MCG-FNeWS}) to help evaluate the performance of different\napproaches from participants.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 16:40:20 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Cao", "Juan", ""], ["Sheng", "Qiang", ""], ["Qi", "Peng", ""], ["Zhong", "Lei", ""], ["Wang", "Yanyan", ""], ["Zhang", "Xueyao", ""]]}, {"id": "1908.10898", "submitter": "Anier Soria Lorente", "authors": "A. Soria-Lorente, E. P\\'erez-Michel, E. Avila-Domenech", "title": "A steganographic approach based on the chaotic fractional map and in the\n  DCT domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A steganographic method based on the chaotic fractional map and in the DCT\ndomain is proposed. This method embeds a secret message in some high frequency\ncoefficients of the image using a 128-bit private key and a chaotic fractional\nmap which generate a permutation indicating the positions where the secret bits\nwill be embedded. An experimental work on the validation of the proposed method\nis also presented, showing performance in imperceptibility, quality, similarity\nand security analysis of the steganographic system. The proposed algorithm\nimproved the level of imperceptibility and Cachin's security of stego-system\nanalyzed through the values of Peak Signal-to-Noise Ratio (PSNR) and the\nRelative Entropy (RE).\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 18:28:04 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Soria-Lorente", "A.", ""], ["P\u00e9rez-Michel", "E.", ""], ["Avila-Domenech", "E.", ""]]}, {"id": "1908.10967", "submitter": "Na Li", "authors": "Na Li, Yongfei Zhang, Yun Zhang, C.-C. Jay Kuo", "title": "On Energy Compaction of 2D Saab Image Transforms", "comments": "10 pages, 9 figures, to appear in Asia-Pacific Signal and Information\n  Processing Association (APSIPA), which will be held on November 18-21, 2019,\n  in Lanzhou, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block Discrete Cosine Transform (DCT) is commonly used in image and video\ncompression due to its good energy compaction property. The Saab transform was\nrecently proposed as an effective signal transform for image understanding. In\nthis work, we study the energy compaction property of the Saab transform in the\ncontext of intra-coding of the High Efficiency Video Coding (HEVC) standard. We\ncompare the energy compaction property of the Saab transform, the DCT, and the\nKarhunen-Loeve transform (KLT) by applying them to different sizes of\nintra-predicted residual blocks in HEVC. The basis functions of the Saab\ntransform are visualized. Extensive experimental results are given to\ndemonstrate the energy compaction capability of the Saab transform.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 22:11:53 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Li", "Na", ""], ["Zhang", "Yongfei", ""], ["Zhang", "Yun", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1908.11331", "submitter": "Xin Zhong", "authors": "Xin Zhong and Frank Y. Shih", "title": "A Robust Image Watermarking System Based on Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2020.3006415", "report-no": null, "categories": "cs.MM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image watermarking is the process of embedding and extracting\nwatermark covertly on a carrier image. Incorporating deep learning networks\nwith image watermarking has attracted increasing attention during recent years.\nHowever, existing deep learning-based watermarking systems cannot achieve\nrobustness, blindness, and automated embedding and extraction simultaneously.\nIn this paper, a fully automated image watermarking system based on deep neural\nnetworks is proposed to generalize the image watermarking processes. An\nunsupervised deep learning structure and a novel loss computation are proposed\nto achieve high capacity and high robustness without any prior knowledge of\npossible attacks. Furthermore, a challenging application of watermark\nextraction from camera-captured images is provided to validate the practicality\nas well as the robustness of the proposed system. Experimental results show the\nsuperiority performance of the proposed system as comparing against several\ncurrently available techniques.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:24:29 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhong", "Xin", ""], ["Shih", "Frank Y.", ""]]}, {"id": "1908.11517", "submitter": "Yang Li", "authors": "Yang Li, Shengbin Meng, Xinfeng Zhang, Shiqi Wang, Yue Wang, Siwei Ma", "title": "UGC-VIDEO: perceptual quality assessment of user-generated videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an ever-expandingvolume of user-generated content\n(UGC) videos available on the Internet. Nevertheless, progress on perceptual\nquality assessmentof UGC videos still remains quite limited. There are many\ndistinguished characteristics of UGC videos in the complete video production\nand delivery chain, and one important property closely relevant to video\nquality is that there does not exist the pristine source after they are\nuploaded to the hosting platform,such that they often undergo multiple\ncompression stages before ultimately viewed. To facilitate the UGC video\nquality assessment,we created a UGC video perceptual quality assessment\ndatabase. It contains 50 source videos collected from TikTok with diverse\ncontent, along with multiple distortion versions generated bythe compression\nwith different quantization levels and coding standards. Subjective quality\nassessment was conducted to evaluate the video quality. Furthermore, we\nbenchmark the database using existing quality assessment algorithms, and\npotential roomis observed to future improve the accuracy of UGC video quality\nmeasures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 03:30:23 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 03:28:36 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Li", "Yang", ""], ["Meng", "Shengbin", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shiqi", ""], ["Wang", "Yue", ""], ["Ma", "Siwei", ""]]}, {"id": "1908.11588", "submitter": "Chang Liu", "authors": "Chang Liu, Yi Dong, Han Yu, Zhiqi Shen, Zhanning Gao, Pan Wang,\n  Changgong Zhang, Peiran Ren, Xuansong Xie, Lizhen Cui, Chunyan Miao", "title": "Generating Persuasive Visual Storylines for Promotional Videos", "comments": "10 pages, accepted by The 28th ACM International Conference on\n  Information and Knowledge Management (CIKM)", "journal-ref": null, "doi": "10.1145/3357384.3357906", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video contents have become a critical tool for promoting products in\nE-commerce. However, the lack of automatic promotional video generation\nsolutions makes large-scale video-based promotion campaigns infeasible. The\nfirst step of automatically producing promotional videos is to generate visual\nstorylines, which is to select the building block footage and place them in an\nappropriate order. This task is related to the subjective viewing experience.\nIt is hitherto performed by human experts and thus, hard to scale. To address\nthis problem, we propose WundtBackpack, an algorithmic approach to generate\nstorylines based on available visual materials, which can be video clips or\nimages. It consists of two main parts, 1) the Learnable Wundt Curve to evaluate\nthe perceived persuasiveness based on the stimulus intensity of a sequence of\nvisual materials, which only requires a small volume of data to train; and 2) a\nclustering-based backpacking algorithm to generate persuasive sequences of\nvisual materials while considering video length constraints. In this way, the\nproposed approach provides a dynamic structure to empower artificial\nintelligence (AI) to organize video footage in order to construct a sequence of\nvisual stimuli with persuasive power. Extensive real-world experiments show\nthat our approach achieves close to 10% higher perceived persuasiveness scores\nby human testers, and 12.5% higher expected revenue compared to the best\nperforming state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 08:13:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Chang", ""], ["Dong", "Yi", ""], ["Yu", "Han", ""], ["Shen", "Zhiqi", ""], ["Gao", "Zhanning", ""], ["Wang", "Pan", ""], ["Zhang", "Changgong", ""], ["Ren", "Peiran", ""], ["Xie", "Xuansong", ""], ["Cui", "Lizhen", ""], ["Miao", "Chunyan", ""]]}]