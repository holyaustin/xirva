[{"id": "2102.00142", "submitter": "Ivan Bajic", "authors": "Ivan V. Baji\\'c", "title": "Latent-Space Inpainting for Packet Loss Concealment in Collaborative\n  Object Detection", "comments": "Extended version of the paper \"Latent Space Inpainting for\n  Loss-Resilient Collaborative Object Detection,\" to be presented at the IEEE\n  International Conference on Communications (ICC), Montreal, Canada, June\n  14-23, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge devices, such as cameras and mobile units, are increasingly capable of\nperforming sophisticated computation in addition to their traditional roles in\nsensing and communicating signals. The focus of this paper is on collaborative\nobject detection, where deep features computed on the edge device from input\nimages are transmitted to the cloud for further processing. We consider the\nimpact of packet loss on the transmitted features and examine several ways for\nrecovering the missing data. In particular, through theory and experiments, we\nshow that methods for image inpainting based on partial differential equations\nwork well for the recovery of missing features in the latent space. The\nobtained results represent the new state of the art for missing data recovery\nin collaborative object detection.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 03:32:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Baji\u0107", "Ivan V.", ""]]}, {"id": "2102.00155", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Chia-Ju Chen, Li-Heng Chen, Yilin Wang, Neil Birkbeck,\n  Balu Adsumilli, and Alan C. Bovik", "title": "Regression or Classification? New Methods to Evaluate No-Reference\n  Picture and Video Quality Models", "comments": "ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video and image quality assessment has long been projected as a regression\nproblem, which requires predicting a continuous quality score given an input\nstimulus. However, recent efforts have shown that accurate quality score\nregression on real-world user-generated content (UGC) is a very challenging\ntask. To make the problem more tractable, we propose two new methods - binary,\nand ordinal classification - as alternatives to evaluate and compare\nno-reference quality models at coarser levels. Moreover, the proposed new tasks\nconvey more practical meaning on perceptually optimized UGC transcoding, or for\npreprocessing on media processing platforms. We conduct a comprehensive\nbenchmark experiment of popular no-reference quality models on recent\nin-the-wild picture and video quality datasets, providing reliable baselines\nfor both evaluation methods to support further studies. We hope this work\npromotes coarse-grained perceptual modeling and its applications to efficient\nUGC processing.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 05:40:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Chen", "Chia-Ju", ""], ["Chen", "Li-Heng", ""], ["Wang", "Yilin", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2102.00201", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Yuntae Kim, Soohyeon Lee, Biho Kim, Namjun Jo, Semi\n  Lim, Suyon Lim, Jungtaek Jang, Sehwan Kim, Xavier Serra, Dmitry Bogdanov", "title": "Melon Playlist Dataset: a public dataset for audio-based playlist\n  generation and music tagging", "comments": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the main limitations in the field of audio signal processing is the\nlack of large public datasets with audio representations and high-quality\nannotations due to restrictions of copyrighted commercial music. We present\nMelon Playlist Dataset, a public dataset of mel-spectrograms for 649,091tracks\nand 148,826 associated playlists annotated by 30,652 different tags. All the\ndata is gathered from Melon, a popular Korean streaming service. The dataset is\nsuitable for music information retrieval tasks, in particular, auto-tagging and\nautomatic playlist continuation. Even though the latter can be addressed by\ncollaborative filtering approaches, audio provides opportunities for research\non track suggestions and building systems resistant to the cold-start problem,\nfor which we provide a baseline. Moreover, the playlists and the annotations\nincluded in the Melon Playlist Dataset make it suitable for metric learning and\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 10:13:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ferraro", "Andres", ""], ["Kim", "Yuntae", ""], ["Lee", "Soohyeon", ""], ["Kim", "Biho", ""], ["Jo", "Namjun", ""], ["Lim", "Semi", ""], ["Lim", "Suyon", ""], ["Jang", "Jungtaek", ""], ["Kim", "Sehwan", ""], ["Serra", "Xavier", ""], ["Bogdanov", "Dmitry", ""]]}, {"id": "2102.00502", "submitter": "Yifan Wang", "authors": "Yifan Wang, Zhanxuan Mei, Chia-Yang Tsai, Ioannis Katsavounidis, C.-C.\n  Jay Kuo", "title": "A Machine Learning Approach to Optimal Inverse Discrete Cosine Transform\n  (IDCT) Design", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design of the optimal inverse discrete cosine transform (IDCT) to\ncompensate the quantization error is proposed for effective lossy image\ncompression in this work. The forward and inverse DCTs are designed in pair in\ncurrent image/video coding standards without taking the quantization effect\ninto account. Yet, the distribution of quantized DCT coefficients deviate from\nthat of original DCT coefficients. This is particularly obvious when the\nquality factor of JPEG compressed images is small. To address this problem, we\nfirst use a set of training images to learn the compound effect of forward DCT,\nquantization and dequantization in cascade. Then, a new IDCT kernel is learned\nto reverse the effect of such a pipeline. Experiments are conducted to\ndemonstrate that the advantage of the new method, which has a gain of\n0.11-0.30dB over the standard JPEG over a wide range of quality factors.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 17:53:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Yifan", ""], ["Mei", "Zhanxuan", ""], ["Tsai", "Chia-Yang", ""], ["Katsavounidis", "Ioannis", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2102.00616", "submitter": "Sayan Nag", "authors": "Uddalok Sarkar, Sayan Nag, Medha Basu, Archi Banerjee, Shankha Sanyal,\n  Ranjan Sengupta, Dipak Ghosh", "title": "Neural Network architectures to classify emotions in Indian Classical\n  Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music is often considered as the language of emotions. It has long been known\nto elicit emotions in human being and thus categorizing music based on the type\nof emotions they induce in human being is a very intriguing topic of research.\nWhen the task comes to classify emotions elicited by Indian Classical Music\n(ICM), it becomes much more challenging because of the inherent ambiguity\nassociated with ICM. The fact that a single musical performance can evoke a\nvariety of emotional response in the audience is implicit to the nature of ICM\nrenditions. With the rapid advancements in the field of Deep Learning, this\nMusic Emotion Recognition (MER) task is becoming more and more relevant and\nrobust, hence can be applied to one of the most challenging test case i.e.\nclassifying emotions elicited from ICM. In this paper we present a new dataset\ncalled JUMusEmoDB which presently has 400 audio clips (30 seconds each) where\n200 clips correspond to happy emotions and the remaining 200 clips correspond\nto sad emotion. For supervised classification purposes, we have used 4 existing\ndeep Convolutional Neural Network (CNN) based architectures (resnet18,\nmobilenet v2.0, squeezenet v1.0 and vgg16) on corresponding music spectrograms\nof the 2000 sub-clips (where every clip was segmented into 5 sub-clips of about\n5 seconds each) which contain both time as well as frequency domain\ninformation. The initial results are quite inspiring, and we look forward to\nsetting the baseline values for the dataset using this architecture. This type\nof CNN based classification algorithm using a rich corpus of Indian Classical\nMusic is unique even in the global perspective and can be replicated in other\nmodalities of music also. This dataset is still under development and we plan\nto include more data containing other emotional features as well. We plan to\nmake the dataset publicly available soon.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 03:41:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sarkar", "Uddalok", ""], ["Nag", "Sayan", ""], ["Basu", "Medha", ""], ["Banerjee", "Archi", ""], ["Sanyal", "Shankha", ""], ["Sengupta", "Ranjan", ""], ["Ghosh", "Dipak", ""]]}, {"id": "2102.00646", "submitter": "Yipeng Zhou", "authors": "Xianzhi Zhang, Yipeng Zhou, Di Wu, Miao Hu, James Xi Zheng, Min Chen,\n  Song Guo", "title": "Optimizing Video Caching at the Edge: A Hybrid Multi-Point Process\n  Approach", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is always a challenging problem to deliver a huge volume of videos over\nthe Internet. To meet the high bandwidth and stringent playback demand, one\nfeasible solution is to cache video contents on edge servers based on predicted\nvideo popularity. Traditional caching algorithms (e.g., LRU, LFU) are too\nsimple to capture the dynamics of video popularity, especially long-tailed\nvideos. Recent learning-driven caching algorithms (e.g., DeepCache) show\npromising performance, however, such black-box approaches are lack of\nexplainability and interpretability. Moreover, the parameter tuning requires a\nlarge number of historical records, which are difficult to obtain for videos\nwith low popularity. In this paper, we optimize video caching at the edge using\na white-box approach, which is highly efficient and also completely\nexplainable. To accurately capture the evolution of video popularity, we\ndevelop a mathematical model called \\emph{HRS} model, which is the combination\nof multiple point processes, including Hawkes' self-exciting, reactive and\nself-correcting processes. The key advantage of the HRS model is its\nexplainability, and much less number of model parameters. In addition, all its\nmodel parameters can be learned automatically through maximizing the\nLog-likelihood function constructed by past video request events. Next, we\nfurther design an online HRS-based video caching algorithm. To verify its\neffectiveness, we conduct a series of experiments using real video traces\ncollected from Tencent Video, one of the largest online video providers in\nChina. Experiment results demonstrate that our proposed algorithm outperforms\nthe state-of-the-art algorithms, with 12.3\\% improvement on average in terms of\ncache hit rate under realistic settings.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:47:58 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhang", "Xianzhi", ""], ["Zhou", "Yipeng", ""], ["Wu", "Di", ""], ["Hu", "Miao", ""], ["Zheng", "James Xi", ""], ["Chen", "Min", ""], ["Guo", "Song", ""]]}, {"id": "2102.00648", "submitter": "Shu Zhao", "authors": "Shu Zhao, Dayan Wu, Yucan Zhou, Bo Li and Weiping Wang", "title": "Rescuing Deep Hashing from Dead Bits Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing methods have shown great retrieval accuracy and efficiency in\nlarge-scale image retrieval. How to optimize discrete hash bits is always the\nfocus in deep hashing methods. A common strategy in these methods is to adopt\nan activation function, e.g. $\\operatorname{sigmoid}(\\cdot)$ or\n$\\operatorname{tanh}(\\cdot)$, and minimize a quantization loss to approximate\ndiscrete values. However, this paradigm may make more and more hash bits stuck\ninto the wrong saturated area of the activation functions and never escaped. We\ncall this problem \"Dead Bits Problem~(DBP)\". Besides, the existing quantization\nloss will aggravate DBP as well. In this paper, we propose a simple but\neffective gradient amplifier which acts before activation functions to\nalleviate DBP. Moreover, we devise an error-aware quantization loss to further\nalleviate DBP. It avoids the negative effect of quantization loss based on the\nsimilarity between two images. The proposed gradient amplifier and error-aware\nquantization loss are compatible with a variety of deep hashing methods.\nExperimental results on three datasets demonstrate the efficiency of the\nproposed gradient amplifier and the error-aware quantization loss.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:51:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Shu", ""], ["Wu", "Dayan", ""], ["Zhou", "Yucan", ""], ["Li", "Bo", ""], ["Wang", "Weiping", ""]]}, {"id": "2102.00653", "submitter": "Changsheng Chen", "authors": "Lin Zhao, Changsheng Chen, Jiwu Huang", "title": "Deep Learning-based Forgery Attack on Document Images", "comments": "13 pages, 17 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ongoing popularization of online services, the digital document\nimages have been used in various applications. Meanwhile, there have emerged\nsome deep learning-based text editing algorithms which alter the textual\ninformation of an image . In this work, we present a document forgery algorithm\nto edit practical document images. To achieve this goal, the limitations of\nexisting text editing algorithms towards complicated characters and complex\nbackground are addressed by a set of network design strategies. First, the\nunnecessary confusion in the supervision data is avoided by disentangling the\ntextual and background information in the source images. Second, to capture the\nstructure of some complicated components, the text skeleton is provided as\nauxiliary information and the continuity in texture is considered explicitly in\nthe loss function. Third, the forgery traces induced by the text editing\noperation are mitigated by some post-processing operations which consider the\ndistortions from the print-and-scan channel. Quantitative comparisons of the\nproposed method and the exiting approach have shown the advantages of our\ndesign by reducing the about 2/3 reconstruction error measured in MSE,\nimproving reconstruction quality measured in PSNR and in SSIM by 4 dB and 0.21,\nrespectively. Qualitative experiments have confirmed that the reconstruction\nresults of the proposed method are visually better than the existing approach.\nMore importantly, we have demonstrated the performance of the proposed document\nforgery algorithm under a practical scenario where an attacker is able to alter\nthe textual information in an identity document using only one sample in the\ntarget domain. The forged-and-recaptured samples created by the proposed text\nediting attack and recapturing operation have successfully fooled some existing\ndocument authentication systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 06:00:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Lin", ""], ["Chen", "Changsheng", ""], ["Huang", "Jiwu", ""]]}, {"id": "2102.00921", "submitter": "Alexander Schl\\\"ogl", "authors": "Alexander Schl\\\"ogl, Tobias Kupek, Rainer B\\\"ohme", "title": "Forensicability of Deep Neural Network Inference Pipelines", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methods to infer properties of the execution environment of\nmachine learning pipelines by tracing characteristic numerical deviations in\nobservable outputs. Results from a series of proof-of-concept experiments\nobtained on local and cloud-hosted machines give rise to possible forensic\napplications, such as the identification of the hardware platform used to\nproduce deep neural network predictions. Finally, we introduce boundary samples\nthat amplify the numerical deviations in order to distinguish machines by their\npredicted label only.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:41:49 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 11:57:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Schl\u00f6gl", "Alexander", ""], ["Kupek", "Tobias", ""], ["B\u00f6hme", "Rainer", ""]]}, {"id": "2102.01163", "submitter": "Kaiping Chen", "authors": "Kaiping Chen, Sang Jung Kim, Sebastian Raschka, Qiantong Gao", "title": "Visual Framing of Science Conspiracy Videos: Integrating Machine\n  Learning with Communication Theories to Study the Use of Color and Brightness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an explosion of science conspiracy videos on the\nInternet, challenging science epistemology and public understanding of science.\nScholars have started to examine the persuasion techniques used in conspiracy\nmessages such as uncertainty and fear yet, little is understood about the\nvisual narratives, especially how visual narratives differ in videos that\ndebunk conspiracies versus those that propagate conspiracies. This paper\naddresses this gap in understanding visual framing in conspiracy videos through\nanalyzing millions of frames from conspiracy and counter-conspiracy YouTube\nvideos using computational methods. We found that conspiracy videos tended to\nuse lower color variance and brightness, especially in thumbnails and earlier\nparts of the videos. This paper also demonstrates how researchers can integrate\ntextual and visual features for identifying conspiracies on social media and\ndiscusses the implications of computational modeling for scholars interested in\nstudying visual manipulation in the digital era.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:03:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Kaiping", ""], ["Kim", "Sang Jung", ""], ["Raschka", "Sebastian", ""], ["Gao", "Qiantong", ""]]}, {"id": "2102.01173", "submitter": "Tony Zhao", "authors": "Tony Zhao, Irving Fang, Jeffrey Kim, Gerald Friedland", "title": "Multi-modal Ensemble Models for Predicting Video Memorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling media memorability has been a consistent challenge in the field of\nmachine learning. The Predicting Media Memorability task in MediaEval2020 is\nthe latest benchmark among similar challenges addressing this topic. Building\nupon techniques developed in previous iterations of the challenge, we developed\nensemble methods with the use of extracted video, image, text, and audio\nfeatures. Critically, in this work we introduce and demonstrate the efficacy\nand high generalizability of extracted audio embeddings as a feature for the\ntask of predicting media memorability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:16:52 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhao", "Tony", ""], ["Fang", "Irving", ""], ["Kim", "Jeffrey", ""], ["Friedland", "Gerald", ""]]}, {"id": "2102.01272", "submitter": "Lan Wang", "authors": "Bo Zhang, Di Xiao, Lan Wang, Sen Bai and Lei Yang", "title": "Efficient Compressed Sensing Based Image Coding by Using Gray\n  Transformation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, compressed sensing (CS) based image coding has become a hot\ntopic in image processing field. However, since the bit depth required for\nencoding each CS sample is too large, the compression performance of this\nparadigm is unattractive. To address this issue, a novel CS-based image coding\nsystem by using gray transformation is proposed. In the proposed system, we use\na gray transformation to preprocess the original image firstly and then use CS\nto sample the transformed image. Since gray transformation makes the\nprobability distribution of CS samples centralized, the bit depth required for\nencoding each CS sample is reduced significantly. Consequently, the proposed\nsystem can considerably improve the compression performance of CS-based image\ncoding. Simulation results show that the proposed system outperforms the\ntraditional one without using gray transformation in terms of compression\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:09:56 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhang", "Bo", ""], ["Xiao", "Di", ""], ["Wang", "Lan", ""], ["Bai", "Sen", ""], ["Yang", "Lei", ""]]}, {"id": "2102.01313", "submitter": "Miki Tanaka", "authors": "Miki Tanaka, Hitoshi Kiya", "title": "Fake-image detection with Robust Hashing", "comments": "to be appear in Life Tech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate whether robust hashing has a possibility to\nrobustly detect fake-images even when multiple manipulation techniques such as\nJPEG compression are applied to images for the first time. In an experiment,\nthe proposed fake detection with robust hashing is demonstrated to outperform\nstate-of-the-art one under the use of various datasets including fake images\ngenerated with GANs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 05:10:37 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Tanaka", "Miki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2102.01486", "submitter": "Cheng Ma", "authors": "Cheng Ma, Jiwen Lu, Jie Zhou", "title": "Rank-Consistency Deep Hashing for Scalable Multi-Label Image Search", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2020", "doi": "10.1109/TMM.2020.3034534", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As hashing becomes an increasingly appealing technique for large-scale image\nretrieval, multi-label hashing is also attracting more attention for the\nability to exploit multi-level semantic contents. In this paper, we propose a\nnovel deep hashing method for scalable multi-label image search. Unlike\nexisting approaches with conventional objectives such as contrast and triplet\nlosses, we employ a rank list, rather than pairs or triplets, to provide\nsufficient global supervision information for all the samples. Specifically, a\nnew rank-consistency objective is applied to align the similarity orders from\ntwo spaces, the original space and the hamming space. A powerful loss function\nis designed to penalize the samples whose semantic similarity and hamming\ndistance are mismatched in two spaces. Besides, a multi-label softmax\ncross-entropy loss is presented to enhance the discriminative power with a\nconcise formulation of the derivative function. In order to manipulate the\nneighborhood structure of the samples with different labels, we design a\nmulti-label clustering loss to cluster the hashing vectors of the samples with\nthe same labels by reducing the distances between the samples and their\nmultiple corresponding class centers. The state-of-the-art experimental results\nachieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and\nNUS-WIDE, demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 13:46:58 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ma", "Cheng", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2102.01893", "submitter": "Teruaki Akazawa", "authors": "Teruaki Akazawa, Yuma Kinoshita and Hitoshi Kiya", "title": "Multi-color balancing for correctly adjusting the intensity of target\n  colors", "comments": "\\c{opyright} 2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "In Proc. of 2021 IEEE 3rd Global Conference on Life Sciences and\n  Technologies (LifeTech), pp.8-12, Mar., 2021", "doi": "10.1109/LifeTech52111.2021.9391973", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-color balance method for reducing\ncolor distortions caused by lighting effects. The proposed method allows us to\nadjust three target-colors chosen by a user in an input image so that each\ntarget color is the same as the corresponding destination (benchmark) one. In\ncontrast, white balancing is a typical technique for reducing the color\ndistortions, however, they cannot remove lighting effects on colors other than\nwhite. In an experiment, the proposed method is demonstrated to be able to\nremove lighting effects on selected three colors, and is compared with existing\nwhite balance adjustments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:28:18 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 09:21:37 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Akazawa", "Teruaki", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2102.01974", "submitter": "Alasdair Tran", "authors": "Minjeong Shin, Alasdair Tran, Siqi Wu, Alexander Mathews, Rong Wang,\n  Georgiana Lyall, Lexing Xie", "title": "AttentionFlow: Visualising Influence in Networks of Time Series", "comments": "Published in WSDM 2021. The demo is available at\n  https://attentionflow.ml and code is available at\n  https://github.com/alasdairtran/attentionflow", "journal-ref": "The Proceedings of the Fourteenth ACM International Conference on\n  Web Search and Data Mining (WSDM), 2021", "doi": "10.1145/3437963.3441703", "report-no": null, "categories": "cs.SI cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collective attention on online items such as web pages, search terms, and\nvideos reflects trends that are of social, cultural, and economic interest.\nMoreover, attention trends of different items exhibit mutual influence via\nmechanisms such as hyperlinks or recommendations. Many visualisation tools\nexist for time series, network evolution, or network influence; however, few\nsystems connect all three. In this work, we present AttentionFlow, a new system\nto visualise networks of time series and the dynamic influence they have on one\nanother. Centred around an ego node, our system simultaneously presents the\ntime series on each node using two visual encodings: a tree ring for an\noverview and a line chart for details. AttentionFlow supports interactions such\nas overlaying time series of influence and filtering neighbours by time or\nflux. We demonstrate AttentionFlow using two real-world datasets, VevoMusic and\nWikiTraffic. We show that attention spikes in songs can be explained by\nexternal events such as major awards, or changes in the network such as the\nrelease of a new song. Separate case studies also demonstrate how an artist's\ninfluence changes over their career, and that correlated Wikipedia traffic is\ndriven by cultural interests. More broadly, AttentionFlow can be generalised to\nvisualise networks of time series on physical infrastructures such as road\nnetworks, or natural phenomena such as weather and geological measurements.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:44:46 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Shin", "Minjeong", ""], ["Tran", "Alasdair", ""], ["Wu", "Siqi", ""], ["Mathews", "Alexander", ""], ["Wang", "Rong", ""], ["Lyall", "Georgiana", ""], ["Xie", "Lexing", ""]]}, {"id": "2102.02282", "submitter": "Bruno Di Giorgi", "authors": "Bruno Di Giorgi, Matthias Mauch, Mark Levy", "title": "Downbeat Tracking with Tempo-Invariant Convolutional Neural Networks", "comments": "7 pages, 5 figures, Proceedings of the 21st International Society for\n  Music Information Retrieval Conference, ISMIR 2020", "journal-ref": "Proceedings of the 21st International Society for Music\n  Information Retrieval Conference (2020) 216-222", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human ability to track musical downbeats is robust to changes in tempo,\nand it extends to tempi never previously encountered. We propose a\ndeterministic time-warping operation that enables this skill in a convolutional\nneural network (CNN) by allowing the network to learn rhythmic patterns\nindependently of tempo. Unlike conventional deep learning approaches, which\nlearn rhythmic patterns at the tempi present in the training dataset, the\npatterns learned in our model are tempo-invariant, leading to better tempo\ngeneralisation and more efficient usage of the network capacity. We test the\ngeneralisation property on a synthetic dataset created by rendering the Groove\nMIDI Dataset using FluidSynth, split into a training set containing the\noriginal performances and a test set containing tempo-scaled versions rendered\nwith different SoundFonts (test-time augmentation). The proposed model\ngeneralises nearly perfectly to unseen tempi (F-measure of 0.89 on both\ntraining and test sets), whereas a comparable conventional CNN achieves similar\naccuracy only for the training set (0.89) and drops to 0.54 on the test set.\nThe generalisation advantage of the proposed model extends to real music, as\nshown by results on the GTZAN and Ballroom datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 20:25:36 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Di Giorgi", "Bruno", ""], ["Mauch", "Matthias", ""], ["Levy", "Mark", ""]]}, {"id": "2102.02640", "submitter": "Gang Min", "authors": "Gang Min, Xiongwei Zhang, Xia Zou, Xiangyang Liu", "title": "Low Bit-Rate Wideband Speech Coding: A Deep Generative Model based\n  Approach", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional low bit-rate speech coding approach only handles narrowband\nspeech at 8kHz, which limits further improvements in speech quality. Motivated\nby recent successful exploration of deep learning methods for image and speech\ncompression, this paper presents a new approach through vector quantization\n(VQ) of mel-frequency cepstral coefficients (MFCCs) and using a deep generative\nmodel called WaveGlow to provide efficient and high-quality speech coding. The\ncoding feature is sorely an 80-dimension MFCCs vector for 16kHz wideband\nspeech, then speech coding at the bit-rate throughout 1000-2000 bit/s could be\nscalably implemented by applying different VQ schemes for MFCCs vector. This\nnew deep generative network based codec works fast as the WaveGlow model\nabandons the sample-by-sample autoregressive mechanism. We evaluated this new\napproach over the multi-speaker TIMIT corpus, and experimental results\ndemonstrate that it provides better speech quality compared with the\nstate-of-the-art classic MELPe codec at lower bit-rate.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 14:37:16 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Min", "Gang", ""], ["Zhang", "Xiongwei", ""], ["Zou", "Xia", ""], ["Liu", "Xiangyang", ""]]}, {"id": "2102.02670", "submitter": "Huiyuan Deng", "authors": "Huiyuan Deng, Xiangzhu Meng, Lin Feng", "title": "Multimodal-Aware Weakly Supervised Metric Learning with Self-weighting\n  Triplet Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, we have witnessed a surge of interests in learning a\nsuitable distance metric from weakly supervised data. Most existing methods aim\nto pull all the similar samples closer while push the dissimilar ones as far as\npossible. However, when some classes of the dataset exhibit multimodal\ndistribution, these goals conflict and thus can hardly be concurrently\nsatisfied. Additionally, to ensure a valid metric, many methods require a\nrepeated eigenvalue decomposition process, which is expensive and numerically\nunstable. Therefore, how to learn an appropriate distance metric from weakly\nsupervised data remains an open but challenging problem. To address this issue,\nin this paper, we propose a novel weakly supervised metric learning algorithm,\nnamed MultimoDal Aware weakly supervised Metric Learning (MDaML). MDaML\npartitions the data space into several clusters and allocates the local cluster\ncenters and weight for each sample. Then, combining it with the weighted\ntriplet loss can further enhance the local separability, which encourages the\nlocal dissimilar samples to keep a large distance from the local similar\nsamples. Meanwhile, MDaML casts the metric learning problem into an\nunconstrained optimization on the SPD manifold, which can be efficiently solved\nby Riemannian Conjugate Gradient Descent (RCGD). Extensive experiments\nconducted on 13 datasets validate the superiority of the proposed MDaML.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 07:27:05 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Deng", "Huiyuan", ""], ["Meng", "Xiangzhu", ""], ["Feng", "Lin", ""]]}, {"id": "2102.03179", "submitter": "Matti Pouke", "authors": "Matti Pouke, Katherine J. Mimnaugh, Alexis Chambers, Timo Ojala,\n  Steven M. LaValle", "title": "The Plausibility Paradox for Resized Users in Virtual Environments", "comments": "Preprint. arXiv admin note: substantial text overlap with\n  arXiv:1912.01947", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies and confirms a perceptual phenomenon: when users\ninteract with simulated objects in a virtual environment where the users' scale\ndeviates greatly from normal, there is a mismatch between the object physics\nthey consider realistic and the object physics that would be correct at that\nscale. We report the findings of two studies investigating the relationship\nbetween perceived realism and a physically accurate approximation of reality in\na virtual reality experience in which the user has been scaled by a factor of\nten. Study 1 investigated perception of physics when scaled-down by a factor of\nten, whereas Study 2 focused on enlargement by a similar amount. Studies were\ncarried out as within-subjects experiments in which a total of 84 subjects\nperformed simple interaction tasks with objects under two different physics\nsimulation conditions. In the true physics condition, the objects, when dropped\nand thrown, behaved accurately according to the physics that would be correct\nat that either reduced or enlarged scale in the real world. In the movie\nphysics condition, the objects behaved in a similar manner as they would if no\nscaling of the user had occurred. We found that a significant majority of the\nusers considered the movie physics condition to be the more realistic one.\nHowever, at enlarged scale, many users considered true physics to match their\nexpectations even if they ultimately believed movie physics to be the realistic\ncondition. We argue that our findings have implications for many virtual\nreality and telepresence applications involving operation with simulated or\nphysical objects in abnormal and especially small scales.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 13:55:31 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Pouke", "Matti", ""], ["Mimnaugh", "Katherine J.", ""], ["Chambers", "Alexis", ""], ["Ojala", "Timo", ""], ["LaValle", "Steven M.", ""]]}, {"id": "2102.04910", "submitter": "Konstantinos Konstantoudakis", "authors": "Konstantinos Konstantoudakis, David Breitgand, Alexandros Doumanoglou,\n  Nikolaos Zioulis, Avi Weit, Kyriaki Christaki, Petros Drakoulis, Emmanouil\n  Christakis, Dimitrios Zarpalas, Petros Daras", "title": "Serverless Streaming for Emerging Media: Towards 5G Network-Driven Cost\n  Optimization", "comments": "32 pages, 12 figures, preprint: to appear in \"Multimedia Tools and\n  Applications: 5G Multimedia Communications\" special issue", "journal-ref": null, "doi": "10.1007/s11042-020-10219-7", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Immersive 3D media is an emerging type of media that captures, encodes and\nreconstructs the 3D appearance of people and objects, with applications in\ntele-presence, teleconference, entertainment, gaming and other fields. In this\npaper, we discuss a novel concept of live 3D immersive media streaming in a\nserverless setting. In particular, we present a novel network-centric adaptive\nstreaming framework which deviates from a traditional client-based adaptive\nstreaming used in 2D video. In our framework, the decisions for the production\nof the transcoding profiles are taken in a centralized manner, by considering\nconsumer metrics vs provisioning costs and inferring an expected consumer\nquality of experience and behaviour based on them. In addition, we demonstrate\nthat a naive application of the serverless paradigm might be sub optimal under\nsome common immersive 3D media scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:08:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Konstantoudakis", "Konstantinos", ""], ["Breitgand", "David", ""], ["Doumanoglou", "Alexandros", ""], ["Zioulis", "Nikolaos", ""], ["Weit", "Avi", ""], ["Christaki", "Kyriaki", ""], ["Drakoulis", "Petros", ""], ["Christakis", "Emmanouil", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2102.04958", "submitter": "Robyn Kozierok", "authors": "Robyn Kozierok, John Aberdeen, Cheryl Clark, Christopher Garay,\n  Bradley Goodman, Tonia Korves, Lynette Hirschman, Patricia L. McDermott,\n  Matthew W. Peterson", "title": "Hallmarks of Human-Machine Collaboration: A framework for assessment in\n  the DARPA Communicating with Computers Program", "comments": "20 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": "MITRE Document Number: MTR210002", "categories": "cs.HC cs.AI cs.CL cs.MA cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing desire to create computer systems that can communicate\neffectively to collaborate with humans on complex, open-ended activities.\nAssessing these systems presents significant challenges. We describe a\nframework for evaluating systems engaged in open-ended complex scenarios where\nevaluators do not have the luxury of comparing performance to a single right\nanswer. This framework has been used to evaluate human-machine creative\ncollaborations across story and music generation, interactive block building,\nand exploration of molecular mechanisms in cancer. These activities are\nfundamentally different from the more constrained tasks performed by most\ncontemporary personal assistants as they are generally open-ended, with no\nsingle correct solution, and often no obvious completion criteria.\n  We identified the Key Properties that must be exhibited by successful\nsystems. From there we identified \"Hallmarks\" of success -- capabilities and\nfeatures that evaluators can observe that would be indicative of progress\ntoward achieving a Key Property. In addition to being a framework for\nassessment, the Key Properties and Hallmarks are intended to serve as goals in\nguiding research direction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:13:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kozierok", "Robyn", ""], ["Aberdeen", "John", ""], ["Clark", "Cheryl", ""], ["Garay", "Christopher", ""], ["Goodman", "Bradley", ""], ["Korves", "Tonia", ""], ["Hirschman", "Lynette", ""], ["McDermott", "Patricia L.", ""], ["Peterson", "Matthew W.", ""]]}, {"id": "2102.04993", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc G\\'orriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta\n  Mrak", "title": "Attention-Based Neural Networks for Chroma Intra Prediction in Video\n  Coding", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 2020", "doi": "10.1109/JSTSP.2020.3044482", "report-no": null, "categories": "eess.IV cs.CC cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks can be successfully used to improve several modules of\nadvanced video coding schemes. In particular, compression of colour components\nwas shown to greatly benefit from usage of machine learning models, thanks to\nthe design of appropriate attention-based architectures that allow the\nprediction to exploit specific samples in the reference region. However, such\narchitectures tend to be complex and computationally intense, and may be\ndifficult to deploy in a practical video coding pipeline. This work focuses on\nreducing the complexity of such methodologies, to design a set of simplified\nand cost-effective attention-based architectures for chroma intra-prediction. A\nnovel size-agnostic multi-model approach is proposed to reduce the complexity\nof the inference process. The resulting simplified architecture is still\ncapable of outperforming state-of-the-art methods. Moreover, a collection of\nsimplifications is presented in this paper, to further reduce the complexity\noverhead of the proposed prediction architecture. Thanks to these\nsimplifications, a reduction in the number of parameters of around 90% is\nachieved with respect to the original attention-based methodologies.\nSimplifications include a framework for reducing the overhead of the\nconvolutional operations, a simplified cross-component processing model\nintegrated into the original architecture, and a methodology to perform\ninteger-precision approximations with the aim to obtain fast and hardware-aware\nimplementations. The proposed schemes are integrated into the Versatile Video\nCoding (VVC) prediction pipeline, retaining compression efficiency of\nstate-of-the-art chroma intra-prediction methods based on neural networks,\nwhile offering different directions for significantly reducing coding\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:01:22 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""], ["Mrak", "Marta", ""]]}, {"id": "2102.05067", "submitter": "Silvia Cascianelli PhD", "authors": "Silvia Cascianelli, Gabriele Costante, Alessandro Devo, Thomas A.\n  Ciarfuglia, Paolo Valigi, Mario L. Fravolini", "title": "The Role of the Input in Natural Language Video Description", "comments": "In IEEE Transactions on Multimedia", "journal-ref": "IEEE Transactions on Multimedia, 22(1), 271-283 (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Video Description (NLVD) has recently received strong\ninterest in the Computer Vision, Natural Language Processing (NLP), Multimedia,\nand Autonomous Robotics communities. The State-of-the-Art (SotA) approaches\nobtained remarkable results when tested on the benchmark datasets. However,\nthose approaches poorly generalize to new datasets. In addition, none of the\nexisting works focus on the processing of the input to the NLVD systems, which\nis both visual and textual. In this work, it is presented an extensive study\ndealing with the role of the visual input, evaluated with respect to the\noverall NLP performance. This is achieved performing data augmentation of the\nvisual component, applying common transformations to model camera distortions,\nnoise, lighting, and camera positioning, that are typical in real-world\noperative scenarios. A t-SNE based analysis is proposed to evaluate the effects\nof the considered transformations on the overall visual data distribution. For\nthis study, it is considered the English subset of Microsoft Research Video\nDescription (MSVD) dataset, which is used commonly for NLVD. It was observed\nthat this dataset contains a relevant amount of syntactic and semantic errors.\nThese errors have been amended manually, and the new version of the dataset\n(called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is\nreleased to help to gain insight into the NLVD problem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:00:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cascianelli", "Silvia", ""], ["Costante", "Gabriele", ""], ["Devo", "Alessandro", ""], ["Ciarfuglia", "Thomas A.", ""], ["Valigi", "Paolo", ""], ["Fravolini", "Mario L.", ""]]}, {"id": "2102.05105", "submitter": "Angel Villar-Corrales", "authors": "Angel Villar-Corrales, Franziska Schirrmacher and Christian Riess", "title": "Deep learning architectural designs for super-resolution of noisy images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning have led to significant improvements in\nsingle image super-resolution (SR) research. However, due to the amplification\nof noise during the upsampling steps, state-of-the-art methods often fail at\nreconstructing high-resolution images from noisy versions of their\nlow-resolution counterparts. However, this is especially important for images\nfrom unknown cameras with unseen types of image degradation. In this work, we\npropose to jointly perform denoising and super-resolution. To this end, we\ninvestigate two architectural designs: \"in-network\" combines both tasks at\nfeature level, while \"pre-network\" first performs denoising and then\nsuper-resolution. Our experiments show that both variants have specific\nadvantages: The in-network design obtains the strongest results when the type\nof image corruption is aligned in the training and testing dataset, for any\nchoice of denoiser. The pre-network design exhibits superior performance on\nunseen types of image corruption, which is a pathological failure case of\nexisting super-resolution models. We hope that these findings help to enable\nsuper-resolution also in less constrained scenarios where source camera or\nimaging conditions are not well controlled. Source code and pretrained models\nare available at https://github.com/\nangelvillar96/super-resolution-noisy-images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:09:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Villar-Corrales", "Angel", ""], ["Schirrmacher", "Franziska", ""], ["Riess", "Christian", ""]]}, {"id": "2102.06732", "submitter": "Jiapeng Wang", "authors": "Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang,\n  Shuaitao Zhang, Qianying Wang, Yaqiang Wu, Mingxiang Cai", "title": "Towards Robust Visual Information Extraction in Real World: New Dataset\n  and Novel Solution", "comments": "8 pages, 5 figures, to be published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information extraction (VIE) has attracted considerable attention\nrecently owing to its various advanced applications such as document\nunderstanding, automatic marking and intelligent education. Most existing works\ndecoupled this problem into several independent sub-tasks of text spotting\n(text detection and recognition) and information extraction, which completely\nignored the high correlation among them during optimization. In this paper, we\npropose a robust visual information extraction system (VIES) towards real-world\nscenarios, which is a unified end-to-end trainable framework for simultaneous\ntext detection, recognition and information extraction by taking a single\ndocument image as input and outputting the structured information.\nSpecifically, the information extraction branch collects abundant visual and\nsemantic representations from text spotting for multimodal feature fusion and\nconversely, provides higher-level semantic clues to contribute to the\noptimization of text spotting. Moreover, regarding the shortage of public\nbenchmarks, we construct a fully-annotated dataset called EPHOIE\n(https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for\nboth text spotting and visual information extraction. EPHOIE consists of 1,494\nimages of examination paper head with complex layouts and background, including\na total of 15,771 Chinese handwritten or printed text instances. Compared with\nthe state-of-the-art methods, our VIES shows significant superior performance\non the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used\nSROIE dataset under the end-to-end scenario.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 11:05:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Jiapeng", ""], ["Liu", "Chongyu", ""], ["Jin", "Lianwen", ""], ["Tang", "Guozhi", ""], ["Zhang", "Jiaxin", ""], ["Zhang", "Shuaitao", ""], ["Wang", "Qianying", ""], ["Wu", "Yaqiang", ""], ["Cai", "Mingxiang", ""]]}, {"id": "2102.06774", "submitter": "Hanieh Rafiee", "authors": "Haniyeh Rafiee and Mohammad Fakhredanesh", "title": "Presenting a Method for Improving Echo Hiding", "comments": "14 page, This paper is printed in Journal of Computer and Knowledge\n  Engineering, Vol. 2, No. 1", "journal-ref": null, "doi": "10.22067/CKE.V2I1.74388", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article, one of the most important methods of steganography on VoIP\ncalled echo hiding is improved. This method has advantages in maintaining the\nstatistical and perceptual characteristics of audio signals as well as security\nagainst the sensitivity of the human audio system (HAS). However, it has lots\nof errors in detecting coded and hidden messages, which is detectable using\nexisting steganalysis methods. The percentage of extracting messages in these\nimproved methods of echo hiding is high, but they lower the security of the\nmethod. In this article, a method is presented to improve the method of\nextracting echo hiding, and enhance its security through a combined method\nbased on spread spectrum. To improve the extraction, a wrong hypothesis is\ncorrected and substituted. To improve security using a pseudo-random key\ngeneration algorithm, spread spectrum and echo hiding methods are used\nrandomly. To evaluate the proposed extraction, numerous extraction tests are\ncarried out in the normal state and in the event of attacks. A steganalyser has\nalso been used to assess security improvements. The results gained through\ndifferent experiments on the security of steganography indicate a 3-percent\nincrease in steganalysis errors. The proposed extraction method was modified\nbased on the main method and resulted in more than 10% improvement.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 21:09:36 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Rafiee", "Haniyeh", ""], ["Fakhredanesh", "Mohammad", ""]]}, {"id": "2102.06826", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu, Gen Liu and Xinpeng Zhang", "title": "Hiding Data Hiding", "comments": "https://hzwu.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data hiding is referred to as the art of hiding secret data into a digital\ncover for covert communication. In this letter, we propose a novel method to\ndisguise data hiding tools, including a data embedding tool and a data\nextraction tool, as a deep neural network (DNN) with an ordinary task. After\ntraining a DNN for both style transfer and data hiding, while the DNN can\ntransfer the style of an image to a target one, it can be also used to hide\nsecret data into a cover image or extract secret data from a stego image by\ninputting the trigger signal. In other words, the tools of data hiding are\nhidden to avoid arousing suspicion.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 00:23:58 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 06:39:04 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wu", "Hanzhou", ""], ["Liu", "Gen", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2102.06954", "submitter": "M Anandaraj", "authors": "S Naganandhini, M Anandaraj, S Manojkumar, K Selvaraj, P Ganeshkumar", "title": "An Efficient Framework for Piece Selection Problem in P2P Content\n  Distribution Network Using Fuzzy Programming Approach", "comments": "18 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fuzzy programming approach is used in this article for solving the piece\nselection problem in P2P network with multiple objectives, in which some of the\nfactors are fuzzy in nature. A piece selection problem has been prepared as a\nfuzzy mixed integer goal programming piece selection problem that includes\nthree primary goals: minimizing the download cost and download time and\nmaximizing speed and useful information transmission subject to realistic\nconstraints regarding peer's demand, peer's capacity, peer's dynamicity, etc.\nThe proposed approach has the ability to handle practical situations in a fuzzy\nenvironment and offers a better decision tool for the piece selection decision\nin a dynamic P2P network. An extensive simulation is carried out to demonstrate\nthe effectiveness of the proposed model. The proposed mechanism has the\ncapability to handle practical situations in a fuzzy environment and offers a\nbetter decision tool for the piece selection decision in a decentralized P2P\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 16:47:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Naganandhini", "S", ""], ["Anandaraj", "M", ""], ["Manojkumar", "S", ""], ["Selvaraj", "K", ""], ["Ganeshkumar", "P", ""]]}, {"id": "2102.07195", "submitter": "Mohamed Alrshah", "authors": "Ali A. Elrowayati, Mohamed A. Alrshah, M.F.L. Abdullah, Rohaya Latip", "title": "HEVC Watermarking Techniques for Authentication and Copyright\n  Applications: Challenges and Opportunities", "comments": "Review article, 20 pages", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3004049", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, High-Efficiency Video Coding (HEVC/H.265) has been chosen to\nreplace previous video coding standards, such as H.263 and H.264. Despite the\nefficiency of HEVC, it still lacks reliable and practical functionalities to\nsupport authentication and copyright applications. In order to provide this\nsupport, several watermarking techniques have been proposed by many researchers\nduring the last few years. However, those techniques are still suffering from\nmany issues that need to be considered for future designs. In this paper, a\nSystematic Literature Review (SLR) is introduced to identify HEVC challenges\nand potential research directions for interested researchers and developers.\nThe time scope of this SLR covers all research articles published during the\nlast six years starting from January 2014 up to the end of April 2020.\nForty-two articles have met the criteria of selection out of 343 articles\npublished in this area during the mentioned time scope. A new classification\nhas been drawn followed by an identification of the challenges of implementing\nHEVC watermarking techniques based on the analysis and discussion of those\nchosen articles. Eventually, recommendations for HEVC watermarking techniques\nhave been listed to help researchers to improve the existing techniques or to\ndesign new efficient ones.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 16:56:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Elrowayati", "Ali A.", ""], ["Alrshah", "Mohamed A.", ""], ["Abdullah", "M. F. L.", ""], ["Latip", "Rohaya", ""]]}, {"id": "2102.07365", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari, Siddhartha Chaudhuri, Vivek Borkar, Subhasis\n  Chaudhuri", "title": "A Unified Batch Selection Policy for Active Metric Learning", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.MM math.IT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 06:55:17 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 04:11:42 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 06:04:22 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 12:16:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["Chaudhuri", "Siddhartha", ""], ["Borkar", "Vivek", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2102.08192", "submitter": "Federico Chiariotti", "authors": "Federico Chiariotti", "title": "A Survey on 360-Degree Video: Coding, Quality of Experience and\n  Streaming", "comments": "Submitted to Elsevier Computer Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The commercialization of Virtual Reality (VR) headsets has made immersive and\n360-degree video streaming the subject of intense interest in the industry and\nresearch communities. While the basic principles of video streaming are the\nsame, immersive video presents a set of specific challenges that need to be\naddressed. In this survey, we present the latest developments in the relevant\nliterature on four of the most important ones: (i) omnidirectional video coding\nand compression, (ii) subjective and objective Quality of Experience (QoE) and\nthe factors that can affect it, (iii) saliency measurement and Field of View\n(FoV) prediction, and (iv) the adaptive streaming of immersive 360-degree\nvideos. The final objective of the survey is to provide an overview of the\nresearch on all the elements of an immersive video streaming system, giving the\nreader an understanding of their interplay and performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:39:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Chiariotti", "Federico", ""]]}, {"id": "2102.08893", "submitter": "Maha Mohammed Khan", "authors": "Maha Mohammed Khan", "title": "An Implementation of Vector Quantization using the Genetic Algorithm\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of machine learning(ML) and genetic programming(GP) to the\nimage compression domain has produced promising results in many cases. The need\nfor compression arises due to the exorbitant size of data shared on the\ninternet. Compression is required for text, videos, or images, which are used\nalmost everywhere on web be it news articles, social media posts, blogs,\neducational platforms, medical domain, government services, and many other\nwebsites, need packets for transmission and hence compression is necessary to\navoid overwhelming the network. This paper discusses some of the\nimplementations of image compression algorithms that use techniques such as\nArtificial Neural Networks, Residual Learning, Fuzzy Neural Networks,\nConvolutional Neural Nets, Deep Learning, Genetic Algorithms. The paper also\ndescribes an implementation of Vector Quantization using GA to generate\ncodebook which is used for Lossy image compression. All these approaches prove\nto be very contrasting to the standard approaches to processing images due to\nthe highly parallel and computationally extensive nature of machine learning\nalgorithms. Such non-linear abilities of ML and GP make it widely popular for\nuse in multiple domains. Traditional approaches are also combined with\nartificially intelligent systems, leading to hybrid systems, to achieve better\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 03:57:13 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Khan", "Maha Mohammed", ""]]}, {"id": "2102.09109", "submitter": "Eva Cetinic", "authors": "Eva Cetinic and James She", "title": "Understanding and Creating Art with AI: Review and Outlook", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technologies related to artificial intelligence (AI) have a strong impact on\nthe changes of research and creative practices in visual arts. The growing\nnumber of research initiatives and creative applications that emerge in the\nintersection of AI and art, motivates us to examine and discuss the creative\nand explorative potentials of AI technologies in the context of art. This paper\nprovides an integrated review of two facets of AI and art: 1) AI is used for\nart analysis and employed on digitized artwork collections; 2) AI is used for\ncreative purposes and generating novel artworks. In the context of AI-related\nresearch for art understanding, we present a comprehensive overview of artwork\ndatasets and recent works that address a variety of tasks such as\nclassification, object detection, similarity retrieval, multimodal\nrepresentations, computational aesthetics, etc. In relation to the role of AI\nin creating art, we address various practical and theoretical aspects of AI Art\nand consolidate related works that deal with those topics in detail. Finally,\nwe provide a concise outlook on the future progression and potential impact of\nAI technologies on our understanding and creation of art.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:38:11 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Cetinic", "Eva", ""], ["She", "James", ""]]}, {"id": "2102.09375", "submitter": "Zhe Ma", "authors": "Zhe Ma, Fenghao Liu, Jianfeng Dong, Xiaoye Qu, Yuan He, Shouling Ji", "title": "Hierarchical Similarity Learning for Language-based Product Image\n  Retrieval", "comments": "Accepted by ICASSP 2021. Code and data will be available at\n  https://github.com/liufh1/hsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims for the language-based product image retrieval task. The\nmajority of previous works have made significant progress by designing network\nstructure, similarity measurement, and loss function. However, they typically\nperform vision-text matching at certain granularity regardless of the intrinsic\nmultiple granularities of images. In this paper, we focus on the cross-modal\nsimilarity measurement, and propose a novel Hierarchical Similarity Learning\n(HSL) network. HSL first learns multi-level representations of input data by\nstacked encoders, and object-granularity similarity and image-granularity\nsimilarity are computed at each level. All the similarities are combined as the\nfinal hierarchical cross-modal similarity. Experiments on a large-scale product\nretrieval dataset demonstrate the effectiveness of our proposed method. Code\nand data are available at https://github.com/liufh1/hsl.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:23:16 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ma", "Zhe", ""], ["Liu", "Fenghao", ""], ["Dong", "Jianfeng", ""], ["Qu", "Xiaoye", ""], ["He", "Yuan", ""], ["Ji", "Shouling", ""]]}, {"id": "2102.09744", "submitter": "Zakria Jamali Dr.", "authors": "Zakria, Jianhua Deng, Muhammad Saddam Khokhar, Muhammad Umar Aftab,\n  Jingye Cai, Rajesh Kumar and Jay Kumar", "title": "Trends in Vehicle Re-identification Past, Present, and Future: A\n  Comprehensive Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification (re-id) over surveillance camera network with\nnon-overlapping field of view is an exciting and challenging task in\nintelligent transportation systems (ITS). Due to its versatile applicability in\nmetropolitan cities, it gained significant attention. Vehicle re-id matches\ntargeted vehicle over non-overlapping views in multiple camera network.\nHowever, it becomes more difficult due to inter-class similarity, intra-class\nvariability, viewpoint changes, and spatio-temporal uncertainty. In order to\ndraw a detailed picture of vehicle re-id research, this paper gives a\ncomprehensive description of the various vehicle re-id technologies,\napplicability, datasets, and a brief comparison of different methodologies. Our\npaper specifically focuses on vision-based vehicle re-id approaches, including\nvehicle appearance, license plate, and spatio-temporal characteristics. In\naddition, we explore the main challenges as well as a variety of applications\nin different domains. Lastly, a detailed comparison of current state-of-the-art\nmethods performances over VeRi-776 and VehicleID datasets is summarized with\nfuture directions. We aim to facilitate future research by reviewing the work\nbeing done on vehicle re-id till to date.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 05:02:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zakria", "", ""], ["Deng", "Jianhua", ""], ["Khokhar", "Muhammad Saddam", ""], ["Aftab", "Muhammad Umar", ""], ["Cai", "Jingye", ""], ["Kumar", "Rajesh", ""], ["Kumar", "Jay", ""]]}, {"id": "2102.09847", "submitter": "Juan Gonz\\'alez Salinas", "authors": "Juan Gonz\\'alez, Fernando Boronat, Almanzor Sapena, Javier Pastor", "title": "Key Technologies for Networked Virtual Environments", "comments": "Submitted to Springer Multimedia Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the improvements experienced in technology in the last few years,\nmost especially in virtual reality systems, the number and potential of\nnetworked virtual environments or NVEs and their users are increasing. NVEs aim\nto give distributed users a feeling of immersion in a virtual world and the\npossibility of interacting with other users or with virtual objects inside it,\nlike when they interact in the real world. Being able to provide that feeling\nand natural interactions when the users are geographically separated is one of\nthe goals of these systems. Nevertheless, this goal is especially sensitive to\ndifferent issues, such as different connections with heterogeneous throughput\nor different network latencies, which can lead to consistency and\nsynchronization problems and, thus, to a worsening of the users' quality of\nexperience or QoE. With the purpose of solving these issues, researchers have\nproposed and evaluated numerous technical solutions, in fields like network\narchitectures, data distribution and filtering, resource balancing, computing\nmodels, predictive modeling and synchronization in NVEs. This paper gathers and\nclassifies them, summarizing their advantages and disadvantages, using a new\nway of classification. With the current increase of the number of NVEs and the\nmultiple solutions proposed so far, this work aims to become a useful tool and\na starting point not only for future researchers in this field but also for\nthose who are new in NVEs development, in which guaranteeing a good users' QoE\nis essential.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 10:32:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:57:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gonz\u00e1lez", "Juan", ""], ["Boronat", "Fernando", ""], ["Sapena", "Almanzor", ""], ["Pastor", "Javier", ""]]}, {"id": "2102.10162", "submitter": "Sean Butler Mr", "authors": "Sean Butler", "title": "Clarification of Video Retrieval Query Results by the Automated\n  Insertion of Supporting Shots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Computational Video Editing Systems output video generally follows a\nparticular form, e.g. conversation or music videos, in this way they are domain\nspecific. We describe a recent development in our video annotation and\nsegmentation system to support general computational video editing in which we\nderive a single generic editing strategy from general cinema narrative\nprinciples instead of using a hierarchical film gram-mar. We demonstrate how\nthis single principle coupled with a database of scripts derived from annotated\nvideos leverages the existing video editing knowledge encoded within the\nediting of those sequences in a flexible and generic manner. We discuss the\ncinema theory foundations for this generic editing strategy, review the\nalgorithms used to effect it, and goon by means of examples to show its\nappropriateness in an automated system.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:23:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Butler", "Sean", ""]]}, {"id": "2102.10407", "submitter": "Jun Chen", "authors": "Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny", "title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for\n  Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:02:42 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 18:03:11 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 07:14:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Jun", ""], ["Guo", "Han", ""], ["Yi", "Kai", ""], ["Li", "Boyang", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2102.10898", "submitter": "Andreas Schimpe", "authors": "Andreas Schimpe, Simon Hoffmann and Frank Diermeyer", "title": "Adaptive Video Configuration and Bitrate Allocation for Teleoperated\n  Vehicles", "comments": "Accepted at workshop for Road Vehicle Teleoperation (WS09) at the\n  2021 IEEE Intelligent Vehicles Symposium (IV21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles with autonomous driving capabilities are present on public streets.\nHowever, edge cases remain that still require a human in-vehicle driver.\nAssuming the vehicle manages to come to a safe state in an automated fashion,\nteleoperated driving technology enables a human to resolve the situation\nremotely by a control interface connected via a mobile network. While this is a\npromising solution, it also introduces technical challenges, one of them being\nthe necessity to transmit video data of multiple cameras from the vehicle to\nthe human operator. In this paper, an adaptive video streaming framework\nspecifically designed for teleoperated vehicles is proposed and demonstrated.\nThe framework enables automatic reconfiguration of the video streams of the\nmulti-camera system at runtime. Predictions of variable transmission service\nquality are taken into account. With the objective to improve visual quality,\nthe framework uses so-called rate-quality models to dynamically allocate\nbitrates and select resolution scaling factors. Results from deploying the\nproposed framework on an actual teleoperated driving system are presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:00:47 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 20:04:41 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 14:15:36 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Schimpe", "Andreas", ""], ["Hoffmann", "Simon", ""], ["Diermeyer", "Frank", ""]]}, {"id": "2102.10935", "submitter": "Yazhou Yao", "authors": "Tao Chen, Guosen Xie, Yazhou Yao, Qiong Wang, Fumin Shen, Zhenmin\n  Tang, and Jian Zhang", "title": "Semantically Meaningful Class Prototype Learning for One-Shot Image\n  Semantic Segmentation", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot semantic image segmentation aims to segment the object regions for\nthe novel class with only one annotated image. Recent works adopt the episodic\ntraining strategy to mimic the expected situation at testing time. However,\nthese existing approaches simulate the test conditions too strictly during the\ntraining process, and thus cannot make full use of the given label information.\nBesides, these approaches mainly focus on the foreground-background target\nclass segmentation setting. They only utilize binary mask labels for training.\nIn this paper, we propose to leverage the multi-class label information during\nthe episodic training. It will encourage the network to generate more\nsemantically meaningful features for each category. After integrating the\ntarget class cues into the query features, we then propose a pyramid feature\nfusion module to mine the fused features for the final classifier. Furthermore,\nto take more advantage of the support image-mask pair, we propose a\nself-prototype guidance branch to support image segmentation. It can constrain\nthe network for generating more compact features and a robust prototype for\neach semantic class. For inference, we propose a fused prototype guidance\nbranch for the segmentation of the query image. Specifically, we leverage the\nprediction of the query image to extract the pseudo-prototype and combine it\nwith the initial prototype. Then we utilize the fused prototype to guide the\nfinal segmentation of the query image. Extensive experiments demonstrate the\nsuperiority of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:07:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Tao", ""], ["Xie", "Guosen", ""], ["Yao", "Yazhou", ""], ["Wang", "Qiong", ""], ["Shen", "Fumin", ""], ["Tang", "Zhenmin", ""], ["Zhang", "Jian", ""]]}, {"id": "2102.11237", "submitter": "Sulabh Katiyar", "authors": "Sulabh Katiyar, Samir Kumar Borgohain", "title": "Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings\n  and Data Augmentation", "comments": "Accepted for publication in Springer Book Series: Advances in\n  Intelligent Systems and Computing - ISSN 2194-5357. Upon publication, this\n  article will point to the published one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image Captioning, or the automatic generation of descriptions for images, is\none of the core problems in Computer Vision and has seen considerable progress\nusing Deep Learning Techniques. We propose to use Inception-ResNet\nConvolutional Neural Network as encoder to extract features from images,\nHierarchical Context based Word Embeddings for word representations and a Deep\nStacked Long Short Term Memory network as decoder, in addition to using Image\nData Augmentation to avoid over-fitting. For data Augmentation, we use\nHorizontal and Vertical Flipping in addition to Perspective Transformations on\nthe images. We evaluate our proposed methods with two image captioning\nframeworks- Encoder-Decoder and Soft Attention. Evaluation on widely used\nmetrics have shown that our approach leads to considerable improvement in model\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:15:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Katiyar", "Sulabh", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "2102.11393", "submitter": "Wei Zhou", "authors": "Wei Zhou, Jiahua Xu, Qiuping Jiang, Zhibo Chen", "title": "No-Reference Quality Assessment for 360-degree Images by Analysis of\n  Multi-frequency Information and Local-global Naturalness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360-degree/omnidirectional images (OIs) have achieved remarkable attentions\ndue to the increasing applications of virtual reality (VR). Compared to\nconventional 2D images, OIs can provide more immersive experience to consumers,\nbenefitting from the higher resolution and plentiful field of views (FoVs).\nMoreover, observing OIs is usually in the head mounted display (HMD) without\nreferences. Therefore, an efficient blind quality assessment method, which is\nspecifically designed for 360-degree images, is urgently desired. In this\npaper, motivated by the characteristics of the human visual system (HVS) and\nthe viewing process of VR visual contents, we propose a novel and effective\nno-reference omnidirectional image quality assessment (NR OIQA) algorithm by\nMulti-Frequency Information and Local-Global Naturalness (MFILGN).\nSpecifically, inspired by the frequency-dependent property of visual cortex, we\nfirst decompose the projected equirectangular projection (ERP) maps into\nwavelet subbands. Then, the entropy intensities of low and high frequency\nsubbands are exploited to measure the multi-frequency information of OIs.\nBesides, except for considering the global naturalness of ERP maps, owing to\nthe browsed FoVs, we extract the natural scene statistics features from each\nviewport image as the measure of local naturalness. With the proposed\nmulti-frequency information measurement and local-global naturalness\nmeasurement, we utilize support vector regression as the final image quality\nregressor to train the quality evaluation model from visual quality-related\nfeatures to human ratings. To our knowledge, the proposed model is the first\nno-reference quality assessment method for 360-degreee images that combines\nmulti-frequency information and image naturalness. Experimental results on two\npublicly available OIQA databases demonstrate that our proposed MFILGN\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:52:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhou", "Wei", ""], ["Xu", "Jiahua", ""], ["Jiang", "Qiuping", ""], ["Chen", "Zhibo", ""]]}, {"id": "2102.11506", "submitter": "Sulabh Katiyar", "authors": "Sulabh Katiyar, Samir Kumar Borgohain", "title": "Comparative evaluation of CNN architectures for Image Caption Generation", "comments": "Article Published in International Journal of Advanced Computer\n  Science and Applications(IJACSA), Volume 11 Issue 12, 2020", "journal-ref": "in International Journal of Advanced Computer Science and\n  Applications, 11(12), 2020", "doi": "10.14569/IJACSA.2020.0111291", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Aided by recent advances in Deep Learning, Image Caption Generation has seen\ntremendous progress over the last few years. Most methods use transfer learning\nto extract visual information, in the form of image features, with the help of\npre-trained Convolutional Neural Network models followed by transformation of\nthe visual information using a Caption Generator module to generate the output\nsentences. Different methods have used different Convolutional Neural Network\nArchitectures and, to the best of our knowledge, there is no systematic study\nwhich compares the relative efficacy of different Convolutional Neural Network\narchitectures for extracting the visual information. In this work, we have\nevaluated 17 different Convolutional Neural Networks on two popular Image\nCaption Generation frameworks: the first based on Neural Image Caption (NIC)\ngeneration model and the second based on Soft-Attention framework. We observe\nthat model complexity of Convolutional Neural Network, as measured by number of\nparameters, and the accuracy of the model on Object Recognition task does not\nnecessarily co-relate with its efficacy on feature extraction for Image Caption\nGeneration task.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 05:43:54 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Katiyar", "Sulabh", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "2102.11526", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Yadan Luo and Zi Huang", "title": "Enhanced Modality Transition for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning model is a cross-modality knowledge discovery task, which\ntargets at automatically describing an image with an informative and coherent\nsentence. To generate the captions, the previous encoder-decoder frameworks\ndirectly forward the visual vectors to the recurrent language model, forcing\nthe recurrent units to generate a sentence based on the visual features.\nAlthough these sentences are generally readable, they still suffer from the\nlack of details and highlights, due to the fact that the substantial gap\nbetween the image and text modalities is not sufficiently addressed. In this\nwork, we explicitly build a Modality Transition Module (MTM) to transfer visual\nfeatures into semantic representations before forwarding them to the language\nmodel. During the training phase, the modality transition network is optimised\nby the proposed modality loss, which compares the generated preliminary textual\nencodings with the target sentence vectors from a pre-trained text\nauto-encoder. In this way, the visual vectors are transited into the textual\nsubspace for more contextual and precise language generation. The novel MTM can\nbe incorporated into most of the existing methods. Extensive experiments have\nbeen conducted on the MS-COCO dataset demonstrating the effectiveness of the\nproposed framework, improving the performance by 3.4% comparing to the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:20:12 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Ziwei", ""], ["Luo", "Yadan", ""], ["Huang", "Zi", ""]]}, {"id": "2102.11617", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and\n  Gustav Eje Henter", "title": "A large, crowdsourced evaluation of gesture generation systems on common\n  data: The GENEA Challenge 2020", "comments": "Accepted for publication at the 26th International Conference on\n  Intelligent User Interfaces (IUI'21). 11 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3397481.3450692", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-speech gestures, gestures that accompany speech, play an important role in\nhuman communication. Automatic co-speech gesture generation is thus a key\nenabling technology for embodied conversational agents (ECAs), since humans\nexpect ECAs to be capable of multi-modal communication. Research into gesture\ngeneration is rapidly gravitating towards data-driven methods. Unfortunately,\nindividual research efforts in the field are difficult to compare: there are no\nestablished benchmarks, and each study tends to use its own dataset, motion\nvisualisation, and evaluation methodology. To address this situation, we\nlaunched the GENEA Challenge, a gesture-generation challenge wherein\nparticipating teams built automatic gesture-generation systems on a common\ndataset, and the resulting systems were evaluated in parallel in a large,\ncrowdsourced user study using the same motion-rendering pipeline. Since\ndifferences in evaluation outcomes between systems now are solely attributable\nto differences between the motion-generation methods, this enables benchmarking\nrecent approaches against one another in order to get a better impression of\nthe state of the art in the field. This paper reports on the purpose, design,\nresults, and implications of our challenge.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:54:58 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kucherenko", "Taras", ""], ["Jonell", "Patrik", ""], ["Yoon", "Youngwoo", ""], ["Wolfert", "Pieter", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2102.12613", "submitter": "Yingqiang Qiu", "authors": "Yingqiang Qiu, Yuyan Yang, Qichao Ying, Huanqiang Zeng and Zhenxing\n  Qian", "title": "High-Capacity Framework for Reversible Data Hiding in Encrypted Image\n  Using Pixel Predictions and Entropy Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous reversible data hiding in encrypted images (RDHEI) schemes can be\neither carried out by vacating room before or after data encryption, which\nleads to a separation of the search field in RDHEI. Besides, high capacity\nrelies heavily on vacating room before encryption (VRBE), which significantly\nlowers the payload of vacating room after encryption (VRAE) based schemes. To\naddress this issue, this paper proposes a framework for high-capacity RDHEI for\nboth VRBE and VRAE cases using pixel predictions and entropy encoding. We\npropose an embedding room generation algorithm to produce vacated room by\ngenerating the prediction-error histogram (PEH) of the selected cover using\nadjacency prediction and the median edge detector (MED). In the VRBE scenario,\nwe propose a scheme that generates the embedding room using the proposed\nalgorithm, and encrypts the preprocessed image by using the stream cipher with\ntwo encryption keys. In the VRAE scenario, we propose a scheme that involves an\nimproved block modulation and permutation encryption algorithm where the\nspatial redundancy in the plain-text image can be largely preserved. Then the\nproposed algorithm is applied on the encrypted image to generate the embedding\nroom. At the data hider's side of both the schemes, the data hider locates the\nembedding room and embeds the encrypted additional data. On receiving the\nmarked encrypted image, the receivers with different authentication can\nrespectively conduct error-free data extraction and/or error-free image\nrecovery. The experimental results show that the two schemes in the proposed\nframework can outperform many previous state-of-the-art RDHEI arts. Besides,\nthe proposed schemes can ensure high information security in that little detail\nof the original image can be directly discovered from the encrypted images or\nthe marked encrypted images.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 01:01:06 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 13:41:19 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Qiu", "Yingqiang", ""], ["Yang", "Yuyan", ""], ["Ying", "Qichao", ""], ["Zeng", "Huanqiang", ""], ["Qian", "Zhenxing", ""]]}, {"id": "2102.12620", "submitter": "Zhaoxia Yin", "authors": "Youqing Wu, Wenjing Ma, and Zhaoxia Yin", "title": "High-Capacity Reversible Data Hiding in Encrypted Images using Adaptive\n  Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularization of digital information technology, the reversible\ndata hiding in encrypted images (RDHEI) has gradually become the research\nhotspot of privacy protection in cloud storage. As a technology that can embed\nadditional information in the encrypted domain, extract the embedded\ninformation correctly and recover the original image losslessly, RDHEI has\nreceived a lot of attention from researchers. To embed sufficient additional\ninformation in the encrypted image, a high-capacity RDHEI method using adaptive\nencoding is proposed in this paper. Firstly, the occurrence frequency of\ndifferent prediction errors of the original image is calculated and the\ncorresponding adaptive Huffman coding is generated. Then, the original image is\nencrypted and the encrypted pixels are marked with different Huffman codewords\naccording to the prediction errors. Finally, additional information is embedded\nin the reserved room of marked pixels by bit substitution. Experimental results\nprove that the proposed method outperforms the state-of-the-art methods in\nembedding rate and can extract the embedded information correctly and recover\nthe image losslessly.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 01:29:26 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wu", "Youqing", ""], ["Ma", "Wenjing", ""], ["Yin", "Zhaoxia", ""]]}, {"id": "2102.12699", "submitter": "Marzieh Masoumi", "authors": "Marzieh Masoumi, Ahmad Keshavarz, Reza Fotohi", "title": "File fragment recognition based on content and statistical features", "comments": "16 pages, 7 figures, 8 tables. Multimed Tools Appl (2021)", "journal-ref": null, "doi": "10.1007/s11042-021-10681-x", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, the speed up development and use of digital devices such as\nsmartphones have put people at risk of internet crimes. The evidence of present\ncrimes in a computer file can be easily unreachable by changing the prefix of a\nfile or other algorithms. In more complex cases, either file divided into\ndifferent parts or the parts of a file that has information about the file type\nare deleted, where the file fragment recognition issue is discussed. The known\nfiles are divided into different fragments, and different classification\nalgorithms are used to solve the problems of file fragment recognition. The\nissue of identifying the type of file fragment due to its importance in\ncybercrime issues as well as antivirus has been highly emphasized and has been\naddressed in many articles. Increasing the accuracy in this field on the types\nof widely used files due to the sensitivity of the subject of recognizing the\ntype of file under study is the main goal of researchers in this field. Failure\nto identify the correct type of file will lead to deviations of the results and\nevidence from the main issue or failure to conclude. In this paper, first, the\nfile is divided into different fragments. Then, the file fragment features,\nwhich are obtained from Binary Frequency Distribution, are reduced by 2 feature\nreduction algorithms; Sequential Forward Selection algorithm as well as\nSequential Floating Forward Selection algorithm to delete sparse features that\nresult in increased accuracy and speed. Finally, the reduced features are given\nto 3 Multiclass classifier algorithms, Multilayer Perceptron, Support Vector\nMachines, and K-Nearest Neighbor for classification and comparison of the\nresults. The proposed recognition algorithm can recognize 6 types of useful\nfiles and may distinguish a type of file fragments with higher accuracy than\nthe similar works done.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 06:00:25 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Masoumi", "Marzieh", ""], ["Keshavarz", "Ahmad", ""], ["Fotohi", "Reza", ""]]}]