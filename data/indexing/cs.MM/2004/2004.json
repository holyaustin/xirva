[{"id": "2004.00034", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Christian Kr\\\"uger, Tanja Koji\\'c, Luis Meier, Sebastian M\\\"oller,\n  Jan-Niklas Voigt-Antons", "title": "Development and Validation of Pictographic Scales for Rapid Assessment\n  of Affective States in Virtual Reality", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development and validation of a continuous\npictographic scale for self-reported assessment of affective states in virtual\nenvironments. The developed tool, called Morph A Mood (MAM), consists of a 3D\ncharacter whose facial expression can be adjusted with simple controller\ngestures according to the perceived affective state to capture valence and\narousal scores. It was tested against the questionnaires Pick-A-Mood (PAM) and\nSelf-Assessment Manikin (SAM) in an experiment in which the participants (N =\n32) watched several one-minute excerpts from music videos of the DEAP database\nwithin a virtual environment and assessed their mood after each clip. The\nexperiment showed a high correlation with regard to valence, but only a\nmoderate one with regard to arousal. No statistically significant differences\nwere found between the SAM ratings of this experiment and MAM, but between the\nvalence values of MAM and the DEAP database and between the arousal values of\nMAM and PAM. In terms of user experience, MAM and PAM hardly differ.\nFurthermore, the experiment showed that assessments inside virtual environments\nare significantly faster than with paper-pencil methods, where media devices\nsuch as headphones and display goggles must be put on and taken off.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:02:22 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Kr\u00fcger", "Christian", ""], ["Koji\u0107", "Tanja", ""], ["Meier", "Luis", ""], ["M\u00f6ller", "Sebastian", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.00369", "submitter": "De Mi", "authors": "De Mi, Joe Eyles, Tero Jokela, Swen Petersen, Roman Odarchenko, Ece\n  Ozturk, Duy-Kha Chau, Tuan Tran, Rory Turnbull, Heikki Kokkinen, Baruch\n  Altman, Menno Bot, Darko Ratkaj, Olaf Renner, David Gomez-Barquero and Jordi\n  Joan Gimenez", "title": "Demonstrating Immersive Media Delivery on 5G Broadcast and Multicast\n  Testing Networks", "comments": "16 pages, 22 figures, IEEE Trans. Broadcasting", "journal-ref": null, "doi": "10.1109/TBC.2020.2977546", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents eight demonstrators and one showcase developed within the\n5G-Xcast project. They experimentally demonstrate and validate key technical\nenablers for the future of media delivery, associated with multicast and\nbroadcast communication capabilities in 5th Generation (5G). In 5G-Xcast, three\nexisting testbeds: IRT in Munich (Germany), 5GIC in Surrey (UK), and TUAS in\nTurku (Finland), have been developed into 5G broadcast and multicast testing\nnetworks, which enables us to demonstrate our vision of a converged 5G\ninfrastructure with fixed and mobile accesses and terrestrial broadcast,\ndelivering immersive audio-visual media content. Built upon the improved\ntesting networks, the demonstrators and showcase developed in 5G-Xcast show the\nimpact of the technology developed in the project. Our demonstrations\npredominantly cover use cases belonging to two verticals: Media & Entertainment\nand Public Warning, which are future 5G scenarios relevant to multicast and\nbroadcast delivery. In this paper, we present the development of these\ndemonstrators, the showcase, and the testbeds. We also provide key findings\nfrom the experiments and demonstrations, which not only validate the technical\nsolutions developed in the project, but also illustrate the potential technical\nimpact of these solutions for broadcasters, content providers, operators, and\nother industries interested in the future immersive media delivery.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 08:41:08 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Mi", "De", ""], ["Eyles", "Joe", ""], ["Jokela", "Tero", ""], ["Petersen", "Swen", ""], ["Odarchenko", "Roman", ""], ["Ozturk", "Ece", ""], ["Chau", "Duy-Kha", ""], ["Tran", "Tuan", ""], ["Turnbull", "Rory", ""], ["Kokkinen", "Heikki", ""], ["Altman", "Baruch", ""], ["Bot", "Menno", ""], ["Ratkaj", "Darko", ""], ["Renner", "Olaf", ""], ["Gomez-Barquero", "David", ""], ["Gimenez", "Jordi Joan", ""]]}, {"id": "2004.00849", "submitter": "Bei Liu", "authors": "Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Pixel-BERT to align image pixels with text by deep multi-modal\ntransformers that jointly learn visual and language embedding in a unified\nend-to-end framework. We aim to build a more accurate and thorough connection\nbetween image pixels and language semantics directly from image and sentence\npairs instead of using region-based image features as the most recent vision\nand language tasks. Our Pixel-BERT which aligns semantic connection in pixel\nand text level solves the limitation of task-specific visual representation for\nvision and language tasks. It also relieves the cost of bounding box\nannotations and overcomes the unbalance between semantic labels in visual task\nand language semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence pairs\nfrom Visual Genome dataset and MS-COCO dataset. We propose to use a random\npixel sampling mechanism to enhance the robustness of visual representation and\nto apply the Masked Language Model and Image-Text Matching as pre-training\ntasks. Extensive experiments on downstream tasks with our pre-trained model\nshow that our approach makes the most state-of-the-arts in downstream tasks,\nincluding Visual Question Answering (VQA), image-text retrieval, Natural\nLanguage for Visual Reasoning for Real (NLVR). Particularly, we boost the\nperformance of a single model in VQA task by 2.17 points compared with SOTA\nunder fair comparison.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:39:28 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 09:09:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Zhicheng", ""], ["Zeng", "Zhaoyang", ""], ["Liu", "Bei", ""], ["Fu", "Dongmei", ""], ["Fu", "Jianlong", ""]]}, {"id": "2004.01023", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Andrew Lindley, Anahid Jalali, Martin Boyer,\n  Sergiu Gordea, Ross King", "title": "Multi-Modal Video Forensic Platform for Investigating Post-Terrorist\n  Attack Scenarios", "comments": null, "journal-ref": "In Proceedings of the 11th ACM Multimedia Systems Conference\n  (MMSys2020), June 06-11, 2020, Istanbul, Turkey", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.CY cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forensic investigation of a terrorist attack poses a significant\nchallenge to the investigative authorities, as often several thousand hours of\nvideo footage must be viewed. Large scale Video Analytic Platforms (VAP) assist\nlaw enforcement agencies (LEA) in identifying suspects and securing evidence.\nCurrent platforms focus primarily on the integration of different computer\nvision methods and thus are restricted to a single modality. We present a video\nanalytic platform that integrates visual and audio analytic modules and fuses\ninformation from surveillance cameras and video uploads from eyewitnesses.\nVideos are analyzed according their acoustic and visual content. Specifically,\nAudio Event Detection is applied to index the content according to\nattack-specific acoustic concepts. Audio similarity search is utilized to\nidentify similar video sequences recorded from different perspectives. Visual\nobject detection and tracking are used to index the content according to\nrelevant concepts. Innovative user-interface concepts are introduced to harness\nthe full potential of the heterogeneous results of the analytical modules,\nallowing investigators to more quickly follow-up on leads and eyewitness\nreports.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 14:29:27 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Schindler", "Alexander", ""], ["Lindley", "Andrew", ""], ["Jalali", "Anahid", ""], ["Boyer", "Martin", ""], ["Gordea", "Sergiu", ""], ["King", "Ross", ""]]}, {"id": "2004.01532", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Jan-Niklas Voigt-Antons, Eero Lehtonen, Andres Pinilla Palacios,\n  Danish Ali, Tanja Koji\\'c, Sebastian M\\\"oller", "title": "Comparing emotional states induced by 360$^{\\circ}$ videos via\n  head-mounted display and computer screen", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years 360$^{\\circ}$ videos have been becoming more popular. For\ntraditional media presentations, e.g., on a computer screen, a wide range of\nassessment methods are available. Different constructs, such as perceived\nquality or the induced emotional state of viewers, can be reliably assessed by\nsubjective scales. Many of the subjective methods have only been validated\nusing stimuli presented on a computer screen. This paper is using 360$^{\\circ}$\nvideos to induce varying emotional states. Videos were presented 1) via a\nhead-mounted display (HMD) and 2) via a traditional computer screen.\nFurthermore, participants were asked to rate their emotional state 1) in\nretrospect on the self-assessment manikin scale and 2) continuously on a\n2-dimensional arousal-valence plane. In a repeated measures design, all\nparticipants (N = 18) used both presentation systems and both rating systems.\nResults indicate that there is a statistically significant difference in\ninduced presence due to the presentation system. Furthermore, there was no\nstatistically significant difference in ratings gathered with the two\npresentation systems. Finally, it was found that for arousal measures, a\nstatistically significant difference could be found for the different rating\nmethods, potentially indicating an underestimation of arousal ratings gathered\nin retrospect for screen presentation. In the future, rating methods such as a\n2-dimensional arousal-valence plane could offer the advantage of enabling a\nreliable measurement of emotional states while being more embedded in the\nexperience itself, enabling a more precise capturing of the emotional states.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 12:51:44 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Voigt-Antons", "Jan-Niklas", ""], ["Lehtonen", "Eero", ""], ["Palacios", "Andres Pinilla", ""], ["Ali", "Danish", ""], ["Koji\u0107", "Tanja", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.01545", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Tanja Koji\\'c, Danish Ali, Robert Greinacher, Sebastian M\\\"oller,\n  Jan-Niklas Voigt-Antons", "title": "User Experience of Reading in Virtual Reality -- Finding Values for Text\n  Distance, Size and Contrast", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) has an increasing impact on the market in many fields,\nfrom education and medicine to engineering and entertainment, by creating\ndifferent applications that replicate or in the case of augmentation enhance\nreal-life scenarios. Intending to present realistic environments, VR\napplications are including text that we are surrounded by every day. However,\ntext can only add value to the virtual environment if it is designed and\ncreated in such a way that users can comfortably read it. With the aim to\nexplore what values for text parameters users find comfortable while reading in\nvirtual reality, a study was conducted allowing participants to manipulate text\nparameters such as font size, distance, and contrast. Therefore two different\nstandalone virtual reality devices were used, Oculus Go and Quest, together\nwith three different text samples: Short (2 words), medium (21 words), and long\n(51 words). Participants had the task of setting text parameters to the best\nand worst possible value. Additionally, participants were asked to rate their\nexperience of reading in virtual reality. Results report mean values for\nangular size (the combination of distance and font size) and color contrast\ndepending on the different device used as well as the varying text length, for\nboth tasks. Significant differences were found for values of angular size,\ndepending on the length of the displayed text. However, different device types\nhad no significant influence on text parameters but on the experiences reported\nusing the self-assessment manikin (SAM) scale.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:14:42 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Koji\u0107", "Tanja", ""], ["Ali", "Danish", ""], ["Greinacher", "Robert", ""], ["M\u00f6ller", "Sebastian", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.01555", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Robert Greinacher, Tanja Koji\\'c, Luis Meier, Rudresha Gulaganjihalli\n  Parameshappa, Sebastian M\\\"oller, Jan-Niklas Voigt-Antons", "title": "Impact of Tactile and Visual Feedback on Breathing Rhythm and User\n  Experience in VR Exergaming", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining interconnected wearables provides fascinating opportunities like\naugmenting exergaming with virtual coaches, feedback on the execution of sports\nactivities, or how to improve on them. Breathing rhythm is a particularly\ninteresting physiological dimension since it is easy and unobtrusive to measure\nand gained data provide valuable insights regarding the correct execution of\nmovements, especially when analyzed together with additional movement data in\nreal-time. In this work, we focus on indoor rowing since it is a popular sport\nthat's often done alone without extensive instructions. We compare a visual\nbreathing indication with haptic guidance in order for athletes to maintain a\ncorrect, efficient, and healthy breathing-movement-synchronicity (BMS) while\nworking out. Also, user experience and acceptance of the different modalities\nwere measured. The results show a positive and statistically significant impact\nof purely verbal instructions and purely tactile feedback on BMS and no\nsignificant impact of visual feedback. Interestingly, the subjective ratings\nindicate a strong preference for the visual modality and even an aversion for\nthe haptic feedback, although objectively the performance benefited most from\nusing the latter.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:25:32 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Greinacher", "Robert", ""], ["Koji\u0107", "Tanja", ""], ["Meier", "Luis", ""], ["Parameshappa", "Rudresha Gulaganjihalli", ""], ["M\u00f6ller", "Sebastian", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.01800", "submitter": "Ping Hu", "authors": "Ping Hu, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Stan Sclaroff and\n  Federico Perazzi", "title": "Temporally Distributed Networks for Fast Video Semantic Segmentation", "comments": "[CVPR2020] Project: https://github.com/feinanshan/TDNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TDNet, a temporally distributed network designed for fast and\naccurate video semantic segmentation. We observe that features extracted from a\ncertain high-level layer of a deep CNN can be approximated by composing\nfeatures extracted from several shallower sub-networks. Leveraging the inherent\ntemporal continuity in videos, we distribute these sub-networks over sequential\nframes. Therefore, at each time step, we only need to perform a lightweight\ncomputation to extract a sub-features group from a single sub-network. The full\nfeatures used for segmentation are then recomposed by application of a novel\nattention propagation module that compensates for geometry deformation between\nframes. A grouped knowledge distillation loss is also introduced to further\nimprove the representation power at both full and sub-feature levels.\nExperiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method\nachieves state-of-the-art accuracy with significantly faster speed and lower\nlatency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 22:43:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 00:44:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hu", "Ping", ""], ["Heilbron", "Fabian Caba", ""], ["Wang", "Oliver", ""], ["Lin", "Zhe", ""], ["Sclaroff", "Stan", ""], ["Perazzi", "Federico", ""]]}, {"id": "2004.01872", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u and Rafael F. Schaefer", "title": "Low-complexity and Reliable Transforms for Physical Unclonable Functions", "comments": "To appear in IEEE International Conference on Acoustics, Speech, and\n  Signal Processing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy measurements of a physical unclonable function (PUF) are used to store\nsecret keys with reliability, security, privacy, and complexity constraints. A\nnew set of low-complexity and orthogonal transforms with no multiplication is\nproposed to obtain bit-error probability results significantly better than all\nmethods previously proposed for key binding with PUFs. The uniqueness and\nsecurity performance of a transform selected from the proposed set is shown to\nbe close to optimal. An error-correction code with a low-complexity decoder and\na high code rate is shown to provide a block-error probability significantly\nsmaller than provided by previously proposed codes with the same or smaller\ncode rates.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 06:38:17 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Schaefer", "Rafael F.", ""]]}, {"id": "2004.02067", "submitter": "Zhi Li", "authors": "Zhi Li, Christos G. Bampis, Luk\\'a\\v{s} Krasula, Lucjan Janowski,\n  Ioannis Katsavounidis", "title": "A Simple Model for Subject Behavior in Subjective Experiments", "comments": "14 pages, updated version of the original paper published in Human\n  Vision and Electronic Imaging (HVEI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a subjective experiment to evaluate the perceptual audiovisual quality of\nmultimedia and television services, raw opinion scores collected from test\nsubjects are often noisy and unreliable. To produce the final mean opinion\nscores (MOS), recommendations such as ITU-R BT.500, ITU-T P.910 and ITU-T P.913\nstandardize post-test screening procedures to clean up the raw opinion scores,\nusing techniques such as subject outlier rejection and bias removal. In this\npaper, we analyze the prior standardized techniques to demonstrate their\nweaknesses. As an alternative, we propose a simple model to account for two of\nthe most dominant behaviors of subject inaccuracy: bias and inconsistency. We\nfurther show that this model can also effectively deal with inattentive\nsubjects that give random scores. We propose to use maximum likelihood\nestimation to jointly solve the model parameters, and present two numeric\nsolvers: the first based on the Newton-Raphson method, and the second based on\nan alternating projection (AP). We show that the AP solver generalizes the\nITU-T P.913 post-test screening procedure by weighing a subject's contribution\nto the true quality score by her consistency (thus, the quality scores\nestimated can be interpreted as bias-subtracted consistency-weighted MOS). We\ncompare the proposed methods with the standardized techniques using real\ndatasets and synthetic simulations, and demonstrate that the proposed methods\nare the most valuable when the test conditions are challenging (for example,\ncrowdsourcing and cross-lab studies), offering advantages such as better\nmodel-data fit, tighter confidence intervals, better robustness against subject\noutliers, the absence of hard coded parameters and thresholds, and auxiliary\ninformation on test subjects. The code for this work is open-sourced at\nhttps://github.com/Netflix/sureal.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 01:36:39 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 18:19:44 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 21:21:50 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Li", "Zhi", ""], ["Bampis", "Christos G.", ""], ["Krasula", "Luk\u00e1\u0161", ""], ["Janowski", "Lucjan", ""], ["Katsavounidis", "Ioannis", ""]]}, {"id": "2004.02205", "submitter": "Vivek Sharma", "authors": "Vivek Sharma and Makarand Tapaswi and Rainer Stiefelhagen", "title": "Deep Multimodal Feature Encoding for Video Ordering", "comments": "IEEE International Conference on Computer Vision (ICCV) Workshop on\n  Large Scale Holistic Video Understanding. The datasets and code are available\n  at https://github.com/vivoutlaw/tcbp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  True understanding of videos comes from a joint analysis of all its\nmodalities: the video frames, the audio track, and any accompanying text such\nas closed captions. We present a way to learn a compact multimodal feature\nrepresentation that encodes all these modalities. Our model parameters are\nlearned through a proxy task of inferring the temporal ordering of a set of\nunordered videos in a timeline. To this end, we create a new multimodal dataset\nfor temporal ordering that consists of approximately 30K scenes (2-6 clips per\nscene) based on the \"Large Scale Movie Description Challenge\". We analyze and\nevaluate the individual and joint modalities on three challenging tasks: (i)\ninferring the temporal ordering of a set of videos; and (ii) action\nrecognition. We demonstrate empirically that multimodal representations are\nindeed complementary, and can play a key role in improving the performance of\nmany applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:02:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sharma", "Vivek", ""], ["Tapaswi", "Makarand", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2004.02678", "submitter": "Anyi Rao", "authors": "Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou,\n  Dahua Lin", "title": "A Local-to-Global Approach to Multi-modal Movie Scene Segmentation", "comments": "CVPR2020. Project page: https://anyirao.com/projects/SceneSeg.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene, as the crucial unit of storytelling in movies, contains complex\nactivities of actors and their interactions in a physical environment.\nIdentifying the composition of scenes serves as a critical step towards\nsemantic understanding of movies. This is very challenging -- compared to the\nvideos studied in conventional vision problems, e.g. action recognition, as\nscenes in movies usually contain much richer temporal structures and more\ncomplex semantic information. Towards this goal, we scale up the scene\nsegmentation task by building a large-scale video dataset MovieScenes, which\ncontains 21K annotated scene segments from 150 movies. We further propose a\nlocal-to-global scene segmentation framework, which integrates multi-modal\ninformation across three levels, i.e. clip, segment, and movie. This framework\nis able to distill complex semantics from hierarchical temporal structures over\na long movie, providing top-down guidance for scene segmentation. Our\nexperiments show that the proposed network is able to segment a movie into\nscenes with high accuracy, consistently outperforming previous methods. We also\nfound that pretraining on our MovieScenes can bring significant improvements to\nthe existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:58:08 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:54:57 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 14:30:05 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Rao", "Anyi", ""], ["Xu", "Linning", ""], ["Xiong", "Yu", ""], ["Xu", "Guodong", ""], ["Huang", "Qingqiu", ""], ["Zhou", "Bolei", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.02940", "submitter": "Shadrokh Samavi", "authors": "Mahsa Kadkhodaei, Shadrokh Samavi", "title": "Robust Wavelet-Based Watermarking Using Dynamic Strength Factor", "comments": "four pages, five figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsecured network environments, ownership protection of digital contents,\nsuch as images, is becoming a growing concern. Different watermarking methods\nhave been proposed to address the copyright protection of digital materials.\nWatermarking methods are challenged with conflicting parameters of\nimperceptibility and robustness. While embedding a watermark with a high\nstrength factor increases robustness, it also decreases imperceptibility of the\nwatermark. Thus embedding in visually less sensitive regions, i.e., complex\nimage blocks could satisfy both requirements. This paper presents a new\nwavelet-based watermarking technique using an adaptive strength factor to\ntradeoff between watermark transparency and robustness. We measure variations\nof each image block to adaptively set a strength-factor for embedding the\nwatermark in that block. On the other hand, the decoder uses the selected\ncoefficients to safely extract the watermark through a voting algorithm. The\nproposed method shows better results in terms of PSNR and BER in comparison to\nrecent methods for attacks, such as Median Filter, Gaussian Filter, and JPEG\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:02:31 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kadkhodaei", "Mahsa", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2004.03391", "submitter": "Jarek Duda Dr", "authors": "Jarek Duda", "title": "Exploiting context dependence for image compression with upsampling", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression with upsampling encodes information to succeedingly\nincrease image resolution, for example by encoding differences in FUIF and JPEG\nXL. It is useful for progressive decoding, also often can improve compression\nratio - both for lossless compression and e.g. DC coefficients of lossy.\nHowever, the currently used solutions rather do not exploit context dependence\nfor encoding of such upscaling information. This article discusses simple\ninexpensive general techniques for this purpose, which allowed to save on\naverage $0.645$ bits/difference (between $0.138$ and $1.489$) for the last\nupscaling for 48 standard $512\\times 512$ grayscale 8 bit images - compared to\nassumption of fixed Laplace distribution. Using least squares linear regression\nof context to predict center of Laplace distribution gave on average $0.393$\nbits/difference savings. The remaining savings were obtained by additionally\npredicting width of this Laplace distribution, also using just the least\nsquares linear regression.\n  For RGB images, optimization of color transform alone gave mean $\\approx\n4.6\\%$ size reduction comparing to standard YCrCb if using fixed transform,\n$\\approx 6.3\\%$ if optimizing transform individually for each image. Then\nfurther mean $\\approx 10\\%$ reduction was obtained if predicting Laplace\nparameters based on context. The presented simple inexpensive general\nmethodology can be also used for different types of data like DCT coefficients\nin lossy image compression.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:37:04 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 12:51:20 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 07:56:31 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "2004.03413", "submitter": "Jiguo Li", "authors": "Jiguo Li, Xinfeng Zhang, Chuanmin Jia, Jizheng Xu, Li Zhang, Yue Wang,\n  Siwei Ma, Wen Gao", "title": "Direct Speech-to-image Translation", "comments": "Accepted by JSTSP", "journal-ref": null, "doi": "10.1109/JSTSP.2020.2987417", "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct speech-to-image translation without text is an interesting and useful\ntopic due to the potential applications in human-computer interaction, art\ncreation, computer-aided design. etc. Not to mention that many languages have\nno writing form. However, as far as we know, it has not been well-studied how\nto translate the speech signals into images directly and how well they can be\ntranslated. In this paper, we attempt to translate the speech signals into the\nimage signals without the transcription stage. Specifically, a speech encoder\nis designed to represent the input speech signals as an embedding feature, and\nit is trained with a pretrained image encoder using teacher-student learning to\nobtain better generalization ability on new classes. Subsequently, a stacked\ngenerative adversarial network is used to synthesize high-quality images\nconditioned on the embedding feature. Experimental results on both synthesized\nand real data show that our proposed method is effective to translate the raw\nspeech signals into images without the middle text representation. Ablation\nstudy gives more insights about our method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:05:30 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 12:34:12 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Jiguo", ""], ["Zhang", "Xinfeng", ""], ["Jia", "Chuanmin", ""], ["Xu", "Jizheng", ""], ["Zhang", "Li", ""], ["Wang", "Yue", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2004.03586", "submitter": "Jean-Pierre Briot", "authors": "Jean-Pierre Briot", "title": "From Artificial Neural Networks to Deep Learning for Music Generation --\n  History, Concepts and Trends", "comments": "To appear in the Special Issue on Art, Sound and Design in the Neural\n  Computing and Applications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current wave of deep learning (the hyper-vitamined return of artificial\nneural networks) applies not only to traditional statistical machine learning\ntasks: prediction and classification (e.g., for weather prediction and pattern\nrecognition), but has already conquered other areas, such as translation. A\ngrowing area of application is the generation of creative content, notably the\ncase of music, the topic of this paper. The motivation is in using the capacity\nof modern deep learning techniques to automatically learn musical styles from\narbitrary musical corpora and then to generate musical samples from the\nestimated distribution, with some degree of control over the generation. This\npaper provides a tutorial on music generation based on deep learning\ntechniques. After a short introduction to the topic illustrated by a recent\nexemple, the paper analyzes some early works from the late 1980s using\nartificial neural networks for music generation and how their pioneering\ncontributions have prefigured current techniques. Then, we introduce some\nconceptual framework to analyze the various concepts and dimensions involved.\nVarious examples of recent systems are introduced and analyzed to illustrate\nthe variety of concerns and of techniques.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:33:56 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 22:33:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Briot", "Jean-Pierre", ""]]}, {"id": "2004.04371", "submitter": "Xulong Zhang", "authors": "Xulong Zhang, Yongwei Gao, Yi Yu and Wei Li", "title": "Music Artist Classification with WaveNet Classifier for Raw Waveform\n  Audio Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for music artist classification usually were operated in the frequency\ndomain, in which the input audio samples are processed by the spectral\ntransformation. The WaveNet architecture, originally designed for speech and\nmusic generation. In this paper, we propose an end-to-end architecture in the\ntime domain for this task. A WaveNet classifier was introduced which directly\nmodels the features from a raw audio waveform. The WaveNet takes the waveform\nas the input and several downsampling layers are subsequent to discriminate\nwhich artist the input belongs to. In addition, the proposed method is applied\nto singer identification. The model achieving the best performance obtains an\naverage F1 score of 0.854 on benchmark dataset of Artist20, which is a\nsignificant improvement over the related works. In order to show the\neffectiveness of feature learning of the proposed method, the bottleneck layer\nof the model is visualized.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 05:31:08 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Zhang", "Xulong", ""], ["Gao", "Yongwei", ""], ["Yu", "Yi", ""], ["Li", "Wei", ""]]}, {"id": "2004.04959", "submitter": "Rui Zhao", "authors": "Rui Zhao, Kecheng Zheng, Zheng-jun Zha", "title": "Stacked Convolutional Deep Encoding Network for Video-Text Retrieval", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dominant approaches for cross-modal video-text retrieval task are to\nlearn a joint embedding space to measure the cross-modal similarity. However,\nthese methods rarely explore long-range dependency inside video frames or\ntextual words leading to insufficient textual and visual details. In this\npaper, we propose a stacked convolutional deep encoding network for video-text\nretrieval task, which considers to simultaneously encode long-range and\nshort-range dependency in the videos and texts. Specifically, a multi-scale\ndilated convolutional (MSDC) block within our approach is able to encode\nshort-range temporal cues between video frames or text words by adopting\ndifferent scales of kernel size and dilation size of convolutional layer. A\nstacked structure is designed to expand the receptive fields by repeatedly\nadopting the MSDC block, which further captures the long-range relations\nbetween these cues. Moreover, to obtain more robust textual representations, we\nfully utilize the powerful language model named Transformer in two stages:\npretraining phrase and fine-tuning phrase. Extensive experiments on two\ndifferent benchmark datasets (MSR-VTT, MSVD) show that our proposed method\noutperforms other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:18:12 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhao", "Rui", ""], ["Zheng", "Kecheng", ""], ["Zha", "Zheng-jun", ""]]}, {"id": "2004.05502", "submitter": "Babak Naderi", "authors": "Babak Naderi, Sebastian M\\\"oller", "title": "Application of Just-Noticeable Difference in Quality as Environment\n  Suitability Test for Crowdsourcing Speech Quality Assessment Task", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123093", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing micro-task platforms facilitate subjective media quality\nassessment by providing access to a highly scale-able, geographically\ndistributed and demographically diverse pool of crowd workers. Those workers\nparticipate in the experiment remotely from their own working environment,\nusing their own hardware. In the case of speech quality assessment, preliminary\nwork showed that environmental noise at the listener's side and the listening\ndevice (loudspeaker or headphone) significantly affect perceived quality, and\nconsequently the reliability and validity of subjective ratings. As a\nconsequence, ITU-T Rec. P.808 specifies requirements for the listening\nenvironment of crowd workers when assessing speech quality. In this paper, we\npropose a new Just Noticeable Difference of Quality (JNDQ) test as a remote\nscreening method for assessing the suitability of the work environment for\nparticipating in speech quality assessment tasks. In a laboratory experiment,\nparticipants performed this JNDQ test with different listening devices in\ndifferent listening environments, including a silent room according to ITU-T\nRec. P.800 and a simulated background noise scenario. Results show a\nsignificant impact of the environment and the listening device on the JNDQ\nthreshold. Thus, the combination of listening device and background noise needs\nto be screened in a crowdsourcing speech quality test. We propose a minimum\nthreshold of our JNDQ test as an easily applicable screening method for this\npurpose.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 22:37:59 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Naderi", "Babak", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.05575", "submitter": "Koteswar Rao Jerripothula", "authors": "Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, Junsong Yuan", "title": "Image Co-skeletonization via Co-segmentation", "comments": "13 pages, 12 figures, Submitted to IEEE Transactions on Image\n  Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the joint processing of images have certainly shown its\nadvantages over individual processing. Different from the existing works geared\ntowards co-segmentation or co-localization, in this paper, we explore a new\njoint processing topic: image co-skeletonization, which is defined as joint\nskeleton extraction of objects in an image collection. Object skeletonization\nin a single natural image is a challenging problem because there is hardly any\nprior knowledge about the object. Therefore, we resort to the idea of object\nco-skeletonization, hoping that the commonness prior that exists across the\nimages may help, just as it does for other joint processing problems such as\nco-segmentation. We observe that the skeleton can provide good scribbles for\nsegmentation, and skeletonization, in turn, needs good segmentation. Therefore,\nwe propose a coupled framework for co-skeletonization and co-segmentation tasks\nso that they are well informed by each other, and benefit each other\nsynergistically. Since it is a new problem, we also construct a benchmark\ndataset by annotating nearly 1.8k images spread across 38 categories. Extensive\nexperiments demonstrate that the proposed method achieves promising results in\nall the three possible scenarios of joint-processing: weakly-supervised,\nsupervised, and unsupervised.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:35:54 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jerripothula", "Koteswar Rao", ""], ["Cai", "Jianfei", ""], ["Lu", "Jiangbo", ""], ["Yuan", "Junsong", ""]]}, {"id": "2004.05609", "submitter": "Saeed Shafiee Sabet", "authors": "Saeed Shafiee Sabet, Steven Schmidt, Saman Zadtootaghaj, Carsten\n  Griwodz, Sebastian Moller", "title": "Delay Sensitivity Classification of Cloud Gaming Content", "comments": "Accepted In International Workshop on Immersive Mixed and Virtual\n  Environment Systems 2020. ACM, Istanbul, Turkey", "journal-ref": null, "doi": "10.1145/3386293.3397116", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Gaming is an emerging service that catches growing interest in the\nresearch community as well as industry. While the paradigm shift from a game\nexecution on clients to streaming games from the cloud offers a variety of\nbenefits, the new services also require a highly reliable and low latency\nnetwork to achieve a satisfying Quality of Experience (QoE) for its users.\nUsing a cloud gaming service with high latency would harm the interaction of\nthe user with the game, leading to a decrease in playing performance and thus\nfrustration of players. However, the negative effect of delay on gaming QoE\ndepends strongly on the game content. At a certain level of delay, a slow-paced\ncard game is typically not as delay sensitive as a shooting game. For optimal\nresource allocation and quality estimation, it is highly important for cloud\nproviders, game developers, and network planners to consider the impact of the\ngame content. This paper contributes to a better understanding of the delay\nimpact on QoE for cloud gaming applications by identifying game characteristics\ninfluencing the delay perception of users. In addition, an expert evaluation\nmethodology to quantify these characteristics, as well as a delay sensitivity\nclassification based on a decision tree is presented. The ratings of 14 experts\nfor the quantification indicated an excellent level of agreement which\ndemonstrates the reliability of the proposed method. Additionally, the decision\ntree reached an accuracy of 86.6 % on determining the delay sensitivity classes\nwhich were derived from a large dataset of subjective input quality ratings\nduring a series of experiments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 13:31:49 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 11:48:10 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Sabet", "Saeed Shafiee", ""], ["Schmidt", "Steven", ""], ["Zadtootaghaj", "Saman", ""], ["Griwodz", "Carsten", ""], ["Moller", "Sebastian", ""]]}, {"id": "2004.06163", "submitter": "Wei Zhou", "authors": "Wei Zhou, Qiuping Jiang, Yuwang Wang, Zhibo Chen, Weiping Li", "title": "Blind Quality Assessment for Image Superresolution Using Deep Two-Stream\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous image superresolution (SR) algorithms have been proposed for\nreconstructing high-resolution (HR) images from input images with lower spatial\nresolutions. However, effectively evaluating the perceptual quality of SR\nimages remains a challenging research problem. In this paper, we propose a\nno-reference/blind deep neural network-based SR image quality assessor\n(DeepSRQ). To learn more discriminative feature representations of various\ndistorted SR images, the proposed DeepSRQ is a two-stream convolutional network\nincluding two subcomponents for distorted structure and texture SR images.\nDifferent from traditional image distortions, the artifacts of SR images cause\nboth image structure and texture quality degradation. Therefore, we choose the\ntwo-stream scheme that captures different properties of SR inputs instead of\ndirectly learning features from one image stream. Considering the human visual\nsystem (HVS) characteristics, the structure stream focuses on extracting\nfeatures in structural degradations, while the texture stream focuses on the\nchange in textural distributions. In addition, to augment the training data and\nensure the category balance, we propose a stride-based adaptive cropping\napproach for further improvement. Experimental results on three publicly\navailable SR image quality databases demonstrate the effectiveness and\ngeneralization ability of our proposed DeepSRQ method compared with\nstate-of-the-art image quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:14:28 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zhou", "Wei", ""], ["Jiang", "Qiuping", ""], ["Wang", "Yuwang", ""], ["Chen", "Zhibo", ""], ["Li", "Weiping", ""]]}, {"id": "2004.07532", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Sergio Romero-Tapiador, Julian Fierrez and Ruben\n  Vera-Rodriguez", "title": "DeepFakes Evolution: Analysis of Facial Regions and Fake Detection\n  Performance", "comments": null, "journal-ref": "Proc. International Conference on Pattern Recognition Workshops\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media forensics has attracted a lot of attention in the last years in part\ndue to the increasing concerns around DeepFakes. Since the initial DeepFake\ndatabases from the 1st generation such as UADFV and FaceForensics++ up to the\nlatest databases of the 2nd generation such as Celeb-DF and DFDC, many visual\nimprovements have been carried out, making fake videos almost indistinguishable\nto the human eye. This study provides an exhaustive analysis of both 1st and\n2nd DeepFake generations in terms of facial regions and fake detection\nperformance. Two different methods are considered in our experimental\nframework: i) the traditional one followed in the literature and based on\nselecting the entire face as input to the fake detection system, and ii) a\nnovel approach based on the selection of specific facial regions as input to\nthe fake detection system.\n  Among all the findings resulting from our experiments, we highlight the poor\nfake detection results achieved even by the strongest state-of-the-art fake\ndetectors in the latest DeepFake databases of the 2nd generation, with Equal\nError Rate results ranging from 15% to 30%. These results remark the necessity\nof further research to develop more sophisticated fake detectors.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 08:49:32 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:24:22 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tolosana", "Ruben", ""], ["Romero-Tapiador", "Sergio", ""], ["Fierrez", "Julian", ""], ["Vera-Rodriguez", "Ruben", ""]]}, {"id": "2004.07676", "submitter": "Nicol\\`o Bonettini", "authors": "Nicol\\`o Bonettini, Edoardo Daniele Cannas, Sara Mandelli, Luca Bondi,\n  Paolo Bestagini, Stefano Tubaro", "title": "Video Face Manipulation Detection Through Ensemble of CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, several techniques for facial manipulation in videos\nhave been successfully developed and made available to the masses (i.e.,\nFaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in\nvideo sequences with incredibly realistic results and a very little effort.\nDespite the usefulness of these tools in many fields, if used maliciously, they\ncan have a significantly bad impact on society (e.g., fake news spreading,\ncyber bullying through fake revenge porn). The ability of objectively detecting\nwhether a face has been manipulated in a video sequence is then a task of\nutmost importance. In this paper, we tackle the problem of face manipulation\ndetection in video sequences targeting modern facial manipulation techniques.\nIn particular, we study the ensembling of different trained Convolutional\nNeural Network (CNN) models. In the proposed solution, different models are\nobtained starting from a base network (i.e., EfficientNetB4) making use of two\ndifferent concepts: (i) attention layers; (ii) siamese training. We show that\ncombining these networks leads to promising face manipulation detection results\non two publicly available datasets with more than 119000 videos.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:19:40 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bonettini", "Nicol\u00f2", ""], ["Cannas", "Edoardo Daniele", ""], ["Mandelli", "Sara", ""], ["Bondi", "Luca", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2004.07682", "submitter": "Nicol\\`o Bonettini", "authors": "Nicol\\`o Bonettini, Paolo Bestagini, Simone Milani, Stefano Tubaro", "title": "On the use of Benford's law to detect GAN-generated images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Generative Adversarial Network (GAN) architectures has given\nanyone the ability of generating incredibly realistic synthetic imagery. The\nmalicious diffusion of GAN-generated images may lead to serious social and\npolitical consequences (e.g., fake news spreading, opinion formation, etc.). It\nis therefore important to regulate the widespread distribution of synthetic\nimagery by developing solutions able to detect them. In this paper, we study\nthe possibility of using Benford's law to discriminate GAN-generated images\nfrom natural photographs. Benford's law describes the distribution of the most\nsignificant digit for quantized Discrete Cosine Transform (DCT) coefficients.\nExtending and generalizing this property, we show that it is possible to\nextract a compact feature vector from an image. This feature vector can be fed\nto an extremely simple classifier for GAN-generated image detection purpose.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:42:14 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bonettini", "Nicol\u00f2", ""], ["Bestagini", "Paolo", ""], ["Milani", "Simone", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2004.08030", "submitter": "Predrag Lazic", "authors": "Predrag Lazic", "title": "Smartphone camera based pointer", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large screen displays are omnipresent today as a part of infrastructure for\npresentations and entertainment. Also powerful smartphones with integrated\ncamera(s) are ubiquitous. However, there are not many ways in which smartphones\nand screens can interact besides casting the video from a smartphone. In this\npaper, we present a novel idea that turns a smartphone into a direct virtual\npointer on the screen using the phone's camera. The idea and its implementation\nare simple, robust, efficient and fun to use. Besides the mathematical concepts\nof the idea we accompany the paper with a small javascript project\n(www.mobiletvgames.com) which demonstrates the possibility of the new\ninteraction technique presented as a massive multiplayer game in the HTML5\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:59:23 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lazic", "Predrag", ""]]}, {"id": "2004.08096", "submitter": "Yanghua Jin", "authors": "Naofumi Akimoto, Huachun Zhu, Yanghua Jin, Yoshimitsu Aoki", "title": "Fast Soft Color Segmentation", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of soft color segmentation, defined as decomposing a\ngiven image into several RGBA layers, each containing only homogeneous color\nregions. The resulting layers from decomposition pave the way for applications\nthat benefit from layer-based editing, such as recoloring and compositing of\nimages and videos. The current state-of-the-art approach for this problem is\nhindered by slow processing time due to its iterative nature, and consequently\ndoes not scale to certain real-world scenarios. To address this issue, we\npropose a neural network based method for this task that decomposes a given\nimage into multiple layers in a single forward pass. Furthermore, our method\nseparately decomposes the color layers and the alpha channel layers. By\nleveraging a novel training objective, our method achieves proper assignment of\ncolors amongst layers. As a consequence, our method achieve promising quality\nwithout existing issue of inference speed for iterative approaches. Our\nthorough experimental analysis shows that our method produces qualitative and\nquantitative results comparable to previous methods while achieving a 300,000x\nspeed improvement. Finally, we utilize our proposed method on several\napplications, and demonstrate its speed advantage, especially in video editing.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 07:43:33 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Akimoto", "Naofumi", ""], ["Zhu", "Huachun", ""], ["Jin", "Yanghua", ""], ["Aoki", "Yoshimitsu", ""]]}, {"id": "2004.09476", "submitter": "Chuang Gan", "authors": "Chuang Gan, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio\n  Torralba", "title": "Music Gesture for Visual Sound Separation", "comments": "CVPR 2020. Project page: http://music-gesture.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches have achieved impressive performance on\nvisual sound separation tasks. However, these approaches are mostly built on\nappearance and optical flow like motion feature representations, which exhibit\nlimited abilities to find the correlations between audio signals and visual\npoints, especially when separating multiple instruments of the same types, such\nas multiple violins in a scene. To address this, we propose \"Music Gesture,\" a\nkeypoint-based structured representation to explicitly model the body and\nfinger movements of musicians when they perform music. We first adopt a\ncontext-aware graph network to integrate visual semantic context with body\ndynamics, and then apply an audio-visual fusion model to associate body\nmovements with the corresponding audio signals. Experimental results on three\nmusic performance datasets show: 1) strong improvements upon benchmark metrics\nfor hetero-musical separation tasks (i.e. different instruments); 2) new\nability for effective homo-musical separation for piano, flute, and trumpet\nduets, which to our best knowledge has never been achieved with alternative\nmethods. Project page: http://music-gesture.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:53:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Gan", "Chuang", ""], ["Huang", "Deng", ""], ["Zhao", "Hang", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""]]}, {"id": "2004.10314", "submitter": "Jan Sedmidubsky", "authors": "Jan Sedmidubsky and Pavel Zezula", "title": "Combining Deep Learning Classifiers for 3D Action Recognition", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular task of 3D human action recognition is almost exclusively solved\nby training deep-learning classifiers. To achieve a high recognition accuracy,\nthe input 3D actions are often pre-processed by various normalization or\naugmentation techniques. However, it is not computationally feasible to train a\nclassifier for each possible variant of training data in order to select the\nbest-performing subset of pre-processing techniques for a given dataset. In\nthis paper, we propose to train an independent classifier for each available\npre-processing technique and fuse the classification results based on a strict\nmajority vote rule. Together with a proposed evaluation procedure, we can very\nefficiently determine the best combination of normalization and augmentation\ntechniques for a specific dataset. For the best-performing combination, we can\nretrospectively apply the normalized/augmented variants of input data to train\nonly a single classifier. This also allows us to decide whether it is better to\ntrain a single model, or rather a set of independent classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 21:36:25 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sedmidubsky", "Jan", ""], ["Zezula", "Pavel", ""]]}, {"id": "2004.10345", "submitter": "T.J. Tsai", "authors": "Thitaree Tanprasert, Teerapat Jenrungrot, Meinard Mueller, T.J. Tsai", "title": "MIDI-Sheet Music Alignment Using Bootleg Score Synthesis", "comments": "8 pages, 6 figures, 1 table. Accepted paper at the International\n  Society for Music Information Retrieval Conference (ISMIR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MIDI-sheet music alignment is the task of finding correspondences between a\nMIDI representation of a piece and its corresponding sheet music images. Rather\nthan using optical music recognition to bridge the gap between sheet music and\nMIDI, we explore an alternative approach: projecting the MIDI data into pixel\nspace and performing alignment in the image domain. Our method converts the\nMIDI data into a crude representation of the score that only contains\nrectangular floating notehead blobs, a process we call bootleg score synthesis.\nFurthermore, we project sheet music images into the same bootleg space by\napplying a deep watershed notehead detector and filling in the bounding boxes\naround each detected notehead. Finally, we align the bootleg representations\nusing a simple variant of dynamic time warping. On a dataset of 68 real scanned\npiano scores from IMSLP and corresponding MIDI performances, our method\nachieves a 97.3% accuracy at an error tolerance of one second, outperforming\nseveral baseline systems that employ optical music recognition.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:45:18 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Tanprasert", "Thitaree", ""], ["Jenrungrot", "Teerapat", ""], ["Mueller", "Meinard", ""], ["Tsai", "T. J.", ""]]}, {"id": "2004.10347", "submitter": "T.J. Tsai", "authors": "Daniel Yang, Thitaree Tanprasert, Teerapat Jenrungrot, Mengyi Shan, TJ\n  Tsai", "title": "MIDI Passage Retrieval Using Cell Phone Pictures of Sheet Music", "comments": "8 pages, 8 figures, 1 table. Accepted paper at the International\n  Society for Music Information Retrieval Conference (ISMIR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a cross-modal retrieval problem in which a user would\nlike to retrieve a passage of music from a MIDI file by taking a cell phone\npicture of a physical page of sheet music. While audio-sheet music retrieval\nhas been explored by a number of works, this scenario is novel in that the\nquery is a cell phone picture rather than a digital scan. To solve this\nproblem, we introduce a mid-level feature representation called a bootleg score\nwhich explicitly encodes the rules of Western musical notation. We convert both\nthe MIDI and the sheet music into bootleg scores using deterministic rules of\nmusic and classical computer vision techniques for detecting simple geometric\nshapes. Once the MIDI and cell phone image have been converted into bootleg\nscores, we estimate the alignment using dynamic programming. The most notable\ncharacteristic of our system is that it does test-time adaptation and has no\ntrainable weights at all -- only a set of about 30 hyperparameters. On a\ndataset containing 1000 cell phone pictures taken of 100 scores of classical\npiano music, our system achieves an F measure score of .869 and outperforms\nbaseline systems based on commercial optical music recognition software.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:56:53 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yang", "Daniel", ""], ["Tanprasert", "Thitaree", ""], ["Jenrungrot", "Teerapat", ""], ["Shan", "Mengyi", ""], ["Tsai", "TJ", ""]]}, {"id": "2004.10391", "submitter": "T.J. Tsai", "authors": "TJ Tsai", "title": "Towards Linking the Lakh and IMSLP Datasets", "comments": "5 pages, 4 figures, 1 table. Accepted paper at the International\n  Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053815", "report-no": null, "categories": "eess.AS cs.MM cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of matching a MIDI file against a large\ndatabase of piano sheet music images. Previous sheet-audio and sheet-MIDI\nalignment approaches have primarily focused on a 1-to-1 alignment task, which\nis not a scalable solution for retrieval from large databases. We propose a\nmethod for scalable cross-modal retrieval that might be used to link the Lakh\nMIDI dataset with IMSLP sheet music data. Our approach is to modify a\npreviously proposed feature representation called a symbolic bootleg score to\nbe suitable for hashing. On a database of 5,000 piano scores containing 55,000\nindividual sheet music images, our system achieves a mean reciprocal rank of\n0.84 and an average retrieval time of 25.4 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 04:13:10 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Tsai", "TJ", ""]]}, {"id": "2004.10469", "submitter": "Yijun Quan", "authors": "Yijun Quan, Chang-Tsun Li, Yujue Zhou and Li Li", "title": "Warwick Image Forensics Dataset for Device Fingerprinting In Multimedia\n  Forensics", "comments": "Paper accepted to IEEE International Conference on Multimedia and\n  Expo 2020 (ICME 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device fingerprints like sensor pattern noise (SPN) are widely used for\nprovenance analysis and image authentication. Over the past few years, the\nrapid advancement in digital photography has greatly reshaped the pipeline of\nimage capturing process on consumer-level mobile devices. The flexibility of\ncamera parameter settings and the emergence of multi-frame photography\nalgorithms, especially high dynamic range (HDR) imaging, bring new challenges\nto device fingerprinting. The subsequent study on these topics requires a new\npurposefully built image dataset. In this paper, we present the Warwick Image\nForensics Dataset, an image dataset of more than 58,600 images captured using\n14 digital cameras with various exposure settings. Special attention to the\nexposure settings allows the images to be adopted by different multi-frame\ncomputational photography algorithms and for subsequent device fingerprinting.\nThe dataset is released as an open-source, free for use for the digital\nforensic community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:54:27 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 10:55:05 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Quan", "Yijun", ""], ["Li", "Chang-Tsun", ""], ["Zhou", "Yujue", ""], ["Li", "Li", ""]]}, {"id": "2004.10808", "submitter": "Ben Kybartas", "authors": "Ben Kybartas, Clark Verbrugge, Jonathan Lessard", "title": "Tension Space Analysis for Emergent Narrative", "comments": "14 pages, 7 figures, IEEE Transactions on Games 2020", "journal-ref": null, "doi": "10.1109/TG.2020.2989072", "report-no": null, "categories": "cs.AI cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent narratives provide a unique and compelling approach to interactive\nstorytelling through simulation, and have applications in games, narrative\ngeneration, and virtual agents. However the inherent complexity of simulation\nmakes understanding the expressive potential of emergent narratives difficult,\nparticularly at the design phase of development. In this paper, we present a\nnovel approach to emergent narrative using the narratological theory of\npossible worlds and demonstrate how the design of works in such a system can be\nunderstood through a formal means of analysis inspired by expressive range\nanalysis. Lastly, we propose a novel way through which content may be authored\nfor the emergent narrative system using a sketch-based interface.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:26:09 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Kybartas", "Ben", ""], ["Verbrugge", "Clark", ""], ["Lessard", "Jonathan", ""]]}, {"id": "2004.11056", "submitter": "Maria Santamaria", "authors": "Maria Santamaria, Saverio Blasi, Ebroul Izquierdo, Marta Mrak", "title": "Analytic Simplification of Neural Network based Intra-Prediction Modes\n  for Video Compression", "comments": "To apper in IEEE ICMEW 2020", "journal-ref": "2020 IEEE International Conference on Multimedia & Expo Workshops\n  (ICMEW), 6-10 July 2020, London, United Kingdom", "doi": "10.1109/ICMEW46912.2020.9106027", "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand for video content at higher resolutions, it is\nevermore critical to find ways to limit the complexity of video encoding tasks\nin order to reduce costs, power consumption and environmental impact of video\nservices. In the last few years, algorithms based on Neural Networks (NN) have\nbeen shown to benefit many conventional video coding modules. But while such\ntechniques can considerably improve the compression efficiency, they usually\nare very computationally intensive. It is highly beneficial to simplify models\nlearnt by NN so that meaningful insights can be exploited with the goal of\nderiving less complex solutions. This paper presents two ways to derive\nsimplified intra-prediction from learnt models, and shows that these\nstreamlined techniques can lead to efficient compression solutions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:25:54 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Santamaria", "Maria", ""], ["Blasi", "Saverio", ""], ["Izquierdo", "Ebroul", ""], ["Mrak", "Marta", ""]]}, {"id": "2004.11250", "submitter": "Pu Zhao", "authors": "Wei Niu, Pu Zhao, Zheng Zhan, Xue Lin, Yanzhi Wang, Bin Ren", "title": "Towards Real-Time DNN Inference on Mobile Platforms with Model Pruning\n  and Compiler Optimization", "comments": "accepted by the IJCAI-PRICAI 2020 Demonstrations Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-end mobile platforms rapidly serve as primary computing devices for a\nwide range of Deep Neural Network (DNN) applications. However, the constrained\ncomputation and storage resources on these devices still pose significant\nchallenges for real-time DNN inference executions. To address this problem, we\npropose a set of hardware-friendly structured model pruning and compiler\noptimization techniques to accelerate DNN executions on mobile devices. This\ndemo shows that these optimizations can enable real-time mobile execution of\nmultiple DNN applications, including style transfer, DNN coloring and super\nresolution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 03:18:23 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Niu", "Wei", ""], ["Zhao", "Pu", ""], ["Zhan", "Zheng", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""]]}, {"id": "2004.11449", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, R\\'emi Lebret, Didier Orel, Philippe Sordet, Karl Aberer", "title": "Upgrading the Newsroom: An Automated Image Selection System for News\n  Articles", "comments": "Accepted to ACM Transactions on Multimedia Computing Communications\n  and Applications (ACM TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an automated image selection system to assist photo editors in\nselecting suitable images for news articles. The system fuses multiple textual\nsources extracted from news articles and accepts multilingual inputs. It is\nequipped with char-level word embeddings to help both modeling morphologically\nrich languages, e.g. German, and transferring knowledge across nearby\nlanguages. The text encoder adopts a hierarchical self-attention mechanism to\nattend more to both keywords within a piece of text and informative components\nof a news article. We extensively experiment with our system on a large-scale\ntext-image database containing multimodal multilingual news articles collected\nfrom Swiss local news media websites. The system is compared with multiple\nbaselines with ablation studies and is shown to beat existing text-image\nretrieval methods in a weakly-supervised learning setting. Besides, we also\noffer insights on the advantage of using multiple textual sources and\nmultilingual data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 20:29:26 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liu", "Fangyu", ""], ["Lebret", "R\u00e9mi", ""], ["Orel", "Didier", ""], ["Sordet", "Philippe", ""], ["Aberer", "Karl", ""]]}, {"id": "2004.11490", "submitter": "Babak Naderi", "authors": "Babak Naderi and Sebastian M\\\"oller", "title": "Transformation of Mean Opinion Scores to Avoid Misleading of Ranked\n  based Statistical Techniques", "comments": "his paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123078", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rank correlation coefficients and the ranked-based statistical tests (as\na subset of non-parametric techniques) might be misleading when they are\napplied to subjectively collected opinion scores. Those techniques assume that\nthe data is measured at least at an ordinal level and define a sequence of\nscores to represent a tied rank when they have precisely an equal numeric\nvalue.\n  In this paper, we show that the definition of tied rank, as mentioned above,\nis not suitable for Mean Opinion Scores (MOS) and might be misleading\nconclusions of rank-based statistical techniques. Furthermore, we introduce a\nmethod to overcome this issue by transforming the MOS values considering their\n$95\\%$ Confidence Intervals. The rank correlation coefficients and ranked-based\nstatistical tests can then be safely applied to the transformed values. We also\nprovide open-source software packages in different programming languages to\nutilize the application of our transformation method in the quality of\nexperience domain.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 23:44:58 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Naderi", "Babak", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.11511", "submitter": "Xiushan Nie", "authors": "Xingbo Liu, Xiushan Nie, Qi Dai, Yupan Huang, Yilong Yin", "title": "Reinforcing Short-Length Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the compelling efficiency in retrieval and storage,\nsimilarity-preserving hashing has been widely applied to approximate nearest\nneighbor search in large-scale image retrieval. However, existing methods have\npoor performance in retrieval using an extremely short-length hash code due to\nweak ability of classification and poor distribution of hash bit. To address\nthis issue, in this study, we propose a novel reinforcing short-length hashing\n(RSLH). In this proposed RSLH, mutual reconstruction between the hash\nrepresentation and semantic labels is performed to preserve the semantic\ninformation. Furthermore, to enhance the accuracy of hash representation, a\npairwise similarity matrix is designed to make a balance between accuracy and\ntraining expenditure on memory. In addition, a parameter boosting strategy is\nintegrated to reinforce the precision with hash bits fusion. Extensive\nexperiments on three large-scale image benchmarks demonstrate the superior\nperformance of RSLH under various short-length hashing scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 02:23:52 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liu", "Xingbo", ""], ["Nie", "Xiushan", ""], ["Dai", "Qi", ""], ["Huang", "Yupan", ""], ["Yin", "Yilong", ""]]}, {"id": "2004.11639", "submitter": "Alessandro Ortis", "authors": "Alessandro Ortis and Giovanni Maria Farinella and Sebastiano Battiato", "title": "Survey on Visual Sentiment Analysis", "comments": "This paper is a postprint of a paper accepted by IET Image Processing\n  and is subject to Institution of Engineering and Technology Copyright. When\n  the final version is published, the copy of record will be available at the\n  IET Digital Library", "journal-ref": null, "doi": "10.1049/iet-ipr.2019.1270", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Sentiment Analysis aims to understand how images affect people, in\nterms of evoked emotions. Although this field is rather new, a broad range of\ntechniques have been developed for various data sources and problems, resulting\nin a large body of research. This paper reviews pertinent publications and\ntries to present an exhaustive overview of the field. After a description of\nthe task and the related applications, the subject is tackled under different\nmain headings. The paper also describes principles of design of general Visual\nSentiment Analysis systems from three main points of view: emotional models,\ndataset definition, feature design. A formalization of the problem is\ndiscussed, considering different levels of granularity, as well as the\ncomponents that can affect the sentiment toward an image in different ways. To\nthis aim, this paper considers a structured formalization of the problem which\nis usually used for the analysis of text, and discusses it's suitability in the\ncontext of Visual Sentiment Analysis. The paper also includes a description of\nnew challenges, the evaluation from the viewpoint of progress toward more\nsophisticated systems and related practical applications, as well as a summary\nof the insights resulting from this study.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 10:15:22 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 11:09:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ortis", "Alessandro", ""], ["Farinella", "Giovanni Maria", ""], ["Battiato", "Sebastiano", ""]]}, {"id": "2004.11724", "submitter": "T.J. Tsai", "authors": "TJ Tsai, Daniel Yang, Mengyi Shan, Thitaree Tanprasert, Teerapat\n  Jenrungrot", "title": "Using Cell Phone Pictures of Sheet Music To Retrieve MIDI Passages", "comments": "13 pages, 8 figures, 3 tables. Accepted article in IEEE Transactions\n  on Multimedia. arXiv admin note: text overlap with arXiv:2004.10347", "journal-ref": null, "doi": "10.1109/TMM.2020.2973831", "report-no": null, "categories": "cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates a cross-modal retrieval problem in which a user\nwould like to retrieve a passage of music from a MIDI file by taking a cell\nphone picture of several lines of sheet music. This problem is challenging for\ntwo reasons: it has a significant runtime constraint since it is a user-facing\napplication, and there is very little relevant training data containing cell\nphone images of sheet music. To solve this problem, we introduce a novel\nfeature representation called a bootleg score which encodes the position of\nnoteheads relative to staff lines in sheet music. The MIDI representation can\nbe converted into a bootleg score using deterministic rules of Western musical\nnotation, and the sheet music image can be converted into a bootleg score using\nclassical computer vision techniques for detecting simple geometrical shapes.\nOnce the MIDI and cell phone image have been converted into bootleg scores, we\ncan estimate the alignment using dynamic programming. The most notable\ncharacteristic of our system is that it has no trainable weights at all -- only\na set of about 40 hyperparameters. With a training set of just 400 images, we\nshow that our system generalizes well to a much larger set of 1600 test images\nfrom 160 unseen musical scores. Our system achieves a test F measure score of\n0.89, has an average runtime of 0.90 seconds, and outperforms baseline systems\nbased on music object detection and sheet-audio alignment. We provide extensive\nexperimental validation and analysis of our system.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 00:37:43 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Tsai", "TJ", ""], ["Yang", "Daniel", ""], ["Shan", "Mengyi", ""], ["Tanprasert", "Thitaree", ""], ["Jenrungrot", "Teerapat", ""]]}, {"id": "2004.11838", "submitter": "Firoj Alam", "authors": "Ferda Ofli, Firoj Alam and Muhammad Imran", "title": "Analysis of Social Media Data using Multimodal Deep Learning for\n  Disaster Response", "comments": "Accepted in ISCRAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia content in social media platforms provides significant information\nduring disaster events. The types of information shared include reports of\ninjured or deceased people, infrastructure damage, and missing or found people,\namong others. Although many studies have shown the usefulness of both text and\nimage content for disaster response purposes, the research has been mostly\nfocused on analyzing only the text modality in the past. In this paper, we\npropose to use both text and image modalities of social media data to learn a\njoint representation using state-of-the-art deep learning techniques.\nSpecifically, we utilize convolutional neural networks to define a multimodal\ndeep learning architecture with a modality-agnostic shared representation.\nExtensive experiments on real-world disaster datasets show that the proposed\nmultimodal architecture yields better performance than models trained using a\nsingle modality (e.g., either text or image).\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 19:36:11 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ofli", "Ferda", ""], ["Alam", "Firoj", ""], ["Imran", "Muhammad", ""]]}, {"id": "2004.11974", "submitter": "Alan Abdulla Dr", "authors": "Alan A. Abdulla, Harin Sellahewa, Sabah A. Jassim", "title": "Improving embedding efficiency for digital steganography by exploiting\n  similarities between secret and cover images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital steganography is becoming a common tool for protecting sensitive\ncommunications in various applications such as crime(terrorism) prevention\nwhereby law enforcing personals need to remotely compare facial images captured\nat the scene of crime with faces databases of known criminals(suspects);\nexchanging military maps or surveillance video in hostile\nenvironment(situations); privacy preserving in the healthcare systems when\nstoring or exchanging patient medical images(records); and prevent bank\ncustomers accounts(records) from being accessed illegally by unauthorized\nusers. Existing digital steganography schemes for embedding secret images in\ncover image files tend not to exploit various redundancies in the secret image\nbit-stream to deal with the various conflicting requirements on embedding\ncapacity, stego-image quality, and un-detectibility. This paper is concerned\nwith the development of innovative image procedures and data hiding schemes\nthat exploit, as well as increase, similarities between secret image bit-stream\nand the cover image LSB plane. This will be achieved in two novel steps\ninvolving manipulating both the secret and the cover images,prior to embedding,\nto achieve higher 0:1 ratio in both the secret image bit-stream and the cover\nimage LSB plane. The above two steps strategy has been exploited to use a\nbit-plane(s) mapping technique, instead of bit-plane(s) replacement to make\neach cover pixel usable for secret embedding. This paper will demonstrate that\nthis strategy produces stego-images that have minimal distortion, high\nembedding efficiency, reasonably good stego-image quality and robustness\nagainst 3 well-known targeted steganalysis tools.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:26:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abdulla", "Alan A.", ""], ["Sellahewa", "Harin", ""], ["Jassim", "Sabah A.", ""]]}, {"id": "2004.11977", "submitter": "Alan Abdulla Dr", "authors": "Alan Anwer Abdulla, Harin Sellahewa and Sabah A. Jassim", "title": "Steganography Based on Pixel Intensity Value Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on steganography based on pixel intensity value\ndecomposition. A number of existing schemes such as binary, Fibonacci, Prime,\nNatural, Lucas, and Catalan-Fibonacci (CF) are evaluated in terms of payload\ncapacity and stego quality. A new technique based on a specific representation\nis used to decompose pixel intensity values into 16 (virtual) bit-planes\nsuitable for embedding purposes. The new decomposition scheme has a desirable\nproperty whereby the sum of all bit-planes does not exceed the maximum pixel\nintensity value, i.e. 255. Experimental results demonstrate that the new\ndecomposition scheme offers a better compromise between payload capacity and\nstego quality than other existing decomposition schemes used for embedding\nmessages. However, embedding in the 6th bit-plane onwards, the proposed scheme\noffers better stego quality. In general, the new decomposition technique has\nless effect in terms of quality on pixel value when compared to most existing\npixel intensity value decomposition techniques when embedding messages in\nhigher bit-planes.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:31:38 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abdulla", "Alan Anwer", ""], ["Sellahewa", "Harin", ""], ["Jassim", "Sabah A.", ""]]}, {"id": "2004.11984", "submitter": "Alan Abdulla Dr", "authors": "Alan Anwer Abdulla, Sabah A. Jassim and Harin Sellahewa", "title": "Efficient High Capacity Steganography Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance indicators characterizing modern steganographic techniques\ninclude capacity (i.e. the quantity of data that can be hidden in the cover\nmedium), stego quality (i.e. artifacts visibility), security (i.e.\nundetectability), and strength or robustness (intended as the resistance\nagainst active attacks aimed to destroy the secret message). Fibonacci based\nembedding techniques have been researched and proposed in the literature to\nachieve efficient steganography in terms of capacity with respect to stego\nquality. In this paper, we investigated an innovative idea that extends\nFibonacci-like steganography by bit-plane(s) mapping instead of bit-plane(s)\nreplacement. Our proposed algorithm increases embedding capacity using\nbit-plane mapping to embed two bits of the secret message in three bits of a\npixel of the cover, at the expense of a marginal loss in stego quality. While\nexisting Fibonacci embedding algorithms do not use certain intensities of the\ncover for embedding due to the limitation imposed by the Zeckendorf theorem,\nour proposal solve this problem and make all intensity values candidates for\nembedding. Experimental results demonstrate that the proposed technique double\nthe embedding capacity when compared to existing Fibonacci methods, and it is\nsecure against statistical attacks such as RS, POV, and difference image\nhistogram (DIH).\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:41:16 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abdulla", "Alan Anwer", ""], ["Jassim", "Sabah A.", ""], ["Sellahewa", "Harin", ""]]}, {"id": "2004.11994", "submitter": "Tanwi Mallick", "authors": "Tanwi Mallick and Patha Pratim Das and Arun Kumar Majumdar", "title": "Bharatanatyam Dance Transcription using Multimedia Ontology and Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indian Classical Dance is an over 5000 years' old multi-modal language for\nexpressing emotions. Preservation of dance through multimedia technology is a\nchallenging task. In this paper, we develop a system to generate a parseable\nrepresentation of a dance performance. The system will help to preserve\nintangible heritage, annotate performances for better tutoring, and synthesize\ndance performances. We first attempt to capture the concepts of the basic steps\nof an Indian Classical Dance form, named Bharatanatyam Adavus, in an\nontological model. Next, we build an event-based low-level model that relates\nthe ontology of Adavus to the ontology of multi-modal data streams (RGB-D of\nKinect in this case) for a computationally realizable framework. Finally, the\nontology is used for transcription into Labanotation. We also present a\ntranscription tool for encoding the performances of Bharatanatyam Adavus to\nLabanotation and test it on our recorded data set. Our primary aim is to\ndocument the complex movements of dance in terms of Labanotation using the\nontology.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:22:20 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mallick", "Tanwi", ""], ["Das", "Patha Pratim", ""], ["Majumdar", "Arun Kumar", ""]]}, {"id": "2004.12038", "submitter": "Mohammed Belkhatir", "authors": "M. Belkhatir", "title": "Fuzzy Logic Based Integration of Web Contextual Linguistic Structures\n  for Enriching Conceptual Visual Representations", "comments": null, "journal-ref": null, "doi": "10.1109/TETCI.2018.2849417", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the difficulty of automatically mapping visual features with semantic\ndescriptors, state-of-the-art frameworks have exhibited poor performance in\nterms of coverage and effectiveness for indexing the visual content. This\nprompted us to investigate the use of both the Web as a large information\nsource from where to extract relevant contextual linguistic information and\nbimodal visual-textual indexing as a technique to enrich the vocabulary of\nindex concepts. Our proposal is based on the Signal/Semantic approach for\nmultimedia indexing which generates multi-facetted conceptual representations\nof the visual content. We propose to enrich these image representations with\nconcepts automatically extracted from the visual contextual information. We\nspecifically target the integration of semantic concepts which are more\nspecific than the initial index concepts since they represent the visual\ncontent with greater accuracy and precision. Also, we aim to correct the faulty\nindexes resulting from the automatic semantic tagging. Experimentally, the\ndetails of the prototyping are given and the presented technique is tested in a\nWeb-scale evaluation on 30 queries representing elaborate image scenes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:49:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Belkhatir", "M.", ""]]}, {"id": "2004.12059", "submitter": "Di Zhuang", "authors": "Di Zhuang, Nam Nguyen, Keyu Chen, J. Morris Chang", "title": "SAIA: Split Artificial Intelligence Architecture for Mobile Healthcare\n  System", "comments": "17 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the advancement of deep learning (DL), the Internet of Things and cloud\ncomputing techniques for biomedical and healthcare problems, mobile healthcare\nsystems have received unprecedented attention. Since DL techniques usually\nrequire enormous amount of computation, most of them cannot be directly\ndeployed on the resource-constrained mobile and IoT devices. Hence, most of the\nmobile healthcare systems leverage the cloud computing infrastructure, where\nthe data collected by the mobile and IoT devices would be transmitted to the\ncloud computing platforms for analysis. However, in the contested environments,\nrelying on the cloud might not be practical at all times. For instance, the\nsatellite communication might be denied or disrupted. We propose SAIA, a Split\nArtificial Intelligence Architecture for mobile healthcare systems. Unlike\ntraditional approaches for artificial intelligence (AI) which solely exploits\nthe computational power of the cloud server, SAIA could not only relies on the\ncloud computing infrastructure while the wireless communication is available,\nbut also utilizes the lightweight AI solutions that work locally on the client\nside, hence, it can work even when the communication is impeded. In SAIA, we\npropose a meta-information based decision unit, that could tune whether a\nsample captured by the client should be operated by the embedded AI (i.e.,\nkeeping on the client) or the networked AI (i.e., sending to the server), under\ndifferent conditions. In our experimental evaluation, extensive experiments\nhave been conducted on two popular healthcare datasets. Our results show that\nSAIA consistently outperforms its baselines in terms of both effectiveness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 05:06:51 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 05:00:59 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhuang", "Di", ""], ["Nguyen", "Nam", ""], ["Chen", "Keyu", ""], ["Chang", "J. Morris", ""]]}, {"id": "2004.12091", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u, Peter Trifonov, Muah Kim, Rafael F. Schaefer, and\n  Vladimir Sidorenko", "title": "Randomized Nested Polar Subcode Constructions for Privacy, Secrecy, and\n  Storage", "comments": "Shorter version to appear in 2020 IEEE International Symposium on\n  Information Theory and Applications. Decoding complexity results are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider polar subcodes (PSCs), which are polar codes (PCs) with\ndynamically-frozen symbols, to increase the minimum distance as compared to\ncorresponding PCs. A randomized nested PSC construction with a low-rate PSC and\na high-rate PC, is proposed for list and sequential successive cancellation\ndecoders. This code construction aims to perform lossy compression with side\ninformation. Nested PSCs are used in the key agreement problem with physical\nidentifiers. Gains in terms of the secret-key vs. storage rate ratio as\ncompared to nested PCs with the same list size are illustrated to show that\nnested PSCs significantly improve on nested PCs. The performance of the nested\nPSCs is shown to improve with larger list sizes, which is not the case for\nnested PCs considered.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 08:57:17 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 12:57:02 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 10:26:19 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Trifonov", "Peter", ""], ["Kim", "Muah", ""], ["Schaefer", "Rafael F.", ""], ["Sidorenko", "Vladimir", ""]]}, {"id": "2004.12467", "submitter": "Alan Abdulla Dr", "authors": "Alan A. Abdulla, Harin Sellahewa, and Sabah A. Jassim", "title": "Stego Quality Enhancement by Message Size Reduction and Fibonacci\n  Bit-Plane Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An efficient 2-step steganography technique is proposed to enhance stego\nimage quality and secret message un-detectability. The first step is a\npreprocessing algorithm that reduces the size of secret images without losing\ninformation. This results in improved stego image quality compared to other\nexisting image steganography methods. The proposed secret image size reduction\n(SISR) algorithm is an efficient spatial domain technique. The second step is\nan embedding mechanism that relies on Fibonacci representation of pixel\nintensities to minimize the effect of embedding on the stego image quality. The\nimprovement is attained by using bit-plane(s) mapping instead of bit-plane(s)\nreplacement for embedding. The proposed embedding mechanism outperforms the\nbinary based LSB randomly embedding in two ways: reduced effect on stego\nquality and increased robustness against statistical steganalysers.\nExperimental results demonstrate the benefits of the proposed scheme in terms\nof: 1) SISR ratio (indirectly results in increased capacity); 2) quality of the\nstego; and 3) robustness against steganalysers such as RS, and WS. Furthermore,\nexperimental results show that the proposed SISR algorithm can be extended to\nbe applicable on DICOM standard medical images. Future security standardization\nresearch is proposed that would focus on evaluating the security, performance,\nand effectiveness of steganography algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 20:11:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abdulla", "Alan A.", ""], ["Sellahewa", "Harin", ""], ["Jassim", "Sabah A.", ""]]}, {"id": "2004.12470", "submitter": "Alan Abdulla Dr", "authors": "Alan Anwer Abdulla, Sabah A. Jassim and Harin Sellahewa", "title": "Secure Steganography Technique Based on Bitplane Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with secret hiding in multiple image bitplanes for\nincreased security without undermining capacity. A secure steganographic\nalgorithm based on bitplanes index manipulation is proposed. The index\nmanipulation is confined to the first two Least Significant Bits of the cover\nimage. The proposed algorithm has the property of un-detectability with respect\nto stego quality and payload capacity. Experimental results demonstrate that\nthe proposed technique is secure against statistical attacks such as pair of\nvalue (PoV), Weighted Stego steganalyser (WS), and Multi Bitplane Weighted\nStego steganalyser (MLSB-WS).\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 20:17:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abdulla", "Alan Anwer", ""], ["Jassim", "Sabah A.", ""], ["Sellahewa", "Harin", ""]]}, {"id": "2004.12569", "submitter": "Noshin Amiri", "authors": "Noshin Amiri, Iman Naderi", "title": "DWT-GBT-SVD-based Robust Speech Steganography", "comments": "10 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is a method that can improve network security and make\ncommunications safer. In this method, a secret message is hidden in content\nlike audio signals that should not be perceptible by listening to the audio or\nseeing the signal waves. Also, it should be robust against different common\nattacks such as noise and compression. In this paper, we propose a new speech\nsteganography method based on a combination of Discrete Wavelet Transform,\nGraph-based Transform, and Singular Value Decomposition (SVD). In this method,\nwe first find voiced frames based on energy and zero-crossing counts of the\nframes and then embed a binary message into voiced frames. Experimental results\non the NOIZEUS database show that the proposed method is imperceptible and also\nrobust against Gaussian noise, re-sampling, re-quantization, high pass filter,\nand low pass filter. Also, it is robust against MP3 compression and scaling for\nwatermarking applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 03:35:32 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Amiri", "Noshin", ""], ["Naderi", "Iman", ""]]}, {"id": "2004.12615", "submitter": "Jingjing Li", "authors": "Li Jingjing, Chen Erpeng, Ding Zhengming, Zhu Lei, Lu Ke, Shen Heng\n  Tao", "title": "Maximum Density Divergence for Domain Adaptation", "comments": "Published on IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2991050", "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation addresses the problem of transferring\nknowledge from a well-labeled source domain to an unlabeled target domain where\nthe two domains have distinctive data distributions. Thus, the essence of\ndomain adaptation is to mitigate the distribution divergence between the two\ndomains. The state-of-the-art methods practice this very idea by either\nconducting adversarial training or minimizing a metric which defines the\ndistribution gaps. In this paper, we propose a new domain adaptation method\nnamed Adversarial Tight Match (ATM) which enjoys the benefits of both\nadversarial training and metric learning. Specifically, at first, we propose a\nnovel distance loss, named Maximum Density Divergence (MDD), to quantify the\ndistribution divergence. MDD minimizes the inter-domain divergence (\"match\" in\nATM) and maximizes the intra-class density (\"tight\" in ATM). Then, to address\nthe equilibrium challenge issue in adversarial domain adaptation, we consider\nleveraging the proposed MDD into adversarial domain adaptation framework. At\nlast, we tailor the proposed MDD as a practical learning loss and report our\nATM. Both empirical evaluation and theoretical analysis are reported to verify\nthe effectiveness of the proposed method. The experimental results on four\nbenchmarks, both classical and large-scale, show that our method is able to\nachieve new state-of-the-art performance on most evaluations. Codes and\ndatasets used in this paper are available at {\\it github.com/lijin118/ATM}.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 07:35:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Jingjing", "Li", ""], ["Erpeng", "Chen", ""], ["Zhengming", "Ding", ""], ["Lei", "Zhu", ""], ["Ke", "Lu", ""], ["Tao", "Shen Heng", ""]]}, {"id": "2004.12642", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Jan-Niklas Voigt-Antons, Tanja Koji\\'c, Danish Ali, Sebastian M\\\"oller", "title": "Influence of Hand Tracking as a way of Interaction in Virtual Reality on\n  User Experience", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising interest in Virtual Reality and the fast development and\nimprovement of available devices, new features of interactions are becoming\navailable. One of them that is becoming very popular is hand tracking, as the\nidea to replace controllers for interactions in virtual worlds. This experiment\naims to compare different interaction types in VR using either controllers or\nhand tracking. Participants had to play two simple VR games with various types\nof tasks in those games - grabbing objects or typing numbers. While playing,\nthey were using interactions with different visualizations of hands and\ncontrollers. The focus of this study was to investigate user experience of\nvarying interactions (controller vs. hand tracking) for those two simple tasks.\nResults show that different interaction types statistically significantly\ninfluence reported emotions with Self-Assessment Manikin (SAM), where for hand\ntracking participants were feeling higher valence, but lower arousal and\ndominance. Additionally, task type of grabbing was reported to be more\nrealistic, and participants experienced a higher presence. Surprisingly,\nparticipants rated the interaction type with controllers where both where hands\nand controllers were visualized as statistically most preferred. Finally, hand\ntracking for both tasks was rated with the System Usability Scale (SUS) scale,\nand hand tracking for the task typing was rated as statistically significantly\nmore usable. These results can drive further research and, in the long term,\ncontribute to help selecting the most matching interaction modality for a task.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:43:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Voigt-Antons", "Jan-Niklas", ""], ["Koji\u0107", "Tanja", ""], ["Ali", "Danish", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.12811", "submitter": "Zhi-Song Liu", "authors": "Zhi-Song Liu, Wan-Chi Siu, Li-Wen Wang, Chu-Tak Li, Marie-Paule Cani,\n  Yui-Lam Chan", "title": "Unsupervised Real Image Super-Resolution via Generative Variational\n  AutoEncoder", "comments": "9 pages, 7 figures, CVPR2020 NTIRE2020 Real Image Super-Resolution\n  Challenge", "journal-ref": "2020 IEEE conference on Computer Vision and Pattern Recognition\n  Workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefited from the deep learning, image Super-Resolution has been one of the\nmost developing research fields in computer vision. Depending upon whether\nusing a discriminator or not, a deep convolutional neural network can provide\nan image with high fidelity or better perceptual quality. Due to the lack of\nground truth images in real life, people prefer a photo-realistic image with\nlow fidelity to a blurry image with high fidelity. In this paper, we revisit\nthe classic example based image super-resolution approaches and come up with a\nnovel generative model for perceptual image super-resolution. Given that real\nimages contain various noise and artifacts, we propose a joint image denoising\nand super-resolution model via Variational AutoEncoder. We come up with a\nconditional variational autoencoder to encode the reference for dense feature\nvector which can then be transferred to the decoder for target image denoising.\nWith the aid of the discriminator, an additional overhead of super-resolution\nsubnetwork is attached to super-resolve the denoised image with photo-realistic\nvisual quality. We participated the NTIRE2020 Real Image Super-Resolution\nChallenge. Experimental results show that by using the proposed approach, we\ncan obtain enlarged images with clean and pleasant features compared to other\nsupervised methods. We also compared our approach with state-of-the-art methods\non various datasets to demonstrate the efficiency of our proposed unsupervised\nsuper-resolution model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:49:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liu", "Zhi-Song", ""], ["Siu", "Wan-Chi", ""], ["Wang", "Li-Wen", ""], ["Li", "Chu-Tak", ""], ["Cani", "Marie-Paule", ""], ["Chan", "Yui-Lam", ""]]}, {"id": "2004.13095", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Thomas Jerkovits, Onur G\\\"unl\\\"u, Vladimir Sidorenko, and Gerhard\n  Kramer", "title": "Nested Tailbiting Convolutional Codes for Secrecy, Privacy, and Storage", "comments": "To appear in ACM Workshop on Information Hiding and Multimedia\n  Security 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key agreement problem is considered that has a biometric or physical\nidentifier, a terminal for key enrollment, and a terminal for reconstruction. A\nnested convolutional code design is proposed that performs vector quantization\nduring enrollment and error control during reconstruction. Physical identifiers\nwith small bit error probability illustrate the gains of the design. One\nvariant of the nested convolutional codes improves on the best known key vs.\nstorage rate ratio but it has high complexity. A second variant with lower\ncomplexity performs similar to nested polar codes. The results suggest that the\nchoice of code for key agreement with identifiers depends primarily on the\ncomplexity constraint.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:55:16 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jerkovits", "Thomas", ""], ["G\u00fcnl\u00fc", "Onur", ""], ["Sidorenko", "Vladimir", ""], ["Kramer", "Gerhard", ""]]}, {"id": "2004.13274", "submitter": "Prasanta Bhattacharya", "authors": "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang", "title": "Exploring the contextual factors affecting multimodal emotion\n  recognition in videos", "comments": "Accepted version at IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional expressions form a key part of user behavior on today's digital\nplatforms. While multimodal emotion recognition techniques are gaining research\nattention, there is a lack of deeper understanding on how visual and non-visual\nfeatures can be used to better recognize emotions in certain contexts, but not\nothers. This study analyzes the interplay between the effects of multimodal\nemotion features derived from facial expressions, tone and text in conjunction\nwith two key contextual factors: i) gender of the speaker, and ii) duration of\nthe emotional episode. Using a large public dataset of 2,176 manually annotated\nYouTube videos, we found that while multimodal features consistently\noutperformed bimodal and unimodal features, their performance varied\nsignificantly across different emotions, gender and duration contexts.\nMultimodal features performed particularly better for male speakers in\nrecognizing most emotions. Furthermore, multimodal features performed\nparticularly better for shorter than for longer videos in recognizing neutral\nand happiness, but not sadness and anger. These findings offer new insights\ntowards the development of more context-aware emotion recognition and\nempathetic systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:02:08 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 02:04:23 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 10:28:04 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 01:39:26 GMT"}, {"version": "v5", "created": "Wed, 30 Jun 2021 16:36:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bhattacharya", "Prasanta", ""], ["Gupta", "Raj Kumar", ""], ["Yang", "Yinping", ""]]}, {"id": "2004.13362", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Martin Haug, Paavo Camps, Tobias Umland, Jan-Niklas Voigt-Antons", "title": "Assessing differences in flow state induced by an adaptive music\n  learning software", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology can facilitate self-learning for academic and leisure activities\nsuch as music learning. In general, learning to play an unknown musical song at\nsight on the electric piano or any other instrument can be quite a chore. In a\ntraditional self-learning setting, the musician only gets feedback in terms of\nwhat errors they can hear themselves by comparing what they have played with\nthe score. Research has shown that reaching a flow state creates a more\nenjoyable experience during activities. This work explores whether principles\nfrom flow theory and game design can be applied to make the beginner's musical\nexperience adapted to their need and create higher flow. We created and\nevaluated a tool oriented around these considerations in a study with 21\nparticipants. We found that provided feedback and difficulty scaling can help\nto achieve flow and that the effects get more pronounced the more experience\nwith music participants have. In further research, we want to examine the\ninfluence of our approach to learning sheet music.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 08:33:44 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Haug", "Martin", ""], ["Camps", "Paavo", ""], ["Umland", "Tobias", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.13369", "submitter": "Xuanqin Mou", "authors": "Yang Li and Xuanqin Mou", "title": "SSIM-Based CTU-Level Joint Optimal Bit Allocation and Rate Distortion\n  Optimization", "comments": "An improved version of this manuscript has been accepted by IEEE\n  Transactions on Broadcasting DOI 10.1109/TBC.2021.3068871. The project page\n  is located at http://gr.xjtu.edu.cn/web/xqmou/sosr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural similarity (SSIM)-based distortion $D_\\text{SSIM}$ is more\nconsistent with human perception than the traditional mean squared error\n$D_\\text{MSE}$. To achieve better video quality, many studies on optimal bit\nallocation (OBA) and rate-distortion optimization (RDO) used $D_\\text{SSIM}$ as\nthe distortion metric. However, many of them failed to optimize OBA and RDO\njointly based on SSIM, thus causing a non-optimal R-$D_\\text{SSIM}$\nperformance. This problem is due to the lack of an accurate R-$D_\\text{SSIM}$\nmodel that can be used uniformly in both OBA and RDO. To solve this problem, we\npropose a $D_\\text{SSIM}$-$D_\\text{MSE}$ model first. Based on this model, the\ncomplex R-$D_\\text{SSIM}$ cost in RDO can be calculated as simpler\nR-$D_\\text{MSE}$ cost with a new SSIM-related Lagrange multiplier. This not\nonly reduces the computation burden of SSIM-based RDO, but also enables the\nR-$D_\\text{SSIM}$ model to be uniformly used in OBA and RDO. Moreover, with the\nnew SSIM-related Lagrange multiplier in hand, the joint relationship of\nR-$D_\\text{SSIM}$-$\\lambda_\\text{SSIM}$ (the negative derivative of\nR-$D_\\text{SSIM}$) can be built, based on which the R-$D_\\text{SSIM}$ model\nparameters can be calculated accurately. With accurate and unified\nR-$D_\\text{SSIM}$ model, SSIM-based OBA and SSIM-based RDO are unified together\nin our scheme, called SOSR. Compared with the HEVC reference encoder HM16.20,\nSOSR saves 4%, 10%, and 14% bitrate under the same SSIM in all-intra,\nhierarchical and non-hierarchical low-delay-B configurations, which is superior\nto other state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 08:55:21 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 05:46:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Yang", ""], ["Mou", "Xuanqin", ""]]}, {"id": "2004.14491", "submitter": "Shruti Agarwal", "authors": "Shruti Agarwal (1), Tarek El-Gaaly (2), Hany Farid (1), Ser-Nam Lim\n  (2) ((1) Univeristy of California, Berkeley, Berkeley, CA, USA, (2) Facebook\n  Research, New York, NY, USA)", "title": "Detecting Deep-Fake Videos from Appearance and Behavior", "comments": null, "journal-ref": "IEEE Workshop on Image Forensics and Security, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetically-generated audios and videos -- so-called deep fakes -- continue\nto capture the imagination of the computer-graphics and computer-vision\ncommunities. At the same time, the democratization of access to technology that\ncan create sophisticated manipulated video of anybody saying anything continues\nto be of concern because of its power to disrupt democratic elections, commit\nsmall to large-scale fraud, fuel dis-information campaigns, and create\nnon-consensual pornography. We describe a biometric-based forensic technique\nfor detecting face-swap deep fakes. This technique combines a static biometric\nbased on facial recognition with a temporal, behavioral biometric based on\nfacial expressions and head movements, where the behavioral embedding is\nlearned using a CNN with a metric-learning objective function. We show the\nefficacy of this approach across several large-scale video datasets, as well as\nin-the-wild deep fakes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:38:22 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Agarwal", "Shruti", ""], ["El-Gaaly", "Tarek", ""], ["Farid", "Hany", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2004.14858", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis,\n  Xinchen Du, Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Bj\\\"orn W.\n  Schuller, Iulia Lefter, Erik Cambria, Ioannis Kompatsiaris", "title": "MuSe 2020 -- The First International Multimodal Sentiment Analysis in\n  Real-life Media Challenge and Workshop", "comments": "Baseline Paper MuSe 2020, MuSe Workshop Challenge, ACM Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 is a\nChallenge-based Workshop focusing on the tasks of sentiment recognition, as\nwell as emotion-target engagement and trustworthiness detection by means of\nmore comprehensively integrating the audio-visual and language modalities. The\npurpose of MuSe 2020 is to bring together communities from different\ndisciplines; mainly, the audio-visual emotion recognition community\n(signal-based), and the sentiment analysis community (symbol-based). We present\nthree distinct sub-challenges: MuSe-Wild, which focuses on continuous emotion\n(arousal and valence) prediction; MuSe-Topic, in which participants recognise\ndomain-specific topics as the target of 3-class (low, medium, high) emotions;\nand MuSe-Trust, in which the novel aspect of trustworthiness is to be\npredicted. In this paper, we provide detailed information on MuSe-CaR, the\nfirst of its kind in-the-wild database, which is utilised for the challenge, as\nwell as the state-of-the-art features and modelling approaches applied. For\neach sub-challenge, a competitive baseline for participants is set; namely, on\ntest we report for MuSe-Wild a combined (valence and arousal) CCC of .2568, for\nMuSe-Topic a score (computed as 0.34$\\cdot$ UAR + 0.66$\\cdot$F1) of 76.78 % on\nthe 10-class topic and 40.64 % on the 3-class emotion prediction, and for\nMuSe-Trust a CCC of .4359.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:54:22 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 16:05:49 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 08:37:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Rizos", "Georgios", ""], ["Tzirakis", "Panagiotis", ""], ["Du", "Xinchen", ""], ["Hafner", "Felix", ""], ["Schumann", "Lea", ""], ["Mallol-Ragolta", "Adria", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Lefter", "Iulia", ""], ["Cambria", "Erik", ""], ["Kompatsiaris", "Ioannis", ""]]}]