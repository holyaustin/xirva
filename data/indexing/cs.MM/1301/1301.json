[{"id": "1301.0026", "submitter": "John Scoville", "authors": "John Scoville", "title": "Bounding Lossy Compression using Lossless Codes at Reduced Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An alternative approach to two-part 'critical compression' is presented.\nWhereas previous results were based on summing a lossless code at reduced\nprecision with a lossy-compressed error or noise term, the present approach\nuses a similar lossless code at reduced precision to establish absolute bounds\nwhich constrain an arbitrary lossy data compression algorithm applied to the\noriginal data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 22:42:02 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Scoville", "John", ""]]}, {"id": "1301.0344", "submitter": "Lorenzo Rossi Ph.D", "authors": "Lorenzo Rossi, Jacob Chakareski, Pascal Frossard, Stefania Colonnese", "title": "A Poisson Hidden Markov Model for Multiview Video Traffic", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview video has recently emerged as a means to improve user experience in\nnovel multimedia services. We propose a new stochastic model to characterize\nthe traffic generated by a Multiview Video Coding (MVC) variable bit rate\nsource. To this aim, we resort to a Poisson Hidden Markov Model (P-HMM), in\nwhich the first (hidden) layer represents the evolution of the video activity\nand the second layer represents the frame sizes of the multiple encoded views.\nWe propose a method for estimating the model parameters in long MVC sequences.\nWe then present extensive numerical simulations assessing the model's ability\nto produce traffic with realistic characteristics for a general class of MVC\nsequences. We then extend our framework to network applications where we show\nthat our model is able to accurately describe the sender and receiver buffers\nbehavior in MVC transmission. Finally, we derive a model of user behavior for\ninteractive view selection, which, in conjunction with our traffic model, is\nable to accurately predict actual network load in interactive multiview\nservices.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2013 22:18:06 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Rossi", "Lorenzo", ""], ["Chakareski", "Jacob", ""], ["Frossard", "Pascal", ""], ["Colonnese", "Stefania", ""]]}, {"id": "1301.1894", "submitter": "Trisiladevi C Nagavi", "authors": "Trisiladevi C. Nagavi and Nagappa U. Bhajantri", "title": "An Extensive Analysis of Query by Singing/Humming System Through Query\n  Proportion", "comments": "14 pages,11 figures; The International Journal of Multimedia & Its\n  Applications (IJMA) Vol.4, No.6, December 2012. arXiv admin note: text\n  overlap with arXiv:1003.4083 by other authors", "journal-ref": null, "doi": "10.5121/ijma.2012.4606", "report-no": null, "categories": "cs.MM cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query by Singing/Humming (QBSH) is a Music Information Retrieval (MIR) system\nwith small audio excerpt as query. The rising availability of digital music\nstipulates effective music retrieval methods. Further, MIR systems support\ncontent based searching for music and requires no musical acquaintance. Current\nwork on QBSH focuses mainly on melody features such as pitch, rhythm, note\netc., size of databases, response time, score matching and search algorithms.\nEven though a variety of QBSH techniques are proposed, there is a dearth of\nwork to analyze QBSH through query excerption. Here, we present an analysis\nthat works on QBSH through query excerpt. To substantiate a series of\nexperiments are conducted with the help of Mel-Frequency Cepstral Coefficients\n(MFCC), Linear Predictive Coefficients (LPC) and Linear Predictive Cepstral\nCoefficients (LPCC) to portray the robustness of the knowledge representation.\nProposed experiments attempt to reveal that retrieval performance as well as\nprecision diminishes in the snail phase with the growing database size.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 15:36:40 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Nagavi", "Trisiladevi C.", ""], ["Bhajantri", "Nagappa U.", ""]]}, {"id": "1301.2172", "submitter": "Tarek Zlitni", "authors": "Bassem Bouaziz, Walid Mahdi, Tarek Zlitni and Abdelmajid ben Hamadou", "title": "Content-Based Video Browsing by Text Region Localization and\n  Classification", "comments": "11 pages, 12 figures, International Journal of Video & Image\n  Processing and Network Security IJVIPNS-IJENS Vol:10 No: 01", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of digital video data is increasing over the world. It highlights\nthe need for efficient algorithms that can index, retrieve and browse this data\nby content. This can be achieved by identifying semantic description captured\nautomatically from video structure. Among these descriptions, text within video\nis considered as rich features that enable a good way for video indexing and\nbrowsing. Unlike most video text detection and extraction methods that treat\nvideo sequences as collections of still images, we propose in this paper\nspatiotemporal. video-text localization and identification approach which\nproceeds in two main steps: text region localization and text region\nclassification. In the first step we detect the significant appearance of the\nnew objects in a frame by a split and merge processes applied on binarized edge\nframe pair differences. Detected objects are, a priori, considered as text.\nThey are then filtered according to both local contrast variation and texture\ncriteria in order to get the effective ones. The resulted text regions are\nclassified based on a visual grammar descriptor containing a set of semantic\ntext class regions characterized by visual features. A visual table of content\nis then generated based on extracted text regions occurring within video\nsequence enriched by a semantic identification. The experimentation performed\non a variety of video sequences shows the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:22:40 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Bouaziz", "Bassem", ""], ["Mahdi", "Walid", ""], ["Zlitni", "Tarek", ""], ["Hamadou", "Abdelmajid ben", ""]]}, {"id": "1301.2173", "submitter": "Tarek Zlitni", "authors": "Baseem Bouaziz, Tarek Zlitni and Walid Mahdi", "title": "AViTExt: Automatic Video Text Extraction, A new Approach for video\n  content indexing Application", "comments": "5 pages, 5 figures, 3rd International Conference on Information and\n  Communication Technologies: From Theory to Applications(ICTTA 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spatial temporal video-text detection technique\nwhich proceed in two principal steps:potential text region detection and a\nfiltering process. In the first step we divide dynamically each pair of\nconsecutive video frames into sub block in order to detect change. A\nsignificant difference between homologous blocks implies the appearance of an\nimportant object which may be a text region. The temporal redundancy is then\nused to filter these regions and forms an effective text region. The\nexperimentation driven on a variety of video sequences shows the effectiveness\nof our approach by obtaining a 89,39% as precision rate and 90,19 as recall.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:29:11 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Bouaziz", "Baseem", ""], ["Zlitni", "Tarek", ""], ["Mahdi", "Walid", ""]]}, {"id": "1301.2200", "submitter": "Tarek Zlitni", "authors": "Tarek Zlitni and Walid Mahdi", "title": "A Visual Grammar Approach for TV Program Identification", "comments": "8 pages, 6 figures, (IJCNS) International Journal of Computer and\n  Network Security, Vol. 2, No. 9, September 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic identification of TV programs within TV streams is an important\ntask for archive exploitation. This paper proposes a new spatial-temporal\napproach to identify programs in TV streams in two main steps: First, a\nreference catalogue for video grammars of visual jingles is constructed. We\nexploit visual grammars characterizing instances of the same program type in\norder to identify the various program types in the TV stream. The role of video\ngrammar is to represent the visual invariants for each visual jingle using a\nset of descriptors appropriate for each TV program. Secondly, programs in TV\nstreams are identified by examining the similarity of the video signal to the\nvisual grammars in the catalogue. The main idea of identification process\nconsists in comparing the visual similarity of the video signal signature in TV\nstream to the catalogue elements. After presenting the proposed approach, the\npaper overviews the encouraging experimental results on several streams\nextracted from different channels and composed of several programs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 17:56:02 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Zlitni", "Tarek", ""], ["Mahdi", "Walid", ""]]}, {"id": "1301.3174", "submitter": "Amin Abdel Khalek", "authors": "Amin Abdel Khalek, Constantine Caramanis, Robert W. Heath Jr", "title": "Loss Visibility Optimized Real-time Video Transmission over MIMO Systems", "comments": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structured nature of video data motivates introducing video-aware\ndecisions that make use of this structure for improved video transmission over\nwireless networks. In this paper, we introduce an architecture for real-time\nvideo transmission over multiple-input multiple-output (MIMO) wireless\ncommunication systems using loss visibility side information. We quantify the\nperceptual importance of a packet through the packet loss visibility and use\nthe loss visibility distribution to provide a notion of relative packet\nimportance. To jointly achieve video quality and low latency, we define the\noptimization objective function as the throughput weighted by the loss\nvisibility of each packet, a proxy for the total perceptual value of successful\npackets per unit time. We solve the problem of mapping video packets to MIMO\nsubchannels and adapting per-stream rates to maximize the proposed objective.\nWe show that the solution enables jointly reaping gains in terms of improved\nvideo quality and lower latency. Optimized packet-stream mapping enables\ntransmission of more relevant packets over more reliable streams while unequal\nmodulation opportunistically increases the transmission rate on the stronger\nstreams to enable low latency delivery of high priority packets. We extend the\nsolution to capture codebook-based limited feedback and MIMO mode adaptation.\nResults show that the composite quality and throughput gains are significant\nunder full channel state information as well as limited feedback. Tested on\nH.264-encoded video sequences, for a 4x4 MIMO with 3 spatial streams, the\nproposed architecture achieves 8 dB power reduction for the same video quality\nand supports 2.4x higher throughput due to unequal modulation. Furthermore, the\ngains are achieved at the expense of few bits of cross-layer overhead rather\nthan a complex cross-layer design.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 22:21:36 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 22:19:44 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Khalek", "Amin Abdel", ""], ["Caramanis", "Constantine", ""], ["Heath", "Robert W.", "Jr"]]}, {"id": "1301.3632", "submitter": "Wojciech Mazurczyk", "authors": "Wojciech Mazurczyk, Maciej Karas, Krzysztof Szczypiorski", "title": "SkyDe: a Skype-based Steganographic Method", "comments": "7 pages, 6 figures, 1 table. The paper is submitted to 1st ACM\n  Information Hiding and Multimedia Security Workshop (IH and ACM MMSec\n  conferences merged into a single event), June 17-19, 2013, Montpellier,\n  France", "journal-ref": "International Journal of Computers, Communications & Control\n  (IJCCC), ISSN: 1841- 9836, 8(3), June 2013, pp. 389-400", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SkyDe (Skype Hide), a new steganographic method that\nutilizes Skype encrypted packets with silence to provide the means for\nclandestine communication. It is possible to reuse packets that do not carry\nvoice signals for steganographic purposes because Skype does not use any\nsilence suppression mechanism. The method's proof-of-concept implementation and\nfirst experimental results are presented. They prove that the method is\nfeasible and offers steganographic bandwidth as high as 2.8 kbps.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 09:21:11 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Mazurczyk", "Wojciech", ""], ["Karas", "Maciej", ""], ["Szczypiorski", "Krzysztof", ""]]}, {"id": "1301.4337", "submitter": "Hiren Joshi", "authors": "Mahimn Pandya, Hiren Joshi, Ashish Jani", "title": "A Novel Digital Watermarking Algorithm using Random Matrix Image", "comments": "4 pages, 8 figures", "journal-ref": "International Journal of Computer Applications, Volume 61, Number\n  2, pp. 18-12, 2013", "doi": "10.5120/9900-4481", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The availability of bandwidth for internet access is sufficient enough to\ncommunicate digital assets. These digital assets are subjected to various types\nof threats. [19] As a result of this, protection mechanism required for the\nprotection of digital assets is of priority in research. The threat of current\nfocus is unauthorized copying of digital assets which give boost to piracy.\nThis under the copyright act is illegal and a robust mechanism is required to\ncurb this kind of unauthorized copy. To safeguard the copyright digital assets,\na robust digital watermarking technique is needed. The existing digital\nwatermarking techniques protect digital assets by embedding a digital watermark\ninto a host digital image. This embedding does induce slight distortion in the\nhost image but the distortion is usually too small to be noticed. At the same\ntime the embedded watermark must be robust enough to with stand deliberate\nattacks. There are various techniques of digital watermarking but researchers\nare making constant efforts to increase the robustness of the watermark image.\nThe layered approach of watermarking based on Huffman coding [5] can soon\nincrease the robustness of digital watermark.[11] Ultimately, increasing the\nsecurity of copyright of protection. The proposed work is in similar direction\nwhere in RMI (Random Matrix Image) is used in place of Huffman coding. This\ninnovative algorithm has considerably increased the robustness in digital\nwatermark while also enhancing security of production\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 10:16:21 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 13:24:23 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Pandya", "Mahimn", ""], ["Joshi", "Hiren", ""], ["Jani", "Ashish", ""]]}, {"id": "1301.5793", "submitter": "I\\~naki Ucar", "authors": "I\\~naki Ucar (1), Jorge Navarro-Ortiz (2), Pablo Ameigeiras (2), Juan\n  M. Lopez-Soler (2) ((1) Universidad P\\'ublica de Navarra, (2) Universidad de\n  Granada)", "title": "Video Tester -- A multiple-metric framework for video quality assessment\n  over IP networks", "comments": "5 pages, 5 figures. For the Google Code project, see\n  http://video-tester.googlecode.com/", "journal-ref": "IEEE international Symposium on Broadband Multimedia Systems and\n  Broadcasting, Seoul, 2012, pp. 1-5", "doi": "10.1109/BMSB.2012.6264243", "report-no": null, "categories": "cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an extensible and reusable framework which addresses the\nproblem of video quality assessment over IP networks. The proposed tool\n(referred to as Video-Tester) supports raw uncompressed video encoding and\ndecoding. It also includes different video over IP transmission methods (i.e.:\nRTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it\nis furnished with a rich set of offline analysis capabilities. Video-Tester\nanalysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,\npacket inter-arrival time, jitter and loss rate, as well as GOP size and\nI-frame loss rate). Our design facilitates the integration of virtually any\nexisting video quality metric thanks to the adopted Python-based modular\napproach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video\nquality metric, DIV and PSNR-based MOS estimations. In order to promote its use\nand extension, Video-Tester is open and publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 14:31:10 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Ucar", "I\u00f1aki", ""], ["Navarro-Ortiz", "Jorge", ""], ["Ameigeiras", "Pablo", ""], ["Lopez-Soler", "Juan M.", ""]]}]