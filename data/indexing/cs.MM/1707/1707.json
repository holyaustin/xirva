[{"id": "1707.01221", "submitter": "Yaqi Liu", "authors": "Yaqi Liu, Qingxiao Guan and Xianfeng Zhao", "title": "Copy-move Forgery Detection based on Convolutional Kernel Network", "comments": "26 pages, 8 figures, submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a copy-move forgery detection method based on Convolutional\nKernel Network is proposed. Different from methods based on conventional\nhand-crafted features, Convolutional Kernel Network is a kind of data-driven\nlocal descriptor with the deep convolutional structure. Thanks to the\ndevelopment of deep learning theories and widely available datasets, the\ndata-driven methods can achieve competitive performance on different conditions\nfor its excellent discriminative capability. Besides, our Convolutional Kernel\nNetwork is reformulated as a series of matrix computations and convolutional\noperations which are easy to parallelize and accelerate by GPU, leading to high\nefficiency. Then, appropriate preprocessing and postprocessing for\nConvolutional Kernel Network are adopted to achieve copy-move forgery\ndetection. Particularly, a segmentation-based keypoints distribution strategy\nis proposed and a GPU-based adaptive oversegmentation method is adopted.\nNumerous experiments are conducted to demonstrate the effectiveness and\nrobustness of the GPU version of Convolutional Kernel Network, and the\nstate-of-the-art performance of the proposed copy-move forgery detection method\nbased on Convolutional Kernel Network.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:54:18 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Liu", "Yaqi", ""], ["Guan", "Qingxiao", ""], ["Zhao", "Xianfeng", ""]]}, {"id": "1707.01340", "submitter": "Luca Rossetto M.Sc.", "authors": "Luca Rossetto and Heiko Schuldt", "title": "Web Video in Numbers - An Analysis of Web-Video Metadata", "comments": "Dataset available from http://download-dbis.dmi.unibas.ch/WWIN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web video is often used as a source of data in various fields of study. While\nspecialized subsets of web video, mainly earmarked for dedicated purposes, are\noften analyzed in detail, there is little information available about the\nproperties of web video as a whole. In this paper we present insights gained\nfrom the analysis of the metadata associated with more than 120 million videos\nharvested from two popular web video platforms, vimeo and YouTube, in 2016 and\ncompare their properties with the ones found in commonly used video\ncollections. This comparison has revealed that existing collections do not (or\nno longer) properly reflect the properties of web video \"in the wild\".\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:02:42 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Rossetto", "Luca", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1707.01513", "submitter": "Valery Gorbachev", "authors": "V.N. Gorbachev, L.A. Denisov, E.M. Kaynarova, I.K. Metelev", "title": "On the steganographic image based approach to PDF files protection", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital images can be copied without authorization and have to be protected.\nTwo schemes for watermarking images in PDF document were considered. Both\nschemes include a converter to extract images from PDF pages and return the\nprotected images back. Frequency and spatial domain embedding were used for\nhiding a message presented by a binary pattern. We considered visible and\ninvisible watermarking and found that spatial domain LSB technique can be more\npreferable than frequency embedding using DWT.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 18:02:29 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Gorbachev", "V. N.", ""], ["Denisov", "L. A.", ""], ["Kaynarova", "E. M.", ""], ["Metelev", "I. K.", ""]]}, {"id": "1707.01606", "submitter": "Ekraam Sabir", "authors": "Ayush Jaiswal, Ekraam Sabir, Wael AbdAlmageed, Premkumar Natarajan", "title": "Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images\n  And Text", "comments": "*Ayush Jaiswal and Ekraam Sabir contributed equally to the work in\n  this paper", "journal-ref": "In Proceedings of the 2017 ACM on Multimedia Conference, pp.\n  1465-1471. ACM, 2017", "doi": "10.1145/3123266.3123385", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world multimedia data is often composed of multiple modalities such as\nan image or a video with associated text (e.g. captions, user comments, etc.)\nand metadata. Such multimodal data packages are prone to manipulations, where a\nsubset of these modalities can be altered to misrepresent or repurpose data\npackages, with possible malicious intent. It is, therefore, important to\ndevelop methods to assess or verify the integrity of these multimedia packages.\nUsing computer vision and natural language processing methods to directly\ncompare the image (or video) and the associated caption to verify the integrity\nof a media package is only possible for a limited set of objects and scenes. In\nthis paper, we present a novel deep learning-based approach for assessing the\nsemantic integrity of multimedia packages containing images and captions, using\na reference set of multimedia packages. We construct a joint embedding of\nimages and captions with deep multimodal representation learning on the\nreference dataset in a framework that also provides image-caption consistency\nscores (ICCSs). The integrity of query media packages is assessed as the\ninlierness of the query ICCSs with respect to the reference dataset. We present\nthe MultimodAl Information Manipulation dataset (MAIM), a new dataset of media\npackages from Flickr, which we make available to the research community. We use\nboth the newly created dataset as well as Flickr30K and MS COCO datasets to\nquantitatively evaluate our proposed approach. The reference dataset does not\ncontain unmanipulated versions of tampered query packages. Our method is able\nto achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO,\nrespectively, for detecting semantically incoherent media packages.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 01:25:17 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 00:47:21 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 18:02:08 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2018 00:34:27 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Sabir", "Ekraam", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1707.01613", "submitter": "Haichao Shi", "authors": "Haichao Shi, Jing Dong, Wei Wang, Yinlong Qian, Xiaoyu Zhang", "title": "SSGAN: Secure Steganography Based on Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel strategy of Secure Steganograpy based on Generative\nAdversarial Networks is proposed to generate suitable and secure covers for\nsteganography. The proposed architecture has one generative network, and two\ndiscriminative networks. The generative network mainly evaluates the visual\nquality of the generated images for steganography, and the discriminative\nnetworks are utilized to assess their suitableness for information hiding.\nDifferent from the existing work which adopts Deep Convolutional Generative\nAdversarial Networks, we utilize another form of generative adversarial\nnetworks. By using this new form of generative adversarial networks,\nsignificant improvements are made on the convergence speed, the training\nstability and the image quality. Furthermore, a sophisticated steganalysis\nnetwork is reconstructed for the discriminative network, and the network can\nbetter evaluate the performance of the generated images. Numerous experiments\nare conducted on the publicly available datasets to demonstrate the\neffectiveness and robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 02:05:51 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 02:54:39 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 04:26:48 GMT"}, {"version": "v4", "created": "Sat, 24 Nov 2018 02:32:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Shi", "Haichao", ""], ["Dong", "Jing", ""], ["Wang", "Wei", ""], ["Qian", "Yinlong", ""], ["Zhang", "Xiaoyu", ""]]}, {"id": "1707.01742", "submitter": "Jerrin Thomas Panachakel", "authors": "Jerrin Thomas Panachakel and Anurenjan P.R", "title": "High Resilience Diverse Domain Multilevel Audio Watermarking with\n  Adaptive Threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel diverse domain (DCT-SVD & DWT-SVD) watermarking scheme is proposed in\nthis paper. Here, the watermark is embedded simultaneously onto the two\ndomains. It is shown that an audio signal watermarked using this scheme has\nbetter subjective and objective quality when compared with other watermarking\nschemes. Also proposed are two novel watermark detection algorithms viz., AOT\n(Adaptively Optimised Threshold) and AOTx (AOT eXtended). The fundamental idea\nbehind both is finding an optimum threshold for detecting a known character\nembedded along with the actual watermarks in a known location, with the\nconstraint that the Bit Error Rate (BER) is minimum. This optimum threshold is\nused for detecting the other characters in the watermarks. This approach is\nshown to make the watermarking scheme less susceptible to various signal\nprocessing attacks, thus making the watermarks more robust.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 09:21:37 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Panachakel", "Jerrin Thomas", ""], ["R", "Anurenjan P.", ""]]}, {"id": "1707.02530", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Deep CNN Framework for Audio Event Recognition using Weakly Labeled Web\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of audio event recognition models requires labeled training\ndata, which are generally hard to obtain. One promising source of recordings of\naudio events is the large amount of multimedia data on the web. In particular,\nif the audio content analysis must itself be performed on web audio, it is\nimportant to train the recognizers themselves from such data. Training from\nthese web data, however, poses several challenges, the most important being the\navailability of labels : labels, if any, that may be obtained for the data are\ngenerally {\\em weak}, and not of the kind conventionally required for training\ndetectors or classifiers. We propose that learning algorithms that can exploit\nweak labels offer an effective method to learn from web data. We then propose a\nrobust and efficient deep convolutional neural network (CNN) based framework to\nlearn audio event recognizers from weakly labeled data. The proposed method can\ntrain from and analyze recordings of variable length in an efficient manner and\noutperforms a network trained with {\\em strongly labeled} web data by a\nconsiderable margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 06:16:23 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 07:33:03 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1707.02546", "submitter": "Angelos Valsamis", "authors": "Fotis Aisopos, Angelos Valsamis, Alexandros Psychas, Andreas\n  Menychtas, Theodora Varvarigou", "title": "Efficient Context Management and Personalized User Recommendations in a\n  Smart Social TV environment", "comments": "In GECON2016, 13th International Conference on Economics of Grids,\n  Clouds, Systems, and Services, September 20-22, 2016, Harokopio University,\n  Athens, Greece", "journal-ref": null, "doi": "10.1007/978-3-319-61920-0_8", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of Smart TV and related interconnected devices, second\nscreen solutions have rapidly appeared to provide more content for end-users\nand enrich their TV experience. Given the various data and sources involved -\nvideos, actors, social media and online databases- the aforementioned market\nposes great challenges concerning user context management and sophisticated\nrecommendations that can be addressed to the end-users. This paper presents an\ninnovative Context Management model and a related first and second screen\nrecommendation service, based on a user-item graph analysis as well as\ncollaborative filtering techniques in the context of a Dynamic Social & Media\nContent Syndication (SAM) platform. The model evaluation provided is based on\ndatasets collected online, presenting a comparative analysis concerning\nefficiency and effectiveness of the current approach, and illustrating its\nadded value.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 09:19:21 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Aisopos", "Fotis", ""], ["Valsamis", "Angelos", ""], ["Psychas", "Alexandros", ""], ["Menychtas", "Andreas", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1707.02581", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Albert Jimenez, Jose M. Alvarez and Xavier Giro-i-Nieto", "title": "Class-Weighted Convolutional Features for Visual Instance Search", "comments": "To appear in the British Machine Vision Conference (BMVC), September\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval in realistic scenarios targets large dynamic datasets of\nunlabeled images. In these cases, training or fine-tuning a model every time\nnew images are added to the database is neither efficient nor scalable.\nConvolutional neural networks trained for image classification over large\ndatasets have been proven effective feature extractors for image retrieval. The\nmost successful approaches are based on encoding the activations of\nconvolutional layers, as they convey the image spatial information. In this\npaper, we go beyond this spatial information and propose a local-aware encoding\nof convolutional features based on semantic information predicted in the target\nimage. To this end, we obtain the most discriminative regions of an image using\nClass Activation Maps (CAMs). CAMs are based on the knowledge contained in the\nnetwork and therefore, our approach, has the additional advantage of not\nrequiring external information. In addition, we use CAMs to generate object\nproposals during an unsupervised re-ranking stage after a first fast search.\nOur experiments on two public available datasets for instance retrieval,\nOxford5k and Paris6k, demonstrate the competitiveness of our approach\noutperforming the current state-of-the-art when using off-the-shelf models\ntrained on ImageNet. The source code and model used in this paper are publicly\navailable at http://imatge-upc.github.io/retrieval-2017-cam/.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 13:51:45 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Jimenez", "Albert", ""], ["Alvarez", "Jose M.", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1707.02709", "submitter": "Christos Bampis", "authors": "Christos G. Bampis and Alan C. Bovik", "title": "An Augmented Autoregressive Approach to HTTP Video Stream Quality\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HTTP-based video streaming technologies allow for flexible rate selection\nstrategies that account for time-varying network conditions. Such rate changes\nmay adversely affect the user's Quality of Experience; hence online prediction\nof the time varying subjective quality can lead to perceptually optimised\nbitrate allocation policies. Recent studies have proposed to use dynamic\nnetwork approaches for continuous-time prediction; yet they do not consider\nmultiple video quality models as inputs nor consider forecasting ensembles.\nHere we address the problem of predicting continuous-time subjective quality\nusing multiple inputs fed to a non-linear autoregressive network. By\nconsidering multiple network configurations and by applying simple averaging\nforecasting techniques, we are able to considerably improve prediction\nperformance and decrease forecasting errors.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 06:30:59 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bampis", "Christos G.", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1707.03123", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Marc Assens, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor", "title": "SaltiNet: Scan-path Prediction on 360 Degree Images using Saliency\n  Volumes", "comments": "Winner of the Best Scan-path Award at the Salient360!: Visual\n  attention modeling for 360 degrees Images Grand Challenge of ICME 2017.\n  Presented at the ICCV 2017 Workshop on Egocentric Perception, Interaction and\n  Computing (EPIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SaltiNet, a deep neural network for scanpath prediction trained\non 360-degree images. The model is based on a temporal-aware novel\nrepresentation of saliency information named the saliency volume. The first\npart of the network consists of a model trained to generate saliency volumes,\nwhose parameters are fit by back-propagation computed from a binary cross\nentropy (BCE) loss over downsampled versions of the saliency volumes. Sampling\nstrategies over these volumes are used to generate scanpaths over the\n360-degree images. Our experiments show the advantages of using saliency\nvolumes, and how they can be used for related tasks. Our source code and\ntrained models available at\nhttps://github.com/massens/saliency-360salient-2017.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 04:04:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 06:07:57 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 14:10:40 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 10:11:25 GMT"}, {"version": "v5", "created": "Thu, 17 Aug 2017 10:48:16 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Assens", "Marc", ""], ["McGuinness", "Kevin", ""], ["Giro-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1707.03548", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Yong Xu, Ling Shao, Jian Yang", "title": "Discriminative Block-Diagonal Representation Learning for Image\n  Recognition", "comments": "Accepted by TNNLS, and the matlab codes are available at\n  https://sites.google.com/site/darrenzz219/Home/publication", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2712801", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing block-diagonal representation researches mainly focuses on casting\nblock-diagonal regularization on training data, while only little attention is\ndedicated to concurrently learning both block-diagonal representations of\ntraining and test data. In this paper, we propose a discriminative\nblock-diagonal low-rank representation (BDLRR) method for recognition. In\nparticular, the elaborate BDLRR is formulated as a joint optimization problem\nof shrinking the unfavorable representation from off-block-diagonal elements\nand strengthening the compact block-diagonal representation under the\nsemi-supervised framework of low-rank representation. To this end, we first\nimpose penalty constraints on the negative representation to eliminate the\ncorrelation between different classes such that the incoherence criterion of\nthe extra-class representation is boosted. Moreover, a constructed subspace\nmodel is developed to enhance the self-expressive power of training samples and\nfurther build the representation bridge between the training and test samples,\nsuch that the coherence of the learned intra-class representation is\nconsistently heightened. Finally, the resulting optimization problem is solved\nelegantly by employing an alternative optimization strategy, and a simple\nrecognition algorithm on the learned representation is utilized for final\nprediction. Extensive experimental results demonstrate that the proposed method\nachieves superb recognition results on four face image datasets, three\ncharacter datasets, and the fifteen scene multi-categories dataset. It not only\nshows superior potential on image recognition but also outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 05:33:57 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Shao", "Ling", ""], ["Yang", "Jian", ""]]}, {"id": "1707.03981", "submitter": "Gautam Malu", "authors": "Gautam Malu, Raju S. Bapi, Bipin Indurkhya", "title": "Learning Photography Aesthetics with Deep CNNs", "comments": "Accepted in The 28th Modern Artificial Intelligence and Cognitive\n  Science Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic photo aesthetic assessment is a challenging artificial intelligence\ntask. Existing computational approaches have focused on modeling a single\naesthetic score or a class (good or bad), however these do not provide any\ndetails on why the photograph is good or bad, or which attributes contribute to\nthe quality of the photograph. To obtain both accuracy and human interpretation\nof the score, we advocate learning the aesthetic attributes along with the\nprediction of the overall score. For this purpose, we propose a novel multitask\ndeep convolution neural network, which jointly learns eight aesthetic\nattributes along with the overall aesthetic score. We report near human\nperformance in the prediction of the overall aesthetic score. To understand the\ninternal representation of these attributes in the learned model, we also\ndevelop the visualization technique using back propagation of gradients. These\nvisualizations highlight the important image regions for the corresponding\nattributes, thus providing insights about model's representation of these\nattributes. We showcase the diversity and complexity associated with different\nattributes through a qualitative analysis of the activation maps.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 05:16:03 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Malu", "Gautam", ""], ["Bapi", "Raju S.", ""], ["Indurkhya", "Bipin", ""]]}, {"id": "1707.04092", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Xunyu Lin, Victor Campos, Xavier Giro-i-Nieto, Jordi Torres and\n  Cristian Canton Ferrer", "title": "Disentangling Motion, Foreground and Background Features in Videos", "comments": "Poster presented at the CVPR 2017 Workshop Brave New Ideas for Motion\n  Representations in Videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an unsupervised framework to extract semantically rich\nfeatures for video representation. Inspired by how the human visual system\ngroups objects based on motion cues, we propose a deep convolutional neural\nnetwork that disentangles motion, foreground and background information. The\nproposed architecture consists of a 3D convolutional feature encoder for blocks\nof 16 frames, which is trained for reconstruction tasks over the first and last\nframes of the sequence. A preliminary supervised experiment was conducted to\nverify the feasibility of proposed method by training the model with a fraction\nof videos from the UCF-101 dataset taking as ground truth the bounding boxes\naround the activity regions. Qualitative results indicate that the network can\nsuccessfully segment foreground and background in videos as well as update the\nforeground appearance based on disentangled motion features. The benefits of\nthese learned features are shown in a discriminative classification task, where\ninitializing the network with the proposed pretraining method outperforms both\nrandom initialization and autoencoder pretraining. Our model and source code\nare publicly available at https://imatge-upc.github.io/unsupervised-2017-cvprw/ .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 12:40:28 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 13:50:01 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lin", "Xunyu", ""], ["Campos", "Victor", ""], ["Giro-i-Nieto", "Xavier", ""], ["Torres", "Jordi", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "1707.04918", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini, Yu Jiang, Richard R. Berlin, Lui Sha, Houbing Song", "title": "Towards Physiology-Aware DASH: Bandwidth-Compliant Prioritized Clinical\n  Multimedia Communication in Ambulances", "comments": "15 pages, IEEE Transactions on Multimedia (TMM), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate objective of medical cyber-physical systems is to enhance the\nsafety and effectiveness of patient care. To ensure safe and effective care\nduring emergency patient transfer from rural areas to center tertiary\nhospitals, reliable and real-time communication is essential. Unfortunately,\nreal-time monitoring of patients involves transmission of various clinical\nmultimedia data including videos, medical images, and vital signs, which\nrequires use of mobile network with high-fidelity communication bandwidth.\nHowever, the wireless networks along the roads in rural areas range from 4G to\n2G to low speed satellite links, which poses a significant challenge to\ntransmit critical patient information.\n  In this paper, we present a bandwidth-compliant criticality-aware system for\ntransmission of massive clinical multimedia data adaptive to varying bandwidths\nduring patient transport. Model-based clinical automata are used to determine\nthe criticality of clinical multimedia data. We borrow concepts from DASH, and\npropose physiology-aware adaptation techniques to transmit more critical\nclinical data with higher fidelity in response to changes in disease, clinical\nstates, and bandwidth condition. In collaboration with Carle's ambulance\nservice center, we develop a bandwidth profiler, and use it as proof of concept\nto support our experiments. Our preliminary evaluation results show that our\nsolutions ensure that most critical patient's clinical data are communicated\nwith higher fidelity.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 17:28:13 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Jiang", "Yu", ""], ["Berlin", "Richard R.", ""], ["Sha", "Lui", ""], ["Song", "Houbing", ""]]}, {"id": "1707.05726", "submitter": "Yuanfang Guo", "authors": "Yuanfang Guo, Oscar C. Au, Rui Wang, Lu Fang, Xiaochun Cao", "title": "Halftone Image Watermarking by Content Aware Double-sided Embedding\n  Error Diffusion", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2815181", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we carry out a performance analysis from a probabilistic\nperspective to introduce the EDHVW methods' expected performances and\nlimitations. Then, we propose a new general error diffusion based halftone\nvisual watermarking (EDHVW) method, Content aware Double-sided Embedding Error\nDiffusion (CaDEED), via considering the expected watermark decoding performance\nwith specific content of the cover images and watermark, different noise\ntolerance abilities of various cover image content and the different importance\nlevels of every pixel (when being perceived) in the secret pattern (watermark).\nTo demonstrate the effectiveness of CaDEED, we propose CaDEED with expectation\nconstraint (CaDEED-EC) and CaDEED-NVF&IF (CaDEED-N&I). Specifically, we build\nCaDEED-EC by only considering the expected performances of specific cover\nimages and watermark. By adopting the noise visibility function (NVF) and\nproposing the importance factor (IF) to assign weights to every embedding\nlocation and watermark pixel, respectively, we build the specific method\nCaDEED-N&I. In the experiments, we select the optimal parameters for NVF and IF\nvia extensive experiments. In both the numerical and visual comparisons, the\nexperimental results demonstrate the superiority of our proposed work.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:14:53 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Guo", "Yuanfang", ""], ["Au", "Oscar C.", ""], ["Wang", "Rui", ""], ["Fang", "Lu", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1707.05859", "submitter": "Adam Starr", "authors": "Chiara Zizza, Adam Starr, Devin Hudson, Sai Shreya Nuguri, Prasad\n  Calyam, Zhihai He", "title": "Towards a Social Virtual Reality Learning Environment in High Fidelity", "comments": "4 pages, work-in-progress paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Learning Environments (VLEs) are spaces designed to educate students\nremotely via online platforms. Although traditional VLEs such as iSocial have\nshown promise in educating students, they offer limited immersion that\ndiminishes learning effectiveness. This paper outlines a virtual reality\nlearning environment (VRLE) over a high-speed network, which promotes\neducational effectiveness and efficiency via our creation of flexible content\nand infrastructure which meet established VLE standards with improved\nimmersion. This paper further describes our implementation of multiple learning\nmodules developed in High Fidelity, a \"social VR\" platform. Our experiment\nresults show that the VR mode of content delivery better stimulates the\ngeneralization of lessons to the real world than non-VR lessons and provides\nimproved immersion when compared to an equivalent desktop version.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:12:37 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 21:10:04 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zizza", "Chiara", ""], ["Starr", "Adam", ""], ["Hudson", "Devin", ""], ["Nuguri", "Sai Shreya", ""], ["Calyam", "Prasad", ""], ["He", "Zhihai", ""]]}, {"id": "1707.06163", "submitter": "Georgi Dzhambazov", "authors": "Georgi Dzhambazov, Andre Holzapfel, Ajay Srinivasamurthy, Xavier Serra", "title": "Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio", "comments": "International Society for Music Information Retrieval Conferece\n  (ISMIR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of this study is the automatic detection of onsets of the singing\nvoice in polyphonic audio recordings. Starting with a hypothesis that the\nknowledge of the current position in a metrical cycle (i.e. metrical accent)\ncan improve the accuracy of vocal note onset detection, we propose a novel\nprobabilistic model to jointly track beats and vocal note onsets. The proposed\nmodel extends a state of the art model for beat and meter tracking, in which\na-priori probability of a note at a specific metrical accent interacts with the\nprobability of observing a vocal note onset. We carry out an evaluation on a\nvaried collection of multi-instrument datasets from two music traditions\n(English popular music and Turkish makam) with different types of metrical\ncycles and singing styles. Results confirm that the proposed model reasonably\nimproves vocal note onset detection accuracy compared to a baseline model that\ndoes not take metrical position into account.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:39:09 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Dzhambazov", "Georgi", ""], ["Holzapfel", "Andre", ""], ["Srinivasamurthy", "Ajay", ""], ["Serra", "Xavier", ""]]}, {"id": "1707.06316", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "DenseNet for Dense Flow", "comments": "Accepted at ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches for estimating optical flow have achieved rapid progress\nin the last decade. However, most of them are too slow to be applied in\nreal-time video analysis. Due to the great success of deep learning, recent\nwork has focused on using CNNs to solve such dense prediction problems. In this\npaper, we investigate a new deep architecture, Densely Connected Convolutional\nNetworks (DenseNet), to learn optical flow. This specific architecture is ideal\nfor the problem at hand as it provides shortcut connections throughout the\nnetwork, which leads to implicit deep supervision. We extend current DenseNet\nto a fully convolutional network to learn motion estimation in an unsupervised\nmanner. Evaluation results on three standard benchmarks demonstrate that\nDenseNet is a better fit than other widely adopted CNN architectures for\noptical flow estimation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 22:37:46 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1707.06557", "submitter": "Konstantinos Amplianitis", "authors": "Dominik Rue{\\ss}, Konstantinos Amplianitis, Niklas Deckers, Michele\n  Adduci, Kristian Manthey, Ralf Reulke", "title": "leave a trace - A People Tracking System Meets Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video surveillance always had a negative connotation, among others because of\nthe loss of privacy and because it may not automatically increase public\nsafety. If it was able to detect atypical (i.e. dangerous) situations in real\ntime, autonomously and anonymously, this could change. A prerequisite for this\nis a reliable automatic detection of possibly dangerous situations from video\ndata. This is done classically by object extraction and tracking. From the\nderived trajectories, we then want to determine dangerous situations by\ndetecting atypical trajectories. However, due to ethical considerations it is\nbetter to develop such a system on data without people being threatened or even\nharmed, plus with having them know that there is such a tracking system\ninstalled. Another important point is that these situations do not occur very\noften in real, public CCTV areas and may be captured properly even less. In the\nartistic project leave a trace the tracked objects, people in an atrium of a\ninstitutional building, become actor and thus part of the installation.\nVisualisation in real-time allows interaction by these actors, which in turn\ncreates many atypical interaction situations on which we can develop our\nsituation detection. The data set has evolved over three years and hence, is\nhuge. In this article we describe the tracking system and several approaches\nfor the detection of atypical trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:03:11 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Rue\u00df", "Dominik", ""], ["Amplianitis", "Konstantinos", ""], ["Deckers", "Niklas", ""], ["Adduci", "Michele", ""], ["Manthey", "Kristian", ""], ["Reulke", "Ralf", ""]]}, {"id": "1707.06830", "submitter": "Rahul Sharma", "authors": "Rahul Sharma, Tanaya Guha and Gaurav Sharma", "title": "Multichannel Attention Network for Analyzing Visual Behavior in Public\n  Speaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public speaking is an important aspect of human communication and\ninteraction. The majority of computational work on public speaking concentrates\non analyzing the spoken content, and the verbal behavior of the speakers. While\nthe success of public speaking largely depends on the content of the talk, and\nthe verbal behavior, non-verbal (visual) cues, such as gestures and physical\nappearance also play a significant role. This paper investigates the importance\nof visual cues by estimating their contribution towards predicting the\npopularity of a public lecture. For this purpose, we constructed a large\ndatabase of more than $1800$ TED talk videos. As a measure of popularity of the\nTED talks, we leverage the corresponding (online) viewers' ratings from\nYouTube. Visual cues related to facial and physical appearance, facial\nexpressions, and pose variations are extracted from the video frames using\nconvolutional neural network (CNN) models. Thereafter, an attention-based long\nshort-term memory (LSTM) network is proposed to predict the video popularity\nfrom the sequence of visual features. The proposed network achieves\nstate-of-the-art prediction accuracy indicating that visual cues alone contain\nhighly predictive information about the popularity of a talk. Furthermore, our\nnetwork learns a human-like attention mechanism, which is particularly useful\nfor interpretability, i.e. how attention varies with time, and across different\nvisual cues by indicating their relative importance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 10:37:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Sharma", "Rahul", ""], ["Guha", "Tanaya", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1707.07075", "submitter": "Michele Merler", "authors": "Michele Merler and Dhiraj Joshi and Quoc-Bao Nguyen and Stephen Hammer\n  and John Kent and John R. Smith and Rogerio S. Feris", "title": "Automatic Curation of Golf Highlights using Multimodal Excitement\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production of sports highlight packages summarizing a game's most\nexciting moments is an essential task for broadcast media. Yet, it requires\nlabor-intensive video editing. We propose a novel approach for auto-curating\nsports highlights, and use it to create a real-world system for the editorial\naid of golf highlight reels. Our method fuses information from the players'\nreactions (action recognition such as high-fives and fist pumps), spectators\n(crowd cheering), and commentator (tone of the voice and word analysis) to\ndetermine the most interesting moments of a game. We accurately identify the\nstart and end frames of key shot highlights with additional metadata, such as\nthe player's name and the hole number, allowing personalized content\nsummarization and retrieval. In addition, we introduce new techniques for\nlearning our classifiers with reduced manual training data annotation by\nexploiting the correlation of different modalities. Our work has been\ndemonstrated at a major golf tournament, successfully extracting highlights\nfrom live video streams over four consecutive days.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 00:06:50 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Merler", "Michele", ""], ["Joshi", "Dhiraj", ""], ["Nguyen", "Quoc-Bao", ""], ["Hammer", "Stephen", ""], ["Kent", "John", ""], ["Smith", "John R.", ""], ["Feris", "Rogerio S.", ""]]}, {"id": "1707.07546", "submitter": "Attilio Fiandrotti", "authors": "Attilio Fiandrotti, Rossano Gaeta, Marco Grangetto", "title": "Simple Countermeasures to Mitigate the Effect of Pollution Attack in\n  Network Coding Based Peer-to-Peer Live Streaming", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, Volume 17, Issue 4, April 2015,\n  Pages 562 - 573", "doi": "10.1109/TMM.2015.2402516", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network coding based peer-to-peer streaming represents an effective solution\nto aggregate user capacities and to increase system throughput in live\nmultimedia streaming. Nonetheless, such systems are vulnerable to pollution\nattacks where a handful of malicious peers can disrupt the communication by\ntransmitting just a few bogus packets which are then recombined and relayed by\nunaware honest nodes, further spreading the pollution over the network. Whereas\nprevious research focused on malicious nodes identification schemes and\npollution-resilient coding, in this paper we show pollution countermeasures\nwhich make a standard network coding scheme resilient to pollution attacks.\nThanks to a simple yet effective analytical model of a reference node\ncollecting packets by malicious and honest neighbors, we demonstrate that i)\npackets received earlier are less likely to be polluted and ii) short\ngenerations increase the likelihood to recover a clean generation. Therefore,\nwe propose a recombination scheme where nodes draw packets to be recombined\naccording to their age in the input queue, paired with a decoding scheme able\nto detect the reception of polluted packets early in the decoding process and\nshort generations. The effectiveness of our approach is experimentally\nevaluated in a real system we developed and deployed on hundreds to thousands\npeers. Experimental evidence shows that, thanks to our simple countermeasures,\nthe effect of a pollution attack is almost canceled and the video quality\nexperienced by the peers is comparable to pre-attack levels.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 13:29:17 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Fiandrotti", "Attilio", ""], ["Gaeta", "Rossano", ""], ["Grangetto", "Marco", ""]]}, {"id": "1707.07795", "submitter": "Haodong Li", "authors": "Haodong Li, Weiqi Luo, Quanquan Rao, Jiwu Huang", "title": "Anti-Forensics of Camera Identification and the Triangle Test by\n  Improved Fingerprint-Copy Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fingerprint-copy attack aims to confuse camera identification based on\nsensor pattern noise. However, the triangle test shows that the forged images\nundergone fingerprint-copy attack would share a non-PRNU (Photo-response\nnonuniformity) component with every stolen image, and thus can detect\nfingerprint-copy attack. In this paper, we propose an improved fingerprint-copy\nattack scheme. Our main idea is to superimpose the estimated fingerprint into\nthe target image dispersedly, via employing a block-wise method and using the\nstolen images randomly and partly. We also develop a practical method to\ndetermine the strength of the superimposed fingerprint based on objective image\nquality. In such a way, the impact of non-PRNU component on the triangle test\nis reduced, and our improved fingerprint-copy attack is difficultly detected.\nThe experiments evaluated on 2,900 images from 4 cameras show that our scheme\ncan effectively fool camera identification, and significantly degrade the\nperformance of the triangle test simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 02:39:50 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Li", "Haodong", ""], ["Luo", "Weiqi", ""], ["Rao", "Quanquan", ""], ["Huang", "Jiwu", ""]]}, {"id": "1707.08061", "submitter": "Yuansong Qiao", "authors": "Zhao Liu, Niall Murray, Brian Lee, Enda Fallon and Yuansong Qiao", "title": "MVP2P: Layer-Dependency-Aware Live MVC Video Streaming over Peer-to-Peer\n  Networks", "comments": null, "journal-ref": "Signal Processing: Image Communication 65 (2018) 173-186", "doi": "10.1016/j.image.2018.04.004", "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview video supports observing a scene from different viewpoints. The\nJoint Video Team (JVT) developed H.264/MVC to enhance the compression\nefficiency for multiview video, however, MVC encoded multiview video (MVC\nvideo) still requires high bitrates for transmission. This paper investigates\nlive MVC video streaming over Peer-to-Peer (P2P) networks. The goal is to\nminimize the server bandwidth costs whist ensuring high streaming quality to\npeers. MVC employs intra-view and inter-view prediction structures, which leads\nto a complicated layer dependency relationship. As the peers' outbound\nbandwidth is shared while supplying all the MVC video layers, the bandwidth\nallocation to one MVC layer affects the available outbound bandwidth of the\nother layers. To optimise the utilisation of the peers' outbound bandwidth for\nproviding video layers, a maximum flow based model is proposed which considers\nthe MVC video layer dependency and the layer supplying relationship between\npeers. Based on the model, a layer dependency aware live MVC video streaming\nmethod over a BitTorrent-like P2P network is proposed, named MVP2P. The key\ncomponents of MVP2P include a chunk scheduling strategy and a peer selection\nstrategy for receiving peers, and a bandwidth scheduling algorithm for\nsupplying peers. To evaluate the efficiency of the proposed solution, MVP2P is\ncompared with existing methods considering the constraints of peer bandwidth,\npeer numbers, view switching rates, and peer churns. The test results show that\nMVP2P significantly outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:03:55 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 23:46:11 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Liu", "Zhao", ""], ["Murray", "Niall", ""], ["Lee", "Brian", ""], ["Fallon", "Enda", ""], ["Qiao", "Yuansong", ""]]}, {"id": "1707.08559", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Joon Lee, Mohit Bansal, Alexander C. Berg", "title": "Video Highlight Prediction Using Audience Chat Reactions", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:44:38 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Lee", "Joon", ""], ["Bansal", "Mohit", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1707.09538", "submitter": "Soujanya Poria", "authors": "Erik Cambria, Devamanyu Hazarika, Soujanya Poria, Amir Hussain, R.B.V.\n  Subramaanyam", "title": "Benchmarking Multimodal Sentiment Analysis", "comments": "Accepted in CICLing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework for multimodal sentiment analysis and emotion\nrecognition using convolutional neural network-based feature extraction from\ntext and visual modalities. We obtain a performance improvement of 10% over the\nstate of the art by combining visual, text and audio features. We also discuss\nsome major issues frequently ignored in multimodal sentiment analysis research:\nthe role of speaker-independent models, importance of the modalities and\ngeneralizability. The paper thus serve as a new benchmark for further research\nin multimodal sentiment analysis and also demonstrates the different facets of\nanalysis to be considered while performing such tasks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 16:40:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cambria", "Erik", ""], ["Hazarika", "Devamanyu", ""], ["Poria", "Soujanya", ""], ["Hussain", "Amir", ""], ["Subramaanyam", "R. B. V.", ""]]}, {"id": "1707.09791", "submitter": "Mohsen Abdoli", "authors": "Mohsen Abdoli, F\\'elix Henry, Patric Brault, Pierre Duhamel,\n  Fr\\'ed\\'eric Dufaux", "title": "Intra Prediction Using In-Loop Residual Coding for the post-HEVC\n  Standard", "comments": "6 pages, 5 figure, Conference: IEEE 19th International Workshop on\n  Multimedia Signal Processing, Luton, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few years after standardization of the High Efficiency Video Coding (HEVC),\nnow the Joint Video Exploration Team (JVET) group is exploring post-HEVC video\ncompression technologies. In the intra prediction domain, this effort has\nresulted in an algorithm with 67 internal modes, new filters and tools which\nsignificantly improve HEVC. However, the improved algorithm still suffers from\nthe long distance prediction inaccuracy problem. In this paper, we propose an\nIn-Loop Residual coding Intra Prediction (ILR-IP) algorithm which utilizes\ninner-block reconstructed pixels as references to reduce the distance from\npredicted pixels. This is done by using the ILR signal for partially\nreconstructing each pixel, right after its prediction and before its\nblock-level out-loop residual calculation. The ILR signal is decided in the\nrate-distortion sense, by a brute-force search on a QP-dependent finite\ncodebook that is known to the decoder. Experiments show that the proposed\nILR-IP algorithm improves the existing method in the Joint Exploration Model\n(JEM) up to 0.45% in terms of bit rate saving, without complexity overhead at\nthe decoder side.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 10:11:51 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Abdoli", "Mohsen", ""], ["Henry", "F\u00e9lix", ""], ["Brault", "Patric", ""], ["Duhamel", "Pierre", ""], ["Dufaux", "Fr\u00e9d\u00e9ric", ""]]}]