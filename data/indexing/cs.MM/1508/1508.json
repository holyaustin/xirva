[{"id": "1508.01055", "submitter": "Roman Fedorov", "authors": "Roman Fedorov, Alessandro Camerada, Piero Fraternali, Marco\n  Tagliasacchi", "title": "Estimating snow cover from publicly available images", "comments": "submitted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2016.2535356", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of estimating snow cover in mountainous\nregions, that is, the spatial extent of the earth surface covered by snow. We\nargue that publicly available visual content, in the form of user generated\nphotographs and image feeds from outdoor webcams, can both be leveraged as\nadditional measurement sources, complementing existing ground, satellite and\nairborne sensor data. To this end, we describe two content acquisition and\nprocessing pipelines that are tailored to such sources, addressing the specific\nchallenges posed by each of them, e.g., identifying the mountain peaks,\nfiltering out images taken in bad weather conditions, handling varying\nillumination conditions. The final outcome is summarized in a snow cover index,\nwhich indicates for a specific mountain and day of the year, the fraction of\nvisible area covered by snow, possibly at different elevations. We created a\nmanually labelled dataset to assess the accuracy of the image snow covered area\nestimation, achieving 90.0% precision at 91.1% recall. In addition, we show\nthat seasonal trends related to air temperature are captured by the snow cover\nindex.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 12:46:26 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Fedorov", "Roman", ""], ["Camerada", "Alessandro", ""], ["Fraternali", "Piero", ""], ["Tagliasacchi", "Marco", ""]]}, {"id": "1508.01485", "submitter": "Reem Ahmed Alotaibi", "authors": "Reem Ahmed Alotaibi and Lamiaa A. Elrefaei", "title": "Arabic Text Watermarking: A Review", "comments": "16 pages, 4 tables and 19 figures", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA) Vol. 6, No. 4, July 2015", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The using of the internet with its technologies and applications have been\nincreased rapidly. So, protecting the text from illegal use is too needed .\nText watermarking is used for this purpose. Arabic text has many\ncharacteristics such existing of diacritics , kashida (extension character) and\npoints above or under its letters .Each of Arabic letters can take different\nshapes with different Unicode. These characteristics are utilized in the\nwatermarking process. In this paper, several methods are discussed in the area\nof Arabic text watermarking with its advantages and disadvantages .Comparison\nof these methods is done in term of capacity, robustness and Imperceptibility.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 18:39:19 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Alotaibi", "Reem Ahmed", ""], ["Elrefaei", "Lamiaa A.", ""]]}, {"id": "1508.02284", "submitter": "Simona Samardjiska", "authors": "Simona Samardjiska and Danilo Gligoroski", "title": "Approaching Maximum Embedding Efficiency on Small Covers Using\n  Staircase-Generator Codes", "comments": "Extended version of the paper presented at ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of binary linear codes suitable for steganographic\nmatrix embedding. The main characteristic of the codes is the staircase random\nblock structure of the generator matrix. We propose an efficient list decoding\nalgorithm for the codes that finds a close codeword to a given random word. We\nprovide both theoretical analysis of the performance and stability of the\ndecoding algorithm, as well as practical results. Used for matrix embedding,\nthese codes achieve almost the upper theoretical bound of the embedding\nefficiency for covers in the range of 1000 - 1500 bits, which is at least an\norder of magnitude smaller than the values reported in related works.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 15:28:39 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Samardjiska", "Simona", ""], ["Gligoroski", "Danilo", ""]]}, {"id": "1508.02959", "submitter": "Roman Fedorov", "authors": "Roman Fedorov", "title": "Mountain Peak Detection in Online Social Media", "comments": "M.Sc. Thesis, Politecnico di Milano, Italy, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for the classification of mountain panoramas from\nuser-generated photographs followed by identification and extraction of\nmountain peaks from those panoramas. We have developed an automatic technique\nthat, given as input a geo-tagged photograph, estimates its FOV (Field Of View)\nand the direction of the camera using a matching algorithm on the photograph\nedge maps and a rendered view of the mountain silhouettes that should be seen\nfrom the observer's point of view. The extraction algorithm then identifies the\nmountain peaks present in the photograph and their profiles. We discuss\npossible applications in social fields such as photograph peak tagging on\nsocial portals, augmented reality on mobile devices when viewing a mountain\npanorama, and generation of collective intelligence systems (such as\nenvironmental models) from massive social media collections (e.g. snow water\navailability maps based on mountain peak states extracted from photograph\nhosting services).\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 15:43:16 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Fedorov", "Roman", ""]]}, {"id": "1508.03170", "submitter": "Paulo Figueiredo", "authors": "Paulo Figueiredo and Marta Apar\\'icio and David Martins de Matos and\n  Ricardo Ribeiro", "title": "Generation of Multimedia Artifacts: An Extractive Summarization-based\n  Approach", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore methods for content selection and address the issue of coherence\nin the context of the generation of multimedia artifacts. We use audio and\nvideo to present two case studies: generation of film tributes, and\nlecture-driven science talks. For content selection, we use centrality-based\nand diversity-based summarization, along with topic analysis. To establish\ncoherence, we use the emotional content of music, for film tributes, and ensure\ntopic similarity between lectures and documentaries, for science talks.\nComposition techniques for the production of multimedia artifacts are addressed\nas a means of organizing content, in order to improve coherence. We discuss our\nresults considering the above aspects.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 10:56:42 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Figueiredo", "Paulo", ""], ["Apar\u00edcio", "Marta", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1508.03840", "submitter": "Ibrahim Adeyanju", "authors": "Ibrahim Adeyanju and Comfort Babalola and Kareemat Salaudeen and Biola\n  Oyediran", "title": "3D-Computer Animation for a Yoruba Native Folktale", "comments": "9 pages", "journal-ref": "International Journal of Computer Graphics & Animation (IJCGA)\n  Vol.5, No.3, July 2015", "doi": "10.5121/ijcga.2015.5302", "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer graphics has wide range of applications which are implemented into\ncomputer animation, computer modeling among others. Since the invention of\ncomputer graphics researchers have not paid much of attentions toward the\npossibility of converting oral tales otherwise known as folktales into possible\ncartoon animated videos. This paper is based on how to develop cartoons of\nlocal folktales that will be of huge benefits to Nigerians. The activities were\ndivided into 5 stages; analysis, design, development, implementation and\nevaluation which involved various processes and use of various specialized\nsoftware and hardware. After the implementation of this project, the video\ncharacteristics were evaluated using likert scale. Analysis of 30 user\nresponses indicated that 17 users (56.7 percent) rated the image quality as\nexcellent, the video and image synchronization was rated as excellent by 9\nusers (30 percent), the Background noise was rated excellent by 18 users (60\npercent), the Character Impression was rated Excellent by 11 users (36.67\npercent), the general assessment of the storyline was rated excellent by 17\nusers (56.7 percent), the video Impression was rated excellent by 11 users\n(36.67 percent) and the voice quality was rated by 10 users (33.33 percent) as\nexcellent.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 15:58:11 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Adeyanju", "Ibrahim", ""], ["Babalola", "Comfort", ""], ["Salaudeen", "Kareemat", ""], ["Oyediran", "Biola", ""]]}, {"id": "1508.03868", "submitter": "Brendan Jou", "authors": "Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara,\n  Shih-Fu Chang", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual\n  Sentiment Ontology", "comments": "11 pages, to appear at ACM MM'15", "journal-ref": null, "doi": "10.1145/2733373.2806246", "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every culture and language is unique. Our work expressly focuses on the\nuniqueness of culture and language in relation to human affect, specifically\nsentiment and emotion semantics, and how they manifest in social multimedia. We\ndevelop sets of sentiment- and emotion-polarized visual concepts by adapting\nsemantic structures called adjective-noun pairs, originally introduced by Borth\net al. (2013), but in a multilingual context. We propose a new\nlanguage-dependent method for automatic discovery of these adjective-noun\nconstructs. We show how this pipeline can be applied on a social multimedia\nplatform for the creation of a large-scale multilingual visual sentiment\nconcept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our\nunified ontology is organized hierarchically by multilingual clusters of\nvisually detectable nouns and subclusters of emotionally biased versions of\nthese nouns. In addition, we present an image-based prediction task to show how\ngeneralizable language-specific models are in a multilingual context. A new,\npublicly available dataset of >15.6K sentiment-biased visual concepts across 12\nlanguages with language-specific detector banks, >7.36M images and their\nmetadata is also released.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 21:43:59 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 16:33:13 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 19:07:14 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Jou", "Brendan", ""], ["Chen", "Tao", ""], ["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1508.04978", "submitter": "Krzysztof Szczypiorski", "authors": "Krzysztof Szczypiorski, Artur Janicki, and Steffen Wendzel", "title": "\"The Good, The Bad And The Ugly\": Evaluation of Wi-Fi Steganography", "comments": "6 pages, 6 figures, to appear in Proc. of: ICNIT 2015 - 6th\n  International Conference on Networking and Information Technology, Tokyo,\n  Japan, November 5-6, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method for the evaluation of network\nsteganography algorithms based on the new concept of \"the moving observer\". We\nconsidered three levels of undetectability named: \"good\", \"bad\", and \"ugly\". To\nillustrate this method we chose Wi-Fi steganography as a solid family of\ninformation hiding protocols. We present the state of the art in this area\ncovering well-known hiding techniques for 802.11 networks. \"The moving\nobserver\" approach could help not only in the evaluation of steganographic\nalgorithms, but also might be a starting point for a new detection system of\nnetwork steganography. The concept of a new detection system, called MoveSteg,\nis explained in detail.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 13:35:43 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 22:22:09 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Szczypiorski", "Krzysztof", ""], ["Janicki", "Artur", ""], ["Wendzel", "Steffen", ""]]}, {"id": "1508.05056", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Victor Campos, Amaia Salvador, Brendan Jou and Xavier Gir\\'o-i-Nieto", "title": "Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual\n  Sentiment Prediction", "comments": "Preprint of the paper accepted at the 1st Workshop on Affect and\n  Sentiment in Multimedia (ASM), in ACM MultiMedia 2015. Brisbane, Australia", "journal-ref": null, "doi": "10.1145/2813524.2813530", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual media are powerful means of expressing emotions and sentiments. The\nconstant generation of new content in social networks highlights the need of\nautomated visual sentiment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several vision problems,\ntheir application to the task of sentiment analysis is mostly unexplored and\nthere are few studies regarding how to design CNNs for this purpose. In this\nwork, we study the suitability of fine-tuning a CNN for visual sentiment\nprediction as well as explore performance boosting techniques within this deep\nlearning setting. Finally, we provide a deep-dive analysis into a benchmark,\nstate-of-the-art network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment prediction.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 17:36:48 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 09:43:18 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Campos", "Victor", ""], ["Salvador", "Amaia", ""], ["Jou", "Brendan", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1508.05373", "submitter": "Yun-Fu Liu", "authors": "Yun-Fu Liu, Jing-Ming Guo", "title": "Dot-Diffused Halftoning with Improved Homogeneity", "comments": "Accepted to IEEE Trans. on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2470599", "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the error diffusion, dot diffusion provides an additional\npixel-level parallelism for digital halftoning. However, even though its\nperiodic and blocking artifacts had been eased by previous works, it was still\nfar from satisfactory in terms of the blue noise spectrum perspective. In this\nwork, we strengthen the relationship among the pixel locations of the same\nprocessing order by an iterative halftoning method, and the results demonstrate\na significant improvement. Moreover, a new approach of deriving the averaged\npower spectrum density (APSD) is proposed to avoid the regular sampling of the\nwell-known Bartlett's procedure which inaccurately presents the halftone\nperiodicity of certain halftoning techniques with parallelism. As a result, the\nproposed dot diffusion is substantially superior to the state-of-the-art\nparallel halftoning methods in terms of visual quality and artifact-free\nproperty, and competitive runtime to the theoretical fastest ordered dithering\nis offered simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 19:06:53 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Liu", "Yun-Fu", ""], ["Guo", "Jing-Ming", ""]]}, {"id": "1508.06106", "submitter": "Roman Starosolski", "authors": "Roman Starosolski", "title": "Reversible Denoising and Lifting Based Color Component Transformation\n  for Lossless Image Compression", "comments": "Keywords: Reversible color space transformation; Lifting technique;\n  Denoising; Reversible denoising and lifting step; Detector precision\n  characteristic; Lossless image compression", "journal-ref": "Multimedia Tools and Applications (2020) 79(17-18):11269-11294", "doi": "10.1007/s11042-019-08371-w", "report-no": null, "categories": "cs.MM cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An undesirable side effect of reversible color space transformation, which\nconsists of lifting steps (LSs), is that while removing correlation it\ncontaminates transformed components with noise from other components. Noise\naffects particularly adversely the compression ratios of lossless compression\nalgorithms. To remove correlation without increasing noise, a reversible\ndenoising and lifting step (RDLS) was proposed that integrates denoising\nfilters into LS. Applying RDLS to color space transformation results in a new\nimage component transformation that is perfectly reversible despite involving\nthe inherently irreversible denoising; the first application of such a\ntransformation is presented in this paper. For the JPEG-LS, JPEG 2000, and JPEG\nXR standard algorithms in lossless mode, the application of RDLS to the RDgDb\ncolor space transformation with simple denoising filters is especially\neffective for images in the native optical resolution of acquisition devices.\nIt results in improving compression ratios of all those images in cases when\nunmodified color space transformation either improves or worsens ratios\ncompared with the untransformed image. The average improvement is 5.0-6.0\\% for\ntwo out of the three sets of such images, whereas average ratios of images from\nstandard test-sets are improved by up to 2.2\\%. For the efficient\nimage-adaptive determination of filters for RDLS, a couple of fast\nentropy-based estimators of compression effects that may be used independently\nof the actual compression algorithm are investigated and an immediate filter\nselection method based on the detector precision characteristic model driven by\nimage acquisition parameters is introduced.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 11:17:54 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 18:50:38 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 20:25:10 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 17:44:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Starosolski", "Roman", ""]]}, {"id": "1508.07640", "submitter": "Nasser Eslahi", "authors": "Nasser Eslahi, Ali Aghagolzadeh, Seyed Mehdi Hosseini Andargoli", "title": "Compressive Video Sensing via Dictionary Learning and Forward Prediction", "comments": "26 Pages, 5 Figures, 3 Tables, This paper was presented in part at\n  the 7th International Symposium on Telecommunications. arXiv admin note: text\n  overlap with arXiv:1404.7566 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework for compressive video sensing (CVS)\nthat exploits the inherent spatial and temporal redundancies of a video\nsequence, effectively. The proposed method splits the video sequence into the\nkey and non-key frames followed by dividing each frame into small\nnon-overlapping blocks of equal sizes. At the decoder side, the key frames are\nreconstructed using adaptively learned sparsifying (ALS) basis via $\\ell_0$\nminimization, in order to exploit the spatial redundancy. Also, the\neffectiveness of three well-known dictionary learning algorithms is\ninvestigated in our method. For recovery of the non-key frames, a prediction of\nthe current frame is initialized, by using the previous reconstructed frame, in\norder to exploit the temporal redundancy. The prediction is employed in a\nproper optimization problem to recover the current non-key frame. To compare\nour experimental results with the results of some other methods, we employ peak\nsignal to noise ratio (PSNR) and structural similarity (SSIM) index as the\nquality assessor. The numerical results show the adequacy of our proposed\nmethod in CVS.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 21:47:00 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Eslahi", "Nasser", ""], ["Aghagolzadeh", "Ali", ""], ["Andargoli", "Seyed Mehdi Hosseini", ""]]}]