[{"id": "1607.00321", "submitter": "Tobias Hossfeld", "authors": "Tobias Hossfeld and Poul E. Heegaard and Martin Varela and Sebastian\n  M\\\"oller", "title": "Formal Definition of QoE Metrics", "comments": null, "journal-ref": "Quality and User Experience (2016) 1: 2", "doi": "10.1007/s41233-016-0002-1", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report formally defines the QoE metrics which are introduced\nand discussed in the article \"QoE Beyond the MOS: An In-Depth Look at QoE via\nBetter Metrics and their Relation to MOS\" by Tobias Ho{\\ss}feld, Poul E.\nHeegaard, Martin Varela, Sebastian M\\\"oller, accepted for publication in the\nSpringer journal \"Quality and User Experience\". Matlab scripts for computing\nthe QoE metrics for given data sets are available in GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 17:19:27 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Hossfeld", "Tobias", ""], ["Heegaard", "Poul E.", ""], ["Varela", "Martin", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "1607.00405", "submitter": "Ling He", "authors": "Ling He, Lee Murphy, and Jiebo Luo", "title": "Using Social Media to Promote STEM Education: Matching College Students\n  with Role Models", "comments": "16 pages, 8 figures, accepted by ECML/PKDD 2016, Industrial Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  STEM (Science, Technology, Engineering, and Mathematics) fields have become\nincreasingly central to U.S. economic competitiveness and growth. The shortage\nin the STEM workforce has brought promoting STEM education upfront. The rapid\ngrowth of social media usage provides a unique opportunity to predict users'\nreal-life identities and interests from online texts and photos. In this paper,\nwe propose an innovative approach by leveraging social media to promote STEM\neducation: matching Twitter college student users with diverse LinkedIn STEM\nprofessionals using a ranking algorithm based on the similarities of their\ndemographics and interests. We share the belief that increasing STEM presence\nin the form of introducing career role models who share similar interests and\ndemographics will inspire students to develop interests in STEM related fields\nand emulate their models. Our evaluation on 2,000 real college students\ndemonstrated the accuracy of our ranking algorithm. We also design a novel\nimplementation that recommends matched role models to the students.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 20:58:11 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["He", "Ling", ""], ["Murphy", "Lee", ""], ["Luo", "Jiebo", ""]]}, {"id": "1607.00719", "submitter": "Le Dong", "authors": "Gaipeng Kong, Le Dong, Wenpu Dong, Liang Zheng, Qi Tian", "title": "Coarse2Fine: Two-Layer Fusion For Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of large-scale image retrieval. We propose a\ntwo-layer fusion method which takes advantage of global and local cues and\nranks database images from coarse to fine (C2F). Departing from the previous\nmethods fusing multiple image descriptors simultaneously, C2F is featured by a\nlayered procedure composed by filtering and refining. In particular, C2F\nconsists of three components. 1) Distractor filtering. With holistic\nrepresentations, noise images are filtered out from the database, so the number\nof candidate images to be used for comparison with the query can be greatly\nreduced. 2) Adaptive weighting. For a certain query, the similarity of\ncandidate images can be estimated by holistic similarity scores in\ncomplementary to the local ones. 3) Candidate refining. Accurate retrieval is\nconducted via local features, combining the pre-computed adaptive weights.\nExperiments are presented on two benchmarks, \\emph{i.e.,} Holidays and Ukbench\ndatasets. We show that our method outperforms recent fusion methods in terms of\nstorage consumption and computation complexity, and that the accuracy is\ncompetitive to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:56:20 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kong", "Gaipeng", ""], ["Dong", "Le", ""], ["Dong", "Wenpu", ""], ["Zheng", "Liang", ""], ["Tian", "Qi", ""]]}, {"id": "1607.01159", "submitter": "Wen Hu", "authors": "Wen Hu, Zhi Wang, Lifeng Sun", "title": "Towards Network-Failure-Tolerant Content Delivery for Web Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popularly used to distribute a variety of multimedia content items in today\nInternet, HTTP-based web content delivery still suffers from various content\ndelivery failures. Hindered by the expensive deployment cost, the conventional\nCDN can not deploy as many edge servers as possible to successfully deliver\ncontent items to all users under these delivery failures. In this paper, we\npropose a joint CDN and peer-assisted web content delivery framework to address\nthe delivery failure problem. Different from conventional peer-assisted\napproaches for web content delivery, which mainly focus on alleviating the CDN\nservers bandwidth load, we study how to use a browser-based peer-assisted\nscheme, namely WebRTC, to resolve content delivery failures. To this end, we\ncarry out large-scale measurement studies on how users access and view\nwebpages. Our measurement results demonstrate the challenges (e.g., peers stay\non a webpage extremely short) that can not be directly solved by conventional\nP2P strategies, and some important webpage viewing patterns. Due to these\nunique characteristics, WebRTC peers open up new possibilities for helping the\nweb content delivery, coming with the problem of how to utilize the dynamic\nresources efficiently. We formulate the peer selection that is the critical\nstrategy in our framework, as an optimization problem, and design a heuristic\nalgorithm based on the measurement insights to solve it. Our simulation\nexperiments driven by the traces from Tencent QZone demonstrate the\neffectiveness of our design: compared with non-peer-assisted strategy and\nrandom peer selection strategy, our design significantly improves the\nsuccessful relay ratio of web content items under network failures, e.g., our\ndesign improves the content download ratio up to 60% even when users located in\na particular region (e.g., city) where none can connect to the regional CDN\nserver.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 09:08:56 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Hu", "Wen", ""], ["Wang", "Zhi", ""], ["Sun", "Lifeng", ""]]}, {"id": "1607.01172", "submitter": "Wen Hu", "authors": "Wen Hu, Zhi Wang, Lifeng Sun", "title": "A Measurement Study of TCP Performance for Chunk Delivery in DASH", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Adaptive Streaming over HTTP (DASH) has emerged as an increasingly\npopular paradigm for video streaming [13], in which a video is segmented into\nmany chunks delivered to users by HTTP request/response over Transmission\nControl Protocol (TCP) con- nections. Therefore, it is intriguing to study the\nperformance of strategies implemented in conventional TCPs, which are not\ndedicated for video streaming, e.g., whether chunks are efficiently delivered\nwhen users per- form interactions with the video players. In this paper, we\nconduct mea- surement studies on users chunk requesting traces in DASH from a\nrep- resentative video streaming provider, to investigate users behaviors in\nDASH, and TCP-connection-level traces from CDN servers, to investi- gate the\nperformance of TCP for DASH. By studying how video chunks are delivered in both\nthe slow start and congestion avoidance phases, our observations have revealed\nthe performance characteristics of TCP for DASH as follows: (1) Request\npatterns in DASH have a great impact on the performance of TCP variations\nincluding cubic; (2) Strategies in conventional TCPs may cause user perceived\nquality degradation in DASH streaming; (3) Potential improvement to TCP\nstrategies for better delivery in DASH can be further explored.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 09:51:25 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Hu", "Wen", ""], ["Wang", "Zhi", ""], ["Sun", "Lifeng", ""]]}, {"id": "1607.01261", "submitter": "Ming Ma", "authors": "Ming Ma, Zhi Wang, Yankai Zhang and Lifeng Sun", "title": "Dynamic Flow Scheduling Strategy in Multihoming Video CDNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multihoming for a video Content Delivery Network (CDN) allows edge peering\nservers to deliver video chunks through different Internet Service Providers\n(ISPs), to achieve an improved quality of service (QoS) for video streaming\nusers. However, since traditional strategies for a multihoming video CDN are\nsimply designed according to static rules, e.g., simply sending traffic via a\nISP which is the same as the ISP of client, they fail to dynamically allocate\nresources among different ISPs over time. In this paper, we perform measurement\nstudies to demonstrate that such static allocation mechanism is inefficient to\nmake full utilization of multiple ISPs' resources. To address this problem, we\npropose a dynamic flow scheduling strategy for multihoming video CDN. The\nchallenge is to find the control parameters that can guide the ISP selection\nwhen performing flow scheduling. Using a data-driven approach, we find factors\nthat have a major impact on the performance improvement in the dynamic flow\nscheduling. We further utilize an information gain approach to generate\nparameter combinations that can be used to guide the flow scheduling, i.e., to\ndetermine the ISP each request should be responded by. Our evaluation results\ndemonstrate that our design effectively performs the flow scheduling. In\nparticular, our design yields near optimal performance in a simulation of\nreal-world multihoming setup.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 14:23:57 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Ma", "Ming", ""], ["Wang", "Zhi", ""], ["Zhang", "Yankai", ""], ["Sun", "Lifeng", ""]]}, {"id": "1607.02281", "submitter": "Stavros Nikolopoulos D.", "authors": "Anna Mpanti and Stavros D. Nikolopoulos", "title": "Two RPG Flow-graphs for Software Watermarking using Bitonic Sequences of\n  Self-inverting Permutations", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software watermarking has received considerable attention and was adopted by\nthe software development community as a technique to prevent or discourage\nsoftware piracy and copyright infringement. A wide range of software\nwatermarking techniques has been proposed among which the graph-based methods\nthat encode watermarks as graph structures. Following up on our recently\nproposed methods for encoding watermark numbers $w$ as reducible permutation\nflow-graphs $F[\\pi^*]$ through the use of self-inverting permutations $\\pi^*$,\nin this paper, we extend the types of flow-graphs available for software\nwatermarking by proposing two different reducible permutation flow-graphs\n$F_1[\\pi^*]$ and $F_2[\\pi^*]$ incorporating important properties which are\nderived from the bitonic subsequences composing the self-inverting permutation\n$\\pi^*$. We show that a self-inverting permutation $\\pi^*$ can be efficiently\nencoded into either $F_1[\\pi^*]$ or $F_2[\\pi^*]$ and also efficiently decoded\nfrom theses graph structures. The proposed flow-graphs $F_1[\\pi^*]$ and\n$F_2[\\pi^*]$ enrich the repository of graphs which can encode the same\nwatermark number $w$ and, thus, enable us to embed multiple copies of the same\nwatermark $w$ into an application program $P$. Moreover, the enrichment of that\nrepository with new flow-graphs increases our ability to select a graph\nstructure more similar to the structure of a given application program $P$\nthereby enhancing the resilience of our codec system to attacks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 09:12:07 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 08:57:56 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Mpanti", "Anna", ""], ["Nikolopoulos", "Stavros D.", ""]]}, {"id": "1607.02303", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label\n  Tree Embeddings for Audio Scene Recognition", "comments": "Task1 technical report for the DCASE2016 challenge. arXiv admin note:\n  text overlap with arXiv:1606.07908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this report our audio scene recognition system submitted to\nthe DCASE 2016 challenge. Firstly, given the label set of the scenes, a label\ntree is automatically constructed. This category taxonomy is then used in the\nfeature extraction step in which an audio scene instance is represented by a\nlabel tree embedding image. Different convolutional neural networks, which are\ntailored for the task at hand, are finally learned on top of the image features\nfor scene recognition. Our system reaches an overall recognition accuracy of\n81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute\nimprovements of 8.7% and 6.1% on the development and test data, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:39:05 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:05:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02306", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CaR-FOREST: Joint Classification-Regression Decision Forests for\n  Overlapping Audio Event Detection", "comments": "Task2 and Task3 technical report for the DCASE2016 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our submissions to Task2 and Task3 of the DCASE 2016\nchallenge. The systems aim at dealing with the detection of overlapping audio\nevents in continuous streams, where the detectors are based on random decision\nforests. The proposed forests are jointly trained for classification and\nregression simultaneously. Initially, the training is classification-oriented\nto encourage the trees to select discriminative features from overlapping\nmixtures to separate positive audio segments from the negative ones. The\nregression phase is then carried out to let the positive audio segments vote\nfor the event onsets and offsets, and therefore model the temporal structure of\naudio events. One random decision forest is specifically trained for each event\ncategory of interest. Experimental results on the development data show that\nour systems significantly outperform the baseline on the Task2 evaluation while\nthey are inferior to the baseline in the Task3 evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:42:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:02:09 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02444", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler", "title": "Explaining Deep Convolutional Neural Networks on Music Classification", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been actively adopted in the\nfield of music information retrieval, e.g. genre classification, mood\ndetection, and chord recognition. However, the process of learning and\nprediction is little understood, particularly when it is applied to\nspectrograms. We introduce auralisation of a CNN to understand its underlying\nmechanism, which is based on a deconvolution procedure introduced in [2].\nAuralisation of a CNN is converting the learned convolutional features that are\nobtained from deconvolution into audio signals. In the experiments and\ndiscussions, we explain trained features of a 5-layer CNN based on the\ndeconvolved spectrograms and auralised signals. The pairwise correlations per\nlayers with varying different musical attributes are also investigated to\nunderstand the evolution of the learnt features. It is shown that in the deep\nlayers, the features are learnt to capture textures, the patterns of continuous\ndistributions, rather than shapes of lines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:40:30 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1607.02857", "submitter": "Lars Hertel", "authors": "Lars Hertel, Huy Phan, Alfred Mertins", "title": "Classifying Variable-Length Audio Files with All-Convolutional Networks\n  and Masked Global Pooling", "comments": "Technical report for the DCASE-2016 challenge (task 1 and task 4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a deep all-convolutional neural network with masked global pooling\nto perform single-label classification for acoustic scene classification and\nmulti-label classification for domestic audio tagging in the DCASE-2016\ncontest. Our network achieved an average accuracy of 84.5% on the four-fold\ncross-validation for acoustic scene recognition, compared to the provided\nbaseline of 72.5%, and an average equal error rate of 0.17 for domestic audio\ntagging, compared to the baseline of 0.21. The network therefore improves the\nbaselines by a relative amount of 17% and 19%, respectively. The network only\nconsists of convolutional layers to extract features from the short-time\nFourier transform and one global pooling layer to combine those features. It\nparticularly possesses neither fully-connected layers, besides the\nfully-connected output layer, nor dropout layers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 08:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Hertel", "Lars", ""], ["Phan", "Huy", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.03257", "submitter": "Benjamin Elizalde", "authors": "Benjamin Elizalde, Guan-Lin Chao, Ming Zeng, Ian Lane", "title": "City-Identification of Flickr Videos Using Semantic Acoustic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  City-identification of videos aims to determine the likelihood of a video\nbelonging to a set of cities. In this paper, we present an approach using only\naudio, thus we do not use any additional modality such as images, user-tags or\ngeo-tags. In this manner, we show to what extent the city-location of videos\ncorrelates to their acoustic information. Success in this task suggests\nimprovements can be made to complement the other modalities. In particular, we\npresent a method to compute and use semantic acoustic features to perform\ncity-identification and the features show semantic evidence of the\nidentification. The semantic evidence is given by a taxonomy of urban sounds\nand expresses the potential presence of these sounds in the city- soundtracks.\nWe used the MediaEval Placing Task set, which contains Flickr videos labeled by\ncity. In addition, we used the UrbanSound8K set containing audio clips labeled\nby sound- type. Our method improved the state-of-the-art performance and\nprovides a novel semantic approach to this task\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 08:30:45 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Elizalde", "Benjamin", ""], ["Chao", "Guan-Lin", ""], ["Zeng", "Ming", ""], ["Lane", "Ian", ""]]}, {"id": "1607.03273", "submitter": "Emrah Akyol", "authors": "Kivanc Mihcak, Emrah Akyol, Tamer Basar, Cedric Langbort", "title": "Scalar Quadratic-Gaussian Soft Watermarking Games", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the zero-sum game problem of soft watermarking: The hidden\ninformation (watermark) comes from a continuum and has a perceptual value; the\nreceiver generates an estimate of the embedded watermark to minimize the\nexpected estimation error (unlike the conventional watermarking schemes where\nboth the hidden information and the receiver output are from a discrete finite\nset). Applications include embedding a multimedia content into another. We\nconsider in this paper the scalar Gaussian case and use expected mean-squared\ndistortion. We formulate the resulting problem as a zero-sum game between the\nencoder & receiver pair and the attacker. We show that for the lin- ear\nencoder, the optimal attacker is Gaussian-affine, derive the optimal system\nparameters in that case, and discuss the corresponding system behavior. We also\nprovide numerical results to gain further insight and understanding of the\nsystem behavior at optimality.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 09:09:49 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Mihcak", "Kivanc", ""], ["Akyol", "Emrah", ""], ["Basar", "Tamer", ""], ["Langbort", "Cedric", ""]]}, {"id": "1607.03401", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xiaochun Cao, and Yuan Yao", "title": "Parsimonious Mixed-Effects HodgeRank for Crowdsourced Preference\n  Aggregation", "comments": "10 pages, ACM Multimedia (full paper) accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowdsourced preference aggregation, it is often assumed that all the\nannotators are subject to a common preference or utility function which\ngenerates their comparison behaviors in experiments. However, in reality\nannotators are subject to variations due to multi-criteria, abnormal, or a\nmixture of such behaviors. In this paper, we propose a parsimonious\nmixed-effects model based on HodgeRank, which takes into account both the fixed\neffect that the majority of annotators follows a common linear utility model,\nand the random effect that a small subset of annotators might deviate from the\ncommon significantly and exhibits strongly personalized preferences. HodgeRank\nhas been successfully applied to subjective quality evaluation of multimedia\nand resolves pairwise crowdsourced ranking data into a global consensus ranking\nand cyclic conflicts of interests. As an extension, our proposed methodology\nfurther explores the conflicts of interests through the random effect in\nannotator specific variations. The key algorithm in this paper establishes a\ndynamic path from the common utility to individual variations, with different\nlevels of parsimony or sparsity on personalization, based on newly developed\nLinearized Bregman Algorithms with Inverse Scale Space method. Finally the\nvalidity of the methodology are supported by experiments with both simulated\nexamples and three real-world crowdsourcing datasets, which shows that our\nproposed method exhibits better performance (i.e. smaller test error) compared\nwith HodgeRank due to its parsimonious property.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:30:10 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Yao", "Yuan", ""]]}, {"id": "1607.03406", "submitter": "Renata Khasanova", "authors": "Renata Khasanova, Xiaowen Dong, Pascal Frossard", "title": "Multi-modal image retrieval with random walk on multi-layer graphs", "comments": null, "journal-ref": null, "doi": "10.1109/ISM.2016.0011", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large collections of image data is still a challenging\nproblem due to the difficulty of capturing the true concepts in visual data.\nThe similarity between images could be computed using different and possibly\nmultimodal features such as color or edge information or even text labels. This\nmotivates the design of image analysis solutions that are able to effectively\nintegrate the multi-view information provided by different feature sets. We\ntherefore propose a new image retrieval solution that is able to sort images\nthrough a random walk on a multi-layer graph, where each layer corresponds to a\ndifferent type of information about the image data. We study in depth the\ndesign of the image graph and propose in particular an effective method to\nselect the edge weights for the multi-layer graph, such that the image ranking\nscores are optimised. We then provide extensive experiments in different\nreal-world photo collections, which confirm the high performance of our new\nimage retrieval algorithm that generally surpasses state-of-the-art solutions\ndue to a more meaningful image similarity computation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:35:01 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Khasanova", "Renata", ""], ["Dong", "Xiaowen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1607.04378", "submitter": "Liping Jing Dr.", "authors": "Liping Jing, Bo Liu, Jaeyoung Choi, Adam Janin, Julia Bernd, Michael\n  W. Mahoney, and Gerald Friedland", "title": "DCAR: A Discriminative and Compact Audio Representation to Improve Event\n  Detection", "comments": "An abbreviated version of this paper will be published in ACM\n  Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel two-phase method for audio representation,\nDiscriminative and Compact Audio Representation (DCAR), and evaluates its\nperformance at detecting events in consumer-produced videos. In the first phase\nof DCAR, each audio track is modeled using a Gaussian mixture model (GMM) that\nincludes several components to capture the variability within that track. The\nsecond phase takes into account both global structure and local structure. In\nthis phase, the components are rendered more discriminative and compact by\nformulating an optimization problem on Grassmannian manifolds, which we found\nrepresents the structure of audio effectively.\n  Our experiments used the YLI-MED dataset (an open TRECVID-style video corpus\nbased on YFCC100M), which includes ten events. The results show that the\nproposed DCAR representation consistently outperforms state-of-the-art audio\nrepresentations. DCAR's advantage over i-vector, mv-vector, and GMM\nrepresentations is significant for both easier and harder discrimination tasks.\nWe discuss how these performance differences across easy and hard cases follow\nfrom how each type of model leverages (or doesn't leverage) the intrinsic\nstructure of the data. Furthermore, DCAR shows a particularly notable accuracy\nadvantage on events where humans have more difficulty classifying the videos,\ni.e., events with lower mean annotator confidence.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 04:28:14 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Jing", "Liping", ""], ["Liu", "Bo", ""], ["Choi", "Jaeyoung", ""], ["Janin", "Adam", ""], ["Bernd", "Julia", ""], ["Mahoney", "Michael W.", ""], ["Friedland", "Gerald", ""]]}, {"id": "1607.04965", "submitter": "Huynh Van Luong", "authors": "Huynh Van Luong and Nikos Deligiannis and S{\\o}ren Forchhammer and\n  Andr\\'e Kaup", "title": "Distributed Coding of Multiview Sparse Sources with Joint Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support of applications involving multiview sources in distributed object\nrecognition using lightweight cameras, we propose a new method for the\ndistributed coding of sparse sources as visual descriptor histograms extracted\nfrom multiview images. The problem is challenging due to the computational and\nenergy constraints at each camera as well as the limitations regarding\ninter-camera communication. Our approach addresses these challenges by\nexploiting the sparsity of the visual descriptor histograms as well as their\nintra- and inter-camera correlations. Our method couples distributed source\ncoding of the sparse sources with a new joint recovery algorithm that\nincorporates multiple side information signals, where prior knowledge (low\nquality) of all the sparse sources is initially sent to exploit their\ncorrelations. Experimental evaluation using the histograms of shift-invariant\nfeature transform (SIFT) descriptors extracted from multiview images shows that\nour method leads to bit-rate saving of up to 43% compared to the\nstate-of-the-art distributed compressed sensing method with independent\nencoding of the sources.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 07:41:43 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Van Luong", "Huynh", ""], ["Deligiannis", "Nikos", ""], ["Forchhammer", "S\u00f8ren", ""], ["Kaup", "Andr\u00e9", ""]]}, {"id": "1607.05765", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Features and Kernels for Audio Event Recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important problems in audio event detection research is\nabsence of benchmark results for comparison with any proposed method. Different\nworks consider different sets of events and datasets which makes it difficult\nto comprehensively analyze any novel method with an existing one. In this paper\nwe propose to establish results for audio event recognition on two recent\npublicly-available datasets. In particular we use Gaussian Mixture model based\nfeature representation and combine them with linear as well as non-linear\nkernel Support Vector Machines.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 21:29:03 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1607.05808", "submitter": "Miaohui Wang", "authors": "Miaohui Wang and Ngan King Ngi", "title": "Hybrid Video Signal Coding Technologies: Past, Current and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing needs for high-quality video applications have resulted in a lot\nof studies and developments in video signal coding. This chapter presents some\nadvanced techniques in enhancing the rate-distortion performance of the\nblock-based hybrid video coding systems. Additionally, as can be seen from the\ndevelopments of H.264/AVC and HEVC, most of the current coding tools, such as\nprediction, transformation and entropy coding, have less room to improve in the\ncompression performance. On the other hand, loop filer in the modern video\nstandards shows the promising results. Thus, we believe that loop filter can be\nthe candidate in contributing to higher video compression for the\nnext-generation video coding. Specifically, improvements on ALF and SAO are\nalso introduced, and the simulation results show that the proposed methods\noutperform the existing method, which offer new degrees of freedom to improve\nthe overall rate-distortion performance. As a result, they can be the candidate\ncoding tools for the next-generation video codec.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 03:23:49 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Wang", "Miaohui", ""], ["Ngi", "Ngan King", ""]]}, {"id": "1607.06215", "submitter": "Qiyue Yin", "authors": "Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, Liang Wang", "title": "A Comprehensive Survey on Cross-modal Retrieval", "comments": "20 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, cross-modal retrieval has drawn much attention due to the\nrapid growth of multimodal data. It takes one type of data as the query to\nretrieve relevant data of another type. For example, a user can use a text to\nretrieve relevant pictures or videos. Since the query and its retrieved results\ncan be of different modalities, how to measure the content similarity between\ndifferent modalities of data remains a challenge. Various methods have been\nproposed to deal with such a problem. In this paper, we first review a number\nof representative methods for cross-modal retrieval and classify them into two\nmain groups: 1) real-valued representation learning, and 2) binary\nrepresentation learning. Real-valued representation learning methods aim to\nlearn real-valued common representations for different modalities of data. To\nspeed up the cross-modal retrieval, a number of binary representation learning\nmethods are proposed to map different modalities of data into a common Hamming\nspace. Then, we introduce several multimodal datasets in the community, and\nshow the experimental results on two commonly used multimodal datasets. The\ncomparison reveals the characteristic of different kinds of cross-modal\nretrieval methods, which is expected to benefit both practical applications and\nfuture research. Finally, we discuss open problems and future research\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 07:20:44 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Wang", "Kaiye", ""], ["Yin", "Qiyue", ""], ["Wang", "Wei", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "1607.06532", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang, Hsin-Hsi Chen", "title": "Novel Word Embedding and Translation-based Language Modeling for\n  Extractive Speech Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding methods revolve around learning continuous distributed vector\nrepresentations of words with neural networks, which can capture semantic\nand/or syntactic cues, and in turn be used to induce similarity measures among\nwords, sentences and documents in context. Celebrated methods can be\ncategorized as prediction-based and count-based methods according to the\ntraining objectives and model architectures. Their pros and cons have been\nextensively analyzed and evaluated in recent studies, but there is relatively\nless work continuing the line of research to develop an enhanced learning\nmethod that brings together the advantages of the two model families. In\naddition, the interpretation of the learned word representations still remains\nsomewhat opaque. Motivated by the observations and considering the pressing\nneed, this paper presents a novel method for learning the word representations,\nwhich not only inherits the advantages of classic word embedding methods but\nalso offers a clearer and more rigorous interpretation of the learned word\nrepresentations. Built upon the proposed word embedding method, we further\nformulate a translation-based language modeling framework for the extractive\nspeech summarization task. A series of empirical evaluations demonstrate the\neffectiveness of the proposed word representation learning and language\nmodeling techniques in extractive speech summarization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 00:20:09 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Liu", "Shih-Hung", ""], ["Chen", "Berlin", ""], ["Wang", "Hsin-Min", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "1607.06667", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathanael Perraudin, Nicki Holighaus, Piotr Majdak, Peter Balazs", "title": "Inpainting of long audio segments with similarity graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for the compensation of long duration data loss in\naudio signals, in particular music. The concealment of such signal defects is\nbased on a graph that encodes signal structure in terms of time-persistent\nspectral similarity. A suitable candidate segment for the substitution of the\nlost content is proposed by an intuitive optimization scheme and smoothly\ninserted into the gap, i.e. the lost or distorted signal region. Extensive\nlistening tests show that the proposed algorithm provides highly promising\nresults when applied to a variety of real-world music signals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 13:12:33 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 19:14:19 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 17:15:08 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 10:15:47 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Holighaus", "Nicki", ""], ["Majdak", "Piotr", ""], ["Balazs", "Peter", ""]]}, {"id": "1607.06803", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani and Mansour Jamzad", "title": "Restoring highly corrupted images by impulse noise using radial basis\n  functions interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Preserving details in restoring images highly corrupted by impulse noise\nremains a challenging problem. We proposed an algorithm based on radial basis\nfunctions (RBF) interpolation which estimates the intensities of corrupted\npixels by their neighbors. In this algorithm, first intensity values of noisy\npixels in the corrupted image are estimated using RBFs. Next, the image is\nsmoothed. The proposed algorithm can effectively remove the highly dense\nimpulse noise. Experimental results show the superiority of the proposed\nalgorithm in comparison to the recent similar methods both in noise suppression\nand detail preservation. Extensive simulations show better results in measure\nof peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM),\nespecially when the image is corrupted by very highly dense impulse noise.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 19:44:01 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 06:22:22 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 16:55:20 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Jamzad", "Mansour", ""]]}, {"id": "1607.07697", "submitter": "Raj Bhagath Pudi", "authors": "P Raj Bhagath, Kallol Mallick, Jayanta Mukherjee, Sudipta Mukopadhayay", "title": "Low-complexity feedback-channel-free distributed video coding using\n  Local Rank Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new feedback-channel-free Distributed Video\nCoding (DVC) algorithm using Local Rank Transform (LRT). The encoder computes\nLRT by considering selected neighborhood pixels of Wyner-Ziv frame. The ranks\nfrom the modified LRT are merged, and their positions are entropy coded and\nsent to the decoder. In addition, means of each block of Wyner-Ziv frame are\nalso transmitted to assist motion estimation. Using these measurements, the\ndecoder generates side information (SI) by implementing motion estimation and\ncompensation in LRT domain. An iterative algorithm is executed on SI using LRT\nto reconstruct the Wyner-Ziv frame. Experimental results show that the coding\nefficiency of our codec is close to the efficiency of pixel domain distributed\nvideo coders based on Low-Density Parity Check and Accumulate (LDPCA) or turbo\ncodes, with less encoder complexity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 06:13:21 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Bhagath", "P Raj", ""], ["Mallick", "Kallol", ""], ["Mukherjee", "Jayanta", ""], ["Mukopadhayay", "Sudipta", ""]]}, {"id": "1607.07824", "submitter": "Patrick Bas Dr", "authors": "Patrick Bas", "title": "Natural Steganography: cover-source switching for better steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new steganographic scheme relying on the principle of\ncover-source switching, the key idea being that the embedding should switch\nfrom one cover-source to another. The proposed implementation, called Natural\nSteganography, considers the sensor noise naturally present in the raw images\nand uses the principle that, by the addition of a specific noise the\nsteganographic embedding tries to mimic a change of ISO sensitivity. The\nembedding methodology consists in 1) perturbing the image in the raw domain, 2)\nmodeling the perturbation in the processed domain, 3) embedding the payload in\nthe processed domain. We show that this methodology is easily tractable\nwhenever the processes are known and enables to embed large and undetectable\npayloads. We also show that already used heuristics such as synchronization of\nembedding changes or detectability after rescaling can be respectively\nexplained by operations such as color demosaicing and down-scaling kernels.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 18:02:44 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Bas", "Patrick", ""]]}, {"id": "1607.07947", "submitter": "Juan Quiroz", "authors": "Min Yang Lee, Vahab Iranmanesh, Juan C. Quiroz", "title": "A New Approach to SMS Steganography using Mathematical Equations", "comments": "6 pages, International Conference on Computer Applications &\n  Technology, ICCAT 2015, November 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Information Technology, cyber-crime has always been a worrying\nissue for online users. Phishing, social engineering, and third party attacks\nhave made people reluctant to share their personal information, even with\ntrusted entities. Messages that are sent via Short Message Service (SMS) are\neasily copied and hacked by using special software. To enforce the security of\nsending messages through mobile phones, one solution is SMS steganography. SMS\nSteganography is a technique that hides a secret message in the SMS. We propose\na new approach for SMS steganography that uses a mathematical equation as the\nstego media in order to transmit the data. With this approach, we can hide up\nto 35 characters (25%) of a secret message on a single SMS with maximum of 140\ncharacters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 03:42:49 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Lee", "Min Yang", ""], ["Iranmanesh", "Vahab", ""], ["Quiroz", "Juan C.", ""]]}]