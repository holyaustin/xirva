[{"id": "1803.00303", "submitter": "Dimitrios Tsilimantos", "authors": "Dimitrios Tsilimantos, Theodoros Karagkioules, Stefan Valentin", "title": "Classifying flows and buffer state for YouTube's HTTP adaptive streaming\n  service in mobile networks", "comments": "13 pages, 12 figures. Accepted for publication in the proceedings of\n  ACM Multimedia Systems Conference (MMSys) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate cross-layer information is very useful to optimize mobile networks\nfor specific applications. However, providing application-layer information to\nlower protocol layers has become very difficult due to the wide adoption of\nend-to-end encryption and due to the absence of cross-layer signaling\nstandards. As an alternative, this paper presents a traffic profiling solution\nto passively estimate parameters of HTTP Adaptive Streaming (HAS) applications\nat the lower layers. By observing IP packet arrivals, our machine learning\nsystem identifies video flows and detects the state of an HAS client's\nplay-back buffer in real time. Our experiments with YouTube's mobile client\nshow that Random Forests achieve very high accuracy even with a strong\nvariation of link quality. Since this high performance is achieved at IP level\nwith a small, generic feature set, our approach requires no Deep Packet\nInspection (DPI), comes at low complexity, and does not interfere with\nend-to-end encryption. Traffic profiling is, thus, a powerful new tool for\nmonitoring and managing even encrypted HAS traffic in mobile networks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 10:59:20 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 14:33:33 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Tsilimantos", "Dimitrios", ""], ["Karagkioules", "Theodoros", ""], ["Valentin", "Stefan", ""]]}, {"id": "1803.00702", "submitter": "Emad Grais", "authors": "Emad M. Grais, Dominic Ward, and Mark D. Plumbley", "title": "Raw Multi-Channel Audio Source Separation using Multi-Resolution\n  Convolutional Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised multi-channel audio source separation requires extracting useful\nspectral, temporal, and spatial features from the mixed signals. The success of\nmany existing systems is therefore largely dependent on the choice of features\nused for training. In this work, we introduce a novel multi-channel,\nmulti-resolution convolutional auto-encoder neural network that works on raw\ntime-domain signals to determine appropriate multi-resolution features for\nseparating the singing-voice from stereo music. Our experimental results show\nthat the proposed method can achieve multi-channel audio source separation\nwithout the need for hand-crafted features or any pre- or post-processing.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 03:47:47 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Grais", "Emad M.", ""], ["Ward", "Dominic", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1803.01146", "submitter": "Yafei Li", "authors": "Mingliang Xu, Hao Su, Yafei Li, Xi Li, Jing Liao, Jianwei Niu, Pei Lv\n  and Bing Zhou", "title": "Stylize Aesthetic QR Code", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continued proliferation of smart mobile devices, Quick Response (QR)\ncode has become one of the most-used types of two-dimensional code in the\nworld. Aiming at beautifying the visual-unpleasant appearance of QR codes,\nexisting works have developed a series of techniques. However, these works\nstill leave much to be desired, such as personalization, artistry, and\nrobustness. To address these issues, in this paper, we propose a novel type of\naesthetic QR codes, SEE (Stylize aEsthEtic) QR code, and a three-stage approach\nto automatically produce such robust style-oriented codes. Specifically, in the\nfirst stage, we propose a method to generate an optimized baseline aesthetic QR\ncode, which reduces the visual contrast between the noise-like black/white\nmodules and the blended image. In the second stage, to obtain art style QR\ncode, we tailor an appropriate neural style transformation network to endow the\nbaseline aesthetic QR code with artistic elements. In the third stage, we\ndesign an error-correction mechanism by balancing two competing terms, visual\nquality and readability, to ensure the performance robust. Extensive\nexperiments demonstrate that SEE QR code has high quality in terms of both\nvisual appearance and robustness, and also offers a greater variety of\npersonalized choices to users.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 11:36:42 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 09:48:10 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Xu", "Mingliang", ""], ["Su", "Hao", ""], ["Li", "Yafei", ""], ["Li", "Xi", ""], ["Liao", "Jing", ""], ["Niu", "Jianwei", ""], ["Lv", "Pei", ""], ["Zhou", "Bing", ""]]}, {"id": "1803.01339", "submitter": "Huseyin Hacihabiboglu", "authors": "Mert Burkay Coteli, Orhun Olgun and Huseyin Hacihabiboglu", "title": "Multiple Sound Source Localisation with Steered Response Power Density\n  and Hierarchical Grid Refinement", "comments": "14 pages, 10 figures, 4 tables, submitted to IEEE/ACM Transactions on\n  Audio, Speech and Language Processing (03 March 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the direction-of-arrival (DOA) of sound sources is an important\nstep in sound field analysis. Rigid spherical microphone arrays allow the\ncalculation of a compact spherical harmonic representation of the sound field.\nA basic method for analysing sound fields recorded using such arrays is steered\nresponse power (SRP) maps wherein the source DOA can be estimated as the\nsteering direction that maximises the output power of a maximally-directive\nbeam. This approach is computationally costly since it requires steering the\nbeam in all possible directions. This paper presents an extension to SRP called\nsteered response power density (SRPD) and an associated, signal-adaptive search\nmethod called hierarchical grid refinement (HiGRID) for reducing the number of\nsteering directions needed for DOA estimation. The proposed method can localise\ncoherent as well as incoherent sources while jointly providing the number of\nprominent sources in the scene. It is shown to be robust to reverberation and\nadditive white noise. An evaluation of the proposed method using simulations\nand real recordings under highly reverberant conditions as well as a comparison\nwith state- of-the-art methods are presented.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 11:32:21 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Coteli", "Mert Burkay", ""], ["Olgun", "Orhun", ""], ["Hacihabiboglu", "Huseyin", ""]]}, {"id": "1803.02280", "submitter": "Yafei Li", "authors": "Mingliang Xu, Qingfeng Li, Jianwei Niu, Xiting Liu, Weiwei Xu, Pei Lv,\n  and Bing Zhou", "title": "ART-UP: A Novel Method for Generating Scanning-robust Aesthetic QR codes", "comments": "15pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QR codes are usually scanned in different environments, so they must be\nrobust to variations in illumination, scale, coverage, and camera angles.\nAesthetic QR codes improve the visual quality, but subtle changes in their\nappearance may cause scanning failure. In this paper, a new method to generate\nscanning-robust aesthetic QR codes is proposed, which is based on a\nmodule-based scanning probability estimation model that can effectively balance\nthe tradeoff between visual quality and scanning robustness. Our method locally\nadjusts the luminance of each module by estimating the probability of\nsuccessful sampling. The approach adopts the hierarchical, coarse-to-fine\nstrategy to enhance the visual quality of aesthetic QR codes, which\nsequentially generate the following three codes: a binary aesthetic QR code, a\ngrayscale aesthetic QR code, and the final color aesthetic QR code. Our\napproach also can be used to create QR codes with different visual styles by\nadjusting some initialization parameters. User surveys and decoding experiments\nwere adopted for evaluating our method compared with state-of-the-art\nalgorithms, which indicates that the proposed approach has excellent\nperformance in terms of both visual quality and scanning robustness.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:19:08 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Xu", "Mingliang", ""], ["Li", "Qingfeng", ""], ["Niu", "Jianwei", ""], ["Liu", "Xiting", ""], ["Xu", "Weiwei", ""], ["Lv", "Pei", ""], ["Zhou", "Bing", ""]]}, {"id": "1803.02623", "submitter": "Behrouz Bolourian Haghighi", "authors": "Behrouz Bolourian Haghighi, Amir Hossein Taherinia, Amir Hossein\n  Mohajerzadeh", "title": "TRLG: Fragile blind quad watermarking for image tamper detection and\n  recovery by providing compact digests with quality optimized using LWT and GA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient fragile blind quad watermarking scheme for image\ntamper detection and recovery based on lifting wavelet transform and genetic\nalgorithm is proposed. TRLG generates four compact digests with super quality\nbased on lifting wavelet transform and halftoning technique by distinguishing\nthe types of image blocks. In other words, for each 2*2 non-overlap blocks,\nfour chances for recovering destroyed blocks are considered. A special\nparameter estimation technique based on genetic algorithm is performed to\nimprove and optimize the quality of digests and watermarked image. Furthermore,\nCCS map is used to determine the mapping block for embedding information,\nencrypting and confusing the embedded information. In order to improve the\nrecovery rate, Mirror-aside and Partner-block are proposed. The experiments\nthat have been conducted to evaluate the performance of TRLG proved the\nsuperiority in terms of quality of the watermarked and recovered image, tamper\nlocalization and security compared with state-of-the-art methods. The results\nindicate that the PSNR and SSIM of the watermarked image are about 46 dB and\napproximately one, respectively. Also, the mean of PSNR and SSIM of several\nrecovered images which has been destroyed about 90% is reached to 24 dB and\n0.86, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:47:18 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Haghighi", "Behrouz Bolourian", ""], ["Taherinia", "Amir Hossein", ""], ["Mohajerzadeh", "Amir Hossein", ""]]}, {"id": "1803.03777", "submitter": "Yuxin Peng", "authors": "Xin Huang and Yuxin Peng", "title": "Deep Cross-media Knowledge Transfer", "comments": "10 pages, accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-media retrieval is a research hotspot in multimedia area, which aims to\nperform retrieval across different media types such as image and text. The\nperformance of existing methods usually relies on labeled data for model\ntraining. However, cross-media data is very labor consuming to collect and\nlabel, so how to transfer valuable knowledge in existing data to new data is a\nkey problem towards application. For achieving the goal, this paper proposes\ndeep cross-media knowledge transfer (DCKT) approach, which transfers knowledge\nfrom a large-scale cross-media dataset to promote the model training on another\nsmall-scale cross-media dataset. The main contributions of DCKT are: (1)\nTwo-level transfer architecture is proposed to jointly minimize the media-level\nand correlation-level domain discrepancies, which allows two important and\ncomplementary aspects of knowledge to be transferred: intra-media semantic and\ninter-media correlation knowledge. It can enrich the training information and\nboost the retrieval accuracy. (2) Progressive transfer mechanism is proposed to\niteratively select training samples with ascending transfer difficulties, via\nthe metric of cross-media domain consistency with adaptive feedback. It can\ndrive the transfer process to gradually reduce vast cross-media domain\ndiscrepancy, so as to enhance the robustness of model training. For verifying\nthe effectiveness of DCKT, we take the largescale dataset XMediaNet as source\ndomain, and 3 widelyused datasets as target domain for cross-media retrieval.\nExperimental results show that DCKT achieves promising improvement on retrieval\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 08:53:07 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""]]}, {"id": "1803.03849", "submitter": "Arda Senocak", "authors": "Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon", "title": "Learning to Localize Sound Source in Visual Scenes", "comments": "To appear in CVPR 2018. Total 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual events are usually accompanied by sounds in our daily lives. We pose\nthe question: Can the machine learn the correspondence between visual scene and\nthe sound, and localize the sound source only by observing sound and visual\nscene pairs like human? In this paper, we propose a novel unsupervised\nalgorithm to address the problem of localizing the sound source in visual\nscenes. A two-stream network structure which handles each modality, with\nattention mechanism is developed for sound source localization. Moreover,\nalthough our network is formulated within the unsupervised learning framework,\nit can be extended to a unified architecture with a simple modification for the\nsupervised and semi-supervised learning settings as well. Meanwhile, a new\nsound source dataset is developed for performance evaluation. Our empirical\nevaluation shows that the unsupervised method eventually go through false\nconclusion in some cases. We show that even with a few supervision, false\nconclusion is able to be corrected and the source of sound in a visual scene\ncan be localized effectively.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 18:19:02 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Senocak", "Arda", ""], ["Oh", "Tae-Hyun", ""], ["Kim", "Junsik", ""], ["Yang", "Ming-Hsuan", ""], ["Kweon", "In So", ""]]}, {"id": "1803.04053", "submitter": "Navaneeth Kamballur Kottayil", "authors": "Navaneeth Kamballur Kottayil, Giuseppe Valenzise, Frederic Dufaux and\n  Irene Cheng", "title": "Learning Local Distortion Visibility From Image Quality Data-sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of local distortion visibility thresholds is critical in\nmany image and video processing applications. Existing methods require an\naccurate modeling of the human visual system, and are derived through\npshycophysical experiments with simple, artificial stimuli. These approaches,\nhowever, are difficult to generalize to natural images with complex types of\ndistortion. In this paper, we explore a different perspective, and we\ninvestigate whether it is possible to learn local distortion visibility from\nimage quality scores. We propose a convolutional neural network based\noptimization framework to infer local detection thresholds in a distorted\nimage. Our model is trained on multiple quality datasets, and the results are\ncorrelated with empirical visibility thresholds collected on complex stimuli in\na recent study. Our results are comparable to state-of-the-art mathematical\nmodels that were trained on phsycovisual data directly. This suggests that it\nis possible to predict psychophysical phenomena from visibility information\nembedded in image quality scores.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 22:01:37 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Kottayil", "Navaneeth Kamballur", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""], ["Cheng", "Irene", ""]]}, {"id": "1803.04061", "submitter": "Di Chen", "authors": "Di Chen, Zoe Liu, Yaowu Xu, Fengqing Zhu, Edward Delp", "title": "Multi-Reference Video Coding Using Stillness Detection", "comments": "4 pages, 3 figures, IS&T Electronic Imaging on Visual Information\n  Processing and Communication Conference. (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoders of AOM/AV1 codec consider an input video sequence as succession of\nframes grouped in Golden-Frame (GF) groups. The coding structure of a GF group\nis fixed with a given GF group size. In the current AOM/AV1 encoder, video\nframes are coded using a hierarchical, multilayer coding structure within one\nGF group. It has been observed that the use of multilayer coding structure may\nresult in worse coding performance if the GF group presents consistent\nstillness across its frames. This paper proposes a new approach that adaptively\ndesigns the Golden-Frame (GF) group coding structure through the use of\nstillness detection. Our new approach hence develops an automatic stillness\ndetection scheme using three metrics extracted from each GF group. It then\ndifferentiates those GF groups of stillness from other non- still GF groups and\nuses different GF coding structures accordingly. Experimental result\ndemonstrates a consistent coding gain using the new approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 23:01:49 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chen", "Di", ""], ["Liu", "Zoe", ""], ["Xu", "Yaowu", ""], ["Zhu", "Fengqing", ""], ["Delp", "Edward", ""]]}, {"id": "1803.04617", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi, Said Nader-Esfahani, and Alireza Nasiri Avanaki", "title": "Robust LSB Watermarking Optimized for Local Structural Similarity", "comments": "ICEE, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growth of the Internet and networked multimedia systems has emphasized the\nneed for copyright protection of the media. Media can be images, audio clips,\nvideos and etc. Digital watermarking is today extensively used for many\napplications such as authentication of ownership or identification of illegal\ncopies. Digital watermark is an invisible or maybe visible structure added to\nthe original media (known as asset). Images are considered as communication\nchannel when they are subject to a watermark embedding procedure so in the case\nof embedding a digital watermark in an image, the capacity of the channel\nshould be considered. There is a trade-off between imperceptibility, robustness\nand capacity for embedding a watermark in an asset. In the case of image\nwatermarks, it is reasonable that the watermarking algorithm should depend on\nthe content and structure of the image. Conventionally, mean squared error\n(MSE) has been used as a common distortion measure to assess the quality of the\nimages. Newly developed quality metrics proposed some distortion measures that\nare based on human visual system (HVS). These metrics show that MSE is not\nbased on HVS and it has a lack of accuracy when dealing with perceptually\nimportant signals such as images and videos. SSIM or structural similarity is a\nstate of the art HVS based image quality criterion that has recently been of\nmuch interest. In this paper we propose a robust least significant bit (LSB)\nwatermarking scheme which is optimized for structural similarity. The watermark\nis embedded into a host image through an adaptive algorithm. Various attacks\nexamined on the embedding approach and simulation results revealed the fact\nthat the watermarked sequence can be extracted with an acceptable accuracy\nafter all attacks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 04:49:18 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Banitalebi", "Amin", ""], ["Nader-Esfahani", "Said", ""], ["Avanaki", "Alireza Nasiri", ""]]}, {"id": "1803.04656", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, and Panos Nasiopoulos", "title": "Effect of Eye Dominance on the Perception of Stereoscopic 3D Video", "comments": null, "journal-ref": "ICIP, 2014", "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric schemes have widespread applications in the 3D video transmission\npipeline. The significance of eye dominance becomes a concern when designing\nsuch schemes. In this paper, in order to investigate the effect of eye\ndominance on the perceptual 3D video quality, a database of representative\nasymmetric stereoscopic sequences is prepared and the overall 3D quality of\nthese sequences is evaluated through subjective experiments. Experiment results\nshowed that viewers find an asymmetric video more pleasant when the view with\nhigher quality is projected to their dominant eye. Moreover, the eye dominance\nchanges the mean opinion quality score by 16 % at most, a result caused by\nslight asymmetric video compression. For all other representative types of\nasymmetry, the statistical difference is much lower and in some cases even\nnegligible.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 06:53:20 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Banitalebi-Dehkordi", "Amin", ""], ["Pourazad", "Mahsa T.", ""], ["Nasiopoulos", "Panos", ""]]}, {"id": "1803.04680", "submitter": "Ren Yang", "authors": "Ren Yang, Mai Xu, Zulin Wang, Tianyi Li", "title": "Multi-Frame Quality Enhancement for Compressed Video", "comments": "to appear in CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00697", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed great success in applying deep learning to\nenhance the quality of compressed image/video. The existing approaches mainly\nfocus on enhancing the quality of a single frame, ignoring the similarity\nbetween consecutive frames. In this paper, we investigate that heavy quality\nfluctuation exists across compressed video frames, and thus low quality frames\ncan be enhanced using the neighboring high quality frames, seen as Multi-Frame\nQuality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach\nfor compressed video, as a first attempt in this direction. In our approach, we\nfirstly develop a Support Vector Machine (SVM) based detector to locate Peak\nQuality Frames (PQFs) in compressed video. Then, a novel Multi-Frame\nConvolutional Neural Network (MF-CNN) is designed to enhance the quality of\ncompressed video, in which the non-PQF and its nearest two PQFs are as the\ninput. The MF-CNN compensates motion between the non-PQF and PQFs through the\nMotion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement\nsubnet (QE-subnet) reduces compression artifacts of the non-PQF with the help\nof its nearest PQFs. Finally, the experiments validate the effectiveness and\ngenerality of our MFQE approach in advancing the state-of-the-art quality\nenhancement of compressed video. The code of our MFQE approach is available at\nhttps://github.com/ryangBUAA/MFQE.git\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 08:40:15 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 01:18:08 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:55:21 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 02:09:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yang", "Ren", ""], ["Xu", "Mai", ""], ["Wang", "Zulin", ""], ["Li", "Tianyi", ""]]}, {"id": "1803.04749", "submitter": "Pengpeng Yang", "authors": "Pengpeng Yang, Rongrong Ni, Yao Zhao, Gang Cao, Wei Zhao", "title": "Robust Contrast Enhancement Forensics Using Pixel and Histogram Domain\n  CNNs", "comments": "7 pages, submitted to ICME2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast enhancement (CE) forensics has always been ofconcern to image\nforensics community. It can provide aneffective tool for recovering image\nhistory and identifyingtampered images. Although several CE forensic\nalgorithmshave been proposed, their robustness against some processingis still\nunsatisfactory, such as JPEG compression and anti-forensic attacks. In order to\nattenuate such deficiency, inthis paper we first present a discriminability\nanalysis of CEforensics in pixel and gray level histogram domains. Then, insuch\ntwo domains, two end-to-end methods based on convo-lutional neural networks\n(P-CNN, H-CNN) are proposed toachieve robust CE forensics against pre-JPEG\ncompressionand anti-forensics attacks. Experimental results show that\ntheproposed methods achieve much better performance than thestate-of-the-art\nschemes for CE detection in the case of noother operation and comparable\nperformance when pre-JPEGcompression and anti-foresics attacks is used.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 12:32:52 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 09:25:21 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 15:32:33 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Yang", "Pengpeng", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""], ["Cao", "Gang", ""], ["Zhao", "Wei", ""]]}, {"id": "1803.04805", "submitter": "Shunquan Tan", "authors": "Jishen Zeng and Shunquan Tan and Guangqing Liu and Bin Li and Jiwu\n  Huang", "title": "WISERNet: Wider Separate-then-reunion Network for Steganalysis of Color\n  Images", "comments": "Accepted by IEEE IEEE Transactions on Information Forensics &\n  Security. DOI (identifier) 10.1109/TIFS.2019.2904413", "journal-ref": null, "doi": "10.1109/TIFS.2019.2904413", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, deep steganalyzers in spatial domain have been all designed\nfor gray-scale images. In this paper, we propose WISERNet (the wider\nseparate-then-reunion network) for steganalysis of color images. We provide\ntheoretical rationale to claim that the summation in normal convolution is one\nsort of \"linear collusion attack\" which reserves strong correlated patterns\nwhile impairs uncorrelated noises. Therefore in the bottom convolutional layer\nwhich aims at suppressing correlated image contents, we adopt separate\nchannel-wise convolution without summation instead. Conversely, in the upper\nconvolutional layers we believe that the summation in normal convolution is\nbeneficial. Therefore we adopt united normal convolution in those layers and\nmake them remarkably wider to reinforce the effect of \"linear collusion\nattack\". As a result, our proposed wide-and-shallow, separate-then-reunion\nnetwork structure is specifically suitable for color image steganalysis. We\nhave conducted extensive experiments on color image datasets generated from\nBOSSBase raw images and another large-scale dataset which contains 100,000 raw\nimages, with different demosaicking algorithms and down-sampling algorithms.\nThe experimental results show that our proposed network outperforms other\nstate-of-the-art color image steganalytic models either hand-crafted or learned\nusing deep networks in the literature by a clear margin. Specifically, it is\nnoted that the detection performance gain is achieved with less than half the\ncomplexity compared to the most advanced deep-learning steganalyzer as far as\nwe know, which is scarce in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 13:52:52 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 07:24:21 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Zeng", "Jishen", ""], ["Tan", "Shunquan", ""], ["Liu", "Guangqing", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "1803.04966", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi-Dehkordi, Mehdi Banitalebi-Dehkordi, Jamshid Abouei,\n  and Said Nader-Esfahani", "title": "An Improvement Technique based on Structural Similarity Thresholding for\n  Digital Watermarking", "comments": null, "journal-ref": "ACENG, 2014", "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital watermarking is extensively used in ownership authentication and\ncopyright protection. In this paper, we propose an efficient thresholding\nscheme to improve the watermark embedding procedure in an image. For the\nproposed algorithm, watermark casting is performed separately in each block of\nan image, and embedding in each block continues until a certain structural\nsimilarity threshold is reached. Numerical evaluations demonstrate that our\nscheme improves the imperceptibility of the watermark when the capacity remains\nfix, and at the same time, robustness against attacks is assured. The proposed\nmethod is applicable to most image watermarking algorithms. We verify this\nissue on watermarking schemes in Discrete Cosine Transform (DCT), wavelet, and\nspatial domain.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 06:23:39 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Banitalebi-Dehkordi", "Amin", ""], ["Banitalebi-Dehkordi", "Mehdi", ""], ["Abouei", "Jamshid", ""], ["Nader-Esfahani", "Said", ""]]}, {"id": "1803.05596", "submitter": "Yongtao Liu", "authors": "Yongtao Liu, Xiaopeng Fan, Yang Wang, Debin Zhao and Wen Gao", "title": "A nonlinear transform based analog video transmission framework", "comments": "6 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft-cast, a cross-layer design for wireless video transmission, is proposed\nto solve the drawbacks of digital video transmission: threshold transmission\nframework achieving the same effect. Specifically, in encoder, we carry out\npower allocation on the transformed coefficients and encode the coefficients\nbased on the new formulation of power distortion. In decoder, the process of\nLLSE estimator is also improved. Accompanied with the inverse nonlinear\ntransform, DCT coefficients can be recovered depending on the scaling factors ,\nLLSE estimator coefficients and metadata. Experiment results show that our\nproposed framework outperforms the Soft-cast in PSNR 1.08 dB and the MSSIM gain\nreaches to 2.35% when transmitting under the same bandwidth and total power.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 05:23:12 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Liu", "Yongtao", ""], ["Fan", "Xiaopeng", ""], ["Wang", "Yang", ""], ["Zhao", "Debin", ""], ["Gao", "Wen", ""]]}, {"id": "1803.05747", "submitter": "Hongfei Fan", "authors": "Hongfei Fan, Lin Ding, Xiaodong Xie, Huizhu Jia, Wen Gao", "title": "Joint Rate Allocation with Both Look-ahead And Feedback Model For High\n  Efficiency Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of joint rate allocation among multiple coded video streams is\nto share the bandwidth to meet the demands of minimum average distortion\n(minAVE) or minimum distortion variance (minVAR). In previous works on minVAR\nproblems, bits are directly assigned in proportion to their complexity measures\nand we call it look-ahead allocation model (LAM), which leads to the fact that\nthe performance will totally depend on the accuracy of the complexity measures.\nThis paper proposes a look-ahead and feedback allocation model (LFAM) for joint\nrate allocation for High Efficiency Video Coding (HEVC) platform which requires\nnegligible computational cost. We derive the model from the target function of\nminVAR theoretically. The bits are assigned according to the complexity\nmeasures, the distortion and bitrate values fed back by the encoder together.\nWe integrated the proposed allocation model in HEVC reference software HM16.0\nand several complexity measures were applied to our allocation model. Results\ndemonstrate that our proposed LFAM performs better than LAM, and an average of\n65.94% variance of mean square error (MSE) is saved with different complexity\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 13:50:04 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Fan", "Hongfei", ""], ["Ding", "Lin", ""], ["Xie", "Xiaodong", ""], ["Jia", "Huizhu", ""], ["Gao", "Wen", ""]]}, {"id": "1803.06874", "submitter": "Christian Timmerer", "authors": "Anatoliy Zabrovskiy, Christian Feldmann, Christian Timmerer", "title": "Multi-Codec DASH Dataset", "comments": "6 pages, submitted to ACM MMSys'18 (dataset track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of bandwidth-hungry applications and services is constantly\ngrowing. HTTP adaptive streaming of audio-visual content accounts for the\nmajority of today's internet traffic. Although the internet bandwidth increases\nalso constantly, audio-visual compression technology is inevitable and we are\ncurrently facing the challenge to be confronted with multiple video codecs.\nThis paper proposes a multi-codec DASH dataset comprising AVC, HEVC, VP9, and\nAV1 in order to enable interoperability testing and streaming experiments for\nthe efficient usage of these codecs under various conditions. We adopt state of\nthe art encoding and packaging options and also provide basic quality metrics\nalong with the DASH segments. Additionally, we briefly introduce a multi-codec\nDASH scheme and possible usage scenarios. Finally, we provide a preliminary\nevaluation of the encoding efficiency in the context of HTTP adaptive streaming\nservices and applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 11:32:36 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zabrovskiy", "Anatoliy", ""], ["Feldmann", "Christian", ""], ["Timmerer", "Christian", ""]]}, {"id": "1803.07488", "submitter": "Alexander Sagel", "authors": "Alexander Sagel and Hao Shen", "title": "Dynamic Variational Autoencoders for Visual Process Modeling", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053660", "report-no": null, "categories": "cs.NE cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of modeling visual processes by leveraging deep\ngenerative architectures for learning linear, Gaussian representations from\nobserved sequences. We propose a joint learning framework, combining a vector\nautoregressive model and Variational Autoencoders. This results in an\narchitecture that allows Variational Autoencoders to simultaneously learn a\nnon-linear observation as well as a linear state model from sequences of\nframes. We validate our approach on artificial sequences and dynamic textures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 15:38:40 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 08:08:45 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 14:19:04 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Sagel", "Alexander", ""], ["Shen", "Hao", ""]]}, {"id": "1803.07789", "submitter": "Wei Huang Dr.", "authors": "Wei Huang, Lianghui Ding, Hung-Yu Wei, Jenq-Neng Hwang, Yiling Xu,\n  Wenjun Zhang", "title": "QoE-Oriented Resource Allocation for 360-degree Video Transmission over\n  Heterogeneous Networks", "comments": "submitted to Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive media streaming, especially virtual reality (VR)/360-degree video\nstreaming which is very bandwidth demanding, has become more and more popular\ndue to the rapid growth of the multimedia and networking deployments. To better\nexplore the usage of resource and achieve better quality of experience (QoE)\nperceived by users, this paper develops an application-layer scheme to jointly\nexploit the available bandwidth from the LTE and Wi-Fi networks in 360-degree\nvideo streaming. This newly proposed scheme and the corresponding solution\nalgorithms utilize the saliency of video, prediction of users' view and the\nstatus information of users to obtain an optimal association of the users with\ndifferent Wi-Fi access points (APs) for maximizing the system's utility.\nBesides, a novel buffer strategy is proposed to mitigate the influence of\nshort-time prediction problem for transmitting 360-degree videos in\ntime-varying networks. The promising performance and low complexity of the\nproposed scheme and algorithms are validated in simulations with various\n360-degree videos.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 08:07:12 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Huang", "Wei", ""], ["Ding", "Lianghui", ""], ["Wei", "Hung-Yu", ""], ["Hwang", "Jenq-Neng", ""], ["Xu", "Yiling", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1803.08177", "submitter": "Jacob Chakareski", "authors": "Jacob Chakareski, Ridvan Aksu, Xavier Corbillon, Gwendal Simon, and\n  Viswanathan Swaminathan", "title": "Viewport-Driven Rate-Distortion Optimized 360{\\deg} Video Streaming", "comments": "To appear at IEEE ICC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of virtual and augmented reality communications and\n360{\\deg} video streaming is moving video communication systems into much more\ndynamic and resource-limited operating settings. The enormous data volume of\n360{\\deg} videos requires an efficient use of network bandwidth to maintain the\ndesired quality of experience for the end user. To this end, we propose a\nframework for viewport-driven rate-distortion optimized 360{\\deg} video\nstreaming that integrates the user view navigation pattern and the\nspatiotemporal rate-distortion characteristics of the 360{\\deg} video content\nto maximize the delivered user quality of experience for the given\nnetwork/system resources. The framework comprises a methodology for\nconstructing dynamic heat maps that capture the likelihood of navigating\ndifferent spatial segments of a 360{\\deg} video over time by the user, an\nanalysis and characterization of its spatiotemporal rate-distortion\ncharacteristics that leverage preprocessed spatial tilling of the 360{\\deg}\nview sphere, and an optimization problem formulation that characterizes the\ndelivered user quality of experience given the user navigation patterns,\n360{\\deg} video encoding decisions, and the available system/network resources.\nOur experimental results demonstrate the advantages of our framework over the\nconventional approach of streaming a monolithic uniformly encoded 360{\\deg}\nvideo and a state-of-the-art reference method. Considerable video quality gains\nof 4 - 5 dB are demonstrated in the case of two popular 4K 360{\\deg} videos.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 23:58:35 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Chakareski", "Jacob", ""], ["Aksu", "Ridvan", ""], ["Corbillon", "Xavier", ""], ["Simon", "Gwendal", ""], ["Swaminathan", "Viswanathan", ""]]}, {"id": "1803.08460", "submitter": "Yi Zhu", "authors": "Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao", "title": "Towards Universal Representation for Unseen Action Recognition", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unseen Action Recognition (UAR) aims to recognise novel action categories\nwithout training examples. While previous methods focus on inner-dataset\nseen/unseen splits, this paper proposes a pipeline using a large-scale training\nsource to achieve a Universal Representation (UR) that can generalise to a more\nrealistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a\nGeneralised Multiple-Instance Learning (GMIL) problem and discover\n'building-blocks' from the large-scale ActivityNet dataset using distribution\nkernels. Essential visual and semantic components are preserved in a shared\nspace to achieve the UR that can efficiently generalise to new datasets.\nPredicted UR exemplars can be improved by a simple semantic adaptation, and\nthen an unseen action can be directly recognised using UR during the test.\nWithout further training, extensive experiments manifest significant\nimprovements over the UCF101 and HMDB51 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:02:45 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zhu", "Yi", ""], ["Long", "Yang", ""], ["Guan", "Yu", ""], ["Newsam", "Shawn", ""], ["Shao", "Ling", ""]]}, {"id": "1803.08489", "submitter": "Hanhe Lin", "authors": "Hanhe Lin, Vlad Hosu and Dietmar Saupe", "title": "KonIQ-10k: Towards an ecologically valid and large-scale IQA database", "comments": "Image database, image quality assessment, diversity sampling,\n  crowdsourcing", "journal-ref": null, "doi": "10.1109/TIP.2020.2967829", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge in applying state-of-the-art deep learning methods to\npredict image quality in-the-wild is the relatively small size of existing\nquality scored datasets. The reason for the lack of larger datasets is the\nmassive resources required in generating diverse and publishable content. We\npresent a new systematic and scalable approach to create large-scale, authentic\nand diverse image datasets for Image Quality Assessment (IQA). We show how we\nbuilt an IQA database, KonIQ-10k, consisting of 10,073 images, on which we\nperformed very large scale crowdsourcing experiments in order to obtain\nreliable quality ratings from 1,467 crowd workers (1.2 million ratings). We\nargue for its ecological validity by analyzing the diversity of the dataset, by\ncomparing it to state-of-the-art IQA databases, and by checking the reliability\nof our user studies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:50:05 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lin", "Hanhe", ""], ["Hosu", "Vlad", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1803.08636", "submitter": "Chunbiao Zhu", "authors": "Chunbiao Zhu, Xing Cai, Kan Huang, Thomas H Li, Ge Li", "title": "PDNet: Prior-model Guided Depth-enhanced Network for Salient Object\n  Detection", "comments": "This paper is under review. Project website:\n  https://github.com/ChunbiaoZhu/PDNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (FCNs) have shown outstanding performance\nin many computer vision tasks including salient object detection. However,\nthere still remains two issues needed to be addressed in deep learning based\nsaliency detection. One is the lack of tremendous amount of annotated data to\ntrain a network. The other is the lack of robustness for extracting salient\nobjects in images containing complex scenes. In this paper, we present a new\narchitecture$ - $PDNet, a robust prior-model guided depth-enhanced network for\nRGB-D salient object detection. In contrast to existing works, in which RGB-D\nvalues of image pixels are fed directly to a network, the proposed architecture\nis composed of a master network for processing RGB values, and a sub-network\nmaking full use of depth cues and incorporate depth-based features into the\nmaster network. To overcome the limited size of the labeled RGB-D dataset for\ntraining, we employ a large conventional RGB dataset to pre-train the master\nnetwork, which proves to contribute largely to the final accuracy. Extensive\nevaluations over five benchmark datasets demonstrate that our proposed method\nperforms favorably against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 02:04:47 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 13:53:32 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhu", "Chunbiao", ""], ["Cai", "Xing", ""], ["Huang", "Kan", ""], ["Li", "Thomas H", ""], ["Li", "Ge", ""]]}, {"id": "1803.08670", "submitter": "Toru Ogawa", "authors": "Toru Ogawa, Atsushi Otsubo, Rei Narita, Yusuke Matsui, Toshihiko\n  Yamasaki, Kiyoharu Aizawa", "title": "Object Detection for Comics using Manga109 Annotations", "comments": "http://www.manga109.org/en/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of digitized comics, image understanding techniques are\nbecoming important. In this paper, we focus on object detection, which is a\nfundamental task of image understanding. Although convolutional neural networks\n(CNN)-based methods archived good performance in object detection for\nnaturalistic images, there are two problems in applying these methods to the\ncomic object detection task. First, there is no large-scale annotated comics\ndataset. The CNN-based methods require large-scale annotations for training.\nSecondly, the objects in comics are highly overlapped compared to naturalistic\nimages. This overlap causes the assignment problem in the existing CNN-based\nmethods. To solve these problems, we proposed a new annotation dataset and a\nnew CNN model. We annotated an existing image dataset of comics and created the\nlargest annotation dataset, named Manga109-annotations. For the assignment\nproblem, we proposed a new CNN-based detector, SSD300-fork. We compared\nSSD300-fork with other detection methods using Manga109-annotations and\nconfirmed that our model outperformed them based on the mAP score.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 06:54:48 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 05:35:40 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ogawa", "Toru", ""], ["Otsubo", "Atsushi", ""], ["Narita", "Rei", ""], ["Matsui", "Yusuke", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1803.09033", "submitter": "Anyi Rao", "authors": "Anyi Rao and Francis Lau", "title": "Automatic Music Accompanist", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic musical accompaniment is where a human musician is accompanied by a\ncomputer musician. The computer musician is able to produce musical\naccompaniment that relates musically to the human performance. The\naccompaniment should follow the performance using observations of the notes\nthey are playing. This paper describes a complete and detailed construction of\na score following and accompanying system using Hidden Markov Models (HMMs). It\ndetails how to train a score HMM, how to deal with polyphonic input, how this\nHMM work when following score, how to build up a musical accompanist. It\nproposes a new parallel hidden Markov model for score following and a fast\ndecoding algorithm to deal with performance errors.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 02:06:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Rao", "Anyi", ""], ["Lau", "Francis", ""]]}, {"id": "1803.09043", "submitter": "Bin Li", "authors": "Weixuan Tang, Bin Li, Shunquan Tan, Mauro Barni, and Jiwu Huang", "title": "CNN Based Adversarial Embedding with Minimum Alteration for Image\n  Steganography", "comments": "Submitted to IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": "10.1109/TIFS.2019.2891237", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, steganographic schemes were designed in a way to preserve image\nstatistics or steganalytic features. Since most of the state-of-the-art\nsteganalytic methods employ a machine learning (ML) based classifier, it is\nreasonable to consider countering steganalysis by trying to fool the ML\nclassifiers. However, simply applying perturbations on stego images as\nadversarial examples may lead to the failure of data extraction and introduce\nunexpected artefacts detectable by other classifiers. In this paper, we present\na steganographic scheme with a novel operation called adversarial embedding,\nwhich achieves the goal of hiding a stego message while at the same time\nfooling a convolutional neural network (CNN) based steganalyzer. The proposed\nmethod works under the conventional framework of distortion minimization.\nAdversarial embedding is achieved by adjusting the costs of image element\nmodifications according to the gradients backpropagated from the CNN classifier\ntargeted by the attack. Therefore, modification direction has a higher\nprobability to be the same as the sign of the gradient. In this way, the so\ncalled adversarial stego images are generated. Experiments demonstrate that the\nproposed steganographic scheme is secure against the targeted adversary-unaware\nsteganalyzer. In addition, it deteriorates the performance of other\nadversary-aware steganalyzers opening the way to a new class of modern\nsteganographic schemes capable to overcome powerful CNN-based steganalysis.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 02:40:49 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Tang", "Weixuan", ""], ["Li", "Bin", ""], ["Tan", "Shunquan", ""], ["Barni", "Mauro", ""], ["Huang", "Jiwu", ""]]}, {"id": "1803.09219", "submitter": "Yan Ke", "authors": "Jia Liu, Tanping Zhou, Zhuo Zhang, Yan Ke, Yu Lei, Minqing Zhang,\n  Xiaoyuan Yang", "title": "Digital Cardan Grille: A Modern Approach for Information Hiding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new framework for construction of Cardan grille for\ninformation hiding is proposed. Based on the semantic image inpainting\ntechnique, the stego image are driven by secret messages directly. A mask\ncalled Digital Cardan Grille (DCG) for determining the hidden location is\nintroduced to hide the message. The message is written to the corrupted region\nthat needs to be filled in the corrupted image in advance. Then the corrupted\nimage with secret message is feeded into a Generative Adversarial Network (GAN)\nfor semantic completion. The adversarial game not only reconstruct the\ncorrupted image , but also generate a stego image which contains the logic\nrationality of image content. The experimental results verify the feasibility\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 09:22:03 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 03:31:40 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Liu", "Jia", ""], ["Zhou", "Tanping", ""], ["Zhang", "Zhuo", ""], ["Ke", "Yan", ""], ["Lei", "Yu", ""], ["Zhang", "Minqing", ""], ["Yang", "Xiaoyuan", ""]]}, {"id": "1803.09403", "submitter": "Ye Yao", "authors": "Ye Yao, Weitong Hu, Wei Zhang, Ting Wu and Yun-Qing Shi", "title": "Distinguishing Computer-generated Graphics from Natural Images Based on\n  Sensor Pattern Noise and Deep Learning", "comments": "This paper has been published by Sensors. doi:10.3390/s18041296;\n  Sensors 2018, 18(4), 1296", "journal-ref": "Sensors 2018, 18(4), 1296", "doi": "10.3390/s18041296", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-generated graphics (CGs) are images generated by computer software.\nThe~rapid development of computer graphics technologies has made it easier to\ngenerate photorealistic computer graphics, and these graphics are quite\ndifficult to distinguish from natural images (NIs) with the naked eye. In this\npaper, we propose a method based on sensor pattern noise (SPN) and deep\nlearning to distinguish CGs from NIs. Before being fed into our convolutional\nneural network (CNN)-based model, these images---CGs and NIs---are clipped into\nimage patches. Furthermore, three high-pass filters (HPFs) are used to remove\nlow-frequency signals, which represent the image content. These filters are\nalso used to reveal the residual signal as well as SPN introduced by the\ndigital camera device. Different from the traditional methods of distinguishing\nCGs from NIs, the proposed method utilizes a five-layer CNN to classify the\ninput image patches. Based on the classification results of the image patches,\nwe deploy a majority vote scheme to obtain the classification results for the\nfull-size images. The~experiments have demonstrated that (1) the proposed\nmethod with three HPFs can achieve better results than that with only one HPF\nor no HPF and that (2) the proposed method with three HPFs achieves 100\\%\naccuracy, although the NIs undergo a JPEG compression with a quality factor of\n75.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 03:59:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 13:57:42 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Yao", "Ye", ""], ["Hu", "Weitong", ""], ["Zhang", "Wei", ""], ["Wu", "Ting", ""], ["Shi", "Yun-Qing", ""]]}, {"id": "1803.10889", "submitter": "Sai Ma", "authors": "Sai Ma, Qingxiao Guan, Xianfeng Zhao, Yaqi Liu", "title": "Weakening the Detecting Capability of CNN-based Steganalysis", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, the application of deep learning in steganalysis has drawn many\nresearchers' attention. Most of the proposed steganalytic deep learning models\nare derived from neural networks applied in computer vision. These kinds of\nneural networks have distinguished performance. However, all these kinds of\nback-propagation based neural networks may be cheated by forging input named\nthe adversarial example. In this paper we propose a method to generate\nsteganographic adversarial example in order to enhance the steganographic\nsecurity of existing algorithms. These adversarial examples can increase the\ndetection error of steganalytic CNN. The experiments prove the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 01:10:22 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Ma", "Sai", ""], ["Guan", "Qingxiao", ""], ["Zhao", "Xianfeng", ""], ["Liu", "Yaqi", ""]]}, {"id": "1803.11157", "submitter": "Pengpeng Yang", "authors": "Wei Zhao and Pengpeng Yang and Rongrong Ni and Yao Zhao and Haorui Wu", "title": "Security Consideration For Deep Learning-Based Image Forensics", "comments": null, "journal-ref": null, "doi": "10.1587/transinf.2018EDL8091", "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image forensics community has paied attention to the research on\nthe design of effective algorithms based on deep learning technology and facts\nproved that combining the domain knowledge of image forensics and deep learning\nwould achieve more robust and better performance than the traditional schemes.\nInstead of improving it, in this paper, the safety of deep learning based\nmethods in the field of image forensics is taken into account. To the best of\nour knowledge, this is a first work focusing on this topic. Specifically, we\nexperimentally find that the method using deep learning would fail when adding\nthe slight noise into the images (adversarial images). Furthermore, two kinds\nof strategys are proposed to enforce security of deep learning-based method.\nFirstly, an extra penalty term to the loss function is added, which is referred\nto the 2-norm of the gradient of the loss with respect to the input images, and\nthen an novel training method are adopt to train the model by fusing the normal\nand adversarial images. Experimental results show that the proposed algorithm\ncan achieve good performance even in the case of adversarial images and provide\na safety consideration for deep learning-based image forensics\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:06:00 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 09:54:20 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Zhao", "Wei", ""], ["Yang", "Pengpeng", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""], ["Wu", "Haorui", ""]]}, {"id": "1803.11286", "submitter": "Seyyed Hossein Soleymani", "authors": "Seyyed Hossein Soleymani, Amir Hossein Taherinia", "title": "High Capacity Image Data Hiding of Scanned Text Documents Using Improved\n  Quadtree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, an effective method was introduced to steganography of text\ndocument in the host image. In the available steganography methods, the message\nhas a random form. Therefore, the embedding capacity is generally low. In the\nproposed method, the main underlying idea was the sparse property of scanned\ndocuments. The scanned documents were converted from gray-level form to binary\nvalues by halftoning idea and then the information-included parts were\nextracted using the improved quadtree and separated from document context.\nNext, in order to compress the extracted parts, an algorithm was proposed based\non reading the binary string bits, ignoring the zero behind the number, and\nconverting them to decimal values. Embedding capacity of the proposed method is\nhigher than that of other available methods with a random-based message.\nTherefore, the proposed method can be used in the secure and intangible\ntransfer of text documents in the host image.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 23:25:15 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Soleymani", "Seyyed Hossein", ""], ["Taherinia", "Amir Hossein", ""]]}, {"id": "1803.11334", "submitter": "Zhengming Zhang", "authors": "Zhengming Zhang, Yaru Zheng, Meng Hua, Yongming Huang and Luxi Yang", "title": "Cache-Enabled Dynamic Rate Allocation via Deep Self-Transfer\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching and rate allocation are two promising approaches to support video\nstreaming over wireless network. However, existing rate allocation designs do\nnot fully exploit the advantages of the two approaches. This paper investigates\nthe problem of cache-enabled QoE-driven video rate allocation problem. We\nestablish a mathematical model for this problem, and point out that it is\ndifficult to solve the problem with traditional dynamic programming. Then we\npropose a deep reinforcement learning approaches to solve it. First, we model\nthe problem as a Markov decision problem. Then we present a deep Q-learning\nalgorithm with a special knowledge transfer process to find out effective\nallocation policy. Finally, numerical results are given to demonstrate that the\nproposed solution can effectively maintain high-quality user experience of\nmobile user moving among small cells. We also investigate the impact of\nconfiguration of critical parameters on the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 04:15:51 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Zhang", "Zhengming", ""], ["Zheng", "Yaru", ""], ["Hua", "Meng", ""], ["Huang", "Yongming", ""], ["Yang", "Luxi", ""]]}]