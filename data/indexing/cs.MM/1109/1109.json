[{"id": "1109.0216", "submitter": "Asadollah Shahbahrami", "authors": "Asadollah Shahbahrami, Ramin Bahrampour, Mobin Sabbaghi Rostami, and\n  Mostafa Ayoubi Mobarhan", "title": "Evaluation of Huffman and Arithmetic Algorithms for Multimedia\n  Compression Standards", "comments": "11 pages; http://airccse.org/journal/ijcsea/current.html\n  International Journal of Computer Science, Engineering and Applications\n  (IJCSEA) August 2011, Volume 1, Number 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is a technique to reduce the quantity of data without excessively\nreducing the quality of the multimedia data. The transition and storing of\ncompressed multimedia data is much faster and more efficient than original\nuncompressed multimedia data. There are various techniques and standards for\nmultimedia data compression, especially for image compression such as the JPEG\nand JPEG2000 standards. These standards consist of different functions such as\ncolor space conversion and entropy coding. Arithmetic and Huffman coding are\nnormally used in the entropy coding phase. In this paper we try to answer the\nfollowing question. Which entropy coding, arithmetic or Huffman, is more\nsuitable compared to other from the compression ratio, performance, and\nimplementation points of view? We have implemented and tested Huffman and\narithmetic algorithms. Our implemented results show that compression ratio of\narithmetic coding is better than Huffman coding, while the performance of the\nHuffman coding is higher than Arithmetic coding. In addition, implementation of\nHuffman coding is much easier than the Arithmetic coding.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 15:34:23 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Shahbahrami", "Asadollah", ""], ["Bahrampour", "Ramin", ""], ["Rostami", "Mobin Sabbaghi", ""], ["Mobarhan", "Mostafa Ayoubi", ""]]}, {"id": "1109.0753", "submitter": "Kinjal Shah H", "authors": "Kinjal Shah, Gagan Dua, Dharmendar Sharma, Priyanka Mishra, Nitin\n  Rakesh", "title": "Transmission of Successful Route Error Message(RERR) in Routing Aware\n  Multiple Description Video Coding over Mobile Ad-Hoc Network", "comments": "9 pages,2 figures, 1 table for algorithm", "journal-ref": null, "doi": "10.5121/ijma.2011.3305", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video transmission over mobile ad-hoc networks is becoming important as these\nnetworks become more widely used in the wireless networks. We propose a\nrouting-aware multiple description video coding approach to support video\ntransmission over mobile ad-hoc networks with single and multiple path\ntransport. We build a model to estimate the packet loss probability of each\npacket transmitted over the network based on the standard ad-hoc routing\nmessages and network parameters without losing the RERR message. We then\ncalculate the frame loss probability in order to eliminate error without any\nloss of data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 20:03:32 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Shah", "Kinjal", ""], ["Dua", "Gagan", ""], ["Sharma", "Dharmendar", ""], ["Mishra", "Priyanka", ""], ["Rakesh", "Nitin", ""]]}, {"id": "1109.0776", "submitter": "EPTCS", "authors": "Lucas Beyak, Jacques Carette (McMaster University)", "title": "SAGA: A DSL for Story Management", "comments": "In Proceedings DSL 2011, arXiv:1109.0323", "journal-ref": "EPTCS 66, 2011, pp. 48-67", "doi": "10.4204/EPTCS.66.3", "report-no": null, "categories": "cs.PL cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video game development is currently a very labour-intensive endeavour.\nFurthermore it involves multi-disciplinary teams of artistic content creators\nand programmers, whose typical working patterns are not easily meshed. SAGA is\nour first effort at augmenting the productivity of such teams.\n  Already convinced of the benefits of DSLs, we set out to analyze the domains\npresent in games in order to find out which would be most amenable to the DSL\napproach. Based on previous work, we thus sought those sub-parts that already\nhad a partially established vocabulary and at the same time could be well\nmodeled using classical computer science structures. We settled on the 'story'\naspect of video games as the best candidate domain, which can be modeled using\nstate transition systems.\n  As we are working with a specific company as the ultimate customer for this\nwork, an additional requirement was that our DSL should produce code that can\nbe used within a pre-existing framework. We developed a full system (SAGA)\ncomprised of a parser for a human-friendly language for 'story events', an\ninternal representation of design patterns for implementing object-oriented\nstate-transitions systems, an instantiator for these patterns for a specific\n'story', and three renderers (for C++, C# and Java) for the instantiated\nabstract code.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 01:56:19 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Beyak", "Lucas", "", "McMaster University"], ["Carette", "Jacques", "", "McMaster University"]]}, {"id": "1109.1145", "submitter": "Pravin Kamde Mr", "authors": "Pravin M. Kamde, Dr. Siddu. P. Algur", "title": "A Survey on Web Multimedia Mining", "comments": "13 Pages; The International Journal of Multimedia & Its Applications\n  (IJMA) Vol.3, No.3, August 2011", "journal-ref": null, "doi": "10.5121/ijma", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern developments in digital media technologies has made transmitting and\nstoring large amounts of multi/rich media data (e.g. text, images, music, video\nand their combination) more feasible and affordable than ever before. However,\nthe state of the art techniques to process, mining and manage those rich media\nare still in their infancy. Advances developments in multimedia acquisition and\nstorage technology the rapid progress has led to the fast growing incredible\namount of data stored in databases. Useful information to users can be revealed\nif these multimedia files are analyzed. Multimedia mining deals with the\nextraction of implicit knowledge, multimedia data relationships, or other\npatterns not explicitly stored in multimedia files. Also in retrieval, indexing\nand classification of multimedia data with efficient information fusion of the\ndifferent modalities is essential for the system's overall performance. The\npurpose of this paper is to provide a systematic overview of multimedia mining.\nThis article is also represents the issues in the application process component\nfor multimedia mining followed by the multimedia mining models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 11:12:55 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Kamde", "Pravin M.", ""], ["Algur", "Dr. Siddu. P.", ""]]}, {"id": "1109.1265", "submitter": "Rui Costa", "authors": "Rui A. Costa and Diogo Ferreira and Jo\\~ao Barros", "title": "FEBER: Feedback Based Erasure Recovery for Real-Time Multicast over\n  802.11 Networks", "comments": "This paper has been withdrawn due to changes in the results obtained\n  in a different testbed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of broadcasting data streams over a wireless network\nfor multiple receivers with reliability and timely delivery guarantees. In our\nframework, we consider packets that need to be delivered within a given time\ninterval, after which the packet is no longer useful at the application layer.\nWe set the notion of critical packet and, based on periodic feedback from the\nreceivers, we propose a retransmission scheme that will guarantee timely\ndelivery of such packets, as well as packets that are innovative for other\nreceivers. Our solution provides a trade-off between packet delivery ratio and\nbandwidth use, which contrasts with existing approaches such as FEC and ARQ,\nwhere the focus is on ensuring reliability first, offering no guarantees of\ntimely delivery of data. We evaluate the performance of our proposal in a\n802.11 wireless network testbed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 18:53:27 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2011 15:25:37 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2012 10:20:21 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Costa", "Rui A.", ""], ["Ferreira", "Diogo", ""], ["Barros", "Jo\u00e3o", ""]]}, {"id": "1109.1583", "submitter": "Trong Duong Quoc", "authors": "Trong Duong Quoc, Heiko Perkuhn, Daniel Catrein, Uwe Naumann and Toni\n  Anwar", "title": "Optimization and Evaluation of a Multimedia Streaming Service on Hybrid\n  Telco cloud", "comments": "20 pages; International Journal on Cloud Computing: Services and\n  Architecture (IJCCSA), 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent developments in cloud computing, a paradigm shift from rather\nstatic deployment of resources to more dynamic, on-demand practices means more\nflexibility and better utilization of resources. This demands new ways to\nefficiently configure networks.\n  In this paper, we will characterize a class of competitive cloud services\nthat telecom operators could provide based on the characteristics of telecom\ninfrastructure through an applicable streaming service architecture. Then, we\nwill model this architecture as a cost-based mathematic model. This model\nprovides a tool to evaluate and compare the cost of software services for\ndifferent telecom network topologies and deployment strategies. Additionally,\nwith each topology it acts as a means to characterize the deployment solution\nthat yields the lowest resource usage over the entire network. These\napplications are illustrated through numerical analysis. Finally, a\nproof-of-concept prototype is deployed to shows dynamic properties of the\nservice in the architecture and the model above.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 21:34:26 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Quoc", "Trong Duong", ""], ["Perkuhn", "Heiko", ""], ["Catrein", "Daniel", ""], ["Naumann", "Uwe", ""], ["Anwar", "Toni", ""]]}, {"id": "1109.2325", "submitter": "Baisa Gunjal", "authors": "Baisa L. Gunjal and Suresh N.Mali", "title": "Secured color image watermarking technique in DWT-DCT domain", "comments": "9 pages; International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol.1, No. 3, August 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The multilayer secured DWT-DCT and YIQ color space based image watermarking\ntechnique with robustness and better correlation is presented here. The\nsecurity levels are increased by using multiple pn sequences, Arnold\nscrambling, DWT domain, DCT domain and color space conversions. Peak signal to\nnoise ratio and Normalized correlations are used as measurement metrics. The\n512x512 sized color images with different histograms are used for testing and\nwatermark of size 64x64 is embedded in HL region of DWT and 4x4 DCT is used.\n'Haar' wavelet is used for decomposition and direct flexing factor is used. We\ngot PSNR value is 63.9988 for flexing factor k=1 for Lena image and the maximum\nNC 0.9781 for flexing factor k=4 in Q color space. The comparative performance\nin Y, I and Q color space is presented. The technique is robust for different\nattacks like scaling, compression, rotation etc.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 16:24:00 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Gunjal", "Baisa L.", ""], ["Mali", "Suresh N.", ""]]}, {"id": "1109.4979", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Yuxin Peng", "title": "Latent Semantic Learning with Structured Sparse Representation for Human\n  Action Recognition", "comments": "The short version of this paper appears in ICCV 2011", "journal-ref": null, "doi": "10.1016/j.patcog.2012.09.027", "report-no": null, "categories": "cs.MM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel latent semantic learning method for extracting\nhigh-level features (i.e. latent semantics) from a large vocabulary of abundant\nmid-level features (i.e. visual keywords) with structured sparse\nrepresentation, which can help to bridge the semantic gap in the challenging\ntask of human action recognition. To discover the manifold structure of\nmidlevel features, we develop a spectral embedding approach to latent semantic\nlearning based on L1-graph, without the need to tune any parameter for graph\nconstruction as a key step of manifold learning. More importantly, we construct\nthe L1-graph with structured sparse representation, which can be obtained by\nstructured sparse coding with its structured sparsity ensured by novel L1-norm\nhypergraph regularization over mid-level features. In the new embedding space,\nwe learn latent semantics automatically from abundant mid-level features\nthrough spectral clustering. The learnt latent semantics can be readily used\nfor human action recognition with SVM by defining a histogram intersection\nkernel. Different from the traditional latent semantic analysis based on topic\nmodels, our latent semantic learning method can explore the manifold structure\nof mid-level features in both L1-graph construction and spectral embedding,\nwhich results in compact but discriminative high-level features. The\nexperimental results on the commonly used KTH action dataset and unconstrained\nYouTube action dataset show the superior performance of our method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 00:39:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lu", "Zhiwu", ""], ["Peng", "Yuxin", ""]]}, {"id": "1109.6297", "submitter": "Ignacio Ramirez", "authors": "Ignacio Ram\\'irez and Guillermo Sapiro", "title": "Low-rank data modeling via the Minimum Description Length principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust low-rank matrix estimation is a topic of increasing interest, with\npromising applications in a variety of fields, from computer vision to data\nmining and recommender systems. Recent theoretical results establish the\nability of such data models to recover the true underlying low-rank matrix when\na large portion of the measured matrix is either missing or arbitrarily\ncorrupted. However, if low rank is not a hypothesis about the true nature of\nthe data, but a device for extracting regularity from it, no current guidelines\nexist for choosing the rank of the estimated matrix. In this work we address\nthis problem by means of the Minimum Description Length (MDL) principle -- a\nwell established information-theoretic approach to statistical inference -- as\na guideline for selecting a model for the data at hand. We demonstrate the\npractical usefulness of our formal approach with results for complex background\nextraction in video sequences.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 18:56:22 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Ram\u00edrez", "Ignacio", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1109.6851", "submitter": "Mohammad Sadegh Talebi", "authors": "Mohammad Hassan Hajiesmaili, Ali Sehati, Ahmad Khonsari and Mohammad\n  Sadegh Talebi", "title": "Content-Aware Rate Control for Video Transmission with Buffer\n  Constraints in Multipath Networks", "comments": "This paper has been withdrawn by the last author since there is a\n  minor change in the list of authors. In the new version, the last author is\n  not included in the paper any longer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being an integral part of the network traffic, nowadays it's vital to design\nrobust mechanisms to provide QoS for multimedia applications. The main goal of\nthis paper is to provide an efficient solution to support content-aware video\ntransmission mechanism with buffer underflow avoidance at the receiver in\nmultipath networks. Towards this, we introduce a content-aware time-varying\nutility function, where the quality impacts of video content is incorporated\ninto its definition. Using the proposed utility function, we formulate a\nmultipath Dynamic Network Utility Maximization (DNUM) problem for the rate\nallocation of video streams, where it takes into account QoS demand of video\nstreams in terms of buffer underflow avoidance. Finally, using primal-dual\nmethod, we propose a distributed solution that optimally allocates the shared\nbandwidth to video streams. The numerical examples demonstrate the efficacy of\nthe proposed content-aware rate allocation algorithm for video sources in both\nsingle and multiple path network models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 15:26:30 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2012 16:27:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hajiesmaili", "Mohammad Hassan", ""], ["Sehati", "Ali", ""], ["Khonsari", "Ahmad", ""], ["Talebi", "Mohammad Sadegh", ""]]}, {"id": "1109.6862", "submitter": "Pravin Kamde Mr", "authors": "Sankirti S. and P. M. Kamade", "title": "Video OCR for Video Indexing", "comments": "3 Pages", "journal-ref": "IACSIT International Journal of Engineering and Technology, Vol.3,\n  No.3, June 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video OCR is a technique that can greatly help to locate the topics of\ninterest in video via the automatic extraction and reading of captions and\nannotations. Text in video can provide key indexing information. Recognizing\nsuch text for search application is critical. Major difficult problem for\ncharacter recognition for videos is degraded and deformated characters, low\nresolution characters or very complex background. To tackle the problem\npreprocessing on text image plays vital role. Most of the OCR engines are\nworking on the binary image so to find a better binarization procedure for\nimage to get a desired result is important.Accurate binarization process\nminimizes the error rate of video OCR.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 15:51:12 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["S.", "Sankirti", ""], ["Kamade", "P. M.", ""]]}]