[{"id": "1708.00223", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Shengfeng He, Linchao Bao and Qingxiong\n  Yang", "title": "Learning to Hallucinate Face Images via Component Generation and\n  Enhancement", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sr/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage method for face hallucination. First, we generate\nfacial components of the input image using CNNs. These components represent the\nbasic facial structures. Second, we synthesize fine-grained facial structures\nfrom high resolution training images. The details of these structures are\ntransferred into facial components for enhancement. Therefore, we generate\nfacial components to approximate ground truth global appearance in the first\nstage and enhance them through recovering details in the second stage. The\nexperiments demonstrate that our method performs favorably against\nstate-of-the-art methods\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["He", "Shengfeng", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00224", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang", "title": "Fast Preprocessing for Robust Face Sketch Synthesis", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sketch/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based face sketch synthesis methods usually meet the challenging\nproblem that input photos are captured in different lighting conditions from\ntraining photos. The critical step causing the failure is the search of similar\npatch candidates for an input photo patch. Conventional illumination invariant\npatch distances are adopted rather than directly relying on pixel intensity\ndifference, but they will fail when local contrast within a patch changes. In\nthis paper, we propose a fast preprocessing method named Bidirectional\nLuminance Remapping (BLR), which interactively adjust the lighting of training\nand input photos. Our method can be directly integrated into state-of-the-art\nexemplar-based methods to improve their robustness with ignorable computational\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00225", "submitter": "Yibing Song", "authors": "Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson Lau, Ming-Hsuan\n  Yang", "title": "CREST: Convolutional Residual Learning for Visual Tracking", "comments": "ICCV 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filters (DCFs) have been shown to perform\nsuperiorly in visual tracking. They only need a small set of training samples\nfrom the initial frame to generate an appearance model. However, existing DCFs\nlearn the filters separately from feature extraction, and update these filters\nusing a moving average operation with an empirical weight. These DCF trackers\nhardly benefit from the end-to-end training. In this paper, we propose the\nCREST algorithm to reformulate DCFs as a one-layer convolutional neural\nnetwork. Our method integrates feature extraction, response map generation as\nwell as model update into the neural networks for an end-to-end training. To\nreduce model degradation during online update, we apply residual learning to\ntake appearance changes into account. Extensive experiments on the benchmark\ndatasets demonstrate that our CREST tracker performs favorably against\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:47:20 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Gong", "Lijun", ""], ["Zhang", "Jiawei", ""], ["Lau", "Rynson", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.00255", "submitter": "Xiang Chen", "authors": "Xiang Chen, Bowei Chen, Mohan Kankanhalli", "title": "MM2RTB: Bringing Multimedia Metrics to Real-Time Bidding", "comments": "In Proceedings of AdKDD and TargetAd, Halifax, NS, Canada, August,\n  14, 2017, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In display advertising, users' online ad experiences are important for the\nadvertising effectiveness. However, users have not been well accommodated in\nreal-time bidding (RTB). This further influences their site visits and\nperception of the displayed banner ads. In this paper, we propose a novel\ncomputational framework which brings multimedia metrics, like the contextual\nrelevance, the visual saliency and the ad memorability into RTB to improve the\nusers' ad experiences as well as maintain the benefits of the publisher and the\nadvertiser. We aim at developing a vigorous ecosystem by optimizing the\ntrade-offs among all stakeholders. The framework considers the scenario of a\nwebpage with multiple ad slots. Our experimental results show that the benefits\nof the advertiser and the user can be significantly improved if the publisher\nwould slightly sacrifice his short-term revenue. The improved benefits will\nincrease the advertising requests (demand) and the site visits (supply), which\ncan further boost the publisher's revenue in the long run.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:36:57 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Chen", "Xiang", ""], ["Chen", "Bowei", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1708.00684", "submitter": "Gjorgji Strezoski", "authors": "Gjorgji Strezoski, Marcel Worring", "title": "OmniArt: Multi-task Deep Learning for Artistic Data Analysis", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast amounts of artistic data is scattered on-line from both museums and art\napplications. Collecting, processing and studying it with respect to all\naccompanying attributes is an expensive process. With a motivation to speed up\nand improve the quality of categorical analysis in the artistic domain, in this\npaper we propose an efficient and accurate method for multi-task learning with\na shared representation applied in the artistic domain. We continue to show how\ndifferent multi-task configurations of our method behave on artistic data and\noutperform handcrafted feature approaches as well as convolutional neural\nnetworks. In addition to the method and analysis, we propose a challenge like\nnature to the new aggregated data set with almost half a million samples and\nstructured meta-data to encourage further research and societal engagement.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 10:20:22 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Strezoski", "Gjorgji", ""], ["Worring", "Marcel", ""]]}, {"id": "1708.00930", "submitter": "Paolo Bestagini", "authors": "Mauro Barni, Luca Bondi, Nicol\\`o Bonettini, Paolo Bestagini, Andrea\n  Costanzo, Marco Maggini, Benedetta Tondi, Stefano Tubaro", "title": "Aligned and Non-Aligned Double JPEG Detection Using Convolutional Neural\n  Networks", "comments": "Submitted to Journal of Visual Communication and Image Representation\n  (first submission: March 20, 2017; second submission: August 2, 2017)", "journal-ref": null, "doi": "10.1016/j.jvcir.2017.09.003", "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide diffusion of JPEG coding standard, the image forensic\ncommunity has devoted significant attention to the development of double JPEG\n(DJPEG) compression detectors through the years. The ability of detecting\nwhether an image has been compressed twice provides paramount information\ntoward image authenticity assessment. Given the trend recently gained by\nconvolutional neural networks (CNN) in many computer vision tasks, in this\npaper we propose to use CNNs for aligned and non-aligned double JPEG\ncompression detection. In particular, we explore the capability of CNNs to\ncapture DJPEG artifacts directly from images. Results show that the proposed\nCNN-based detectors achieve good performance even with small size images (i.e.,\n64x64), outperforming state-of-the-art solutions, especially in the non-aligned\ncase. Besides, good results are also achieved in the commonly-recognized\nchallenging case in which the first quality factor is larger than the second\none.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 21:11:27 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Barni", "Mauro", ""], ["Bondi", "Luca", ""], ["Bonettini", "Nicol\u00f2", ""], ["Bestagini", "Paolo", ""], ["Costanzo", "Andrea", ""], ["Maggini", "Marco", ""], ["Tondi", "Benedetta", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1708.00973", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli", "title": "Attention Transfer from Web Images for Video Recognition", "comments": "ACM Multimedia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep learning based video classifiers for action recognition\nrequires a large amount of labeled videos. The labeling process is\nlabor-intensive and time-consuming. On the other hand, large amount of\nweakly-labeled images are uploaded to the Internet by users everyday. To\nharness the rich and highly diverse set of Web images, a scalable approach is\nto crawl these images to train deep learning based classifier, such as\nConvolutional Neural Networks (CNN). However, due to the domain shift problem,\nthe performance of Web images trained deep classifiers tend to degrade when\ndirectly deployed to videos. One way to address this problem is to fine-tune\nthe trained models on videos, but sufficient amount of annotated videos are\nstill required. In this work, we propose a novel approach to transfer knowledge\nfrom image domain to video domain. The proposed method can adapt to the target\ndomain (i.e. video data) with limited amount of training data. Our method maps\nthe video frames into a low-dimensional feature space using the\nclass-discriminative spatial attention map for CNNs. We design a novel Siamese\nEnergyNet structure to learn energy functions on the attention maps by jointly\noptimizing two loss functions, such that the attention map corresponding to a\nground truth concept would have higher energy. We conduct extensive experiments\non two challenging video recognition datasets (i.e. TVHI and UCF101), and\ndemonstrate the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 02:41:15 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1708.01292", "submitter": "Cristina Segalin", "authors": "Cristina Segalin, Fabio Celli, Luca Polonio, Michal Kosinski, David\n  Stillwell, Nicu Sebe, Marco Cristani, Bruno Lepri", "title": "What your Facebook Profile Picture Reveals about your Personality", "comments": null, "journal-ref": null, "doi": "10.1145/3123266.3123331", "report-no": null, "categories": "cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People spend considerable effort managing the impressions they give others.\nSocial psychologists have shown that people manage these impressions\ndifferently depending upon their personality. Facebook and other social media\nprovide a new forum for this fundamental process; hence, understanding people's\nbehaviour on social media could provide interesting insights on their\npersonality. In this paper we investigate automatic personality recognition\nfrom Facebook profile pictures. We analyze the effectiveness of four families\nof visual features and we discuss some human interpretable patterns that\nexplain the personality traits of the individuals. For example, extroverts and\nagreeable individuals tend to have warm colored pictures and to exhibit many\nfaces in their portraits, mirroring their inclination to socialize; while\nneurotic ones have a prevalence of pictures of indoor places. Then, we propose\na classification approach to automatically recognize personality traits from\nthese visual features. Finally, we compare the performance of our\nclassification approach to the one obtained by human raters and we show that\ncomputer-based classifications are significantly more accurate than averaged\nhuman-based classifications for Extraversion and Neuroticism.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 19:58:36 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 07:51:17 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Segalin", "Cristina", ""], ["Celli", "Fabio", ""], ["Polonio", "Luca", ""], ["Kosinski", "Michal", ""], ["Stillwell", "David", ""], ["Sebe", "Nicu", ""], ["Cristani", "Marco", ""], ["Lepri", "Bruno", ""]]}, {"id": "1708.02100", "submitter": "Andreas Arzt", "authors": "Andreas Arzt and Matthias Dorfer", "title": "Aktuelle Entwicklungen in der Automatischen Musikverfolgung", "comments": "In German. Published in Maximilian Eibl, Martin Gaedke (Hrsg.):\n  INFORMATIK 2017. Lecture Notes in Informatics (LNI), Gesellschaft f\\\"ur\n  Informatik, Bonn 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present current trends in real-time music tracking (a.k.a.\nscore following). Casually speaking, these algorithms \"listen\" to a live\nperformance of music, compare the audio signal to an abstract representation of\nthe score, and \"read\" along in the sheet music. In this way at any given time\nthe exact position of the musician(s) in the sheet music is computed. Here, we\nfocus on the aspects of flexibility and usability of these algorithms. This\ncomprises work on automatic identification and flexible tracking of the piece\nbeing played as well as current approaches based on Deep Learning. The latter\nenables direct learning of correspondences between complex audio data and\nimages of the sheet music, avoiding the complicated and time-consuming\ndefinition of a mid-level representation.\n  -----\n  Diese Arbeit befasst sich mit aktuellen Entwicklungen in der automatischen\nMusikverfolgung durch den Computer. Es handelt sich dabei um Algorithmen, die\neiner musikalischen Auff\\\"uhrung \"zuh\\\"oren\", das aufgenommene Audiosignal mit\neiner (abstrakten) Repr\\\"asentation des Notentextes vergleichen und sozusagen\nin diesem mitlesen. Der Algorithmus kennt also zu jedem Zeitpunkt die Position\nder Musiker im Notentext. Neben der Vermittlung eines generellen \\\"Uberblicks,\nliegt der Schwerpunkt dieser Arbeit auf der Beleuchtung des Aspekts der\nFlexibilit\\\"at und der einfacheren Nutzbarkeit dieser Algorithmen. Es wird\ndargelegt, welche Schritte get\\\"atigt wurden (und aktuell get\\\"atigt werden) um\nden Prozess der automatischen Musikverfolgung einfacher zug\\\"anglich zu machen.\nDies umfasst Arbeiten zur automatischen Identifikation von gespielten St\\\"ucken\nund deren flexible Verfolgung ebenso wie aktuelle Ans\\\"atze mithilfe von Deep\nLearning, die es erlauben Bild und Ton direkt zu verbinden, ohne Umwege \\\"uber\nabstrakte und nur unter gro{\\ss}em Zeitaufwand zu erstellende\nZwischenrepr\\\"asentationen.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 13:00:28 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Arzt", "Andreas", ""], ["Dorfer", "Matthias", ""]]}, {"id": "1708.02840", "submitter": "Pawel Cyrta", "authors": "Pawel Cyrta, Tomasz Trzci\\'nski, Wojciech Stokowiec", "title": "Speaker Diarization using Deep Recurrent Convolutional Neural Networks\n  for Speaker Embeddings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-67220-5_10", "report-no": null, "categories": "cs.SD cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method of speaker diarization that employs a\ndeep learning architecture to learn speaker embeddings. In contrast to the\ntraditional approaches that build their speaker embeddings using manually\nhand-crafted spectral features, we propose to train for this purpose a\nrecurrent convolutional neural network applied directly on magnitude\nspectrograms. To compare our approach with the state of the art, we collect and\nrelease for the public an additional dataset of over 6 hours of fully annotated\nbroadcast material. The results of our evaluation on the new dataset and three\nother benchmark datasets show that our proposed method significantly\noutperforms the competitors and reduces diarization error rate by a large\nmargin of over 30% with respect to the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:53:01 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 13:49:45 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Cyrta", "Pawel", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Stokowiec", "Wojciech", ""]]}, {"id": "1708.02859", "submitter": "Abbas Mehrabi", "authors": "Abbas Mehrabi, Matti Siekkinen, and Antti Yl\\\"a-J\\\"a\\\"aski", "title": "Joint Optimization of QoE and Fairness Through Network Assisted Adaptive\n  Mobile Video Streaming", "comments": "Accepted, IEEE International Conference on Wireless and Mobile\n  Computing, Networking and Communications (WiMob) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MPEG has recently proposed Server and Network Assisted Dynamic Adaptive\nStreaming over HTTP (SAND-DASH) for video streaming over the Internet. In\ncontrast to the purely client-based video streaming in which each client makes\nits own decision to adjust its bitrate, SAND-DASH enables a group of\nsimultaneous clients to select their bitrates in a coordinated fashion in order\nto improve resource utilization and quality of experience. In this paper, we\nstudy the performance of such an adaptation strategy compared to the\ntraditional approach with large number of clients having mobile Internet\naccess. We propose a multi-servers multi-coordinators (MSs-MCs) framework to\nmodel groups of remote clients accessing video content replicated to spatially\ndistributed edge servers. We then formulate an optimization problem to maximize\njointly the QoE of individual clients, proportional fairness in allocating the\nlimited resources of base stations as well as balancing the utilized resources\namong multiple serves. We then present an efficient heuristic-based solution to\nthe problem and perform simulations in order to explore parameter space of the\nscheme as well as to compare the performance to purely client-based DASH.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 14:46:57 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mehrabi", "Abbas", ""], ["Siekkinen", "Matti", ""], ["Yl\u00e4-J\u00e4\u00e4ski", "Antti", ""]]}, {"id": "1708.02898", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, Herv\\'e J\\'egou and Jeff Johnson", "title": "An evaluation of large-scale methods for image instance and class\n  discovery", "comments": "Published at ACM Multimedia workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at discovering meaningful subsets of related images from\nlarge image collections without annotations. We search groups of images related\nat different levels of semantic, i.e., either instances or visual classes.\nWhile k-means is usually considered as the gold standard for this task, we\nevaluate and show the interest of diffusion methods that have been neglected by\nthe state of the art, such as the Markov Clustering algorithm.\n  We report results on the ImageNet and the Paris500k instance dataset, both\nenlarged with images from YFCC100M. We evaluate our methods with a labelling\ncost that reflects how much effort a human would require to correct the\ngenerated clusters.\n  Our analysis highlights several properties. First, when powered with an\nefficient GPU implementation, the cost of the discovery process is small\ncompared to computing the image descriptors, even for collections as large as\n100 million images. Second, we show that descriptions selected for instance\nsearch improve the discovery of object classes. Third, the Markov Clustering\ntechnique consistently outperforms other methods; to our knowledge it has never\nbeen considered in this large scale scenario.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:29:21 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Johnson", "Jeff", ""]]}, {"id": "1708.02991", "submitter": "Nematollah Zarmehi", "authors": "Nematollah Zarmehi and Mohammad Javad Barikbin", "title": "Robust Video Watermarking against H.264 and H.265 Compression Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a robust watermarking method for uncompressed video data\nagainst H.264/AVC and H.265/HEVC compression standards. We embed the watermark\ndata in the mid-range transform coefficients of a block that is less similar to\nits corresponding block in the previous and next frames. This idea makes the\nwatermark robust against the compression standards that use the inter\nprediction technique. The last two video compression standards also use inter\nprediction for motion compensation like previous video compression standards.\nTherefore, the proposed method is also well suited with these standards.\nSimulation results show the adequate robustness and transparency of our\nwatermarking scheme.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:59:49 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Zarmehi", "Nematollah", ""], ["Barikbin", "Mohammad Javad", ""]]}, {"id": "1708.03880", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Weisi Lin, Shiqi Wang, Long Xu, Leida Li", "title": "Image Quality Assessment Guided Deep Neural Networks Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many computer vision problems, the deep neural networks are trained and\nvalidated based on the assumption that the input images are pristine (i.e.,\nartifact-free). However, digital images are subject to a wide range of\ndistortions in real application scenarios, while the practical issues regarding\nimage quality in high level visual information understanding have been largely\nignored. In this paper, in view of the fact that most widely deployed deep\nlearning models are susceptible to various image distortions, the distorted\nimages are involved for data augmentation in the deep neural network training\nprocess to learn a reliable model for practical applications. In particular, an\nimage quality assessment based label smoothing method, which aims at\nregularizing the label distribution of training images, is further proposed to\ntune the objective functions in learning the neural network. Experimental\nresults show that the proposed method is effective in dealing with both low and\nhigh quality images in the typical image classification task.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 09:51:07 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Zhuo", ""], ["Lin", "Weisi", ""], ["Wang", "Shiqi", ""], ["Xu", "Long", ""], ["Li", "Leida", ""]]}, {"id": "1708.04078", "submitter": "Fang-Zhou Jiang", "authors": "Fang-Zhou Jiang, Kanchana Thilakarathna, Sirine Mrabet, Mohamed Ali\n  Kaafar, and Aruna Seneviratne", "title": "uStash: a Novel Mobile Content Delivery System for Improving User QoE in\n  Public Transport", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile data traffic is growing exponentially and it is even more challenging\nto distribute content efficiently while users are \"on the move\" such as in\npublic transport.The use of mobile devices for accessing content (e.g. videos)\nwhile commuting are both expensive and unreliable, although it is becoming\ncommon practice worldwide. Leveraging on the spatial and temporal correlation\nof content popularity and users' diverse network connectivity, we propose a\nnovel content distribution system, \\textit{uStash}, which guarantees better QoE\nwith regards to access delays and cost of usage. The proposed collaborative\ndownload and content stashing schemes provide the uStash provider the\nflexibility to control the cost of content access via cellular networks. We\nmodel the uStash system in a probabilistic framework and thereby analytically\nderive the optimal portions for collaborative downloading. Then, we validate\nthe proposed models using real-life trace driven simulations. In particular, we\nuse dataset from 22 inter-city buses running on 6 different routes and from a\nmobile VoD service provider to show that uStash reduces the cost of monthly\ncellular data by approximately 50\\% and the expected delay for content access\nby 60\\% compared to content downloaded via users' cellular network connections.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 11:28:13 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Jiang", "Fang-Zhou", ""], ["Thilakarathna", "Kanchana", ""], ["Mrabet", "Sirine", ""], ["Kaafar", "Mohamed Ali", ""], ["Seneviratne", "Aruna", ""]]}, {"id": "1708.04301", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao, Andrew Clark and Radha Poovendran", "title": "Attacking Automatic Video Analysis Algorithms: A Case Study of Google\n  Cloud Video Intelligence API", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growth of video data on Internet, automatic video analysis has\ngained a lot of attention from academia as well as companies such as Facebook,\nTwitter and Google. In this paper, we examine the robustness of video analysis\nalgorithms in adversarial settings. Specifically, we propose targeted attacks\non two fundamental classes of video analysis algorithms, namely video\nclassification and shot detection. We show that an adversary can subtly\nmanipulate a video in such a way that a human observer would perceive the\ncontent of the original video, but the video analysis algorithm will return the\nadversary's desired outputs.\n  We then apply the attacks on the recently released Google Cloud Video\nIntelligence API. The API takes a video file and returns the video labels\n(objects within the video), shot changes (scene changes within the video) and\nshot labels (description of video events over time). Through experiments, we\nshow that the API generates video and shot labels by processing only the first\nframe of every second of the video. Hence, an adversary can deceive the API to\noutput only her desired video and shot labels by periodically inserting an\nimage into the video at the rate of one frame per second. We also show that the\npattern of shot changes returned by the API can be mostly recovered by an\nalgorithm that compares the histograms of consecutive frames. Based on our\nequivalent model, we develop a method for slightly modifying the video frames,\nin order to deceive the API into generating our desired pattern of shot\nchanges. We perform extensive experiments with different videos and show that\nour attacks are consistently successful across videos with different\ncharacteristics. At the end, we propose introducing randomness to video\nanalysis algorithms as a countermeasure to our attacks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:10:04 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Clark", "Andrew", ""], ["Poovendran", "Radha", ""]]}, {"id": "1708.04308", "submitter": "Yuxin Peng", "authors": "Xin Huang, Yuxin Peng and Mingkuan Yuan", "title": "MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal\n  Retrieval", "comments": "12 pages, submitted to IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval has drawn wide interest for retrieval across different\nmodalities of data. However, existing methods based on DNN face the challenge\nof insufficient cross-modal training data, which limits the training\neffectiveness and easily leads to overfitting. Transfer learning is for\nrelieving the problem of insufficient training data, but it mainly focuses on\nknowledge transfer only from large-scale datasets as single-modal source domain\nto single-modal target domain. Such large-scale single-modal datasets also\ncontain rich modal-independent semantic knowledge that can be shared across\ndifferent modalities. Besides, large-scale cross-modal datasets are very\nlabor-consuming to collect and label, so it is significant to fully exploit the\nknowledge in single-modal datasets for boosting cross-modal retrieval. This\npaper proposes modal-adversarial hybrid transfer network (MHTN), which to the\nbest of our knowledge is the first work to realize knowledge transfer from\nsingle-modal source domain to cross-modal target domain, and learn cross-modal\ncommon representation. It is an end-to-end architecture with two subnetworks:\n(1) Modal-sharing knowledge transfer subnetwork is proposed to jointly transfer\nknowledge from a large-scale single-modal dataset in source domain to all\nmodalities in target domain with a star network structure, which distills\nmodal-independent supplementary knowledge for promoting cross-modal common\nrepresentation learning. (2) Modal-adversarial semantic learning subnetwork is\nproposed to construct an adversarial training mechanism between common\nrepresentation generator and modality discriminator, making the common\nrepresentation discriminative for semantics but indiscriminative for modalities\nto enhance cross-modal semantic consistency during transfer process.\nComprehensive experiments on 4 widely-used datasets show its effectiveness and\ngenerality.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 07:50:52 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1708.04776", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Jinwei Qi and Yuxin Yuan", "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent\n  Attention Network", "comments": "13 pages, submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, cross-modal retrieval plays an indispensable role to flexibly find\ninformation across different modalities of data. Effectively measuring the\nsimilarity between different modalities of data is the key of cross-modal\nretrieval. Different modalities such as image and text have imbalanced and\ncomplementary relationships, which contain unequal amount of information when\ndescribing the same semantics. For example, images often contain more details\nthat cannot be demonstrated by textual descriptions and vice versa. Existing\nworks based on Deep Neural Network (DNN) mostly construct one common space for\ndifferent modalities to find the latent alignments between them, which lose\ntheir exclusive modality-specific characteristics. Different from the existing\nworks, we propose modality-specific cross-modal similarity measurement (MCSM)\napproach by constructing independent semantic space for each modality, which\nadopts end-to-end framework to directly generate modality-specific cross-modal\nsimilarity without explicit common representation. For each semantic space,\nmodality-specific characteristics within one modality are fully exploited by\nrecurrent attention network, while the data of another modality is projected\ninto this space with attention based joint embedding to utilize the learned\nattention weights for guiding the fine-grained cross-modal correlation\nlearning, which can capture the imbalanced and complementary relationships\nbetween different modalities. Finally, the complementarity between the semantic\nspaces for different modalities is explored by adaptive fusion of the\nmodality-specific cross-modal similarities to perform cross-modal retrieval.\nExperiments on the widely-used Wikipedia and Pascal Sentence datasets as well\nas our constructed large-scale XMediaNet dataset verify the effectiveness of\nour proposed approach, outperforming 9 state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 05:43:54 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Peng", "Yuxin", ""], ["Qi", "Jinwei", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1708.05127", "submitter": "Di Hu", "authors": "Xuelong Li and Di Hu and Feiping Nie", "title": "Deep Binary Reconstruction for Cross-modal Hashing", "comments": "8 pages, 5 figures, accepted by ACM Multimedia 2017", "journal-ref": null, "doi": "10.1145/3123266.3123355", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand of massive multimodal data storage and\norganization, cross-modal retrieval based on hashing technique has drawn much\nattention nowadays. It takes the binary codes of one modality as the query to\nretrieve the relevant hashing codes of another modality. However, the existing\nbinary constraint makes it difficult to find the optimal cross-modal hashing\nfunction. Most approaches choose to relax the constraint and perform\nthresholding strategy on the real-value representation instead of directly\nsolving the original objective. In this paper, we first provide a concrete\nanalysis about the effectiveness of multimodal networks in preserving the\ninter- and intra-modal consistency. Based on the analysis, we provide a\nso-called Deep Binary Reconstruction (DBRC) network that can directly learn the\nbinary hashing codes in an unsupervised fashion. The superiority comes from a\nproposed simple but efficient activation function, named as Adaptive Tanh\n(ATanh). The ATanh function can adaptively learn the binary codes and be\ntrained via back-propagation. Extensive experiments on three benchmark datasets\ndemonstrate that DBRC outperforms several state-of-the-art methods in both\nimage2text and text2image retrieval task.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 04:05:58 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 01:35:54 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Xuelong", ""], ["Hu", "Di", ""], ["Nie", "Feiping", ""]]}, {"id": "1708.05291", "submitter": "Goncalo Mordido", "authors": "Gon\\c{c}alo Mordido, Jo\\~ao Magalh\\~aes and Sofia Cavaco", "title": "Automatic Organisation and Quality Analysis of User-Generated Content\n  with Audio Fingerprinting", "comments": "EUSIPCO 2017 - 25th European Signal Processing Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase of the quantity of user-generated content experienced in social\nmedia has boosted the importance of analysing and organising the content by its\nquality. Here, we propose a method that uses audio fingerprinting to organise\nand infer the quality of user-generated audio content. The proposed method\ndetects the overlapping segments between different audio clips to organise and\ncluster the data according to events, and to infer the audio quality of the\nsamples. A test setup with concert recordings manually crawled from YouTube is\nused to validate the presented method. The results show that the proposed\nmethod achieves better results than previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 14:08:09 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Mordido", "Gon\u00e7alo", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""], ["Cavaco", "Sofia", ""]]}, {"id": "1708.05302", "submitter": "Gon\\c{c}alo Mordido", "authors": "Gon\\c{c}alo Mordido, Jo\\~ao Magalh\\~aes, Sofia Cavaco", "title": "Automatic Organisation, Segmentation, and Filtering of User-Generated\n  Audio Content", "comments": "MMSP 2017 - IEEE 19th International Workshop on Multimedia Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using solely the information retrieved by audio fingerprinting techniques, we\npropose methods to treat a possibly large dataset of user-generated audio\ncontent, that (1) enable the grouping of several audio files that contain a\ncommon audio excerpt (i.e., are relative to the same event), and (2) give\ninformation about how those files are correlated in terms of time and quality\ninside each event. Furthermore, we use supervised learning to detect incorrect\nmatches that may arise from the audio fingerprinting algorithm itself, whilst\nensuring our model learns with previous predictions. All the presented methods\nwere further validated by user-generated recordings of several different\nconcerts manually crawled from YouTube.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 14:19:17 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Mordido", "Gon\u00e7alo", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""], ["Cavaco", "Sofia", ""]]}, {"id": "1708.05851", "submitter": "Di Hu", "authors": "Xuelong Li and Di Hu and Xiaoqiang Lu", "title": "Image2song: Song Retrieval via Bridging Image Content and Lyric Words", "comments": "13 pages, 13 figures, accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image is usually taken for expressing some kinds of emotions or purposes,\nsuch as love, celebrating Christmas. There is another better way that combines\nthe image and relevant song to amplify the expression, which has drawn much\nattention in the social network recently. Hence, the automatic selection of\nsongs should be expected. In this paper, we propose to retrieve semantic\nrelevant songs just by an image query, which is named as the image2song\nproblem. Motivated by the requirements of establishing correlation in\nsemantic/content, we build a semantic-based song retrieval framework, which\nlearns the correlation between image content and lyric words. This model uses a\nconvolutional neural network to generate rich tags from image regions, a\nrecurrent neural network to model lyric, and then establishes correlation via a\nmulti-layer perceptron. To reduce the content gap between image and lyric, we\npropose to make the lyric modeling focus on the main image content via a tag\nattention. We collect a dataset from the social-sharing multimodal data to\nstudy the proposed problem, which consists of (image, music clip, lyric)\ntriplets. We demonstrate that our proposed model shows noticeable results in\nthe image2song retrieval task and provides suitable songs. Besides, the\nsong2image task is also performed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 14:17:44 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Li", "Xuelong", ""], ["Hu", "Di", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1708.05922", "submitter": "Tuan Ho", "authors": "Tuan Ho, Ioannis Schizas, K. R. Rao, Madhukar Budagavi", "title": "360-degree Video Stitching for Dual-fisheye Lens Cameras Based On Rigid\n  Moving Least Squares", "comments": "Preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-fisheye lens cameras are becoming popular for 360-degree video capture,\nespecially for User-generated content (UGC), since they are affordable and\nportable. Images generated by the dual-fisheye cameras have limited overlap and\nhence require non-conventional stitching techniques to produce high-quality\n360x180-degree panoramas. This paper introduces a novel method to align these\nimages using interpolation grids based on rigid moving least squares.\nFurthermore, jitter is the critical issue arising when one applies the\nimage-based stitching algorithms to video. It stems from the unconstrained\nmovement of stitching boundary from one frame to another. Therefore, we also\npropose a new algorithm to maintain the temporal coherence of stitching\nboundary to provide jitter-free 360-degree videos. Results show that the method\nproposed in this paper can produce higher quality stitched images and videos\nthan prior work.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 02:10:00 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Ho", "Tuan", ""], ["Schizas", "Ioannis", ""], ["Rao", "K. R.", ""], ["Budagavi", "Madhukar", ""]]}, {"id": "1708.05970", "submitter": "Christophe Guyeux", "authors": "Christophe Guyeux and Jacques M. Bahi", "title": "An improved watermarking scheme for Internet applications", "comments": "Proceedings of INTERNET'2010, 2nd Int. Conf. on Evolving Internet.\n  Valencia (Spain), September 20-25, 2010. pp. 119-124", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a data hiding scheme ready for Internet applications is\nproposed. An existing scheme based on chaotic iterations is improved, to\nrespond to some major Internet security concerns, such as digital rights\nmanagement, communication over hidden channels, and social search engines. By\nusing Reed Solomon error correcting codes and wavelets domain, we show that\nthis data hiding scheme can be improved to solve issues and requirements raised\nby these Internet fields.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 14:22:48 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Guyeux", "Christophe", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1708.06039", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier\n  Giro-i-Nieto, Brendan Jou and Shih-Fu Chang", "title": "More cat than cute? Interpretable Prediction of Adjective-Noun Pairs", "comments": "Oral paper at ACM Multimedia 2017 Workshop on Multimodal\n  Understanding of Social, Affective and Subjective Attributes (MUSA2)", "journal-ref": null, "doi": "10.1145/3132515.3132520", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of affect-rich multimedia resources has bolstered\ninterest in understanding sentiment and emotions in and from visual content.\nAdjective-noun pairs (ANP) are a popular mid-level semantic construct for\ncapturing affect via visually detectable concepts such as \"cute dog\" or\n\"beautiful landscape\". Current state-of-the-art methods approach ANP prediction\nby considering each of these compound concepts as individual tokens, ignoring\nthe underlying relationships in ANPs. This work aims at disentangling the\ncontributions of the `adjectives' and `nouns' in the visual prediction of ANPs.\nTwo specialised classifiers, one trained for detecting adjectives and another\nfor nouns, are fused to predict 553 different ANPs. The resulting ANP\nprediction model is more interpretable as it allows us to study contributions\nof the adjective and noun components. Source code and models are available at\nhttps://imatge-upc.github.io/affective-2017-musa2/ .\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 00:33:05 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Fernandez", "Delia", ""], ["Woodward", "Alejandro", ""], ["Campos", "Victor", ""], ["Giro-i-Nieto", "Xavier", ""], ["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.06417", "submitter": "Lee Prangnell", "authors": "Lee Prangnell", "title": "Visually Lossless Coding in HEVC: A High Bit Depth and 4:4:4 Capable\n  JND-Based Perceptual Quantisation Technique for HEVC", "comments": "Preprint: Elsevier Signal Processing: Image Communication (Journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing prevalence of high bit depth and YCbCr 4:4:4 video\ndata, it is desirable to develop a JND-based visually lossless coding technique\nwhich can account for high bit depth 4:4:4 data in addition to standard 8-bit\nprecision chroma subsampled data. In this paper, we propose a Coding Block\n(CB)-level JND-based luma and chroma perceptual quantisation technique for HEVC\nnamed Pixel-PAQ. Pixel-PAQ exploits both luminance masking and chrominance\nmasking to achieve JND-based visually lossless coding; the proposed method is\ncompatible with high bit depth YCbCr 4:4:4 video data of any resolution. When\napplied to YCbCr 4:4:4 high bit depth video data, Pixel-PAQ can achieve vast\nbitrate reductions, of up to 75% (68.6% over four QP data points), compared\nwith a state-of-the-art luma-based JND method for HEVC named IDSQ. Moreover,\nthe participants in the subjective evaluations confirm that visually lossless\ncoding is successfully achieved by Pixel-PAQ (at a PSNR value of 28.04 dB in\none test).\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 20:46:54 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:04:14 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 09:51:32 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 08:54:43 GMT"}, {"version": "v5", "created": "Mon, 12 Feb 2018 18:44:15 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Prangnell", "Lee", ""]]}, {"id": "1708.06495", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Fumin Shen, Li Liu, Fan Zhu, Dongxiang Zhang,\n  and Heng-Tao Shen", "title": "Towards Automatic Construction of Diverse, High-quality Image Dataset", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of labeled image datasets has been shown critical for\nhigh-level image understanding, which continuously drives the progress of\nfeature designing and models developing. However, constructing labeled image\ndatasets is laborious and monotonous. To eliminate manual annotation, in this\nwork, we propose a novel image dataset construction framework by employing\nmultiple textual queries. We aim at collecting diverse and accurate images for\ngiven queries from the Web. Specifically, we formulate noisy textual queries\nremoving and noisy images filtering as a multi-view and multi-instance learning\nproblem separately. Our proposed approach not only improves the accuracy but\nalso enhances the diversity of the selected images. To verify the effectiveness\nof our proposed approach, we construct an image dataset with 100 categories.\nThe experiments show significant performance gains by using the generated data\nof our approach on several tasks, such as image classification, cross-dataset\ngeneralization, and object detection. The proposed method also consistently\noutperforms existing weakly supervised and web-supervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 04:36:12 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 07:08:08 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Shen", "Fumin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Zhang", "Dongxiang", ""], ["Shen", "Heng-Tao", ""]]}, {"id": "1708.06656", "submitter": "Zheyan Shen", "authors": "Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, Peixuan Chen", "title": "Causally Regularized Learning with Agnostic Data Selection Bias", "comments": "Oral paper of 2018 ACM Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240577", "report-no": null, "categories": "cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of previous machine learning algorithms are proposed based on the i.i.d.\nhypothesis. However, this ideal assumption is often violated in real\napplications, where selection bias may arise between training and testing\nprocess. Moreover, in many scenarios, the testing data is not even available\nduring the training process, which makes the traditional methods like transfer\nlearning infeasible due to their need on prior of test distribution. Therefore,\nhow to address the agnostic selection bias for robust model learning is of\nparamount importance for both academic research and real applications. In this\npaper, under the assumption that causal relationships among variables are\nrobust across domains, we incorporate causal technique into predictive modeling\nand propose a novel Causally Regularized Logistic Regression (CRLR) algorithm\nby jointly optimize global confounder balancing and weighted logistic\nregression. Global confounder balancing helps to identify causal features,\nwhose causal effect on outcome are stable across domains, then performing\nlogistic regression on those causal features constructs a robust predictive\nmodel against the agnostic bias. To validate the effectiveness of our CRLR\nalgorithm, we conduct comprehensive experiments on both synthetic and real\nworld datasets. Experimental results clearly demonstrate that our CRLR\nalgorithm outperforms the state-of-the-art methods, and the interpretability of\nour method can be fully depicted by the feature visualization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:49:07 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 16:33:36 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Shen", "Zheyan", ""], ["Cui", "Peng", ""], ["Kuang", "Kun", ""], ["Li", "Bo", ""], ["Chen", "Peixuan", ""]]}, {"id": "1708.06858", "submitter": "Yale Song", "authors": "Haojian Jin, Yale Song, Koji Yatani", "title": "ElasticPlay: Interactive Video Summarization with Dynamic Time Budgets", "comments": "ACM Multimedia 2017 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video consumption is being shifted from sit-and-watch to selective skimming.\nExisting video player interfaces, however, only provide indirect manipulation\nto support this emerging behavior. Video summarization alleviates this issue to\nsome extent, shortening a video based on the desired length of a summary as an\ninput variable. But an optimal length of a summarized video is often not\navailable in advance. Moreover, the user cannot edit the summary once it is\nproduced, limiting its practical applications. We argue that video\nsummarization should be an interactive, mixed-initiative process in which users\nhave control over the summarization procedure while algorithms help users\nachieve their goal via video understanding. In this paper, we introduce\nElasticPlay, a mixed-initiative approach that combines an advanced video\nsummarization technique with direct interface manipulation to help users\ncontrol the video summarization process. Users can specify a time budget for\nthe remaining content while watching a video; our system then immediately\nupdates the playback plan using our proposed cut-and-forward algorithm,\ndetermining which parts to skip or to fast-forward. This interactive process\nallows users to fine-tune the summarization result with immediate feedback. We\nshow that our system outperforms existing video summarization techniques on the\nTVSum50 dataset. We also report two lab studies (22 participants) and a\nMechanical Turk deployment study (60 participants), and show that the\nparticipants responded favorably to ElasticPlay.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 00:25:13 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Jin", "Haojian", ""], ["Song", "Yale", ""], ["Yatani", "Koji", ""]]}, {"id": "1708.07154", "submitter": "Fatih Kamisli", "authors": "Fatih Kamisli", "title": "Lossless Image and Intra-frame Compression with Integer-to-Integer DST", "comments": "Draft consisting of 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video coding standards are primarily designed for efficient lossy\ncompression, but it is also desirable to support efficient lossless compression\nwithin video coding standards using small modifications to the lossy coding\narchitecture. A simple approach is to skip transform and quantization, and\nsimply entropy code the prediction residual. However, this approach is\ninefficient at compression. A more efficient and popular approach is to skip\ntransform and quantization but also process the residual block with DPCM, along\nthe horizontal or vertical direction, prior to entropy coding. This paper\nexplores an alternative approach based on processing the residual block with\ninteger-to-integer (i2i) transforms. I2i transforms can map integer pixels to\ninteger transform coefficients without increasing the dynamic range and can be\nused for lossless compression. We focus on lossless intra coding and develop\nnovel i2i approximations of the odd type-3 DST (ODST-3). Experimental results\nwith the HEVC reference software show that the developed i2i approximations of\nthe ODST-3 improve lossless intra-frame compression efficiency with respect to\nHEVC version 2, which uses the popular DPCM method, by an average 2.7% without\na significant effect on computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 19:06:19 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Kamisli", "Fatih", ""]]}, {"id": "1708.08288", "submitter": "Yibing Song", "authors": "Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang, Ming-Hsuan\n  Yang", "title": "Stylizing Face Images via Multiple Exemplars", "comments": "In CVIU 2017. Project Page:\n  http://www.cs.cityu.edu.hk/~yibisong/cviu17/index.html", "journal-ref": null, "doi": "10.1016/j.cviu.2017.08.009", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of transferring the style of a headshot photo to face\nimages. Existing methods using a single exemplar lead to inaccurate results\nwhen the exemplar does not contain sufficient stylized facial components for a\ngiven photo. In this work, we propose an algorithm to stylize face images using\nmultiple exemplars containing different subjects in the same style. Patch\ncorrespondences between an input photo and multiple exemplars are established\nusing a Markov Random Field (MRF), which enables accurate local energy transfer\nvia Laplacian stacks. As image patches from multiple exemplars are used, the\nboundaries of facial components on the target image are inevitably\ninconsistent. The artifacts are removed by a post-processing step using an\nedge-preserving filter. Experimental results show that the proposed algorithm\nconsistently produces visually pleasing results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:36:33 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Song", "Yibing", ""], ["Bao", "Linchao", ""], ["He", "Shengfeng", ""], ["Yang", "Qingxiong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.08988", "submitter": "Tuan Ho", "authors": "Tuan Ho, Madhukar Budagavi", "title": "Dual-fisheye lens stitching for 360-degree imaging", "comments": "ICASSP 17 preprint, Proc. of the 42nd IEEE International Conference\n  on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, USA, March\n  2017", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952541", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-fisheye lens cameras have been increasingly used for 360-degree\nimmersive imaging. However, the limited overlapping field of views and\nmisalignment between the two lenses give rise to visible discontinuities in the\nstitching boundaries. This paper introduces a novel method for dual-fisheye\ncamera stitching that adaptively minimizes the discontinuities in the\noverlapping regions to generate full spherical 360-degree images. Results show\nthat this approach can produce good quality stitched images for Samsung Gear\n360 -- a dual-fisheye camera, even with hard-to-stitch objects in the stitching\nborders.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 03:23:48 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Ho", "Tuan", ""], ["Budagavi", "Madhukar", ""]]}, {"id": "1708.09535", "submitter": "Lei Chen", "authors": "Lei Chen, Shihong Wang", "title": "A secure blind watermarking scheme based on DCT domain of the scrambled\n  image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a secure blind watermarking scheme. The main idea of\nthe scheme not only protects the watermark information but also the embedding\npositions. To achieve a higher level of security, we propose a sub key\ngeneration mechanism based on the singular value decomposition and hash\nfunction, where sub keys depend on both the main key and the feature codes of\nthe original image. The different sub keys ensure that the embedding positions\nare randomly selected for different original images. Watermark is embedded in\nthe Discrete Cosine Transform (DCT) coefficients of the scrambled original\nimage. Simulation results show that such embedded method resolves well the\ncontradiction of imperceptibility and robustness. Based on good correlation\nproperties of chaotic sequences, we design a detection method, which can\naccurately compute geometric transformation (rotation and translation\ntransformations) parameters. The security analysis, including key space\nanalysis, key sensitivity analysis, cryptanalysis, and the comparison results\ndemonstrate that the proposed watermarking scheme also achieves high security.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 02:24:44 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Chen", "Lei", ""], ["Wang", "Shihong", ""]]}, {"id": "1708.09644", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro,\n  Carlo Regazzoni, Nicu Sebe", "title": "Abnormal Event Detection in Videos using Generative Adversarial Nets", "comments": "Best Paper / Student Paper Award Finalist, IEEE International\n  Conference on Image Processing (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the abnormality detection problem in crowded scenes.\nWe propose to use Generative Adversarial Nets (GANs), which are trained using\nnormal frames and corresponding optical-flow images in order to learn an\ninternal representation of the scene normality. Since our GANs are trained with\nonly normal data, they are not able to generate abnormal events. At testing\ntime the real data are compared with both the appearance and the motion\nrepresentations reconstructed by our GANs and abnormal areas are detected by\ncomputing local differences. Experimental results on challenging abnormality\ndetection datasets show the superiority of the proposed method compared to the\nstate of the art in both frame-level and pixel-level abnormality detection\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 09:56:02 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Nabi", "Moin", ""], ["Sangineto", "Enver", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""], ["Sebe", "Nicu", ""]]}]