[{"id": "1601.00299", "submitter": "Mehdi Safarpour", "authors": "Mehdi Safarpour, Mostafa Charmi", "title": "Capacity Enlargement Of The PVD Steganography Method Using The GLM\n  Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most steganographic methods, increasing in the capacity leads to decrease\nin the quality of the stego-image, so in this paper, we propose to combine two\nexisting techniques, Pixel value differencing and Gray Level Modification, to\ncome up with a hybrid steganography scheme which can hide more information\nwithout having to compromise much on the quality of the stego-image.\nExperimental results demonstrate that the proposed approach has larger capacity\nwhile its results are imperceptible. In comparison with original PVD method\ncriterion of the quality is declined by 2% dB averagely while the capacity is\nincreased around 25%.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 14:23:32 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Safarpour", "Mehdi", ""], ["Charmi", "Mostafa", ""]]}, {"id": "1601.00599", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer, Daniel Schopfhauser", "title": "Multimodal Classification of Events in Social Media", "comments": "Preprint of accepted manuscript for the Elsevier Image and Vision\n  Computing Journal (IMAVIS). The paper will be published by IMAVIS under DOI\n  10.1016/j.imavis.2015.12.004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of social media hosted on platforms like Flickr and Instagram\nis related to social events. The task of social event classification refers to\nthe distinction of event and non-event-related content as well as the\nclassification of event types (e.g. sports events, concerts, etc.). In this\npaper, we provide an extensive study of textual, visual, as well as multimodal\nrepresentations for social event classification. We investigate strengths and\nweaknesses of the modalities and study synergy effects between the modalities.\nExperimental results obtained with our multimodal representation outperform\nstate-of-the-art methods and provide a new baseline for future research.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 18:29:33 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Schopfhauser", "Daniel", ""]]}, {"id": "1601.01386", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Jamil Ahmad, Haleem Farman, Zahoor Jan", "title": "A New Image Steganographic Technique using Pattern based Bits Shuffling\n  and Magic LSB for Grayscale Images", "comments": "A short paper of 6 pages", "journal-ref": "Sindh University Research Journal-SURJ (Science Series) 47.4\n  (2015)", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Steganography is a growing research area of information security where\nsecret information is embedded in innocent-looking public communication. This\npaper proposes a novel crystographic technique for grayscale images in spatial\ndomain. The secret data is encrypted and shuffled using pattern based bits\nshuffling algorithm (PBSA) and a secret key. The encrypted data is then\nembedded in the cover image using magic least significant bit (M-LSB) method.\nExperimentally, the proposed method is evaluated by qualitative and\nquantitative analysis which validates the effectiveness of the proposed method\nin contrast to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 03:32:27 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Muhammad", "Khan", ""], ["Ahmad", "Jamil", ""], ["Farman", "Haleem", ""], ["Jan", "Zahoor", ""]]}, {"id": "1601.01408", "submitter": "Suleiman Mustafa", "authors": "Suleiman Mustafa, Hannan Xiao", "title": "Comparison of cinepak, intel, microsoft video and indeo codec for video\n  compression", "comments": "13 pages, 1 figure, 7 tables, journal paper", "journal-ref": "The International Journal of Multimedia and Its Applications\n  (IJMA), Volume 7, Number 6 December 2015", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The file size and picture quality are factors to be considered for streaming,\nstorage and transmitting videos over networks. This work compares Cinepak,\nIntel, Microsoft Video and Indeo Codec for video compression. The peak signal\nto noise ratio is used to compare the quality of such video compressed using\nAVI codecs. The most widely used objective measurement by developers of video\nprocessing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise\nRation is measured on a logarithmic scale and depends on the mean squared error\n(MSE) between an original and an impaired image or video, relative to (2n-1)2.\n  Previous research done regarding assessing of video quality has been mainly\nby the use of subjective methods, and there is still no standard method for\nobjective assessments. Although it has been considered that compression might\nnot be significant in future as storage and transmission capabilities improve,\nbut at low bandwidths compression makes communication possible.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 06:15:16 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Mustafa", "Suleiman", ""], ["Xiao", "Hannan", ""]]}, {"id": "1601.02076", "submitter": "Sachin Mungmode Mr.", "authors": "Sachin Mungmode, R. R. Sedamkar, Niranjan Kulkarni", "title": "An Enhanced Edge Adaptive Steganography Approach Using Threshold Value\n  for Region Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to improve the quality and the modification rate of a\nStego Image. The input image provided for estimating the quality of an image\nand the modified rate is a bitmap image. The threshold value is used as a\nparameter for selecting the high frequency pixels from the Cover Image. The\ndata embedding process are performed on the pixels that are found with the help\nof Threshold value by using LSBMR. The quality of an image is estimated by the\nvalue of PSNR and the modification rate of an image is estimated by the value\nof MSE. The proposed approach achieves about 0.2 to 0.6 % of improvement in the\nquality of an image and about 4 to 10 % of improvement in the modification rate\nof an image compared to the edge detection techniques such as Sobel and Canny.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 04:48:08 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mungmode", "Sachin", ""], ["Sedamkar", "R. R.", ""], ["Kulkarni", "Niranjan", ""]]}, {"id": "1601.02852", "submitter": "Tae-Hyun Oh", "authors": "Jinsoo Choi, Tae-Hyun Oh, In So Kweon", "title": "Human Attention Estimation for Natural Images: An Automatic Gaze\n  Refinement Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo collections and its applications today attempt to reflect user\ninteractions in various forms. Moreover, photo collections aim to capture the\nusers' intention with minimum effort through applications capturing user\nintentions. Human interest regions in an image carry powerful information about\nthe user's behavior and can be used in many photo applications. Research on\nhuman visual attention has been conducted in the form of gaze tracking and\ncomputational saliency models in the computer vision community, and has shown\nconsiderable progress. This paper presents an integration between implicit gaze\nestimation and computational saliency model to effectively estimate human\nattention regions in images on the fly. Furthermore, our method estimates human\nattention via implicit calibration and incremental model updating without any\nactive participation from the user. We also present extensive analysis and\npossible applications for personal photo collections.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 13:31:38 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1601.02913", "submitter": "Xinchao Li", "authors": "Xinchao Li, Peng Xu, Yue Shi, Martha Larson, Alan Hanjalic", "title": "Learning Subclass Representations for Visually-varied Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a subclass-representation approach that predicts\nthe probability of a social image belonging to one particular class. We explore\nthe co-occurrence of user-contributed tags to find subclasses with a strong\nconnection to the top level class. We then project each image on to the\nresulting subclass space to generate a subclass representation for the image.\nThe novelty of the approach is that subclass representations make use of not\nonly the content of the photos themselves, but also information on the\nco-occurrence of their tags, which determines membership in both subclasses and\ntop-level classes. The novelty is also that the images are classified into\nsmaller classes, which have a chance of being more visually stable and easier\nto model. These subclasses are used as a latent space and images are\nrepresented in this space by their probability of relatedness to all of the\nsubclasses. In contrast to approaches directly modeling each top-level class\nbased on the image content, the proposed method can exploit more information\nfor visually diverse classes. The approach is evaluated on a set of $2$ million\nphotos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale\nFlickr-tag Image Classification Grand Challenge. Experiments show that the\nproposed system delivers sound performance for visually diverse classes\ncompared with methods that directly model top classes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:30:58 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Li", "Xinchao", ""], ["Xu", "Peng", ""], ["Shi", "Yue", ""], ["Larson", "Martha", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1601.03239", "submitter": "Victor Schetinger", "authors": "Victor Schetinger, Massimo Iuliani, Alessandro Piva, Manuel M.\n  Oliveira", "title": "Digital Image Forensics vs. Image Composition: An Indirect Arms Race", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of image composition is constantly trying to improve the ways in\nwhich an image can be altered and enhanced. While this is usually done in the\nname of aesthetics and practicality, it also provides tools that can be used to\nmaliciously alter images. In this sense, the field of digital image forensics\nhas to be prepared to deal with the influx of new technology, in a constant\narms-race. In this paper, the current state of this arms-race is analyzed,\nsurveying the state-of-the-art and providing means to compare both sides. A\nnovel scale to classify image forensics assessments is proposed, and\nexperiments are performed to test composition techniques in regards to\ndifferent forensics traces. We show that even though research in forensics\nseems unaware of the advanced forms of image composition, it possesses the\nbasic tools to detect it.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 13:38:36 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Schetinger", "Victor", ""], ["Iuliani", "Massimo", ""], ["Piva", "Alessandro", ""], ["Oliveira", "Manuel M.", ""]]}, {"id": "1601.04473", "submitter": "Saeed Ranjbar Alvar", "authors": "Saeed R. Alvar, Fatih Kamisli", "title": "Lossless Intra Coding in HEVC with 3-tap Filters", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a pixel-by-pixel spatial prediction method for lossless\nintra coding within High Efficiency Video Coding (HEVC). A well-known previous\npixel-by-pixel spatial prediction method uses only two neighboring pixels for\nprediction, based on the angular projection idea borrowed from block-based\nintra prediction in lossy coding. This paper explores a method which uses three\nneighboring pixels for prediction according to a two-dimensional correlation\nmodel, and the used neighbor pixels and prediction weights change depending on\nintra mode. To find the best prediction weights for each intra mode, a\ntwo-stage offline optimization algorithm is used and a number of implementation\naspects are discussed to simplify the proposed prediction method. The proposed\nmethod is implemented in the HEVC reference software and experimental results\nshow that the explored 3-tap filtering method can achieve an average 11.34%\nbitrate reduction over the default lossless intra coding in HEVC. The proposed\nmethod also decreases average decoding time by 12.7% while it increases average\nencoding time by 9.7%\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 11:22:29 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Alvar", "Saeed R.", ""], ["Kamisli", "Fatih", ""]]}, {"id": "1601.04522", "submitter": "Xinchao Li", "authors": "Xinchao Li, Ju Liu, Jiande Sun, Xiaohui Yang, and Wei Liu", "title": "Multiple Watermarking Algorithm Based on Spread Transform Dither\n  Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple watermarking technique, embedding several watermarks in one carrier,\nhas enabled many interesting applications. In this study, a novel multiple\nwatermarking algorithm is proposed based on the spirit of spread transform\ndither modulation (STDM). It can embed multiple watermarks into the same region\nand the same transform domain of one image; meanwhile, the embedded watermarks\ncan be extracted independently and blindly in the detector without any\ninterference. Furthermore, to improve the fidelity of the watermarked image,\nthe properties of the dither modulation quantizer and the proposed multiple\nwatermarks embedding strategy are investigated, and two practical optimization\nmethods are proposed. Finally, to enhance the application flexibility, an\nextension of the proposed algorithm is proposed which can sequentially embeds\ndifferent watermarks into one image during each stage of its circulation.\nCompared with the pioneering multiple watermarking algorithms, the proposed one\nowns more flexibility in practical application and is more robust against\ndistortion due to basic operations such as random noise, JPEG compression and\nvolumetric scaling.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 14:15:06 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Li", "Xinchao", ""], ["Liu", "Ju", ""], ["Sun", "Jiande", ""], ["Yang", "Xiaohui", ""], ["Liu", "Wei", ""]]}, {"id": "1601.05313", "submitter": "Rafael Rodriguez-Sanchez", "authors": "Rafael Rodr\\'iguez-S\\'anchez and Enrique S. Quintana-Ort\\'i", "title": "Architecture-Aware Optimization of an HEVC decoder on Asymmetric\n  Multicore Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-power asymmetric multicore processors (AMPs) attract considerable\nattention due to their appealing performance-power ratio for energy-constrained\nenvironments. However, these processors pose a significant programming\nchallenge due to the integration of cores with different performance\ncapabilities, asking for an asymmetry-aware scheduling solution that carefully\ndistributes the workload.\n  The recent HEVC standard, which offers several high-level parallelization\nstrategies, is an important application that can benefit from an implementation\ntailored for the low-power AMPs present in many current mobile or hand-held\ndevices. In this scenario, we present an architecture-aware implementation of\nan HEVC decoder that embeds a criticality-aware scheduling strategy tuned for a\nSamsung Exynos 5422 system-on-chip furnished with an ARM big.LITTLE AMP. The\nperformance and energy efficiency of our solution is further enhanced by\nexploiting the NEON vector engine available in the ARM big.LITTLE architecture.\nExperimental results expose a 1080p real-time HEVC decoding at 24 frames/sec,\nand a reduction of energy consumption over 20%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 15:55:29 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1601.06439", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tsuhan Chen", "title": "Who Ordered This?: Exploiting Implicit User Tag Order Preferences for\n  Personalized Image Tagging", "comments": null, "journal-ref": null, "doi": "10.1109/ICMEW.2016.7574753", "report-no": null, "categories": "cs.IR cs.HC cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes a person pick certain tags over others when tagging an image? Does\nthe order that a person presents tags for a given image follow an implicit bias\nthat is personal? Can these biases be used to improve existing automated image\ntagging systems? We show that tag ordering, which has been largely overlooked\nby the image tagging community, is an important cue in understanding user\ntagging behavior and can be used to improve auto-tagging systems. Inspired by\nthe assumption that people order their tags, we propose a new way of measuring\ntag preferences, and also propose a new personalized tagging objective function\nthat explicitly considers a user's preferred tag orderings. We also provide a\n(partially) greedy algorithm that produces good solutions to our new objective\nand under certain conditions produces an optimal solution. We validate our\nmethod on a subset of Flickr images that spans 5000 users, over 5200 tags, and\nover 90,000 images. Our experiments show that exploiting personalized tag\norders improves the average performance of state-of-art approaches both on\nper-image and per-user bases.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:03:29 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1601.06440", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tsuhan Chen", "title": "QUOTE: \"Querying\" Users as Oracles in Tag Engines - A Semi-Supervised\n  Learning Approach to Personalized Image Tagging", "comments": null, "journal-ref": null, "doi": "10.1109/ISM.2016.0016", "report-no": null, "categories": "cs.IR cs.LG cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One common trend in image tagging research is to focus on visually relevant\ntags, and this tends to ignore the personal and social aspect of tags,\nespecially on photoblogging websites such as Flickr. Previous work has\ncorrectly identified that many of the tags that users provide on images are not\nvisually relevant (i.e. representative of the salient content in the image) and\nthey go on to treat such tags as noise, ignoring that the users chose to\nprovide those tags over others that could have been more visually relevant.\nAnother common assumption about user generated tags for images is that the\norder of these tags provides no useful information for the prediction of tags\non future images. This assumption also tends to define usefulness in terms of\nwhat is visually relevant to the image. For general tagging or labeling\napplications that focus on providing visual information about image content,\nthese assumptions are reasonable, but when considering personalized image\ntagging applications, these assumptions are at best too rigid, ignoring user\nchoice and preferences.\n  We challenge the aforementioned assumptions, and provide a machine learning\napproach to the problem of personalized image tagging with the following\ncontributions: 1.) We reformulate the personalized image tagging problem as a\nsearch/retrieval ranking problem, 2.) We leverage the order of tags, which does\nnot always reflect visual relevance, provided by the user in the past as a cue\nto their tag preferences, similar to click data, 3.) We propose a technique to\naugment sparse user tag data (semi-supervision), and 4.) We demonstrate the\nefficacy of our method on a subset of Flickr images, showing improvement over\nprevious state-of-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:07:16 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1601.06603", "submitter": "Sibo Song", "authors": "Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal,\n  Jie Lin", "title": "Egocentric Activity Recognition with Multimodal Fisher Vector", "comments": "5 pages, 4 figures, ICASSP 2016 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of wearable devices, research on egocentric\nactivity recognition has received much attention recently. In this paper, we\nbuild a Multimodal Egocentric Activity dataset which includes egocentric videos\nand sensor data of 20 fine-grained and diverse activity categories. We present\na novel strategy to extract temporal trajectory-like features from sensor data.\nWe propose to apply the Fisher Kernel framework to fuse video and temporal\nenhanced sensor features. Experiment results show that with careful design of\nfeature extraction and fusion algorithm, sensor data can enhance\ninformation-rich video data. We make publicly available the Multimodal\nEgocentric Activity dataset to facilitate future research.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 13:57:07 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Song", "Sibo", ""], ["Cheung", "Ngai-Man", ""], ["Chandrasekhar", "Vijay", ""], ["Mandal", "Bappaditya", ""], ["Lin", "Jie", ""]]}, {"id": "1601.06615", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita\n  Prabhu, Srinivas S S Kruthiventi and R. Venkatesh Babu", "title": "A Taxonomy of Deep Convolutional Neural Nets for Computer Vision", "comments": "Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm)", "journal-ref": "Frontiers in Robotics and AI 2(36), January 2016", "doi": "10.3389/frobt.2015.00036", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional architectures for solving computer vision problems and the degree\nof success they enjoyed have been heavily reliant on hand-crafted features.\nHowever, of late, deep learning techniques have offered a compelling\nalternative -- that of automatically learning problem-specific features. With\nthis new paradigm, every problem in computer vision is now being re-examined\nfrom a deep learning perspective. Therefore, it has become important to\nunderstand what kind of deep networks are suitable for a given problem.\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\nexist, a survey specific to computer vision is missing. We specifically\nconsider one form of deep networks widely used in computer vision -\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\nand then examine the broad variations proposed over time to suit different\napplications. We hope that our recipe-style survey will serve as a guide,\nparticularly for novice practitioners intending to use deep-learning techniques\nfor computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:25:07 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Srinivas", "Suraj", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Mopuri", "Konda Reddy", ""], ["Prabhu", "Nikita", ""], ["Kruthiventi", "Srinivas S S", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1601.06756", "submitter": "Onur G\\\"unl\\\"u", "authors": "Onur G\\\"unl\\\"u and Gerhard Kramer", "title": "Privacy, Secrecy, and Storage with Multiple Noisy Measurements of\n  Identifiers", "comments": "To appear in IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key-leakage-storage region is derived for a generalization of a classic\ntwo-terminal key agreement model. The additions to the model are that the\nencoder observes a hidden, or noisy, version of the identifier, and that the\nencoder and decoder can perform multiple measurements. To illustrate the\nbehavior of the region, the theory is applied to binary identifiers and noise\nmodeled via binary symmetric channels. In particular, the key-leakage-storage\nregion is simplified by applying Mrs. Gerber's lemma twice in different\ndirections to a Markov chain. The growth in the region as the number of\nmeasurements increases is quantified. The amount by which the privacy-leakage\nrate reduces for a hidden identifier as compared to a noise-free (visible)\nidentifier at the encoder is also given. If the encoder incorrectly models the\nsource as visible, it is shown that substantial secrecy leakage may occur and\nthe reliability of the reconstructed key might decrease.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:29:21 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 18:57:08 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Kramer", "Gerhard", ""]]}, {"id": "1601.07232", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, T. V. Cooklev", "title": "Robust Image Watermarking Using Non-Regular Wavelets", "comments": "13 pages, 11 figures", "journal-ref": "Signal, Image and Video Processing, September 2009, Volume 3,\n  Issue 3, pp 241-250", "doi": "10.1007/s11760-008-0070-7", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to watermarking digital images using non-regular wavelets is\nadvanced. Non-regular transforms spread the energy in the transform domain. The\nproposed method leads at the same time to increased image quality and increased\nrobustness with respect to lossy compression. The approach provides robust\nwatermarking by suitably creating watermarked messages that have energy\ncompaction and frequency spreading. Our experimental results show that the\napplication of non-regular wavelets, instead of regular ones, can furnish a\nsuperior robust watermarking scheme. The generated watermarked data is more\nimmune against non-intentional JPEG and JPEG2000 attacks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 00:18:26 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Cintra", "R. J.", ""], ["Cooklev", "T. V.", ""]]}, {"id": "1601.07262", "submitter": "Ye Zhu", "authors": "Ye Zhu, Tian-Tsong Ng, Xuanjing Shen, Bihan Wen", "title": "Revisiting copy-move forgery detection by considering realistic image\n  with similar but genuine objects", "comments": "The version of ICASSP2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many images, of natural or man-made scenes often contain Similar but Genuine\nObjects (SGO). This poses a challenge to existing Copy-Move Forgery Detection\n(CMFD) methods which match the key points / blocks, solely based on the pair\nsimilarity in the scene. To address such issue, we propose a novel CMFD method\nusing Scaled Harris Feature Descriptors (SHFD) that preform consistently well\non forged images with SGO. It involves the following main steps: (i) Pyramid\nscale space and orientation assignment are used to keep scaling and rotation\ninvariance; (ii) Combined features are applied for precise texture description;\n(iii) Similar features of two points are matched and RANSAC is used to remove\nthe false matches. The experimental results indicate that the proposed\nalgorithm is effective in detecting SGO and copy-move forgery, which compares\nfavorably to existing methods. Our method exhibits high robustness even when an\nimage is operated by geometric transformation and post-processing\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 04:58:16 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Zhu", "Ye", ""], ["Ng", "Tian-Tsong", ""], ["Shen", "Xuanjing", ""], ["Wen", "Bihan", ""]]}, {"id": "1601.07884", "submitter": "Xinchao Li", "authors": "Xinchao Li, Martha A. Larson, Alan Hanjalic", "title": "Geo-distinctive Visual Element Matching for Location Estimation of\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an image representation and matching approach that substantially\nimproves visual-based location estimation for images. The main novelty of the\napproach, called distinctive visual element matching (DVEM), is its use of\nrepresentations that are specific to the query image whose location is being\npredicted. These representations are based on visual element clouds, which\nrobustly capture the connection between the query and visual evidence from\ncandidate locations. We then maximize the influence of visual elements that are\ngeo-distinctive because they do not occur in images taken at many other\nlocations. We carry out experiments and analysis for both geo-constrained and\ngeo-unconstrained location estimation cases using two large-scale,\npublicly-available datasets: the San Francisco Landmark dataset with $1.06$\nmillion street-view images and the MediaEval '15 Placing Task dataset with\n$5.6$ million geo-tagged images from Flickr. We present examples that\nillustrate the highly-transparent mechanics of the approach, which are based on\ncommon sense observations about the visual patterns in image collections. Our\nresults show that the proposed method delivers a considerable performance\nimprovement compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 20:13:01 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Li", "Xinchao", ""], ["Larson", "Martha A.", ""], ["Hanjalic", "Alan", ""]]}]