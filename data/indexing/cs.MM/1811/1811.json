[{"id": "1811.00162", "submitter": "Shang-Yu Su", "authors": "Yu-An Wang, Yu-Kai Huang, Tzu-Chuan Lin, Shang-Yu Su, Yun-Nung Chen", "title": "Modeling Melodic Feature Dependency with Modularized Variational\n  Auto-Encoder", "comments": "The first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic melody generation has been a long-time aspiration for both AI\nresearchers and musicians. However, learning to generate euphonious melodies\nhas turned out to be highly challenging. This paper introduces 1) a new variant\nof variational autoencoder (VAE), where the model structure is designed in a\nmodularized manner in order to model polyphonic and dynamic music with domain\nknowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly\nmodels the dependency between melodic features. The proposed framework is\ncapable of generating distinct melodies that sounds natural, and the\nexperiments for evaluating generated music clips show that the proposed model\noutperforms the baselines in human evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:59:04 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Wang", "Yu-An", ""], ["Huang", "Yu-Kai", ""], ["Lin", "Tzu-Chuan", ""], ["Su", "Shang-Yu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1811.00454", "submitter": "Emad Grais", "authors": "Emad M. Grais, Hagen Wierstorf, Dominic Ward, Russell Mason, Mark D.\n  Plumbley", "title": "Referenceless Performance Evaluation of Audio Source Separation using\n  Deep Neural Networks", "comments": null, "journal-ref": "This paper will be presented at EUSIPCO 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current performance evaluation for audio source separation depends on\ncomparing the processed or separated signals with reference signals. Therefore,\ncommon performance evaluation toolkits are not applicable to real-world\nsituations where the ground truth audio is unavailable. In this paper, we\npropose a performance evaluation technique that does not require reference\nsignals in order to assess separation quality. The proposed technique uses a\ndeep neural network (DNN) to map the processed audio into its quality score.\nOur experiment results show that the DNN is capable of predicting the\nsources-to-artifacts ratio from the blind source separation evaluation toolkit\nwithout the need for reference signals.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:50:42 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Grais", "Emad M.", ""], ["Wierstorf", "Hagen", ""], ["Ward", "Dominic", ""], ["Mason", "Russell", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1811.00818", "submitter": "Juheon Lee", "authors": "Juheon Lee, Seohyun Kim, Kyogu Lee", "title": "Listen to Dance: Music-driven choreography generation using\n  Autoregressive Encoder-Decoder Network", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic choreography generation is a challenging task because it often\nrequires an understanding of two abstract concepts - music and dance - which\nare realized in the two different modalities, namely audio and video,\nrespectively. In this paper, we propose a music-driven choreography generation\nsystem using an auto-regressive encoder-decoder network. To this end, we first\ncollect a set of multimedia clips that include both music and corresponding\ndance motion. We then extract the joint coordinates of the dancer from video\nand the mel-spectrogram of music from audio, and train our network using\nmusic-choreography pairs as input. Finally, a novel dance motion is generated\nat the inference time when only music is given as an input. We performed a user\nstudy for a qualitative evaluation of the proposed method, and the results show\nthat the proposed model is able to generate musically meaningful and natural\ndance movements given an unheard song.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 11:03:28 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lee", "Juheon", ""], ["Kim", "Seohyun", ""], ["Lee", "Kyogu", ""]]}, {"id": "1811.01504", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Deep Multiple Description Coding by Learning Scalar Quantization", "comments": "8 pages, 4 figures. (DCC 2019: Data Compression Conference). Testing\n  datasets for \"Deep Optimized Multiple Description Image Coding via Scalar\n  Quantization Learning\" can be found in the website of\n  https://github.com/mdcnn/Deep-Multiple-Description-Coding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep multiple description coding framework, whose\nquantizers are adaptively learned via the minimization of multiple description\ncompressive loss. Firstly, our framework is built upon auto-encoder networks,\nwhich have multiple description multi-scale dilated encoder network and\nmultiple description decoder networks. Secondly, two entropy estimation\nnetworks are learned to estimate the informative amounts of the quantized\ntensors, which can further supervise the learning of multiple description\nencoder network to represent the input image delicately. Thirdly, a pair of\nscalar quantizers accompanied by two importance-indicator maps is automatically\nlearned in an end-to-end self-supervised way. Finally, multiple description\nstructural dissimilarity distance loss is imposed on multiple description\ndecoded images in pixel domain for diversified multiple description generations\nrather than on feature tensors in feature domain, in addition to multiple\ndescription reconstruction loss. Through testing on two commonly used datasets,\nit is verified that our method is beyond several state-of-the-art multiple\ndescription coding approaches in terms of coding efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 03:49:23 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 09:20:33 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 09:25:48 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1811.01820", "submitter": "Sara Mandelli", "authors": "Sara Mandelli, Paolo Bestagini, Luisa Verdoliva, Stefano Tubaro", "title": "Facing Device Attribution Problem for Stabilized Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem deeply investigated by multimedia forensics researchers is the one\nof detecting which device has been used to capture a video. This enables to\ntrace down the owner of a video sequence, which proves extremely helpful to\nsolve copyright infringement cases as well as to fight distribution of illicit\nmaterial (e.g., underage clips, terroristic threats, etc.). Currently, the most\npromising methods to tackle this task exploit unique noise traces left by\ncamera sensors on acquired images. However, given the recent advancements in\nmotion stabilization of video content, robustness of sensor pattern noise-based\ntechniques are strongly hindered. Indeed, video stabilization introduces\ngeometric transformations between video frames, thus making camera fingerprint\nestimation problematic with classical approaches. In this paper, we deal with\nthe challenging problem of attributing stabilized videos to their recording\ndevice. Specifically, we propose: (i) a strategy to extract the characteristic\nfingerprint of a device, starting from either a set of images or stabilized\nvideo sequences; (ii) a strategy to match a stabilized video sequence with a\ngiven fingerprint in order to solve the device attribution problem. The\nproposed methodology is tested on videos coming from a set of different\nsmartphones, taken from the modern publicly available Vision Dataset. The\nconducted experiments also provide an interesting insight on the effect of\nmodern smartphones video stabilization algorithms on specific video frames.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:04:25 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Mandelli", "Sara", ""], ["Bestagini", "Paolo", ""], ["Verdoliva", "Luisa", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1811.03214", "submitter": "Olivier Augereau", "authors": "Marco Stricker, Olivier Augereau, Koichi Kise, Motoi Iwata", "title": "Facial Landmark Detection for Manga Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of facial landmark detection has been widely covered for pictures\nof human faces, but it is still a challenge for drawings. Indeed, the\nproportions and symmetry of standard human faces are not always used for comics\nor mangas. The personal style of the author, the limitation of colors, etc.\nmakes the landmark detection on faces in drawings a difficult task. Detecting\nthe landmarks on manga images will be useful to provide new services for easily\nediting the character faces, estimating the character emotions, or generating\nautomatically some animations such as lip or eye movements.\n  This paper contains two main contributions: 1) a new landmark annotation\nmodel for manga faces, and 2) a deep learning approach to detect these\nlandmarks. We use the \"Deep Alignment Network\", a multi stage architecture\nwhere the first stage makes an initial estimation which gets refined in further\nstages. The first results show that the proposed method succeed to accurately\nfind the landmarks in more than 80% of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:36:51 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Stricker", "Marco", ""], ["Augereau", "Olivier", ""], ["Kise", "Koichi", ""], ["Iwata", "Motoi", ""]]}, {"id": "1811.03713", "submitter": "Huili Chen", "authors": "Huili Chen, Bita Darvish Rouhani, Xinwei Fan, Osman Cihan Kilinc, and\n  Farinaz Koushanfar", "title": "Performance Comparison of Contemporary DNN Watermarking Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNNs shall be considered as the intellectual property (IP) of the model\nbuilder due to the impeding cost of designing/training a highly accurate model.\nResearch attempts have been made to protect the authorship of the trained model\nand prevent IP infringement using DNN watermarking techniques. In this paper,\nwe provide a comprehensive performance comparison of the state-of-the-art DNN\nwatermarking methodologies according to the essential requisites for an\neffective watermarking technique. We identify the pros and cons of each scheme\nand provide insights into the underlying rationale. Empirical results\ncorroborate that DeepSigns framework proposed in [4] has the best overall\nperformance in terms of the evaluation metrics. Our comparison facilitates the\ndevelopment of pending watermarking approaches and enables the model owner to\ndeploy the watermarking scheme that satisfying her requirements.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 23:15:50 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Chen", "Huili", ""], ["Rouhani", "Bita Darvish", ""], ["Fan", "Xinwei", ""], ["Kilinc", "Osman Cihan", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1811.03732", "submitter": "Kejiang Chen", "authors": "Kejiang Chen, Hang Zhou, Hanqing Zhao, Dongdong Chen, Weiming Zhang,\n  Nenghai Yu", "title": "Distribution-Preserving Steganography Based on Text-to-Speech Generative\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is the art and science of hiding secret messages in public\ncommunication so that the presence of the secret messages cannot be detected.\nThere are two distribution-preserving steganographic frameworks, one is\nsampling-based and the other is compression-based. The former requires a\nperfect sampler which yields data following the same distribution, and the\nlatter needs explicit distribution of generative objects. However, these two\nconditions are too strict even unrealistic in the traditional data environment,\ne.g. the distribution of natural images is hard to seize. Fortunately,\ngenerative models bring new vitality to distribution-preserving steganography,\nwhich can serve as the perfect sampler or provide the explicit distribution of\ngenerative media. Take text-to-speech generation task as an example, we propose\ndistribution-preserving steganography based on WaveGlow and WaveNet, which\ncorresponds to the former two categories. Steganalysis experiments and\ntheoretical analysis are conducted to demonstrate that the proposed methods can\npreserve the distribution.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 01:34:59 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 15:53:16 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 12:36:34 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Chen", "Kejiang", ""], ["Zhou", "Hang", ""], ["Zhao", "Hanqing", ""], ["Chen", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.04115", "submitter": "Soumyabrata Dev", "authors": "Murhaf Hossari, Soumyabrata Dev, Matthew Nicholson, Killian McCabe,\n  Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu and Fran\\c{c}ois Piti\\'e", "title": "ADNet: A Deep Network for Detecting Adverts", "comments": "Published in Proc. 26th Irish Conference on Artificial Intelligence\n  and Cognitive Science (AICS 2018), First two authors contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online video advertising gives content providers the ability to deliver\ncompelling content, reach a growing audience, and generate additional revenue\nfrom online media. Recently, advertising strategies are designed to look for\noriginal advert(s) in a video frame, and replacing them with new adverts. These\nstrategies, popularly known as product placement or embedded marketing, greatly\nhelp the marketing agencies to reach out to a wider audience. However, in the\nexisting literature, such detection of candidate frames in a video sequence for\nthe purpose of advert integration, is done manually. In this paper, we propose\na deep-learning architecture called ADNet, that automatically detects the\npresence of advertisements in video frames. Our approach is the first of its\nkind that automatically detects the presence of adverts in a video frame, and\nachieves state-of-the-art results on a public dataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 19:41:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hossari", "Murhaf", ""], ["Dev", "Soumyabrata", ""], ["Nicholson", "Matthew", ""], ["McCabe", "Killian", ""], ["Nautiyal", "Atul", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Xu", "Wei", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1811.04193", "submitter": "Rafael Diniz", "authors": "Rafael Diniz, Alan L. V. Guedes, Sergio Colcher", "title": "A Ginga-enabled Digital Radio Mondiale Broadcasting chain: Signaling and\n  Definitions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "ISSN 0103-9741 ISSN 0103-9741", "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  ISDB-T International standard is currently adopted by most Latin America\ncountries and is already installed in most TV sets sold in recent years in the\nregion. To support interactive applications in Digital TV receivers, ISDB-T\ndefines the middleware Ginga. Similar to Digital TV, Digital Radio standards\nalso provide the means to carry interactive applications; however, their\nspecifications for interactive applications are usually more restricted than\nthe ones used in Digital TV. Also, interactive applications for Digital TV and\nDigital Radio are usually incompatible. Motivated by such observations, this\nreport considers the importance of interactive applications for both TV and\nRadio Broadcasting and the advantages of using the same middleware and\nlanguages specification for Digital TV and Radio. More specifically, it\nestablishes the signaling and definitions on how to transport and execute\nGinga-NCL and Ginga-HTML5 applications over DRM (Digital Radio Mondiale)\ntransmission. Ministry of Science, Technology, Innovation and Communication of\nBrazil is carrying trials with Digital Radio Mondiale standard in order to\ndefine the reference model of the Brazilian Digital Radio System (Portuguese:\nSistema Brasileiro de R\\'adio Digital - SBRD).\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 04:22:28 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 17:59:20 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Diniz", "Rafael", ""], ["Guedes", "Alan L. V.", ""], ["Colcher", "Sergio", ""]]}, {"id": "1811.04357", "submitter": "Bryan Wang", "authors": "Bryan Wang and Yi-Hsuan Yang", "title": "PerformanceNet: Score-to-Audio Music Generation with Multi-Band\n  Convolutional Residual Network", "comments": "8 pages, 6 figures, AAAI 2019 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music creation is typically composed of two parts: composing the musical\nscore, and then performing the score with instruments to make sounds. While\nrecent work has made much progress in automatic music generation in the\nsymbolic domain, few attempts have been made to build an AI model that can\nrender realistic music audio from musical scores. Directly synthesizing audio\nwith sound sample libraries often leads to mechanical and deadpan results,\nsince musical scores do not contain performance-level information, such as\nsubtle changes in timing and dynamics. Moreover, while the task may sound like\na text-to-speech synthesis problem, there are fundamental differences since\nmusic audio has rich polyphonic sounds. To build such an AI performer, we\npropose in this paper a deep convolutional model that learns in an end-to-end\nmanner the score-to-audio mapping between a symbolic representation of music\ncalled the piano rolls and an audio representation of music called the\nspectrograms. The model consists of two subnets: the ContourNet, which uses a\nU-Net structure to learn the correspondence between piano rolls and\nspectrograms and to give an initial result; and the TextureNet, which further\nuses a multi-band residual network to refine the result by adding the spectral\ntexture of overtones and timbre. We train the model to generate music clips of\nthe violin, cello, and flute, with a dataset of moderate size. We also present\nthe result of a user study that shows our model achieves higher mean opinion\nscore (MOS) in naturalness and emotional expressivity than a WaveNet-based\nmodel and two commercial sound libraries. We open our source code at\nhttps://github.com/bwang514/PerformanceNet\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 05:55:39 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wang", "Bryan", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1811.04419", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Thomas Lidy, Andreas Rauber", "title": "Multi-Temporal Resolution Convolutional Neural Networks for Acoustic\n  Scene Classification", "comments": "In Proceedings of the Detection and Classification of Acoustic Scenes\n  and Events 2017 Workshop (DCASE2017), November 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a Deep Neural Network architecture for the task of\nacoustic scene classification which harnesses information from increasing\ntemporal resolutions of Mel-Spectrogram segments. This architecture is composed\nof separated parallel Convolutional Neural Networks which learn spectral and\ntemporal representations for each input resolution. The resolutions are chosen\nto cover fine-grained characteristics of a scene's spectral texture as well as\nits distribution of acoustic events. The proposed model shows a 3.56% absolute\nimprovement of the best performing single resolution model and 12.49% of the\nDCASE 2017 Acoustic Scenes Classification task baseline.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 14:05:52 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Schindler", "Alexander", ""], ["Lidy", "Thomas", ""], ["Rauber", "Andreas", ""]]}, {"id": "1811.05185", "submitter": "Silvia Rossi", "authors": "Silvia Rossi, Francesca De Simone, Pascal Frossard and Laura Toni", "title": "Spherical clustering of users navigating 360{\\deg} content", "comments": "5 pages, conference (Published in: ICASSP 2019 - 2019 IEEE\n  International Conference on Acoustics, Speech and Signal Processing (ICASSP))", "journal-ref": "Published in: ICASSP 2019 - 2019 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP.2019.8683854", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Virtual Reality (VR) applications, understanding how users explore the\nomnidirectional content is important to optimize content creation, to develop\nuser-centric services, or even to detect disorders in medical applications.\nClustering users based on their common navigation patterns is a first direction\nto understand users behaviour. However, classical clustering techniques fail in\nidentifying these common paths, since they are usually focused on minimizing a\nsimple distance metric. In this paper, we argue that minimizing the distance\nmetric does not necessarily guarantee to identify users that experience similar\nnavigation path in the VR domain. Therefore, we propose a graph-based method to\nidentify clusters of users who are attending the same portion of the spherical\ncontent over time. The proposed solution takes into account the spherical\ngeometry of the content and aims at clustering users based on the actual\noverlap of displayed content among users. Our method is tested on real VR user\nnavigation patterns. Results show that our solution leads to clusters in which\nat least 85% of the content displayed by one user is shared among the other\nusers belonging to the same cluster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:51:09 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 21:05:54 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Rossi", "Silvia", ""], ["De Simone", "Francesca", ""], ["Frossard", "Pascal", ""], ["Toni", "Laura", ""]]}, {"id": "1811.05550", "submitter": "Li-Chia Yang", "authors": "Lamtharn Hantrakul, Li-Chia Yang", "title": "Neural Wavetable: a playable wavetable synthesizer using neural networks", "comments": "2 pages, Accepted by Conference on Neural Information Processing\n  Systems (NIPS), Workshop on Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Wavetable, a proof-of-concept wavetable synthesizer that\nuses neural networks to generate playable wavetables. The system can produce\nnew, distinct waveforms through the interpolation of traditional wavetables in\nan autoencoder's latent space. It is available as a VST/AU plugin for use in a\nDigital Audio Workstation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:27:17 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 19:54:20 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Hantrakul", "Lamtharn", ""], ["Yang", "Li-Chia", ""]]}, {"id": "1811.05760", "submitter": "Aniruddha Bhattacharya", "authors": "Aniruddha Bhattacharya and K.V. Kadambari", "title": "A Multimodal Approach towards Emotion Recognition of Music using Audio\n  and Lyrical Content", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MoodNet - A Deep Convolutional Neural Network based architecture\nto effectively predict the emotion associated with a piece of music given its\naudio and lyrical content.We evaluate different architectures consisting of\nvarying number of two-dimensional convolutional and subsampling layers,followed\nby dense layers.We use Mel-Spectrograms to represent the audio content and word\nembeddings-specifically 100 dimensional word vectors, to represent the textual\ncontent represented by the lyrics.We feed input data from both modalities to\nour MoodNet architecture.The output from both the modalities are then fused as\na fully connected layer and softmax classfier is used to predict the category\nof emotion.Using F1-score as our metric,our results show excellent performance\nof MoodNet over the two datasets we experimented on-The MIREX Multimodal\ndataset and the Million Song Dataset.Our experiments reflect the hypothesis\nthat more complex models perform better with more training data.We also observe\nthat lyrics outperform audio as a better expressed modality and conclude that\ncombining and using features from multiple modalities for prediction tasks\nresult in superior performance in comparison to using a single modality as\ninput.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 20:51:03 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Bhattacharya", "Aniruddha", ""], ["Kadambari", "K. V.", ""]]}, {"id": "1811.06166", "submitter": "Tianchi Huang", "authors": "Tianchi Huang, Xin Yao, Chenglei Wu, Rui-Xiao Zhang, Zhangyuan Pang,\n  Lifeng Sun", "title": "Tiyuntsong: A Self-Play Reinforcement Learning Approach for ABR Video\n  Streaming", "comments": "Published in ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing reinforcement learning~(RL)-based adaptive bitrate~(ABR) approaches\noutperform the previous fixed control rules based methods by improving the\nQuality of Experience~(QoE) score, as the QoE metric can hardly provide clear\nguidance for optimization, finally resulting in the unexpected strategies. In\nthis paper, we propose \\emph{Tiyuntsong}, a self-play reinforcement learning\napproach with generative adversarial network~(GAN)-based method for ABR video\nstreaming. Tiyuntsong learns strategies automatically by training two agents\nwho are competing against each other. Note that the competition results are\ndetermined by a set of rules rather than a numerical QoE score that allows\nclearer optimization objectives. Meanwhile, we propose GAN Enhancement Module\nto extract hidden features from the past status for preserving the information\nwithout the limitations of sequence lengths. Using testbed experiments, we show\nthat the utilization of GAN significantly improves the Tiyuntsong's\nperformance. By comparing the performance of ABRs, we observe that Tiyuntsong\nalso betters existing ABR algorithms in the underlying metrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 04:29:49 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 14:05:02 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 14:40:30 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Huang", "Tianchi", ""], ["Yao", "Xin", ""], ["Wu", "Chenglei", ""], ["Zhang", "Rui-Xiao", ""], ["Pang", "Zhangyuan", ""], ["Sun", "Lifeng", ""]]}, {"id": "1811.06193", "submitter": "Kamran Kowsari", "authors": "Mojtaba Heidarysafa, James Reed, Kamran Kowsari, April Celeste\n  R.Leviton, Janet I. Warren, and Donald E. Brown", "title": "From Videos to URLs: A Multi-Browser Guide To Extract User's Behavior\n  with Optical Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.RO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking users' activities on the World Wide Web (WWW) allows researchers to\nanalyze each user's internet behavior as time passes and for the amount of time\nspent on a particular domain. This analysis can be used in research design, as\nresearchers may access to their participant's behaviors while browsing the web.\nWeb search behavior has been a subject of interest because of its real-world\napplications in marketing, digital advertisement, and identifying potential\nthreats online. In this paper, we present an image-processing based method to\nextract domains which are visited by a participant over multiple browsers\nduring a lab session. This method could provide another way to collect users'\nactivities during an online session given that the session recorder collected\nthe data. The method can also be used to collect the textual content of\nweb-pages that an individual visits for later analysis\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 05:59:05 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 00:24:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Reed", "James", ""], ["Kowsari", "Kamran", ""], ["Leviton", "April Celeste R.", ""], ["Warren", "Janet I.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1811.06616", "submitter": "Thanh Nguyen Xuan", "authors": "Xuan Thanh Nguyen, Thanh Ha Le and Hongchuan Yu", "title": "Motion Style Extraction Based on Sparse Coding Decomposition", "comments": "Presented at ACM SIGGRAPH ASIA Workshop: Data-Driven Animation\n  Techniques (D2AT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse coding-based framework for motion style decomposition and\nsynthesis. Dynamic Time Warping is firstly used to synchronized input motions\nin the time domain as a pre-processing step. A sparse coding-based\ndecomposition has been proposed, we also introduce the idea of core component\nand basic motion. Decomposed motions are then combined, transfer to synthesize\nnew motions. Lastly, we develop limb length constraint as a post-processing\nstep to remove distortion skeletons. Our framework has the advantage of less\ntime-consuming, no manual alignment and large dataset requirement. As a result,\nour experiments show smooth and natural synthesized motion.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 22:50:54 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Nguyen", "Xuan Thanh", ""], ["Le", "Thanh Ha", ""], ["Yu", "Hongchuan", ""]]}, {"id": "1811.06663", "submitter": "Linsen Dong", "authors": "Guanyu Gao, Linsen Dong, Huaizheng Zhang, Yonggang Wen, and Wenjun\n  Zeng", "title": "Content-Aware Personalised Rate Adaptation for Adaptive Streaming via\n  Deep Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive bitrate (ABR) streaming is the de facto solution for achieving\nsmooth viewing experiences under unstable network conditions. However, most of\nthe existing rate adaptation approaches for ABR are content-agnostic, without\nconsidering the semantic information of the video content. Nevertheless,\nsemantic information largely determines the informativeness and interestingness\nof the video content, and consequently affects the QoE for video streaming. One\ncommon case is that the user may expect higher quality for the parts of video\ncontent that are more interesting or informative so as to reduce video\ndistortion and information loss, given that the overall bitrate budgets are\nlimited. This creates two main challenges for such a problem: First, how to\ndetermine which parts of the video content are more interesting? Second, how to\nallocate bitrate budgets for different parts of the video content with\ndifferent significances? To address these challenges, we propose a\nContent-of-Interest (CoI) based rate adaptation scheme for ABR. We first design\na deep learning approach for recognizing the interestingness of the video\ncontent, and then design a Deep Q-Network (DQN) approach for rate adaptation by\nincorporating video interestingness information. The experimental results show\nthat our method can recognize video interestingness precisely, and the bitrate\nallocation for ABR can be aligned with the interestingness of video content\nwhile not compromising the performances on objective QoE metrics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:05:45 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Gao", "Guanyu", ""], ["Dong", "Linsen", ""], ["Zhang", "Huaizheng", ""], ["Wen", "Yonggang", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1811.07417", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "PerSIM: Multi-resolution Image Quality Assessment in the Perceptually\n  Uniform Color Domain", "comments": "5 pages, 1 figure, 3 tables", "journal-ref": "2015 IEEE International Conference on Image Processing (ICIP),\n  Quebec City, QC, 2015, pp. 1682-1686", "doi": "10.1109/ICIP.2015.7351087", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An average observer perceives the world in color instead of black and white.\nMoreover, the visual system focuses on structures and segments instead of\nindividual pixels. Based on these observations, we propose a full reference\nobjective image quality metric modeling visual system characteristics and\nchroma similarity in the perceptually uniform color domain (Lab). Laplacian of\nGaussian features are obtained in the L channel to model the retinal ganglion\ncells in human visual system and color similarity is calculated over the a and\nb channels. In the proposed perceptual similarity index (PerSIM), a\nmulti-resolution approach is followed to mimic the hierarchical nature of human\nvisual system. LIVE and TID2013 databases are used in the validation and PerSIM\noutperforms all the compared metrics in the overall databases in terms of\nranking, monotonic behavior and linearity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 22:42:32 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.07485", "submitter": "Chenchen Li", "authors": "Chenchen Li, Jialin Wang, Hongwei Wang, Miao Zhao, Wenjie Li, Xiaotie\n  Deng", "title": "Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural\n  Networks", "comments": "Draft, 25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User emotion analysis toward videos is to automatically recognize the general\nemotional status of viewers from the multimedia content embedded in the online\nvideo stream. Existing works fall in two categories: 1) visual-based methods,\nwhich focus on visual content and extract a specific set of features of videos.\nHowever, it is generally hard to learn a mapping function from low-level video\npixels to high-level emotion space due to great intra-class variance. 2)\ntextual-based methods, which focus on the investigation of user-generated\ncomments associated with videos. The learned word representations by\ntraditional linguistic approaches typically lack emotion information and the\nglobal comments usually reflect viewers' high-level understandings rather than\ninstantaneous emotions. To address these limitations, in this paper, we propose\nto jointly utilize video content and user-generated texts simultaneously for\nemotion analysis. In particular, we introduce exploiting a new type of\nuser-generated texts, i.e., \"danmu\", which are real-time comments floating on\nthe video and contain rich information to convey viewers' emotional opinions.\nTo enhance the emotion discriminativeness of words in textual feature\nextraction, we propose Emotional Word Embedding (EWE) to learn text\nrepresentations by jointly considering their semantics and emotions.\nAfterwards, we propose a novel visual-textual emotion analysis model with Deep\nCoupled Video and Danmu Neural networks (DCVDN), in which visual and textual\nfeatures are synchronously extracted and fused to form a comprehensive\nrepresentation by deep-canonically-correlated-autoencoder-based multi-view\nlearning. Through extensive experiments on a self-crawled real-world\nvideo-danmu dataset, we prove that DCVDN significantly outperforms the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:51:19 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Chenchen", ""], ["Wang", "Jialin", ""], ["Wang", "Hongwei", ""], ["Zhao", "Miao", ""], ["Li", "Wenjie", ""], ["Deng", "Xiaotie", ""]]}, {"id": "1811.08012", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "A Comparative Study of Computational Aesthetics", "comments": "6 pages, 5 figures, 1 table", "journal-ref": "2014 IEEE International Conference on Image Processing (ICIP),\n  Paris, 2014, pp. 590-594", "doi": "10.1109/ICIP.2014.7025118", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective metrics model image quality by quantifying image degradations or\nestimating perceived image quality. However, image quality metrics do not model\nwhat makes an image more appealing or beautiful. In order to quantify the\naesthetics of an image, we need to take it one step further and model the\nperception of aesthetics. In this paper, we examine computational aesthetics\nmodels that use hand-crafted, generic and hybrid descriptors. We show that\ngeneric descriptors can perform as well as state of the art hand-crafted\naesthetics models that use global features. However, neither generic nor\nhand-crafted features is sufficient to model aesthetics when we only use global\nfeatures without considering spatial composition or distribution. We also\nfollow a visual dictionary approach similar to state of the art methods and\nshow that it performs poorly without the spatial pyramid step.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:46:12 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08412", "submitter": "Qian Wang", "authors": "Qian Wang, Ning Jia, Toby P. Breckon", "title": "A Baseline for Multi-Label Image Classification Using An Ensemble of\n  Deep Convolutional Neural Networks", "comments": "IEEE International Conference on Image Processing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on multi-label image classification have focused on designing\nmore complex architectures of deep neural networks such as the use of attention\nmechanisms and region proposal networks. Although performance gains have been\nreported, the backbone deep models of the proposed approaches and the\nevaluation metrics employed in different works vary, making it difficult to\ncompare each fairly. Moreover, due to the lack of properly investigated\nbaselines, the advantage introduced by the proposed techniques are often\nambiguous. To address these issues, we make a thorough investigation of the\nmainstream deep convolutional neural network architectures for multi-label\nimage classification and present a strong baseline. With the use of proper data\naugmentation techniques and model ensembles, the basic deep architectures can\nachieve better performance than many existing more complex ones on three\nbenchmark datasets, providing great insight for the future studies on\nmulti-label image classification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:34:22 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 15:30:10 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 10:06:24 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Qian", ""], ["Jia", "Ning", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1811.08429", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Boosting in Image Quality Assessment", "comments": "Paper: 6 pages, 5 tables, 1 figure, Presentation: 16 slides\n  [Ancillary files]", "journal-ref": "D. Temel and G. AlRegib, \"Boosting in image quality assessment,\"\n  2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP),\n  Montreal, QC, 2016, pp. 1-6", "doi": "10.1109/MMSP.2016.7813335", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the effect of boosting in image quality assessment\nthrough multi-method fusion. Existing multi-method studies focus on proposing a\nsingle quality estimator. On the contrary, we investigate the generalizability\nof multi-method fusion as a framework. In addition to support vector machines\nthat are commonly used in the multi-method fusion, we propose using neural\nnetworks in the boosting. To span different types of image quality assessment\nalgorithms, we use quality estimators based on fidelity, perceptually-extended\nfidelity, structural similarity, spectral similarity, color, and learning. In\nthe experiments, we perform k-fold cross validation using the LIVE, the\nmultiply distorted LIVE, and the TID 2013 databases and the performance of\nimage quality assessment algorithms are measured via accuracy-, linearity-, and\nranking-based metrics. Based on the experiments, we show that boosting methods\ngenerally improve the performance of image quality assessment and the level of\nimprovement depends on the type of the boosting algorithm. Our experimental\nresults also indicate that boosting the worst performing quality estimator with\ntwo or more additional methods leads to statistically significant performance\nenhancements independent of the boosting technique and neural network-based\nboosting outperforms support vector machine-based boosting when two or more\nmethods are fused.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:16:16 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08817", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Effectiveness of 3VQM in Capturing Depth Inconsistencies", "comments": "Paper: 5 pages, 1 figure, 1 table, Presentation: 15 slides [Ancillary\n  files]", "journal-ref": "D. Temel and G. AlRegib, \"Effectiveness of 3VQM in capturing depth\n  inconsistencies,\" IVMSP 2013, Seoul, 2013, pp. 1-4", "doi": "10.1109/IVMSPW.2013.6611918", "report-no": null, "categories": "eess.IV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D video quality metric (3VQM) was proposed to evaluate the temporal and\nspatial variation of the depth errors for the depth values that would lead to\ninconsistencies between left and right views, fast changing disparities, and\ngeometric distortions. Previously, we evaluated 3VQM against subjective scores.\nIn this paper, we show the effectiveness of 3VQM in capturing errors and\ninconsistencies that exist in the rendered depth-based 3D videos. We further\ninvestigate how 3VQM could measure excessive disparities, fast changing\ndisparities, geometric distortions, temporal flickering and/or spatial noise in\nthe form of depth cues inconsistency. Results show that 3VQM best captures the\ndepth inconsistencies based on errors in the reference views. However, the\nmetric is not sensitive to depth map mild errors such as those resulting from\nblur. We also performed a subjective quality test and showed that 3VQM performs\nbetter than PSNR, weighted PSNR and SSIM in terms of accuracy, coherency and\nconsistency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:37:15 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08821", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Coding of 3D Videos Based on Visual Discomfort", "comments": "Paper: 5 pages, 3 figures, 2 tables, Presentation: 20 slides\n  [Ancillary files]", "journal-ref": "2013 Asilomar Conference on Signals, Systems and Computers,\n  Pacific Grove, CA, 2013, pp. 1356-1360", "doi": "10.1109/ACSSC.2013.6810515", "report-no": null, "categories": "eess.IV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a rate-distortion optimization method for 3D videos based on\nvisual discomfort estimation. We calculate visual discomfort in the encoded\ndepth maps using two indexes: temporal outliers (TO) and spatial outliers (SO).\nThese two indexes are used to measure the difference between the processed\ndepth map and the ground truth depth map. These indexes implicitly depend on\nthe amount of edge information within a frame and on the amount of motion\nbetween frames. Moreover, we fuse these indexes considering the temporal and\nspatial complexities of the content. We test the proposed method on a number of\nvideos and compare the results with the default rate-distortion algorithms in\nthe H.264/AVC codec. We evaluate rate-distortion algorithms by comparing\nachieved bit-rates, visual degradations in the depth sequences and the fidelity\nof the depth videos measured by SSIM and PSNR.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:50:27 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08891", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "A Comparative Study of Quality and Content-Based Spatial Pooling\n  Strategies in Image Quality Assessment", "comments": "Paper: 5 pages, 8 figures, Presentation: 21 slides [Ancillary files]", "journal-ref": "2015 IEEE GlobalSIP, Orlando, FL, 2015, pp. 732-736", "doi": "10.1109/GlobalSIP.2015.7418293", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of quantifying image quality consists of engineering the quality\nfeatures and pooling these features to obtain a value or a map. There has been\na significant research interest in designing the quality features but pooling\nis usually overlooked compared to feature design. In this work, we compare the\nstate of the art quality and content-based spatial pooling strategies and show\nthat although features are the key in any image quality assessment, pooling\nalso matters. We also propose a quality-based spatial pooling strategy that is\nbased on linearly weighted percentile pooling (WPP). Pooling strategies are\nanalyzed for squared error, SSIM and PerSIM in LIVE, multiply distorted LIVE\nand TID2013 image databases.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:36:01 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08927", "submitter": "Dogancan Temel", "authors": "Mohit Prabhushankar and Dogancan Temel and Ghassan AlRegib", "title": "Generating Adaptive and Robust Filter Sets Using an Unsupervised\n  Learning Framework", "comments": "Paper:5 pages, 5 figures, 3 tables and Poster [Ancillary files]", "journal-ref": "2017 IEEE International Conference on Image Processing (ICIP),\n  Beijing, 2017, pp. 3041-3045", "doi": "10.1109/ICIP.2017.8296841", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an adaptive unsupervised learning framework,\nwhich utilizes natural images to train filter sets. The applicability of these\nfilter sets is demonstrated by evaluating their performance in two contrasting\napplications - image quality assessment and texture retrieval. While assessing\nimage quality, the filters need to capture perceptual differences based on\ndissimilarities between a reference image and its distorted version. In texture\nretrieval, the filters need to assess similarity between texture images to\nretrieve closest matching textures. Based on experiments, we show that the\nfilter responses span a set in which a monotonicity-based metric can measure\nboth the perceptual dissimilarity of natural images and the similarity of\ntexture images. In addition, we corrupt the images in the test set and\ndemonstrate that the proposed method leads to robust and reliable retrieval\nperformance compared to existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:02:33 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.08947", "submitter": "Dogancan Temel", "authors": "Mohit Prabhushankar and Dogancan Temel and Ghassan AlRegib", "title": "MS-UNIQUE: Multi-model and Sharpness-weighted Unsupervised Image Quality\n  Estimation", "comments": "Paper: 6 pages, 6 figures, 2 tables and Presentation: 21 slides\n  [Ancillary files]", "journal-ref": "The Electronic Imaging, IQSP XIV, Burlingame, California, USA,\n  Jan. 29 Feb. 2, 2017", "doi": "10.2352/ISSN.2470-1173.2017.12.IQSP-223", "report-no": null, "categories": "eess.IV cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we train independent linear decoder models to estimate the\nperceived quality of images. More specifically, we calculate the responses of\nindividual non-overlapping image patches to each of the decoders and scale\nthese responses based on the sharpness characteristics of filter set. We use\nmultiple linear decoders to capture different abstraction levels of the image\npatches. Training each model is carried out on 100,000 image patches from the\nImageNet database in an unsupervised fashion. Color space selection and ZCA\nWhitening are performed over these patches to enhance the descriptiveness of\nthe data. The proposed quality estimator is tested on the LIVE and the TID 2013\nimage quality assessment databases. Performance of the proposed method is\ncompared against eleven other state of the art methods in terms of accuracy,\nconsistency, linearity, and monotonic behavior. Based on experimental results,\nthe proposed method is generally among the top performing quality estimators in\nall categories.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:55:56 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.09192", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Oleksiy Ostapenko, Patrick J\\\"ahnichen, Tassilo Klein,\n  Moin Nabi", "title": "Self Paced Adversarial Training for Multimodal Few-shot Learning", "comments": "To appear at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning algorithms yield remarkable results in many\nvisual recognition tasks. However, they still fail to provide satisfactory\nresults in scarce data regimes. To a certain extent this lack of data can be\ncompensated by multimodal information. Missing information in one modality of a\nsingle data point (e.g. an image) can be made up for in another modality (e.g.\na textual description). Therefore, we design a few-shot learning task that is\nmultimodal during training (i.e. image and text) and single-modal during test\ntime (i.e. image). In this regard, we propose a self-paced class-discriminative\ngenerative adversarial network incorporating multimodality in the context of\nfew-shot learning. The proposed approach builds upon the idea of cross-modal\ndata generation in order to alleviate the data sparsity problem. We improve\nfew-shot learning accuracies on the finegrained CUB and Oxford-102 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 14:29:45 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Pahde", "Frederik", ""], ["Ostapenko", "Oleksiy", ""], ["J\u00e4hnichen", "Patrick", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1811.09301", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Image Quality Assessment and Color Difference", "comments": "Paper: 5 pages, 5 figures, 2 tables, and Presentation [Ancillary\n  files]", "journal-ref": "2014 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Atlanta, GA, 2014, pp. 970-974", "doi": "10.1109/GlobalSIP.2014.7032265", "report-no": null, "categories": "eess.IV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An average healthy person does not perceive the world in just black and\nwhite. Moreover, the perceived world is not composed of pixels and through\nvision humans perceive structures. However, the acquisition and display systems\ndiscretize the world. Therefore, we need to consider pixels, structures and\ncolors to model the quality of experience. Quality assessment methods use the\npixel-wise and structural metrics whereas color science approaches use the\npatch-based color differences. In this work, we combine these approaches by\nextending CIEDE2000 formula with perceptual color difference to assess image\nquality. We examine how perceptual color difference-based metric (PCDM)\nperforms compared to PSNR, CIEDE2000, SSIM, MS-SSIM and CW-SSIM on the LIVE\ndatabase. In terms of linear correlation, PCDM obtains compatible results under\nwhite noise (97.9%), Jpeg (95.9%) and Jp2k (95.6%) with an overall correlation\nof 92.7%. We also show that PCDM captures color-based artifacts that can not be\ncaptured by structure-based metrics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 20:35:34 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1811.09776", "submitter": "Heinrich S\\\"obke", "authors": "Heinrich S\\\"obke and Maria Reichelt", "title": "Sewer Rats in Teaching Action: An explorative field study on students'\n  perception of a game-based learning app in graduate engineering education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-based technologies and mobile learning aids open up many opportunities\nfor learners; however, evidence-based decisions on their appropriate use are\nnecessary. This explorative study (N = 100) examines the role of game elements\nin university education using a game-based learning app for mobile devices. The\neducational goal of the app is to support students in the field of engineering\nto memorize factual knowledge. The study investigates how the game-based app\naffects learners' motivation. It analyses the perceived impact and appeal as\nwell as the game elements as an incentive in learners' perception. To realize\nthis aim, the study combines structured methods like questionnaires with\nsemi-structured methods like thinking aloud, game diaries, and interviews. The\nresults indicate that flexible tem-poral and spatial use of the app was an\nimportant factor of learners' motivation. The app allowed more spontaneous\ninvolvement with the subject matter and the learners took advantage of an\nimproved attitude toward the subject matter. However, only a low impact on\nintrinsic motivation could be observed. We discuss reasons and present\npractical implications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 06:40:06 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["S\u00f6bke", "Heinrich", ""], ["Reichelt", "Maria", ""]]}, {"id": "1811.09967", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Ankit Shah, Bhiksha Raj, Alex Hauptmann", "title": "Learning Sound Events From Webly Labeled Data", "comments": "Accepted IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last couple of years, weakly labeled learning has turned out to be an\nexciting approach for audio event detection. In this work, we introduce webly\nlabeled learning for sound events which aims to remove human supervision\naltogether from the learning process. We first develop a method of obtaining\nlabeled audio data from the web (albeit noisy), in which no manual labeling is\ninvolved. We then describe methods to efficiently learn from these webly\nlabeled audio recordings. In our proposed system, WeblyNet, two deep neural\nnetworks co-teach each other to robustly learn from webly labeled data, leading\nto around 17% relative improvement over the baseline method. The method also\ninvolves transfer learning to obtain efficient representations\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 07:23:44 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 18:22:45 GMT"}, {"version": "v3", "created": "Sun, 16 Jun 2019 20:36:42 GMT"}, {"version": "v4", "created": "Sun, 14 Jul 2019 06:21:34 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kumar", "Anurag", ""], ["Shah", "Ankit", ""], ["Raj", "Bhiksha", ""], ["Hauptmann", "Alex", ""]]}, {"id": "1811.10175", "submitter": "Zongyi Xu", "authors": "Zongyi Xu, Qianni Zhang, Shiyang Cheng", "title": "Multilevel active registration for kinect human body scans: from low\n  quality to high quality", "comments": "14 pages, the Journal of Multimedia Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of 3D human body has been a challenging research topic for over\ndecades. Most of the traditional human body registration methods require manual\nassistance, or other auxiliary information such as texture and markers. The\nmajority of these methods are tailored for high-quality scans from expensive\nscanners. Following the introduction of the low-quality scans from\ncost-effective devices such as Kinect, the 3D data capturing of human body\nbecomes more convenient and easier. However, due to the inevitable holes,\nnoises and outliers in the low-quality scan, the registration of human body\nbecomes even more challenging. To address this problem, we propose a fully\nautomatic active registration method which deforms a high-resolution template\nmesh to match the low-quality human body scans. Our registration method\noperates on two levels of statistical shape models: (1) the first level is a\nholistic body shape model that defines the basic figure of human; (2) the\nsecond level includes a set of shape models for every body part, aiming at\ncapturing more body details. Our fitting procedure follows a coarse-to-fine\napproach that is robust and efficient. Experiments show that our method is\ncomparable with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 04:22:54 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Xu", "Zongyi", ""], ["Zhang", "Qianni", ""], ["Cheng", "Shiyang", ""]]}, {"id": "1811.10826", "submitter": "Arnav Dhamija", "authors": "Abhishek Thakur, Arnav Dhamija, Tejeshwar Reddy G", "title": "VECTORS: Video communication through opportunistic relays and scalable\n  video coding", "comments": "13 pages, 6 figures, and under 3000 words for submission to the\n  SoftwareX journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-sourced video distribution is frequently of interest in the local\nvicinity. In this paper, we propose a novel design to transfer such content\nover opportunistic networks with adaptive quality encoding to achieve\nreasonable delay bounds. The video segments are transmitted between source and\ndestination in a delay tolerant manner using the Nearby Connections Android\nlibrary. This implementation can be applied to multiple domains, including farm\nmonitoring, wildlife, and environmental tracking, disaster response scenarios,\netc. In this work, we present the design of an opportunistic contact based\nsystem, and we discuss basic results for the trial runs within our institute.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:10:51 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Thakur", "Abhishek", ""], ["Dhamija", "Arnav", ""], ["G", "Tejeshwar Reddy", ""]]}, {"id": "1811.11969", "submitter": "Lijun Yu", "authors": "Lijun Yu, Dawei Zhang, Xiangqun Chen, Alexander Hauptmann", "title": "Traffic Danger Recognition With Surveillance Cameras Without Training\n  Data", "comments": "To be published in proceedings of Advanced Video and Signal-based\n  Surveillance (AVSS), 2018 15th IEEE International Conference on, pp. 378-383,\n  IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a traffic danger recognition model that works with arbitrary\ntraffic surveillance cameras to identify and predict car crashes. There are too\nmany cameras to monitor manually. Therefore, we developed a model to predict\nand identify car crashes from surveillance cameras based on a 3D reconstruction\nof the road plane and prediction of trajectories. For normal traffic, it\nsupports real-time proactive safety checks of speeds and distances between\nvehicles to provide insights about possible high-risk areas. We achieve good\nprediction and recognition of car crashes without using any labeled training\ndata of crashes. Experiments on the BrnoCompSpeed dataset show that our model\ncan accurately monitor the road, with mean errors of 1.80% for distance\nmeasurement, 2.77 km/h for speed measurement, 0.24 m for car position\nprediction, and 2.53 km/h for speed prediction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:16:40 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yu", "Lijun", ""], ["Zhang", "Dawei", ""], ["Chen", "Xiangqun", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1811.12563", "submitter": "Tessie Chao", "authors": "Tianqi Zhao", "title": "Deep Multimodal Learning: An Effective Method for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos have become ubiquitous on the Internet. And video analysis can provide\nlots of information for detecting and recognizing objects as well as help\npeople understand human actions and interactions with the real world. However,\nfacing data as huge as TB level, effective methods should be applied. Recurrent\nneural network (RNN) architecture has wildly been used on many sequential\nlearning problems such as Language Model, Time-Series Analysis, etc. In this\npaper, we propose some variations of RNN such as stacked bidirectional LSTM/GRU\nnetwork with attention mechanism to categorize large-scale video data. We also\nexplore different multimodal fusion methods. Our model combines both visual and\naudio information on both video and frame level and received great result.\nEnsemble methods are also applied. Because of its multimodal characteristics,\nwe decide to call this method Deep Multimodal Learning(DML). Our DML-based\nmodel was trained on Google Cloud and our own server and was tested in a\nwell-known video classification competition on Kaggle held by Google.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:05:41 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhao", "Tianqi", ""]]}, {"id": "1811.12687", "submitter": "Ya Zhou", "authors": "Ya Zhou, Zhibo Chen, and Weiping Li", "title": "Hybrid Distortion Aggregated Visual Comfort Assessment for Stereoscopic\n  Image Retargeting", "comments": "13 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual comfort is a quite important factor in 3D media service. Few research\nefforts have been carried out in this area especially in case of 3D content\nretargeting which may introduce more complicated visual distortions. In this\npaper, we propose a Hybrid Distortion Aggregated Visual Comfort Assessment\n(HDA-VCA) scheme for stereoscopic retargeted images (SRI), considering\naggregation of hybrid distortions including structure distortion, information\nloss, binocular incongruity and semantic distortion. Specifically, a Local-SSIM\nfeature is proposed to reflect the local structural distortion of SRI, and\ninformation loss is represented by Dual Natural Scene Statistics (D-NSS)\nfeature extracted from the binocular summation and difference channels.\nRegarding binocular incongruity, visual comfort zone, window violation,\nbinocular rivalry, and accommodation-vergence conflict of human visual system\n(HVS) are evaluated. Finally, the semantic distortion is represented by the\ncorrelation distance of paired feature maps extracted from original\nstereoscopic image and its retargeted image by using trained deep neural\nnetwork. We validate the effectiveness of HDA-VCA on published Stereoscopic\nImage Retargeting Database (SIRD) and two stereoscopic image databases IEEE-SA\nand NBU 3D-VCA. The results demonstrate HDA-VCA's superior performance in\nhandling hybrid distortions compared to state-of-the-art VCA schemes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 09:46:29 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhou", "Ya", ""], ["Chen", "Zhibo", ""], ["Li", "Weiping", ""]]}, {"id": "1811.12915", "submitter": "Pawe{\\l} Korus", "authors": "Pawel Korus", "title": "Large-Scale and Fine-Grained Evaluation of Popular JPEG Forgery\n  Localization Schemes", "comments": "Supplementary materials for online code publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, researchers have proposed various approaches to JPEG forgery\ndetection and localization. In most cases, experimental evaluation was limited\nto JPEG quality levels that are multiples of 5 or 10. Each study used a\ndifferent dataset, making it difficult to directly compare the reported\nresults. The goal of this work is to perform a unified, large-scale and\nfine-grained evaluation of the most popular state-of-the-art detectors. The\nobtained results allow to compare the detectors with respect to various\ncriteria, and shed more light on the compression configurations where reliable\ntampering localization can be expected.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:57:58 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 18:40:07 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Korus", "Pawel", ""]]}]