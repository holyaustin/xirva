[{"id": "1810.00267", "submitter": "Wieslaw Kopec", "authors": "Kinga Skorupska, Manuel Nu\\~nez, Wies{\\l}aw Kope\\'c, Rados{\\l}aw\n  Nielek", "title": "Older Adults and Crowdsourcing: Android TV App for Evaluating TEDx\n  Subtitle Quality", "comments": null, "journal-ref": null, "doi": "10.1145/3274428", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the insights from an exploratory qualitative pilot\nstudy testing the feasibility of a solution that would encourage older adults\nto participate in online crowdsourcing tasks in a non-computer scenario.\nTherefore, we developed an Android TV application using Amara API to retrieve\nsubtitles for TEDx talks which allows the participants to detect and categorize\nerrors to support the quality of the translation and transcription processes.\nIt relies on the older adults' innate skills as long-time native language users\nand the motivating factors of this socially and personally beneficial task. The\nstudy allowed us to verify the underlying concept of using Smart TVs as\ninterfaces for crowdsourcing, as well as possible barriers, including the\ninterface, configuration issues, topics and the process itself. We have also\nassessed the older adults' interaction and engagement with this TV-enabled\nonline crowdsourcing task and we are convinced that the design of our setup\naddresses some key barriers to crowdsourcing by older adults. It also validates\navenues for further research in this area focused on such considerations as\nautonomy and freedom of choice, familiarity, physical and cognitive comfort as\nwell as building confidence and the edutainment value.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 21:18:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Skorupska", "Kinga", ""], ["Nu\u00f1ez", "Manuel", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""]]}, {"id": "1810.00549", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Ruipeng Chen, Lei Zhu, Zuping Zhang, Fang Huang,\n  Yunwu Lin", "title": "SVS-JOIN: Efficient Spatial Visual Similarity Join over Multimedia Data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the big data era, massive amount of multimedia data with geo-tags has been\ngenerated and collected by mobile smart devices equipped with mobile\ncommunications module and position sensor module. This trend has put forward\nhigher request on large-scale of geo-multimedia data retrieval. Spatial\nsimilarity join is one of the important problem in the area of spatial\ndatabase. Previous works focused on textual document with geo-tags, rather than\ngeo-multimedia data such as geo-images. In this paper, we study a novel search\nproblem named spatial visual similarity join (SVS-JOIN for short), which aims\nto find similar geo-image pairs in both the aspects of geo-location and visual\ncontent. We propose the definition of SVS-JOIN at the first time and present\nhow to measure geographical similarity and visual similarity. Then we introduce\na baseline inspired by the method for textual similarity join and a extension\nnamed SVS-JOIN$_G$ which applies spatial grid strategy to improve the\nefficiency. To further improve the performance of search, we develop a novel\napproach called SVS-JOIN$_Q$ which utilizes a quadtree and a global inverted\nindex. Experimental evaluations on real geo-image datasets demonstrate that our\nsolution has a really high performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 06:40:00 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Chen", "Ruipeng", ""], ["Zhu", "Lei", ""], ["Zhang", "Zuping", ""], ["Huang", "Fang", ""], ["Lin", "Yunwu", ""]]}, {"id": "1810.00644", "submitter": "Xiushan Nie", "authors": "Xingbo Liu and Xiushan Nie and Yilong Yin", "title": "Fusion Hashing: A General Framework for Self-improvement of Hashing", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely used for efficient similarity search based on its\nquery and storage efficiency. To obtain better precision, most studies focus on\ndesigning different objective functions with different constraints or penalty\nterms that consider neighborhood information. In this paper, in contrast to\nexisting hashing methods, we propose a novel generalized framework called\nfusion hashing (FH) to improve the precision of existing hashing methods\nwithout adding new constraints or penalty terms. In the proposed FH, given an\nexisting hashing method, we first execute it several times to get several\ndifferent hash codes for a set of training samples. We then propose two novel\nfusion strategies that combine these different hash codes into one set of final\nhash codes. Based on the final hash codes, we learn a simple linear hash\nfunction for the samples that can significantly improve model precision. In\ngeneral, the proposed FH can be adopted in existing hashing method and achieve\nmore precise and stable performance compared to the original hashing method\nwith little extra expenditure in terms of time and space. Extensive experiments\nwere performed based on three benchmark datasets and the results demonstrate\nthe superior performance of the proposed framework\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 12:12:35 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Xingbo", ""], ["Nie", "Xiushan", ""], ["Yin", "Yilong", ""]]}, {"id": "1810.01248", "submitter": "Xutan Peng", "authors": "Xutan Peng, Chenghua Lin, Chen Li, Zhi Cai, Jianxin Li", "title": "A Lightweight Music Texture Transfer System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning researches on the transformation problems for image and text\nhave raised great attention. However, present methods for music feature\ntransfer using neural networks are far from practical application. In this\npaper, we initiate a novel system for transferring the texture of music, and\nrelease it as an open source project. Its core algorithm is composed of a\nconverter which represents sounds as texture spectra, a corresponding\nreconstructor and a feed-forward transfer network. We evaluate this system from\nmultiple perspectives, and experimental results reveal that it achieves\nconvincing results in both sound effects and computational performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:03:53 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 15:00:30 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Peng", "Xutan", ""], ["Lin", "Chenghua", ""], ["Li", "Chen", ""], ["Cai", "Zhi", ""], ["Li", "Jianxin", ""]]}, {"id": "1810.01482", "submitter": "Houssam Nassif", "authors": "Houssam Nassif, Kemal Oral Cansizlar, Mitchell Goodman, and SVN\n  Vishwanathan", "title": "Diversifying Music Recommendations", "comments": "Machine Learning for Music Discovery Workshop at the 33rd\n  International Conference on Machine Learning (ICML'16), New York, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We compare submodular and Jaccard methods to diversify Amazon Music\nrecommendations. Submodularity significantly improves recommendation quality\nand user engagement. Unlike the Jaccard method, our submodular approach\nincorporates item relevance score within its optimization function, and\nproduces a relevant and uniformly diverse set.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:02:20 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Nassif", "Houssam", ""], ["Cansizlar", "Kemal Oral", ""], ["Goodman", "Mitchell", ""], ["Vishwanathan", "SVN", ""]]}, {"id": "1810.01520", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Markus Schedl, Paul Lamere, Ching-Wei Chen", "title": "An Analysis of Approaches Taken in the ACM RecSys Challenge 2018 for\n  Automatic Music Playlist Continuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ACM Recommender Systems Challenge 2018 focused on the task of automatic\nmusic playlist continuation, which is a form of the more general task of\nsequential recommendation. Given a playlist of arbitrary length with some\nadditional meta-data, the task was to recommend up to 500 tracks that fit the\ntarget characteristics of the original playlist. For the RecSys Challenge,\nSpotify released a dataset of one million user-generated playlists.\nParticipants could compete in two tracks, i.e., main and creative tracks.\nParticipants in the main track were only allowed to use the provided training\nset, however, in the creative track, the use of external public sources was\npermitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams\nsubmitted 239 runs to the creative track. The highest performing team in the\nmain track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average\nnumber of recommended songs clicks of 1.784. In the creative track, an\nR-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was\nobtained by the best team. This article provides an overview of the challenge,\nincluding motivation, task definition, dataset description, and evaluation. We\nfurther report and analyze the results obtained by the top performing teams in\neach track and explore the approaches taken by the winners. We finally\nsummarize our key findings, discuss generalizability of approaches and results\nto domains other than music, and list the open avenues and possible future\ndirections in the area of automatic playlist continuation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 21:19:19 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 22:13:33 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zamani", "Hamed", ""], ["Schedl", "Markus", ""], ["Lamere", "Paul", ""], ["Chen", "Ching-Wei", ""]]}, {"id": "1810.03226", "submitter": "Eunjeong Koh", "authors": "Eunjeong Stella Koh, Shlomo Dubnov, and Dustin Wright", "title": "Rethinking Recurrent Latent Variable Model for Music Composition", "comments": "Published as a conference paper at IEEE MMSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a model for capturing musical features and creating novel\nsequences of music, called the Convolutional Variational Recurrent Neural\nNetwork. To generate sequential data, the model uses an encoder-decoder\narchitecture with latent probabilistic connections to capture the hidden\nstructure of music. Using the sequence-to-sequence model, our generative model\ncan exploit samples from a prior distribution and generate a longer sequence of\nmusic. We compare the performance of our proposed model with other types of\nNeural Networks using the criteria of Information Rate that is implemented by\nVariable Markov Oracle, a method that allows statistical characterization of\nmusical information dynamics and detection of motifs in a song. Our results\nsuggest that the proposed model has a better statistical resemblance to the\nmusical structure of the training data, which improves the creation of new\nsequences of music in the style of the originals.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 23:22:56 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Koh", "Eunjeong Stella", ""], ["Dubnov", "Shlomo", ""], ["Wright", "Dustin", ""]]}, {"id": "1810.03402", "submitter": "Di Hu", "authors": "Di Hu, Feiping Nie, Xuelong Li", "title": "Deep LDA Hashing", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional supervised hashing methods based on classification do not\nentirely meet the requirements of hashing technique, but Linear Discriminant\nAnalysis (LDA) does. In this paper, we propose to perform a revised LDA\nobjective over deep networks to learn efficient hashing codes in a truly\nend-to-end fashion. However, the complicated eigenvalue decomposition within\neach mini-batch in every epoch has to be faced with when simply optimizing the\ndeep network w.r.t. the LDA objective. In this work, the revised LDA objective\nis transformed into a simple least square problem, which naturally overcomes\nthe intractable problems and can be easily solved by the off-the-shelf\noptimizer. Such deep extension can also overcome the weakness of LDA Hashing in\nthe limited linear projection and feature learning. Amounts of experiments are\nconducted on three benchmark datasets. The proposed Deep LDA Hashing shows\nnearly 70 points improvement over the conventional one on the CIFAR-10 dataset.\nIt also beats several state-of-the-art methods on various metrics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:27:35 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Hu", "Di", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1810.03414", "submitter": "Di Hu", "authors": "Di Hu, Feiping Nie, Xuelong Li", "title": "Dense Multimodal Fusion for Hierarchically Joint Representation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple modalities can provide more valuable information than single one by\ndescribing the same contents in various ways. Hence, it is highly expected to\nlearn effective joint representation by fusing the features of different\nmodalities. However, previous methods mainly focus on fusing the shallow\nfeatures or high-level representations generated by unimodal deep networks,\nwhich only capture part of the hierarchical correlations across modalities. In\nthis paper, we propose to densely integrate the representations by greedily\nstacking multiple shared layers between different modality-specific networks,\nwhich is named as Dense Multimodal Fusion (DMF). The joint representations in\ndifferent shared layers can capture the correlations in different levels, and\nthe connection between shared layers also provides an efficient way to learn\nthe dependence among hierarchical correlations. These two properties jointly\ncontribute to the multiple learning paths in DMF, which results in faster\nconvergence, lower training loss, and better performance. We evaluate our model\non three typical multimodal learning tasks, including audiovisual speech\nrecognition, cross-modal retrieval, and multimodal classification. The\nnoticeable performance in the experiments demonstrates that our model can learn\nmore effective joint representation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:52:36 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Hu", "Di", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1810.03916", "submitter": "Taha Alfaqheri T.A", "authors": "Taha Alfaqheri, Seif Allah El Mesloul Nasri, Abdul Hamid Sadka", "title": "3D Holoscopic Imaging for Cultural Heritage Digitalisation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing interest in archaeology has enabled the discovery of an immense\nnumber of cultural heritage assets and historical sites. Hence, preservation of\nCH through digitalisation is becoming a primordial requirement for many\ncountries as a part of national cultural programs. However, CH digitalisation\nis still posing serious challenges such as cost and time-consumption. In this\nmanuscript, 3D holoscopic (H3D) technology is applied to capture small sized CH\nassets. The H3D camera utilises micro lens array within a single aperture lens\nand typical 2D sensor to acquire 3D information. This technology allows 3D\nautostereoscopic visualisation with full motion parallax if convenient\nMicrolens Array (MLA)is used on the display side. Experimental works have shown\neasiness and simplicity of H3D acquisition compared to existing technologies.\nIn fact, H3D capture process took an equal time of shooting a standard 2D\nimage. These advantages qualify H3D technology to be cost effective and\ntime-saving technology for cultural heritage 3D digitisation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 11:23:13 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Alfaqheri", "Taha", ""], ["Nasri", "Seif Allah El Mesloul", ""], ["Sadka", "Abdul Hamid", ""]]}, {"id": "1810.03973", "submitter": "Zeqing Fu", "authors": "Zeqing Fu, Wei Hu, Zongming Guo", "title": "Local Frequency Interpretation and Non-Local Self-Similarity on Graph\n  for Point Cloud Inpainting", "comments": "11 pages, 11 figures, submitted to IEEE Transactions on Image\n  Processing at 2018.09.04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D scanning devices and depth sensors mature, point clouds have attracted\nincreasing attention as a format for 3D object representation, with\napplications in various fields such as tele-presence, navigation and heritage\nreconstruction. However, point clouds usually exhibit holes of missing data,\nmainly due to the limitation of acquisition techniques and complicated\nstructure. Further, point clouds are defined on irregular non-Euclidean\ndomains, which is challenging to address especially with conventional signal\nprocessing tools. Hence, leveraging on recent advances in graph signal\nprocessing, we propose an efficient point cloud inpainting method, exploiting\nboth the local smoothness and the non-local self-similarity in point clouds.\nSpecifically, we first propose a frequency interpretation in graph nodal\ndomain, based on which we introduce the local graph-signal smoothness prior in\norder to describe the local smoothness of point clouds. Secondly, we explore\nthe characteristics of non-local self-similarity, by globally searching for the\nmost similar area to the missing region. The similarity metric between two\nareas is defined based on the direct component and the anisotropic graph total\nvariation of normals in each area. Finally, we formulate the hole-filling step\nas an optimization problem based on the selected most similar area and\nregularized by the graph-signal smoothness prior. Besides, we propose\nvoxelization and automatic hole detection methods for the point cloud prior to\ninpainting. Experimental results show that the proposed approach outperforms\nfour competing methods significantly, both in objective and subjective quality.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 13:08:49 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Fu", "Zeqing", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""]]}, {"id": "1810.04401", "submitter": "Luca Rossetto M.Sc.", "authors": "Luca Rossetto, Heiko Schuldt, George Awad, Asad A. Butt", "title": "V3C - a Research Video Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of smartphones as recording devices and the massive\ngrowth in bandwidth, the number and volume of video collections has increased\nsignificantly in the last years. This poses novel challenges to the management\nof these large-scale video data and especially to the analysis of and retrieval\nfrom such video collections. At the same time, existing video datasets used for\nresearch and experimentation are either not large enough to represent current\ncollections or do not reflect the properties of video commonly found on the\nInternet in terms of content, length, or resolution. In this paper, we\nintroduce the Vimeo Creative Commons Collection, in short V3C, a collection of\n28'450 videos (with overall length of about 3'800 hours) published under\ncreative commons license on Vimeo. V3C comes with a shot segmentation for each\nvideo, together with the resulting keyframes in original as well as reduced\nresolution and additional metadata. It is intended to be used from 2019 at the\nInternational large-scale TREC Video Retrieval Evaluation campaign (TRECVid).\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 07:50:58 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 19:33:04 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Rossetto", "Luca", ""], ["Schuldt", "Heiko", ""], ["Awad", "George", ""], ["Butt", "Asad A.", ""]]}, {"id": "1810.04531", "submitter": "David Semedo", "authors": "David Semedo, Jo\\~ao Magalh\\~aes, Fl\\'avio Martins", "title": "Inferring User Gender from User Generated Visual Content on a Deep\n  Semantic Space", "comments": "To appear in EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the task of gender classification on picture sharing\nsocial media networks such as Instagram and Flickr. We aim to infer the gender\nof an user given only a small set of the images shared in its profile. We make\nthe assumption that user's images contain a collection of visual elements that\nimplicitly encode discriminative patterns that allow inferring its gender, in a\nlanguage independent way. This information can then be used in personalisation\nand recommendation. Our main hypothesis is that semantic visual features are\nmore adequate for discriminating high-level classes.\n  The gender detection task is formalised as: given an user's profile,\nrepresented as a bag of images, we want to infer the gender of the user. Social\nmedia profiles can be noisy and contain confounding factors, therefore we\nclassify bags of user-profile's images to provide a more robust prediction.\nExperiments using a dataset from the picture sharing social network Instagram\nshow that the use of multiple images is key to improve detection performance.\nMoreover, we verify that deep semantic features are more suited for gender\ndetection than low-level image representations. The methods proposed can infer\nthe gender with precision scores higher than 0.825, and the best performing\nmethod achieving 0.911 precision.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 13:50:04 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Semedo", "David", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""], ["Martins", "Fl\u00e1vio", ""]]}, {"id": "1810.04547", "submitter": "David Semedo", "authors": "David Semedo, Jo\\~ao Magalh\\~aes", "title": "Temporal Cross-Media Retrieval with Soft-Smoothing", "comments": "To appear in ACM MM 2018", "journal-ref": null, "doi": "10.1145/3240508.3240665", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia information have strong temporal correlations that shape the way\nmodalities co-occur over time. In this paper we study the dynamic nature of\nmultimedia and social-media information, where the temporal dimension emerges\nas a strong source of evidence for learning the temporal correlations across\nvisual and textual modalities. So far, cross-media retrieval models, explored\nthe correlations between different modalities (e.g. text and image) to learn a\ncommon subspace, in which semantically similar instances lie in the same\nneighbourhood. Building on such knowledge, we propose a novel temporal\ncross-media neural architecture, that departs from standard cross-media\nmethods, by explicitly accounting for the temporal dimension through temporal\nsubspace learning. The model is softly-constrained with temporal and\ninter-modality constraints that guide the new subspace learning task by\nfavouring temporal correlations between semantically similar and temporally\nclose instances. Experiments on three distinct datasets show that accounting\nfor time turns out to be important for cross-media retrieval. Namely, the\nproposed method outperforms a set of baselines on the task of temporal\ncross-media retrieval, demonstrating its effectiveness for performing temporal\nsubspace learning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 14:27:02 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Semedo", "David", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}, {"id": "1810.05263", "submitter": "Osama Alkishriwo", "authors": "Aya H. S. Abdelgader, Raneem A. Aboughalia, Osama A. S. Alkishriwo", "title": "Combined Image Encryption and Steganography Algorithm in the Spatial\n  Domain", "comments": "6 pages", "journal-ref": "First Conference for Engineering Sciences and Technology\n  (CEST-2018)", "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years, steganography has emerged as one of the main research areas\nin information security. Least significant bit (LSB) steganography is one of\nthe fundamental and conventional spatial domain methods, which is capable of\nhiding larger secret information in a cover image without noticeable visual\ndistortions. In this paper, a combined algorithm based on LSB steganography and\nchaotic encryption is proposed. Experimental results show the feasibility of\nthe proposed method. In comparison with existing steganographic spatial domain\nbased algorithms, the suggested algorithm is shown to have some advantages over\nexisting ones, namely, larger key space and a higher level of security against\nsome existing attacks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 21:39:39 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Abdelgader", "Aya H. S.", ""], ["Aboughalia", "Raneem A.", ""], ["Alkishriwo", "Osama A. S.", ""]]}, {"id": "1810.05810", "submitter": "Peng Gao", "authors": "Yipeng Ma, Chun Yuan, Peng Gao, Fei Wang", "title": "Efficient Multi-level Correlating for Visual Tracking", "comments": "Accepted by ACCV'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF) based tracking algorithms have demonstrated favorable\nperformance recently. Nevertheless, the top performance trackers always employ\ncomplicated optimization methods which constraint their real-time applications.\nHow to accelerate the tracking speed while retaining the tracking accuracy is a\nsignificant issue. In this paper, we propose a multi-level CF-based tracking\napproach named MLCFT which further explores the potential capacity of CF with\ntwo-stage detection: primal detection and oriented re-detection. The cascaded\ndetection scheme is simple but competent to prevent model drift and accelerate\nthe speed. An effective fusion method based on relative entropy is introduced\nto combine the complementary features extracted from deep and shallow layers of\nconvolutional neural networks (CNN). Moreover, a novel online model update\nstrategy is utilized in our tracker, which enhances the tracking performance\nfurther. Experimental results demonstrate that our proposed approach\noutperforms the most state-of-the-art trackers while tracking at speed of\nexceeded 16 frames per second on challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 06:58:46 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ma", "Yipeng", ""], ["Yuan", "Chun", ""], ["Gao", "Peng", ""], ["Wang", "Fei", ""]]}, {"id": "1810.06030", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Yunwu Lin, Lei Zhu, Anfeng Liu, Zuping Zhang, Fang\n  Huang", "title": "CNN-VWII: An Efficient Approach for Large-Scale Video Retrieval by Image\n  Queries", "comments": "submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to solve the problem of large-scale video retrieval by a\nquery image. Firstly, we define the problem of top-$k$ image to video query.\nThen, we combine the merits of convolutional neural networks(CNN for short) and\nBag of Visual Word(BoVW for short) module to design a model for video frames\ninformation extraction and representation. In order to meet the requirements of\nlarge-scale video retrieval, we proposed a visual weighted inverted index(VWII\nfor short) and related algorithm to improve the efficiency and accuracy of\nretrieval process. Comprehensive experiments show that our proposed technique\nachieves substantial improvements (up to an order of magnitude speed up) over\nthe state-of-the-art techniques with similar accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 13:00:39 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Lin", "Yunwu", ""], ["Zhu", "Lei", ""], ["Liu", "Anfeng", ""], ["Zhang", "Zuping", ""], ["Huang", "Fang", ""]]}, {"id": "1810.06219", "submitter": "Philipp Blandfort", "authors": "Tushar Karayil, Philipp Blandfort, J\\\"orn Hees, Andreas Dengel", "title": "The Focus-Aspect-Polarity Model for Predicting Subjective Noun\n  Attributes in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective visual interpretation is a challenging yet important topic in\ncomputer vision. Many approaches reduce this problem to the prediction of\nadjective- or attribute-labels from images. However, most of these do not take\nattribute semantics into account, or only process the image in a holistic\nmanner. Furthermore, there is a lack of relevant datasets with fine-grained\nsubjective labels. In this paper, we propose the Focus-Aspect-Polarity model to\nstructure the process of capturing subjectivity in image processing, and\nintroduce a novel dataset following this way of modeling. We run experiments on\nthis dataset to compare several deep learning methods and find that\nincorporating context information based on tensor multiplication in several\ncases outperforms the default way of information fusion (concatenation).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:14:38 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Karayil", "Tushar", ""], ["Blandfort", "Philipp", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "1810.06936", "submitter": "Alberto Garcia-Garcia", "authors": "Pablo Martinez-Gonzalez, Sergiu Oprea, Alberto Garcia-Garcia, Alvaro\n  Jover-Alvarez, Sergio Orts-Escolano, Jose Garcia-Rodriguez", "title": "UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation", "comments": "Published in Virtual Reality journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven algorithms have surpassed traditional techniques in almost every\naspect in robotic vision problems. Such algorithms need vast amounts of quality\ndata to be able to work properly after their training process. Gathering and\nannotating that sheer amount of data in the real world is a time-consuming and\nerror-prone task. Those problems limit scale and quality. Synthetic data\ngeneration has become increasingly popular since it is faster to generate and\nautomatic to annotate. However, most of the current datasets and environments\nlack realism, interactions, and details from the real world. UnrealROX is an\nenvironment built over Unreal Engine 4 which aims to reduce that reality gap by\nleveraging hyperrealistic indoor scenes that are explored by robot agents which\nalso interact with objects in a visually realistic manner in that simulated\nworld. Photorealistic scenes and robots are rendered by Unreal Engine into a\nvirtual reality headset which captures gaze so that a human operator can move\nthe robot and use controllers for the robotic hands; scene information is\ndumped on a per-frame basis so that it can be reproduced offline to generate\nraw data and ground truth annotations. This virtual reality environment enables\nrobotic vision researchers to generate realistic and visually plausible data\nwith full ground truth for a wide variety of problems such as class and\ninstance semantic segmentation, object detection, depth estimation, visual\ngrasping, and navigation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 11:43:50 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 17:58:02 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Martinez-Gonzalez", "Pablo", ""], ["Oprea", "Sergiu", ""], ["Garcia-Garcia", "Alberto", ""], ["Jover-Alvarez", "Alvaro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1810.07248", "submitter": "Ali Emami", "authors": "Mahdi Ahmadi, Alireza Norouzi, S.M.Reza Soroushmehr, Nader Karimi,\n  Kayvan Najarian, Shadrokh Samavi and Ali Emami", "title": "ReDMark: Framework for Residual Diffusion Watermarking on Deep Networks", "comments": "33 pages (Single column), 10 figures, 5 tables, one appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid growth of machine learning tools and specifically deep\nnetworks in various computer vision and image processing areas, application of\nConvolutional Neural Networks for watermarking have recently emerged. In this\npaper, we propose a deep end-to-end diffusion watermarking framework (ReDMark)\nwhich can be adapted for any desired transform space. The framework is composed\nof two Fully Convolutional Neural Networks with the residual structure for\nembedding and extraction. The whole deep network is trained end-to-end to\nconduct a blind secure watermarking. The framework is customizable for the\nlevel of robustness vs. imperceptibility. It is also adjustable for the\ntrade-off between capacity and robustness. The proposed framework simulates\nvarious attacks as a differentiable network layer to facilitate end-to-end\ntraining. For JPEG attack, a differentiable approximation is utilized, which\ndrastically improves the watermarking robustness to this attack. Another\nimportant characteristic of the proposed framework, which leads to improved\nsecurity and robustness, is its capability to diffuse watermark information\namong a relatively wide area of the image. Comparative results versus recent\nstate-of-the-art researches highlight the superiority of the proposed framework\nin terms of imperceptibility and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 23:07:15 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 09:53:39 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 09:32:01 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ahmadi", "Mahdi", ""], ["Norouzi", "Alireza", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Najarian", "Kayvan", ""], ["Samavi", "Shadrokh", ""], ["Emami", "Ali", ""]]}, {"id": "1810.07911", "submitter": "Zhiding Yu", "authors": "Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, Jinsong Wang", "title": "Domain Adaptation for Semantic Segmentation via Class-Balanced\n  Self-Training", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent deep networks achieved state of the art performance on a variety of\nsemantic segmentation tasks. Despite such progress, these models often face\nchallenges in real world `wild tasks' where large difference between labeled\ntraining/source data and unseen test/target data exists. In particular, such\ndifference is often referred to as `domain gap', and could cause significantly\ndecreased performance which cannot be easily remedied by further increasing the\nrepresentation power. Unsupervised domain adaptation (UDA) seeks to overcome\nsuch problem without target domain labels. In this paper, we propose a novel\nUDA framework based on an iterative self-training procedure, where the problem\nis formulated as latent variable loss minimization, and can be solved by\nalternatively generating pseudo labels on target data and re-training the model\nwith these labels. On top of self-training, we also propose a novel\nclass-balanced self-training framework to avoid the gradual dominance of large\nclasses on pseudo-label generation, and introduce spatial priors to refine\ngenerated labels. Comprehensive experiments show that the proposed methods\nachieve state of the art semantic segmentation performance under multiple major\nUDA settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 06:20:02 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:51:52 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zou", "Yang", ""], ["Yu", "Zhiding", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Wang", "Jinsong", ""]]}, {"id": "1810.08169", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang and Ming Jiang", "title": "Exploiting High-Level Semantics for No-Reference Image Quality\n  Assessment of Realistic Blur Images", "comments": "correct typos, e.g., \"avarage\" -> \"average\" in the figure of the\n  proposed framework", "journal-ref": "Proceedings of the 2017 ACM on Multimedia Conference", "doi": "10.1145/3123266.3123322", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To guarantee a satisfying Quality of Experience (QoE) for consumers, it is\nrequired to measure image quality efficiently and reliably. The neglect of the\nhigh-level semantic information may result in predicting a clear blue sky as\nbad quality, which is inconsistent with human perception. Therefore, in this\npaper, we tackle this problem by exploiting the high-level semantics and\npropose a novel no-reference image quality assessment method for realistic blur\nimages. Firstly, the whole image is divided into multiple overlapping patches.\nSecondly, each patch is represented by the high-level feature extracted from\nthe pre-trained deep convolutional neural network model. Thirdly, three\ndifferent kinds of statistical structures are adopted to aggregate the\ninformation from different patches, which mainly contain some common statistics\n(i.e., the mean\\&standard deviation, quantiles and moments). Finally, the\naggregated features are fed into a linear regression model to predict the image\nquality. Experiments show that, compared with low-level features, high-level\nfeatures indeed play a more critical role in resolving the aforementioned\nchallenging problem for quality estimation. Besides, the proposed method\nsignificantly outperforms the state-of-the-art methods on two realistic blur\nimage databases and achieves comparable performance on two synthetic blur image\ndatabases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 17:15:26 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "1810.08339", "submitter": "Qin He", "authors": "Qin He, Dingquan Li, Tingting Jiang, Ming Jiang", "title": "Quality Assessment for Tone-Mapped HDR Images Using Multi-Scale and\n  Multi-Layer Information", "comments": "This paper has 6 pages, 3 tables and 2 figures in total, corrects a\n  typo in the accepted version", "journal-ref": "Proceedings of 2018 IEEE International Conference on Multimedia\n  and Expo Workshops (ICMEW)", "doi": "10.1109/ICMEW.2018.8551502", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tone mapping operators and multi-exposure fusion methods allow us to enjoy\nthe informative contents of high dynamic range (HDR) images with standard\ndynamic range devices, but also introduce distortions into HDR contents.\nTherefore methods are needed to evaluate tone-mapped image quality. Due to the\ncomplexity of possible distortions in a tone-mapped image, information from\ndifferent scales and different levels should be considered when predicting\ntone-mapped image quality. So we propose a new no-reference method of\ntone-mapped image quality assessment based on multi-scale and multi-layer\nfeatures that are extracted from a pre-trained deep convolutional neural\nnetwork model. After being aggregated, the extracted features are mapped to\nquality predictions by regression. The proposed method is tested on the largest\npublic database for TMIQA and compared to existing no-reference methods. The\nexperimental results show that the proposed method achieves better performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 03:00:19 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 19:21:18 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["He", "Qin", ""], ["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "1810.09067", "submitter": "Zhihao Du", "authors": "Zhihao Du, Xueliang Zhang and Jiqing Han", "title": "Investigation of Monaural Front-End Processing for Robust ASR without\n  Retraining or Joint-Training", "comments": "5 pages, 0 figures, 4 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, monaural speech separation has been formulated as a\nsupervised learning problem, which has been systematically researched and shown\nthe dramatical improvement of speech intelligibility and quality for human\nlisteners. However, it has not been well investigated whether the methods can\nbe employed as the front-end processing and directly improve the performance of\na machine listener, i.e., an automatic speech recognizer, without retraining or\njoint-training the acoustic model. In this paper, we explore the effectiveness\nof the independent front-end processing for the multi-conditional trained ASR\non the CHiME-3 challenge. We find that directly feeding the enhanced features\nto ASR can make 36.40% and 11.78% relative WER reduction for the GMM-based and\nDNN-based ASR respectively. We also investigate the affect of noisy phase and\ngeneralization ability under unmatched noise condition.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 03:17:42 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 14:57:33 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Du", "Zhihao", ""], ["Zhang", "Xueliang", ""], ["Han", "Jiqing", ""]]}, {"id": "1810.09833", "submitter": "Weiqing Min", "authors": "Shuqiang Jiang and Weiqing Min and Shuhuan Mei", "title": "Hierarchy-Dependent Cross-Platform Multi-View Feature Learning for Venue\n  Category Prediction", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we focus on visual venue category prediction, which can\nfacilitate various applications for location-based service and personalization.\nConsidering that the complementarity of different media platforms, it is\nreasonable to leverage venue-relevant media data from different platforms to\nboost the prediction performance. Intuitively, recognizing one venue category\ninvolves multiple semantic cues, especially objects and scenes, and thus they\nshould contribute together to venue category prediction. In addition, these\nvenues can be organized in a natural hierarchical structure, which provides\nprior knowledge to guide venue category estimation. Taking these aspects into\naccount, we propose a Hierarchy-dependent Cross-platform Multi-view Feature\nLearning (HCM-FL) framework for venue category prediction from videos by\nleveraging images from other platforms. HCM-FL includes two major components,\nnamely Cross-Platform Transfer Deep Learning (CPTDL) and Multi-View Feature\nLearning with the Hierarchical Venue Structure (MVFL-HVS). CPTDL is capable of\nreinforcing the learned deep network from videos using images from other\nplatforms. Specifically, CPTDL first trained a deep network using videos. These\nimages from other platforms are filtered by the learnt network and these\nselected images are then fed into this learnt network to enhance it. Two kinds\nof pre-trained networks on the ImageNet and Places dataset are employed.\nMVFL-HVS is then developed to enable multi-view feature fusion. It is capable\nof embedding the hierarchical structure ontology to support more discriminative\njoint feature learning. We conduct the experiment on videos from Vine and\nimages from Foursqure. These experimental results demonstrate the advantage of\nour proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 13:19:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Jiang", "Shuqiang", ""], ["Min", "Weiqing", ""], ["Mei", "Shuhuan", ""]]}, {"id": "1810.10226", "submitter": "Hanbing Zhan", "authors": "Zhou Zhao, Hanbing Zhan, Lingtao Meng, Jun Xiao, Jun Yu, Min Yang, Fei\n  Wu, Deng Cai", "title": "Textually Guided Ranking Network for Attentional Image Retweet Modeling", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retweet prediction is a challenging problem in social media sites (SMS). In\nthis paper, we study the problem of image retweet prediction in social media,\nwhich predicts the image sharing behavior that the user reposts the image\ntweets from their followees. Unlike previous studies, we learn user preference\nranking model from their past retweeted image tweets in SMS. We first propose\nheterogeneous image retweet modeling network (IRM) that exploits users' past\nretweeted image tweets with associated contexts, their following relations in\nSMS and preference of their followees. We then develop a novel attentional\nmulti-faceted ranking network learning framework with textually guided\nmulti-modal neural networks for the proposed heterogenous IRM network to learn\nthe joint image tweet representations and user preference representations for\nprediction task. The extensive experiments on a large-scale dataset from\nTwitter site shows that our method achieves better performance than other\nstate-of-the-art solutions to the problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 07:43:20 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhao", "Zhou", ""], ["Zhan", "Hanbing", ""], ["Meng", "Lingtao", ""], ["Xiao", "Jun", ""], ["Yu", "Jun", ""], ["Yang", "Min", ""], ["Wu", "Fei", ""], ["Cai", "Deng", ""]]}, {"id": "1810.11137", "submitter": "Kedar Tatwawadi", "authors": "Ashutosh Bhown, Soham Mukherjee, Sean Yang, Shubham Chandak, Irena\n  Fischer-Hwang, Kedar Tatwawadi, Judith Fan, Tsachy Weissman", "title": "Towards improved lossy image compression: Human image reconstruction\n  with public-domain images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression has been studied extensively in the context of\ntypical loss functions such as RMSE, MS-SSIM, etc. However, compression at low\nbitrates generally produces unsatisfying results. Furthermore, the availability\nof massive public image datasets appears to have hardly been exploited in image\ncompression. Here, we present a paradigm for eliciting human image\nreconstruction in order to perform lossy image compression. In this paradigm,\none human describes images to a second human, whose task is to reconstruct the\ntarget image using publicly available images and text instructions. The\nresulting reconstructions are then evaluated by human raters on the Amazon\nMechanical Turk platform and compared to reconstructions obtained using\nstate-of-the-art compressor WebP. Our results suggest that prioritizing\nsemantic visual elements may be key to achieving significant improvements in\nimage compression, and that our paradigm can be used to develop a more\nhuman-centric loss function.\n  The images, results and additional data are available at\nhttps://compression.stanford.edu/human-compression\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 23:34:37 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 04:26:49 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 03:01:38 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Bhown", "Ashutosh", ""], ["Mukherjee", "Soham", ""], ["Yang", "Sean", ""], ["Chandak", "Shubham", ""], ["Fischer-Hwang", "Irena", ""], ["Tatwawadi", "Kedar", ""], ["Fan", "Judith", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1810.11801", "submitter": "Bo Fu", "authors": "Bo Fu, Yi Li, Xianghai Wang", "title": "Image Super-Resolution Using TV Priori Guided Convolutional Network", "comments": "This paper is underviewring in Journal of Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We proposed a TV priori information guided deep learning method for single\nimage super-resolution(SR). The new alogorithm up-sample method based on TV\npriori, new learning method and neural networks architecture are embraced in\nour TV guided priori Convolutional Neural Network which diretcly learns an end\nto end mapping between the low level to high level images.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 11:49:24 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Fu", "Bo", ""], ["Li", "Yi", ""], ["Wang", "Xianghai", ""]]}, {"id": "1810.11973", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu", "title": "Feature Bagging for Steganographer Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional steganalysis algorithms focus on detecting the existence of\nsteganography in a single object. In practice, one may face a complex scenario\nwhere one or some of multiple users also called actors are guilty of using\nsteganography, which is defined as the steganographer identification problem\n(SIP). This requires steganalysis experts to design effective and robust\ndetection algorithms to identify the guilty actor(s). The mainstream works use\nclustering, ensemble and anomaly detection, where distances in high dimensional\nspace between features of actors are determined to find out the outlier(s)\ncorresponding to steganographer(s). However, in high dimensional space, feature\npoints could be sparse such that distances between feature points may become\nrelatively similar to each other, which cannot benefit the detection. Moreover,\nit is well-known in machine learning that combining techniques such as boosting\nand bagging can be effective in improving detection performance. This motivates\nthe authors in this paper to present a feature bagging approach to SIP. The\nproposed work merges results from multiple detection sub-models, each of which\nfeature space is randomly sampled from the raw full dimensional space. We\ncreate a new dataset called ImgNetEase including 5108 images downloaded from a\nsocial website to mimic the real-world scenario. We extract PEV-274 features\nfrom images, and take nsF5 as the steganographic algorithm for evaluation.\nExperiments have shown that our work improves the detection accuracy\nsignificantly on created dataset in most cases, which has shown the superiority\nand applicability.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 06:30:54 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Wu", "Hanzhou", ""]]}, {"id": "1810.12521", "submitter": "Yi Zhu", "authors": "Yi Zhu and Jia Xue and Shawn Newsam", "title": "Gated Transfer Network for Transfer Learning", "comments": "Accepted at ACCV 2018. Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have led to a series of breakthroughs in computer vision\ngiven sufficient annotated training datasets. For novel tasks with limited\nlabeled data, the prevalent approach is to transfer the knowledge learned in\nthe pre-trained models to the new tasks by fine-tuning. Classic model\nfine-tuning utilizes the fact that well trained neural networks appear to learn\ncross domain features. These features are treated equally during transfer\nlearning. In this paper, we explore the impact of feature selection in model\nfine-tuning by introducing a transfer module, which assigns weights to features\nextracted from pre-trained models. The proposed transfer module proves the\nimportance of feature selection for transferring models from source to target\ndomains. It is shown to significantly improve upon fine-tuning results with\nonly marginal extra computational cost. We also incorporate an auxiliary\nclassifier as an extra regularizer to avoid over-fitting. Finally, we build a\nGated Transfer Network (GTN) based on our transfer module and achieve\nstate-of-the-art results on six different tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:31:48 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Xue", "Jia", ""], ["Newsam", "Shawn", ""]]}, {"id": "1810.12522", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Random Temporal Skipping for Multirate Video Analysis", "comments": "Accepted at ACCV 2018. Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches to video understanding adopt temporal\njittering to simulate analyzing the video at varying frame rates. However, this\ndoes not work well for multirate videos, in which actions or subactions occur\nat different speeds. The frame sampling rate should vary in accordance with the\ndifferent motion speeds. In this work, we propose a simple yet effective\nstrategy, termed random temporal skipping, to address this situation. This\nstrategy effectively handles multirate videos by randomizing the sampling rate\nduring training. It is an exhaustive approach, which can potentially cover all\nmotion speed variations. Furthermore, due to the large temporal skipping, our\nnetwork can see video clips that originally cover over 100 frames. Such a time\nrange is enough to analyze most actions/events. We also introduce an\nocclusion-aware optical flow learning method that generates improved motion\nmaps for human action recognition. Our framework is end-to-end trainable, runs\nin real-time, and achieves state-of-the-art performance on six widely adopted\nvideo benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:35:43 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1810.12568", "submitter": "Xi Zhang", "authors": "Xi Zhang, Xiaolin Wu", "title": "Nonlinear Prediction of Multidimensional Signals via Deep Regression\n  with Applications to Image Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) have enjoyed great successes in\nmany signal processing applications because they can learn complex, non-linear\ncausal relationships from input to output. In this light, DCNNs are well suited\nfor the task of sequential prediction of multidimensional signals, such as\nimages, and have the potential of improving the performance of traditional\nlinear predictors. In this research we investigate how far DCNNs can push the\nenvelop in terms of prediction precision. We propose, in a case study, a\ntwo-stage deep regression DCNN framework for nonlinear prediction of\ntwo-dimensional image signals. In the first-stage regression, the proposed deep\nprediction network (PredNet) takes the causal context as input and emits a\nprediction of the present pixel. Three PredNets are trained with the regression\nobjectives of minimizing $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ norms of\nprediction residuals, respectively. The second-stage regression combines the\noutputs of the three PredNets to generate an even more precise and robust\nprediction. The proposed deep regression model is applied to lossless\npredictive image coding, and it outperforms the state-of-the-art linear\npredictors by appreciable margin.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:14:11 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1810.13151", "submitter": "Chenghao Yang", "authors": "Jing Yu, Chenghao Yang, Zengchang Qin, Zhuoqian Yang, Yue Hu and\n  Weifeng Zhang", "title": "Semantic Modeling of Textual Relationships in Cross-Modal Retrieval", "comments": "To appear in KSEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature modeling of different modalities is a basic problem in current\nresearch of cross-modal information retrieval. Existing models typically\nproject texts and images into one embedding space, in which semantically\nsimilar information will have a shorter distance. Semantic modeling of textural\nrelationships is notoriously difficult. In this paper, we propose an approach\nto model texts using a featured graph by integrating multi-view textual\nrelationships including semantic relations, statistical co-occurrence, and\nprior relations in the knowledge base. A dual-path neural network is adopted to\nlearn multi-modal representations of information and cross-modal similarity\nmeasure jointly. We use a Graph Convolutional Network (GCN) for generating\nrelation-aware text representations, and use a Convolutional Neural Network\n(CNN) with non-linearities for image representations. The cross-modal\nsimilarity measure is learned by distance metric learning. Experimental results\nshow that, by leveraging the rich relational semantics in texts, our model can\noutperform the state-of-the-art models by 3.4% and 6.3% on accuracy on two\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 08:24:15 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 13:24:59 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 10:35:01 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Yu", "Jing", ""], ["Yang", "Chenghao", ""], ["Qin", "Zengchang", ""], ["Yang", "Zhuoqian", ""], ["Hu", "Yue", ""], ["Zhang", "Weifeng", ""]]}]