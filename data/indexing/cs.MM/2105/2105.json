[{"id": "2105.00136", "submitter": "Haifan Gong", "authors": "Haifan Gong, Guanqi Chen, Sishuo Liu, Yizhou Yu, Guanbin Li", "title": "Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical\n  Visual Question Answering", "comments": "ICMR '21: ACM International Conference on Multimedia Retrieval,\n  Taipei, Taiwan, August 21-24, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the severe lack of labeled data, existing methods of medical visual\nquestion answering usually rely on transfer learning to obtain effective image\nfeature representation and use cross-modal fusion of visual and linguistic\nfeatures to achieve question-related answer prediction. These two phases are\nperformed independently and without considering the compatibility and\napplicability of the pre-trained features for cross-modal fusion. Thus, we\nreformulate image feature pre-training as a multi-task learning paradigm and\nwitness its extraordinary superiority, forcing it to take into account the\napplicability of features for the specific image comprehension task.\nFurthermore, we introduce a cross-modal self-attention~(CMSA) module to\nselectively capture the long-range contextual relevance for more effective\nfusion of visual and linguistic features. Experimental results demonstrate that\nthe proposed method outperforms existing state-of-the-art methods. Our code and\nmodels are available at https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:49:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gong", "Haifan", ""], ["Chen", "Guanqi", ""], ["Liu", "Sishuo", ""], ["Yu", "Yizhou", ""], ["Li", "Guanbin", ""]]}, {"id": "2105.00397", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Linchao Zhu, Bang Zhang, Pan Pan, Yi Yang", "title": "OR-Net: Pointwise Relational Inference for Data Completion under Partial\n  Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contemporary data-driven methods are typically fed with full supervision on\nlarge-scale datasets which limits their applicability. However, in the actual\nsystems with limitations such as measurement error and data acquisition\nproblems, people usually obtain incomplete data. Although data completion has\nattracted wide attention, the underlying data pattern and relativity are still\nunder-developed. Currently, the family of latent variable models allows\nlearning deep latent variables over observed variables by fitting the marginal\ndistribution. As far as we know, current methods fail to perceive the data\nrelativity under partial observation. Aiming at modeling incomplete data, this\nwork uses relational inference to fill in the incomplete data. Specifically, we\nexpect to approximate the real joint distribution over the partial observation\nand latent variables, thus infer the unseen targets respectively. To this end,\nwe propose Omni-Relational Network (OR-Net) to model the pointwise relativity\nin two aspects: (i) On one hand, the inner relationship is built among the\ncontext points in the partial observation; (ii) On the other hand, the unseen\ntargets are inferred by learning the cross-relationship with the observed data\npoints. It is further discovered that the proposed method can be generalized to\ndifferent scenarios regardless of whether the physical structure can be\nobserved or not. It is demonstrated that the proposed OR-Net can be well\ngeneralized for data completion tasks of various modalities, including function\nregression, image completion on MNIST and CelebA datasets, and also sequential\nmotion generation conditioned on the observed poses.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 06:05:54 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 09:28:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Feng", "Qianyu", ""], ["Zhu", "Linchao", ""], ["Zhang", "Bang", ""], ["Pan", "Pan", ""], ["Yang", "Yi", ""]]}, {"id": "2105.00567", "submitter": "Roberto Azevedo", "authors": "Roberto G. de A. Azevedo, Neil Birkbeck, Ivan Janatra, Balu Adsumilli,\n  Pascal Frossard", "title": "Multi-feature 360 Video Quality Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/OJCAS.2021.3073891", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a new method for the visual quality assessment of 360-degree\n(omnidirectional) videos. The proposed method is based on computing multiple\nspatio-temporal objective quality features on viewports extracted from\n360-degree videos. A new model is learnt to properly combine these features\ninto a metric that closely matches subjective quality scores. The main\nmotivations for the proposed approach are that: 1) quality metrics computed on\nviewports better captures the user experience than metrics computed on the\nprojection domain; 2) the use of viewports easily supports different projection\nmethods being used in current 360-degree video systems; and 3) no individual\nobjective image quality metric always performs the best for all types of visual\ndistortions, while a learned combination of them is able to adapt to different\nconditions. Experimental results, based on both the largest available\n360-degree videos quality dataset and a cross-dataset validation, demonstrate\nthat the proposed metric outperforms state-of-the-art 360-degree and 2D video\nquality metrics.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 22:40:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Azevedo", "Roberto G. de A.", ""], ["Birkbeck", "Neil", ""], ["Janatra", "Ivan", ""], ["Adsumilli", "Balu", ""], ["Frossard", "Pascal", ""]]}, {"id": "2105.00641", "submitter": "Hanne Stenzel Dr", "authors": "Hanne Stenzel, Davide Berghi, Marco Volino, Philip J.B. Jackson", "title": "Naturalistic audio-visual volumetric sequences dataset of sounding\n  actions for six degree-of-freedom interaction", "comments": "for dataset visit cvssp.org/data/navvs; accepted as poster in IEEE VR\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As audio-visual systems increasingly bring immersive and interactive\ncapabilities into our work and leisure activities, so the need for naturalistic\ntest material grows. New volumetric datasets have captured high-quality 3D\nvideo, but accompanying audio is often neglected, making it hard to test an\nintegrated bimodal experience. Designed to cover diverse sound types and\nfeatures, the presented volumetric dataset was constructed from audio and video\nstudio recordings of scenes to yield forty short action sequences. Potential\nuses in technical and scientific tests are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 06:16:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Stenzel", "Hanne", ""], ["Berghi", "Davide", ""], ["Volino", "Marco", ""], ["Jackson", "Philip J. B.", ""]]}, {"id": "2105.00708", "submitter": "Yan-Bo Lin", "authors": "Yan-Bo Lin and Yu-Chiang Frank Wang", "title": "Exploiting Audio-Visual Consistency with Partial Supervision for Spatial\n  Audio Generation", "comments": "AAAI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human perceives rich auditory experience with distinct sound heard by ears.\nVideos recorded with binaural audio particular simulate how human receives\nambient sound. However, a large number of videos are with monaural audio only,\nwhich would degrade the user experience due to the lack of ambient information.\nTo address this issue, we propose an audio spatialization framework to convert\na monaural video into a binaural one exploiting the relationship across audio\nand visual components. By preserving the left-right consistency in both audio\nand visual modalities, our learning strategy can be viewed as a self-supervised\nlearning technique, and alleviates the dependency on a large amount of video\ndata with ground truth binaural audio data during training. Experiments on\nbenchmark datasets confirm the effectiveness of our proposed framework in both\nsemi-supervised and fully supervised scenarios, with ablation studies and\nvisualization further support the use of our model for audio spatialization.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:34:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lin", "Yan-Bo", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2105.01415", "submitter": "Xiao Yan", "authors": "Xiao Yan, Zhixiong Di, Bowen Huang, Minjiang Li, Wenqiang Wang,\n  Xiaoyang Zeng, Yibo Fan", "title": "A Power and Area Efficient Lepton Hardware Encoder with Hash-based\n  Memory Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it has been surpassed by many subsequent coding standards, JPEG\noccupies a large share of the storage load of the current data hosting service.\nTo reduce the storage costs, DropBox proposed a lossless secondary compression\nalgorithm, Lepton, to further improve the compression rate of JPEG images.\nHowever, the bloated probability models defined by Lepton severely restrict its\nthroughput and energy efficiency. To solve this problem, we construct an\nefficient access probability-based hash function for the probability models,\nand then propose a hardware-friendly memory optimization method by combining\nthe proposed hash function and the N-way Set-Associative unit. After that, we\ndesign a highly parameterized hardware structure for the probability models and\nfinally implement a power and area efficient Lepton hardware encoder. To the\nbest of our knowledge, this is the first hardware implementation of Lepton. The\nsynthesis result shows that the proposed hardware structure reduces the total\narea of the probability models by 70.97%. Compared with DropBox's software\nsolution, the throughput and the energy efficiency of the proposed Lepton\nhardware encoder are increased by 55.25 and 4899 times respectively. In terms\nof manufacturing cost, the proposed Lepton hardware encoder is also\nsignificantly lower than the general-purpose CPU used by DropBox.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:02:41 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Yan", "Xiao", ""], ["Di", "Zhixiong", ""], ["Huang", "Bowen", ""], ["Li", "Minjiang", ""], ["Wang", "Wenqiang", ""], ["Zeng", "Xiaoyang", ""], ["Fan", "Yibo", ""]]}, {"id": "2105.01466", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Gerhard Hagerer, Bj\\\"orn W. Schuller, Georg Groh", "title": "Unsupervised Graph-based Topic Modeling from Video Transcriptions", "comments": "JT and LS contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To unfold the tremendous amount of audiovisual data uploaded daily to social\nmedia platforms, effective topic modelling techniques are needed. Existing work\ntends to apply variants of topic models on text data sets. In this paper, we\naim at developing a topic extractor on video transcriptions. The model improves\ncoherence by exploiting neural word embeddings through a graph-based clustering\nmethod. Unlike typical topic models, this approach works without knowing the\ntrue number of topics. Experimental results on the real-life multimodal data\nset MuSe-CaR demonstrates that our approach extracts coherent and meaningful\ntopics, outperforming baseline methods. Furthermore, we successfully\ndemonstrate the generalisability of our approach on a pure text review data\nset.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:48:17 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Stappen", "Lukas", ""], ["Hagerer", "Gerhard", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Groh", "Georg", ""]]}, {"id": "2105.01475", "submitter": "Luca Rossetto PhD", "authors": "Luca Rossetto, Klaus Schoeffmann, Abraham Bernstein", "title": "Insights on the V3C2 Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For research results to be comparable, it is important to have common\ndatasets for experimentation and evaluation. The size of such datasets,\nhowever, can be an obstacle to their use. The Vimeo Creative Commons Collection\n(V3C) is a video dataset designed to be representative of video content found\non the web, containing roughly 3800 hours of video in total, split into three\nshards. In this paper, we present insights on the second of these shards (V3C2)\nand discuss their implications for research areas, such as video retrieval, for\nwhich the dataset might be particularly useful. We also provide all the\nextracted data in order to simplify the use of the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 13:10:00 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Rossetto", "Luca", ""], ["Schoeffmann", "Klaus", ""], ["Bernstein", "Abraham", ""]]}, {"id": "2105.01633", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Michelle Lienhart, Annalena B\\\"atz,\n  Bj\\\"orn Schuller", "title": "An Estimation of Online Video User Engagement from Features of\n  Continuous Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Portraying emotion and trustworthiness is known to increase the appeal of\nvideo content. However, the causal relationship between these signals and\nonline user engagement is not well understood. This limited understanding is\npartly due to a scarcity in emotionally annotated data and the varied\nmodalities which express user engagement online. In this contribution, we\nutilise a large dataset of YouTube review videos which includes ca. 600 hours\nof dimensional arousal, valence and trustworthiness annotations. We investigate\nfeatures extracted from these signals against various user engagement\nindicators including views, like/dislike ratio, as well as the sentiment of\ncomments. In doing so, we identify the positive and negative influences which\nsingle features have, as well as interpretable patterns in each dimension which\nrelate to user engagement. Our results demonstrate that smaller boundary ranges\nand fluctuations for arousal lead to an increase in user engagement.\nFurthermore, the extracted time-series features reveal significant (p<0.05)\ncorrelations for each dimension, such as, count below signal mean (arousal),\nnumber of peaks (valence), and absolute energy (trustworthiness). From this, an\neffective combination of features is outlined for approaches aiming to\nautomatically predict several user engagement indicators. In a user engagement\nprediction paradigm we compare all features against semi-automatic\n(cross-task), and automatic (task-specific) feature selection methods. These\nselected feature sets appear to outperform the usage of all features, e.g.,\nusing all features achieves 1.55 likes per day (Lp/d) mean absolute error from\nvalence; this improves through semi-automatic and automatic selection to 1.33\nand 1.23 Lp/d, respectively (data mean 9.72 Lp/d with a std. 28.75 Lp/d).\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:18:47 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Lienhart", "Michelle", ""], ["B\u00e4tz", "Annalena", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "2105.01701", "submitter": "Amaya Dharmasiri", "authors": "Amaya Dharmasiri, Chamara Kattadige, Vincent Zhang, Kanchana\n  Thilakarathna", "title": "Viewport-Aware Dynamic 360{\\deg} Video Segment Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unlike conventional videos, 360{\\deg} videos give freedom to users to turn\ntheir heads, watch and interact with the content owing to its immersive\nspherical environment. Although these movements are arbitrary, similarities can\nbe observed between viewport patterns of different users and different videos.\nIdentifying such patterns can assist both content and network providers to\nenhance the 360{\\deg} video streaming process, eventually increasing the\nend-user Quality of Experience (QoE). But a study on how viewport patterns\ndisplay similarities across different video content, and their potential\napplications has not yet been done. In this paper, we present a comprehensive\nanalysis of a dataset of 88 360{\\deg} videos and propose a novel video\ncategorization algorithm that is based on similarities of viewports. First, we\npropose a novel viewport clustering algorithm that outperforms the existing\nalgorithms in terms of clustering viewports with similar positioning and speed.\nNext, we develop a novel and unique dynamic video segment categorization\nalgorithm that shows notable improvement in similarity for viewport\ndistributions within the clusters when compared to that of existing static\nvideo categorizations.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:47:30 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 12:16:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Dharmasiri", "Amaya", ""], ["Kattadige", "Chamara", ""], ["Zhang", "Vincent", ""], ["Thilakarathna", "Kanchana", ""]]}, {"id": "2105.01705", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc Gorriz Blanch, Issa Khalifeh, Alan Smeaton, Noel O'Connor, Marta\n  Mrak", "title": "Attention-based Stylisation for Exemplar Image Colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exemplar-based colourisation aims to add plausible colours to a grayscale\nimage using the guidance of a colour reference image. Most of the existing\nmethods tackle the task as a style transfer problem, using a convolutional\nneural network (CNN) to obtain deep representations of the content of both\ninputs. Stylised outputs are then obtained by computing similarities between\nboth feature representations in order to transfer the style of the reference to\nthe content of the target input. However, in order to gain robustness towards\ndissimilar references, the stylised outputs need to be refined with a second\ncolourisation network, which significantly increases the overall system\ncomplexity. This work reformulates the existing methodology introducing a novel\nend-to-end colourisation network that unifies the feature matching with the\ncolourisation process. The proposed architecture integrates attention modules\nat different resolutions that learn how to perform the style transfer task in\nan unsupervised way towards decoding realistic colour predictions. Moreover,\naxial attention is proposed to simplify the attention operations and to obtain\na fast but robust cost-effective architecture. Experimental validations\ndemonstrate efficiency of the proposed methodology which generates high quality\nand visual appealing colourisation. Furthermore, the complexity of the proposed\nmethodology is reduced compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:56:26 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Blanch", "Marc Gorriz", ""], ["Khalifeh", "Issa", ""], ["Smeaton", "Alan", ""], ["O'Connor", "Noel", ""], ["Mrak", "Marta", ""]]}, {"id": "2105.02409", "submitter": "Zhi Wang Dr.", "authors": "Zhi Wang, Wenwu Zhu, Lifeng Sun, Han Hu, Ge Ma, Ming Ma, Haitian Pang,\n  Jiahui Ye, Hongshan Li", "title": "Multimedia Edge Computing", "comments": "20 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1702.07627", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we investigate the recent studies on multimedia edge\ncomputing, from sensing not only traditional visual/audio data but also\nindividuals' geographical preference and mobility behaviors, to performing\ndistributed machine learning over such data using the joint edge and cloud\ninfrastructure and using evolutional strategies like reinforcement learning and\nonline learning at edge devices to optimize the quality of experience for\nmultimedia services at the last mile proactively. We provide both a\nretrospective view of recent rapid migration (resp. merge) of cloud multimedia\nto (resp. and) edge-aware multimedia and insights on the fundamental guidelines\nfor designing multimedia edge computing strategies that target satisfying the\nchanging demand of quality of experience. By showing the recent research\nstudies and industrial solutions, we also provide future directions towards\nhigh-quality multimedia services over edge computing.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:01:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Zhi", ""], ["Zhu", "Wenwu", ""], ["Sun", "Lifeng", ""], ["Hu", "Han", ""], ["Ma", "Ge", ""], ["Ma", "Ming", ""], ["Pang", "Haitian", ""], ["Ye", "Jiahui", ""], ["Li", "Hongshan", ""]]}, {"id": "2105.02636", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer and Cigdem Beyan and Fabian Ruth and Olaf Kramer and\n  Ulrich Trautwein and Enkelejda Kasneci", "title": "Estimating Presentation Competence using Multimodal Nonverbal Behavioral\n  Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public speaking and presentation competence plays an essential role in many\nareas of social interaction in our educational, professional, and everyday\nlife. Since our intention during a speech can differ from what is actually\nunderstood by the audience, the ability to appropriately convey our message\nrequires a complex set of skills. Presentation competence is cultivated in the\nearly school years and continuously developed over time. One approach that can\npromote efficient development of presentation competence is the automated\nanalysis of human behavior during a speech based on visual and audio features\nand machine learning. Furthermore, this analysis can be used to suggest\nimprovements and the development of skills related to presentation competence.\nIn this work, we investigate the contribution of different nonverbal behavioral\ncues, namely, facial, body pose-based, and audio-related features, to estimate\npresentation competence. The analyses were performed on videos of 251 students\nwhile the automated assessment is based on manual ratings according to the\nT\\\"ubingen Instrument for Presentation Competence (TIP). Our classification\nresults reached the best performance with early fusion in the same dataset\nevaluation (accuracy of 71.25%) and late fusion of speech, face, and body pose\nfeatures in the cross dataset evaluation (accuracy of 78.11%). Similarly,\nregression results performed the best with fusion strategies.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:09:41 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Beyan", "Cigdem", ""], ["Ruth", "Fabian", ""], ["Kramer", "Olaf", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2105.02824", "submitter": "Mohammad Arif Ul Alam", "authors": "Mohammad Arif Ul Alam", "title": "Activity-Aware Deep Cognitive Fatigue Assessment using Wearables", "comments": "Submitted to EMBC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive fatigue has been a common problem among workers which has become an\nincreasing global problem since the emergence of COVID-19 as a global pandemic.\nWhile existing multi-modal wearable sensors-aided automatic cognitive fatigue\nmonitoring tools have focused on physical and physiological sensors (ECG, PPG,\nActigraphy) analytic on specific group of people (say gamers, athletes,\nconstruction workers), activity-awareness is utmost importance due to its\ndifferent responses on physiology in different person. In this paper, we\npropose a novel framework, Activity-Aware Recurrent Neural Network\n(\\emph{AcRoNN}), that can generalize individual activity recognition and\nimprove cognitive fatigue estimation significantly. We evaluate and compare our\nproposed method with state-of-art methods using one real-time collected dataset\nfrom 5 individuals and another publicly available dataset from 27 individuals\nachieving max. 19% improvement.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 08:41:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Alam", "Mohammad Arif Ul", ""]]}, {"id": "2105.02957", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dhaval Salwala, Edward Curry", "title": "VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the\n  Edge for the Internet of Multimedia Things", "comments": "22 pages, 24 figures, 9 tables, Journal accepted in IEEE Internet of\n  Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2021.3075336", "report-no": null, "categories": "cs.CV cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient video processing is a critical component in many IoMT applications\nto detect events of interest. Presently, many window optimization techniques\nhave been proposed in event processing with an underlying assumption that the\nincoming stream has a structured data model. Videos are highly complex due to\nthe lack of any underlying structured data model. Video stream sources such as\nCCTV cameras and smartphones are resource-constrained edge nodes. At the same\ntime, video content extraction is expensive and requires computationally\nintensive Deep Neural Network (DNN) models that are primarily deployed at\nhigh-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage\nallied windowing approach to accelerate video event analytics in an edge-cloud\nparadigm. VID-WIN runs parallelly across edge and cloud nodes and performs the\nquery and resource-aware optimization for state-based complex event matching.\nVID-WIN exploits the video content and DNN input knobs to accelerate the video\ninference process across nodes. The paper proposes a novel content-driven\nmicro-batch resizing, queryaware caching and micro-batch based utility\nfiltering strategy of video frames under resource-constrained edge nodes to\nimprove the overall system throughput, latency, and network usage. Extensive\nevaluations are performed over five real-world datasets. The experimental\nresults show that VID-WIN video event matching achieves ~2.3X higher throughput\nwith minimal latency and ~99% bandwidth reduction compared to other baselines\nwhile maintaining query-level accuracy and resource bounds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:08:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yadav", "Piyush", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2105.03299", "submitter": "Yunshan Ma", "authors": "Yujuan Ding, Yunshan Ma, Lizi Liao, Wai Keung Wong, Tat-Seng Chua", "title": "Leveraging Multiple Relations for Fashion Trend Forecasting Based on\n  Social Media", "comments": "12 pages, 8 figures", "journal-ref": "IEEE Transaction on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion trend forecasting is of great research significance in providing\nuseful suggestions for both fashion companies and fashion lovers. Although\nvarious studies have been devoted to tackling this challenging task, they only\nstudied limited fashion elements with highly seasonal or simple patterns, which\ncould hardly reveal the real complex fashion trends. Moreover, the mainstream\nsolutions for this task are still statistical-based and solely focus on\ntime-series data modeling, which limit the forecast accuracy. Towards\ninsightful fashion trend forecasting, previous work [1] proposed to analyze\nmore fine-grained fashion elements which can informatively reveal fashion\ntrends. Specifically, it focused on detailed fashion element trend forecasting\nfor specific user groups based on social media data. In addition, it proposed a\nneural network-based method, namely KERN, to address the problem of fashion\ntrend modeling and forecasting. In this work, to extend the previous work, we\npropose an improved model named Relation Enhanced Attention Recurrent (REAR)\nnetwork. Compared to KERN, the REAR model leverages not only the relations\namong fashion elements but also those among user groups, thus capturing more\ntypes of correlations among various fashion trends. To further improve the\nperformance of long-range trend forecasting, the REAR method devises a sliding\ntemporal attention mechanism, which is able to capture temporal patterns on\nfuture horizons better. Extensive experiments and more analysis have been\nconducted on the FIT and GeoStyle datasets to evaluate the performance of REAR.\nExperimental and analytical results demonstrate the effectiveness of the\nproposed REAR model in fashion trend forecasting, which also show the\nimprovement of REAR compared to the KERN.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:52:03 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:41:25 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ding", "Yujuan", ""], ["Ma", "Yunshan", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2105.03611", "submitter": "Chamara Manoj Madarasingha Kattadige BSc (Hons)", "authors": "Chamara Kattadige, Aravindh Raman, Kanchana Thilakarathna, Andra Lutu,\n  Diego Perino", "title": "360NorVic: 360-Degree Video Classification from Mobile Encrypted Video\n  Traffic", "comments": "7 pages, 15 figures, accepted in Workshop on Network and\n  OperatingSystem Support for Digital Audio and Video (NOSSDAV 21)", "journal-ref": null, "doi": "10.1145/3458306.3460998", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Streaming 360{\\deg} video demands high bandwidth and low latency, and poses\nsignificant challenges to Internet Service Providers (ISPs) and Mobile Network\nOperators (MNOs). The identification of 360{\\deg} video traffic can therefore\nbenefits fixed and mobile carriers to optimize their network and provide better\nQuality of Experience (QoE) to the user. However, end-to-end encryption of\nnetwork traffic has obstructed identifying those 360{\\deg} videos from regular\nvideos. As a solution this paper presents 360NorVic, a near-realtime and\noffline Machine Learning (ML) classification engine to distinguish 360{\\deg}\nvideos from regular videos when streamed from mobile devices. We collect packet\nand flow level data for over 800 video traces from YouTube & Facebook\naccounting for 200 unique videos under varying streaming conditions. Our\nresults show that for near-realtime and offline classification at packet level,\naverage accuracy exceeds 95%, and that for flow level, 360NorVic achieves more\nthan 92% average accuracy. Finally, we pilot our solution in the commercial\nnetwork of a large MNO showing the feasibility and effectiveness of 360NorVic\nin production settings.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 06:36:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kattadige", "Chamara", ""], ["Raman", "Aravindh", ""], ["Thilakarathna", "Kanchana", ""], ["Lutu", "Andra", ""], ["Perino", "Diego", ""]]}, {"id": "2105.04090", "submitter": "Shih-Lun Wu", "authors": "Shih-Lun Wu, Yi-Hsuan Yang", "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One\n  Transformer VAE", "comments": "Preprint. 15 pages, 6 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long musical pieces, in which users can\nspecify musical attributes including rhythmic intensity and polyphony (i.e.,\nharmonic fullness) they desire, down to the bar level. Experiments show that\nMuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:44:03 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 03:35:13 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Shih-Lun", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "2105.05409", "submitter": "Xiongwei Wu", "authors": "Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven C.H. Hoi, Qianru\n  Sun", "title": "A Large-Scale Benchmark for Food Image Segmentation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food image segmentation is a critical and indispensible task for developing\nhealth-related applications such as estimating food calories and nutrients.\nExisting food image segmentation models are underperforming due to two reasons:\n(1) there is a lack of high quality food image datasets with fine-grained\ningredient labels and pixel-wise location masks -- the existing datasets either\ncarry coarse ingredient labels or are small in size; and (2) the complex\nappearance of food makes it difficult to localize and recognize ingredients in\nfood images, e.g., the ingredients may overlap one another in the same image,\nand the identical ingredient may appear distinctly in different food images. In\nthis work, we build a new food image dataset FoodSeg103 (and its extension\nFoodSeg154) containing 9,490 images. We annotate these images with 154\ningredient classes and each image has an average of 6 ingredient labels and\npixel-wise masks. In addition, we propose a multi-modality pre-training\napproach called ReLeM that explicitly equips a segmentation model with rich and\nsemantic food knowledge. In experiments, we use three popular semantic\nsegmentation methods (i.e., Dilated Convolution based, Feature Pyramid based,\nand Vision Transformer based) as baselines, and evaluate them as well as ReLeM\non our new datasets. We believe that the FoodSeg103 (and its extension\nFoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to\nfacilitate future works on fine-grained food image understanding. We make all\nthese datasets and methods public at\n\\url{https://xiongweiwu.github.io/foodseg103.html}.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 03:00:07 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wu", "Xiongwei", ""], ["Fu", "Xin", ""], ["Liu", "Ying", ""], ["Lim", "Ee-Peng", ""], ["Hoi", "Steven C. H.", ""], ["Sun", "Qianru", ""]]}, {"id": "2105.06361", "submitter": "Ziyue Xiang", "authors": "Ziyue Xiang, J\\'anos Horv\\'ath, Sriram Baireddy, Paolo Bestagini,\n  Stefano Tubaro, Edward J. Delp", "title": "Forensic Analysis of Video Files Using Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The unprecedented ease and ability to manipulate video content has led to a\nrapid spread of manipulated media. The availability of video editing tools\ngreatly increased in recent years, allowing one to easily generate\nphoto-realistic alterations. Such manipulations can leave traces in the\nmetadata embedded in video files. This metadata information can be used to\ndetermine video manipulations, brand of video recording device, the type of\nvideo editing tool, and other important evidence. In this paper, we focus on\nthe metadata contained in the popular MP4 video wrapper/container. We describe\nour method for metadata extractor that uses the MP4's tree structure. Our\napproach for analyzing the video metadata produces a more compact\nrepresentation. We will describe how we construct features from the metadata\nand then use dimensionality reduction and nearest neighbor classification for\nforensic analysis of a video file. Our approach allows one to visually inspect\nthe distribution of metadata features and make decisions. The experimental\nresults confirm that the performance of our approach surpasses other methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:40:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xiang", "Ziyue", ""], ["Horv\u00e1th", "J\u00e1nos", ""], ["Baireddy", "Sriram", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""], ["Delp", "Edward J.", ""]]}, {"id": "2105.06461", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Ishan Misra, Alexander G. Schwing, and Rohit Girdhar", "title": "3D Spatial Recognition without Spatially Labeled 3D", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition,\nrequiring only scene-level class tags as supervision. WyPR jointly addresses\nthree core 3D recognition tasks: point-level semantic segmentation, 3D proposal\ngeneration, and 3D object detection, coupling their predictions through self\nand cross-task consistency losses. We show that in conjunction with standard\nmultiple-instance learning objectives, WyPR can detect and segment objects in\npoint cloud data without access to any spatial labels at training time. We\ndemonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming\nprior state of the art on weakly-supervised segmentation by more than 6% mIoU.\nIn addition, we set up the first benchmark for weakly-supervised 3D object\ndetection on both datasets, where WyPR outperforms standard approaches and\nestablishes strong baselines for future work.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:58:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Misra", "Ishan", ""], ["Schwing", "Alexander G.", ""], ["Girdhar", "Rohit", ""]]}, {"id": "2105.06524", "submitter": "Hongpeng Guo", "authors": "Hongpeng Guo, Shuochao Yao, Zhe Yang, Qian Zhou, Klara Nahrstedt", "title": "CrossRoI: Cross-camera Region of Interest Optimization for Efficient\n  Real Time Video Analytics at Scale", "comments": "accepted in 12th ACM Multimedia Systems Conference (MMsys 21')", "journal-ref": null, "doi": "10.1145/3458305.3463381", "report-no": null, "categories": "cs.DC cs.CV cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video cameras are pervasively deployed in city scale for public good or\ncommunity safety (i.e. traffic monitoring or suspected person tracking).\nHowever, analyzing large scale video feeds in real time is data intensive and\nposes severe challenges to network and computation systems today. We present\nCrossRoI, a resource-efficient system that enables real time video analytics at\nscale via harnessing the videos content associations and redundancy across a\nfleet of cameras. CrossRoI exploits the intrinsic physical correlations of\ncross-camera viewing fields to drastically reduce the communication and\ncomputation costs. CrossRoI removes the repentant appearances of same objects\nin multiple cameras without harming comprehensive coverage of the scene.\nCrossRoI operates in two phases - an offline phase to establish cross-camera\ncorrelations, and an efficient online phase for real time video inference.\nExperiments on real-world video feeds show that CrossRoI achieves 42% - 65%\nreduction for network overhead and 25% - 34% reduction for response delay in\nreal time video analytics applications with more than 99% query accuracy, when\ncompared to baseline methods. If integrated with SotA frame filtering systems,\nthe performance gains of CrossRoI reach 50% - 80% (network overhead) and 33% -\n61% (end-to-end delay).\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:29:14 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Guo", "Hongpeng", ""], ["Yao", "Shuochao", ""], ["Yang", "Zhe", ""], ["Zhou", "Qian", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2105.06818", "submitter": "Tianrui Hui", "authors": "Tianrui Hui, Shaofei Huang, Si Liu, Zihan Ding, Guanbin Li, Wenguan\n  Wang, Jizhong Han, Fei Wang", "title": "Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor\n  Segmentation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-queried video actor segmentation aims to predict the pixel-level\nmask of the actor which performs the actions described by a natural language\nquery in the target frames. Existing methods adopt 3D CNNs over the video clip\nas a general encoder to extract a mixed spatio-temporal feature for the target\nframe. Though 3D convolutions are amenable to recognizing which actor is\nperforming the queried actions, it also inevitably introduces misaligned\nspatial information from adjacent frames, which confuses features of the target\nframe and yields inaccurate segmentation. Therefore, we propose a collaborative\nspatial-temporal encoder-decoder framework which contains a 3D temporal encoder\nover the video clip to recognize the queried actions, and a 2D spatial encoder\nover the target frame to accurately segment the queried actors. In the decoder,\na Language-Guided Feature Selection (LGFS) module is proposed to flexibly\nintegrate spatial and temporal features from the two encoders. We also propose\na Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine\nspatial- and temporal-relevant linguistic features for multimodal feature\ninteraction in each stage of the two encoders. Our method achieves new\nstate-of-the-art performance on two popular benchmarks with less computational\noverhead than previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:27:53 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Hui", "Tianrui", ""], ["Huang", "Shaofei", ""], ["Liu", "Si", ""], ["Ding", "Zihan", ""], ["Li", "Guanbin", ""], ["Wang", "Wenguan", ""], ["Han", "Jizhong", ""], ["Wang", "Fei", ""]]}, {"id": "2105.07062", "submitter": "Nicol\\`o Felicioni", "authors": "Nicol\\`o Felicioni, Maurizio Ferrari Dacrema, Paolo Cremonesi", "title": "Measuring the User Satisfaction in a Recommendation Interface with\n  Multiple Carousels", "comments": null, "journal-ref": "ACM International Conference on Interactive Media Experiences (IMX\n  '21), June 21--23, 2021, Virtual Event, NY, USA", "doi": "10.1145/3452918.3465493", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for video-on-demand and music streaming services to adopt a user\ninterface composed of several recommendation lists, i.e. widgets or swipeable\ncarousels, each generated according to a specific criterion or algorithm (e.g.\nmost recent, top popular, recommended for you, editors' choice, etc.).\nSelecting the appropriate combination of carousel has significant impact on\nuser satisfaction. A crucial aspect of this user interface is that to measure\nthe relevance a new carousel for the user it is not sufficient to account\nsolely for its individual quality. Instead, it should be considered that other\ncarousels will already be present in the interface. This is not considered by\ntraditional evaluation protocols for recommenders systems, in which each\ncarousel is evaluated in isolation, regardless of (i) which other carousels are\ndisplayed to the user and (ii) the relative position of the carousel with\nrespect to other carousels. Hence, we propose a two-dimensional evaluation\nprotocol for a carousel setting that will measure the quality of a\nrecommendation carousel based on how much it improves upon the quality of an\nalready available set of carousels. Our evaluation protocol takes into account\nalso the position bias, i.e. users do not explore the carousels sequentially,\nbut rather concentrate on the top-left corner of the screen.\n  We report experiments on the movie domain and notice that under a carousel\nsetting the definition of which criteria has to be preferred to generate a list\nof recommended items changes with respect to what is commonly understood.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:33:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Felicioni", "Nicol\u00f2", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2105.07135", "submitter": "Anant Baijal", "authors": "Anant Baijal, Vivek Agarwal and Danny Hyun", "title": "Analyzing Images for Music Recommendation", "comments": "IEEE International Conference on Consumer Electronics (IEEE ICCE\n  2021)", "journal-ref": null, "doi": "10.1109/ICCE50685.2021.9427619", "report-no": null, "categories": "cs.MM cs.AI cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiencing images with suitable music can greatly enrich the overall user\nexperience. The proposed image analysis method treats an artwork image\ndifferently from a photograph image. Automatic image classification is\nperformed using deep-learning based models. An illustrative analysis showcasing\nthe ability of our deep-models to inherently learn and utilize perceptually\nrelevant features when classifying artworks is also presented. The Mean Opinion\nScore (MOS) obtained from subjective assessments of the respective image and\nrecommended music pairs supports the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:14:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Baijal", "Anant", ""], ["Agarwal", "Vivek", ""], ["Hyun", "Danny", ""]]}, {"id": "2105.07139", "submitter": "Wei Zhou", "authors": "Wei Zhou, Zhou Wang, Zhibo Chen", "title": "Image Super-Resolution Quality Assessment: Structural Fidelity Versus\n  Statistical Naturalness", "comments": "Accepted by QoMEX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) algorithms reconstruct high-resolution\n(HR) images with their low-resolution (LR) counterparts. It is desirable to\ndevelop image quality assessment (IQA) methods that can not only evaluate and\ncompare SISR algorithms, but also guide their future development. In this\npaper, we assess the quality of SISR generated images in a two-dimensional (2D)\nspace of structural fidelity versus statistical naturalness. This allows us to\nobserve the behaviors of different SISR algorithms as a tradeoff in the 2D\nspace. Specifically, SISR methods are traditionally designed to achieve high\nstructural fidelity but often sacrifice statistical naturalness, while recent\ngenerative adversarial network (GAN) based algorithms tend to create more\nnatural-looking results but lose significantly on structural fidelity.\nFurthermore, such a 2D evaluation can be easily fused to a scalar quality\nprediction. Interestingly, we find that a simple linear combination of a\nstraightforward local structural fidelity and a global statistical naturalness\nmeasures produce surprisingly accurate predictions of SISR image quality when\ntested using public subject-rated SISR image datasets. Code of the proposed\nSFSN model is publicly available at \\url{https://github.com/weizhou-geek/SFSN}.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:31:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhou", "Wei", ""], ["Wang", "Zhou", ""], ["Chen", "Zhibo", ""]]}, {"id": "2105.07175", "submitter": "Tianrui Hui", "authors": "Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, Guanbin Li", "title": "Cross-Modal Progressive Comprehension for Referring Segmentation", "comments": "Accepted by TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a natural language expression and an image/video, the goal of referring\nsegmentation is to produce the pixel-level masks of the entities described by\nthe subject of the expression. Previous approaches tackle this problem by\nimplicit feature interaction and fusion between visual and linguistic\nmodalities in a one-stage manner. However, human tends to solve the referring\nproblem in a progressive manner based on informative words in the expression,\ni.e., first roughly locating candidate entities and then distinguishing the\ntarget one. In this paper, we propose a Cross-Modal Progressive Comprehension\n(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I\n(Image) module and a CMPC-V (Video) module to improve referring image and video\nsegmentation models. For image data, our CMPC-I module first employs entity and\nattribute words to perceive all the related entities that might be considered\nby the expression. Then, the relational words are adopted to highlight the\ntarget entity as well as suppress other irrelevant ones by spatial graph\nreasoning. For video data, our CMPC-V module further exploits action words\nbased on CMPC-I to highlight the correct entity matched with the action cues by\ntemporal graph reasoning. In addition to the CMPC, we also introduce a simple\nyet effective Text-Guided Feature Exchange (TGFE) module to integrate the\nreasoned multimodal features corresponding to different levels in the visual\nbackbone under the guidance of textual information. In this way, multi-level\nfeatures can communicate with each other and be mutually refined based on the\ntextual context. Combining CMPC-I or CMPC-V with TGFE can form our image or\nvideo version referring segmentation frameworks and our frameworks achieve new\nstate-of-the-art performances on four referring image segmentation benchmarks\nand three referring video segmentation benchmarks respectively.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 08:55:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Liu", "Si", ""], ["Hui", "Tianrui", ""], ["Huang", "Shaofei", ""], ["Wei", "Yunchao", ""], ["Li", "Bo", ""], ["Li", "Guanbin", ""]]}, {"id": "2105.07553", "submitter": "Zheng Zhang", "authors": "Xunguang Wang, Zheng Zhang, Baoyuan Wu, Fumin Shen, Guangming Lu", "title": "Prototype-supervised Adversarial Network for Targeted Attack of Deep\n  Hashing", "comments": "This paper has been accepted by CVPR 2021, and the related codes\n  could be available at https://github.com/xunguangwang/ProS-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its powerful capability of representation learning and high-efficiency\ncomputation, deep hashing has made significant progress in large-scale image\nretrieval. However, deep hashing networks are vulnerable to adversarial\nexamples, which is a practical secure problem but seldom studied in\nhashing-based retrieval field. In this paper, we propose a novel\nprototype-supervised adversarial network (ProS-GAN), which formulates a\nflexible generative architecture for efficient and effective targeted hashing\nattack. To the best of our knowledge, this is the first generation-based method\nto attack deep hashing networks. Generally, our proposed framework consists of\nthree parts, i.e., a PrototypeNet, a generator, and a discriminator.\nSpecifically, the designed PrototypeNet embeds the target label into the\nsemantic representation and learns the prototype code as the category-level\nrepresentative of the target label. Moreover, the semantic representation and\nthe original image are jointly fed into the generator for a flexible targeted\nattack. Particularly, the prototype code is adopted to supervise the generator\nto construct the targeted adversarial example by minimizing the Hamming\ndistance between the hash code of the adversarial example and the prototype\ncode. Furthermore, the generator is against the discriminator to simultaneously\nencourage the adversarial examples visually realistic and the semantic\nrepresentation informative. Extensive experiments verify that the proposed\nframework can efficiently produce adversarial examples with better targeted\nattack performance and transferability over state-of-the-art targeted attack\nmethods of deep hashing. The related codes could be available at\nhttps://github.com/xunguangwang/ProS-GAN .\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 00:31:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xunguang", ""], ["Zhang", "Zheng", ""], ["Wu", "Baoyuan", ""], ["Shen", "Fumin", ""], ["Lu", "Guangming", ""]]}, {"id": "2105.07558", "submitter": "Debajyoti Halder", "authors": "Debajyoti Halder, Prashant Kumar, Saksham Bhushan, and Anand M.\n  Baswade", "title": "fybrrStream: A WebRTC based Efficient and Scalable P2P Live Streaming\n  Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The demand for streaming media and live video conferencing is at peak and\nexpected to grow further, thereby the need for low-cost streaming services with\nbetter quality and lower latency is essential. Therefore, in this paper, we\npropose a novel peer-to-peer (P2P) live streaming platform, called fybrrStream,\nwhere a logical mesh and physical tree i.e., hybrid topology-based approach is\nleveraged for low latency streaming. fybrrStream distributes the load on\nparticipating peers in a hierarchical manner by considering their network\nbandwidth, network latency, and node stability. fybrrStream costs as low as the\ncost of just hosting a light-weight website and the performance is comparable\nto the existing state-of-the-art media streaming services. We evaluated and\ntested the proposed fybrrStream platform with real-field experiments using 50+\nusers spread across India and results obtained show significant improvements in\nthe live streaming performance over other schemes.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 00:55:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Halder", "Debajyoti", ""], ["Kumar", "Prashant", ""], ["Bhushan", "Saksham", ""], ["Baswade", "Anand M.", ""]]}, {"id": "2105.07585", "submitter": "Yujuan Ding", "authors": "Yujuan Ding, Yunshan Ma, Wai Keung Wong, Tat-Seng Chua", "title": "Leveraging Two Types of Global Graph for Sequential Fashion\n  Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3460426.3463638", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sequential fashion recommendation is of great significance in online fashion\nshopping, which accounts for an increasing portion of either fashion retailing\nor online e-commerce. The key to building an effective sequential fashion\nrecommendation model lies in capturing two types of patterns: the personal\nfashion preference of users and the transitional relationships between adjacent\nitems. The two types of patterns are usually related to user-item interaction\nand item-item transition modeling respectively. However, due to the large sets\nof users and items as well as the sparse historical interactions, it is\ndifficult to train an effective and efficient sequential fashion recommendation\nmodel. To tackle these problems, we propose to leverage two types of global\ngraph, i.e., the user-item interaction graph and item-item transition graph, to\nobtain enhanced user and item representations by incorporating higher-order\nconnections over the graphs. In addition, we adopt the graph kernel of LightGCN\nfor the information propagation in both graphs and propose a new design for\nitem-item transition graph. Extensive experiments on two established sequential\nfashion recommendation datasets validate the effectiveness and efficiency of\nour approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:02:04 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 01:05:22 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 02:08:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ding", "Yujuan", ""], ["Ma", "Yunshan", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2105.07841", "submitter": "Bolu John Folayan Dr.", "authors": "Bolu John Folayan, Olumide Samuel Ogunjobi, Prosper Zannu, Taiwo\n  Ajibolu Balofin", "title": "Post-war Civil War Propaganda Techniques and Media Spins in Nigeria and\n  Journalism Practice", "comments": null, "journal-ref": null, "doi": "10.24297/jssr.v17i.8993", "report-no": null, "categories": "cs.CY cs.MM physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In public relations and political communication, a spin is a form of\npropaganda achieved through knowingly presenting a biased interpretation of an\nevent or issues. It is also the act of presenting narratives to influence\npublic opinion about events, people or and ideas. In war time, various forms of\nspins are employed by antagonists to push their brigades to victory and wear\nout the opponents. During the Nigerian civil war, quite a number of these spins\nwere dominant for example GOWON (Go On With One Nigeria); On Aburi We Stand, O\nLe Ku Ija Ore. Post-war years presented different spins and fifty years after\nthe war, different spins continue to push emerging narratives (e.g.\nmarginalization, restructuring. This paper investigates and analyzes the\ndifferent propaganda techniques and spins in the narratives of the Nigerian\ncivil in the past five years through a content analysis of three national\nnewspapers: The Nigerian Tribune, Daily Trust and Sun Newspapers. Findings\nconfirm that propaganda and spins are not limited to war time, but are actively\ndeployed in peace time. This development places additional challenge on\njournalists to uphold the canons of balance, truth and fairness in reporting\nsensitive national issues.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:00:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Folayan", "Bolu John", ""], ["Ogunjobi", "Olumide Samuel", ""], ["Zannu", "Prosper", ""], ["Balofin", "Taiwo Ajibolu", ""]]}, {"id": "2105.08052", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Mia Chiquier, Hod Lipson, Carl Vondrick", "title": "The Boombox: Visual Reconstruction from Acoustic Vibrations", "comments": "Website: boombox.cs.columbia.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce The Boombox, a container that uses acoustic vibrations to\nreconstruct an image of its inside contents. When an object interacts with the\ncontainer, they produce small acoustic vibrations. The exact vibration\ncharacteristics depend on the physical properties of the box and the object. We\ndemonstrate how to use this incidental signal in order to predict visual\nstructure. After learning, our approach remains effective even when a camera\ncannot view inside the box. Although we use low-cost and low-power contact\nmicrophones to detect the vibrations, our results show that learning from\nmulti-modal data enables us to transform cheap acoustic sensors into rich\nvisual sensors. Due to the ubiquity of containers, we believe integrating\nperception capabilities into them will enable new applications in\nhuman-computer interaction and robotics. Our project website is at:\nboombox.cs.columbia.edu\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:58:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Boyuan", ""], ["Chiquier", "Mia", ""], ["Lipson", "Hod", ""], ["Vondrick", "Carl", ""]]}, {"id": "2105.08191", "submitter": "Gangadharan Esakki", "authors": "Gangadharan Esakki, Andreas Panayides, Venkatesh Jatla, Marios\n  Pattichis", "title": "Adaptive Video Encoding For Different Video Codecs", "comments": "Video codecs, Video signal processing, Video coding, Video\n  compression, Video quality, Video streaming, Adaptive video streaming,\n  Versatile Video Coding, AV1, HEVC", "journal-ref": "IEEE Access 2021", "doi": "10.1109/ACCESS.2021.3077313", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  By 2022, we expect video traffic to reach 82% of the total internet traffic.\nUndoubtedly, the abundance of video-driven applications will likely lead\ninternet video traffic percentage to a further increase in the near future,\nenabled by associate advances in video devices' capabilities. In response to\nthis ever-growing demand, the Alliance for Open Media (AOM) and the Joint Video\nExperts Team (JVET) have demonstrated strong and renewed interest in developing\nnew video codecs. In the fast-changing video codecs' landscape, there is thus,\na genuine need to develop adaptive methods that can be universally applied to\ndifferent codecs. In this study, we formulate video encoding as a\nmulti-objective optimization process where video quality (as a function of VMAF\nand PSNR), bitrate demands, and encoding rate (in encoded frames per second)\nare jointly optimized, going beyond the standard video encoding approaches that\nfocus on rate control targeting specific bandwidths. More specifically, we\ncreate a dense video encoding space (offline) and then employ regression to\ngenerate forward prediction models for each one of the afore-described\noptimization objectives, using only Pareto-optimal points. We demonstrate our\nadaptive video encoding approach that leverages the generated forward\nprediction models that qualify for real-time adaptation using different codecs\n(e.g., SVT-AV1 and x265) for a variety of video datasets and resolutions. To\nmotivate our approach and establish the promise for future fast VVC encoders,\nwe also perform a comparative performance evaluation using both subjective and\nobjective metrics and report on bitrate savings among all possible pairs\nbetween VVC, SVT-AV1, x265, and VP9 codecs.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:06:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Esakki", "Gangadharan", ""], ["Panayides", "Andreas", ""], ["Jatla", "Venkatesh", ""], ["Pattichis", "Marios", ""]]}, {"id": "2105.08350", "submitter": "Sirui Guo", "authors": "Wenfa Qi, Wei Hu, Sirui Guo, Zongming Guo, Xiang Wang", "title": "Generic Reversible Visible Watermarking Via Regularized Graph Fourier\n  Transform Coding", "comments": "This manuscript is submitted to IEEE Transactions on Image Processing\n  on May 17th 2021. It has 13 pages, 10 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible visible watermarking (RVW) is an active copyright protection\nmechanism. It not only transparently superimposes copyright patterns on\nspecific positions of digital images or video frames to declare the copyright\nownership information, but also completely erases the visible watermark image\nand thus enables restoring the original host image without any distortion.\nHowever, existing RVW algorithms mostly construct the reversible mapping\nmechanism for a specific visible watermarking scheme, which is not general.\nHence, we propose a generic RVW framework to accommodate various visible\nwatermarking schemes, which is based on Regularized Graph Fourier Transform\n(GFT) coding. In particular, we obtain a reconstruction data packet -- the\ncompressed difference image between the watermarked image and the original host\nimage, which is embedded into the watermarked image via any conventional\nreversible data hiding method to facilitate the blind recovery of the host\nimage. The key is to achieve compact compression of the difference image for\nefficient embedding of the reconstruction data packet. To this end, we propose\nregularized GFT coding, where the difference image is smoothed via the graph\nLaplacian regularizer for more efficient compression and then encoded by\nmulti-resolution GFTs in an approximately optimal manner. Experimental results\nshow that the proposed method achieves the state-of-the-art performance with\nhigh data compression efficiency, which is applicable to both gray-scale and\ncolor images. In addition, the proposed generic framework accommodates various\nvisible watermarking algorithms, which demonstrates strong versatility.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:22:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Qi", "Wenfa", ""], ["Hu", "Wei", ""], ["Guo", "Sirui", ""], ["Guo", "Zongming", ""], ["Wang", "Xiang", ""]]}, {"id": "2105.08643", "submitter": "Zekai Chen", "authors": "Zekai Chen, Maiwang Shi, Xiao Zhang, Haochao Ying", "title": "ASM2TV: An Adaptive Semi-Supervised Multi-Task Multi-View Learning\n  Framework", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world scenarios, such as human activity recognition (HAR) in IoT,\ncan be formalized as a multi-task multi-view learning problem. Each specific\ntask consists of multiple shared feature views collected from multiple sources,\neither homogeneous or heterogeneous. Common among recent approaches is to\nemploy a typical hard/soft sharing strategy at the initial phase separately for\neach view across tasks to uncover common knowledge, underlying the assumption\nthat all views are conditionally independent. On the one hand, multiple views\nacross tasks possibly relate to each other under practical situations. On the\nother hand, supervised methods might be insufficient when labeled data is\nscarce. To tackle these challenges, we introduce a novel framework ASM2TV for\nsemi-supervised multi-task multi-view learning. We present a new perspective\nnamed gating control policy, a learnable task-view-interacted sharing policy\nthat adaptively selects the most desirable candidate shared block for any view\nacross any task, which uncovers more fine-grained task-view-interacted\nrelatedness and improves inference efficiency. Significantly, our proposed\ngathering consistency adaption procedure takes full advantage of large amounts\nof unlabeled fragmented time-series, making it a general framework that\naccommodates a wide range of applications. Experiments on two diverse\nreal-world HAR benchmark datasets collected from various subjects and sources\ndemonstrate our framework's superiority over other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:15:32 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Zekai", ""], ["Shi", "Maiwang", ""], ["Zhang", "Xiao", ""], ["Ying", "Haochao", ""]]}, {"id": "2105.08649", "submitter": "Zekai Chen", "authors": "Zekai Chen, Fangtian Zhong, Zhumin Chen, Xiao Zhang, Robert Pless,\n  Xiuzhen Cheng", "title": "DCAP: Deep Cross Attentional Product Network for User Response\n  Prediction", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:27:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Zekai", ""], ["Zhong", "Fangtian", ""], ["Chen", "Zhumin", ""], ["Zhang", "Xiao", ""], ["Pless", "Robert", ""], ["Cheng", "Xiuzhen", ""]]}, {"id": "2105.08809", "submitter": "Fatma Abdeo", "authors": "Fatma S. Abousaleh, Wen-Huang Cheng, Neng-Hao Yu, and Yu Tsao", "title": "Multimodal Deep Learning Framework for Image Popularity Prediction on\n  Social Media", "comments": "14 pages, 11 figures, 7 tables", "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems. 2020 Nov\n  9", "doi": "10.1109/TCDS.2020.3036690", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Billions of photos are uploaded to the web daily through various types of\nsocial networks. Some of these images receive millions of views and become\npopular, whereas others remain completely unnoticed. This raises the problem of\npredicting image popularity on social media. The popularity of an image can be\naffected by several factors, such as visual content, aesthetic quality, user,\npost metadata, and time. Thus, considering all these factors is essential for\naccurately predicting image popularity. In addition, the efficiency of the\npredictive model also plays a crucial role. In this study, motivated by\nmultimodal learning, which uses information from various modalities, and the\ncurrent success of convolutional neural networks (CNNs) in various fields, we\npropose a deep learning model, called visual-social convolutional neural\nnetwork (VSCNN), which predicts the popularity of a posted image by\nincorporating various types of visual and social features into a unified\nnetwork model. VSCNN first learns to extract high-level representations from\nthe input visual and social features by utilizing two individual CNNs. The\noutputs of these two networks are then fused into a joint network to estimate\nthe popularity score in the output layer. We assess the performance of the\nproposed method by conducting extensive experiments on a dataset of\napproximately 432K images posted on Flickr. The simulation results demonstrate\nthat the proposed VSCNN model significantly outperforms state-of-the-art\nmodels, with a relative improvement of greater than 2.33%, 7.59%, and 14.16% in\nterms of Spearman's Rho, mean absolute error, and mean squared error,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:58:58 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Abousaleh", "Fatma S.", ""], ["Cheng", "Wen-Huang", ""], ["Yu", "Neng-Hao", ""], ["Tsao", "Yu", ""]]}, {"id": "2105.08899", "submitter": "Leo Yu Zhang Dr.", "authors": "Yushu Zhang, Xiangli Xiao, Leo Yu Zhang, Zhe Liu, and Jiwu Huang", "title": "CREAMS: Copyrighted Cloud Media Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of the big data era drives the media data owner to seek help from\nthe cloud platform for data hosting and sharing. Sharing media data through the\ncloud suffers three key security/privacy problems including the leakage of data\nprivacy, the infringement on the data owner's copyright, and the infringement\non the user's right. Existing techniques such as attribute-based encryption,\nproxy re-encryption, and asymmetric fingerprinting are insufficient to solve\nall three problems. In this work, we consider the scheme design of being\ncapable of addressing these three problems simultaneously. Associating the\nadditive homomorphic proxy re-encryption technique with the asymmetric\nfingerprinting based on user-side embedding, we bring forward two novel cloud\nmedia sharing schemes: CREAMS-I and CREAMS-II. Among them, CREAMS-II has better\nsecurity performance, while CREAMS-I has more outstanding cloud-side\nefficiency. It is demonstrated that both proposed schemes can solve the\nexisting three problems well and have advantages over existing peers. In\naddition, these two schemes can also be seen as an instantiation of\nprivacy-preserving outsourcing of asymmetric fingerprinting, from which the\nowner can reap substantial savings in local storage, communication, and\ncomputing resources. The feasibility of CREAMS-I and CREAMS-II is also verified\nby simulation.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:03:22 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Yushu", ""], ["Xiao", "Xiangli", ""], ["Zhang", "Leo Yu", ""], ["Liu", "Zhe", ""], ["Huang", "Jiwu", ""]]}, {"id": "2105.09153", "submitter": "Cheryl Tollola", "authors": "C. Tollola", "title": "Procedural animations in interactive art experiences -- A state of the\n  art review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art review broadly oversees the use of novel research\nutilized in the creation of virtual environments applied in interactive art\nexperiences, with a specific focus on the application of procedural animation\nin spatially augmented reality (SAR) exhibitions. These art exhibitions\nfrequently combine sensory displays that appeal, replace, and augment the\nvisual, auditory and touch or haptic senses. We analyze and break down\nart-technology related innovations in the last three years, and thoroughly\nidentify the most recent and vibrant applications of interactive art\nexperiences in the review of numerous installation applications, studies, and\nevents. Display mediums such as virtual reality, augmented reality, mixed\nreality, and robotics are overviewed in the context of art experiences such as\nvisual art museums, park or historic site tours, live concerts, and theatre. We\nexplore research and extrapolate how recent innovations can lead to different\napplications that will be seen in the future.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 05:14:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Tollola", "C.", ""]]}, {"id": "2105.09280", "submitter": "Hassan Noura", "authors": "Hassan N. Noura, Ola Salman, Rapha\\\"el Couturier", "title": "A Deep Learning Scheme for Efficient Multimedia IoT Data Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Given the voluminous nature of the multimedia sensed data, the Multimedia\nInternet of Things (MIoT) devices and networks will present several limitations\nin terms of power and communication overhead. One traditional solution to cope\nwith the large-size data challenge is to use lossy compression. However,\ncurrent lossy compression schemes require low compression rate to guarantee\nacceptable perceived image quality, which results in a low reduction of the\ncommunicated data size and consequently a low reduction in the energy and\nbandwidth consumption. Thus, an efficient compression solution is required for\nstriking a good balance between data size (and consequently communication\noverhead) and visual degradation. In this paper, a Deep-Learning (DL)\nsuper-resolution model is applied to recuperate high quality images (at the\napplication server side) given as input degraded images with a high compression\nratio (at the sender side). The experimental analysis shows the effectiveness\nof the proposed solution in enhancing the visual quality of the compressed and\ndown-scaled images. Consequently, the proposed solution reduces the overall\ncommunication overhead and power consumption of limited MIoT devices.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 08:35:11 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Noura", "Hassan N.", ""], ["Salman", "Ola", ""], ["Couturier", "Rapha\u00ebl", ""]]}, {"id": "2105.09281", "submitter": "Shahrokh Paravarzar", "authors": "Shahrokh Paravarzar, Javaneh Alavi", "title": "A Decade of Research for Image Compression In Multimedia Laboratory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement of technology, we have supercomputers with high\nprocessing power and affordable prices. In addition, using multimedia expanded\nall around the world. This caused a vast use of images and videos in different\nfields. As this kind of data consists of a large amount of information, there\nis a need to use compression methods to store, manage or transfer them better\nand faster. One effective technique, which was introduced is variable\nresolution. This technique stimulates human vision and divides regions in\npictures into two different parts, including the area of interest that needs\nmore detail and periphery parts with less detail. This results in better\ncompression. The variable resolution was used for image, video, and 3D motion\ndata compression. This paper investigates the mentioned technique and some\nother research in this regard.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:11:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Paravarzar", "Shahrokh", ""], ["Alavi", "Javaneh", ""]]}, {"id": "2105.09284", "submitter": "Preslav Nakov", "authors": "Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio\n  Silvestri, Hamed Firooz, Preslav Nakov, Giovanni Da San Martino", "title": "SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and\n  Images", "comments": "propaganda, disinformation, misinformation, fake news, memes,\n  multimodality", "journal-ref": "SemEval-2021", "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in\nTexts and Images: the data, the annotation guidelines, the evaluation setup,\nthe results, and the participating systems. The task focused on memes and had\nthree subtasks: (i) detecting the techniques in the text, (ii) detecting the\ntext spans where the techniques are used, and (iii) detecting techniques in the\nentire meme, i.e., both in the text and in the image. It was a popular task,\nattracting 71 registrations, and 22 teams that eventually made an official\nsubmission on the test set. The evaluation results for the third subtask\nconfirmed the importance of both modalities, the text and the image. Moreover,\nsome teams reported benefits when not just combining the two modalities, e.g.,\nby using early or late fusion, but rather modeling the interaction between them\nin a joint model.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 05:00:53 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Dimitrov", "Dimitar", ""], ["Ali", "Bishr Bin", ""], ["Shaar", "Shaden", ""], ["Alam", "Firoj", ""], ["Silvestri", "Fabrizio", ""], ["Firooz", "Hamed", ""], ["Nakov", "Preslav", ""], ["Martino", "Giovanni Da San", ""]]}, {"id": "2105.09999", "submitter": "Li-Heng Chen", "authors": "Li-Heng Chen, Christos G. Bampis, Zhi Li, Chao Chen and Alan C. Bovik", "title": "Convolutional Block Design for Learned Fractional Downsampling", "comments": "4 pages conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The layers of convolutional neural networks (CNNs) can be used to alter the\nresolution of their inputs, but the scaling factors are limited to integer\nvalues. However, in many image and video processing applications, the ability\nto resize by a fractional factor would be advantageous. One example is\nconversion between resolutions standardized for video compression, such as from\n1080p to 720p. To solve this problem, we propose an alternative building block,\nformulated as a conventional convolutional layer followed by a differentiable\nresizer. More concretely, the convolutional layer preserves the resolution of\nthe input, while the resizing operation is fully handled by the resizer. In\nthis way, any CNN architecture can be adapted for non-integer resizing. As an\napplication, we replace the resizing convolutional layer of a modern deep\ndownsampling model by the proposed building block, and apply it to an adaptive\nbitrate video streaming scenario. Our experimental results show that an\nimprovement in coding efficiency over the conventional Lanczos algorithm is\nattained, in terms of PSNR, SSIM, and VMAF on test videos.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:24:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Li-Heng", ""], ["Bampis", "Christos G.", ""], ["Li", "Zhi", ""], ["Chen", "Chao", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2105.10005", "submitter": "C.-H. Huck Yang", "authors": "C.-H. Huck Yang, Mohit Chhabra, Y.-C. Liu, Quan Kong, Tomoaki\n  Yoshinaga, Tomokazu Murakami", "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments", "comments": "Accepted to IEEE ICIP 2021", "journal-ref": "2021 IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 19:38:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:29:32 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 01:36:22 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 06:52:21 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Chhabra", "Mohit", ""], ["Liu", "Y. -C.", ""], ["Kong", "Quan", ""], ["Yoshinaga", "Tomoaki", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "2105.10754", "submitter": "Caglar Yildirim", "authors": "Michael Carroll, Ethan Osborne, Caglar Yildirim", "title": "Effects of VR Gaming and Game Genre on Player Experience", "comments": "2019 IEEE Games, Entertainment, Media Conference (GEM)", "journal-ref": null, "doi": "10.1109/GEM.2019.8811554", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the increasing availability of modern virtual reality (VR) headsets, the\nuse and applications of VR technology for gaming purposes have become more\npervasive than ever. Despite the growing popularity of VR gaming, user studies\ninto how it might affect the player experience (PX) during the gameplay are\nscarce. Accordingly, the current study investigated the effects of VR gaming\nand game genre on PX. We compared PX metrics for two game genres, strategy\n(more interactive) and racing (less interactive), across two gaming platforms,\nVR and traditional desktop gaming. Participants were randomly assigned to one\nof the gaming platforms, played both a strategy and racing game on their\ncorresponding platform, and provided PX ratings. Results revealed that,\nregardless of the game genre, participants in the VR gaming condition\nexperienced a greater level of sense of presence than did those in the desktop\ngaming condition. That said, results showed that the two gaming platforms did\nnot significantly differ from one another in PX ratings. As for the effect of\ngame genre, participants provided greater PX ratings for the strategy game than\nfor the racing game, regardless of whether the game was played on a VR headset\nor desktop computer. Collectively, these results indicate that although VR\ngaming affords a greater sense of presence in the game environment, this\nincrease in presence does not seem to translate into a more satisfactory PX\nwhen playing either a strategy or racing game.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:09:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Carroll", "Michael", ""], ["Osborne", "Ethan", ""], ["Yildirim", "Caglar", ""]]}, {"id": "2105.11095", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Nader Karim, Pejman Khadivi, Shahram Shirani, Shadrokh\n  Samavi", "title": "Robust Watermarking using Diffusion of Logo into Autoencoder Feature\n  Maps", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital contents have grown dramatically in recent years, leading to\nincreased attention to copyright. Image watermarking has been considered one of\nthe most popular methods for copyright protection. With the recent advancements\nin applying deep neural networks in image processing, these networks have also\nbeen used in image watermarking. Robustness and imperceptibility are two\nchallenging features of watermarking methods that the trade-off between them\nshould be satisfied. In this paper, we propose to use an end-to-end network for\nwatermarking. We use a convolutional neural network (CNN) to control the\nembedding strength based on the image content. Dynamic embedding helps the\nnetwork to have the lowest effect on the visual quality of the watermarked\nimage. Different image processing attacks are simulated as a network layer to\nimprove the robustness of the model. Our method is a blind watermarking\napproach that replicates the watermark string to create a matrix of the same\nsize as the input image. Instead of diffusing the watermark data into the input\nimage, we inject the data into the feature space and force the network to do\nthis in regions that increase the robustness against various attacks.\nExperimental results show the superiority of the proposed method in terms of\nimperceptibility and robustness compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 05:18:33 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jamali", "Maedeh", ""], ["Karim", "Nader", ""], ["Khadivi", "Pejman", ""], ["Shirani", "Shahram", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2105.11131", "submitter": "Guoqiang Liang", "authors": "Guoqiang Liang, Yanbing Lv, Shucheng Li, Shizhou Zhang, Yanning Zhang", "title": "Unsupervised Video Summarization with a Convolutional Attentive\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive growth of video data, video summarization, which attempts\nto seek the minimum subset of frames while still conveying the main story, has\nbecome one of the hottest topics. Nowadays, substantial achievements have been\nmade by supervised learning techniques, especially after the emergence of deep\nlearning. However, it is extremely expensive and difficult to collect human\nannotation for large-scale video datasets. To address this problem, we propose\na convolutional attentive adversarial network (CAAN), whose key idea is to\nbuild a deep summarizer in an unsupervised way. Upon the generative adversarial\nnetwork, our overall framework consists of a generator and a discriminator. The\nformer predicts importance scores for all frames of a video while the latter\ntries to distinguish the score-weighted frame features from original frame\nfeatures. Specifically, the generator employs a fully convolutional sequence\nnetwork to extract global representation of a video, and an attention-based\nnetwork to output normalized importance scores. To learn the parameters, our\nobjective function is composed of three loss functions, which can guide the\nframe-level importance score prediction collaboratively. To validate this\nproposed method, we have conducted extensive experiments on two public\nbenchmarks SumMe and TVSum. The results show the superiority of our proposed\nmethod against other state-of-the-art unsupervised approaches. Our method even\noutperforms some published supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 07:24:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liang", "Guoqiang", ""], ["Lv", "Yanbing", ""], ["Li", "Shucheng", ""], ["Zhang", "Shizhou", ""], ["Zhang", "Yanning", ""]]}, {"id": "2105.11563", "submitter": "Chamara Manoj Madarasingha Kattadige BSc (Hons)", "authors": "Chamara Kattadige, Kanchana Thilakarathna", "title": "VAD360: Viewport Aware Dynamic 360-Degree Video Frame Tiling", "comments": "10, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  360{\\deg} videos a.k.a. spherical videos are getting popular among users\nnevertheless, omnidirectional view of these videos demands high bandwidth and\nprocessing power at the end devices. Recently proposed viewport aware streaming\nmechanisms can reduce the amount of data transmitted by streaming a limited\nportion of the frame covering the current user viewport (VP). However, they\nstill suffer from sending a high amount of redundant data, as the fixed tile\nmechanisms can not provide finer granularity to the user VP. Though making the\ntiles smaller can provide a finer granularity for user viewport, high encoding\noverhead incurred. To overcome this trade-off, in this paper, we present a\ncomputational geometric approach based adaptive tiling mechanism named VAD360,\nwhich takes visual attention information on the 360{\\deg} video frame as the\ninput and provide a suitable non-overlapping variable size tile cover on the\nframe. Experimental results shows that VAD360 can save up to 31.1% of pixel\nredundancy before compression and 35.4% of bandwidth saving compared to\nrecently proposed fixed tile configurations, providing tile schemes within\n0.98($\\pm$0.11)s time frame.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 22:48:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Kattadige", "Chamara", ""], ["Thilakarathna", "Kanchana", ""]]}, {"id": "2105.11826", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng\n  Chua, Jinyoung Moon, Hong-Han Shuai", "title": "Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend\n  Forecasting", "comments": null, "journal-ref": "ICMR 2021", "doi": "10.1145/3460426.3463598", "report-no": null, "categories": "cs.LG cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This companion paper supports the replication of the fashion trend\nforecasting experiments with the KERN (Knowledge Enhanced Recurrent Network)\nmethod that we presented in the ICMR 2020. We provide an artifact that allows\nthe replication of the experiments using a Python implementation. The artifact\nis easy to deploy with simple installation, training and evaluation. We\nreproduce the experiments conducted in the original paper and obtain similar\nperformance as previously reported. The replication results of the experiments\nsupport the main claims in the original paper.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:53:11 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ma", "Yunshan", ""], ["Ding", "Yujuan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""], ["Moon", "Jinyoung", ""], ["Shuai", "Hong-Han", ""]]}, {"id": "2105.11941", "submitter": "Jingwen Fu", "authors": "Jingwen Fu, Xiaoyi Zhang, Yuwang Wang, Wenjun Zeng, Sam Yang and\n  Grayson Hilliard", "title": "Understanding Mobile GUI: from Pixel-Words to Screen-Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of mobile phones makes mobile GUI understanding an important\ntask. Most previous works in this domain require human-created metadata of\nscreens (e.g. View Hierarchy) during inference, which unfortunately is often\nnot available or reliable enough for GUI understanding. Inspired by the\nimpressive success of Transformers in NLP tasks, targeting for purely\nvision-based GUI understanding, we extend the concepts of Words/Sentence to\nPixel-Words/Screen-Sentence, and propose a mobile GUI understanding\narchitecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the\nindividual Words, we define the Pixel-Words as atomic visual components (text\nand graphic components), which are visually consistent and semantically clear\nacross screenshots of a large variety of design styles. The Pixel-Words\nextracted from a screenshot are aggregated into Screen-Sentence with a Screen\nTransformer proposed to model their relations. Since the Pixel-Words are\ndefined as atomic visual components, the ambiguity between their visual\nappearance and semantics is dramatically reduced. We are able to make use of\nmetadata available in training data to auto-generate high-quality annotations\nfor Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words\nannotations is built based on the public RICO dataset, which will be released\nto help to address the lack of high-quality training data in this area. We\ntrain a detector to extract Pixel-Words from screenshots on this dataset and\nachieve metadata-free GUI understanding during inference. We conduct\nexperiments and show that Pixel-Words can be well extracted on RICO-PW and well\ngeneralized to a new dataset, P2S-UI, collected by ourselves. The effectiveness\nof PW2SS is further verified in the GUI understanding tasks including relation\nprediction, clickability prediction, screen retrieval, and app type\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:45:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fu", "Jingwen", ""], ["Zhang", "Xiaoyi", ""], ["Wang", "Yuwang", ""], ["Zeng", "Wenjun", ""], ["Yang", "Sam", ""], ["Hilliard", "Grayson", ""]]}, {"id": "2105.12043", "submitter": "Wenhao Wu", "authors": "Lining Wang, Haosen Yang, Wenhao Wu, Hongxun Yao, Hujie Huang", "title": "Temporal Action Proposal Generation with Transformers", "comments": "The first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer networks are effective at modeling long-range contextual\ninformation and have recently demonstrated exemplary performance in the natural\nlanguage processing domain. Conventionally, the temporal action proposal\ngeneration (TAPG) task is divided into two main sub-tasks: boundary prediction\nand proposal confidence prediction, which rely on the frame-level dependencies\nand proposal-level relationships separately. To capture the dependencies at\ndifferent levels of granularity, this paper intuitively presents a unified\ntemporal action proposal generation framework with original Transformers,\ncalled TAPG Transformer, which consists of a Boundary Transformer and a\nProposal Transformer. Specifically, the Boundary Transformer captures long-term\ntemporal dependencies to predict precise boundary information and the Proposal\nTransformer learns the rich inter-proposal relationships for reliable\nconfidence evaluation. Extensive experiments are conducted on two popular\nbenchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG\nTransformer outperforms state-of-the-art methods. Equipped with the existing\naction classifier, our method achieves remarkable performance on the temporal\naction localization task. Codes and models will be available.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:22:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Lining", ""], ["Yang", "Haosen", ""], ["Wu", "Wenhao", ""], ["Yao", "Hongxun", ""], ["Huang", "Hujie", ""]]}, {"id": "2105.12085", "submitter": "Wenhao Wu", "authors": "Wenhao Wu, Yuxiang Zhao, Yanwu Xu, Xiao Tan, Dongliang He, Zhikang\n  Zou, Jin Ye, Yingying Li, Mingde Yao, Zichao Dong, Yifeng Shi", "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level\n  Representation Learning", "comments": "Accepted by ACMMM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:09:57 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 13:43:37 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wu", "Wenhao", ""], ["Zhao", "Yuxiang", ""], ["Xu", "Yanwu", ""], ["Tan", "Xiao", ""], ["He", "Dongliang", ""], ["Zou", "Zhikang", ""], ["Ye", "Jin", ""], ["Li", "Yingying", ""], ["Yao", "Mingde", ""], ["Dong", "Zichao", ""], ["Shi", "Yifeng", ""]]}, {"id": "2105.12700", "submitter": "Luka Murn", "authors": "Luka Murn, Marc Gorriz Blanch, Maria Santamaria, Fiona Rivera, Marta\n  Mrak", "title": "Towards Transparent Application of Machine Learning in Video Processing", "comments": "International Broadcasting Convention, 11-14 Sep 2020, Amsterdam,\n  Netherlands (Technical Paper section, Virtual)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:24:23 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:35:54 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Murn", "Luka", ""], ["Blanch", "Marc Gorriz", ""], ["Santamaria", "Maria", ""], ["Rivera", "Fiona", ""], ["Mrak", "Marta", ""]]}, {"id": "2105.13096", "submitter": "Shanxiang Lyu", "authors": "Jieni Lin, Junren Qin, Shanxiang Lyu, Bingwen Feng, Jiabo Wang", "title": "Lattice-Based Minimum-Distortion Data Hiding", "comments": "5 pages, to appear in IEEE communications letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Lattices have been conceived as a powerful tool for data hiding. While\nconventional studies and applications focus on achieving the optimal robustness\nversus distortion tradeoff, in some applications such as data hiding in\nmedical/physiological signals, the primary concern is to achieve a minimum\namount of distortion to the cover signal. In this paper, we revisit the\ncelebrated quantization index modulation (QIM) scheme and propose a\nminimum-distortion version of it, referred to as MD-QIM. The crux of MD-QIM is\nto move the data point to only the boundary of the Voronoi region of the\nlattice point indexed by a message, which suffices for subsequent correct\ndecoding. At any fixed code rate, the scheme achieves the minimum amount of\ndistortion by sacrificing the robustness to the additive white Gaussian noise\n(AWGN) attacks. Simulation results confirm that our scheme significantly\noutperforms QIM in terms of mean square error (MSE), peak signal to noise ratio\n(PSNR) and percentage residual difference (PRD).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:48:01 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 15:25:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lin", "Jieni", ""], ["Qin", "Junren", ""], ["Lyu", "Shanxiang", ""], ["Feng", "Bingwen", ""], ["Wang", "Jiabo", ""]]}, {"id": "2105.13295", "submitter": "Michael Proulx", "authors": "Michael J. Proulx, Theodoros Eracleous, Ben Spencer, Anna Passfield,\n  Alexandra de Sousa, and Ali Mohammadi", "title": "Electromagnetic actuation for a vibrotactile display: Assessing stimuli\n  complexity and usability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory substitution has influenced the design of many tactile visual\nsubstitution systems with the aim of offering visual aids for the blind. This\npaper focuses on whether a novel electromagnetic vibrotactile display, a four\nby four vibrotactile matrix of taxels, can serve as an aid for dynamic\ncommunication for visually impaired people. A mixed methods approach was used\nto firstly assess whether pattern complexity affected undergraduate\nparticipants' perceptive success, and secondly, if participants total score\npositively correlated with their perceived success ratings. A thematic analysis\nwas also conducted on participants' experiences with the vibrotactile display\nand what methods of interaction they used. The results indicated that complex\npatterns were less accurately perceived than simple and linear patterns\nrespectively, and no significant correlation was found between participants'\nscore and perceived success ratings. Additionally, most participants interacted\nwith the vibrotactile display in similar ways using one finger to feel one\ntaxel at a time; arguably, the most effective strategy from previous research.\nThis technology could have applications to navigational and communication aids\nfor the visually impaired and road users.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:45:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Proulx", "Michael J.", ""], ["Eracleous", "Theodoros", ""], ["Spencer", "Ben", ""], ["Passfield", "Anna", ""], ["de Sousa", "Alexandra", ""], ["Mohammadi", "Ali", ""]]}, {"id": "2105.14538", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, Marcel Worring", "title": "Longer Version for \"Deep Context-Encoding Network for Retinal Image\n  Captioning\"", "comments": "This paper is a longer version of \"Deep Context-Encoding Network for\n  Retinal Image Captioning\" which is accepted by IEEE International Conference\n  on Image Processing (ICIP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating medical reports for retinal images is one of the\npromising ways to help ophthalmologists reduce their workload and improve work\nefficiency. In this work, we propose a new context-driven encoding network to\nautomatically generate medical reports for retinal images. The proposed model\nis mainly composed of a multi-modal input encoder and a fused-feature decoder.\nOur experimental results show that our proposed method is capable of\neffectively leveraging the interactive information between the input image and\ncontext, i.e., keywords in our case. The proposed method creates more accurate\nand meaningful reports for retinal images than baseline models and achieves\nstate-of-the-art performance. This performance is shown in several commonly\nused metrics for the medical report generation task: BLEU-avg (+16%), CIDEr\n(+10.2%), and ROUGE (+8.6%).\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:37:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Yang", "Chao-Han Huck", ""], ["Worring", "Marcel", ""]]}, {"id": "2105.14550", "submitter": "Wei Sun", "authors": "Wei Sun and Xiongkuo Min and Guangtao Zhai and Siwei Ma", "title": "Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature\n  Fusion and Iterative Mixed Database Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) is very important for both end-users and\nservice-providers since a high-quality image can significantly improve the\nuser's quality of experience (QoE). Most existing blind image quality\nassessment (BIQA) models were developed for synthetically distorted images,\nhowever, they perform poorly on in-the-wild images, which are widely existed in\nvarious practical applications. In this paper, we propose a novel BIQA model\nfor in-the-wild images by addressing two critical problems in this field: how\nto learn better quality-aware features, and how to solve the problem of\ninsufficient training samples. Considering that perceptual visual quality is\naffected by both low-level visual features and high-level semantic information,\nwe first propose a staircase structure to hierarchically integrate the features\nfrom intermediate layers into the final feature representation, which enables\nthe model to make full use of visual information from low-level to high-level.\nThen an iterative mixed database training (IMDT) strategy is proposed to train\nthe BIQA model on multiple databases simultaneously, so the model can benefit\nfrom the increase in both training samples and image content and distortion\ndiversity and can learn a more general feature representation. Experimental\nresults show that the proposed model outperforms other state-of-the-art BIQA\nmodels on six in-the-wild IQA databases by a large margin. Moreover, the\nproposed model shows an excellent performance in the cross-database evaluation\nexperiments, which further demonstrates that the learned feature representation\nis robust to images sampled from various distributions.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:04:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sun", "Wei", ""], ["Min", "Xiongkuo", ""], ["Zhai", "Guangtao", ""], ["Ma", "Siwei", ""]]}]