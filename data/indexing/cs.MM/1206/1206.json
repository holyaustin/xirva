[{"id": "1206.0692", "submitter": "Alexander Davydov", "authors": "Alexander Y. Davydov", "title": "Signal and Image Processing with Sinlets", "comments": "26 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM math-ph math.MP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new family of localized orthonormal bases - sinlets -\nwhich are well suited for both signal and image processing and analysis.\nOne-dimensional sinlets are related to specific solutions of the time-dependent\nharmonic oscillator equation. By construction, each sinlet is infinitely\ndifferentiable and has a well-defined and smooth instantaneous frequency known\nin analytical form. For square-integrable transient signals with infinite\nsupport, one-dimensional sinlet basis provides an advantageous alternative to\nthe Fourier transform by rendering accurate signal representation via a\ncountable set of real-valued coefficients. The properties of sinlets make them\nsuitable for analyzing many real-world signals whose frequency content changes\nwith time including radar and sonar waveforms, music, speech, biological\necholocation sounds, biomedical signals, seismic acoustic waves, and signals\nemployed in wireless communication systems. One-dimensional sinlet bases can be\nused to construct two- and higher-dimensional bases with variety of potential\napplications including image analysis and representation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2012 18:02:03 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2012 21:19:38 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2012 23:11:17 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Davydov", "Alexander Y.", ""]]}, {"id": "1206.1866", "submitter": "Yuanyi Xue", "authors": "Yuanyi Xue and Yao Wang", "title": "Perceptual quality comparison between single-layer and scalable videos\n  at the same spatial, temporal and amplitude resolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the perceptual quality difference between scalable and\nsingle-layer videos coded at the same spatial, temporal and amplitude\nresolution (STAR) is investigated through a subjective test using a mobile\nplatform. Three source videos are considered and for each source video\nsingle-layer and scalable video are compared at 9 different STARs. We utilize\npaired comparison methods with and without tie option. Results collected from\n10 subjects in the without \"tie\" option and 6 subjects in the with \"tie\" option\nshow that there is no significant quality difference between scalable and\nsinglelayer video when coded at the same STAR. An analysis of variance (ANOVA)\ntest is also performed to further confirm the finding.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2012 20:15:30 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Xue", "Yuanyi", ""], ["Wang", "Yao", ""]]}, {"id": "1206.1943", "submitter": "Rajiv Ranjan Dr.", "authors": "Dimitrios Georgakopoulos, Rajiv Ranjan, Karan Mitra, Xiangmin Zhou", "title": "MediaWise - Designing a Smart Media Cloud", "comments": "This paper appears as the invited keynote paper in the proceedings of\n  the International Conference on Advances in Cloud Computing (ACC-2012), July\n  26-28, 2012, Bangalore, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MediaWise project aims to expand the scope of existing media delivery\nsystems with novel cloud, personalization and collaboration capabilities that\ncan serve the needs of more users, communities, and businesses. The project\ndevelops a MediaWise Cloud platform that supports do-it-yourself creation,\nsearch, management, and consumption of multimedia content. The MediaWise Cloud\nsupports pay-as-you-go models and elasticity that are similar to those offered\nby commercially available cloud services. However, unlike existing commercial\nCDN services providers such as Limelight Networks and Akamai the MediaWise\nCloud require no ownerships of computing infrastructure and instead rely on the\npublic Internet and public cloud services (e.g., commercial cloud storage to\nstore its content). In addition to integrating such public cloud services into\na public cloud-based Content Delivery Network, the MediaWise Cloud also\nprovides advanced Quality of Service (QoS) management as required for the\ndelivery of streamed and interactive high resolution multimedia content. In\nthis paper, we give a brief overview of MediaWise Cloud architecture and\npresent a comprehensive discussion on research objectives related to its\nservice components. Finally, we also compare the features supported by the\nexisting CDN services against the envisioned objectives of MediaWise Cloud.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2012 14:07:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2012 03:12:37 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Georgakopoulos", "Dimitrios", ""], ["Ranjan", "Rajiv", ""], ["Mitra", "Karan", ""], ["Zhou", "Xiangmin", ""]]}, {"id": "1206.2320", "submitter": "YenFu Ou", "authors": "Yen-Fu Ou, Yuanyi Xue, Yao Wang", "title": "Q-STAR:A Perceptual Video Quality Model Considering Impact of Spatial,\n  Temporal, and Amplitude Resolutions", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the impact of spatial, temporal and amplitude\nresolution (STAR) on the perceptual quality of a compressed video. Subjective\nquality tests were carried out on a mobile device. Seven source sequences are\nincluded in the tests and for each source sequence we have 27 test\nconfigurations generated by JSVM encoder (3 QP levels, 3 spatial resolutions,\nand 3 temporal resolutions), resulting a total of 189 processed video sequences\n(PVSs). Videos coded at different spatial resolutions are displayed at the full\nscreen size of the mobile platform. Subjective data reveal that the impact of\nspatial resolution (SR), temporal resolution (TR) and quantization stepsize\n(QS) can each be captured by a function with a single content-dependent\nparameter. The joint impact of SR, TR and QS can be accurately modeled by the\nproduct of these three functions with only three parameters. We further find\nthat the quality decay rates with SR and QS, respectively are independent of\nTR, and likewise, the decay rate with TR is independent of SR and QS,\nrespectively. However, there is a significant interaction between the effects\nof SR and QS. The overall quality model is further validated on five other\ndatasets with very high accuracy. The complete model correlates well with the\nsubjective ratings with a Pearson Correlation Coefficient (PCC) of 0.991.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2012 19:06:07 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Ou", "Yen-Fu", ""], ["Xue", "Yuanyi", ""], ["Wang", "Yao", ""]]}, {"id": "1206.2484", "submitter": "Puneet  Singh", "authors": "Puneet Singh, Ashutosh Kapoor, Vishal Kaushik and Hima Bindu\n  Maringanti", "title": "Architecture for Automated Tagging and Clustering of Song Files\n  According to Mood", "comments": "7 pages", "journal-ref": "IJCSI Volume 7, Issue 4, No 2, pp 11-17, July 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music is one of the basic human needs for recreation and entertainment. As\nsong files are digitalized now a days, and digital libraries are expanding\ncontinuously, which makes it difficult to recall a song. Thus need of a new\nclassification system other than genre is very obvious and mood based\nclassification system serves the purpose very well. In this paper we will\npresent a well-defined architecture to classify songs into different mood-based\ncategories, using audio content analysis, affective value of song lyrics to map\na song onto a psychological-based emotion space and information from online\nsources. In audio content analysis we will use music features such as\nintensity, timbre and rhythm including their subfeatures to map music in a\n2-Dimensional emotional space. In lyric based classification 1-Dimensional\nemotional space is used. Both the results are merged onto a 2-Dimensional\nemotional space, which will classify song into a particular mood category.\nFinally clusters of mood based song files are formed and arranged according to\ndata acquired from various Internet sources.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 10:28:11 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Singh", "Puneet", ""], ["Kapoor", "Ashutosh", ""], ["Kaushik", "Vishal", ""], ["Maringanti", "Hima Bindu", ""]]}, {"id": "1206.2510", "submitter": "David Novak", "authors": "David Novak, Petr Volny, Pavel Zezula", "title": "Generic Subsequence Matching Framework: Modularity, Flexibility,\n  Efficiency", "comments": "This is an extended version of a paper published on DEXA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsequence matching has appeared to be an ideal approach for solving many\nproblems related to the fields of data mining and similarity retrieval. It has\nbeen shown that almost any data class (audio, image, biometrics, signals) is or\ncan be represented by some kind of time series or string of symbols, which can\nbe seen as an input for various subsequence matching approaches. The variety of\ndata types, specific tasks and their partial or full solutions is so wide that\nthe choice, implementation and parametrization of a suitable solution for a\ngiven task might be complicated and time-consuming; a possibly fruitful\ncombination of fragments from different research areas may not be obvious nor\neasy to realize. The leading authors of this field also mention the\nimplementation bias that makes difficult a proper comparison of competing\napproaches. Therefore we present a new generic Subsequence Matching Framework\n(SMF) that tries to overcome the aforementioned problems by a uniform frame\nthat simplifies and speeds up the design, development and evaluation of\nsubsequence matching related systems. We identify several relatively separate\nsubtasks solved differently over the literature and SMF enables to combine them\nin straightforward manner achieving new quality and efficiency. This framework\ncan be used in many application domains and its components can be reused\neffectively. Its strictly modular architecture and openness enables also\ninvolvement of efficient solutions from different fields, for instance\nefficient metric-based indexes. This is an extended version of a paper\npublished on DEXA 2012.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 12:39:04 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Novak", "David", ""], ["Volny", "Petr", ""], ["Zezula", "Pavel", ""]]}, {"id": "1206.2625", "submitter": "Zhan Ma", "authors": "Zhan Ma and Hao Hu and Meng Xu and Yao Wang", "title": "Rate Model for Compressed Video Considering Impacts Of Spatial, Temporal\n  and Amplitude Resolutions and Its Applications for Video Coding and\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the impacts of spatial, temporal and amplitude\nresolution (STAR) on the bit rate of a compressed video. We propose an\nanalytical rate model in terms of the quantization stepsize, frame size and\nframe rate. Experimental results reveal that the increase of the video rate as\nthe individual resolution increases follows a power function. Hence, the\nproposed model expresses the rate as the product of power functions of the\nquantization stepsize, frame size and frame rate, respectively. The proposed\nrate model is analytically tractable, requiring only four content dependent\nparameters. We also propose methods for predicting the model parameters from\ncontent features that can be computed from original video. Simulation results\nshow that model predicted rates fit the measured data very well with high\nPearson correlation (PC) and small relative root mean square error (RRMSE). The\nsame model function works for different coding scenarios (including scalable\nand non-scalable video, temporal prediction using either hierarchical B or IPPP\nstructure, etc.) with very high accuracy (average PC $>$ 0.99), but the values\nof model parameters differ. Using the proposed rate model and the quality model\nintroduced in a separate work, we show how to optimize the STAR for a given\nrate constraint, which is important for both encoder rate control and scalable\nvideo adaptation. Furthermore, we demonstrate how to order the spatial,\ntemporal and amplitude layers of a scalable video in a rate-quality optimized\nway.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 19:13:02 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Ma", "Zhan", ""], ["Hu", "Hao", ""], ["Xu", "Meng", ""], ["Wang", "Yao", ""]]}, {"id": "1206.2847", "submitter": "Nicolas Friot", "authors": "Nicolas Friot, Christophe Guyeux, and Jacques M. Bahi", "title": "Topological study and Lyapunov exponent of a secure steganographic\n  scheme", "comments": "This paper has been withdrawn because of it's acceptation in the\n  conference SECRYPT'2013. (See my profile on Research Gate or the conference\n  web site). ******* http://www.researchgate.net/profile/Nicolas_Friot/\n  ******** http://www.secrypt.icete.org/ ********", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CIS2 is a steganographic scheme proposed in the information hiding\nliterature, belonging into the small category of algorithms being both stego\nand topologically secure. Due to its stego-security, this scheme is able to\nface attacks that take place into the \"watermark only attack\" framework. Its\ntopological security reinforce its capability to face attacks in other\nframeworks as \"known message attack\" or \"known original attack\", in the\nSimmons' prisoner problem. In this research work, the study of topological\nproperties of C I S 2 is enlarged by describing this scheme as iterations over\nthe real line, and investigating other security properties of topological\nnature as the Lyapunov exponent. Results show that this scheme is able to\nwithdraw a malicious attacker in the \"estimated original attack\" context too.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:44:05 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2013 05:51:59 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Friot", "Nicolas", ""], ["Guyeux", "Christophe", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1206.4256", "submitter": "Mehdi Khalili", "authors": "Mehdi Khalili", "title": "A Novel Effective, Secure and Robust CDMA Digital Image Watermarking in\n  YUV Color Space Using DWT2", "comments": "9 Pages, 16 Figures, International Journal of Computer Science\n  Issues,May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is allocated to CDMA digital images watermarking for ownership\nverification and image authentication applications, which for more security,\nwatermark W is converted to a sequence and then a random binary sequence R of\nsize n is adopted to encrypt the watermark; where n is the size of the\nwatermark. This adopting process uses a pseudo-random number generator to\ndetermine the pixel to be used on a given key. After converting the host image\nto YUV color space and then wavelet decomposition of Y channel, this adopted\nwatermark is embedded into the selected subbands coefficients of Y channel\nusing the correlation properties of additive pseudo- random noise patterns. The\nexperimental results show that the proposed approach provides extra\nimperceptibility, security and robustness against JPEG compression and\ndifferent noises attacks compared to the similar proposed methods. Moreover,\nthe proposed approach has no need of the original image to extract watermarks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2012 16:30:49 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Khalili", "Mehdi", ""]]}, {"id": "1206.4326", "submitter": "Vijayaraghavan Thirumalai", "authors": "Vijayaraghavan Thirumalai and Pascal Frossard", "title": "Joint Reconstruction of Multi-view Compressed Images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2013.2240006", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed representation of correlated multi-view images is an\nimportant problem that arise in vision sensor networks. This paper concentrates\non the joint reconstruction problem where the distributively compressed\ncorrelated images are jointly decoded in order to improve the reconstruction\nquality of all the compressed images. We consider a scenario where the images\ncaptured at different viewpoints are encoded independently using common coding\nsolutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among\ndifferent cameras. A central decoder first estimates the underlying correlation\nmodel from the independently compressed images which will be used for the joint\nsignal recovery. The joint reconstruction is then cast as a constrained convex\noptimization problem that reconstructs total-variation (TV) smooth images that\ncomply with the estimated correlation model. At the same time, we add\nconstraints that force the reconstructed images to be consistent with their\ncompressed versions. We show by experiments that the proposed joint\nreconstruction scheme outperforms independent reconstruction in terms of image\nquality, for a given target bit rate. In addition, the decoding performance of\nour proposed algorithm compares advantageously to state-of-the-art distributed\ncoding schemes based on disparity learning and on the DISCOVER.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2012 20:16:04 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Thirumalai", "Vijayaraghavan", ""], ["Frossard", "Pascal", ""]]}, {"id": "1206.4513", "submitter": "Mehdi Khalili", "authors": "Mehdi Khalili, David Asatryan", "title": "Improved DWT Based Watermarking Using JPEG-YCbCr", "comments": "4 Pages, 5 Figures, 3 Tables", "journal-ref": "CSIT Conference, Yerevan, Armenia, September 26-30, 2011", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a blind, Secure, imperceptible and robust watermarking\nalgorithm based on wavelet transform domain is proposed in which for more\nsecurity, the watermark W is converted to a sequence and then a random binary\nsequence R of size n is adopted to encrypt the watermark, where n is the size\nof the watermark image. Afterwards, the encrypted watermark sequence W1 is\ngenerated by executing exclusive-OR operation on W and R. This generated\nwatermark embeds into low frequency selected coefficients of Y channel wavelet\ndecomposition of JPEG-YCbCr using LSB insertion technique. The experimental\nresults show that the proposed algorithm increases the security and\nimperceptibility of watermark and has better robustness against wavelet\ncompression and cropping attacks compared to the earlier work in [1].\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:30:08 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Khalili", "Mehdi", ""], ["Asatryan", "David", ""]]}, {"id": "1206.4520", "submitter": "Mehdi Khalili", "authors": "Mehdi Khalili, David Asatryan", "title": "Effective Digital Image Watermarking in YCbCr Color Space Accompanied by\n  Presenting a Novel Technique Using DWT", "comments": "12 Pages, 4 Figures, 8 Tables", "journal-ref": "Mathematical Problems of Computer Science, 2010", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a quantization based watermark casting and blind watermark\nretrieval algorithm operating in YCbCr color space using discrete wavelet\ntransform (DWT), for ownership verification and image authentication\napplications is implemented. This method uses implicit visual masking by\ninserting watermark bits into only the wavelet coefficients of high magnitude,\nin Y channel of YCbCr color space. A blind watermark retrieval technique that\ncan detect the embedded watermark without the help from the original\nuncorrupted image is devised which is computationally efficient. The new\nwatermarking algorithm combines and adapts various aspects from existing\nwatermarking methods. Experimental results show that the proposed technique to\nembed watermark provides extra imperceptibility and robustness against various\nsignal processing attacks in comparison with the same technique in RGB color\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:54:26 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Khalili", "Mehdi", ""], ["Asatryan", "David", ""]]}, {"id": "1206.4582", "submitter": "Mehdi Khalili", "authors": "Mehdi Khalili", "title": "A Comparison between Digital Image Watermarking in Tow Different Color\n  Spaces Using DWT2", "comments": "5 pages, 4 Figures, 6 Tables. arXiv admin note: text overlap with\n  arXiv:1007.5136 by other authors", "journal-ref": "Computer Science and Information Technologies, 28 September - 2\n  October, 2009, Yerevan, Armenia", "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel digital watermarking for ownership verification and image\nauthentication applications using discrete wavelet transform (DWT) is proposed\nin this paper. Most previous proposed watermarking algorithms embed sequences\nof random numbers as watermarks. Here binary images are taken as watermark for\nembedding. In the proposed approach, the host image is converted into the YCbCr\ncolor space and then its Y channel decomposed into wavelet coefficients. The\nselected approximation coefficients are quantized and then their four least\nsignificant bits of the quantized coefficients are replaced by the watermark\nusing LSB insertion technique. At last, the watermarked image is synthesized\nfrom the changed and unchanged DWT coefficients. The experiments show that the\nproposed approach provides extra imperceptibility and robustness against\nwavelet compression compared to the traditional embedding methods in RGB color\nspace. Moreover, the proposed approach has no need of the original image to\nextract watermarks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 19:11:10 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Khalili", "Mehdi", ""]]}, {"id": "1206.4662", "submitter": "Ivo Shterev", "authors": "Ivo Shterev (Duke University), David Dunson (Duke University)", "title": "Bayesian Watermark Attacks", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an application of statistical machine learning to the\nfield of watermarking. We propose a new attack model on additive\nspread-spectrum watermarking systems. The proposed attack is based on Bayesian\nstatistics. We consider the scenario in which a watermark signal is repeatedly\nembedded in specific, possibly chosen based on a secret message bitstream,\nsegments (signals) of the host data. The host signal can represent a patch of\npixels from an image or a video frame. We propose a probabilistic model that\ninfers the embedded message bitstream and watermark signal, directly from the\nwatermarked data, without access to the decoder. We develop an efficient Markov\nchain Monte Carlo sampler for updating the model parameters from their\nconjugate full conditional posteriors. We also provide a variational Bayesian\nsolution, which further increases the convergence speed of the algorithm.\nExperiments with synthetic and real image signals demonstrate that the attack\nmodel is able to correctly infer a large part of the message bitstream and\nobtain a very accurate estimate of the watermark signal.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:30:35 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Shterev", "Ivo", "", "Duke University"], ["Dunson", "David", "", "Duke University"]]}]