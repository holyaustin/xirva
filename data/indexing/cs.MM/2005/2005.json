[{"id": "2005.00836", "submitter": "Saman Zadtootaghaj", "authors": "Markus Utke, Saman Zadtootaghaj, Steven Schmidt, Sebastian M\\\"oller", "title": "Towards Deep Learning Methods for Quality Assessment of\n  Computer-Generated Imagery", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video gaming streaming services are growing rapidly due to new services such\nas passive video streaming, e.g. Twitch.tv, and cloud gaming, e.g. Nvidia\nGeforce Now. In contrast to traditional video content, gaming content has\nspecial characteristics such as extremely high motion for some games, special\nmotion patterns, synthetic content and repetitive content, which makes the\nstate-of-the-art video and image quality metrics perform weaker for this\nspecial computer generated content. In this paper, we outline our plan to build\na deep learningbased quality metric for video gaming quality assessment. In\naddition, we present initial results by training the network based on VMAF\nvalues as a ground truth to give some insights on how to build a metric in\nfuture. The paper describes the method that is used to choose an appropriate\nConvolutional Neural Network architecture. Furthermore, we estimate the size of\nthe required subjective quality dataset which achieves a sufficiently high\nperformance. The results show that by taking around 5k images for training of\nthe last six modules of Xception, we can obtain a relatively high performance\nmetric to assess the quality of distorted video games.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 14:08:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Utke", "Markus", ""], ["Zadtootaghaj", "Saman", ""], ["Schmidt", "Steven", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2005.00974", "submitter": "Zihao Wang", "authors": "Srutarshi Banerjee, Zihao W. Wang, Henry H. Chopp, Oliver Cossairt,\n  Aggelos Katsaggelos", "title": "Lossy Event Compression based on Image-derived Quad Trees and Poisson\n  Disk Sampling", "comments": "8 main pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With several advantages over conventional RGB cameras, event cameras have\nprovided new opportunities for tackling visual tasks under challenging\nscenarios with fast motion, high dynamic range, and/or power constraint. Yet\nunlike image/video compression, the performance of event compression algorithm\nis far from satisfying and practical. The main challenge for compressing events\nis the unique event data form, i.e., a stream of asynchronously fired event\ntuples each encoding the 2D spatial location, timestamp, and polarity (denoting\nan increase or decrease in brightness). Since events only encode temporal\nvariations, they lack spatial structure which is crucial for compression. To\naddress this problem, we propose a novel event compression algorithm based on a\nquad tree (QT) segmentation map derived from the adjacent intensity images. The\nQT informs 2D spatial priority within the 3D space-time volume. In the event\nencoding step, events are first aggregated over time to form polarity-based\nevent histograms. The histograms are then variably sampled via Poisson Disk\nSampling prioritized by the QT based segmentation map. Next, differential\nencoding and run length encoding are employed for encoding the spatial and\npolarity information of the sampled events, respectively, followed by Huffman\nencoding to produce the final encoded events. Our Poisson Disk Sampling based\nLossy Event Compression (PDS-LEC) algorithm performs rate-distortion based\noptimal allocation. On average, our algorithm achieves greater than 6x\ncompression compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 03:18:43 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 07:41:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Banerjee", "Srutarshi", ""], ["Wang", "Zihao W.", ""], ["Chopp", "Henry H.", ""], ["Cossairt", "Oliver", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "2005.02149", "submitter": "Jan Zah\\'alka", "authors": "Jan Zah\\'alka, Marcel Worring and Jarke J. van Wijk", "title": "II-20: Intelligent and pragmatic analytic categorization of image\n  collections", "comments": "9 pages, 7 figures, 1 table. Camera-ready paper, to appear in IEEE\n  VIS 2020 and IEEE TVCG in January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce II-20 (Image Insight 2020), a multimedia analytics approach for\nanalytic categorization of image collections. Advanced visualizations for image\ncollections exist, but they need tight integration with a machine model to\nsupport analytic categorization. Directly employing computer vision and\ninteractive learning techniques gravitates towards search. Analytic\ncategorization, however, is not machine classification (the difference between\nthe two is called the pragmatic gap): a human adds/redefines/deletes categories\nof relevance on the fly to build insight, whereas the machine classifier is\nrigid and non-adaptive. Analytic categorization that brings the user to insight\nrequires a flexible machine model that allows dynamic sliding on the\nexploration-search axis, as well as semantic interactions. II-20 brings 3 major\ncontributions to multimedia analytics on image collections and towards closing\nthe pragmatic gap. Firstly, a machine model that closely follows the user's\ninteractions and dynamically models her categories of relevance. II-20's model,\nin addition to matching and exceeding the state of the art w. r. t. relevance,\nallows the user to dynamically slide on the exploration-search axis without\nadditional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris\nmetaphor that synergizes with the model. It allows the model to analyze the\ncollection by itself with minimal interaction from the user and complements the\nclassic grid metaphor. Thirdly, the fast-forward interaction, allowing the user\nto harness the model to quickly expand (\"fast-forward\") the categories of\nrelevance, expands the multimedia analytics semantic interaction dictionary.\nAutomated experiments show that II-20's model outperforms the state of the art\nand also demonstrate Tetris's analytic quality. User studies confirm that II-20\nis an intuitive, efficient, and effective multimedia analytics tool.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:43:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 10:24:46 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 09:30:17 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zah\u00e1lka", "Jan", ""], ["Worring", "Marcel", ""], ["van Wijk", "Jarke J.", ""]]}, {"id": "2005.02154", "submitter": "Xiu-Shen Wei", "authors": "Benyi Hu, Ren-Jie Song, Xiu-Shen Wei, Yazhou Yao, Xian-Sheng Hua, and\n  Yuehu Liu", "title": "PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by\n  Deep Convolutional Neural Networks", "comments": "Accepted by ACM Multimedia Conference 2020. PyRetri is open-source\n  and available at https://github.com/PyRetri/PyRetri", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress of applying deep learning methods to the field\nof content-based image retrieval, there has not been a software library that\ncovers these methods in a unified manner. In order to fill this gap, we\nintroduce PyRetri, an open source library for deep learning based unsupervised\nimage retrieval. The library encapsulates the retrieval process in several\nstages and provides functionality that covers various prominent methods for\neach stage. The idea underlying its design is to provide a unified platform for\ndeep learning based image retrieval research, with high usability and\nextensibility. To the best of our knowledge, this is the first open-source\nlibrary for unsupervised image retrieval by deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:17:18 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 13:12:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Hu", "Benyi", ""], ["Song", "Ren-Jie", ""], ["Wei", "Xiu-Shen", ""], ["Yao", "Yazhou", ""], ["Hua", "Xian-Sheng", ""], ["Liu", "Yuehu", ""]]}, {"id": "2005.02472", "submitter": "Alireza Zareian", "authors": "Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng\n  Ji, Shih-Fu Chang", "title": "Cross-media Structured Common Space for Multimedia Event Extraction", "comments": "Accepted as an oral paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to\nextract events and their arguments from multimedia documents. We develop the\nfirst benchmark and collect a dataset of 245 multimedia news articles with\nextensively annotated events and arguments. We propose a novel method, Weakly\nAligned Structured Embedding (WASE), that encodes structured representations of\nsemantic information from textual and visual data into a common embedding\nspace. The structures are aligned across modalities by employing a weakly\nsupervised training strategy, which enables exploiting available resources\nwithout explicit cross-media annotation. Compared to uni-modal state-of-the-art\nmethods, our approach achieves 4.0% and 9.8% absolute F-score gains on text\nevent argument role labeling and visual event extraction. Compared to\nstate-of-the-art multimedia unstructured representations, we achieve 8.3% and\n5.0% absolute F-score gains on multimedia event extraction and argument role\nlabeling, respectively. By utilizing images, we extract 21.4% more event\nmentions than traditional text-only methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:21:53 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Manling", ""], ["Zareian", "Alireza", ""], ["Zeng", "Qi", ""], ["Whitehead", "Spencer", ""], ["Lu", "Di", ""], ["Ji", "Heng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2005.03286", "submitter": "Fabio Poiesi", "authors": "Matteo Bortolon, Paul Chippendale, Stefano Messelodi and Fabio Poiesi", "title": "Multi-view data capture using edge-synchronised mobiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data capture permits free-viewpoint video (FVV) content creation.\nTo this end, several users must capture video streams, calibrated in both time\nand pose, framing the same object/scene, from different viewpoints.\nNew-generation network architectures (e.g. 5G) promise lower latency and larger\nbandwidth connections supported by powerful edge computing, properties that\nseem ideal for reliable FVV capture. We have explored this possibility, aiming\nto remove the need for bespoke synchronisation hardware when capturing a scene\nfrom multiple viewpoints, making it possible through off-the-shelf mobiles. We\npropose a novel and scalable data capture architecture that exploits edge\nresources to synchronise and harvest frame captures. We have designed an edge\ncomputing unit that supervises the relaying of timing triggers to and from\nmultiple mobiles, in addition to synchronising frame harvesting. We empirically\nshow the benefits of our edge computing unit by analysing latencies and show\nthe quality of 3D reconstruction outputs against an alternative and popular\ncentralised solution based on Unity3D.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:13:20 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Bortolon", "Matteo", ""], ["Chippendale", "Paul", ""], ["Messelodi", "Stefano", ""], ["Poiesi", "Fabio", ""]]}, {"id": "2005.03297", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng\n  Chua", "title": "Knowledge Enhanced Neural Fashion Trend Forecasting", "comments": "8 pages, 9 figures, ICMR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion trend forecasting is a crucial task for both academia and industry.\nAlthough some efforts have been devoted to tackling this challenging task, they\nonly studied limited fashion elements with highly seasonal or simple patterns,\nwhich could hardly reveal the real fashion trends. Towards insightful fashion\ntrend forecasting, this work focuses on investigating fine-grained fashion\nelement trends for specific user groups. We first contribute a large-scale\nfashion trend dataset (FIT) collected from Instagram with extracted time series\nfashion element records and user information. Further-more, to effectively\nmodel the time series data of fashion elements with rather complex patterns, we\npropose a Knowledge EnhancedRecurrent Network model (KERN) which takes\nadvantage of the capability of deep recurrent neural networks in modeling\ntime-series data. Moreover, it leverages internal and external knowledge in\nfashion domain that affects the time-series patterns of fashion element trends.\nSuch incorporation of domain knowledge further enhances the deep learning model\nin capturing the patterns of specific fashion elements and predicting the\nfuture trends. Extensive experiments demonstrate that the proposed KERN model\ncan effectively capture the complicated patterns of objective fashion elements,\ntherefore making preferable fashion trend forecast.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:42:17 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 09:14:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ma", "Yunshan", ""], ["Ding", "Yujuan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2005.03310", "submitter": "Pranab K. Muhuri Dr.", "authors": "Zubair Ashraf, Mukul Lata Roy, Pranab K.Muhuri, and Q. M. Danish\n  Lohani", "title": "Interval type-2 fuzzy logic system based similarity evaluation for image\n  steganography", "comments": null, "journal-ref": "Heliyon 6(5) (2020) e03771", "doi": "10.1016/j.heliyon.2020.e03771", "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity measure, also called information measure, is a concept used to\ndistinguish different objects. It has been studied from different contexts by\nemploying mathematical, psychological, and fuzzy approaches. Image\nsteganography is the art of hiding secret data into an image in such a way that\nit cannot be detected by an intruder. In image steganography, hiding secret\ndata in the plain or non-edge regions of the image is significant due to the\nhigh similarity and redundancy of the pixels in their neighborhood. However,\nthe similarity measure of the neighboring pixels, i.e., their proximity in\ncolor space, is perceptual rather than mathematical. This paper proposes an\ninterval type 2 fuzzy logic system (IT2 FLS) to determine the similarity\nbetween the neighboring pixels by involving an instinctive human perception\nthrough a rule-based approach. The pixels of the image having high similarity\nvalues, calculated using the proposed IT2 FLS similarity measure, are selected\nfor embedding via the least significant bit (LSB) method. We term the proposed\nprocedure of steganography as IT2 FLS LSB method. Moreover, we have developed\ntwo more methods, namely, type 1 fuzzy logic system based least significant\nbits (T1FLS LSB) and Euclidean distance based similarity measures for least\nsignificant bit (SM LSB) steganographic methods. Experimental simulations were\nconducted for a collection of images and quality index metrics, such as PSNR,\nUQI, and SSIM are used. All the three steganographic methods are applied on\ndatasets and the quality metrics are calculated. The obtained stego images and\nresults are shown and thoroughly compared to determine the efficacy of the IT2\nFLS LSB method. Finally, we have done a comparative analysis of the proposed\napproach with the existing well-known steganographic methods to show the\neffectiveness of our proposed steganographic method.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:12:46 GMT"}], "update_date": "2020-06-28", "authors_parsed": [["Ashraf", "Zubair", ""], ["Roy", "Mukul Lata", ""], ["Muhuri", "Pranab K.", ""], ["Lohani", "Q. M. Danish", ""]]}, {"id": "2005.03373", "submitter": "Mario Montagud Climent", "authors": "Chris Hughes and Mario Montagud", "title": "Accessibility in 360-degree video players", "comments": "25 pages, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any media experience must be fully inclusive and accessible to all users\nregardless of their ability. With the current trend towards immersive\nexperiences, such as Virtual Reality (VR) and 360-degree video, it becomes key\nthat these environments are adapted to be fully accessible. However, until\nrecently the focus has been mostly on adapting the existing techniques to fit\nimmersive displays, rather than considering new approaches for accessibility\ndesigned specifically for these increasingly relevant media experiences. This\npaper surveys a wide range of 360-degree video players and examines the\nfeatures they include for dealing with accessibility, such as Subtitles, Audio\nDescription, Sign Language, User Interfaces, and other interaction features,\nlike voice control and support for multi-screen scenarios. These features have\nbeen chosen based on guidelines from standardization contributions, like in the\nWorld Wide Web Consortium (W3C) and the International Communication Union\n(ITU), and from research contributions for making 360-degree video consumption\nexperiences accessible. The in-depth analysis has been part of a research\neffort towards the development of a fully inclusive and accessible 360-degree\nvideo player. The paper concludes by discussing how the newly developed player\nhas gone above and beyond the existing solutions and guidelines, by providing\naccessibility features that meet the expectations for a widely used immersive\nmedium, like 360-degree video.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 10:40:47 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Hughes", "Chris", ""], ["Montagud", "Mario", ""]]}, {"id": "2005.03756", "submitter": "Mario Montagud Climent", "authors": "Peter tho Pesch (IRT, Germany), Romain Bouqueau (Motion Spell,\n  France), Mario Montagud (i2CAT, Spain)", "title": "White Paper: Recommendations for immersive accessibility services", "comments": "26 pages, White Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides recommendations on how to integrate accessibility\nsolutions, like subtitling, audio description and sign language, with immersive\nmedia services, with a focus on 360-degree video and spatial audio. It provides\nan in-depth analysis of the features provided by state-of-the-art standard\nsolutions to achieve this goal, and elaborates on the finding and proposed\nsolutions from the EU H2020 ImAc project to address existing gaps. The proposed\nsolutions are described qualitatively and technically, including example\nimplementations. The document is intended to serve as a valuable information\nsource for early adopters who plan to provide accessibility services to their\nportfolio with standard-compliant solutions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:10:53 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Pesch", "Peter tho", "", "IRT, Germany"], ["Bouqueau", "Romain", "", "Motion Spell,\n  France"], ["Montagud", "Mario", "", "i2CAT, Spain"]]}, {"id": "2005.03912", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Debesh Jha, Hugo Lewi Hammer, H{\\aa}vard D.\n  Johansen, Dag Johansen, P{\\aa}l Halvorsen, Michael A. Riegler", "title": "An Extensive Study on Cross-Dataset Bias and Evaluation Metrics\n  Interpretation for Machine Learning applied to Gastrointestinal Tract\n  Abnormality Classification", "comments": "30 pages, 12 figures, 8 tables, Accepted for ACM Transactions on\n  Computing for Healthcare", "journal-ref": null, "doi": "10.1145/3386295", "report-no": null, "categories": "cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise and efficient automated identification of Gastrointestinal (GI) tract\ndiseases can help doctors treat more patients and improve the rate of disease\ndetection and identification. Currently, automatic analysis of diseases in the\nGI tract is a hot topic in both computer science and medical-related journals.\nNevertheless, the evaluation of such an automatic analysis is often incomplete\nor simply wrong. Algorithms are often only tested on small and biased datasets,\nand cross-dataset evaluations are rarely performed. A clear understanding of\nevaluation metrics and machine learning models with cross datasets is crucial\nto bring research in the field to a new quality level. Towards this goal, we\npresent comprehensive evaluations of five distinct machine learning models\nusing Global Features and Deep Neural Networks that can classify 16 different\nkey types of GI tract conditions, including pathological findings, anatomical\nlandmarks, polyp removal conditions, and normal findings from images captured\nby common GI tract examination instruments. In our evaluation, we introduce\nperformance hexagons using six performance metrics such as recall, precision,\nspecificity, accuracy, F1-score, and Matthews Correlation Coefficient to\ndemonstrate how to determine the real capabilities of models rather than\nevaluating them shallowly. Furthermore, we perform cross-dataset evaluations\nusing different datasets for training and testing. With these cross-dataset\nevaluations, we demonstrate the challenge of actually building a generalizable\nmodel that could be used across different hospitals. Our experiments clearly\nshow that more sophisticated performance metrics and evaluation methods need to\nbe applied to get reliable models rather than depending on evaluations of the\nsplits of the same dataset, i.e., the performance metrics should always be\ninterpreted together rather than relying on a single metric.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 08:59:31 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Thambawita", "Vajira", ""], ["Jha", "Debesh", ""], ["Hammer", "Hugo Lewi", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""], ["Halvorsen", "P\u00e5l", ""], ["Riegler", "Michael A.", ""]]}, {"id": "2005.04400", "submitter": "Franz G\\\"otz-Hahn", "authors": "Franz G\\\"otz-Hahn, Vlad Hosu, Dietmar Saupe", "title": "Comment on \"No-Reference Video Quality Assessment Based on the Temporal\n  Pooling of Deep Features\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Neural Processing Letters 50,3 (2019) a machine learning approach to blind\nvideo quality assessment was proposed. It is based on temporal pooling of\nfeatures of video frames, taken from the last pooling layer of deep\nconvolutional neural networks. The method was validated on two established\nbenchmark datasets and gave results far better than the previous\nstate-of-the-art. In this letter we report the results from our careful\nreimplementations. The performance results, claimed in the paper, cannot be\nreached, and are even below the state-of-the-art by a large margin. We show\nthat the originally reported wrong performance results are a consequence of two\ncases of data leakage. Information from outside the training dataset was used\nin the fine-tuning stage and in the model evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:28:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["G\u00f6tz-Hahn", "Franz", ""], ["Hosu", "Vlad", ""], ["Saupe", "Dietmar", ""]]}, {"id": "2005.04425", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Kiyoharu Aizawa, Azuma Fujimoto, Atsushi Otsubo, Toru Ogawa, Yusuke\n  Matsui, Koki Tsubota, Hikaru Ikuta", "title": "Building a Manga Dataset \"Manga109\" with Annotations for Multimedia\n  Applications", "comments": "10 pages, 8 figures", "journal-ref": "IEEE MultiMedia 2020", "doi": "10.1109/MMUL.2020.2987895", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manga, or comics, which are a type of multimodal artwork, have been left\nbehind in the recent trend of deep learning applications because of the lack of\na proper dataset. Hence, we built Manga109, a dataset consisting of a variety\nof 109 Japanese comic books (94 authors and 21,142 pages) and made it publicly\navailable by obtaining author permissions for academic use. We carefully\nannotated the frames, speech texts, character faces, and character bodies; the\ntotal number of annotations exceeds 500k. This dataset provides numerous manga\nimages and annotations, which will be beneficial for use in machine learning\nalgorithms and their evaluation. In addition to academic use, we obtained\nfurther permission for a subset of the dataset for industrial use. In this\narticle, we describe the details of the dataset and present a few examples of\nmultimedia processing applications (detection, retrieval, and generation) that\napply existing deep learning methods and are made possible by the dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 12:26:58 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:07:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Aizawa", "Kiyoharu", ""], ["Fujimoto", "Azuma", ""], ["Otsubo", "Atsushi", ""], ["Ogawa", "Toru", ""], ["Matsui", "Yusuke", ""], ["Tsubota", "Koki", ""], ["Ikuta", "Hikaru", ""]]}, {"id": "2005.04558", "submitter": "Xiaowei Tang", "authors": "Xiao-Wei Tang and Xin-Lin Huang", "title": "A Design of SDR-based Pseudo-Analog Wireless Video Transmission System", "comments": null, "journal-ref": "Mobile Networks and Applications, 2020", "doi": "10.1007/s11036-020-01592-6", "report-no": null, "categories": "eess.SP cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-analog wireless video transmission technology can improve the\neffectiveness, reliability, and robustness of the conventional digital system\nin video broadcast scenarios. Although some prototypes of IEEE 802.11 series\nhave been developed for researchers to do simulations and experiments, they are\nusually expensive and provide very limited access to the physical layer. More\nimportantly, these prototypes cannot be used to verify the correctness of the\nnew proposed pseudo-analog wireless video transmission algorithms directly due\nto limited modulation modes they can support. In this paper, we present a novel\ndesign of software radio platform (SDR)-based pseudo-analog wireless video\ntransceiver which is completely transparent and allows users to learn all the\nimplementation details. Firstly, we prove that the analog method can also\nachieve the optimal performance as the digital method from the perspective of\nthe rate-distortion theory. Then, we describe the two hardware implementation\ndifficulties existed in the designing process including the data format\nmodification and the non-linear distortion. Next, we introduce the\nimplementation details of the designed transceiver. Finally, we analyze the\nperformance of the designed transceiver. Specifically, the results show that\nthe designed system can work effectively in both simulations and experiments.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 03:17:26 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Tang", "Xiao-Wei", ""], ["Huang", "Xin-Lin", ""]]}, {"id": "2005.05319", "submitter": "Shadrokh Samavi", "authors": "Mohsen Hajabdolahi, Nader Karimi, Shahram Shirani, Shadrokh Samavi", "title": "Hardware Implementation of Adaptive Watermarking Based on Local Spatial\n  Disorder Analysis", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing use of the internet and the ease of exchange of\nmultimedia content, the protection of ownership rights has become a significant\nconcern. Watermarking is an efficient means for this purpose. In many\napplications, real-time watermarking is required, which demands hardware\nimplementation of low complexity and robust algorithm. In this paper, an\nadaptive watermarking is presented, which uses embedding in different\nbit-planes to achieve transparency and robustness. Local disorder of pixels is\nanalyzed to control the strength of the watermark. A new low complexity method\nfor disorder analysis is proposed, and its hardware implantation is presented.\nAn embedding method is proposed, which causes lower degradation in the\nwatermarked image. Also, the performance of proposed watermarking architecture\nis improved by a pipe-line structure and is tested on an FPGA device. Results\nshow that the algorithm produces transparent and robust watermarked images. The\nsynthesis report from FPGA implementation illustrates a low complexity hardware\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:23:14 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Hajabdolahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Shirani", "Shahram", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2005.05535", "submitter": "Liu Kunlin", "authors": "Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa\n  Marangonda, Chris Um\\'e, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian\n  Jiang, Sheng Zhang, Pingyu Wu, Bo Zhou, Weiming Zhang", "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 03:26:55 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 09:00:36 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 14:40:07 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 06:33:52 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 07:07:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Perov", "Ivan", ""], ["Gao", "Daiheng", ""], ["Chervoniy", "Nikolay", ""], ["Liu", "Kunlin", ""], ["Marangonda", "Sugasa", ""], ["Um\u00e9", "Chris", ""], ["Dpfks", "Mr.", ""], ["Facenheim", "Carl Shift", ""], ["RP", "Luis", ""], ["Jiang", "Jian", ""], ["Zhang", "Sheng", ""], ["Wu", "Pingyu", ""], ["Zhou", "Bo", ""], ["Zhang", "Weiming", ""]]}, {"id": "2005.06047", "submitter": "Yixiong Zou", "authors": "Yixiong Zou, Shanghang Zhang, Ke Chen, Yonghong Tian, Yaowei Wang,\n  Jos\\'e M. F. Moura", "title": "Compositional Few-Shot Recognition with Primitive Discovery and\n  Enhancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) aims at recognizing novel classes given only few\ntraining samples, which still remains a great challenge for deep learning.\nHowever, humans can easily recognize novel classes with only few samples. A key\ncomponent of such ability is the compositional recognition that human can\nperform, which has been well studied in cognitive science but is not well\nexplored in FSL. Inspired by such capability of humans, to imitate humans'\nability of learning visual primitives and composing primitives to recognize\nnovel classes, we propose an approach to FSL to learn a feature representation\ncomposed of important primitives, which is jointly trained with two parts, i.e.\nprimitive discovery and primitive enhancing. In primitive discovery, we focus\non learning primitives related to object parts by self-supervision from the\norder of image splits, avoiding extra laborious annotations and alleviating the\neffect of semantic gaps. In primitive enhancing, inspired by current studies on\nthe interpretability of deep networks, we provide our composition view for the\nFSL baseline model. To modify this model for effective composition, inspired by\nboth mathematical deduction and biological studies (the Hebbian Learning rule\nand the Winner-Take-All mechanism), we propose a soft composition mechanism by\nenlarging the activation of important primitives while reducing that of others,\nso as to enhance the influence of important primitives and better utilize these\nprimitives to compose novel classes. Extensive experiments on public benchmarks\nare conducted on both the few-shot image classification and video recognition\ntasks. Our method achieves the state-of-the-art performance on all these\ndatasets and shows better interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:01:25 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:02:01 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 00:03:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Zou", "Yixiong", ""], ["Zhang", "Shanghang", ""], ["Chen", "Ke", ""], ["Tian", "Yonghong", ""], ["Wang", "Yaowei", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "2005.06944", "submitter": "Hannes Fassold", "authors": "Hannes Fassold, Antonis Karakottas, Dorothea Tsatsou, Dimitrios\n  Zarpalas, Barnabas Takacs, Christian Fuhrhop, Angelo Manfredi, Nicolas Patz,\n  Simona Tonoli, Iana Dulskaia", "title": "The Hyper360 toolset for enriched 360$^\\circ$ video", "comments": "accepted for Collaborative Research and Innovation Projects track at\n  ICME 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360$^\\circ$ video is a novel media format, rapidly becoming adopted in media\nproduction and consumption as part of todays ongoing virtual reality\nrevolution. Due to its novelty, there is a lack of tools for producing highly\nengaging 360$^\\circ$ video for consumption on a multitude of platforms. In this\nwork, we describe the work done so far in the Hyper360 project on tools for\n360$^\\circ$ video. Furthermore, the first pilots which have been produced with\nthe Hyper360 tools are presented.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 13:50:30 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Fassold", "Hannes", ""], ["Karakottas", "Antonis", ""], ["Tsatsou", "Dorothea", ""], ["Zarpalas", "Dimitrios", ""], ["Takacs", "Barnabas", ""], ["Fuhrhop", "Christian", ""], ["Manfredi", "Angelo", ""], ["Patz", "Nicolas", ""], ["Tonoli", "Simona", ""], ["Dulskaia", "Iana", ""]]}, {"id": "2005.07074", "submitter": "Joon Son Chung", "authors": "Soo-Whan Chung, Soyeon Choe, Joon Son Chung, Hong-Goo Kang", "title": "FaceFilter: Audio-visual speech separation using still images", "comments": "Under submission as a conference paper. Video examples:\n  https://youtu.be/ku9xoLh62E", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1065", "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to separate a target speaker's speech from a\nmixture of two speakers using a deep audio-visual speech separation network.\nUnlike previous works that used lip movement on video clips or pre-enrolled\nspeaker information as an auxiliary conditional feature, we use a single face\nimage of the target speaker. In this task, the conditional feature is obtained\nfrom facial appearance in cross-modal biometric task, where audio and visual\nidentity representations are shared in latent space. Learnt identities from\nfacial images enforce the network to isolate matched speakers and extract the\nvoices from mixed speech. It solves the permutation problem caused by swapped\nchannel outputs, frequently occurred in speech separation tasks. The proposed\nmethod is far more practical than video-based speech separation since user\nprofile images are readily available on many platforms. Also, unlike\nspeaker-aware separation methods, it is applicable on separation with unseen\nspeakers who have never been enrolled before. We show strong qualitative and\nquantitative results on challenging real-world examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 15:42:31 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Soo-Whan", ""], ["Choe", "Soyeon", ""], ["Chung", "Joon Son", ""], ["Kang", "Hong-Goo", ""]]}, {"id": "2005.07356", "submitter": "Mohammed Belkhatir", "authors": "B. Tahayna, M. Belkhatir", "title": "Near-duplicate video detection featuring coupled temporal and perceptual\n  visual structures and logical inference based matching", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipm.2011.03.003", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper an architecture for near-duplicate video detection\nbased on: (i) index and query signature based structures integrating temporal\nand perceptual visual features and (ii) a matching framework computing the\nlogical inference between index and query documents. As far as indexing is\nconcerned, instead of concatenating low-level visual features in\nhigh-dimensional spaces which results in curse of dimensionality and redundancy\nissues, we adopt a perceptual symbolic representation based on color and\ntexture concepts. For matching, we propose to instantiate a retrieval model\nbased on logical inference through the coupling of an N-gram sliding window\nprocess and theoretically-sound lattice-based structures. The techniques we\ncover are robust and insensitive to general video editing and/or degradation,\nmaking it ideal for re-broadcasted video search. Experiments are carried out on\nlarge quantities of video data collected from the TRECVID 02, 03 and 04\ncollections and real-world video broadcasts recorded from two German TV\nstations. An empirical comparison over two state-of-the-art dynamic programming\ntechniques is encouraging and demonstrates the advantage and feasibility of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 04:45:52 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Tahayna", "B.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.07384", "submitter": "Xinyu Huang", "authors": "Xinyu Huang, Lijun He, Liejun Wang, Fan Li", "title": "Towards 5G: Joint Optimization of Video Segment Cache, Transcoding and\n  Resource Allocation for Adaptive Video Streaming in a Muti-access Edge\n  Computing Network", "comments": "13 pages, 9 figures, submitted to IEEE Transactions on Vehicular\n  Technology (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cache and transcoding of the multi-access edge computing (MEC) server and\nwireless resource allocation in eNodeB interact and determine the quality of\nexperience (QoE) of dynamic adaptive streaming over HTTP (DASH) clients in MEC\nnetworks. However, the relationship among the three factors has not been\nexplored, which has led to limited improvement in clients' QoE. Therefore, we\npropose a joint optimization framework of video segment cache and transcoding\nin MEC servers and resource allocation to improve the QoE of DASH clients.\nBased on the established framework, we develop a MEC cache management mechanism\nthat consists of the MEC cache partition, video segment deletion, and MEC cache\nspace transfer. Then, a joint optimization algorithm that combines video\nsegment cache and transcoding in the MEC server and resource allocation is\nproposed. In the algorithm, the clients' channel state and the playback status\nand cooperation among MEC servers are employed to estimate the client's\npriority, video segment presentation switch and continuous playback time.\nConsidering the above four factors, we develop a utility function model of\nclients' QoE. Then, we formulate a mixed-integer nonlinear programming\nmathematical model to maximize the total utility of DASH clients, where the\nvideo segment cache and transcoding strategy and resource allocation strategy\nare jointly optimized. To solve this problem, we propose a low-complexity\nheuristic algorithm that decomposes the original problem into multiple\nsubproblems. The simulation results show that our proposed algorithms\nefficiently improve client's throughput, received video quality and hit ratio\nof video segments while decreasing the playback rebuffering time, video segment\npresentation switch and system backhaul traffic.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:10:58 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Huang", "Xinyu", ""], ["He", "Lijun", ""], ["Wang", "Liejun", ""], ["Li", "Fan", ""]]}, {"id": "2005.07925", "submitter": "Lee Prangnell", "authors": "Lee Prangnell", "title": "Spatiotemporal Adaptive Quantization for Video Compression Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JCT-VC HEVC HM 16 includes a Coding Unit (CU) level adaptive Quantization\nParameter (QP) technique named AdaptiveQP. It is designed to perceptually\nadjust the QP in Y, Cb and Cr Coding Blocks (CBs) based only on the variance of\nsamples in a luma CB. In this paper, we propose an adaptive quantisation\ntechnique that consists of two contributions. The first contribution relates to\naccounting for the variance of chroma samples, in addition to luma samples, in\na CU. The second contribution relates to accounting for CU temporal information\nas well as CU spatial information. Moreover, we integrate into our method a\nlambda refined QP technique to reduce complexity associated multiple QP\noptimizations in the Rate Distortion Optimization process. We evaluate the\nproposed technique on 4:4:4, 4:2:2, 4:2:0 and 4:0:0 YCbCr test sequences, for\nwhich we quantify the results using the Bjontegaard Delta Rate (BD-Rate)\nmetric. Our method achieves a maximum BD-Rate reduction of 23.1% (Y), 26.7%\n(Cr) and 25.2% (Cb). Furthermore, a maximum encoding time reduction of 4.4% is\nachieved.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 09:58:41 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Prangnell", "Lee", ""]]}, {"id": "2005.07928", "submitter": "Lee Prangnell", "authors": "Lee Prangnell and Victor Sanchez", "title": "Spatiotemporal Adaptive Quantization for the Perceptual Video Coding of\n  RGB 4:4:4 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the spectral sensitivity phenomenon of the Human Visual System (HVS),\nthe color channels of raw RGB 4:4:4 sequences contain significant psychovisual\nredundancies; these redundancies can be perceptually quantized. The default\nquantization systems in the HEVC standard are known as Uniform Reconstruction\nQuantization (URQ) and Rate Distortion Optimized Quantization (RDOQ); URQ and\nRDOQ are not perceptually optimized for the coding of RGB 4:4:4 video data. In\nthis paper, we propose a novel spatiotemporal perceptual quantization technique\nnamed SPAQ. With application for RGB 4:4:4 video data, SPAQ exploits HVS\nspectral sensitivity-related color masking in addition to spatial masking and\ntemporal masking; SPAQ operates at the Coding Block (CB) level and the\nPrediction Unit (PU) level. The proposed technique perceptually adjusts the\nQuantization Step Size (QStep) at the CB level if high variance spatial data in\nG, B and R CBs is detected and also if high motion vector magnitudes in PUs are\ndetected. Compared with anchor 1 (HEVC HM 16.17 RExt), SPAQ considerably\nreduces bitrates with a maximum reduction of approximately 80%. The Mean\nOpinion Score (MOS) in the subjective evaluations, in addition to the SSIM\nscores, show that SPAQ successfully achieves perceptually lossless compression\ncompared with anchors.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:02:01 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Prangnell", "Lee", ""], ["Sanchez", "Victor", ""]]}, {"id": "2005.08210", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u", "title": "Multi-Entity and Multi-Enrollment Key Agreement with Correlated Noise", "comments": "To appear in the IEEE Transactions on Information Forensics and\n  Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic model for key agreement with a remote (or hidden) source is extended\nto a multi-user model with joint secrecy and privacy constraints over all\nentities that do not trust each other after key agreement. Multiple entities\nusing different measurements of the same source through broadcast channels\n(BCs) to agree on mutually-independent local secret keys are considered. Our\nmodel is the proper multi-user extension of the basic model since the encoder\nand decoder pairs are not assumed to trust other pairs after key agreement,\nunlike assumed in the literature. Strong secrecy constraints imposed on all\nsecret keys jointly, which is more stringent than separate secrecy leakage\nconstraints for each secret key considered in the literature, are satisfied.\nInner bounds for maximum key rate, and minimum privacy-leakage and\ndatabase-storage rates are proposed for any finite number of entities. Inner\nand outer bounds for degraded and less-noisy BCs are given to illustrate cases\nwith strong privacy. A multi-enrollment model that is used for common physical\nunclonable functions is also considered to establish inner and outer bounds for\nkey-leakage-storage regions that differ only in the Markov chains imposed. For\nthis special case, the encoder and decoder measurement channels have the same\nchannel transition matrix and secrecy leakage is measured for each secret key\nseparately. We illustrate cases for which it is useful to have multiple\nenrollments as compared to a single enrollment and vice versa.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 10:44:25 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 10:32:24 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 09:53:42 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 15:31:48 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""]]}, {"id": "2005.08448", "submitter": "Shuang Xu", "authors": "Shuang Xu, Zixiang Zhao, Yicheng Wang, Chunxia Zhang, Junmin Liu,\n  Jiangshe Zhang", "title": "Deep Convolutional Sparse Coding Networks for Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion is a significant problem in many fields including digital\nphotography, computational imaging and remote sensing, to name but a few.\nRecently, deep learning has emerged as an important tool for image fusion. This\npaper presents three deep convolutional sparse coding (CSC) networks for three\nkinds of image fusion tasks (i.e., infrared and visible image fusion,\nmulti-exposure image fusion, and multi-modal image fusion). The CSC model and\nthe iterative shrinkage and thresholding algorithm are generalized into\ndictionary convolution units. As a result, all hyper-parameters are learned\nfrom data. Our extensive experiments and comprehensive comparisons reveal the\nsuperiority of the proposed networks with regard to quantitative evaluation and\nvisual inspection.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 04:12:01 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Xu", "Shuang", ""], ["Zhao", "Zixiang", ""], ["Wang", "Yicheng", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2005.08449", "submitter": "Di Hu", "authors": "Di Hu, Xuhong Li, Lichao Mou, Pu Jin, Dong Chen, Liping Jing,\n  Xiaoxiang Zhu, Dejing Dou", "title": "Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial scene recognition is a fundamental task in remote sensing and has\nrecently received increased interest. While the visual information from\noverhead images with powerful models and efficient algorithms yields\nconsiderable performance on scene recognition, it still suffers from the\nvariation of ground objects, lighting conditions etc. Inspired by the\nmulti-channel perception theory in cognition science, in this paper, for\nimproving the performance on the aerial scene recognition, we explore a novel\naudiovisual aerial scene recognition task using both images and sounds as\ninput. Based on an observation that some specific sound events are more likely\nto be heard at a given geographic location, we propose to exploit the knowledge\nfrom the sound events to improve the performance on the aerial scene\nrecognition. For this purpose, we have constructed a new dataset named AuDio\nVisual Aerial sceNe reCognition datasEt (ADVANCE). With the help of this\ndataset, we evaluate three proposed approaches for transferring the sound event\nknowledge to the aerial scene recognition task in a multimodal learning\nframework, and show the benefit of exploiting the audio information for the\naerial scene recognition. The source code is publicly available for\nreproducibility purposes.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 04:14:16 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 03:33:17 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hu", "Di", ""], ["Li", "Xuhong", ""], ["Mou", "Lichao", ""], ["Jin", "Pu", ""], ["Chen", "Dong", ""], ["Jing", "Liping", ""], ["Zhu", "Xiaoxiang", ""], ["Dou", "Dejing", ""]]}, {"id": "2005.08527", "submitter": "Yang Li", "authors": "Yang Li, Shengbin Meng, Xinfeng Zhang, Shiqi Wang, Yue Wang, Siwei Ma", "title": "User-generated Video Quality Assessment: A Subjective and Objective\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have observed an exponential increase of user-generated content\n(UGC) videos. The distinguished characteristic of UGC videos originates from\nthe video production and delivery chain, as they are usually acquired and\nprocessed by non-professional users before uploading to the hosting platforms\nfor sharing. As such, these videos usually undergo multiple distortion stages\nthat may affect visual quality before ultimately being viewed. Inspired by the\nincreasing consensus that the optimization of the video coding and processing\nshall be fully driven by the perceptual quality, in this paper, we propose to\nstudy the quality of the UGC videos from both objective and subjective\nperspectives. We first construct a UGC video quality assessment (VQA) database,\naiming to provide useful guidance for the UGC video coding and processing in\nthe hosting platform. The database contains source UGC videos uploaded to the\nplatform and their transcoded versions that are ultimately enjoyed by\nend-users, along with their subjective scores. Furthermore, we develop an\nobjective quality assessment algorithm that automatically evaluates the quality\nof the transcoded videos based on the corrupted reference, which is in\naccordance with the application scenarios of UGC video sharing in the hosting\nplatforms. The information from the corrupted reference is well leveraged and\nthe quality is predicted based on the inferred quality maps with deep neural\nnetworks (DNN). Experimental results show that the proposed method yields\nsuperior performance. Both subjective and objective evaluations of the UGC\nvideos also shed lights on the design of perceptual UGC video coding.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:36:11 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Yang", ""], ["Meng", "Shengbin", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shiqi", ""], ["Wang", "Yue", ""], ["Ma", "Siwei", ""]]}, {"id": "2005.08606", "submitter": "You Jin Kim", "authors": "You Jin Kim, Hee Soo Heo, Soo-Whan Chung and Bong-Jin Lee", "title": "End-to-End Lip Synchronisation Based on Pattern Classification", "comments": "slt 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to synchronise audio and video of a talking face\nusing deep neural network models. Existing works have trained networks on proxy\ntasks such as cross-modal similarity learning, and then computed similarities\nbetween audio and video frames using a sliding window approach. While these\nmethods demonstrate satisfactory performance, the networks are not trained\ndirectly on the task. To this end, we propose an end-to-end trained network\nthat can directly predict the offset between an audio stream and the\ncorresponding video stream. The similarity matrix between the two modalities is\nfirst computed from the features, then the inference of the offset can be\nconsidered to be a pattern recognition problem where the matrix is considered\nequivalent to an image. The feature extractor and the classifier are trained\njointly. We demonstrate that the proposed approach outperforms the previous\nwork by a large margin on LRS2 and LRS3 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:42:32 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 06:55:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kim", "You Jin", ""], ["Heo", "Hee Soo", ""], ["Chung", "Soo-Whan", ""], ["Lee", "Bong-Jin", ""]]}, {"id": "2005.08752", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, He Sun, Xianming Liu, and Jiayi Ma", "title": "Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral\n  Imagery", "comments": "Accepted for publication at IEEE Transactions on Computational\n  Imaging", "journal-ref": null, "doi": "10.1109/TCI.2020.2996075", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, single gray/RGB image super-resolution reconstruction task has been\nextensively studied and made significant progress by leveraging the advanced\nmachine learning techniques based on deep convolutional neural networks\n(DCNNs). However, there has been limited technical development focusing on\nsingle hyperspectral image super-resolution due to the high-dimensional and\ncomplex spectral patterns in hyperspectral image. In this paper, we make a step\nforward by investigating how to adapt state-of-the-art residual learning based\nsingle gray/RGB image super-resolution approaches for computationally efficient\nsingle hyperspectral image super-resolution, referred as SSPSR. Specifically,\nwe introduce a spatial-spectral prior network (SSPN) to fully exploit the\nspatial information and the correlation between the spectra of the\nhyperspectral data. Considering that the hyperspectral training samples are\nscarce and the spectral dimension of hyperspectral image data is very high, it\nis nontrivial to train a stable and effective deep network. Therefore, a group\nconvolution (with shared network parameters) and progressive upsampling\nframework is proposed. This will not only alleviate the difficulty in feature\nextraction due to high-dimension of the hyperspectral data, but also make the\ntraining process more stable. To exploit the spatial and spectral prior, we\ndesign a spatial-spectral block (SSB), which consists of a spatial residual\nmodule and a spectral attention residual module. Experimental results on some\nhyperspectral images demonstrate that the proposed SSPSR method enhances the\ndetails of the recovered high-resolution hyperspectral images, and outperforms\nstate-of-the-arts. The source code is available at\n\\url{https://github.com/junjun-jiang/SSPSR\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:25:50 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 03:26:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Jiang", "Junjun", ""], ["Sun", "He", ""], ["Liu", "Xianming", ""], ["Ma", "Jiayi", ""]]}, {"id": "2005.09100", "submitter": "Kostadin Kushlev", "authors": "Kostadin Kushlev and Matthew R Leitao", "title": "The Effects of Smartphones on Well-Being: Theoretical Integration and\n  Research Agenda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.IT cs.MM econ.GN math.IT q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smartphones become ever more integrated in peoples lives, a burgeoning new\narea of research has emerged on their well-being effects. We propose that\ndisparate strands of research and apparently contradictory findings can be\nintegrated under three basic hypotheses, positing that smartphones influence\nwell-being by (1) replacing other activities (displacement hypothesis), (2)\ninterfering with concurrent activities (interference hypothesis), and (3)\naffording access to information and activities that would otherwise be\nunavailable (complementarity hypothesis). Using this framework, we highlight\nmethodological issues and go beyond net effects to examine how and when phones\nboost versus hurt well-being. We examine both psychological and contextual\nmediators and moderators of the effects, thus outlining an agenda for future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:25:51 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kushlev", "Kostadin", ""], ["Leitao", "Matthew R", ""]]}, {"id": "2005.09199", "submitter": "Mansoor Ahmed-Rengers", "authors": "Mansoor Ahmed-Rengers", "title": "FrameProv: Towards End-To-End Video Provenance", "comments": null, "journal-ref": "New Security Paradigms Workshop, 2019", "doi": "10.1145/3368860.3368866", "report-no": null, "categories": "cs.CR cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video feeds are often deliberately used as evidence, as in the case of CCTV\nfootage; but more often than not, the existence of footage of a supposed event\nis perceived as proof of fact in the eyes of the public at large. This reliance\nrepresents a societal vulnerability given the existence of easy-to-use editing\ntools and means to fabricate entire video feeds using machine learning. And, as\nthe recent barrage of fake news and fake porn videos have shown, this isn't\nmerely an academic concern, it is actively been exploited. I posit that this\nexploitation is only going to get more insidious. In this position paper, I\nintroduce a long term project that aims to mitigate some of the most egregious\nforms of manipulation by embedding trustworthy components in the video\ntransmission chain. Unlike earlier works, I am not aiming to do tamper\ndetection or other forms of forensics -- approaches I think are bound to fail\nin the face of the reality of necessary editing and compression -- instead, the\naim here is to provide a way for the video publisher to prove the integrity of\nthe video feed as well as make explicit any edits they may have performed. To\ndo this, I present a novel data structure, a video-edit specification language\nand supporting infrastructure that provides end-to-end video provenance, from\nthe camera sensor to the viewer. I have implemented a prototype of this system\nand am in talks with journalists and video editors to discuss the best ways\nforward with introducing this idea to the mainstream.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 03:52:21 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ahmed-Rengers", "Mansoor", ""]]}, {"id": "2005.09302", "submitter": "Xiaowei Tang", "authors": "Xiao-Wei Tang, Xin-Lin Huang, Fei Hu, Qingjiang Shi", "title": "Human-Perception-Oriented Pseudo Analog Video Transmissions with Deep\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Vehicular Technology, 2020", "doi": "10.1109/TVT.2020.3003478", "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pseudo analog transmission has gained increasing attentions due to\nits ability to alleviate the cliff effect in video multicast scenarios. The\nexisting pseudo analog systems are sorely optimized under the minimum mean\nsquared error criterion without taking the perceptual video quality into\nconsideration. In this paper, we propose a human-perception-based pseudo analog\nvideo transmission system named ROIC-Cast, which aims to intelligently enhance\nthe transmission quality of the region-of-interest (ROI) parts. Firstly, the\nclassic deep learning based saliency detection algorithm is adopted to\ndecompose the continuous video sequences into ROI and non-ROI blocks. Secondly,\nan effective compression method is used to reduce the data amount of side\ninformation generated by the ROI extraction module. Then, the power allocation\nscheme is formulated as a convex problem, and the optimal transmission power\nfor both ROI and non-ROI blocks is derived in a closed form. Finally, the\nsimulations are conducted to validate the proposed system by comparing with a\nfew of existing systems, e.g., KMV-Cast, SoftCast, and DAC-RAN. The proposed\nROIC-Cast can achieve over 4.1dB peak signal- to-noise ratio gains of ROI\ncompared with other systems, given the channel signal-to-noise ratio as -5dB,\n0dB, 5dB, and 10dB, respectively. This significant performance improvement is\ndue to the automatic ROI extraction, high-efficiency data compression as well\nas adaptive power allocation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:14:36 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Tang", "Xiao-Wei", ""], ["Huang", "Xin-Lin", ""], ["Hu", "Fei", ""], ["Shi", "Qingjiang", ""]]}, {"id": "2005.09309", "submitter": "Randy Frans Fela", "authors": "Randy Frans Fela, Nick Zacharov, S{\\o}ren Forchhammer", "title": "Towards a Perceived Audiovisual Quality Model for Immersive Content", "comments": "2020 Twelfth International Conference on Quality of Multimedia\n  Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the quality of multimedia content focusing on 360 video\nand ambisonic spatial audio reproduced using a head-mounted display and a\nmultichannel loudspeaker setup. Encoding parameters following basic video\nquality test conditions for 360 videos were selected and a low-bitrate codec\nwas used for the audio encoder. Three subjective experiments were performed for\nthe audio, video, and audiovisual respectively. Peak signal-to-noise ratio\n(PSNR) and its variants for 360 videos were computed to obtain objective\nquality metrics and subsequently correlated with the subjective video scores.\nThis study shows that a Cross-Format SPSNR-NN has a slightly higher linear and\nmonotonic correlation over all video sequences. Based on the audiovisual model,\na power model shows a highest correlation between test data and predicted\nscores. We concluded that to enable the development of superior predictive\nmodel, a high quality, critical, synchronized audiovisual database is required.\nFurthermore, comprehensive assessor training may be beneficial prior to the\ntesting to improve the assessors' discrimination ability particularly with\nrespect to multichannel audio reproduction. In order to further improve the\nperformance of audiovisual quality models for immersive content, in addition to\ndeveloping broader and critical audiovisual databases, the subjective testing\nmethodology needs to be evolved to provide greater resolution and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:24:28 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Fela", "Randy Frans", ""], ["Zacharov", "Nick", ""], ["Forchhammer", "S\u00f8ren", ""]]}, {"id": "2005.09639", "submitter": "Mohammed Belkhatir", "authors": "F. Fauzi, H. J. Long, M. Belkhatir", "title": "Webpage Segmentation for Extracting Images and Their Surrounding\n  Contextual Information", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.02156", "journal-ref": null, "doi": "10.1145/1631272.1631379", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web images come in hand with valuable contextual information. Although this\ninformation has long been mined for various uses such as image annotation,\nclustering of images, inference of image semantic content, etc., insufficient\nattention has been given to address issues in mining this contextual\ninformation. In this paper, we propose a webpage segmentation algorithm\ntargeting the extraction of web images and their contextual information based\non their characteristics as they appear on webpages. We conducted a user study\nto obtain a human-labeled dataset to validate the effectiveness of our method\nand experiments demonstrated that our method can achieve better results\ncompared to an existing segmentation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:00:03 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Fauzi", "F.", ""], ["Long", "H. J.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.09984", "submitter": "Sara Mandelli", "authors": "Sara Mandelli, Fabrizio Argenti, Paolo Bestagini, Massimo Iuliani,\n  Alessandro Piva, Stefano Tubaro", "title": "A Modified Fourier-Mellin Approach for Source Device Identification on\n  Stabilized Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To decide whether a digital video has been captured by a given device,\nmultimedia forensic tools usually exploit characteristic noise traces left by\nthe camera sensor on the acquired frames. This analysis requires that the noise\npattern characterizing the camera and the noise pattern extracted from video\nframes under analysis are geometrically aligned. However, in many practical\nscenarios this does not occur, thus a re-alignment or synchronization has to be\nperformed. Current solutions often require time consuming search of the\nrealignment transformation parameters. In this paper, we propose to overcome\nthis limitation by searching scaling and rotation parameters in the frequency\ndomain. The proposed algorithm tested on real videos from a well-known\nstate-of-the-art dataset shows promising results.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 12:06:40 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Mandelli", "Sara", ""], ["Argenti", "Fabrizio", ""], ["Bestagini", "Paolo", ""], ["Iuliani", "Massimo", ""], ["Piva", "Alessandro", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2005.10161", "submitter": "Mu Mu", "authors": "Mu Mu, Murtada Dohan, Alison Goodyear, Gary Hill, Cleyon Johns, and\n  Andreas Mauthe", "title": "User Attention and Behaviour in Virtual Reality Art Encounter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of consumer virtual reality (VR) headsets and creative\ntools, content creators have started to experiment with new forms of\ninteractive audience experience using immersive media. Understanding user\nattention and behaviours in virtual environment can greatly inform creative\nprocesses in VR. We developed an abstract VR painting and an experimentation\nsystem to study audience encounters through eye gaze and movement tracking. The\ndata from a user experiment with 35 participants reveal a range of user\nactivity patterns in art exploration. Deep learning models are used to study\nthe connections between behavioural data and audience background. New\nintegrated methods to visualise user attention as part of the artwork are also\ndeveloped as a feedback loop to the content creator.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 16:09:57 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Mu", "Mu", ""], ["Dohan", "Murtada", ""], ["Goodyear", "Alison", ""], ["Hill", "Gary", ""], ["Johns", "Cleyon", ""], ["Mauthe", "Andreas", ""]]}, {"id": "2005.10322", "submitter": "Felice Antonio Merra", "authors": "Yashar Deldjoo and Tommaso Di Noia and Felice Antonio Merra", "title": "A survey on Adversarial Recommender Systems: from Attack/Defense\n  strategies to Generative Adversarial Networks", "comments": "37 pages, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent-factor models (LFM) based on collaborative filtering (CF), such as\nmatrix factorization (MF) and deep CF methods, are widely used in modern\nrecommender systems (RS) due to their excellent performance and recommendation\naccuracy. However, success has been accompanied with a major new arising\nchallenge: many applications of machine learning (ML) are adversarial in\nnature. In recent years, it has been shown that these methods are vulnerable to\nadversarial examples, i.e., subtle but non-random perturbations designed to\nforce recommendation models to produce erroneous outputs.\n  The goal of this survey is two-fold: (i) to present recent advances on\nadversarial machine learning (AML) for the security of RS (i.e., attacking and\ndefense recommendation models), (ii) to show another successful application of\nAML in generative adversarial networks (GANs) for generative applications,\nthanks to their ability for learning (high-dimensional) data distributions. In\nthis survey, we provide an exhaustive literature review of 74 articles\npublished in major RS and ML journals and conferences. This review serves as a\nreference for the RS community, working on the security of RS or on generative\nmodels using GANs to improve their quality.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:17:11 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 10:48:34 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "2005.10777", "submitter": "Jing Huo", "authors": "Jing Huo, Shiyin Jin, Wenbin Li, Jing Wu, Yu-Kun Lai, Yinghuan Shi,\n  Yang Gao", "title": "Manifold Alignment for Semantically Aligned Style Transfer", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a content image and a style image, the goal of style transfer is to\nsynthesize an output image by transferring the target style to the content\nimage. Currently, most of the methods address the problem with global style\ntransfer, assuming styles can be represented by global statistics, such as Gram\nmatrices or covariance matrices. In this paper, we make a different assumption\nthat local semantically aligned (or similar) regions between the content and\nstyle images should share similar style patterns. Based on this assumption,\ncontent features and style features are seen as two sets of manifolds and a\nmanifold alignment based style transfer (MAST) method is proposed. MAST is a\nsubspace learning method which learns a common subspace of the content and\nstyle features. In the common subspace, content and style features with larger\nfeature similarity or the same semantic meaning are forced to be close. The\nlearned projection matrices are added with orthogonality constraints so that\nthe mapping can be bidirectional, which allows us to project the content\nfeatures into the common subspace, and then into the original style space. By\nusing a pre-trained decoder, promising stylized images are obtained. The method\nis further extended to allow users to specify corresponding semantic regions\nbetween content and style images or using semantic segmentation maps as\nguidance. Extensive experiments show the proposed MAST achieves appealing\nresults in style transfer.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:52:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Huo", "Jing", ""], ["Jin", "Shiyin", ""], ["Li", "Wenbin", ""], ["Wu", "Jing", ""], ["Lai", "Yu-Kun", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2005.10801", "submitter": "Farhad Pakdaman", "authors": "Farhad Pakdaman, Mohammad Ali Adelimanesh, Moncef Gabbouj, Mahmoud\n  Reza Hashemi", "title": "Complexity Analysis Of Next-Generation VVC Encoding and Decoding", "comments": "IEEE ICIP 2020", "journal-ref": "Proceedings of International Conference on Image Processing\n  (ICIP), (2020) 3134-3138", "doi": "10.1109/ICIP40778.2020.9190983", "report-no": null, "categories": "cs.MM cs.CC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the next generation video compression standard, Versatile Video Coding\n(VVC), provides a superior compression efficiency, its computational complexity\ndramatically increases. This paper thoroughly analyzes this complexity for both\nencoder and decoder of VVC Test Model 6, by quantifying the complexity\nbreak-down for each coding tool and measuring the complexity and memory\nrequirements for VVC encoding/decoding. These extensive analyses are performed\nfor six video sequences of 720p, 1080p, and 2160p, under Low-Delay (LD),\nRandom-Access (RA), and All-Intra (AI) conditions (a total of 320\nencoding/decoding). Results indicate that the VVC encoder and decoder are 5x\nand 1.5x more complex compared to HEVC in LD, and 31x and 1.8x in AI,\nrespectively. Detailed analysis of coding tools reveals that in LD on average,\nmotion estimation tools with 53%, transformation and quantization with 22%, and\nentropy coding with 7% dominate the encoding complexity. In decoding, loop\nfilters with 30%, motion compensation with 20%, and entropy decoding with 16%,\nare the most complex modules. Moreover, the required memory bandwidth for VVC\nencoding/decoding are measured through memory profiling, which are 30x and 3x\nof HEVC. The reported results and insights are a guide for future research and\nimplementations of energy-efficient VVC encoder/decoder.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:30:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Pakdaman", "Farhad", ""], ["Adelimanesh", "Mohammad Ali", ""], ["Gabbouj", "Moncef", ""], ["Hashemi", "Mahmoud Reza", ""]]}, {"id": "2005.11043", "submitter": "Hongyu Li", "authors": "Hongyu Li, Xiaogang Huang, Zhihui Fu, and Xiaolin Li", "title": "Arbitrary-sized Image Training and Residual Kernel Learning: Towards\n  Image Fraud Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving original noise residuals in images are critical to image fraud\nidentification. Since the resizing operation during deep learning will damage\nthe microstructures of image noise residuals, we propose a framework for\ndirectly training images of original input scales without resizing. Our\narbitrary-sized image training method mainly depends on the pseudo-batch\ngradient descent (PBGD), which bridges the gap between the input batch and the\nupdate batch to assure that model updates can normally run for arbitrary-sized\nimages.\n  In addition, a 3-phase alternate training strategy is designed to learn\noptimal residual kernels for image fraud identification. With the learnt\nresidual kernels and PBGD, the proposed framework achieved the state-of-the-art\nresults in image fraud identification, especially for images with small\ntampered regions or unseen images with different tampering distributions.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 07:57:24 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Li", "Hongyu", ""], ["Huang", "Xiaogang", ""], ["Fu", "Zhihui", ""], ["Li", "Xiaolin", ""]]}, {"id": "2005.11735", "submitter": "Marcin Plata", "authors": "Marcin Plata and Piotr Syga", "title": "Robust Spatial-spread Deep Neural Image Watermarking", "comments": "The article was accepted on TrustCom 2020: The 19th IEEE\n  International Conference on Trust, Security and Privacy in Computing and\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking is an operation of embedding an information into an image in a\nway that allows to identify ownership of the image despite applying some\ndistortions on it. In this paper, we presented a novel end-to-end solution for\nembedding and recovering the watermark in the digital image using convolutional\nneural networks. The method is based on spreading the message over the spatial\ndomain of the image, hence reducing the \"local bits per pixel\" capacity. To\nobtain the model we used adversarial training and applied noiser layers between\nthe encoder and the decoder. Moreover, we broadened the spectrum of typically\nconsidered attacks on the watermark and by grouping the attacks according to\ntheir scope, we achieved high general robustness, most notably against JPEG\ncompression, Gaussian blurring, subsampling or resizing. To help us in the\nmodels training we also proposed a precise differentiable approximation of\nJPEG.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 12:51:25 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 13:14:42 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Plata", "Marcin", ""], ["Syga", "Piotr", ""]]}, {"id": "2005.11742", "submitter": "Yu Zeng", "authors": "Yu Zeng, Zhe Lin, Jimei Yang, Jianming Zhang, Eli Shechtman, Huchuan\n  Lu", "title": "High-Resolution Image Inpainting with Iterative Confidence Feedback and\n  Guided Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image inpainting methods often produce artifacts when dealing with\nlarge holes in real applications. To address this challenge, we propose an\niterative inpainting method with a feedback mechanism. Specifically, we\nintroduce a deep generative model which not only outputs an inpainting result\nbut also a corresponding confidence map. Using this map as feedback, it\nprogressively fills the hole by trusting only high-confidence pixels inside the\nhole at each iteration and focuses on the remaining pixels in the next\niteration. As it reuses partial predictions from the previous iterations as\nknown pixels, this process gradually improves the result. In addition, we\npropose a guided upsampling network to enable generation of high-resolution\ninpainting results. We achieve this by extending the Contextual Attention\nmodule to borrow high-resolution feature patches in the input image.\nFurthermore, to mimic real object removal scenarios, we collect a large object\nmask dataset and synthesize more realistic training data that better simulates\nuser inputs. Experiments show that our method significantly outperforms\nexisting methods in both quantitative and qualitative evaluations. More results\nand Web APP are available at https://zengxianyu.github.io/iic.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 13:23:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 05:52:30 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zeng", "Yu", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Zhang", "Jianming", ""], ["Shechtman", "Eli", ""], ["Lu", "Huchuan", ""]]}, {"id": "2005.11780", "submitter": "Chen Lv", "authors": "Zhongxu Hu, Yang Xing, Chen Lv, Peng Hang, Jie Liu", "title": "Deep Convolutional Neural Network-based Bernoulli Heatmap for Head Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head pose estimation is a crucial problem for many tasks, such as driver\nattention, fatigue detection, and human behaviour analysis. It is well known\nthat neural networks are better at handling classification problems than\nregression problems. It is an extremely nonlinear process to let the network\noutput the angle value directly for optimization learning, and the weight\nconstraint of the loss function will be relatively weak. This paper proposes a\nnovel Bernoulli heatmap for head pose estimation from a single RGB image. Our\nmethod can achieve the positioning of the head area while estimating the angles\nof the head. The Bernoulli heatmap makes it possible to construct fully\nconvolutional neural networks without fully connected layers and provides a new\nidea for the output form of head pose estimation. A deep convolutional neural\nnetwork (CNN) structure with multiscale representations is adopted to maintain\nhigh-resolution information and low-resolution information in parallel. This\nkind of structure can maintain rich, high-resolution representations. In\naddition, channelwise fusion is adopted to make the fusion weights learnable\ninstead of simple addition with equal weights. As a result, the estimation is\nspatially more precise and potentially more accurate. The effectiveness of the\nproposed method is empirically demonstrated by comparing it with other\nstate-of-the-art methods on public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:36:29 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Hu", "Zhongxu", ""], ["Xing", "Yang", ""], ["Lv", "Chen", ""], ["Hang", "Peng", ""], ["Liu", "Jie", ""]]}, {"id": "2005.11923", "submitter": "Srujan Teja Thomdapu", "authors": "Srujan Teja Thomdapu, Palash Katiyar and Ketan Rajawat", "title": "Dynamic Cache Management In Content Delivery Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of content delivery networks (CDN) continues to rise with the\nexponential increase in the generation and consumption of electronic media. In\norder to ensure a high quality of experience, CDNs often deploy cache servers\nthat are capable of storing some of the popular files close to the user. Such\nedge caching solutions not only increase the content availability, but also\nresult in higher download rates and lower latency at the user. We consider the\nproblem of content placement from an optimization perspective. Different from\nthe classical eviction-based algorithms, the present work formulates the\ncontent placement problem from an optimization perspective and puts forth an\nonline algorithm for the same. In contrast to the existing optimization-based\nsolutions, the proposed algorithm is incremental and incurs very low\ncomputation cost, while yielding storage allocations that are provably\nnear-optimal. The proposed algorithm can handle time varying content\npopularity, thereby obviating the need for periodically estimating demand\ndistribution. Using synthetic and real IPTV data, we show that the proposed\npolicies outperform all the state of art caching techniques in terms of various\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 05:08:58 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Thomdapu", "Srujan Teja", ""], ["Katiyar", "Palash", ""], ["Rajawat", "Ketan", ""]]}, {"id": "2005.12439", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Kefei Wu, Jong-Hwi Park, Wei Zhu, Jiebo Luo", "title": "Personalized Fashion Recommendation from Personal Social Media Data: An\n  Item-to-Set Metric Learning Approach", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of online shopping for fashion products, accurate fashion\nrecommendation has become a critical problem. Meanwhile, social networks\nprovide an open and new data source for personalized fashion analysis. In this\nwork, we study the problem of personalized fashion recommendation from social\nmedia data, i.e. recommending new outfits to social media users that fit their\nfashion preferences. To this end, we present an item-to-set metric learning\nframework that learns to compute the similarity between a set of historical\nfashion items of a user to a new fashion item. To extract features from\nmulti-modal street-view fashion items, we propose an embedding module that\nperforms multi-modality feature extraction and cross-modality gated fusion. To\nvalidate the effectiveness of our approach, we collect a real-world social\nmedia dataset. Extensive experiments on the collected dataset show the superior\nperformance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 23:24:24 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Zheng", "Haitian", ""], ["Wu", "Kefei", ""], ["Park", "Jong-Hwi", ""], ["Zhu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2005.12524", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Palaiahnakote Shivakumara, Umapada Pal, Tong Lu and\n  Michael Blumenstein", "title": "A New Unified Method for Detecting Text from Marathon Runners and Sports\n  Players in Video", "comments": "Accepted in Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting text located on the torsos of marathon runners and sports players\nin video is a challenging issue due to poor quality and adverse effects caused\nby flexible/colorful clothing, and different structures of human bodies or\nactions. This paper presents a new unified method for tackling the above\nchallenges. The proposed method fuses gradient magnitude and direction\ncoherence of text pixels in a new way for detecting candidate regions.\nCandidate regions are used for determining the number of temporal frame\nclusters obtained by K-means clustering on frame differences. This process in\nturn detects key frames. The proposed method explores Bayesian probability for\nskin portions using color values at both pixel and component levels of temporal\nframes, which provides fused images with skin components. Based on skin\ninformation, the proposed method then detects faces and torsos by finding\nstructural and spatial coherences between them. We further propose adaptive\npixels linking a deep learning model for text detection from torso regions. The\nproposed method is tested on our own dataset collected from marathon/sports\nvideo and three standard datasets, namely, RBNR, MMM and R-ID of marathon\nimages, to evaluate the performance. In addition, the proposed method is also\ntested on the standard natural scene datasets, namely, CTW1500 and MS-COCO text\ndatasets, to show the objectiveness of the proposed method. A comparative study\nwith the state-of-the-art methods on bib number/text detection of different\ndatasets shows that the proposed method outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 05:54:28 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Nag", "Sauradip", ""], ["Shivakumara", "Palaiahnakote", ""], ["Pal", "Umapada", ""], ["Lu", "Tong", ""], ["Blumenstein", "Michael", ""]]}, {"id": "2005.12788", "submitter": "Tianchi Huang", "authors": "Tianchi Huang, Rui-Xiao Zhang, Lifeng Sun", "title": "Self-play Reinforcement Learning for Video Transmission", "comments": "To appear in NOSSDAV'20", "journal-ref": null, "doi": "10.1145/3386290.3396930", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video transmission services adopt adaptive algorithms to ensure users'\ndemands. Existing techniques are often optimized and evaluated by a function\nthat linearly combines several weighted metrics. Nevertheless, we observe that\nthe given function fails to describe the requirement accurately. Thus, such\nproposed methods might eventually violate the original needs. To eliminate this\nconcern, we propose \\emph{Zwei}, a self-play reinforcement learning algorithm\nfor video transmission tasks. Zwei aims to update the policy by\nstraightforwardly utilizing the actual requirement. Technically, Zwei samples a\nnumber of trajectories from the same starting point and instantly estimates the\nwin rate w.r.t the competition outcome. Here the competition result represents\nwhich trajectory is closer to the assigned requirement. Subsequently, Zwei\noptimizes the strategy by maximizing the win rate. To build Zwei, we develop\nsimulation environments, design adequate neural network models, and invent\ntraining methods for dealing with different requirements on various video\ntransmission scenarios. Trace-driven analysis over two representative tasks\ndemonstrates that Zwei optimizes itself according to the assigned requirement\nfaithfully, outperforming the state-of-the-art methods under all considered\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:12:08 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Huang", "Tianchi", ""], ["Zhang", "Rui-Xiao", ""], ["Sun", "Lifeng", ""]]}, {"id": "2005.13331", "submitter": "Felipe Sampaio", "authors": "Arthur Cerveira, Luciano Agostini, Bruno Zatt, Felipe Sampaio", "title": "Memory Assessment of Versatile Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a memory assessment of the next-generation Versatile\nVideo Coding (VVC). The memory analyses are performed adopting as a baseline\nthe state-of-the-art High-Efficiency Video Coding (HEVC). The goal is to offer\ninsights and observations of how critical the memory requirements of VVC are\naggravated, compared to HEVC. The adopted methodology consists of two sets of\nexperiments: (1) an overall memory profiling and (2) an inter-prediction\nspecific memory analysis. The results obtained in the memory profiling show\nthat VVC access up to 13.4x more memory than HEVC. Moreover, the\ninter-prediction module remains (as in HEVC) the most resource-intensive\noperation in the encoder: 60%-90% of the memory requirements. The\ninter-prediction specific analysis demonstrates that VVC requires up to 5.3x\nmore memory accesses than HEVC. Furthermore, our analysis indicates that up to\n23% of such growth is due to VVC novel-CU sizes (larger than 64x64).\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 12:52:07 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 11:29:52 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Cerveira", "Arthur", ""], ["Agostini", "Luciano", ""], ["Zatt", "Bruno", ""], ["Sampaio", "Felipe", ""]]}, {"id": "2005.13770", "submitter": "Run Wang", "authors": "Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, Lei Ma,\n  Yang Liu", "title": "DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake\n  Voices", "comments": "Accepted by ACM MM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CR cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in voice synthesis, AI-synthesized fake voices are\nindistinguishable to human ears and widely are applied to produce realistic and\nnatural DeepFakes, exhibiting real threats to our society. However, effective\nand robust detectors for synthesized fake voices are still in their infancy and\nare not ready to fully tackle this emerging threat. In this paper, we devise a\nnovel approach, named \\emph{DeepSonar}, based on monitoring neuron behaviors of\nspeaker recognition (SR) system, \\ie, a deep neural network (DNN), to discern\nAI-synthesized fake voices. Layer-wise neuron behaviors provide an important\ninsight to meticulously catch the differences among inputs, which are widely\nemployed for building safety, robust, and interpretable DNNs. In this work, we\nleverage the power of layer-wise neuron activation patterns with a conjecture\nthat they can capture the subtle differences between real and AI-synthesized\nfake voices, in providing a cleaner signal to classifiers than raw inputs.\nExperiments are conducted on three datasets (including commercial products from\nGoogle, Baidu, \\etc) containing both English and Chinese languages to\ncorroborate the high detection rates (98.1\\% average accuracy) and low false\nalarm rates (about 2\\% error rate) of DeepSonar in discerning fake voices.\nFurthermore, extensive experimental results also demonstrate its robustness\nagainst manipulation attacks (\\eg, voice conversion and additive real-world\nnoises). Our work further poses a new insight into adopting neuron behaviors\nfor effective and robust AI aided multimedia fakes forensics as an inside-out\napproach instead of being motivated and swayed by various artifacts introduced\nin synthesizing fakes.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 04:02:52 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 03:07:11 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 13:37:57 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Run", ""], ["Juefei-Xu", "Felix", ""], ["Huang", "Yihao", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Liu", "Yang", ""]]}, {"id": "2005.13876", "submitter": "Christian Otto", "authors": "Jianwei Shi, Christian Otto, Anett Hoppe, Peter Holtz, Ralph Ewerth", "title": "Investigating Correlations of Automatically Extracted Multimodal\n  Features and Lecture Video Quality", "comments": null, "journal-ref": "SALMM '19: Proceedings of the 1st International Workshop on Search\n  as Learning with Multimedia Information, co-located with ACM Multimedia 2019", "doi": "10.1145/3347451.3356731", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking and recommendation of multimedia content such as videos is usually\nrealized with respect to the relevance to a user query. However, for lecture\nvideos and MOOCs (Massive Open Online Courses) it is not only required to\nretrieve relevant videos, but particularly to find lecture videos of high\nquality that facilitate learning, for instance, independent of the video's or\nspeaker's popularity. Thus, metadata about a lecture video's quality are\ncrucial features for learning contexts, e.g., lecture video recommendation in\nsearch as learning scenarios. In this paper, we investigate whether\nautomatically extracted features are correlated to quality aspects of a video.\nA set of scholarly videos from a Mass Open Online Course (MOOC) is analyzed\nregarding audio, linguistic, and visual features. Furthermore, a set of\ncross-modal features is proposed which are derived by combining transcripts,\naudio, video, and slide content. A user study is conducted to investigate the\ncorrelations between the automatically collected features and human ratings of\nquality aspects of a lecture video. Finally, the impact of our features on the\nknowledge gain of the participants is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 09:46:53 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Shi", "Jianwei", ""], ["Otto", "Christian", ""], ["Hoppe", "Anett", ""], ["Holtz", "Peter", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2005.13884", "submitter": "Zhenghao Shi", "authors": "Zhaorun Zhou, Zhenghao Shi, Mingtao Guo, Yaning Feng, Minghua Zhao", "title": "CGGAN: A Context Guided Generative Adversarial Network For Single Image\n  Dehazing", "comments": "12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image haze removal is highly desired for the application of computer vision.\nThis paper proposes a novel Context Guided Generative Adversarial Network\n(CGGAN) for single image dehazing. Of which, an novel new encoder-decoder is\nemployed as the generator. And it consists of a feature-extraction-net, a\ncontext-extractionnet, and a fusion-net in sequence. The feature extraction-net\nacts as a encoder, and is used for extracting haze features. The\ncontext-extraction net is a multi-scale parallel pyramid decoder, and is used\nfor extracting the deep features of the encoder and generating coarse dehazing\nimage. The fusion-net is a decoder, and is used for obtaining the final\nhaze-free image. To obtain more better results, multi-scale information\nobtained during the decoding process of the context extraction decoder is used\nfor guiding the fusion decoder. By introducing an extra coarse decoder to the\noriginal encoder-decoder, the CGGAN can make better use of the deep feature\ninformation extracted by the encoder. To ensure our CGGAN work effectively for\ndifferent haze scenarios, different loss functions are employed for the two\ndecoders. Experiments results show the advantage and the effectiveness of our\nproposed CGGAN, evidential improvements over existing state-of-the-art methods\nare obtained.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:14:30 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhou", "Zhaorun", ""], ["Shi", "Zhenghao", ""], ["Guo", "Mingtao", ""], ["Feng", "Yaning", ""], ["Zhao", "Minghua", ""]]}, {"id": "2005.13983", "submitter": "Weixia Zhang", "authors": "Weixia Zhang and Kede Ma and Guangtao Zhai and Xiaokang Yang", "title": "Uncertainty-Aware Blind Image Quality Assessment in the Laboratory and\n  Wild", "comments": "Accepted to IEEE TIP. The implementations are available at\n  https://github.com/zwx8981/UNIQUE", "journal-ref": null, "doi": "10.1109/TIP.2021.3061932", "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of blind image quality assessment (BIQA) models has been\nsignificantly boosted by end-to-end optimization of feature engineering and\nquality regression. Nevertheless, due to the distributional shift between\nimages simulated in the laboratory and captured in the wild, models trained on\ndatabases with synthetic distortions remain particularly weak at handling\nrealistic distortions (and vice versa). To confront the\ncross-distortion-scenario challenge, we develop a \\textit{unified} BIQA model\nand an approach of training it for both synthetic and realistic distortions. We\nfirst sample pairs of images from individual IQA databases, and compute a\nprobability that the first image of each pair is of higher quality. We then\nemploy the fidelity loss to optimize a deep neural network for BIQA over a\nlarge number of such image pairs. We also explicitly enforce a hinge constraint\nto regularize uncertainty estimation during optimization. Extensive experiments\non six IQA databases show the promise of the learned method in blindly\nassessing image quality in the laboratory and wild. In addition, we demonstrate\nthe universality of the proposed training strategy by using it to improve\nexisting BIQA models.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:35:23 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 05:12:39 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 09:32:54 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 08:27:23 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 15:46:10 GMT"}, {"version": "v6", "created": "Tue, 23 Feb 2021 09:45:41 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Kede", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2005.14317", "submitter": "Alvaro Pastor", "authors": "Alvaro Pastor", "title": "Augmenting reality: On the shared history of perceptual illusion and\n  video projection mapping", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Perceptual illusions based on the spatial correspondence between objects and\ndisplayed images have been pursued by artists and scientists since the 15th\ncentury, mastering optics to create crucial techniques as the linear\nperspective and devices as the Magic Lantern. Contemporary video projection\nmapping inherits and further extends this drive to produce perceptual illusions\nin space by incorporating the required real time capabilities for dynamically\nsuperposing the imaginary onto physical objects under fluid real world\nconditions. A critical milestone has been reached in the creation of the\ntechnical possibilities for all encompassing, untethered synthetic reality\nexperiences available to the plain senses, where every surface may act as a\nscreen and the relation to everyday objects is open to alterations.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:07:29 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pastor", "Alvaro", ""]]}, {"id": "2005.14405", "submitter": "Parul Gupta", "authors": "Komal Chugh, Parul Gupta, Abhinav Dhall and Ramanathan Subramanian", "title": "Not made for each other- Audio-Visual Dissonance-based Deepfake\n  Detection and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose detection of deepfake videos based on the dissimilarity between\nthe audio and visual modalities, termed as the Modality Dissonance Score (MDS).\nWe hypothesize that manipulation of either modality will lead to dis-harmony\nbetween the two modalities, eg, loss of lip-sync, unnatural facial and lip\nmovements, etc. MDS is computed as an aggregate of dissimilarity scores between\naudio and visual segments in a video. Discriminative features are learnt for\nthe audio and visual channels in a chunk-wise manner, employing the\ncross-entropy loss for individual modalities, and a contrastive loss that\nmodels inter-modality similarity. Extensive experiments on the DFDC and\nDeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art\nby up to 7%. We also demonstrate temporal forgery localization, and show how\nour technique identifies the manipulated video segments.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 06:09:33 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 03:13:38 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 15:09:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chugh", "Komal", ""], ["Gupta", "Parul", ""], ["Dhall", "Abhinav", ""], ["Subramanian", "Ramanathan", ""]]}]