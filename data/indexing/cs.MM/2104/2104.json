[{"id": "2104.00234", "submitter": "Xiushan Nie", "authors": "Xinfang Liu, Xiushan Nie (Member, IEEE), Zhifang Tan, Jie Guo, Yilong\n  Yin", "title": "A Survey on Natural Language Video Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language video localization (NLVL), which aims to locate a target\nmoment from a video that semantically corresponds to a text query, is a novel\nand challenging task. Toward this end, in this paper, we present a\ncomprehensive survey of the NLVL algorithms, where we first propose the\npipeline of NLVL, and then categorize them into supervised and\nweakly-supervised methods, following by the analysis of the strengths and\nweaknesses of each kind of methods. Subsequently, we present the dataset,\nevaluation protocols and the general performance analysis. Finally, the\npossible perspectives are obtained by summarizing the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:30:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Liu", "Xinfang", "", "Member, IEEE"], ["Nie", "Xiushan", "", "Member, IEEE"], ["Tan", "Zhifang", ""], ["Guo", "Jie", ""], ["Yin", "Yilong", ""]]}, {"id": "2104.00239", "submitter": "Jinxing Zhou", "authors": "Jinxing Zhou, Liang Zheng, Yiran Zhong, Shijie Hao, Meng Wang", "title": "Positive Sample Propagation along the Audio-Visual Event Line", "comments": "Accepted to CVPR 2021. Code is available at\n  https://github.com/jasongief/PSP_CVPR_2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual and audio signals often coexist in natural environments, forming\naudio-visual events (AVEs). Given a video, we aim to localize video segments\ncontaining an AVE and identify its category. In order to learn discriminative\nfeatures for a classifier, it is pivotal to identify the helpful (or positive)\naudio-visual segment pairs while filtering out the irrelevant ones, regardless\nwhether they are synchronized or not. To this end, we propose a new positive\nsample propagation (PSP) module to discover and exploit the closely related\naudio-visual pairs by evaluating the relationship within every possible pair.\nIt can be done by constructing an all-pair similarity map between each audio\nand visual segment, and only aggregating the features from the pairs with high\nsimilarity scores. To encourage the network to extract high correlated features\nfor positive samples, a new audio-visual pair similarity loss is proposed. We\nalso propose a new weighting branch to better exploit the temporal correlations\nin weakly supervised setting. We perform extensive experiments on the public\nAVE dataset and achieve new state-of-the-art accuracy in both fully and weakly\nsupervised settings, thus verifying the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:53:57 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 07:28:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhou", "Jinxing", ""], ["Zheng", "Liang", ""], ["Zhong", "Yiran", ""], ["Hao", "Shijie", ""], ["Wang", "Meng", ""]]}, {"id": "2104.00437", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Xavier Favory, Konstantinos Drossos, Yuntae Kim and\n  Dmitry Bogdanov", "title": "Enriched Music Representations with Multiple Cross-modal Contrastive\n  Learning", "comments": "Accepted for publication to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2021.3071082", "report-no": "SPL-30069-2021", "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modeling various aspects that make a music piece unique is a challenging\ntask, requiring the combination of multiple sources of information. Deep\nlearning is commonly used to obtain representations using various sources of\ninformation, such as the audio, interactions between users and songs, or\nassociated genre metadata. Recently, contrastive learning has led to\nrepresentations that generalize better compared to traditional supervised\nmethods. In this paper, we present a novel approach that combines multiple\ntypes of information related to music using cross-modal contrastive learning,\nallowing us to learn an audio feature from heterogeneous data simultaneously.\nWe align the latent representations obtained from playlists-track interactions,\ngenre metadata, and the tracks' audio, by maximizing the agreement between\nthese modality representations using a contrastive loss. We evaluate our\napproach in three tasks, namely, genre classification, playlist continuation\nand automatic tagging. We compare the performances with a baseline audio-based\nCNN trained to predict these modalities. We also study the importance of\nincluding multiple sources of information when training our embedding model.\nThe results suggest that the proposed method outperforms the baseline in all\nthe three downstream tasks and achieves comparable performance to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:41:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ferraro", "Andres", ""], ["Favory", "Xavier", ""], ["Drossos", "Konstantinos", ""], ["Kim", "Yuntae", ""], ["Bogdanov", "Dmitry", ""]]}, {"id": "2104.00636", "submitter": "Joseph Zammit", "authors": "Joseph Zammit and Ian J Wassell", "title": "Distributed Video Adaptive Block Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video block compressive sensing has been studied for use in resource\nconstrained scenarios, such as wireless sensor networks, but the approach still\nsuffers from low performance and long reconstruction time. Inspired by\nclassical distributed video coding, we design a lightweight encoder with\ncomputationally intensive operations, such as video frame interpolation,\nperformed at the decoder. Straying from recent trends in training end-to-end\nneural networks, we propose two algorithms that leverage convolutional neural\nnetwork components to reconstruct video with greatly reduced reconstruction\ntime. At the encoder, we leverage temporal correlation between frames and\ndeploy adaptive techniques based on compressive measurements from previous\nframes. At the decoder, we exploit temporal correlation by using video frame\ninterpolation and temporal differential pulse code modulation. Simulations show\nthat our two proposed algorithms, VAL-VFI and VAL-IDA-VFI reconstruct higher\nquality video, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:31:37 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zammit", "Joseph", ""], ["Wassell", "Ian J", ""]]}, {"id": "2104.00805", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Lore Goetschalckx, Anelise Newman, Aude Oliva", "title": "Memorability: An image-computable measure of information utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pixels in an image, and the objects, scenes, and actions that they\ncompose, determine whether an image will be memorable or forgettable. While\nmemorability varies by image, it is largely independent of an individual\nobserver. Observer independence is what makes memorability an image-computable\nmeasure of information, and eligible for automatic prediction. In this chapter,\nwe zoom into memorability with a computational lens, detailing the\nstate-of-the-art algorithms that accurately predict image memorability relative\nto human behavioral data, using image features at different scales from raw\npixels to semantic labels. We discuss the design of algorithms and\nvisualizations for face, object, and scene memorability, as well as algorithms\nthat generalize beyond static scenes to actions and videos. We cover the\nstate-of-the-art deep learning approaches that are the current front runners in\nthe memorability prediction space. Beyond prediction, we show how recent A.I.\napproaches can be used to create and modify visual memorability. Finally, we\npreview the computational applications that memorability can power, from\nfiltering visual streams to enhancing augmented reality interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:38:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Goetschalckx", "Lore", ""], ["Newman", "Anelise", ""], ["Oliva", "Aude", ""]]}, {"id": "2104.00807", "submitter": "Hilmi Enes Egilmez", "authors": "Ankitesh K. Singh, Hilmi E. Egilmez, Reza Pourreza, Muhammed Coban,\n  Marta Karczewicz, Taco S. Cohen", "title": "A Combined Deep Learning based End-to-End Video Coding Architecture for\n  YUV Color Space", "comments": "5 pages, submitted to as a conference paper. arXiv admin note: text\n  overlap with arXiv:2103.01760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning based end-to-end video coding (DLEC)\narchitectures are designed specifically for RGB color format, yet the video\ncoding standards, including H.264/AVC, H.265/HEVC and H.266/VVC developed over\npast few decades, have been designed primarily for YUV 4:2:0 format, where the\nchrominance (U and V) components are subsampled to achieve superior compression\nperformances considering the human visual system. While a broad number of\npapers on DLEC compare these two distinct coding schemes in RGB domain, it is\nideal to have a common evaluation framework in YUV 4:2:0 domain for a more fair\ncomparison. This paper introduces a new DLEC architecture for video coding to\neffectively support YUV 4:2:0 and compares its performance against the HEVC\nstandard under a common evaluation framework. The experimental results on YUV\n4:2:0 video sequences show that the proposed architecture can outperform HEVC\nin intra-frame coding, however inter-frame coding is not as efficient on\ncontrary to the RGB coding results reported in recent papers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:41:06 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Singh", "Ankitesh K.", ""], ["Egilmez", "Hilmi E.", ""], ["Pourreza", "Reza", ""], ["Coban", "Muhammed", ""], ["Karczewicz", "Marta", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2104.00955", "submitter": "Yuan Gao", "authors": "Yuan Gao, Maoguo Gong, Yu Xie, A. K. Qin", "title": "An attention-based unsupervised adversarial model for movie review spam\n  detection", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, vol. 23, pp. 784-796, 2021", "doi": "10.1109/TMM.2020.2990085", "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of the Internet, online reviews have become a valuable\ninformation resource for people. However, the authenticity of online reviews\nremains a concern, and deceptive reviews have become one of the most urgent\nnetwork security problems to be solved. Review spams will mislead users into\nmaking suboptimal choices and inflict their trust in online reviews. Most\nexisting research manually extracted features and labeled training samples,\nwhich are usually complicated and time-consuming. This paper focuses primarily\non a neglected emerging domain - movie review, and develops a novel\nunsupervised spam detection model with an attention mechanism. By extracting\nthe statistical features of reviews, it is revealed that users will express\ntheir sentiments on different aspects of movies in reviews. An attention\nmechanism is introduced in the review embedding, and the conditional generative\nadversarial network is exploited to learn users' review style for different\ngenres of movies. The proposed model is evaluated on movie reviews crawled from\nDouban, a Chinese online community where people could express their feelings\nabout movies. The experimental results demonstrate the superior performance of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:30:47 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gao", "Yuan", ""], ["Gong", "Maoguo", ""], ["Xie", "Yu", ""], ["Qin", "A. K.", ""]]}, {"id": "2104.00959", "submitter": "Pavlos Sermpezis", "authors": "Theodoros Giannakas, Pavlos Sermpezis, Anastasios Giovanidis,\n  Thrasyvoulos Spyropoulos, George Arvanitakis", "title": "Fairness in Network-Friendly Recommendations", "comments": "IEEE International Symposium on a World of Wireless, Mobile and\n  Multimedia Networks (WoWMoM), 2021", "journal-ref": "IEEE International Symposium on a World of Wireless, Mobile and\n  Multimedia Networks (WoWMoM), Jun 2021, Pisa (virtual), Italy", "doi": "10.1109/WoWMoM51794.2021.00020", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As mobile traffic is dominated by content services (e.g., video), which\ntypically use recommendation systems, the paradigm of network-friendly\nrecommendations (NFR) has been proposed recently to boost the network\nperformance by promoting content that can be efficiently delivered (e.g.,\ncached at the edge). NFR increase the network performance, however, at the cost\nof being unfair towards certain contents when compared to the standard\nrecommendations. This unfairness is a side effect of NFR that has not been\nstudied in literature. Nevertheless, retaining fairness among contents is a key\noperational requirement for content providers. This paper is the first to study\nthe fairness in NFR, and design fair-NFR. Specifically, we use a set of metrics\nthat capture different notions of fairness, and study the unfairness created by\nexisting NFR schemes. Our analysis reveals that NFR can be significantly\nunfair. We identify an inherent trade-off between the network gains achieved by\nNFR and the resulting unfairness, and derive bounds for this trade-off. We show\nthat existing NFR schemes frequently operate far from the bounds, i.e., there\nis room for improvement. To this end, we formulate the design of Fair-NFR\n(i.e., NFR with fairness guarantees compared to the baseline recommendations)\nas a linear optimization problem. Our results show that the Fair-NFR can\nachieve high network gains (similar to non-fair-NFR) with little unfairness.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:50:39 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Giannakas", "Theodoros", ""], ["Sermpezis", "Pavlos", ""], ["Giovanidis", "Anastasios", ""], ["Spyropoulos", "Thrasyvoulos", ""], ["Arvanitakis", "George", ""]]}, {"id": "2104.01104", "submitter": "Yanyuan Qin", "authors": "Yanyuan Qin", "title": "Adaptive Bitrate Streaming Over Cellular Networks: Rate Adaptation and\n  Data Savings Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adaptive bitrate streaming (ABR) has become thede factotechnique for\nvideostreaming over the Internet. Despite a flurry of techniques, achieving\nhigh quality ABRstreaming over cellular networks remains a tremendous\nchallenge. First, the design ofan ABR scheme needs to balance conflicting\nQuality of Experience (QoE) metrics suchas video quality, quality changes,\nstalls and startup performance, which is even harderunder highly dynamic\nbandwidth in cellular network. Second, streaming providers havebeen moving\ntowards using Variable Bitrate (VBR) encodings for the video content,which\nintroduces new challenges for ABR streaming, whose nature and implicationsare\nlittle understood. Third, mobile video streaming consumes a lot of data.\nAlthoughmany video and network providers currently offer data saving options,\nthe existingpractices are suboptimal in QoE and resource usage. Last, when the\naudio and videotracks are stored separately, video and audio rate adaptation\nneeds to be dynamicallycoordinated to achieve good overall streaming\nexperience, which presents interestingchallenges while, somewhat surprisingly,\nhas received little attention by the researchcommunity. In this dissertation,\nwe tackle each of the above four challenges.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:56:00 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 20:48:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Qin", "Yanyuan", ""]]}, {"id": "2104.01301", "submitter": "Palak Tiwary", "authors": "Palak Tiwary and Sanjida Ahmed", "title": "Multimedia Technology Applications and Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia related research and development has evolved rapidly in the last\nfew years with advancements in hardware, software and network infrastructures.\nAs a result, multimedia has been integrated into domains like Healthcare and\nMedicine, Human facial feature extraction and tracking, pose recognition,\ndisparity estimation, etc. This survey gives an overview of the various\nmultimedia technologies and algorithms developed in the domains mentioned.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 03:23:06 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tiwary", "Palak", ""], ["Ahmed", "Sanjida", ""]]}, {"id": "2104.01548", "submitter": "Jingwen Hou", "authors": "Jingwen Hou, Sheng Yang, Weisi Lin, Baoquan Zhao, Yuming Fang", "title": "Learning Image Aesthetic Assessment from Object-level Visual Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As it is said by Van Gogh, great things are done by a series of small things\nbrought together. Aesthetic experience arises from the aggregation of\nunderlying visual components. However, most existing deep image aesthetic\nassessment (IAA) methods over-simplify the IAA process by failing to model\nimage aesthetics with clearly-defined visual components as building blocks. As\na result, the connection between resulting aesthetic predictions and underlying\nvisual components is mostly invisible and hard to be explicitly controlled,\nwhich limits the model in both performance and interpretability. This work aims\nto model image aesthetics from the level of visual components. Specifically,\nobject-level regions detected by a generic object detector are defined as\nvisual components, namely object-level visual components (OVCs). Then generic\nfeatures representing OVCs are aggregated for the aesthetic prediction based\nupon proposed object-level and graph attention mechanisms, which dynamically\ndetermines the importance of individual OVCs and relevance between OVC pairs,\nrespectively. Experimental results confirm the superiority of our framework\nover previous relevant methods in terms of SRCC and PLCC on the aesthetic\nrating distribution prediction. Besides, quantitative analysis is done towards\nmodel interpretation by observing how OVCs contribute to aesthetic predictions,\nwhose results are found to be supported by psychology on aesthetics and\nphotography rules. To the best of our knowledge, this is the first attempt at\nthe interpretation of a deep IAA model.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 07:03:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hou", "Jingwen", ""], ["Yang", "Sheng", ""], ["Lin", "Weisi", ""], ["Zhao", "Baoquan", ""], ["Fang", "Yuming", ""]]}, {"id": "2104.02026", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Di Hu, Chenliang Xu", "title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound\n  Separation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are rich synchronized audio and visual events in our daily life. Inside\nthe events, audio scenes are associated with the corresponding visual objects;\nmeanwhile, sounding objects can indicate and help to separate their individual\nsounds in the audio track. Based on this observation, in this paper, we propose\na cyclic co-learning (CCoL) paradigm that can jointly learn sounding object\nvisual grounding and audio-visual sound separation in a unified framework.\nConcretely, we can leverage grounded object-sound relations to improve the\nresults of sound separation. Meanwhile, benefiting from discriminative\ninformation from separated sounds, we improve training example sampling for\nsounding object grounding, which builds a co-learning cycle for the two tasks\nand makes them mutually beneficial. Extensive experiments show that the\nproposed framework outperforms the compared recent approaches on both tasks,\nand they can benefit from each other with our cyclic co-learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:30:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tian", "Yapeng", ""], ["Hu", "Di", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.02046", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Jaros{\\l}aw Kowalski, Julia Paluch, Anna\n  Jaskulska, Kinga Skorupska, Marcin Niewi\\'nski, Maciej Krzywicki, Cezary\n  Biele", "title": "Older Adults and Brain-Computer Interface: An Exploratory Study", "comments": null, "journal-ref": null, "doi": "10.1145/3411763.3451663", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory study, we examine the possibilities of non-invasive\nBrain-Computer Interface (BCI) in the context of Smart Home Technology (SHT)\ntargeted at older adults. During two workshops, one stationary, and one online\nvia Zoom, we researched the insights of the end users concerning the potential\nof the BCI in the SHT setting. We explored its advantages and drawbacks, and\nthe features older adults see as vital as well as the ones that they would\nbenefit from. Apart from evaluating the participants' perception of such\ndevices during the two workshops we also analyzed some key considerations\nresulting from the insights gathered during the workshops, such as potential\nbarriers, ways to mitigate them, strengths and opportunities connected to BCI.\nThese may be useful for designing BCI interaction paradigms and pinpointing\nareas of interest to pursue in further studies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:49:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Kowalski", "Jaros\u0142aw", ""], ["Paluch", "Julia", ""], ["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Niewi\u0144ski", "Marcin", ""], ["Krzywicki", "Maciej", ""], ["Biele", "Cezary", ""]]}, {"id": "2104.02100", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Krzysztof Kalinowski, Monika Kornacka, Kinga\n  Skorupska, Julia Paluch, Anna Jaskulska, Grzegorz Pochwatko, Jakub Mo\\.zaryn,\n  Pawe{\\l} Kobyli\\'nski, Piotr Gago", "title": "VR Hackathon with Goethe Institute: Lessons Learned from Organizing a\n  Transdisciplinary VR Hackathon", "comments": null, "journal-ref": null, "doi": "10.1145/3411763.3443432", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we report a case study of a Language Learning Bauhaus VR\nhackathon with Goethe Institute. It was organized as an educational and\nresearch project to tap into the dynamics of transdisciplinary teams challenged\nwith a specific requirement. In our case, it was to build a Bauhaus-themed\nGerman Language Learning VR App. We constructed this experiment to simulate how\nrepresentatives of different disciplines may work together towards a very\nspecific purpose under time pressure. So, each participating team consisted of\nmembers of various expert-fields: software development (Unity or Unreal),\ndesign, psychology and linguistics. The results of this study cast light on the\nrecommended cycle of design thinking and customer-centered design in VR.\nEspecially in interdisciplinary rapid prototyping conditions, where\nstakeholders initially do not share competences. They also showcase educational\nbenefits of working in transdisciplinary environments. This study, combined\nwith our previous work on human factors in rapid software development and\nco-design, including hackathon dynamics, allowed us to formulate\nrecommendations for organizing content creation VR hackathons for specific\npurposes. We also provide guidelines on how to prepare the participants to work\nin rapid prototyping VR environments and benefit from such experiences in the\nlong term.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:09:24 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Kalinowski", "Krzysztof", ""], ["Kornacka", "Monika", ""], ["Skorupska", "Kinga", ""], ["Paluch", "Julia", ""], ["Jaskulska", "Anna", ""], ["Pochwatko", "Grzegorz", ""], ["Mo\u017caryn", "Jakub", ""], ["Kobyli\u0144ski", "Pawe\u0142", ""], ["Gago", "Piotr", ""]]}, {"id": "2104.02605", "submitter": "Zejun Li", "authors": "Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, Xuanjing Huang", "title": "An Unsupervised Sampling Approach for Image-Sentence Matching Using\n  Document-Level Structural Information", "comments": "To be published in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of unsupervised image-sentence\nmatching. Existing research explores to utilize document-level structural\ninformation to sample positive and negative instances for model training.\nAlthough the approach achieves positive results, it introduces a sampling bias\nand fails to distinguish instances with high semantic similarity. To alleviate\nthe bias, we propose a new sampling strategy to select additional\nintra-document image-sentence pairs as positive or negative samples.\nFurthermore, to recognize the complex pattern in intra-document samples, we\npropose a Transformer based model to capture fine-grained features and\nimplicitly construct a graph for each document, where concepts in a document\nare introduced to bridge the representation learning of images and sentences in\nthe context of a document. Experimental results show the effectiveness of our\napproach to alleviate the bias and learn well-aligned multimodal\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 05:43:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Zejun", ""], ["Wei", "Zhongyu", ""], ["Fan", "Zhihao", ""], ["Shan", "Haijun", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2104.02618", "submitter": "Pablo Perez", "authors": "Pablo Perez, Lucjan Janowski, Narciso Garcia and Margaret Pinson", "title": "Subjective Assessment Experiments That Recruit Few Observers With\n  Repetitions (FOWR)", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that it is possible to characterize subject bias\nand variance in subjective assessment tests. Apparent differences among\nsubjects can, for the most part, be explained by random factors. Building on\nthat theory, we propose a subjective test design where three to four team\nmembers each rate the stimuli multiple times. The results are comparable to a\nhigh performing objective metric. This provides a quick and simple way to\nanalyze new technologies and perform pre-tests for subjective assessment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:55:13 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Perez", "Pablo", ""], ["Janowski", "Lucjan", ""], ["Garcia", "Narciso", ""], ["Pinson", "Margaret", ""]]}, {"id": "2104.02655", "submitter": "Tao Li", "authors": "Tao Li and Min Soo Choi", "title": "DeepBlur: A Simple and Effective Method for Natural Image Obfuscation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing privacy concern due to the popularity of social media and\nsurveillance systems, along with advances in face recognition software.\nHowever, established image obfuscation techniques are either vulnerable to\nre-identification attacks by human or deep learning models, insufficient in\npreserving image fidelity, or too computationally intensive to be practical. To\ntackle these issues, we present DeepBlur, a simple yet effective method for\nimage obfuscation by blurring in the latent space of an unconditionally\npre-trained generative model that is able to synthesize photo-realistic facial\nimages. We compare it with existing methods by efficiency and image quality,\nand evaluate against both state-of-the-art deep learning models and industrial\nproducts (e.g., Face++, Microsoft face service). Experiments show that our\nmethod produces high quality outputs and is the strongest defense for most test\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 19:31:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Tao", ""], ["Choi", "Min Soo", ""]]}, {"id": "2104.02656", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod K Kurmi, Vipul Bajaj, Badri N Patro, K S Venkatesh, Vinay P\n  Namboodiri, Preethi Jyothi", "title": "Collaborative Learning to Generate Audio-Video Jointly", "comments": "ICASSP 2021 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There have been a number of techniques that have demonstrated the generation\nof multimedia data for one modality at a time using GANs, such as the ability\nto generate images, videos, and audio. However, so far, the task of multi-modal\ngeneration of data, specifically for audio and videos both, has not been\nsufficiently well-explored. Towards this, we propose a method that demonstrates\nthat we are able to generate naturalistic samples of video and audio data by\nthe joint correlated generation of audio and video modalities. The proposed\nmethod uses multiple discriminators to ensure that the audio, video, and the\njoint output are also indistinguishable from real-world samples. We present a\ndataset for this task and show that we are able to generate realistic samples.\nThis method is validated using various standard metrics such as Inception\nScore, Frechet Inception Distance (FID) and through human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:00:51 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Bajaj", "Vipul", ""], ["Patro", "Badri N", ""], ["Venkatesh", "K S", ""], ["Namboodiri", "Vinay P", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2104.02687", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A. Efros,\n  Trevor Darrell", "title": "Strumming to the Beat: Audio-Conditioned Contrastive Video Textures", "comments": "Project website at https://medhini.github.io/audio_video_textures/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a non-parametric approach for infinite video texture synthesis\nusing a representation learned via contrastive learning. We take inspiration\nfrom Video Textures, which showed that plausible new videos could be generated\nfrom a single one by stitching its frames together in a novel yet consistent\norder. This classic work, however, was constrained by its use of hand-designed\ndistance metrics, limiting its use to simple, repetitive videos. We draw on\nrecent techniques from self-supervised learning to learn this distance metric,\nallowing us to compare frames in a manner that scales to more challenging\ndynamics, and to condition on other data, such as audio. We learn\nrepresentations for video frames and frame-to-frame transition probabilities by\nfitting a video-specific model trained using contrastive learning. To\nsynthesize a texture, we randomly sample frames with high transition\nprobabilities to generate diverse temporally smooth videos with novel sequences\nand transitions. The model naturally extends to an audio-conditioned setting\nwithout requiring any finetuning. Our model outperforms baselines on human\nperceptual scores, can handle a diverse range of input videos, and can combine\nsemantic and audio-visual cues in order to synthesize videos that synchronize\nwell with an audio signal.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:24:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Ginosar", "Shiry", ""], ["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "2104.02850", "submitter": "Jin Liu", "authors": "Jin Liu, Peng Chen, Tao Liang, Zhaoxing Li, Cai Yu, Shuqiao Zou, Jiao\n  Dai, Jizhong Han", "title": "LI-Net: Large-Pose Identity-Preserving Face Reenactment Network", "comments": "IEEE International Conference on Multimedia and Expo(ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face reenactment is a challenging task, as it is difficult to maintain\naccurate expression, pose and identity simultaneously. Most existing methods\ndirectly apply driving facial landmarks to reenact source faces and ignore the\nintrinsic gap between two identities, resulting in the identity mismatch issue.\nBesides, they neglect the entanglement of expression and pose features when\nencoding driving faces, leading to inaccurate expressions and visual artifacts\non large-pose reenacted faces. To address these problems, we propose a\nLarge-pose Identity-preserving face reenactment network, LI-Net. Specifically,\nthe Landmark Transformer is adopted to adjust driving landmark images, which\naims to narrow the identity gap between driving and source landmark images.\nThen the Face Rotation Module and the Expression Enhancing Generator decouple\nthe transformed landmark image into pose and expression features, and reenact\nthose attributes separately to generate identity-preserving faces with accurate\nexpressions and poses. Both qualitative and quantitative experimental results\ndemonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:41:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Jin", ""], ["Chen", "Peng", ""], ["Liang", "Tao", ""], ["Li", "Zhaoxing", ""], ["Yu", "Cai", ""], ["Zou", "Shuqiao", ""], ["Dai", "Jiao", ""], ["Han", "Jizhong", ""]]}, {"id": "2104.02967", "submitter": "Sanqing Qu", "authors": "Sanqing Qu, Guang Chen, Zhijun Li, Lijun Zhang, Fan Lu, Alois Knoll", "title": "ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal\n  Action Localization", "comments": "Submitted to TIP. Code is available at\n  https://github.com/ispc-lab/ACM-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly-supervised temporal action localization aims to localize action\ninstances temporal boundary and identify the corresponding action category with\nonly video-level labels. Traditional methods mainly focus on foreground and\nbackground frames separation with only a single attention branch and class\nactivation sequence. However, we argue that apart from the distinctive\nforeground and background frames there are plenty of semantically ambiguous\naction context frames. It does not make sense to group those context frames to\nthe same background class since they are semantically related to a specific\naction category. Consequently, it is challenging to suppress action context\nframes with only a single class activation sequence. To address this issue, in\nthis paper, we propose an action-context modeling network termed ACM-Net, which\nintegrates a three-branch attention module to measure the likelihood of each\ntemporal point being action instance, context, or non-action background,\nsimultaneously. Then based on the obtained three-branch attention values, we\nconstruct three-branch class activation sequences to represent the action\ninstances, contexts, and non-action backgrounds, individually. To evaluate the\neffectiveness of our ACM-Net, we conduct extensive experiments on two benchmark\ndatasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method\ncan outperform current state-of-the-art methods, and even achieve comparable\nperformance with fully-supervised methods. Code can be found at\nhttps://github.com/ispc-lab/ACM-Net\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:39:57 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Qu", "Sanqing", ""], ["Chen", "Guang", ""], ["Li", "Zhijun", ""], ["Zhang", "Lijun", ""], ["Lu", "Fan", ""], ["Knoll", "Alois", ""]]}, {"id": "2104.02971", "submitter": "Jiashuo Yu", "authors": "Jiashuo Yu, Ying Cheng, Rui Feng", "title": "MPN: Multimodal Parallel Network for Audio-Visual Event Localization", "comments": "IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-visual event localization aims to localize an event that is both\naudible and visible in the wild, which is a widespread audio-visual scene\nanalysis task for unconstrained videos. To address this task, we propose a\nMultimodal Parallel Network (MPN), which can perceive global semantics and\nunmixed local information parallelly. Specifically, our MPN framework consists\nof a classification subnetwork to predict event categories and a localization\nsubnetwork to predict event boundaries. The classification subnetwork is\nconstructed by the Multimodal Co-attention Module (MCM) and obtains global\ncontexts. The localization subnetwork consists of Multimodal Bottleneck\nAttention Module (MBAM), which is designed to extract fine-grained\nsegment-level contents. Extensive experiments demonstrate that our framework\nachieves the state-of-the-art performance both in fully supervised and weakly\nsupervised settings on the Audio-Visual Event (AVE) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:44:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yu", "Jiashuo", ""], ["Cheng", "Ying", ""], ["Feng", "Rui", ""]]}, {"id": "2104.03236", "submitter": "Herv\\'e Le Borgne", "authors": "Omar Adjali and Romaric Besan\\c{c}on and Olivier Ferret and Herve Le\n  Borgne and Brigitte Grau", "title": "Multimodal Entity Linking for Tweets", "comments": null, "journal-ref": "In: Jose J. et al. (eds) Advances in Information Retrieval. ECIR\n  2020. Lecture Notes in Computer Science, vol 12035. Springer, Cham", "doi": "10.1007/978-3-030-45439-5_31", "report-no": null, "categories": "cs.IR cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many information extraction applications, entity linking (EL) has emerged\nas a crucial task that allows leveraging information about named entities from\na knowledge base. In this paper, we address the task of multimodal entity\nlinking (MEL), an emerging research field in which textual and visual\ninformation is used to map an ambiguous mention to an entity in a knowledge\nbase (KB). First, we propose a method for building a fully annotated Twitter\ndataset for MEL, where entities are defined in a Twitter KB. Then, we propose a\nmodel for jointly learning a representation of both mentions and entities from\ntheir textual and visual contexts. We demonstrate the effectiveness of the\nproposed model by evaluating it on the proposed dataset and highlight the\nimportance of leveraging visual information when it is available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:40:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Adjali", "Omar", ""], ["Besan\u00e7on", "Romaric", ""], ["Ferret", "Olivier", ""], ["Borgne", "Herve Le", ""], ["Grau", "Brigitte", ""]]}, {"id": "2104.03531", "submitter": "Zhao Kang", "authors": "Juncheng Lv and Zhao Kang and Xiao Lu and Zenglin Xu", "title": "Pseudo-supervised Deep Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2021", "doi": "10.1109/TIP.2021.3079800", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Encoder (AE)-based deep subspace clustering (DSC) methods have achieved\nimpressive performance due to the powerful representation extracted using deep\nneural networks while prioritizing categorical separability. However,\nself-reconstruction loss of an AE ignores rich useful relation information and\nmight lead to indiscriminative representation, which inevitably degrades the\nclustering performance. It is also challenging to learn high-level similarity\nwithout feeding semantic labels. Another unsolved problem facing DSC is the\nhuge memory cost due to $n\\times n$ similarity matrix, which is incurred by the\nself-expression layer between an encoder and decoder. To tackle these problems,\nwe use pairwise similarity to weigh the reconstruction loss to capture local\nstructure information, while a similarity is learned by the self-expression\nlayer. Pseudo-graphs and pseudo-labels, which allow benefiting from uncertain\nknowledge acquired during network training, are further employed to supervise\nsimilarity learning. Joint learning and iterative training facilitate to obtain\nan overall optimal solution. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our approach. By combining with the $k$-nearest\nneighbors algorithm, we further show that our method can address the\nlarge-scale and out-of-sample problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:25:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lv", "Juncheng", ""], ["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Xu", "Zenglin", ""]]}, {"id": "2104.04371", "submitter": "Babak Naderi", "authors": "Babak Naderi, Sebastian M\\\"oller, Ross Cutler", "title": "Speech Quality Assessment in Crowdsourcing: Comparison Category Rating\n  Method", "comments": "Accepted for QoMEX2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, Quality of Experience (QoE) for a communication system is\nevaluated through a subjective test. The most common test method for speech QoE\nis the Absolute Category Rating (ACR), in which participants listen to a set of\nstimuli, processed by the underlying test conditions, and rate their perceived\nquality for each stimulus on a specific scale. The Comparison Category Rating\n(CCR) is another standard approach in which participants listen to both\nreference and processed stimuli and rate their quality compared to the other\none. The CCR method is particularly suitable for systems that improve the\nquality of input speech. This paper evaluates an adaptation of the CCR test\nprocedure for assessing speech quality in the crowdsourcing set-up. The CCR\nmethod was introduced in the ITU-T Rec. P.800 for laboratory-based experiments.\nWe adapted the test for the crowdsourcing approach following the guidelines\nfrom ITU-T Rec. P.800 and P.808. We show that the results of the CCR procedure\nvia crowdsourcing are highly reproducible. We also compared the CCR test\nresults with widely used ACR test procedures obtained in the laboratory and\ncrowdsourcing. Our results show that the CCR procedure in crowdsourcing is a\nreliable and valid test method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:04:06 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Naderi", "Babak", ""], ["M\u00f6ller", "Sebastian", ""], ["Cutler", "Ross", ""]]}, {"id": "2104.04474", "submitter": "Chavit Denninnart", "authors": "Chavit Denninnart, Mohsen Amini Salehi", "title": "Harnessing the Potential of Function-Reuse in Multimedia Cloud Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based computing systems can get oversubscribed due to the budget\nconstraints of their users or limitations in certain resource types. The\noversubscription can, in turn, degrade the users perceived Quality of Service\n(QoS). The approach we investigate to mitigate both the oversubscription and\nthe incurred cost is based on smart reusing of the computation needed to\nprocess the service requests (i.e., tasks). We propose a reusing paradigm for\nthe tasks that are waiting for execution. This paradigm can be particularly\nimpactful in serverless platforms where multiple users can request similar\nservices simultaneously. Our motivation is a multimedia streaming engine that\nprocesses the media segments in an on-demand manner. We propose a mechanism to\nidentify various types of \"mergeable\" tasks and aggregate them to improve the\nQoS and mitigate the incurred cost. We develop novel approaches to determine\nwhen and how to perform task aggregation such that the QoS of other tasks is\nnot affected. Evaluation results show that the proposed mechanism can improve\nthe QoS by significantly reducing the percentage of tasks missing their\ndeadlines %. In addition, it can and reduce the overall time (and subsequently\nthe incurred cost) of utilizing cloud services by more than 9%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:45:53 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Denninnart", "Chavit", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2104.04669", "submitter": "Siwei Dong", "authors": "Siwei Dong, Tiejun Huang and Yonghong Tian", "title": "Spike Camera and Its Coding Methods", "comments": "Accepted in DCC 2017. arXiv admin note: text overlap with\n  arXiv:1907.08769", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a spike camera with a distinct video capture scheme and\nproposes two methods of decoding the spike stream for texture reconstruction.\nThe spike camera captures light and accumulates the converted luminance\nintensity at each pixel. A spike is fired when the accumulated intensity\nexceeds the dispatch threshold. The spike stream generated by the camera\nindicates the luminance variation. Analyzing the patterns of the spike stream\nmakes it possible to reconstruct the picture of any moment which enables the\nplayback of high speed movement.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 02:42:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dong", "Siwei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "2104.04678", "submitter": "Mansi Sharma", "authors": "Mansi Sharma, Santosh Kumar", "title": "A Flexible Lossy Depth Video Coding Scheme Based on Low-rank Tensor\n  Modelling and HEVC Intra Prediction for Free Viewpoint Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compression quality losses of depth sequences determine quality of view\nsynthesis in free-viewpoint video. The depth map intra prediction in 3D\nextensions of the HEVC applies intra modes with auxiliary depth modeling modes\n(DMMs) to better preserve depth edges and handle motion discontinuities.\nAlthough such modes enable high efficiency compression, but at the cost of very\nhigh encoding complexity. Skipping conventional intra coding modes and DMMs in\ndepth coding limits practical applicability of the HEVC for 3D display\napplications. In this paper, we introduce a novel low-complexity scheme for\ndepth video compression based on low-rank tensor decomposition and HEVC intra\ncoding. The proposed scheme leverages spatial and temporal redundancy by\ncompactly representing the depth sequence as a high-order tensor. Tensor\nfactorization into a set of factor matrices following CANDECOMP PARAFAC (CP)\ndecomposition via alternating least squares give a low-rank approximation of\nthe scene geometry. Further, compression of factor matrices with HEVC intra\nprediction support arbitrary target accuracy by flexible adjustment of bitrate,\nvarying tensor decomposition ranks and quantization parameters. The results\ndemonstrate proposed approach achieves significant rate gains by efficiently\ncompressing depth planes in low-rank approximated representation. The proposed\nalgorithm is applied to encode depth maps of benchmark Ballet and Breakdancing\nsequences. The decoded depth sequences are used for view synthesis in a\nmulti-view video system, maintaining appropriate rendering quality.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 04:07:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sharma", "Mansi", ""], ["Kumar", "Santosh", ""]]}, {"id": "2104.04765", "submitter": "Nitin Khanna Dr.", "authors": "Vinay Verma, Deepak Singh, and Nitin Khanna", "title": "Q-matrix Unaware Double JPEG Detection using DCT-Domain Deep BiLSTM\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The double JPEG compression detection has received much attention in recent\nyears due to its applicability as a forensic tool for the most widely used JPEG\nfile format. Existing state-of-the-art CNN-based methods either use histograms\nof all the frequencies or rely on heuristics to select histograms of specific\nlow frequencies to classify single and double compressed images. However, even\namidst lower frequencies of double compressed images/patches, histograms of all\nthe frequencies do not have distinguishable features to separate them from\nsingle compressed images. This paper directly extracts the quantized DCT\ncoefficients from the JPEG images without decompressing them in the pixel\ndomain, obtains all AC frequencies' histograms, uses a module based on $1\\times\n1$ depth-wise convolutions to learn the inherent relation between each\nhistogram and corresponding q-factor, and utilizes a tailor-made BiLSTM network\nfor selectively encoding these feature vector sequences. The proposed system\noutperforms several baseline methods on a relatively large and diverse publicly\navailable dataset of single and double compressed patches. Another essential\naspect of any single vs. double JPEG compression detection system is handling\nthe scenario where test patches are compressed with entirely different\nquantization matrices (Q-matrices) than those used while training; different\ncamera manufacturers and image processing software generally utilize their\ncustomized quantization matrices. A set of extensive experiments shows that the\nproposed system trained on a single dataset generalizes well on other datasets\ncompressed with completely unseen quantization matrices and outperforms the\nstate-of-the-art methods in both seen and unseen quantization matrices\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:41:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Verma", "Vinay", ""], ["Singh", "Deepak", ""], ["Khanna", "Nitin", ""]]}, {"id": "2104.05060", "submitter": "Mario Montagud Climent", "authors": "Mario Montagud, Jie Li, Gianluca Cernigliario, Abdallah El Ali, Sergi\n  Fernandez, Pablo Cesar", "title": "Towards SocialVR: Evaluating a Novel Technology for Watching Videos\n  Together", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social VR enables people to interact over distance with others in real-time.\nIt allows remote people, typically represented as avatars, to communicate and\nperform activities together in a join shared virtual environment, extending the\ncapabilities of traditional social platforms like Facebook and Netflix. This\npaper explores the benefits and drawbacks provided by a lightweight and\nlow-cost Social VR platform (SocialVR), in which users are captured by several\ncameras and reconstructed in real-time. In particular, the paper contributes\nwith (1) the design and evaluation of an experimental protocol for Social VR\nexperiences; (2) the report of a production workflow for this new type of media\nexperiences; and (3) the results of experiments with both end-users (N=15\npairs) and professionals (N=25) to evaluate the potential of the SocialVR\nplatform. Results from the questionnaires and semi-structured interviews show\nthat end-users rated positively towards the experiences provided by the\nSocialVR platform, which enabled them to sense emotions and communicate\neffortlessly. End-users perceived the photo-realistic experience of SocialVR\nsimilar to face-to-face scenarios and appreciated this new creative medium.\nFrom a commercial perspective, professionals confirmed the potential of this\ncommunication medium and encourage further research for the adoption of the\nplatform in the commercial landscape\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 17:34:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Montagud", "Mario", ""], ["Li", "Jie", ""], ["Cernigliario", "Gianluca", ""], ["Ali", "Abdallah El", ""], ["Fernandez", "Sergi", ""], ["Cesar", "Pablo", ""]]}, {"id": "2104.05845", "submitter": "Yue Yang", "authors": "Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar,\n  Chris Callison-Burch", "title": "Visual Goal-Step Inference using wikiHow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural events can often be thought of as a high level goal composed of a\nsequence of steps. Inferring the sub-sequence of steps of a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task where a model\nis given a textual goal and must choose a plausible step towards that goal from\namong four candidate images. Our task is challenging for state-of-the-art\nmuitimodal models. We introduce a novel dataset harvested from wikiHow that\nconsists of 772,294 images representing human actions. We show that the\nknowledge learned from our data can effectively transfer to other datasets like\nHowTo100M, increasing the multiple-choice accuracy by 15% to 20%. Our task will\nfacilitate multi-modal reasoning about procedural events.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:20:09 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yang", "Yue", ""], ["Panagopoulou", "Artemis", ""], ["Lyu", "Qing", ""], ["Zhang", "Li", ""], ["Yatskar", "Mark", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2104.05947", "submitter": "Mohit Chandra", "authors": "Mohit Chandra, Dheeraj Pailla, Himanshu Bhatia, Aadilmehdi Sanchawala,\n  Manish Gupta, Manish Shrivastava, Ponnurangam Kumaraguru", "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using\n  Multimodal Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 05:22:55 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:05:21 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 19:30:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chandra", "Mohit", ""], ["Pailla", "Dheeraj", ""], ["Bhatia", "Himanshu", ""], ["Sanchawala", "Aadilmehdi", ""], ["Gupta", "Manish", ""], ["Shrivastava", "Manish", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2104.06162", "submitter": "Xudong Xu", "authors": "Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, Dahua Lin", "title": "Visually Informed Binaural Audio Generation without Binaural Audios", "comments": "Accepted by CVPR 2021. Code, models, and demo video are available on\n  our webpage: \\<https://sheldontsui.github.io/projects/PseudoBinaural>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stereophonic audio, especially binaural audio, plays an essential role in\nimmersive viewing environments. Recent research has explored generating\nvisually guided stereophonic audios supervised by multi-channel audio\ncollections. However, due to the requirement of professional recording devices,\nexisting datasets are limited in scale and variety, which impedes the\ngeneralization of supervised methods in real-world scenarios. In this work, we\npropose PseudoBinaural, an effective pipeline that is free of binaural\nrecordings. The key insight is to carefully build pseudo visual-stereo pairs\nwith mono data for training. Specifically, we leverage spherical harmonic\ndecomposition and head-related impulse response (HRIR) to identify the\nrelationship between spatial locations and received binaural audios. Then in\nthe visual modality, corresponding visual cues of the mono data are manually\nplaced at sound source positions to form the pairs. Compared to\nfully-supervised paradigms, our binaural-recording-free pipeline shows great\nstability in cross-dataset evaluation and achieves comparable performance under\nsubjective preference. Moreover, combined with binaural recordings, our method\nis able to further boost the performance of binaural audio generation under\nsupervised settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:07:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Xudong", ""], ["Zhou", "Hang", ""], ["Liu", "Ziwei", ""], ["Dai", "Bo", ""], ["Wang", "Xiaogang", ""], ["Lin", "Dahua", ""]]}, {"id": "2104.06183", "submitter": "Ying Cui", "authors": "Chengjun Guo, Ying Cui, Zhi Liu and Derrick Wing Kwan Ng", "title": "Optimal Transmission of Multi-Quality Tiled 360 VR Video in MIMO-OFDMA\n  Systems", "comments": "6 pages, 4 figures, to appear in IEEE ICC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the optimal transmission of a multi-quality tiled 360\nvirtual reality (VR) video from a multi-antenna server (e.g., access point or\nbase station) to multiple single-antenna users in a multiple-input\nmultiple-output (MIMO)-orthogonal frequency division multiple access (OFDMA)\nsystem. We minimize the total transmission power with respect to the subcarrier\nallocation constraints, rate allocation constraints, and successful\ntransmission constraints, by optimizing the beamforming vector and subcarrier,\ntransmission power and rate allocation. The formulated resource allocation\nproblem is a challenging mixed discrete-continuous optimization problem. We\nobtain an asymptotically optimal solution in the case of a large antenna array,\nand a suboptimal solution in the general case. As far as we know, this is the\nfirst work providing optimization-based design for 360 VR video transmission in\nMIMO-OFDMA systems. Finally, by numerical results, we show that the proposed\nsolutions achieve significant improvement in performance compared to the\nexisting solutions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:34:09 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Guo", "Chengjun", ""], ["Cui", "Ying", ""], ["Liu", "Zhi", ""], ["Ng", "Derrick Wing Kwan", ""]]}, {"id": "2104.06517", "submitter": "Eunjeong Koh", "authors": "Eunjeong Koh and Shlomo Dubnov", "title": "Comparison and Analysis of Deep Audio Embeddings for Music Emotion\n  Recognition", "comments": "AAAI Workshop on Affective Content Analysis 2021 Camera Ready Version", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion is a complicated notion present in music that is hard to capture even\nwith fine-tuned feature engineering. In this paper, we investigate the utility\nof state-of-the-art pre-trained deep audio embedding methods to be used in the\nMusic Emotion Recognition (MER) task. Deep audio embedding methods allow us to\nefficiently capture the high dimensional features into a compact\nrepresentation. We implement several multi-class classifiers with deep audio\nembeddings to predict emotion semantics in music. We investigate the\neffectiveness of L3-Net and VGGish deep audio embedding methods for music\nemotion inference over four music datasets. The experiments with several\nclassifiers on the task show that the deep audio embedding solutions can\nimprove the performances of the previous baseline MER models. We conclude that\ndeep audio embeddings represent musical emotion semantics for the MER task\nwithout expert human engineering.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 21:09:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Koh", "Eunjeong", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2104.06876", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Gene Cheung, Pascal Frossard, H.Vicky Zhao and Jiwu Huang", "title": "Landmarking for Navigational Streaming of Stored High-Dimensional Media", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern media data such as 360 videos and light field (LF) images are\ntypically captured in much higher dimensions than the observers' visual\ndisplays. To efficiently browse high-dimensional media over\nbandwidth-constrained networks, a navigational streaming model is considered: a\nclient navigates the large media space by dictating a navigation path to a\nserver, who in response transmits the corresponding pre-encoded media data\nunits (MDU) to the client one-by-one in sequence. Intra-coding an MDU (I-MDU)\nwould result in a large bitrate but I-MDU can be randomly accessed, while\ninter-coding an MDU (P-MDU) using another MDU as a predictor incurs a small\ncoding cost but imposes an order where the predictor must be first transmitted\nand decoded. From a compression perspective, the technical challenge is: how to\nachieve coding gain via inter-coding of MDUs, while enabling adequate random\naccess for satisfactory user navigation. To address this problem, we propose\nlandmarks, a selection of key MDUs from the high-dimensional media. Using\nlandmarks as predictors, nearby MDUs in local neighborhoods are intercoded,\nresulting in a predictive MDU structure with controlled coding cost. It means\nthat any requested MDU can be decoded by at most transmitting a landmark and an\ninter-coded MDU, enabling navigational random access. To build a landmarked MDU\nstructure, we employ tree-structured vector quantizer (TSVQ) to first optimize\nlandmark locations, then iteratively add/remove inter-coded MDUs as refinements\nusing a fast branch-and-bound technique. Taking interactive LF images and\nviewport adaptive 360 images as illustrative applications, and I-, P- and\npreviously proposed merge frames to intra- and inter-code MDUs, we show\nexperimentally that landmarked MDU structures can noticeably reduce the\nexpected transmission cost compared with MDU structures without landmarks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:18:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yuan", "Yuan", ""], ["Cheung", "Gene", ""], ["Frossard", "Pascal", ""], ["Zhao", "H. Vicky", ""], ["Huang", "Jiwu", ""]]}, {"id": "2104.07473", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach,\n  Chenliang Xu", "title": "Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video\n  Super-Resolution", "comments": "Journal version of \"Zooming Slow-Mo: Fast and Accurate One-Stage\n  Space-Time Video Super-Resolution\"(CVPR-2020). 14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the space-time video super-resolution, which aims\nat generating a high-resolution (HR) slow-motion video from a low-resolution\n(LR) and low frame rate (LFR) video sequence. A na\\\"ive method is to decompose\nit into two sub-tasks: video frame interpolation (VFI) and video\nsuper-resolution (VSR). Nevertheless, temporal interpolation and spatial\nupscaling are intra-related in this problem. Two-stage approaches cannot fully\nmake use of this natural property. Besides, state-of-the-art VFI or VSR deep\nnetworks usually have a large frame reconstruction module in order to obtain\nhigh-quality photo-realistic video frames, which makes the two-stage approaches\nhave large models and thus be relatively time-consuming. To overcome the\nissues, we present a one-stage space-time video super-resolution framework,\nwhich can directly reconstruct an HR slow-motion video sequence from an input\nLR and LFR video. Instead of reconstructing missing LR intermediate frames as\nVFI models do, we temporally interpolate LR frame features of the missing LR\nframes capturing local temporal contexts by a feature temporal interpolation\nmodule. Extensive experiments on widely used benchmarks demonstrate that the\nproposed framework not only achieves better qualitative and quantitative\nperformance on both clean and noisy LR frames but also is several times faster\nthan recent state-of-the-art two-stage networks. The source code is released in\nhttps://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 .\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Tian", "Yapeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Allebach", "Jan P.", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.07569", "submitter": "Monu Verma", "authors": "Monu Verma (Student, Member, IEEE), Santosh Kumar Vipparthi (Member,\n  IEEE), and Girdhari Singh", "title": "AffectiveNet: Affective-Motion Feature Learningfor Micro Expression\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Micro-expressions are hard to spot due to fleeting and involuntary moments of\nfacial muscles. Interpretation of micro emotions from video clips is a\nchallenging task. In this paper we propose an affective-motion imaging that\ncumulates rapid and short-lived variational information of micro expressions\ninto a single response. Moreover, we have proposed an\nAffectiveNet:affective-motion feature learning network that can perceive subtle\nchanges and learns the most discriminative dynamic features to describe the\nemotion classes. The AffectiveNet holds two blocks: MICRoFeat and MFL block.\nMICRoFeat block conserves the scale-invariant features, which allows network to\ncapture both coarse and tiny edge variations. While MFL block learns\nmicro-level dynamic variations from two different intermediate convolutional\nlayers. Effectiveness of the proposed network is tested over four datasets by\nusing two experimental setups: person independent (PI) and cross dataset (CD)\nvalidation. The experimental results of the proposed network outperforms the\nstate-of-the-art approaches with significant margin for MER approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:24:56 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Verma", "Monu", "", "Student, Member, IEEE"], ["Vipparthi", "Santosh Kumar", "", "Member,\n  IEEE"], ["Singh", "Girdhari", ""]]}, {"id": "2104.07719", "submitter": "Guangxing Han", "authors": "Guangxing Han, Shiyuan Huang, Jiawei Ma, Yicheng He, Shih-Fu Chang", "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with\n  Attentive Feature Alignment", "comments": "14 pages; Typos corrected and references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection (FSOD) aims to detect objects using only few\nexamples. It's critically needed for many practical applications but so far\nremains challenging. We propose a meta-learning based few-shot object detection\nmethod by transferring meta-knowledge learned from data-abundant base classes\nto data-scarce novel classes. Our method incorporates a coarse-to-fine approach\ninto the proposal based object detection framework and integrates prototype\nbased classifiers into both the proposal generation and classification stages.\nTo improve proposal generation for few-shot novel classes, we propose to learn\na lightweight matching network to measure the similarity between each spatial\nposition in the query image feature map and spatially-pooled class features,\ninstead of the traditional object/nonobject classifier, thus generating\ncategory-specific proposals and improving proposal recall for novel classes. To\naddress the spatial misalignment between generated proposals and few-shot class\nexamples, we propose a novel attentive feature alignment method, thus improving\nthe performance of few-shot object detection. Meanwhile we jointly learn a\nFaster R-CNN detection head for base classes. Extensive experiments conducted\non multiple FSOD benchmarks show our proposed approach achieves state of the\nart results under (incremental) few-shot learning settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:01:27 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:30:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Guangxing", ""], ["Huang", "Shiyuan", ""], ["Ma", "Jiawei", ""], ["He", "Yicheng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2104.08328", "submitter": "Ekrem \\c{C}etinkaya", "authors": "Ekrem Cetinkaya, Hadi Amirpour, Mohammad Ghanbari, and Christian\n  Timmerer", "title": "CTU Depth Decision Algorithms for HEVC: A Survey", "comments": "27 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High-Efficiency Video Coding (HEVC) surpasses its predecessors in encoding\nefficiency by introducing new coding tools at the cost of an increased encoding\ntime-complexity. The Coding Tree Unit (CTU) is the main building block used in\nHEVC. In the HEVC standard, frames are divided into CTUs with the predetermined\nsize of up to 64x64 pixels. Each CTU is then divided recursively into a number\nof equally sized square areas, known as Coding Units (CUs). Although this\ndiversity of frame partitioning increases encoding efficiency, it also causes\nan increase in the time complexity due to the increased number of ways to find\nthe optimal partitioning. To address this complexity, numerous algorithms have\nbeen proposed to eliminate unnecessary searches during partitioning CTUs by\nexploiting the correlation in the video. In this paper, existing CTU depth\ndecision algorithms for HEVC are surveyed. These algorithms are categorized\ninto two groups, namely statistics and machine learning approaches. Statistics\napproaches are further subdivided into neighboring and inherent approaches.\nNeighboring approaches exploit the similarity between adjacent CTUs to limit\nthe depth range of the current CTU, while inherent approaches use only the\navailable information within the current CTU. Machine learning approaches try\nto extract and exploit similarities implicitly. Traditional methods like\nsupport vector machines or random forests use manually selected features, while\nrecently proposed deep learning methods extract features during training.\nFinally, this paper discusses extending these methods to more recent video\ncoding formats such as Versatile Video Coding (VVC) and AOMedia Video 1(AV1).\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:24:29 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 06:59:56 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Cetinkaya", "Ekrem", ""], ["Amirpour", "Hadi", ""], ["Ghanbari", "Mohammad", ""], ["Timmerer", "Christian", ""]]}, {"id": "2104.08510", "submitter": "Meng Liu", "authors": "Meng Liu, Longbiao Wang, Kong Aik Lee, Hanyi Zhang, Chang Zeng, Jianwu\n  Dang", "title": "Exploring Deep Learning for Joint Audio-Visual Lip Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-visual (AV) lip biometrics is a promising authentication technique that\nleverages the benefits of both the audio and visual modalities in speech\ncommunication. Previous works have demonstrated the usefulness of AV lip\nbiometrics. However, the lack of a sizeable AV database hinders the exploration\nof deep-learning-based audio-visual lip biometrics. To address this problem, we\ncompile a moderate-size database using existing public databases. Meanwhile, we\nestablish the DeepLip AV lip biometrics system realized with a convolutional\nneural network (CNN) based video module, a time-delay neural network (TDNN)\nbased audio module, and a multimodal fusion module. Our experiments show that\nDeepLip outperforms traditional speaker recognition models in context modeling\nand achieves over 50% relative improvements compared with our best single\nmodality baseline, with an equal error rate of 0.75% and 1.11% on the test\ndatasets, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 10:51:55 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Meng", ""], ["Wang", "Longbiao", ""], ["Lee", "Kong Aik", ""], ["Zhang", "Hanyi", ""], ["Zeng", "Chang", ""], ["Dang", "Jianwu", ""]]}, {"id": "2104.08592", "submitter": "Paulo Nuno Vicente", "authors": "Loup M. Langton, Mercedes L. de Uriarte, Kim Grinfeder, Paulo Nuno\n  Vicente", "title": "New Technology, New Rules for Journalism and a New World of Engagement", "comments": "Proceedings WJEC 2019, Paris", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ways in which people learn, communicate and engage in discussion have\nchanged profoundly during the past decade. As Jenkins related in her book, The\nConvergence Crisis: An Impending Paradigm Shift in Advertising, Millenials do\nnot want to be told the whole story. Rather, they want someone to begin a\nconversation that will engage others to become participants in the development\nof that story (2015). Technology now allows that to happen, sometimes with\nunintended and/or ill consequences, but technology also generates a dynamic\npotential to create international and interactive discourse aimed at addressing\nshared global challenges.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 16:33:43 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Langton", "Loup M.", ""], ["de Uriarte", "Mercedes L.", ""], ["Grinfeder", "Kim", ""], ["Vicente", "Paulo Nuno", ""]]}, {"id": "2104.08910", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu", "title": "Towards Open-World Text-Guided Face Image Generation and Manipulation", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.03308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing text-guided image synthesis methods can only produce limited\nquality results with at most \\mbox{$\\text{256}^2$} resolution and the textual\ninstructions are constrained in a small Corpus. In this work, we propose a\nunified framework for both face image generation and manipulation that produces\ndiverse and high-quality images with an unprecedented resolution at 1024 from\nmultimodal inputs. More importantly, our method supports open-world scenarios,\nincluding both image and text, without any re-training, fine-tuning, or\npost-processing. To be specific, we propose a brand new paradigm of text-guided\nimage generation and manipulation based on the superior characteristics of a\npretrained GAN model. Our proposed paradigm includes two novel strategies. The\nfirst strategy is to train a text encoder to obtain latent codes that align\nwith the hierarchically semantic of the aforementioned pretrained GAN model.\nThe second strategy is to directly optimize the latent codes in the latent\nspace of the pretrained GAN model with guidance from a pretrained language\nmodel. The latent codes can be randomly sampled from a prior distribution or\ninverted from a given image, which provides inherent supports for both image\ngeneration and manipulation from multi-modal inputs, such as sketches or\nsemantic labels, with textual guidance. To facilitate text-guided multi-modal\nsynthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset\nconsisting of real face images and corresponding semantic segmentation map,\nsketch, and textual descriptions. Extensive experiments on the introduced\ndataset demonstrate the superior performance of our proposed method. Code and\ndata are available at https://github.com/weihaox/TediGAN.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:56:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Wu", "Baoyuan", ""]]}, {"id": "2104.09036", "submitter": "Yanqiao Zhu", "authors": "Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, Liang Wang", "title": "Mining Latent Structures for Multimedia Recommendation", "comments": "Accepted to ACM Multimedia 2021. Authors' version", "journal-ref": null, "doi": "10.1145/3474085.3475259", "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia content is of predominance in the modern Web era. Investigating\nhow users interact with multimodal items is a continuing concern within the\nrapid development of recommender systems. The majority of previous work focuses\non modeling user-item interactions with multimodal features included as side\ninformation. However, this scheme is not well-designed for multimedia\nrecommendation. Specifically, only collaborative item-item relationships are\nimplicitly modeled through high-order item-user-item relations. Considering\nthat items are associated with rich contents in multiple modalities, we argue\nthat the latent semantic item-item structures underlying these multimodal\ncontents could be beneficial for learning better item representations and\nfurther boosting recommendation. To this end, we propose a LATent sTructure\nmining method for multImodal reCommEndation, which we term LATTICE for brevity.\nTo be specific, in the proposed LATTICE model, we devise a novel modality-aware\nstructure learning layer, which learns item-item structures for each modality\nand aggregates multiple modalities to obtain latent item graphs. Based on the\nlearned latent graphs, we perform graph convolutions to explicitly inject\nhigh-order item affinities into item representations. These enriched item\nrepresentations can then be plugged into existing collaborative filtering\nmethods to make more accurate recommendations. Extensive experiments on three\nreal-world datasets demonstrate the superiority of our method over\nstate-of-the-art multimedia recommendation methods and validate the efficacy of\nmining latent item-item relationships from multimodal features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 03:50:24 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 02:27:33 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhang", "Jinghao", ""], ["Zhu", "Yanqiao", ""], ["Liu", "Qiang", ""], ["Wu", "Shu", ""], ["Wang", "Shuhui", ""], ["Wang", "Liang", ""]]}, {"id": "2104.09411", "submitter": "Yong Liu Stephen", "authors": "Chenyi Lei, Shixian Luo, Yong Liu, Wanggui He, Jiamang Wang, Guoxin\n  Wang, Haihong Tang, Chunyan Miao, Houqiang Li", "title": "Understanding Chinese Video and Language via Contrastive Multimodal\n  Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pre-trained neural models have recently achieved impressive performances\nin understanding multimodal content. However, it is still very challenging to\npre-train neural models for video and language understanding, especially for\nChinese video-language data, due to the following reasons. Firstly, existing\nvideo-language pre-training algorithms mainly focus on the co-occurrence of\nwords and video frames, but ignore other valuable semantic and structure\ninformation of video-language content, e.g., sequential order and\nspatiotemporal relationships. Secondly, there exist conflicts between video\nsentence alignment and other proxy tasks. Thirdly, there is a lack of\nlarge-scale and high-quality Chinese video-language datasets (e.g., including\n10 million unique videos), which are the fundamental success conditions for\npre-training techniques.\n  In this work, we propose a novel video-language understanding framework named\nVICTOR, which stands for VIdeo-language understanding via Contrastive\nmulTimOdal pRe-training. Besides general proxy tasks such as masked language\nmodeling, VICTOR constructs several novel proxy tasks under the contrastive\nlearning paradigm, making the model be more robust and able to capture more\ncomplex multimodal semantic and structural relationships from different\nperspectives. VICTOR is trained on a large-scale Chinese video-language\ndataset, including over 10 million complete videos with corresponding\nhigh-quality textual descriptions. We apply the pre-trained VICTOR model to a\nseries of downstream applications and demonstrate its superior performances,\ncomparing against the state-of-the-art pre-training methods such as VideoBERT\nand UniVL. The codes and trained checkpoints will be publicly available to\nnourish further developments of the research community.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:58:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lei", "Chenyi", ""], ["Luo", "Shixian", ""], ["Liu", "Yong", ""], ["He", "Wanggui", ""], ["Wang", "Jiamang", ""], ["Wang", "Guoxin", ""], ["Tang", "Haihong", ""], ["Miao", "Chunyan", ""], ["Li", "Houqiang", ""]]}, {"id": "2104.09779", "submitter": "Xing Wei", "authors": "Xing Wei and Chenyang Yang", "title": "Privacy-aware VR streaming", "comments": "9 pages, 7 figures, submit to ACM for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive tile-based virtual reality (VR) video streaming employs the current\ntracking data of a user to predict future requested tiles, then renders and\ndelivers the predicted tiles to be requested before playback. The quality of\nexperience (QoE) depends on the overall performance of prediction, computing\n(i.e., rendering) and communication. All prior works neglect that users may\nhave privacy requirement, i.e., not all the current tracking data are allowed\nto be uploaded. In this paper, we investigate the privacy-aware VR streaming.\nWe first establish a dataset that collects the privacy requirement of 66 users\namong 18 panoramic videos. The dataset shows that the privacy requirements of\n360$^{\\circ}$ videos are heterogeneous. Only 41\\% of the total watched videos\nhave no privacy requirement. Based on these findings, we formulate the privacy\nrequirement as the \\textit{degree of privacy} (DoP), and investigate the impact\nof DoP on the proactive VR streaming. First, we find that with DoP, the length\nof the observation window and prediction window of a tile predictor should be\nvariable. Then, we jointly optimize the durations for computing and\ntransmitting the selected tiles as well as the computing and communication\ncapability, aimed at maximizing the QoE given arbitrary predictor and\nconfigured resources. From the obtained optimal closed-form solution, we find a\nresource-saturated region where DoP has no impact on the QoE and a\nresource-unsaturated region where the two-fold impacts of DoP are\ncontradictory. On the one hand, the increase of DoP will degrade the prediction\nperformance and thus degrade the QoE. On the other hand, the increase of DoP\nwill improve the capability of computing and communication and thus improve the\nQoE. Simulation results using two predictors and a real dataset validate the\nanalysis and demonstrate the overall impact of DoP on the QoE.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:28:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wei", "Xing", ""], ["Yang", "Chenyang", ""]]}, {"id": "2104.09832", "submitter": "Tianyun Liu", "authors": "Tianyun Liu and Diqun Yan", "title": "Identification of fake stereo audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Channel is one of the important criterions for digital audio quality.\nGeneral-ly, stereo audio two channels can provide better perceptual quality\nthan mono audio. To seek illegal commercial benefit, one might convert mono\naudio to stereo one with fake quality. Identifying of stereo faking audio is\nstill a less-investigated audio forensic issue. In this paper, a stereo faking\ncorpus is first present, which is created by Haas Effect technique. Then the\neffect of stereo faking on Mel Frequency Cepstral Coefficients (MFCC) is\nanalyzed to find the difference between the real and faked stereo audio.\nFi-nally, an effective algorithm for identifying stereo faking audio is\nproposed, in which 80-dimensional MFCC features and Support Vector Machine\n(SVM) classifier are adopted. The experimental results on three datasets with\nfive different cut-off frequencies show that the proposed algorithm can\nef-fectively detect stereo faking audio and achieve a good robustness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:35:38 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Liu", "Tianyun", ""], ["Yan", "Diqun", ""]]}, {"id": "2104.10054", "submitter": "Xiaohan Wang", "authors": "Xiaohan Wang, Linchao Zhu, Yi Yang", "title": "T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-video retrieval is a challenging task that aims to search relevant video\ncontents based on natural language descriptions. The key to this problem is to\nmeasure text-video similarities in a joint embedding space. However, most\nexisting methods only consider the global cross-modal similarity and overlook\nthe local details. Some works incorporate the local comparisons through\ncross-modal local matching and reasoning. These complex operations introduce\ntremendous computation. In this paper, we design an efficient global-local\nalignment method. The multi-modal video sequences and text features are\nadaptively aggregated with a set of shared semantic centers. The local\ncross-modal similarities are computed between the video feature and text\nfeature within the same center. This design enables the meticulous local\ncomparison and reduces the computational cost of the interaction between each\ntext-video pair. Moreover, a global alignment method is proposed to provide a\nglobal cross-modal measurement that is complementary to the local perspective.\nThe global aggregated visual features also provide additional supervision,\nwhich is indispensable to the optimization of the learnable semantic centers.\nWe achieve consistent improvements on three standard text-video retrieval\nbenchmarks and outperform the state-of-the-art by a clear margin.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:26:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Xiaohan", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "2104.10116", "submitter": "Joshua Ebenezer", "authors": "Joshua P. Ebenezer, Yongjun Wu, Hai Wei, Sriram Sethuraman, Zongyi Liu", "title": "Detection of Audio-Video Synchronization Errors Via Event Detection", "comments": "To be published in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new method and a large-scale database to detect audio-video\nsynchronization(A/V sync) errors in tennis videos. A deep network is trained to\ndetect the visual signature of the tennis ball being hit by the racquet in the\nvideo stream. Another deep network is trained to detect the auditory signature\nof the same event in the audio stream. During evaluation, the audio stream is\nsearched by the audio network for the audio event of the ball being hit. If the\nevent is found in audio, the neighboring interval in video is searched for the\ncorresponding visual signature. If the event is not found in the video stream\nbut is found in the audio stream, A/V sync error is flagged. We developed a\nlarge-scaled database of 504,300 frames from 6 hours of videos of tennis\nevents, simulated A/V sync errors, and found our method achieves high accuracy\non the task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:54:44 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ebenezer", "Joshua P.", ""], ["Wu", "Yongjun", ""], ["Wei", "Hai", ""], ["Sethuraman", "Sriram", ""], ["Liu", "Zongyi", ""]]}, {"id": "2104.10557", "submitter": "Tingtian Li", "authors": "Tingtian Li, Zixun Sun, Haoruo Zhang, Jin Li, Ziming Wu, Hui Zhan,\n  Yipeng Yu, Hengcan Shi", "title": "Deep Music Retrieval for Fine-Grained Videos by Exploiting\n  Cross-Modal-Encoded Voice-Overs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the witness of the rapidly growing popularity of short videos on\ndifferent Internet platforms has intensified the need for a background music\n(BGM) retrieval system. However, existing video-music retrieval methods only\nbased on the visual modality cannot show promising performance regarding videos\nwith fine-grained virtual contents. In this paper, we also investigate the\nwidely added voice-overs in short videos and propose a novel framework to\nretrieve BGM for fine-grained short videos. In our framework, we use the\nself-attention (SA) and the cross-modal attention (CMA) modules to explore the\nintra- and the inter-relationships of different modalities respectively. For\nbalancing the modalities, we dynamically assign different weights to the modal\nfeatures via a fusion gate. For paring the query and the BGM embeddings, we\nintroduce a triplet pseudo-label loss to constrain the semantics of the modal\nembeddings. As there are no existing virtual-content video-BGM retrieval\ndatasets, we build and release two virtual-content video datasets HoK400 and\nCFM400. Experimental results show that our method achieves superior performance\nand outperforms other state-of-the-art methods with large margins.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:29:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Tingtian", ""], ["Sun", "Zixun", ""], ["Zhang", "Haoruo", ""], ["Li", "Jin", ""], ["Wu", "Ziming", ""], ["Zhan", "Hui", ""], ["Yu", "Yipeng", ""], ["Shi", "Hengcan", ""]]}, {"id": "2104.11037", "submitter": "Antonios Liapis", "authors": "Antonios Liapis", "title": "10 Years of the PCG workshop: Past and Future Trends", "comments": "10 pages", "journal-ref": "Proceedings of the FDG Workshop on Procedural Content Generation,\n  2020", "doi": "10.1145/3402942.3409598", "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of 2020, the international workshop on Procedural Content Generation\nenters its second decade. The annual workshop, hosted by the international\nconference on the Foundations of Digital Games, has collected a corpus of 95\npapers published in its first 10 years. This paper provides an overview of the\nworkshop's activities and surveys the prevalent research topics emerging over\nthe years.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:02:29 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Liapis", "Antonios", ""]]}, {"id": "2104.11116", "submitter": "Hang Zhou", "authors": "Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang,\n  Ziwei Liu", "title": "Pose-Controllable Talking Face Generation by Implicitly Modularized\n  Audio-Visual Representation", "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2021. Code and models are available at\n  https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While accurate lip synchronization has been achieved for arbitrary-subject\naudio-driven talking face generation, the problem of how to efficiently drive\nthe head pose remains. Previous methods rely on pre-estimated structural\ninformation such as landmarks and 3D parameters, aiming to generate\npersonalized rhythmic movements. However, the inaccuracy of such estimated\ninformation under extreme conditions would lead to degradation problems. In\nthis paper, we propose a clean yet effective framework to generate\npose-controllable talking faces. We operate on raw face images, using only a\nsingle photo as an identity reference. The key is to modularize audio-visual\nrepresentations by devising an implicit low-dimension pose code. Substantially,\nboth speech content and head pose information lie in a joint non-identity\nembedding space. While speech content information can be defined by learning\nthe intrinsic synchronization between audio-visual modalities, we identify that\na pose code will be complementarily learned in a modulated convolution-based\nreconstruction framework.\n  Extensive experiments show that our method generates accurately lip-synced\ntalking faces whose poses are controllable by other videos. Moreover, our model\nhas multiple advanced capabilities including extreme view robustness and\ntalking face frontalization. Code, models, and demo videos are available at\nhttps://hangz-nju-cuhk.github.io/projects/PC-AVS.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:10:26 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhou", "Hang", ""], ["Sun", "Yasheng", ""], ["Wu", "Wayne", ""], ["Loy", "Chen Change", ""], ["Wang", "Xiaogang", ""], ["Liu", "Ziwei", ""]]}, {"id": "2104.11178", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu\n  Chang, Yin Cui, Boqing Gong", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw\n  Video, Audio and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:07:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Akbari", "Hassan", ""], ["Yuan", "Linagzhe", ""], ["Qian", "Rui", ""], ["Chuang", "Wei-Hong", ""], ["Chang", "Shih-Fu", ""], ["Cui", "Yin", ""], ["Gong", "Boqing", ""]]}, {"id": "2104.11317", "submitter": "Mahmoud Darwich", "authors": "Mahmoud Darwich, Yasser Ismail, Talal Darwich, Magdy Bayoumi", "title": "Improving Hierarchy Storage for Video Streaming in Cloud", "comments": null, "journal-ref": "2021 IEEE 7th World Forum on Internet of Things (WF-IoT)", "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Frequently accessed video streams are pre-transcoded into several formats to\nsatisfy the characteristics of all display devices. Storing several video\nstream formats imposes a high cost on video stream providers using the old\nclassical way. Alternatively, cloud providers offer a high flexibility of using\ntheir services and at a low cost relatively. Therefore, video stream companies\nadopted cloud technology to store their video streams. Generally, having all\nvideo streams stored in one type of cloud storage, the cost rises gradually.\nMore importantly, the variation of the access pattern to frequently accessed\nvideo streams impacts negatively the storage cost and increases it\nsignificantly. To optimize storage usage and lower its cost, we propose a\nmethod that manages the cloud hierarchy storage. Particularly, we develop an\nalgorithm that operates on parts of different videos that are frequently\naccessed and stores them in their suitable storage type cloud. Experiments came\nup with promising results on reducing the cost of using cloud storage by 18.75\n%.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 20:56:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Darwich", "Mahmoud", ""], ["Ismail", "Yasser", ""], ["Darwich", "Talal", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "2104.11530", "submitter": "Junaid Ahmed Ghauri", "authors": "Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth", "title": "Supervised Video Summarization via Multiple Feature Sets with Parallel\n  Attention", "comments": "Accepted in IEEE International Conference on Multimedia and Expo\n  (ICME) 2021 (They have copyright to publish camera ready version of this\n  work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assignment of importance scores to particular frames or (short) segments\nin a video is crucial for summarization, but also a difficult task. Previous\nwork utilizes only one source of visual features. In this paper, we suggest a\nnovel model architecture that combines three feature sets for visual content\nand motion to predict importance scores. The proposed architecture utilizes an\nattention mechanism before fusing motion features and features representing the\n(static) visual content, i.e., derived from an image classification model.\nComprehensive experimental evaluations are reported for two well-known\ndatasets, SumMe and TVSum. In this context, we identify methodological issues\non how previous work used these benchmark datasets, and present a fair\nevaluation scheme with appropriate data splits that can be used in future work.\nWhen using static and motion features with parallel attention mechanism, we\nimprove state-of-the-art results for SumMe, while being on par with the state\nof the art for the other dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:46:35 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 16:07:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ghauri", "Junaid Ahmed", ""], ["Hakimov", "Sherzod", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.11560", "submitter": "Wenliang Dai", "authors": "Wenliang Dai, Samuel Cahyawijaya, Yejin Bang, Pascale Fung", "title": "Weakly-supervised Multi-task Learning for Multimodal Affect Recognition", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal affect recognition constitutes an important aspect for enhancing\ninterpersonal relationships in human-computer interaction. However, relevant\ndata is hard to come by and notably costly to annotate, which poses a\nchallenging barrier to build robust multimodal affect recognition systems.\nModels trained on these relatively small datasets tend to overfit and the\nimprovement gained by using complex state-of-the-art models is marginal\ncompared to simple baselines. Meanwhile, there are many different multimodal\naffect recognition datasets, though each may be small. In this paper, we\npropose to leverage these datasets using weakly-supervised multi-task learning\nto improve the generalization performance on each of them. Specifically, we\nexplore three multimodal affect recognition tasks: 1) emotion recognition; 2)\nsentiment analysis; and 3) sarcasm recognition. Our experimental results show\nthat multi-tasking can benefit all these tasks, achieving an improvement up to\n2.9% accuracy and 3.3% F1-score. Furthermore, our method also helps to improve\nthe stability of model performance. In addition, our analysis suggests that\nweak supervision can provide a comparable contribution to strong supervision if\nthe tasks are highly correlated.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:36:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dai", "Wenliang", ""], ["Cahyawijaya", "Samuel", ""], ["Bang", "Yejin", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.11568", "submitter": "Lorin Sweeney", "authors": "Lorin Sweeney, Graham Healy, Alan F. Smeaton", "title": "The Influence of Audio on Video Memorability with an Audio Gestalt\n  Regulated Video Memorability System", "comments": "6 pages, 3 figures, 4 tables, paper accepted in CBMI 2021 for\n  publication and oral presentation", "journal-ref": null, "doi": "10.1109/CBMI50038.2021.9461903", "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memories are the tethering threads that tie us to the world, and memorability\nis the measure of their tensile strength. The threads of memory are spun from\nfibres of many modalities, obscuring the contribution of a single fibre to a\nthread's overall tensile strength. Unfurling these fibres is the key to\nunderstanding the nature of their interaction, and how we can ultimately create\nmore meaningful media content. In this paper, we examine the influence of audio\non video recognition memorability, finding evidence to suggest that it can\nfacilitate overall video recognition memorability rich in high-level (gestalt)\naudio features. We introduce a novel multimodal deep learning-based late-fusion\nsystem that uses audio gestalt to estimate the influence of a given video's\naudio on its overall short-term recognition memorability, and selectively\nleverages audio features to make a prediction accordingly. We benchmark our\naudio gestalt based system on the Memento10k short-term video memorability\ndataset, achieving top-2 state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:53:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sweeney", "Lorin", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2104.12357", "submitter": "Yuzhi Zhao", "authors": "Yuzhi Zhao, Lai-Man Po, Wing-Yin Yu, Yasar Abbas Ur Rehman, Mengyang\n  Liu, Yujia Zhang, Weifeng Ou", "title": "VCGAN: Video Colorization with Hybrid Generative Adversarial Network", "comments": "Submitted Major Revision Manuscript of IEEE Transactions on\n  Multimedia (TMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid recurrent Video Colorization with Hybrid Generative\nAdversarial Network (VCGAN), an improved approach to video colorization using\nend-to-end learning. The VCGAN addresses two prevalent issues in the video\ncolorization domain: Temporal consistency and unification of colorization\nnetwork and refinement network into a single architecture. To enhance\ncolorization quality and spatiotemporal consistency, the mainstream of\ngenerator in VCGAN is assisted by two additional networks, i.e., global feature\nextractor and placeholder feature extractor, respectively. The global feature\nextractor encodes the global semantics of grayscale input to enhance\ncolorization quality, whereas the placeholder feature extractor acts as a\nfeedback connection to encode the semantics of the previous colorized frame in\norder to maintain spatiotemporal consistency. If changing the input for\nplaceholder feature extractor as grayscale input, the hybrid VCGAN also has the\npotential to perform image colorization. To improve the consistency of far\nframes, we propose a dense long-term loss that smooths the temporal disparity\nof every two remote frames. Trained with colorization and temporal losses\njointly, VCGAN strikes a good balance between color vividness and video\ncontinuity. Experimental results demonstrate that VCGAN produces higher-quality\nand temporally more consistent colorful videos than existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 05:50:53 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhao", "Yuzhi", ""], ["Po", "Lai-Man", ""], ["Yu", "Wing-Yin", ""], ["Rehman", "Yasar Abbas Ur", ""], ["Liu", "Mengyang", ""], ["Zhang", "Yujia", ""], ["Ou", "Weifeng", ""]]}, {"id": "2104.12465", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring", "title": "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video\n  Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional video summarization methods generate fixed video representations\nregardless of user interest. Therefore such methods limit users' expectations\nin content search and exploration scenarios. Multi-modal video summarization is\none of the methods utilized to address this problem. When multi-modal video\nsummarization is used to help video exploration, a text-based query is\nconsidered as one of the main drivers of video summary generation, as it is\nuser-defined. Thus, encoding the text-based query and the video effectively are\nboth important for the task of multi-modal video summarization. In this work, a\nnew method is proposed that uses a specialized attention network and\ncontextualized word representations to tackle this task. The proposed model\nconsists of a contextualized video summary controller, multi-modal attention\nmechanisms, an interactive attention network, and a video summary generator.\nBased on the evaluation of the existing multi-modal video summarization\nbenchmark, experimental results show that the proposed model is effective with\nthe increase of +5.88% in accuracy and +4.06% increase of F1-score, compared\nwith the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:50:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Murn", "Luka", ""], ["Mrak", "Marta", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12471", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Marcel Worring", "title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image captioning automatically generates a medical description to\ndescribe the content of a given medical image. A traditional medical image\ncaptioning model creates a medical description only based on a single medical\nimage input. Hence, an abstract medical description or concept is hard to be\ngenerated based on the traditional approach. Such a method limits the\neffectiveness of medical image captioning. Multi-modal medical image captioning\nis one of the approaches utilized to address this problem. In multi-modal\nmedical image captioning, textual input, e.g., expert-defined keywords, is\nconsidered as one of the main drivers of medical description generation. Thus,\nencoding the textual input and the medical image effectively are both important\nfor the task of multi-modal medical image captioning. In this work, a new\nend-to-end deep multi-modal medical image captioning model is proposed.\nContextualized keyword representations, textual feature reinforcement, and\nmasked self-attention are used to develop the proposed approach. Based on the\nevaluation of the existing multi-modal medical image captioning dataset,\nexperimental results show that the proposed model is effective with the\nincrease of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:08:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12507", "submitter": "Jiaoyang Yin", "authors": "Jiaoyang Yin, Yiling Xu, Hao Chen, Yunfei Zhang, Steve Appleby, Zhan\n  Ma", "title": "ANT: Learning Accurate Network Throughput for Better Adaptive Video\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Bit Rate (ABR) decision plays a crucial role for ensuring\nsatisfactory Quality of Experience (QoE) in video streaming applications, in\nwhich past network statistics are mainly leveraged for future network bandwidth\nprediction. However, most algorithms, either rules-based or learning-driven\napproaches, feed throughput traces or classified traces based on traditional\nstatistics (i.e., mean/standard deviation) to drive ABR decision, leading to\ncompromised performances in specific scenarios. Given the diverse network\nconnections (e.g., WiFi, cellular and wired link) from time to time, this paper\nthus proposes to learn the ANT (a.k.a., Accurate Network Throughput) model to\ncharacterize the full spectrum of network throughput dynamics in the past for\nderiving the proper network condition associated with a specific cluster of\nnetwork throughput segments (NTS). Each cluster of NTS is then used to generate\na dedicated ABR model, by which we wish to better capture the network dynamics\nfor diverse connections. We have integrated the ANT model with existing\nreinforcement learning (RL)-based ABR decision engine, where different ABR\nmodels are applied to respond to the accurate network sensing for better rate\ndecision. Extensive experiment results show that our approach can significantly\nimprove the user QoE by 65.5% and 31.3% respectively, compared with the\nstate-of-the-art Pensive and Oboe, across a wide range of network scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:15:53 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 12:28:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yin", "Jiaoyang", ""], ["Xu", "Yiling", ""], ["Chen", "Hao", ""], ["Zhang", "Yunfei", ""], ["Appleby", "Steve", ""], ["Ma", "Zhan", ""]]}, {"id": "2104.12734", "submitter": "Xiyang Luo", "authors": "Xiyang Luo, Yinxiao Li, Huiwen Chang, Ce Liu, Peyman Milanfar, Feng\n  Yang", "title": "DVMark: A Deep Multiscale Framework for Video Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video watermarking embeds a message into a cover video in an imperceptible\nmanner, which can be retrieved even if the video undergoes certain\nmodifications or distortions. Traditional watermarking methods are often\nmanually designed for particular types of distortions and thus cannot\nsimultaneously handle a broad spectrum of distortions. To this end, we propose\na robust deep learning-based solution for video watermarking that is end-to-end\ntrainable. Our model consists of a novel multiscale design where the watermarks\nare distributed across multiple spatial-temporal scales. It gains robustness\nagainst various distortions through a differentiable distortion layer, whereas\nnon-differentiable distortions, such as popular video compression standards,\nare modeled by a differentiable proxy. Extensive evaluations on a wide variety\nof distortions show that our method outperforms traditional video watermarking\nmethods as well as deep image watermarking models by a large margin. We further\ndemonstrate the practicality of our method on a realistic video-editing\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:24:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Luo", "Xiyang", ""], ["Li", "Yinxiao", ""], ["Chang", "Huiwen", ""], ["Liu", "Ce", ""], ["Milanfar", "Peyman", ""], ["Yang", "Feng", ""]]}, {"id": "2104.12770", "submitter": "Gangadharan Esakki", "authors": "Gangadharan Esakki", "title": "Adaptive Encoding for Constrained Video Delivery in HEVC, VP9, AV1 and\n  VVC Compression Standards and Adaptation to Video Content", "comments": "Video codecs, Pareto front, Regression models, Video encoding, Video\n  quality assessment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The dissertation proposes the use of a multi-objective optimization framework\nfor designing and selecting among enhanced GOP configurations in video\ncompression standards. The proposed methods achieve fine optimization over a\nset of general modes that include: (i) maximum video quality, (ii) minimum\nbitrate, (iii) maximum encoding rate (previously minimum encoding time mode)\nand (iv) can be shown to improve upon the YouTube/Netflix default encoder mode\nsettings over a set of opposing constraints to guarantee satisfactory\nperformance. The dissertation describes the implementation of a codec-agnostic\napproach using different video coding standards (x265, VP9, AV1) on a wide\nrange of videos derived from different video datasets. The results demonstrate\nthat the optimal encoding parameters obtained from the Pareto front space can\nprovide significant bandwidth savings without sacrificing video quality. This\nis achieved by the use of effective regression models that allow for the\nselection of video encoding settings that are jointly optimal in the encoding\ntime, bitrate, and video quality space. The dissertation applies the proposed\nmethods to x265, VP9, AV1 and using new GOP configurations in x265, delivering\nover 40% of the optimal encodings in two standard reference videos.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:16:18 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Esakki", "Gangadharan", ""]]}, {"id": "2104.12865", "submitter": "Zhao Wang", "authors": "Zhao Wang, Changyue Ma, Yan Ye", "title": "Multi-Density Attention Network for Loop Filtering in Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video compression is a basic requirement for consumer and professional video\napplications alike. Video coding standards such as H.264/AVC and H.265/HEVC are\nwidely deployed in the market to enable efficient use of bandwidth and storage\nfor many video applications. To reduce the coding artifacts and improve the\ncompression efficiency, neural network based loop filtering of the\nreconstructed video has been developed in the literature. However, loop\nfiltering is a challenging task due to the variation in video content and\nsampling densities. In this paper, we propose a on-line scaling based\nmulti-density attention network for loop filtering in video compression. The\ncore of our approach lies in several aspects: (a) parallel multi-resolution\nconvolution streams for extracting multi-density features, (b) single attention\nbranch to learn the sample correlations and generate mask maps, (c) a\nchannel-mutual attention procedure to fuse the data from multiple branches, (d)\non-line scaling technique to further optimize the output results of network\naccording to the actual signal. The proposed multi-density attention network\nlearns rich features from multiple sampling densities and performs robustly on\nvideo content of different resolutions. Moreover, the online scaling process\nenhances the signal adaptability of the off-line pre-trained model.\nExperimental results show that 10.18% bit-rate reduction at the same video\nquality can be achieved over the latest Versatile Video Coding (VVC) standard.\nThe objective performance of the proposed algorithm outperforms the\nstate-of-the-art methods and the subjective quality improvement is obvious in\nterms of detail preservation and artifact alleviation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:46:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Zhao", ""], ["Ma", "Changyue", ""], ["Ye", "Yan", ""]]}, {"id": "2104.13000", "submitter": "Jiyuan Liu", "authors": "Siqi Wang, Jiyuan Liu, Guang Yu, Xinwang Liu, Sihang Zhou, En Zhu,\n  Yuexiang Yang, Jianping Yin", "title": "Multi-view Deep One-class Classification: A Systematic Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classification (OCC), which models one single positive class and\ndistinguishes it from the negative class, has been a long-standing topic with\npivotal application to realms like anomaly detection. As modern society often\ndeals with massive high-dimensional complex data spawned by multiple sources,\nit is natural to consider OCC from the perspective of multi-view deep learning.\nHowever, it has not been discussed by the literature and remains an unexplored\ntopic. Motivated by this blank, this paper makes four-fold contributions:\nFirst, to our best knowledge, this is the first work that formally identifies\nand formulates the multi-view deep OCC problem. Second, we take recent advances\nin relevant areas into account and systematically devise eleven different\nbaseline solutions for multi-view deep OCC, which lays the foundation for\nresearch on multi-view deep OCC. Third, to remedy the problem that limited\nbenchmark datasets are available for multi-view deep OCC, we extensively\ncollect existing public data and process them into more than 30 new multi-view\nbenchmark datasets via multiple means, so as to provide a publicly available\nevaluation platform for multi-view deep OCC. Finally, by comprehensively\nevaluating the devised solutions on benchmark datasets, we conduct a thorough\nanalysis on the effectiveness of the designed baselines, and hopefully provide\nother researchers with beneficial guidance and insight to multi-view deep OCC.\nOur data and codes are opened at https://github.com/liujiyuan13/MvDOCC-datasets\nand https://github.com/liujiyuan13/MvDOCC-code respectively to facilitate\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 06:44:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Siqi", ""], ["Liu", "Jiyuan", ""], ["Yu", "Guang", ""], ["Liu", "Xinwang", ""], ["Zhou", "Sihang", ""], ["Zhu", "En", ""], ["Yang", "Yuexiang", ""], ["Yin", "Jianping", ""]]}, {"id": "2104.13044", "submitter": "Xian-Feng Han", "authors": "Xian-Feng Han and Yi-Fei Jin and Hui-Xian Cheng and Guo-Qiang Xiao", "title": "Dual Transformer for Point Cloud Analysis", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the tremendous success of transformer in natural language\nprocessing and image understanding tasks, in this paper, we present a novel\npoint cloud representation learning architecture, named Dual Transformer\nNetwork (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT)\nmodule. Specifically, by aggregating the well-designed point-wise and\nchannel-wise multi-head self-attention models simultaneously, DPCT module can\ncapture much richer contextual dependencies semantically from the perspective\nof position and channel. With the DPCT module as a fundamental component, we\nconstruct the DTNet for performing point cloud analysis in an end-to-end\nmanner. Extensive quantitative and qualitative experiments on publicly\navailable benchmarks demonstrate the effectiveness of our proposed transformer\nframework for the tasks of 3D point cloud classification and segmentation,\nachieving highly competitive performance in comparison with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:41:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["Jin", "Yi-Fei", ""], ["Cheng", "Hui-Xian", ""], ["Xiao", "Guo-Qiang", ""]]}, {"id": "2104.13053", "submitter": "Xian-Feng Han", "authors": "Xian-Feng Han and Zhang-Yue He and Jia Chen and Guo-Qiang Xiao", "title": "Cross-Level Cross-Scale Cross-Attention Network for Point Cloud\n  Representation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-attention mechanism recently achieves impressive advancement in Natural\nLanguage Processing (NLP) and Image Processing domains. And its permutation\ninvariance property makes it ideally suitable for point cloud processing.\nInspired by this remarkable success, we propose an end-to-end architecture,\ndubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point\ncloud representation learning. First, a point-wise feature pyramid module is\nintroduced to hierarchically extract features from different scales or\nresolutions. Then a cross-level cross-attention is designed to model long-range\ninter-level and intra-level dependencies. Finally, we develop a cross-scale\ncross-attention module to capture interactions between-and-within scales for\nrepresentation enhancement. Compared with state-of-the-art approaches, our\nnetwork can obtain competitive performance on challenging 3D object\nclassification, point cloud segmentation tasks via comprehensive experimental\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 09:01:14 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["He", "Zhang-Yue", ""], ["Chen", "Jia", ""], ["Xiao", "Guo-Qiang", ""]]}, {"id": "2104.13748", "submitter": "Matthias Springstein", "authors": "Matthias Springstein and Eric M\\\"uller-Budack and Ralph Ewerth", "title": "QuTI! Quantifying Text-Image Consistency in Multimodal Documents", "comments": "Accepted for publication in: International ACM SIGIR Conference on\n  Research and Development in Information Retrieval 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web and social media platforms have become popular sources for\nnews and information. Typically, multimodal information, e.g., image and text\nis used to convey information more effectively and to attract attention. While\nin most cases image content is decorative or depicts additional information, it\nhas also been leveraged to spread misinformation and rumors in recent years. In\nthis paper, we present a Web-based demo application that automatically\nquantifies the cross-modal relations of entities (persons, locations, and\nevents) in image and text. The applications are manifold. For example, the\nsystem can help users to explore multimodal articles more efficiently, or can\nassist human assessors and fact-checking efforts in the verification of the\ncredibility of news stories, tweets, or other multimodal documents.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:28:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Springstein", "Matthias", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.14085", "submitter": "Jungin Park", "authors": "Jungin Park, Jiyoung Lee, Kwanghoon Sohn", "title": "Bridge to Answer: Structure-aware Graph Interaction Network for Video\n  Question Answering", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel method, termed Bridge to Answer, to infer correct\nanswers for questions about a given video by leveraging adequate graph\ninteractions of heterogeneous crossmodal graphs. To realize this, we learn\nquestion conditioned visual graphs by exploiting the relation between video and\nquestion to enable each visual node using question-to-visual interactions to\nencompass both visual and linguistic cues. In addition, we propose bridged\nvisual-to-visual interactions to incorporate two complementary visual\ninformation on appearance and motion by placing the question graph as an\nintermediate bridge. This bridged architecture allows reliable message passing\nthrough compositional semantics of the question to generate an appropriate\nanswer. As a result, our method can learn the question conditioned visual\nrepresentations attributed to appearance and motion that show powerful\ncapability for video question answering. Extensive experiments prove that the\nproposed method provides effective and superior performance than\nstate-of-the-art methods on several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:02:37 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Park", "Jungin", ""], ["Lee", "Jiyoung", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2104.14109", "submitter": "Cong Wang", "authors": "Cong Wang, Fan Tang, Yong Zhang, Weiming Dong, Tieru Wu", "title": "Towards Harmonized Regional Style Transfer and Manipulation for Facial\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional facial image synthesis conditioned on semantic mask has achieved\ngreat success using generative adversarial networks. However, the appearance of\ndifferent regions may be inconsistent with each other when conducting regional\nimage editing. In this paper, we focus on the problem of harmonized regional\nstyle transfer and manipulation for facial images. The proposed approach\nsupports regional style transfer and manipulation at the same time. A\nmulti-scale encoder and style mapping networks are proposed in our work. The\nencoder is responsible for extracting regional styles of real faces. Style\nmapping networks generate styles from random samples for all facial regions. As\nthe key part of our work, we propose a multi-region style attention module to\nadapt the multiple regional style embeddings from a reference image to a target\nimage for generating harmonious and plausible results. Furthermore, we propose\na new metric \"harmony score\" and conduct experiments in a challenging setting:\nthree widely used face datasets are involved and we test the model by\ntransferring the regional facial appearance between datasets. Images in\ndifferent datasets are usually quite different, which makes the inconsistency\nbetween target and reference regions more obvious. Results show that our model\ncan generate reliable style transfer and multi-modal manipulation results\ncompared with SOTAs. Furthermore, we show two face editing applications using\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:01:27 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wang", "Cong", ""], ["Tang", "Fan", ""], ["Zhang", "Yong", ""], ["Dong", "Weiming", ""], ["Wu", "Tieru", ""]]}, {"id": "2104.14115", "submitter": "Jianzhao Liu", "authors": "Jianzhao Liu, Wei Zhou, Jiahua Xu, Xin Li, Shukun An and Zhibo Chen", "title": "LIQA: Lifelong Blind Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing blind image quality assessment (BIQA) methods are mostly designed in\na disposable way and cannot evolve with unseen distortions adaptively, which\ngreatly limits the deployment and application of BIQA models in real-world\nscenarios. To address this problem, we propose a novel Lifelong blind Image\nQuality Assessment (LIQA) approach, targeting to achieve the lifelong learning\nof BIQA. Without accessing to previous training data, our proposed LIQA can not\nonly learn new distortions, but also mitigate the catastrophic forgetting of\nseen distortions. Specifically, we adopt the Split-and-Merge distillation\nstrategy to train a single-head network that makes task-agnostic predictions.\nIn the split stage, we first employ a distortion-specific generator to obtain\nthe pseudo features of each seen distortion. Then, we use an auxiliary\nmulti-head regression network to generate the predicted quality of each seen\ndistortion. In the merge stage, we replay the pseudo features paired with\npseudo labels to distill the knowledge of multiple heads, which can build the\nfinal regressed single head. Experimental results demonstrate that the proposed\nLIQA method can handle the continuous shifts of different distortion types and\neven datasets. More importantly, our LIQA model can achieve stable performance\neven if the task sequence is long.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:24:58 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Liu", "Jianzhao", ""], ["Zhou", "Wei", ""], ["Xu", "Jiahua", ""], ["Li", "Xin", ""], ["An", "Shukun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2104.14170", "submitter": "Xing Wei", "authors": "Xing Wei and Chenyang Yang", "title": "Spatial Privacy-aware VR streaming", "comments": "7 pages, 5 figures, submit to IEEE for possible publication. arXiv\n  admin note: substantial text overlap with arXiv:2104.09779", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive tile-based virtual reality (VR) video streaming employs the current\ntracking data of a user to predict future requested tiles, then renders and\ndelivers the predicted tiles before playback. Very recently, privacy protection\nin proactive VR video streaming starts to raise concerns. However, existing\nprivacy protection may fail even with privacy-preserve federated learning. This\nis because when the future requested tiles can be predicted accurately, the\nuser-behavior-related data can still be recovered from the predicted tiles. In\nthis paper, we consider how to protect privacy even with accurate predictors\nand investigate the impact of privacy requirement on the quality of experience\n(QoE). To this end, we first add extra \\textit{camouflaged} tile requests to\nthe real tile requests and model the privacy requirement as the \\textit{spatial\ndegree of privacy} (sDoP). By ensuring sDoP, the real tile requests can be\nhidden and privacy can be protected. Then, we jointly optimize the durations\nfor prediction, computing, and transmitting, aimed at maximizing the\nprivacy-aware QoE given arbitrary predictor and configured resources. From the\nobtained optimal closed-form solution, we find that the impacts of sDoP on the\nQoE are two sides of the same coin. On the one side the increase of sDoP\nimproves the capability of communication and computing hence improves QoE. On\nthe other side it degrades the prediction performance hence degrades the QoE.\nThe overall impact depends on which factor dominates the QoE. Simulation with\ntwo predictors on a real dataset verifies the analysis and shows that the\noverall impact of sDoP is to improve the QoE.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 07:53:02 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 15:30:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wei", "Xing", ""], ["Yang", "Chenyang", ""]]}, {"id": "2104.14522", "submitter": "Enes Altinisik", "authors": "Enes Altinisik, H\\\"usrev Taha Sencar", "title": "Automatic Generation of H.264 Parameter Sets to Recover Video File\n  Fragments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of decoding video file fragments when the necessary\nencoding parameters are missing. With this objective, we propose a method that\nautomatically generates H.264 video headers containing these parameters and\nextracts coded pictures in the partially available compressed video data. To\naccomplish this, we examined a very large corpus of videos to learn patterns of\nencoding settings commonly used by encoders and created a parameter dictionary.\nFurther, to facilitate a more efficient search our method identifies\ncharacteristics of a coded bitstream to discriminate the entropy coding mode.\nIt also utilizes the application logs created by the decoder to identify\ncorrect parameter values. Evaluation of the effectiveness of the proposed\nmethod on more than 55K videos with diverse provenance shows that it can\ngenerate valid headers on average in 11.3 decoding trials per video. This\nresult represents an improvement by more than a factor of 10 over the\nconventional approach of video header stitching to recover video file\nfragments.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:40:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:25:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Altinisik", "Enes", ""], ["Sencar", "H\u00fcsrev Taha", ""]]}, {"id": "2104.14532", "submitter": "Bolu John Folayan Dr.", "authors": "Bolu John Folayan, Olubunmi Ajibade, Olubunmi Dipo-Adedoyin, Toyin\n  Segun Onayinka, Toluwani Titilola Folayan", "title": "The Big Brother NaijaTV Reality Show as Coordinate of Media Functions\n  and Dysfunctions", "comments": null, "journal-ref": "Journal of Social Science ResearchVol 17(2021) ISSN: 2321-1091", "doi": "10.24297/jssr.v17i.9015", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mass media play at least five basic functions which include news\ndissemination, surveillance of the environment, correlation of the components\nof the society, entertainment and transmission of social heritage. Sometimes,\ndisruptions and impairments do occur in the performance of these roles and some\nof these basic functions become dysfunctions, which turn the media into\npurveyor of negative values. The present study investigates how popular the\nNigerian TV reality show, Big Brother Naija BBN, is perceived by its viewers.\nThree hundred heavy viewers of the program were surveyed from Lagos and Ede,\nSouth-West Nigeria, and their opinions and attitudes were sought regarding, why\nthey like or dislike the program; the gratifications that those who like the\nprogram derive and whether the BBN, as media content, is generally functional\nor dysfunctional to the society. Sixty six per cent 66 33.7 of respondents like\nthe program because it entertains. Half of the respondents, 99 50.5 dislike\nimmoral aspects of the program. The viewers affirm that the eviction part of\nthe program was their highest form of gratification. Most respondents, despite\npublic outcry against the program, consider the program to be functional.\nFindings reinforce the postulation that TV viewers are not passive consumers of\nmedia contents.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:47:58 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Folayan", "Bolu John", ""], ["Ajibade", "Olubunmi", ""], ["Dipo-Adedoyin", "Olubunmi", ""], ["Onayinka", "Toyin Segun", ""], ["Folayan", "Toluwani Titilola", ""]]}, {"id": "2104.14799", "submitter": "Laure Pr\\'etet", "authors": "Laure Pretet, Gael Richard, Geoffroy Peeters", "title": "Cross-Modal Music-Video Recommendation: A Study of Design Choices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study music/video cross-modal recommendation, i.e.\nrecommending a music track for a video or vice versa. We rely on a\nself-supervised learning paradigm to learn from a large amount of unlabelled\ndata. We rely on a self-supervised learning paradigm to learn from a large\namount of unlabelled data. More precisely, we jointly learn audio and video\nembeddings by using their co-occurrence in music-video clips. In this work, we\nbuild upon a recent video-music retrieval system (the VM-NET), which originally\nrelies on an audio representation obtained by a set of statistics computed over\nhandcrafted features. We demonstrate here that using audio representation\nlearning such as the audio embeddings provided by the pre-trained MuSimNet,\nOpenL3, MusicCNN or by AudioSet, largely improves recommendations. We also\nvalidate the use of the cross-modal triplet loss originally proposed in the\nVM-NET compared to the binary cross-entropy loss commonly used in\nself-supervised learning. We perform all our experiments using the Music Video\nDataset (MVD).\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:35:55 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Pretet", "Laure", ""], ["Richard", "Gael", ""], ["Peeters", "Geoffroy", ""]]}, {"id": "2104.14802", "submitter": "Xinjian Zhang", "authors": "Xinjian Zhang, Yi Xu, Su Yang, Longwen Gao, Huyang Sun", "title": "Dance Generation with Style Embedding: Learning and Transferring Latent\n  Representations of Dance Styles", "comments": "Submit to IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreography refers to creation of dance steps and motions for dances\naccording to the latent knowledge in human mind, where the created dance\nmotions are in general style-specific and consistent. So far, such latent\nstyle-specific knowledge about dance styles cannot be represented explicitly in\nhuman language and has not yet been learned in previous works on music-to-dance\ngeneration tasks. In this paper, we propose a novel music-to-dance synthesis\nframework with controllable style embeddings. These embeddings are learned\nrepresentations of style-consistent kinematic abstraction of reference dance\nclips, which act as controllable factors to impose style constraints on dance\ngeneration in a latent manner. Thus, the dance styles can be transferred to\ndance motions by merely modifying the style embeddings. To support this study,\nwe build a large music-to-dance dataset. The qualitative and quantitative\nevaluations demonstrate the advantage of our proposed framework, as well as the\nability of synthesizing diverse styles of dances from identical music via style\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:36:49 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Xinjian", ""], ["Xu", "Yi", ""], ["Yang", "Su", ""], ["Gao", "Longwen", ""], ["Sun", "Huyang", ""]]}, {"id": "2104.14868", "submitter": "Ahmet M. Tekalp", "authors": "Onur Kele\\c{s}, M. Ak{\\i}n Y{\\i}lmaz, A. Murat Tekalp, Cansu Korkmaz,\n  Zafer Dogan", "title": "On the Computation of PSNR for a Set of Images or Video", "comments": "accepted for publication in Picture Coding Symposium (PCS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When comparing learned image/video restoration and compression methods, it is\ncommon to report peak-signal to noise ratio (PSNR) results. However, there does\nnot exist a generally agreed upon practice to compute PSNR for sets of images\nor video. Some authors report average of individual image/frame PSNR, which is\nequivalent to computing a single PSNR from the geometric mean of individual\nimage/frame mean-square error (MSE). Others compute a single PSNR from the\narithmetic mean of frame MSEs for each video. Furthermore, some compute the\nMSE/PSNR of Y-channel only, while others compute MSE/PSNR for RGB channels.\nThis paper investigates different approaches to computing PSNR for sets of\nimages, single video, and sets of video and the relation between them. We show\nthe difference between computing the PSNR based on arithmetic vs. geometric\nmean of MSE depends on the distribution of MSE over the set of images or video,\nand that this distribution is task-dependent. In particular, these two methods\nyield larger differences in restoration problems, where the MSE is\nexponentially distributed and smaller differences in compression problems,\nwhere the MSE distribution is narrower. We hope this paper will motivate the\ncommunity to clearly describe how they compute reported PSNR values to enable\nconsistent comparison.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:52:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kele\u015f", "Onur", ""], ["Y\u0131lmaz", "M. Ak\u0131n", ""], ["Tekalp", "A. Murat", ""], ["Korkmaz", "Cansu", ""], ["Dogan", "Zafer", ""]]}, {"id": "2104.14994", "submitter": "Golsa Tahmasebzadeh", "authors": "Golsa Tahmasebzadeh, Endri Kacupaj, Eric M\\\"uller-Budack, Sherzod\n  Hakimov, Jens Lehmann, Ralph Ewerth", "title": "GeoWINE: Geolocation based Wiki, Image,News and Event Retrieval", "comments": "Accepted for publication in: International ACM SIGIR Conference on\n  Research and Development in Information Retrieval 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of social media, geolocation inference on news or events has\nbecome a very important task. In this paper, we present the GeoWINE\n(Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective\nmodular system for multimodal retrieval which expects only a single image as\ninput. The GeoWINE system consists of five modules in order to retrieve related\ninformation from various sources. The first module is a state-of-the-art model\nfor geolocation estimation of images. The second module performs a\ngeospatial-based query for entity retrieval using the Wikidata knowledge graph.\nThe third module exploits four different image embedding representations, which\nare used to retrieve most similar entities compared to the input image. The\nembeddings are derived from the tasks of geolocation estimation, place\nrecognition, ImageNet-based image classification, and their combination. The\nlast two modules perform news and event retrieval from EventRegistry and the\nOpen Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for\nend-users and is insightful for experts for reconfiguration to individual\nsetups. The GeoWINE achieves promising results in entity label prediction for\nimages on Google Landmarks dataset. The demonstrator is publicly available at\nhttp://cleopatra.ijs.si/geowine/.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:27:50 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 12:04:05 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tahmasebzadeh", "Golsa", ""], ["Kacupaj", "Endri", ""], ["M\u00fcller-Budack", "Eric", ""], ["Hakimov", "Sherzod", ""], ["Lehmann", "Jens", ""], ["Ewerth", "Ralph", ""]]}]