[{"id": "1502.00087", "submitter": "Andrea Tassi", "authors": "Andrea Tassi, Ioannis Chatzigeorgiou, Dejan Vukobratovi\\'c", "title": "On Optimization of Network-coded Scalable Multimedia Service\n  Multicasting", "comments": "Proc. of the 6th Systems and Networks Optimization for Wireless\n  (SNOW) Workshop 2015, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM cs.NI cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the near future, the delivery of multimedia multicast services over\nnext-generation networks is likely to become one of the main pillars of future\ncellular networks. In this extended abstract, we address the issue of\nefficiently multicasting layered video services by defining a novel\noptimization paradigm that is based on an Unequal Error Protection\nimplementation of Random Linear Network Coding, and aims to ensure target\nservice coverages by using a limited amount of radio resources.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 09:45:29 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Tassi", "Andrea", ""], ["Chatzigeorgiou", "Ioannis", ""], ["Vukobratovi\u0107", "Dejan", ""]]}, {"id": "1502.00296", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, V. S. Dimitrov, H. M. de Oliveira, R. M. Campello de\n  Souza", "title": "Fragile Watermarking Using Finite Field Trigonometrical Transforms", "comments": "9 pages, 7 figures, 2 tables", "journal-ref": "Image Communication, Volume 24, Issue 7, August, 2009, pp. 587-597", "doi": "10.1016/j.image.2009.04.003", "report-no": null, "categories": "cs.MM cs.IT math.IT math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fragile digital watermarking has been applied for authentication and\nalteration detection in images. Utilizing the cosine and Hartley transforms\nover finite fields, a new transform domain fragile watermarking scheme is\nintroduced. A watermark is embedded into a host image via a blockwise\napplication of two-dimensional finite field cosine or Hartley transforms.\nAdditionally, the considered finite field transforms are adjusted to be number\ntheoretic transforms, appropriate for error-free calculation. The employed\ntechnique can provide invisible fragile watermarking for authentication systems\nwith tamper location capability. It is shown that the choice of the finite\nfield characteristic is pivotal to obtain perceptually invisible watermarked\nimages. It is also shown that the generated watermarked images can be used as\npublicly available signature data for authentication purposes.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 18:43:29 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""], ["de Oliveira", "H. M.", ""], ["de Souza", "R. M. Campello", ""]]}, {"id": "1502.00555", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "A Discrete Tchebichef Transform Approximation for Image and Video Coding", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": "IEEE Signal Processing Letters, vol. 22, issue 8, pp. 1137-1141,\n  2015", "doi": "10.1109/LSP.2015.2389899", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a low-complexity approximation for the discrete\nTchebichef transform (DTT). The proposed forward and inverse transforms are\nmultiplication-free and require a reduced number of additions and bit-shifting\noperations. Numerical compression simulations demonstrate the efficiency of the\nproposed transform for image and video coding. Furthermore, Xilinx Virtex-6\nFPGA based hardware realization shows 44.9% reduction in dynamic power\nconsumption and 64.7% lower area when compared to the literature.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 14:07:44 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1502.00592", "submitter": "Renato J Cintra", "authors": "C. J. Tablada, F. M. Bayer, R. J. Cintra", "title": "A Class of DCT Approximations Based on the Feig-Winograd Algorithm", "comments": "26 pages, 4 figures, 5 tables, fixed arithmetic complexity in Table\n  IV", "journal-ref": "Signal Processing, vol. 113, pp. 38-51, August 2015", "doi": "10.1016/j.sigpro.2015.01.011", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of matrices based on a parametrization of the Feig-Winograd\nfactorization of 8-point DCT is proposed. Such parametrization induces a matrix\nsubspace, which unifies a number of existing methods for DCT approximation. By\nsolving a comprehensive multicriteria optimization problem, we identified\nseveral new DCT approximations. Obtained solutions were sought to possess the\nfollowing properties: (i) low multiplierless computational complexity, (ii)\northogonality or near orthogonality, (iii) low complexity invertibility, and\n(iv) close proximity and performance to the exact DCT. Proposed approximations\nwere submitted to assessment in terms of proximity to the DCT, coding\nperformance, and suitability for image compression. Considering Pareto\nefficiency, particular new proposed approximations could outperform various\nexisting methods archived in literature.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 19:39:46 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 21:25:18 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Tablada", "C. J.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1502.01120", "submitter": "Emad Abd-Elrahman", "authors": "Emad Abd-Elrahman, Tarek Rekik and Hossam Afifi", "title": "Optimization of Quality of Experience through File Duplication in Video\n  Sharing Servers", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers of short videos on Internet can have a bad Quality of Experience\nQoE due to the long distance between the consumers and the servers that hosting\nthe videos. We propose an optimization of the file allocation in\ntelecommunication operators content sharing servers to improve the QoE through\nfiles duplication, thus bringing the files closer to the consumers. This\noptimization allows the network operator to set the level of QoE and to have\ncontrol over the users access cost by setting a number of parameters. Two\noptimization methods are given and are followed by a comparison of their\nefficiency. Also, the hosting costs versus the gain of optimization are\nanalytically discussed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 08:26:53 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Abd-Elrahman", "Emad", ""], ["Rekik", "Tarek", ""], ["Afifi", "Hossam", ""]]}, {"id": "1502.01122", "submitter": "Emad Abd-Elrahman", "authors": "Emad Abd-Elrahman, Mohamed Boutabia and Hossam Afifi", "title": "Hash Chain Links Resynchronization Methods in Video Streaming Security\n  Performance Comparison", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash chains provide a secure and light way of security to data authentication\nincluding two aspects: Data Integrity and Data Origin Authentication. The real\nchallenge of using the hash chains is how it could recover the synchronization\nstate and continue keeping the hash link in case of packet loss? Based on the\npacket loss tolerance and some accepted delay of video delivery which are\nrepresenting the permitted tolerance for heavy loaded applications, we propose\ndifferent mechanisms for such synchronization recovery. Each mechanism is\nsuitable to use according to the video use case and the low capabilities of end\ndevices. This paper proposes comparative results between them based on the\nstatus of each one and its overhead. Then, we propose a hybrid technique based\nRedundancy Code (RC). This hybrid algorithm is simulated and compared\nanalytically against the other techniques (SHHC, TSP, MLHC and TSS). Moreover,\na global performance evaluation in terms of delay and overhead is conducted for\nall techniques.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 08:39:31 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Abd-Elrahman", "Emad", ""], ["Boutabia", "Mohamed", ""], ["Afifi", "Hossam", ""]]}, {"id": "1502.01264", "submitter": "Adebayo Omotosho Mr", "authors": "Adebayo Omotosho, Omotanwa Adegbola, Olaniyi Olayemi Mikail, Justice\n  Emuoyibofarhe", "title": "A Secure Electronic Prescription System Using Steganography with\n  Encryption Key Implementation", "comments": "Published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years health care has seen major improvement due to the introduction\ninformation and communication technology with electronic medical prescription\nbeing one the areas benefiting from it. Within the overall context of\nprotection of health care information, privacy of prescription data needs\nspecial treatment. This paper presents an e-prescription system that addresses\nsome challenges pertaining to the prescription privacy protection in the\nprocess of drug prescription. The developed system uses spread spectrum image\nsteganography algorithm with Advanced Encryption Standard (AES) key\nimplementation to provide a secure means of delivering medical prescription to\nthe parties involved. The architecture for encoding and decoding was\nimplemented with an electronic health record. The software development tools\nused were PHP and MySQL database management system for front end and backend\ndata management respectively. The designed system demonstration shows that the\nsynergistic combination of steganography and cryptography technologies in\nmedical prescription is capable of providing a secure transmission to properly\nprovide security for patients medical prescription.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 17:38:34 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Omotosho", "Adebayo", ""], ["Adegbola", "Omotanwa", ""], ["Mikail", "Olaniyi Olayemi", ""], ["Emuoyibofarhe", "Justice", ""]]}, {"id": "1502.01707", "submitter": "Trifun Savic", "authors": "Trifun Savic, Radoje Albijanic", "title": "CS reconstruction of the speech and musical signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Compressive sensing approach to the speech and musical\nsignals is considered in this paper. Compressive sensing (CS) is a new approach\nto the signal sampling that allows signal reconstruction from a small set of\nrandomly acquired samples. This method is developed for the signals that\nexhibit the sparsity in a certain domain. Here we have observed two sparsity\ndomains: discrete Fourier and discrete cosine transform domain. Furthermore,\ntwo different types of audio signals are analyzed in terms of sparsity and CS\nperformance - musical and speech signals. Comparative analysis of the CS\nreconstruction using different number of signal samples is performed in the two\ndomains of sparsity. It is shown that the CS can be successfully applied to\nboth, musical and speech signals, but the speech signals are more demanding in\nterms of the number of observations. Also, our results show that discrete\ncosine transform domain allows better reconstruction using lower number of\nobservations, compared to the Fourier transform domain, for both types of\nsignals.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 20:40:41 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Savic", "Trifun", ""], ["Albijanic", "Radoje", ""]]}, {"id": "1502.01996", "submitter": "Ivan Knezevic", "authors": "Jelena Music, Ivan Knezevic and Edis Franca", "title": "Wavelet based Watermarking approach in the Compressive Sensing Scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide distribution and usage of digital media, an important issue\nis protection of the digital content. There is a number of algorithms and\ntechniques developed for the digital watermarking.In this paper, the invisible\nimage watermark procedure is considered. Watermark is created as a pseudo\nrandom sequence, embedded in the certain region of the image, obtained using\nHaar wavelet decomposition. Generally, the watermarking procedure should be\nrobust to the various attacks-filtering, noise etc. Here we assume the\nCompressive sensing scenario as a new signal processing technique that may\ninfluence the robustness. The focus of this paper was the possibility of the\nwatermark detection under Compressive Sensing attack with different number of\navailable image coefficients. The quality of the reconstructed images has been\nevaluated using Peak Signal to Noise Ratio (PSNR).The theory is supported with\nexperimental results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 19:16:45 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Music", "Jelena", ""], ["Knezevic", "Ivan", ""], ["Franca", "Edis", ""]]}, {"id": "1502.02125", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Contextual Online Learning for Multimedia Content Aggregation", "comments": "To appear in IEEE Transactions on Multimedia, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed a tremendous growth in the volume as well as\nthe diversity of multimedia content generated by a multitude of sources (news\nagencies, social media, etc.). Faced with a variety of content choices,\nconsumers are exhibiting diverse preferences for content; their preferences\noften depend on the context in which they consume content as well as various\nexogenous events. To satisfy the consumers' demand for such diverse content,\nmultimedia content aggregators (CAs) have emerged which gather content from\nnumerous multimedia sources. A key challenge for such systems is to accurately\npredict what type of content each of its consumers prefers in a certain\ncontext, and adapt these predictions to the evolving consumers' preferences,\ncontexts and content characteristics. We propose a novel, distributed, online\nmultimedia content aggregation framework, which gathers content generated by\nmultiple heterogeneous producers to fulfill its consumers' demand for content.\nSince both the multimedia content characteristics and the consumers'\npreferences and contexts are unknown, the optimal content aggregation strategy\nis unknown a priori. Our proposed content aggregation algorithm is able to\nlearn online what content to gather and how to match content and users by\nexploiting similarities between consumer types. We prove bounds for our\nproposed learning algorithms that guarantee both the accuracy of the\npredictions as well as the learning speed. Importantly, our algorithms operate\nefficiently even when feedback from consumers is missing or content and\npreferences evolve over time. Illustrative results highlight the merits of the\nproposed content aggregation system in a variety of settings.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 11:14:10 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 12:04:41 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1502.02809", "submitter": "Qingbo Kang", "authors": "Qingbo Kang, Ke Li, Hu Chen", "title": "An SVD-based Fragile Watermarking Scheme With Grouped Blocks", "comments": "8 pages, 10 figures, already accept by 2014 2nd International\n  Conference on Information Technology and Electronic Commerce (ICITEC 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel fragile watermarking scheme for digital image\nauthentication which is based on Singular Value Decomposition(SVD) and grouped\nblocks. The watermark bits which include two types of bits are inserted into\nthe least significant bit(LSB) plane of the host image using the adaptive\nchaotic map to determine the positions. The groped blocks break the block-wise\nindependence and therefore can withstand the Vector Quantization attack(VQ\nattack). The inserting positions are related to the statistical information of\nimage block data, in order to increase the security and provide an auxiliary\nway to authenticate the image data. The effectiveness of the proposed scheme is\nchecked by a variety of attacks, and the experimental results prove that it has\na remarkable tamper detection ability and also has a precise locating ability.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 08:18:10 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 04:19:27 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Kang", "Qingbo", ""], ["Li", "Ke", ""], ["Chen", "Hu", ""]]}, {"id": "1502.02943", "submitter": "Konstantin Miller", "authors": "Konstantin Miller, Dilip Bethanabhotla, Giuseppe Caire, Adam Wolisz", "title": "A Control-Theoretic Approach to Adaptive Video Streaming in Dense\n  Wireless Networks", "comments": "Submitted", "journal-ref": "IEEE Transactions on Multimedia, 2015, vol. 17, no. 8, pp. 1309 -\n  1322", "doi": "10.1109/TMM.2015.2441002", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the way people consume video content has been undergoing a dramatic\nchange. Plain TV sets, that have been the center of home entertainment for a\nlong time, are losing grounds to Hybrid TV's, PC's, game consoles, and, more\nrecently, mobile devices such as tablets and smartphones. The new predominant\nparadigm is: watch what I want, when I want, and where I want.\n  The challenges of this shift are manifold. On the one hand, broadcast\ntechnologies such as DVB-T/C/S need to be extended or replaced by mechanisms\nsupporting asynchronous viewing, such as IPTV and video streaming over\nbest-effort networks, while remaining scalable to millions of users. On the\nother hand, the dramatic increase of wireless data traffic begins to stretch\nthe capabilities of the existing wireless infrastructure to its limits.\nFinally, there is a challenge to video streaming technologies to cope with a\nhigh heterogeneity of end-user devices and dynamically changing network\nconditions, in particular in wireless and mobile networks.\n  In the present work, our goal is to design an efficient system that supports\na high number of unicast streaming sessions in a dense wireless access network.\nWe address this goal by jointly considering the two problems of wireless\ntransmission scheduling and video quality adaptation, using techniques inspired\nby the robustness and simplicity of Proportional-Integral-Derivative (PID)\ncontrollers. We show that the control-theoretic approach allows to efficiently\nutilize available wireless resources, providing high Quality of Experience\n(QoE) to a large number of users.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 15:18:21 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Miller", "Konstantin", ""], ["Bethanabhotla", "Dilip", ""], ["Caire", "Giuseppe", ""], ["Wolisz", "Adam", ""]]}, {"id": "1502.02969", "submitter": "Lin Jianbiao", "authors": "Ke Ji, Jianbiao Lin, Hui Li, Ao Wang, Tianjing Tang", "title": "A DCT And SVD based Watermarking Technique To Identify Tag", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of the multimedia,the secure of the multimedia is\nget more concerned. as far as we know , Digital watermarking is an effective\nway to protect copyright. The watermark must be generally hidden does not\naffect the quality of the original image. In this paper,a novel way based on\ndiscrete cosine transform(DCT) and singular value decomposition(SVD) .In the\nproposed way,we decomposition the image into 8*8 blocks, next we use the DCT to\nget the transformed block,then we choose the diagonal to embed the information,\nafter we do this, we recover the image and then we decomposition the image to\n8*8 blocks,we use the SVD way to get the diagonal matrix and embed the\ninformation in the matrix. next we extract the information use both inverse of\nDCT and SVD, as we all know,after we embed the information seconded time , the\ninformation we first information we embed must be changed, we choose a measure\nway called Peak Signal to Noise Ratio(PSNR) to estimate the similarity of the\ntwo image, and set a threshold to ensure whether the information is same or\nnot.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 16:23:07 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Ji", "Ke", ""], ["Lin", "Jianbiao", ""], ["Li", "Hui", ""], ["Wang", "Ao", ""], ["Tang", "Tianjing", ""]]}, {"id": "1502.03190", "submitter": "Xiahong Lin", "authors": "Xiahong Lin, Zhi Wang, Lifeng Sun", "title": "MAP: Microblogging Assisted Profiling of TV Shows", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-14445-0_38", "report-no": null, "categories": "cs.IR cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Online microblogging services that have been increasingly used by people to\nshare and exchange information, have emerged as a promising way to profiling\nmultimedia contents, in a sense to provide users a socialized abstraction and\nunderstanding of these contents. In this paper, we propose a microblogging\nprofiling framework, to provide a social demonstration of TV shows. Challenges\nfor this study lie in two folds: First, TV shows are generally offline, i.e.,\nmost of them are not originally from the Internet, and we need to create a\nconnection between these TV shows with online microblogging services; Second,\ncontents in a microblogging service are extremely noisy for video profiling,\nand we need to strategically retrieve the most related information for the TV\nshow profiling.To address these challenges, we propose a MAP, a\nmicroblogging-assisted profiling framework, with contributions as follows: i)\nWe propose a joint user and content retrieval scheme, which uses information\nabout both actors and topics of a TV show to retrieve related microblogs; ii)\nWe propose a social-aware profiling strategy, which profiles a video according\nto not only its content, but also the social relationship of its microblogging\nusers and its propagation in the social network; iii) We present some\ninteresting analysis, based on our framework to profile real-world TV shows.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 04:13:30 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Lin", "Xiahong", ""], ["Wang", "Zhi", ""], ["Sun", "Lifeng", ""]]}, {"id": "1502.03802", "submitter": "Yuanyi Xue", "authors": "Yuanyi Xue and Yi Zhou and Yao Wang", "title": "A two-stage video coding framework with both self-adaptive redundant\n  dictionary and adaptively orthonormalized DCT basis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a two-stage video coding framework, as an extension\nof our previous one-stage framework in [1]. The two-stage frameworks consists\ntwo different dictionaries. Specifically, the first stage directly finds the\nsparse representation of a block with a self-adaptive dictionary consisting of\nall possible inter-prediction candidates by solving an L0-norm minimization\nproblem using an improved orthogonal matching pursuit with embedded\northonormalization (eOMP) algorithm, and the second stage codes the residual\nusing DCT dictionary adaptively orthonormalized to the subspace spanned by the\nfirst stage atoms. The transition of the first stage and the second stage is\ndetermined based on both stages' quantization stepsizes and a threshold. We\nfurther propose a complete context adaptive entropy coder to efficiently code\nthe locations and the coefficients of chosen first stage atoms. Simulation\nresults show that the proposed coder significantly improves the RD performance\nover our previous one-stage coder. More importantly, the two-stage coder, using\na fixed block size and inter-prediction only, outperforms the H.264 coder\n(x264) and is competitive with the HEVC reference coder (HM) over a large rate\nrange.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 20:41:15 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Xue", "Yuanyi", ""], ["Zhou", "Yi", ""], ["Wang", "Yao", ""]]}, {"id": "1502.04149", "submitter": "Po-Sen Huang", "authors": "Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for\n  Monaural Source Separation", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol.23, no.12, pp.2136-2147, Dec. 2015", "doi": "10.1109/TASLP.2015.2468583", "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monaural source separation is important for many real world applications. It\nis challenging because, with only a single channel of information available,\nwithout any constraints, an infinite number of solutions are possible. In this\npaper, we explore joint optimization of masking functions and deep recurrent\nneural networks for monaural source separation tasks, including monaural speech\nseparation, monaural singing voice separation, and speech denoising. The joint\noptimization of the deep recurrent neural networks with an extra masking layer\nenforces a reconstruction constraint. Moreover, we explore a discriminative\ncriterion for training neural networks to further enhance the separation\nperformance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT\ndatasets for speech separation, singing voice separation, and speech denoising\ntasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to\nNMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and\n4.32--5.42 dB GSIR gain compared to existing models in the singing voice\nseparation task, and outperform NMF and DNN baselines in the speech denoising\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 23:22:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 04:22:20 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2015 04:20:33 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2015 02:58:01 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Huang", "Po-Sen", ""], ["Kim", "Minje", ""], ["Hasegawa-Johnson", "Mark", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1502.04666", "submitter": "Cong Zhang", "authors": "Cong Zhang and Jiangchuan Liu", "title": "On Crowdsourced Interactive Live Streaming: A Twitch.TV-Based\n  Measurement Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empowered by today's rich tools for media generation and collaborative\nproduction, the multimedia service paradigm is shifting from the conventional\nsingle source, to multi-source, to many sources, and now toward {\\em\ncrowdsource}. Such crowdsourced live streaming platforms as Twitch.tv allow\ngeneral users to broadcast their content to massive viewers, thereby greatly\nexpanding the content and user bases. The resources available for these\nnon-professional broadcasters however are limited and unstable, which\npotentially impair the streaming quality and viewers' experience. The diverse\nlive interactions among the broadcasters and viewers can further aggravate the\nproblem.\n  In this paper, we present an initial investigation on the modern crowdsourced\nlive streaming systems. Taking Twitch as a representative, we outline their\ninside architecture using both crawled data and captured traffic of local\nbroadcasters/viewers. Closely examining the access data collected in a\ntwo-month period, we reveal that the view patterns are determined by both\nevents and broadcasters' sources. Our measurements explore the unique source-\nand event-driven views, showing that the current delay strategy on the viewer's\nside substantially impacts the viewers' interactive experience, and there is\nsignificant disparity between the long broadcast latency and the short live\nmessaging latency. On the broadcaster's side, the dynamic uploading capacity is\na critical challenge, which noticeably affects the smoothness of live streaming\nfor viewers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 19:06:52 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 18:35:01 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Zhang", "Cong", ""], ["Liu", "Jiangchuan", ""]]}, {"id": "1502.05751", "submitter": "Enzo De Sena", "authors": "Enzo De Sena, Huseyin Hacihabiboglu, Zoran Cvetkovic, Julius O. Smith\n  III", "title": "Efficient Synthesis of Room Acoustics via Scattering Delay Networks", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  Vol. 23, No. 9, September 2015", "doi": "10.1109/TASLP.2015.2438547", "report-no": null, "categories": "cs.SD cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An acoustic reverberator consisting of a network of delay lines connected via\nscattering junctions is proposed. All parameters of the reverberator are\nderived from physical properties of the enclosure it simulates. It allows for\nsimulation of unequal and frequency-dependent wall absorption, as well as\ndirectional sources and microphones. The reverberator renders the first-order\nreflections exactly, while making progressively coarser approximations of\nhigher-order reflections. The rate of energy decay is close to that obtained\nwith the image method (IM) and consistent with the predictions of Sabine and\nEyring equations. The time evolution of the normalized echo density, which was\npreviously shown to be correlated with the perceived texture of reverberation,\nis also close to that of IM. However, its computational complexity is one to\ntwo orders of magnitude lower, comparable to the computational complexity of a\nfeedback delay network (FDN), and its memory requirements are negligible.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 23:58:36 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 14:27:05 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["De Sena", "Enzo", ""], ["Hacihabiboglu", "Huseyin", ""], ["Cvetkovic", "Zoran", ""], ["Smith", "Julius O.", "III"]]}, {"id": "1502.06078", "submitter": "Radu Arsinte", "authors": "Ioan Sorin Comsa, Radu Arsinte", "title": "Evaluating QoS Parameters for IPTV Distribution in Heterogeneous\n  Networks", "comments": "7 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": "EM 2/2010", "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work presents an architecture developed to evaluate the QoS\nparameters for the IPTV heterogeneous network. At its very basic level lie two\nsoftware technologies: Video LAN and Windows Media Services with two operating\nsystems: Windows and Linux. Three types of streams are analyzed, which will be\ntransmitted to a Linux VLC client through means of the aggregation and access\nservers. The first stream is generated in real time by a capture camera,\nprocessed by the encapsulated VC-1 encoder and sent to the Media Server, while\nthe second one is of VoD(Video on Demand) type and the third one will be\nhandled by DVBViewer through the MPEG TS form. The first stream is transcoded\nin H.264-AAC such that the Linux stations will recognize its format. Through\nthe simultaneous transmission of the three streams, we are analyzing their\nperformance from a QoS parameters point of view by means of an application\nimplemented in C programming language. The stream transporting the DVB-S\ntelevision content was proven to ensure the best performance regarding loss of\npackets, delays and jitter.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:26:33 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Comsa", "Ioan Sorin", ""], ["Arsinte", "Radu", ""]]}, {"id": "1502.06080", "submitter": "Weiyao Lin", "authors": "Yuanzhe Chen, Weiyao Lin, Chongyang Zhang, Zhenzhong Chen, Ning Xu,\n  Jun Xie", "title": "Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise\n  Tone Mapping", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 23,\n  no. 1, pp. 74-82, 2013", "doi": "10.1109/TCSVT.2012.2203198", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video enhancement plays an important role in various video applications. In\nthis paper, we propose a new intra-and-inter-constraint-based video enhancement\napproach aiming to 1) achieve high intra-frame quality of the entire picture\nwhere multiple region-of-interests (ROIs) can be adaptively and simultaneously\nenhanced, and 2) guarantee the inter-frame quality consistencies among video\nframes. We first analyze features from different ROIs and create a piecewise\ntone mapping curve for the entire frame such that the intra-frame quality of a\nframe can be enhanced. We further introduce new inter-frame constraints to\nimprove the temporal quality consistency. Experimental results show that the\nproposed algorithm obviously outperforms the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:36:26 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Chen", "Yuanzhe", ""], ["Lin", "Weiyao", ""], ["Zhang", "Chongyang", ""], ["Chen", "Zhenzhong", ""], ["Xu", "Ning", ""], ["Xie", "Jun", ""]]}, {"id": "1502.06103", "submitter": "Ana Miletic", "authors": "Ana Miletic, Nemanja Ivanovic", "title": "Compressive sensing based velocity estimation in video data", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the use of compressive sensing based algorithms for\nvelocity estimation of moving vehicles. The procedure is based on sparse\nreconstruction algorithms combined with time-frequency analysis applied to\nvideo data. This algorithm provides an accurate estimation of object's velocity\neven in the case of a very reduced number of available video frames. The\ninfluence of crucial parameters is analysed for different types of moving\nvehicles.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 13:19:34 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Miletic", "Ana", ""], ["Ivanovic", "Nemanja", ""]]}, {"id": "1502.06314", "submitter": "Cong Zhang", "authors": "Fei Chen, Cong Zhang, Feng Wang, Jiangchuan Liu", "title": "Crowdsourced Live Streaming over the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empowered by today's rich tools for media generation and distribution, and\nthe convenient Internet access, crowdsourced streaming generalizes the\nsingle-source streaming paradigm by including massive contributors for a video\nchannel. It calls a joint optimization along the path from crowdsourcers,\nthrough streaming servers, to the end-users to minimize the overall latency.\nThe dynamics of the video sources, together with the globalized request demands\nand the high computation demand from each sourcer, make crowdsourced live\nstreaming challenging even with powerful support from modern cloud computing.\nIn this paper, we present a generic framework that facilitates a cost-effective\ncloud service for crowdsourced live streaming. Through adaptively leasing, the\ncloud servers can be provisioned in a fine granularity to accommodate\ngeo-distributed video crowdsourcers. We present an optimal solution to deal\nwith service migration among cloud instances of diverse lease prices. It also\naddresses the location impact to the streaming quality. To understand the\nperformance of the proposed strategies in the realworld, we have built a\nprototype system running over the planetlab and the Amazon/Microsoft Cloud. Our\nextensive experiments demonstrate that the effectiveness of our solution in\nterms of deployment cost and streaming quality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 04:44:02 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Chen", "Fei", ""], ["Zhang", "Cong", ""], ["Wang", "Feng", ""], ["Liu", "Jiangchuan", ""]]}, {"id": "1502.07209", "submitter": "Zuxuan Wu", "authors": "Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, Shih-Fu Chang", "title": "Exploiting Feature and Class Relationships in Video Categorization with\n  Regularized Deep Neural Networks", "comments": "Please cite the officially published IEEE TPAMI version if you find\n  this work helpful", "journal-ref": "IEEE TPAMI 40.2 (2018): 352-364", "doi": "10.1109/TPAMI.2017.2670560", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenging problem of categorizing videos\naccording to high-level semantics such as the existence of a particular human\naction or a complex event. Although extensive efforts have been devoted in\nrecent years, most existing works combined multiple video features using simple\nfusion strategies and neglected the utilization of inter-class semantic\nrelationships. This paper proposes a novel unified framework that jointly\nexploits the feature relationships and the class relationships for improved\ncategorization performance. Specifically, these two types of relationships are\nestimated and utilized by rigorously imposing regularizations in the learning\nprocess of a deep neural network (DNN). Such a regularized DNN (rDNN) can be\nefficiently realized using a GPU-based implementation with an affordable\ntraining cost. Through arming the DNN with better capability of harnessing both\nthe feature and the class relationships, the proposed rDNN is more suitable for\nmodeling video semantics. With extensive experimental evaluations, we show that\nrDNN produces superior performance over several state-of-the-art approaches. On\nthe well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain\nvery competitive results: 66.9\\% and 73.5\\% respectively in terms of mean\naverage precision. In addition, to substantially evaluate our rDNN and\nstimulate future research on large scale video categorization, we collect and\nrelease a new benchmark dataset, called FCVID, which contains 91,223 Internet\nvideos and 239 manually annotated categories.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 15:41:48 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 20:37:34 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Jiang", "Yu-Gang", ""], ["Wu", "Zuxuan", ""], ["Wang", "Jun", ""], ["Xue", "Xiangyang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1502.07808", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Jamil Ahmad, Naeem Ur Rehman, Zahoor Jan, Rashid Jalal\n  Qureshi", "title": "A Secure Cyclic Steganographic Technique for Color Images using\n  Randomization", "comments": "8", "journal-ref": "Technical Journal, University of Engineering and Technology\n  Taxila, Pakistan, vol. 19, pp. 57-64, 2014", "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Security is a major concern in today's modern era. Almost all the\ncommunicating bodies want the security, confidentiality and integrity of their\npersonal data. But this security goal cannot be achieved easily when we are\nusing an open network like Internet. Steganography provides one of the best\nsolutions to this problem. This paper represents a new Cyclic Steganographic T\nechnique (CST) based on Least Significant Bit (LSB) for true color (RGB)\nimages. The proposed method hides the secret data in the LSBs of cover image\npixels in a randomized cyclic manner. The proposed technique is evaluated using\nboth subjective and objective analysis using histograms changeability, Peak\nSignal-to-Noise Ratio (PSNR) and Mean Square Error (MSE). Experimentally it is\nfound that the proposed method gives promising results in terms of security,\nimperceptibility and robustness as compared to some existent methods and\nvindicates this new algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 03:05:43 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Muhammad", "Khan", ""], ["Ahmad", "Jamil", ""], ["Rehman", "Naeem Ur", ""], ["Jan", "Zahoor", ""], ["Qureshi", "Rashid Jalal", ""]]}, {"id": "1502.07828", "submitter": "Luca Baroffio", "authors": "Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi,\n  Stefano Tubaro", "title": "Hybrid coding of visual content and local image features", "comments": "submitted to IEEE International Conference on Image Processing", "journal-ref": null, "doi": "10.1109/ICIP.2015.7351258", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed visual analysis applications, such as mobile visual search or\nVisual Sensor Networks (VSNs) require the transmission of visual content on a\nbandwidth-limited network, from a peripheral node to a processing unit.\nTraditionally, a Compress-Then-Analyze approach has been pursued, in which\nsensing nodes acquire and encode the pixel-level representation of the visual\ncontent, that is subsequently transmitted to a sink node in order to be\nprocessed. This approach might not represent the most effective solution, since\nseveral analysis applications leverage a compact representation of the content,\nthus resulting in an inefficient usage of network resources. Furthermore,\ncoding artifacts might significantly impact the accuracy of the visual task at\nhand. To tackle such limitations, an orthogonal approach named\nAnalyze-Then-Compress has been proposed. According to such a paradigm, sensing\nnodes are responsible for the extraction of visual features, that are encoded\nand transmitted to a sink node for further processing. In spite of improved\ntask efficiency, such paradigm implies the central processing node not being\nable to reconstruct a pixel-level representation of the visual content. In this\npaper we propose an effective compromise between the two paradigms, namely\nHybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual\ncontent and local image features. Furthermore, we show how a target tradeoff\nbetween image quality and task accuracy might be achieved by accurately\nallocating the bitrate to either visual content or local features.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 08:44:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Baroffio", "Luca", ""], ["Cesana", "Matteo", ""], ["Redondi", "Alessandro", ""], ["Tagliasacchi", "Marco", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1502.07939", "submitter": "Luca Baroffio", "authors": "Luca Baroffio, Antonio Canclini, Matteo Cesana, Alessandro Redondi,\n  Marco Tagliasacchi, Stefano Tubaro", "title": "Coding local and global binary visual features extracted from video\n  sequences", "comments": "submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2445294", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary local features represent an effective alternative to real-valued\ndescriptors, leading to comparable results for many visual analysis tasks,\nwhile being characterized by significantly lower computational complexity and\nmemory requirements. When dealing with large collections, a more compact\nrepresentation based on global features is often preferred, which can be\nobtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)\nmodel. Several applications, including for example visual sensor networks and\nmobile augmented reality, require visual features to be transmitted over a\nbandwidth-limited network, thus calling for coding techniques that aim at\nreducing the required bit budget, while attaining a target level of efficiency.\nIn this paper we investigate a coding scheme tailored to both local and global\nbinary features, which aims at exploiting both spatial and temporal redundancy\nby means of intra- and inter-frame coding. In this respect, the proposed coding\nscheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)\nparadigm. That is, visual features are extracted from the acquired content,\nencoded at remote nodes, and finally transmitted to a central controller that\nperforms visual analysis. This is in contrast with the traditional approach, in\nwhich visual content is acquired at a node, compressed and then sent to a\ncentral unit for further processing, according to the Compress-Then-Analyze\n(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of\nrate-efficiency curves in the context of two different visual analysis tasks:\nhomography estimation and content-based retrieval. Our results show that the\nnovel ATC paradigm based on the proposed coding primitives can be competitive\nwith CTA, especially in bandwidth limited scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 14:23:39 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Baroffio", "Luca", ""], ["Canclini", "Antonio", ""], ["Cesana", "Matteo", ""], ["Redondi", "Alessandro", ""], ["Tagliasacchi", "Marco", ""], ["Tubaro", "Stefano", ""]]}]