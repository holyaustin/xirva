[{"id": "2012.00119", "submitter": "Gongbo Liang", "authors": "Xin Xing, Gongbo Liang, Hunter Blanton, Muhammad Usman Rafique, Chris\n  Wang, Ai-Ling Lin, Nathan Jacobs", "title": "Dynamic Image for 3D MRI Image Alzheimer's Disease Classification", "comments": "Accepted to ECCV2020 Workshop on BioImage Computing", "journal-ref": null, "doi": "10.1007/978-3-030-66415-2_23", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to apply a 2D CNN architecture to 3D MRI image Alzheimer's disease\nclassification. Training a 3D convolutional neural network (CNN) is\ntime-consuming and computationally expensive. We make use of approximate rank\npooling to transform the 3D MRI image volume into a 2D image to use as input to\na 2D CNN. We show our proposed CNN model achieves $9.5\\%$ better Alzheimer's\ndisease classification accuracy than the baseline 3D models. We also show that\nour method allows for efficient training, requiring only 20% of the training\ntime compared to 3D CNN models. The code is available online:\nhttps://github.com/UkyVision/alzheimer-project.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:39:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Xing", "Xin", ""], ["Liang", "Gongbo", ""], ["Blanton", "Hunter", ""], ["Rafique", "Muhammad Usman", ""], ["Wang", "Chris", ""], ["Lin", "Ai-Ling", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2012.00290", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "MusicTM-Dataset for Joint Representation Learning among Sheet Music,\n  Lyrics, and Musical Audio", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": "CSMT2020", "doi": null, "report-no": null, "categories": "cs.SD cs.DB cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This work present a music dataset named MusicTM-Dataset, which is utilized in\nimproving the representation learning ability of different types of cross-modal\nretrieval (CMR). Little large music dataset including three modalities is\navailable for learning representations for CMR. To collect a music dataset, we\nexpand the original musical notation to synthesize audio and generated\nsheet-music image, and build musical notation based sheet-music image, audio\nclip and syllable-denotation text as fine-grained alignment, such that the\nMusicTM-Dataset can be exploited to receive shared representation for\nmultimodal data points. The MusicTM-Dataset presents 3 kinds of modalities,\nwhich consists of the image of sheet-music, the text of lyrics and synthesized\naudio, their representations are extracted by some advanced models. In this\npaper, we introduce the background of music dataset and express the process of\nour data collection. Based on our dataset, we achieve some basic methods for\nCMR tasks. The MusicTM-Dataset are accessible in https:\n//github.com/dddzeng/MusicTM-Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:18:53 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 14:20:13 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "2012.00346", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "Goluck Konuko, Giuseppe Valenzise, St\\'ephane Lathuili\\`ere", "title": "Ultra-low bitrate video conferencing using deep image animation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we propose a novel deep learning approach for ultra-low bitrate\nvideo compression for video conferencing applications. To address the\nshortcomings of current video compression paradigms when the available\nbandwidth is extremely limited, we adopt a model-based approach that employs\ndeep neural networks to encode motion information as keypoint displacement and\nreconstruct the video signal at the decoder side. The overall system is trained\nin an end-to-end fashion minimizing a reconstruction error on the encoder\noutput. Objective and subjective quality evaluation experiments demonstrate\nthat the proposed approach provides an average bitrate reduction for the same\nvisual quality of more than 80% compared to HEVC.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:06:34 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Konuko", "Goluck", ""], ["Valenzise", "Giuseppe", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""]]}, {"id": "2012.00597", "submitter": "Mahmoud Darwich", "authors": "Mahmoud Darwich, Ege Beyazit, Mohsen Amini Salehiy, Magdy Bayoumi", "title": "Cost Efficient Repository Management for Cloud-Based On-Demand Video\n  Streaming", "comments": null, "journal-ref": null, "doi": "10.1109/MobileCloud.2017.23", "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video transcoding is the process of converting a video to the format\nsupported by the viewer's device. Video transcoding requires huge storage and\ncomputational resources, thus, many video stream providers choose to carry it\nout on the cloud. Video streaming providers generally need to prepare several\nformats of the same video (termed pre-transcoding) and stream the appropriate\nformat to the viewer. However, pre-transcoding requires enormous storage space\nand imposes a significant cost to the stream provider. More importantly,\npre-transcoding proven to be inefficient due to the long-tail access pattern to\nvideo streams in a repository. To reduce the incurred cost, in this research,\nwe propose a method to partially pre-transcode video streams and re-transcode\nthe rest of it in an on-demand manner. We will develop a method to strike a\ntrade-off between pre-transcoding and on-demand transcoding of video streams to\nreduce the overall cost. Experimental results show the efficiency of our\napproach, particularly, when a high percentage of videos are accessed\nfrequently. In such repositories, the proposed approach reduces the incurred\ncost by up to 70\\%.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:06:12 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Darwich", "Mahmoud", ""], ["Beyazit", "Ege", ""], ["Salehiy", "Mohsen Amini", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "2012.00641", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey", "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning", "comments": "Published by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3080920", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:12:30 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 09:22:01 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Dubey", "Shiv Ram", ""]]}, {"id": "2012.01107", "submitter": "Mark Scanlon", "authors": "Samuel Todd Bromley and John Sheppard and Mark Scanlon and Nhien-An\n  Le-Khac", "title": "Retracing the Flow of the Stream: Investigating Kodi Streaming Services", "comments": null, "journal-ref": "Digital Forensics and Cyber Crime: 11th EAI International\n  Conference on Digital Forensics and Cybercrime (ICDF2C), Boston, USA,\n  September 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kodi is of one of the world's largest open-source streaming platforms for\nviewing video content. Easily installed Kodi add-ons facilitate access to\nonline pirated videos and streaming content by facilitating the user to search\nand view copyrighted videos with a basic level of technical knowledge. In some\ncountries, there have been paid child sexual abuse organizations\npublishing/streaming child abuse material to an international paying clientele.\nOpen source software used for viewing videos from the Internet, such as Kodi,\nis being exploited by criminals to conduct their activities. In this paper, we\ndescribe a new method to quickly locate Kodi artifacts and gather information\nfor a successful prosecution. We also evaluate our approach on different\nplatforms; Windows, Android and Linux. Our experiments show the file location,\nartifacts and a history of viewed content including their locations from the\nInternet. Our approach will serve as a resource to forensic investigators to\nexamine Kodi or similar streaming platforms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:47:20 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Bromley", "Samuel Todd", ""], ["Sheppard", "John", ""], ["Scanlon", "Mark", ""], ["Le-Khac", "Nhien-An", ""]]}, {"id": "2012.01837", "submitter": "Haohan Guo", "authors": "Haohan Guo, Heng Lu, Na Hu, Chunlei Zhang, Shan Yang, Lei Xie, Dan Su,\n  Dong Yu", "title": "Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via\n  Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an end-to-end adversarial singing voice conversion\n(EA-SVC) approach. It can directly generate arbitrary singing waveform by given\nphonetic posteriorgram (PPG) representing content, F0 representing pitch, and\nspeaker embedding representing timbre, respectively. Proposed system is\ncomposed of three modules: generator $G$, the audio generation discriminator\n$D_{A}$, and the feature disentanglement discriminator $D_F$. The generator $G$\nencodes the features in parallel and inversely transforms them into the target\nwaveform. In order to make timbre conversion more stable and controllable,\nspeaker embedding is further decomposed to the weighted sum of a group of\ntrainable vectors representing different timbre clusters. Further, to realize\nmore robust and accurate singing conversion, disentanglement discriminator\n$D_F$ is proposed to remove pitch and timbre related information that remains\nin the encoded PPG. Finally, a two-stage training is conducted to keep a stable\nand effective adversarial training process. Subjective evaluation results\ndemonstrate the effectiveness of our proposed methods. Proposed system\noutperforms conventional cascade approach and the WaveNet based end-to-end\napproach in terms of both singing quality and singer similarity. Further\nobjective analysis reveals that the model trained with the proposed two-stage\ntraining strategy can produce a smoother and sharper formant which leads to\nhigher audio quality.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 11:13:27 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Guo", "Haohan", ""], ["Lu", "Heng", ""], ["Hu", "Na", ""], ["Zhang", "Chunlei", ""], ["Yang", "Shan", ""], ["Xie", "Lei", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "2012.01955", "submitter": "Gustavo Marfia", "authors": "Lorenzo Stacchio, Alessia Angeli, Giuseppe Lisanti, Daniela Calanca,\n  Gustavo Marfia", "title": "IMAGO: A family photo album dataset for a socio-historical analysis of\n  the twentieth century", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although one of the most popular practices in photography since the end of\nthe 19th century, an increase in scholarly interest in family photo albums\ndates back to the early 1980s. Such collections of photos may reveal\nsociological and historical insights regarding specific cultures and times.\nThey are, however, in most cases scattered among private homes and only\navailable on paper or photographic film, thus making their analysis by\nacademics such as historians, social-cultural anthropologists and cultural\ntheorists very cumbersome. In this paper, we analyze the IMAGO dataset\nincluding photos belonging to family albums assembled at the University of\nBologna's Rimini campus since 2004. Following a deep learning-based approach,\nthe IMAGO dataset has offered the opportunity of experimenting with photos\ntaken between year 1845 and year 2009, with the goals of assessing the dates\nand the socio-historical contexts of the images, without use of any other\nsources of information. Exceeding our initial expectations, such analysis has\nrevealed its merit not only in terms of the performance of the approach adopted\nin this work, but also in terms of the foreseeable implications and use for the\nbenefit of socio-historical research. To the best of our knowledge, this is the\nfirst work that moves along this path in literature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:28:58 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Stacchio", "Lorenzo", ""], ["Angeli", "Alessia", ""], ["Lisanti", "Giuseppe", ""], ["Calanca", "Daniela", ""], ["Marfia", "Gustavo", ""]]}, {"id": "2012.01980", "submitter": "Wei Zhou", "authors": "Wei Zhou and Zhibo Chen", "title": "Deep Multi-Scale Features Learning for Distorted Image Quality\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image quality assessment (IQA) aims to estimate human perception based image\nvisual quality. Although existing deep neural networks (DNNs) have shown\nsignificant effectiveness for tackling the IQA problem, it still needs to\nimprove the DNN-based quality assessment models by exploiting efficient\nmulti-scale features. In this paper, motivated by the human visual system (HVS)\ncombining multi-scale features for perception, we propose to use pyramid\nfeatures learning to build a DNN with hierarchical multi-scale features for\ndistorted image quality prediction. Our model is based on both residual maps\nand distorted images in luminance domain, where the proposed network contains\nspatial pyramid pooling and feature pyramid from the network structure. Our\nproposed network is optimized in a deep end-to-end supervision manner. To\nvalidate the effectiveness of the proposed method, extensive experiments are\nconducted on four widely-used image quality assessment databases, demonstrating\nthe superiority of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:39:01 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2012.02256", "submitter": "Alexander Ivchenko Vladimirovich", "authors": "Raoul Nigmatullin, Semyon Dorokhin, Alexander Ivchenko", "title": "A Novel Approach to Radiometric Identification", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": "10.3233/FAIA200806", "report-no": null, "categories": "eess.SP cs.CR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper demonstrates that highly accurate radiometric identification is\npossible using CAPoNeF feature engineering method. We tested basic ML\nclassification algorithms on experimental data gathered by SDR. The statistical\nand correlational properties of suggested features were analyzed first with the\nhelp of Point Biserial and Pearson Correlation Coefficients and then using\nP-values. The most relevant features were highlighted. Random Forest provided\n99% accuracy. We give LIME description of model behavior. It turns out that\neven if the dimension of the feature space is reduced to 3, it is still\npossible to classify devices with 99% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 10:54:44 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Nigmatullin", "Raoul", ""], ["Dorokhin", "Semyon", ""], ["Ivchenko", "Alexander", ""]]}, {"id": "2012.02371", "submitter": "Xl Li", "authors": "Songhai Zhang and Xiangli Li and Yingtian Liu and Hongbo Fu", "title": "Scale-aware Insertion of Virtual Objects in Monocular Videos", "comments": "Accepted at ISMAR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a scale-aware method for inserting virtual objects\nwith proper sizes into monocular videos. To tackle the scale ambiguity problem\nof geometry recovery from monocular videos, we estimate the global scale\nobjects in a video with a Bayesian approach incorporating the size priors of\nobjects, where the scene objects sizes should strictly conform to the same\nglobal scale and the possibilities of global scales are maximized according to\nthe size distribution of object categories. To do so, we propose a dataset of\nsizes of object categories: Metric-Tree, a hierarchical representation of sizes\nof more than 900 object categories with the corresponding images. To handle the\nincompleteness of objects recovered from videos, we propose a novel scale\nestimation method that extracts plausible dimensions of objects for scale\noptimization. Experiments have shown that our method for scale estimation\nperforms better than the state-of-the-art methods, and has considerable\nvalidity and robustness for different video scenes. Metric-Tree has been made\navailable at: https://metric-tree.github.io\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:25:24 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Zhang", "Songhai", ""], ["Li", "Xiangli", ""], ["Liu", "Yingtian", ""], ["Fu", "Hongbo", ""]]}, {"id": "2012.02639", "submitter": "Edward Fish", "authors": "Edward Fish, Jon Weinbren, Andrew Gilbert", "title": "Rethinking movie genre classification with fine-grained semantic\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Movie genre classification is an active research area in machine learning.\nHowever, due to the limited labels available, there can be large semantic\nvariations between movies within a single genre definition. We expand these\n'coarse' genre labels by identifying 'fine-grained' semantic information within\nthe multi-modal content of movies. By leveraging pre-trained 'expert' networks,\nwe learn the influence of different combinations of modes for multi-label genre\nclassification. Using a contrastive loss, we continue to fine-tune this\n'coarse' genre classification network to identify high-level intertextual\nsimilarities between the movies across all genre labels. This leads to a more\n'fine-grained' and detailed clustering, based on semantic similarities while\nstill retaining some genre information. Our approach is demonstrated on a newly\nintroduced multi-modal 37,866,450 frame, 8,800 movie trailer dataset,\nMMX-Trailer-20, which includes pre-computed audio, location, motion, and image\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:58:31 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 10:30:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 16:46:09 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Fish", "Edward", ""], ["Weinbren", "Jon", ""], ["Gilbert", "Andrew", ""]]}, {"id": "2012.02961", "submitter": "Yerbolat Khassanov", "authors": "Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat\n  Khassanov, Michael Lewis and Huseyin Atakan Varol", "title": "SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with\n  Visual and Thermal Video Streams", "comments": "20 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present SpeakingFaces as a publicly-available large-scale multimodal\ndataset developed to support machine learning research in contexts that utilize\na combination of thermal, visual, and audio data streams; examples include\nhuman-computer interaction, biometric authentication, recognition systems,\ndomain transfer, and speech recognition. SpeakingFaces is comprised of aligned\nhigh-resolution thermal and visual spectra image streams of fully-framed faces\nsynchronized with audio recordings of each subject speaking approximately 100\nimperative phrases. Data were collected from 142 subjects, yielding over 13,000\ninstances of synchronized data (~3.8 TB). For technical validation, we\ndemonstrate two baseline examples. The first baseline shows classification by\ngender, utilizing different combinations of the three data streams in both\nclean and noisy environments. The second example consists of thermal-to-visual\nfacial image translation, as an instance of domain transfer.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 06:49:42 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:00:44 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 05:27:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Abdrakhmanova", "Madina", ""], ["Kuzdeuov", "Askat", ""], ["Jarju", "Sheikh", ""], ["Khassanov", "Yerbolat", ""], ["Lewis", "Michael", ""], ["Varol", "Huseyin Atakan", ""]]}, {"id": "2012.03308", "submitter": "Weihao Xia", "authors": "Weihao Xia and Yujiu Yang and Jing-Hao Xue and Baoyuan Wu", "title": "TediGAN: Text-Guided Diverse Face Image Generation and Manipulation", "comments": "CVPR 2021. Code: https://github.com/weihaox/TediGAN Data:\n  https://github.com/weihaox/Multi-Modal-CelebA-HQ Video:\n  https://youtu.be/L8Na2f5viAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose TediGAN, a novel framework for multi-modal image\ngeneration and manipulation with textual descriptions. The proposed method\nconsists of three components: StyleGAN inversion module, visual-linguistic\nsimilarity learning, and instance-level optimization. The inversion module maps\nreal images to the latent space of a well-trained StyleGAN. The\nvisual-linguistic similarity learns the text-image matching by mapping the\nimage and text into a common embedding space. The instance-level optimization\nis for identity preservation in manipulation. Our model can produce diverse and\nhigh-quality images with an unprecedented resolution at 1024. Using a control\nmechanism based on style-mixing, our TediGAN inherently supports image\nsynthesis with multi-modal inputs, such as sketches or semantic labels, with or\nwithout instance guidance. To facilitate text-guided multi-modal synthesis, we\npropose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real\nface images and corresponding semantic segmentation map, sketch, and textual\ndescriptions. Extensive experiments on the introduced dataset demonstrate the\nsuperior performance of our proposed method. Code and data are available at\nhttps://github.com/weihaox/TediGAN.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 16:20:19 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 11:52:51 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 06:40:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Wu", "Baoyuan", ""]]}, {"id": "2012.03581", "submitter": "Francesco Picetti", "authors": "Francesco Picetti, Sara Mandelli, Paolo Bestagini, Vincenzo Lipari and\n  Stefano Tubaro", "title": "DIPPAS: A Deep Image Prior PRNU Anonymization Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source device identification is an important topic in image forensics since\nit allows to trace back the origin of an image. Its forensics counter-part is\nsource device anonymization, that is, to mask any trace on the image that can\nbe useful for identifying the source device. A typical trace exploited for\nsource device identification is the Photo Response Non-Uniformity (PRNU), a\nnoise pattern left by the device on the acquired images. In this paper, we\ndevise a methodology for suppressing such a trace from natural images without\nsignificant impact on image quality. Specifically, we turn PRNU anonymization\ninto an optimization problem in a Deep Image Prior (DIP) framework. In a\nnutshell, a Convolutional Neural Network (CNN) acts as generator and returns an\nimage that is anonymized with respect to the source PRNU, still maintaining\nhigh visual quality. With respect to widely-adopted deep learning paradigms,\nour proposed CNN is not trained on a set of input-target pairs of images.\nInstead, it is optimized to reconstruct the PRNU-free image from the original\nimage under analysis itself. This makes the approach particularly suitable in\nscenarios where large heterogeneous databases are analyzed and prevents any\nproblem due to lack of generalization. Through numerical examples on publicly\navailable datasets, we prove our methodology to be effective compared to\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:56:50 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Picetti", "Francesco", ""], ["Mandelli", "Sara", ""], ["Bestagini", "Paolo", ""], ["Lipari", "Vincenzo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2012.03805", "submitter": "Ruibin Yuan", "authors": "Ruibin Yuan, Ge Zhang, Anqiao Yang, Xinyue Zhang", "title": "Diverse Melody Generation from Chinese Lyrics via Mutual Information\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to adapt the method of mutual information\nmaximization into the task of Chinese lyrics conditioned melody generation to\nimprove the generation quality and diversity. We employ scheduled sampling and\nforce decoding techniques to improve the alignment between lyrics and melodies.\nWith our method, which we called Diverse Melody Generation (DMG), a\nsequence-to-sequence model learns to generate diverse melodies heavily\ndepending on the input style ids, while keeping the tonality and improving the\nalignment. The experimental results of subjective tests show that DMG can\ngenerate more pleasing and coherent tunes than baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:48:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yuan", "Ruibin", ""], ["Zhang", "Ge", ""], ["Yang", "Anqiao", ""], ["Zhang", "Xinyue", ""]]}, {"id": "2012.04264", "submitter": "Chih-Hung Liang", "authors": "Chih-Hung Liang, Yu-An Chen, Yueh-Cheng Liu, Winston H. Hsu", "title": "Raw Image Deblurring", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based blind image deblurring plays an essential role in solving\nimage blur since all existing kernels are limited in modeling the real world\nblur. Thus far, researchers focus on powerful models to handle the deblurring\nproblem and achieve decent results. For this work, in a new aspect, we discover\nthe great opportunity for image enhancement (e.g., deblurring) directly from\nRAW images and investigate novel neural network structures benefiting RAW-based\nlearning. However, to the best of our knowledge, there is no available RAW\nimage deblurring dataset. Therefore, we built a new dataset containing both RAW\nimages and processed sRGB images and design a new model to utilize the unique\ncharacteristics of RAW images. The proposed deblurring model, trained solely\nfrom RAW images, achieves the state-of-art performance and outweighs those\ntrained on processed sRGB images. Furthermore, with fine-tuning, the proposed\nmodel, trained on our new dataset, can generalize to other sensors.\nAdditionally, by a series of experiments, we demonstrate that existing\ndeblurring models can also be improved by training on the RAW images in our new\ndataset. Ultimately, we show a new venue for further opportunities based on the\ndevised novel raw-based deblurring method and the brand-new Deblur-RAW dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:03:09 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liang", "Chih-Hung", ""], ["Chen", "Yu-An", ""], ["Liu", "Yueh-Cheng", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2012.04446", "submitter": "Jia Guo", "authors": "Jia Guo, Chen Zhu, Yilun Zhao, Heda Wang, Yao Hu, Xiaofei He, Deng Cai", "title": "LAMP: Label Augmented Multimodal Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal representation learning by pretraining has become an increasing\ninterest due to its easy-to-use and potential benefit for various\nVisual-and-Language~(V-L) tasks. However its requirement of large volume and\nhigh-quality vision-language pairs highly hinders its values in practice. In\nthis paper, we proposed a novel label-augmented V-L pretraining model, named\nLAMP, to address this problem. Specifically, we leveraged auto-generated labels\nof visual objects to enrich vision-language pairs with fine-grained alignment\nand correspondingly designed a novel pretraining task. Besides, we also found\nsuch label augmentation in second-stage pretraining would further universally\nbenefit various downstream tasks. To evaluate LAMP, we compared it with some\nstate-of-the-art models on four downstream tasks. The quantitative results and\nanalysis have well proven the value of labels in V-L pretraining and the\neffectiveness of LAMP.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:32:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Guo", "Jia", ""], ["Zhu", "Chen", ""], ["Zhao", "Yilun", ""], ["Wang", "Heda", ""], ["Hu", "Yao", ""], ["He", "Xiaofei", ""], ["Cai", "Deng", ""]]}, {"id": "2012.04623", "submitter": "Alexander Ivchenko Vladimirovich", "authors": "Aleksandr Ivchenko, Pavel Kononyuk, Alexander Dvorkovich, Liubov\n  Antiufrieva", "title": "Study on the Assessment of the Quality of Experience of Streaming Video", "comments": "11 pages, 16 figures, 7 tables", "journal-ref": "Systems of Signal Synchronization, Generating and Processing in\n  Telecommunications (SYNCHROINFO). IEEE, 2020", "doi": "10.1109/SYNCHROINFO49631.2020.9166092", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic adaptive streaming over HTTP provides the work of most multimedia\nservices, however, the nature of this technology further complicates the\nassessment of the QoE (Quality of Experience). In this paper, the influence of\nvarious objective factors on the subjective estimation of the QoE of streaming\nvideo is studied. The paper presents standard and handcrafted features, shows\ntheir correlation and p-Value of significance. VQA (Video Quality Assessment)\nmodels based on regression and gradient boosting with SRCC reaching up to\n0.9647 on the validation subsample are proposed. The proposed regression models\nare adapted for applied applications (both with and without a reference video);\nthe Gradient Boosting Regressor model is perspective for further improvement of\nthe quality estimation model. We take SQoE-III database, so far the largest and\nmost realistic of its kind. The VQA (video quality assessment) models are\navailable at https://github.com/AleksandrIvchenko/QoE-assesment\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:46:09 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Ivchenko", "Aleksandr", ""], ["Kononyuk", "Pavel", ""], ["Dvorkovich", "Alexander", ""], ["Antiufrieva", "Liubov", ""]]}, {"id": "2012.04925", "submitter": "Aozhu Chen", "authors": "Aozhu Chen, Xinyi Huang, Hailan Lin, Xirong Li", "title": "Towards Annotation-Free Evaluation of Cross-Lingual Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual image captioning, with its ability to caption an unlabeled\nimage in a target language other than English, is an emerging topic in the\nmultimedia field. In order to save the precious human resource from re-writing\nreference sentences per target language, in this paper we make a brave attempt\ntowards annotation-free evaluation of cross-lingual image captioning. Depending\non whether we assume the availability of English references, two scenarios are\ninvestigated. For the first scenario with the references available, we propose\ntwo metrics, i.e., WMDRel and CLinRel. WMDRel measures the semantic relevance\nbetween a model-generated caption and machine translation of an English\nreference using their Word Mover's Distance. By projecting both captions into a\ndeep visual feature space, CLinRel is a visual-oriented cross-lingual relevance\nmeasure. As for the second scenario, which has zero reference and is thus more\nchallenging, we propose CMedRel to compute a cross-media relevance between the\ngenerated caption and the image content, in the same visual feature space as\nused by CLinRel. The promising results show high potential of the new metrics\nfor evaluation with no need of references in the target language.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:09:22 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Chen", "Aozhu", ""], ["Huang", "Xinyi", ""], ["Lin", "Hailan", ""], ["Li", "Xirong", ""]]}, {"id": "2012.05342", "submitter": "Pierre-Etienne Martin", "authors": "Pierre-Etienne Martin (LaBRI, UB), Jenny Benois-Pineau (LaBRI), Renaud\n  P\\'eteri, Julien Morlier", "title": "3D attention mechanism for fine-grained classification of table tennis\n  strokes using a Twin Spatio-Temporal Convolutional Neural Networks", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020),\n  Jan 2021, Milano, Italy", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of recognition of actions in video with low\ninter-class variability such as Table Tennis strokes. Two stream, \"twin\"\nconvolutional neural networks are used with 3D convolutions both on RGB data\nand optical flow. Actions are recognized by classification of temporal windows.\nWe introduce 3D attention modules and examine their impact on classification\nefficiency. In the context of the study of sportsmen performances, a corpus of\nthe particular actions of table tennis strokes is considered. The use of\nattention blocks in the network speeds up the training step and improves the\nclassification scores up to 5% with our twin model. We visualize the impact on\nthe obtained features and notice correlation between attention and player\nmovements and position. Score comparison of state-of-the-art action\nclassification method and proposed approach with attentional blocks is\nperformed on the corpus. Proposed model with attention blocks outperforms\nprevious model without them and our baseline.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 09:55:12 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Martin", "Pierre-Etienne", "", "LaBRI, UB"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["P\u00e9teri", "Renaud", ""], ["Morlier", "Julien", ""]]}, {"id": "2012.05429", "submitter": "Xuefeng Liang", "authors": "Ying Zhou, Xuefeng Liang, Yu Gu, Yifei Yin, Longshan Yao", "title": "Multi-Classifier Interactive Learning for Ambiguous Speech Emotion\n  Recognition", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, speech emotion recognition technology is of great\nsignificance in industrial applications such as call centers, social robots and\nhealth care. The combination of speech recognition and speech emotion\nrecognition can improve the feedback efficiency and the quality of service.\nThus, the speech emotion recognition has been attracted much attention in both\nindustry and academic. Since emotions existing in an entire utterance may have\nvaried probabilities, speech emotion is likely to be ambiguous, which poses\ngreat challenges to recognition tasks. However, previous studies commonly\nassigned a single-label or multi-label to each utterance in certain. Therefore,\ntheir algorithms result in low accuracies because of the inappropriate\nrepresentation. Inspired by the optimally interacting theory, we address the\nambiguous speech emotions by proposing a novel multi-classifier interactive\nlearning (MCIL) method. In MCIL, multiple different classifiers first mimic\nseveral individuals, who have inconsistent cognitions of ambiguous emotions,\nand construct new ambiguous labels (the emotion probability distribution).\nThen, they are retrained with the new labels to interact with their cognitions.\nThis procedure enables each classifier to learn better representations of\nambiguous data from others, and further improves the recognition ability. The\nexperiments on three benchmark corpora (MAS, IEMOCAP, and FAU-AIBO) demonstrate\nthat MCIL does not only improve each classifier's performance, but also raises\ntheir recognition consistency from moderate to substantial.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 02:58:34 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 14:59:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhou", "Ying", ""], ["Liang", "Xuefeng", ""], ["Gu", "Yu", ""], ["Yin", "Yifei", ""], ["Yao", "Longshan", ""]]}, {"id": "2012.05696", "submitter": "Mustafa Othman", "authors": "Mustafa Othman, Ken Chen, Anissa Mokraoui", "title": "A User-experience Driven SSIM-Aware Adaptation Approach for DASH Video\n  Streaming", "comments": "6 pages, (CENTRIC 2019) The Twelfth International Conference on\n  Advances in Human-oriented and Personalized Mechanisms, Technologies, and\n  Services", "journal-ref": null, "doi": null, "report-no": "ISBN: 978-1-61208-754-2", "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dynamic Adaptive Streaming over HTTP (DASH) is a video streaming technique\nlargely used. One key point is the adaptation mechanism which resides at the\nclient's side. This mechanism impacts greatly on the overall Quality of\nExperience (QoE) of the video streaming. In this paper, we propose a new\nadaptation algorithm for DASH, namely SSIM Based Adaptation (SBA). This\nmechanism is user-experience driven: it uses the Structural Similarity Index\nMeasurement (SSIM) as main video perceptual quality indicator; moreover, the\nadaptation is based on a joint consideration of SSIM indicator and the physical\nresources (buffer occupancy, bandwidth) in order to minimize the buffer\nstarvation (rebuffering) and video quality instability, as well as to maximize\nthe overall video quality (through SSIM). To evaluate the performance of our\nproposal, we carried out trace-driven emulation with real traffic traces\n(captured in real mobile network). Comparisons with some representative\nalgorithms (BBA, FESTIVE, OSMF) through major QoE metrics show that our\nadaptation algorithm SBA achieves an efficient adaptation minimizing both the\nrebuffering and instability, whereas the displayed video is maintained at a\nhigh level of bitrate.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:25:55 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Othman", "Mustafa", ""], ["Chen", "Ken", ""], ["Mokraoui", "Anissa", ""]]}, {"id": "2012.06021", "submitter": "Chavit Denninnart", "authors": "Shangrui Wu, Chavit Denninnart, Xiangbo Li, Yang Wang, Mohsen Amini\n  Salehi", "title": "Descriptive and Predictive Analysis of Aggregating Functions in\n  Serverless Clouds: the Case of Video Streaming", "comments": null, "journal-ref": "IEEE HPCC 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless clouds allocate multiple tasks (e.g., micro-services) from\nmultiple users on a shared pool of computing resources. This enables serverless\ncloud providers to reduce their resource usage by transparently aggregate\nsimilar tasks of a certain context (e.g., video processing) that share the\nwhole or part of their computation. To this end, it is crucial to know the\namount of time-saving achieved by aggregating the tasks. Lack of such knowledge\ncan lead to uninformed merging and scheduling decisions that, in turn, can\ncause deadline violation of either the merged tasks or other following tasks.\nAccordingly, in this paper, we study the problem of estimating execution-time\nsaving resulted from merging tasks with the example in the context of video\nprocessing. To learn the execution-time saving in different forms of merging,\nwe first establish a set of benchmarking videos and examine a wide variety of\nvideo processing tasks -- with and without merging in place. We observed that\nalthough merging can save up to 44% in the execution-time, the number of\npossible merging cases is intractable. Hence, in the second part, we leverage\nthe benchmarking results and develop a method based on Gradient Boosting\nDecision Tree (GBDT) to estimate the time-saving for any given task merging\ncase. Experimental results show that the method can estimate the time-saving\nwith the error rate of 0.04, measured based on Root Mean Square Error (RMSE).\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:37:01 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wu", "Shangrui", ""], ["Denninnart", "Chavit", ""], ["Li", "Xiangbo", ""], ["Wang", "Yang", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2012.06567", "submitter": "Yi Zhu", "authors": "Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza Zolfaghari, Yuanjun Xiong,\n  Chongruo Wu, Zhi Zhang, Joseph Tighe, R. Manmatha, Mu Li", "title": "A Comprehensive Study of Deep Video Action Recognition", "comments": "Technical report. Code and model zoo can be found at\n  https://cv.gluon.ai/model_zoo/action_recognition.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action recognition is one of the representative tasks for video\nunderstanding. Over the last decade, we have witnessed great advancements in\nvideo action recognition thanks to the emergence of deep learning. But we also\nencountered new challenges, including modeling long-range temporal information\nin videos, high computation costs, and incomparable results due to datasets and\nevaluation protocol variances. In this paper, we provide a comprehensive survey\nof over 200 existing papers on deep learning for video action recognition. We\nfirst introduce the 17 video action recognition datasets that influenced the\ndesign of models. Then we present video action recognition models in\nchronological order: starting with early attempts at adapting deep learning,\nthen to the two-stream networks, followed by the adoption of 3D convolutional\nkernels, and finally to the recent compute-efficient models. In addition, we\nbenchmark popular methods on several representative datasets and release code\nfor reproducibility. In the end, we discuss open problems and shed light on\nopportunities for video action recognition to facilitate new research ideas.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:54:08 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhu", "Yi", ""], ["Li", "Xinyu", ""], ["Liu", "Chunhui", ""], ["Zolfaghari", "Mohammadreza", ""], ["Xiong", "Yuanjun", ""], ["Wu", "Chongruo", ""], ["Zhang", "Zhi", ""], ["Tighe", "Joseph", ""], ["Manmatha", "R.", ""], ["Li", "Mu", ""]]}, {"id": "2012.06735", "submitter": "Yu Yin", "authors": "Yu Yin, Joseph P. Robinson, Yun Fu", "title": "Multimodal In-bed Pose and Shape Estimation under the Blankets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans spend vast hours in bed -- about one-third of the lifetime on average.\nBesides, a human at rest is vital in many healthcare applications. Typically,\nhumans are covered by a blanket when resting, for which we propose a multimodal\napproach to uncover the subjects so their bodies at rest can be viewed without\nthe occlusion of the blankets above. We propose a pyramid scheme to effectively\nfuse the different modalities in a way that best leverages the knowledge\ncaptured by the multimodal sensors. Specifically, the two most informative\nmodalities (i.e., depth and infrared images) are first fused to generate good\ninitial pose and shape estimation. Then pressure map and RGB images are further\nfused one by one to refine the result by providing occlusion-invariant\ninformation for the covered part, and accurate shape information for the\nuncovered part, respectively. However, even with multimodal data, the task of\ndetecting human bodies at rest is still very challenging due to the extreme\nocclusion of bodies. To further reduce the negative effects of the occlusion\nfrom blankets, we employ an attention-based reconstruction module to generate\nuncovered modalities, which are further fused to update current estimation via\na cyclic fashion. Extensive experiments validate the superiority of the\nproposed model over others.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:35:23 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yin", "Yu", ""], ["Robinson", "Joseph P.", ""], ["Fu", "Yun", ""]]}, {"id": "2012.06809", "submitter": "Laijin Meng", "authors": "Laijin Meng, Xinghao Jiang, Zhenzhen Zhang, Zhaohong Li, and Tanfeng\n  Sun", "title": "Coverless Video Steganography based on Maximum DC Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coverless steganography has been a great interest in recent years, since it\nis a technology that can absolutely resist the detection of steganalysis by not\nmodifying the carriers. However, most existing coverless steganography\nalgorithms select images as carriers, and few studies are reported on coverless\nvideo steganography. In fact, video is a securer and more informative carrier.\nIn this paper, a novel coverless video steganography algorithm based on maximum\nDirect Current (DC) coefficients is proposed. Firstly, a Gaussian distribution\nmodel of DC coefficients considering video coding process is built, which\nindicates that the distribution of changes for maximum DC coefficients in a\nblock is more stable than the adjacent DC coefficients. Then, a novel hash\nsequence generation method based on the maximum DC coefficients is proposed.\nAfter that, the video index structure is established to speed up the efficiency\nof searching videos. In the process of information hiding, the secret\ninformation is converted into binary segments, and the video whose hash\nsequence equals to secret information segment is selected as the carrier\naccording to the video index structure. Finally, all of the selected videos and\nauxiliary information are sent to the receiver. Especially, the subjective\nsecurity of video carriers, the cost of auxiliary information and the\nrobustness to video compression are considered for the first time in this\npaper. Experimental results and analysis show that the proposed algorithm\nperforms better in terms of capacity, robustness, and security, compared with\nthe state-of-the-art coverless steganography algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 13:21:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Meng", "Laijin", ""], ["Jiang", "Xinghao", ""], ["Zhang", "Zhenzhen", ""], ["Li", "Zhaohong", ""], ["Sun", "Tanfeng", ""]]}, {"id": "2012.07430", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Steven Hicks, P{\\aa}l Halvorsen, Michael A. Riegler", "title": "Pyramid-Focus-Augmentation: Medical Image Segmentation with Step-Wise\n  Focus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of findings in the gastrointestinal tract is a challenging but\nalso an important task which is an important building stone for sufficient\nautomatic decision support systems. In this work, we present our solution for\nthe Medico 2020 task, which focused on the problem of colon polyp segmentation.\nWe present our simple but efficient idea of using an augmentation method that\nuses grids in a pyramid-like manner (large to small) for segmentation. Our\nresults show that the proposed methods work as indented and can also lead to\ncomparable results when competing with other methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:34:29 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Thambawita", "Vajira", ""], ["Hicks", "Steven", ""], ["Halvorsen", "P\u00e5l", ""], ["Riegler", "Michael A.", ""]]}, {"id": "2012.08034", "submitter": "Matthew Adiletta", "authors": "Matthew Joseph Adiletta, Oliver Thomas", "title": "An Artistic Visualization of Music Modeling a Synesthetic Experience", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This project brings music to sight. Music can be a visual masterpiece. Some\npeople naturally experience a visualization of audio - a condition called\nsynesthesia. The type of synesthesia explored is when sounds create colors in\nthe 'mind's eye.' Project included interviews with people who experience\nsynesthesia, examination of prior art, and topic research to inform project\ndesign. Audio input, digital signal processing (including Fast Fourier\nTransforms (FFTs)) and data manipulation produce arguments required for our\nvisualization. Arguments are then applied to a physics particle simulator which\nis re-purposed to model a synesthetic experience. The result of the project is\na simulator in MAX 8, which generates a visual performance using particles by\nvarying each particle's position, velocity, and color based on parameters\nextracted via digital processing of input audio.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 01:37:10 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Adiletta", "Matthew Joseph", ""], ["Thomas", "Oliver", ""]]}, {"id": "2012.08256", "submitter": "Dinesh Kumar Vishwakarma Dr", "authors": "Ashima Yadav and Dinesh Kumar Vishwakarma", "title": "A Deep Multi-Level Attentive network for Multimodal Sentiment Analysis", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal sentiment analysis has attracted increasing attention with broad\napplication prospects. The existing methods focuses on single modality, which\nfails to capture the social media content for multiple modalities. Moreover, in\nmulti-modal learning, most of the works have focused on simply combining the\ntwo modalities, without exploring the complicated correlations between them.\nThis resulted in dissatisfying performance for multimodal sentiment\nclassification. Motivated by the status quo, we propose a Deep Multi-Level\nAttentive network, which exploits the correlation between image and text\nmodalities to improve multimodal learning. Specifically, we generate the\nbi-attentive visual map along the spatial and channel dimensions to magnify\nCNNs representation power. Then we model the correlation between the image\nregions and semantics of the word by extracting the textual features related to\nthe bi-attentive visual features by applying semantic attention. Finally,\nself-attention is employed to automatically fetch the sentiment-rich multimodal\nfeatures for the classification. We conduct extensive evaluations on four\nreal-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty\nImages, which verifies the superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:47:17 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Yadav", "Ashima", ""], ["Vishwakarma", "Dinesh Kumar", ""]]}, {"id": "2012.08732", "submitter": "Weiling Chen", "authors": "Tiesong Zhao, Yuting Lin, Yiwen Xu, Weiling Chen, Zhou Wang", "title": "Learning-Based Quality Assessment for Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Super-Resolution (SR) techniques improve visual quality by enhancing\nthe spatial resolution of images. Quality evaluation metrics play a critical\nrole in comparing and optimizing SR algorithms, but current metrics achieve\nonly limited success, largely due to the lack of large-scale quality databases,\nwhich are essential for learning accurate and robust SR quality metrics. In\nthis work, we first build a large-scale SR image database using a novel\nsemi-automatic labeling approach, which allows us to label a large number of\nimages with manageable human workload. The resulting SR Image quality database\nwith Semi-Automatic Ratings (SISAR), so far the largest of SR-IQA database,\ncontains 8,400 images of 100 natural scenes. We train an end-to-end Deep Image\nSR Quality (DISQ) model by employing two-stream Deep Neural Networks (DNNs) for\nfeature extraction, followed by a feature fusion network for quality\nprediction. Experimental results demonstrate that the proposed method\noutperforms state-of-the-art metrics and achieves promising generalization\nperformance in cross-database tests. The SISAR database and DISQ model will be\nmade publicly available to facilitate reproducible research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:06:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhao", "Tiesong", ""], ["Lin", "Yuting", ""], ["Xu", "Yiwen", ""], ["Chen", "Weiling", ""], ["Wang", "Zhou", ""]]}, {"id": "2012.08742", "submitter": "Anna Melman", "authors": "Anna Melman, Pavel Petrov, Alexander Shelupanov", "title": "An adaptive algorithm for embedding information into compressed JPEG\n  images using the QIM method", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of JPEG images makes them good covers for secret messages\nstoring and transmitting. This paper proposes a new algorithm for embedding\ninformation in JPEG images based on the steganographic QIM method. The main\nproblem of such embedding is the vulnerability to statistical steganalysis. To\nsolve this problem, it is proposed to use a variable quantization step, which\nis adaptively selected for each block of the JPEG cover image. Experimental\nresults show that the proposed approach successfully increases the security of\nembedding.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:37:28 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Melman", "Anna", ""], ["Petrov", "Pavel", ""], ["Shelupanov", "Alexander", ""]]}, {"id": "2012.08865", "submitter": "Xiaowei Tang", "authors": "Xiao-Wei Tang, Changsheng You, Shuowen Zhang, Xin-Lin Huang, Rui Zhang", "title": "3D Trajectory Design for UAV-Assisted Oblique Image Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this correspondence, we consider a new unmanned aerial vehicle\n(UAV)-assisted oblique image acquisition system where a UAV is dispatched to\ntake images of multiple ground targets (GTs). To study the three-dimensional\n(3D) UAV trajectory design for image acquisition, we first propose a novel\nUAV-assisted oblique photography model, which characterizes the image\nresolution with respect to the UAV's 3D image-taking location. Then, we\nformulate a 3D UAV trajectory optimization problem to minimize the UAV's\ntraveling distance subject to the image resolution constraints. The formulated\nproblem is shown to be equivalent to a modified 3D traveling salesman problem\nwith neighbourhoods, which is NP-hard in general. To tackle this difficult\nproblem, we propose an iterative algorithm to obtain a high-quality suboptimal\nsolution efficiently, by alternately optimizing the UAV's 3D image-taking\nwaypoints and its visiting order for the GTs. Numerical results show that the\nproposed algorithm significantly reduces the UAV's traveling distance as\ncompared to various benchmark schemes, while meeting the image resolution\nrequirement.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 11:08:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Tang", "Xiao-Wei", ""], ["You", "Changsheng", ""], ["Zhang", "Shuowen", ""], ["Huang", "Xin-Lin", ""], ["Zhang", "Rui", ""]]}, {"id": "2012.08924", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u and Rafael F. Schaefer", "title": "Secret Key Agreement with Physical Unclonable Functions: An Optimality\n  Summary", "comments": "To appear in MDPI Entropy Journal. arXiv admin note: text overlap\n  with arXiv:2002.11687", "journal-ref": null, "doi": "10.3390/e23010016", "report-no": null, "categories": "eess.SP cs.CR cs.CV cs.IT cs.MM math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address security and privacy problems for digital devices and biometrics\nfrom an information-theoretic optimality perspective, where a secret key is\ngenerated for authentication, identification, message encryption/decryption, or\nsecure computations. A physical unclonable function (PUF) is a promising\nsolution for local security in digital devices and this review gives the most\nrelevant summary for information theorists, coding theorists, and signal\nprocessing community members who are interested in optimal PUF constructions.\nLow-complexity signal processing methods such as transform coding that are\ndeveloped to make the information-theoretic analysis tractable are discussed.\nThe optimal trade-offs between the secret-key, privacy-leakage, and storage\nrates for multiple PUF measurements are given. Proposed optimal code\nconstructions that jointly design the vector quantizer and error-correction\ncode parameters are listed. These constructions include modern and algebraic\ncodes such as polar codes and convolutional codes, both of which can achieve\nsmall block-error probabilities at short block lengths, corresponding to a\nsmall number of PUF circuits. Open problems in the PUF literature from a signal\nprocessing, information theory, coding theory, and hardware complexity\nperspectives and their combinations are listed to stimulate further\nadvancements in the research on local privacy and security.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:21:20 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Schaefer", "Rafael F.", ""]]}, {"id": "2012.09290", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal", "title": "Self-Supervised Sketch-to-Image Synthesis", "comments": "AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagining a colored realistic image from an arbitrarily drawn sketch is one\nof the human capabilities that we eager machines to mimic. Unlike previous\nmethods that either requires the sketch-image pairs or utilize low-quantity\ndetected edges as sketches, we study the exemplar-based sketch-to-image (s2i)\nsynthesis task in a self-supervised learning manner, eliminating the necessity\nof the paired sketch data. To this end, we first propose an unsupervised method\nto efficiently synthesize line-sketches for general RGB-only datasets. With the\nsynthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to\ndecouple the content/style features from sketches and RGB-images, and\nsynthesize images that are both content-faithful to the sketches and\nstyle-consistent to the RGB-images. While prior works employ either the\ncycle-consistence loss or dedicated attentional modules to enforce the\ncontent/style fidelity, we show AE's superior performance with pure\nself-supervisions. To further improve the synthesis quality in high resolution,\nwe also leverage an adversarial network to refine the details of synthetic\nimages. Extensive experiments on 1024*1024 resolution demonstrate a new\nstate-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art\ndatasets. Moreover, with the proposed sketch generator, the model shows a\npromising performance on style mixing and style transfer, which require\nsynthesized images to be both style-consistent and semantically meaningful. Our\ncode is available on\nhttps://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch,\nand please visit https://create.playform.io/my-projects?mode=sketch for an\nonline demo of our model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 22:14:06 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 20:40:27 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Bingchen", ""], ["Zhu", "Yizhe", ""], ["Song", "Kunpeng", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "2012.10657", "submitter": "Anurag Singh", "authors": "Anurag Singh, Deepak Kumar Sharma, Sudhir Kumar Sharma", "title": "Self-Supervision based Task-Specific Image Collection Summarization", "comments": "Differences between author about ranking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Successful applications of deep learning (DL) requires large amount of\nannotated data. This often restricts the benefits of employing DL to businesses\nand individuals with large budgets for data-collection and computation.\nSummarization offers a possible solution by creating much smaller\nrepresentative datasets that can allow real-time deep learning and analysis of\nbig data and thus democratize use of DL. In the proposed work, our aim is to\nexplore a novel approach to task-specific image corpus summarization using\nsemantic information and self-supervision. Our method uses a\nclassification-based Wasserstein generative adversarial network (CLSWGAN) as a\nfeature generating network. The model also leverages rotational invariance as\nself-supervision and classification on another task. All these objectives are\nadded on a features from resnet34 to make it discriminative and robust. The\nmodel then generates a summary at inference time by using K-means clustering in\nthe semantic embedding space. Thus, another main advantage of this model is\nthat it does not need to be retrained each time to obtain summaries of\ndifferent lengths which is an issue with current end-to-end models. We also\ntest our model efficacy by means of rigorous experiments both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 10:58:04 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 20:22:55 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 07:20:23 GMT"}, {"version": "v4", "created": "Fri, 1 Jan 2021 08:58:35 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Singh", "Anurag", ""], ["Sharma", "Deepak Kumar", ""], ["Sharma", "Sudhir Kumar", ""]]}, {"id": "2012.10739", "submitter": "Yuhao Zhu", "authors": "Sifan Ye, Ting Wu, Michael Jarvis, Yuhao Zhu", "title": "Digital Reconstruction of Elmina Castle for Mobile Virtual Reality via\n  Point-based Detail Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing 3D models from large, dense point clouds is critical to enable\nVirtual Reality (VR) as a platform for entertainment, education, and heritage\npreservation. Existing 3D reconstruction systems inevitably make trade-offs\nbetween three conflicting goals: the efficiency of reconstruction (e.g., time\nand memory requirements), the visual quality of the constructed scene, and the\nrendering speed on the VR device. This paper proposes a reconstruction system\nthat simultaneously meets all three goals. The key idea is to avoid the\nresource-demanding process of reconstructing a high-polygon mesh altogether.\nInstead, we propose to directly transfer details from the original point cloud\nto a low polygon mesh, which significantly reduces the reconstruction time and\ncost, preserves the scene details, and enables real-time rendering on mobile VR\ndevices.\n  While our technique is general, we demonstrate it in reconstructing cultural\nheritage sites. We for the first time digitally reconstruct the Elmina Castle,\na UNESCO world heritage site at Ghana, from billions of laser-scanned points.\nThe reconstruction process executes on low-end desktop systems without\nrequiring high processing power, making it accessible to the broad community.\nThe reconstructed scenes render on Oculus Go in 60 FPS, providing a real-time\nVR experience with high visual quality. Our project is part of the Digital\nElmina effort (http://digitalelmina.org/) between University of Rochester and\nUniversity of Ghana.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 17:11:43 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 14:49:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Sifan", ""], ["Wu", "Ting", ""], ["Jarvis", "Michael", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2012.10852", "submitter": "Sindhu Hegde", "authors": "Sindhu B Hegde, K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri,\n  C.V. Jawahar", "title": "Visual Speech Enhancement Without A Real Visual Stream", "comments": "10 pages, 4 figures, Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we re-think the task of speech enhancement in unconstrained\nreal-world environments. Current state-of-the-art methods use only the audio\nstream and are limited in their performance in a wide range of real-world\nnoises. Recent works using lip movements as additional cues improve the quality\nof generated speech over \"audio-only\" methods. But, these methods cannot be\nused for several applications where the visual stream is unreliable or\ncompletely absent. We propose a new paradigm for speech enhancement by\nexploiting recent breakthroughs in speech-driven lip synthesis. Using one such\nmodel as a teacher network, we train a robust student network to produce\naccurate lip movements that mask away the noise, thus acting as a \"visual noise\nfilter\". The intelligibility of the speech enhanced by our pseudo-lip approach\nis comparable (< 3% difference) to the case of using real lips. This implies\nthat we can exploit the advantages of using lip movements even in the absence\nof a real video stream. We rigorously evaluate our model using quantitative\nmetrics as well as human evaluations. Additional ablation studies and a demo\nvideo on our website containing qualitative comparisons and results clearly\nillustrate the effectiveness of our approach. We provide a demo video which\nclearly illustrates the effectiveness of our proposed approach on our website:\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/visual-speech-enhancement-without-a-real-visual-stream}.\nThe code and models are also released for future research:\n\\url{https://github.com/Sindhu-Hegde/pseudo-visual-speech-denoising}.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 06:02:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hegde", "Sindhu B", ""], ["Prajwal", "K R", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2012.10890", "submitter": "Guoqing Wang", "authors": "Chao Yang, Guoqing Wang, Dongsheng Li, Huawei Shen, Su Feng, Bin Jiang", "title": "PPGN: Phrase-Guided Proposal Generation Network For Referring Expression\n  Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reference expression comprehension (REC) aims to find the location that the\nphrase refer to in a given image. Proposal generation and proposal\nrepresentation are two effective techniques in many two-stage REC methods.\nHowever, most of the existing works only focus on proposal representation and\nneglect the importance of proposal generation. As a result, the low-quality\nproposals generated by these methods become the performance bottleneck in REC\ntasks. In this paper, we reconsider the problem of proposal generation, and\npropose a novel phrase-guided proposal generation network (PPGN). The main\nimplementation principle of PPGN is refining visual features with text and\ngenerate proposals through regression. Experiments show that our method is\neffective and achieve SOTA performance in benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:21:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Chao", ""], ["Wang", "Guoqing", ""], ["Li", "Dongsheng", ""], ["Shen", "Huawei", ""], ["Feng", "Su", ""], ["Jiang", "Bin", ""]]}, {"id": "2012.11528", "submitter": "Xi Zhu", "authors": "Xi Zhu, Zhendong Mao, Chunxiao Liu, Peng Zhang, Bin Wang, and Yongdong\n  Zhang", "title": "Overcoming Language Priors with Self-supervised Learning for Visual\n  Question Answering", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Visual Question Answering (VQA) models suffer from the language prior\nproblem, which is caused by inherent data biases. Specifically, VQA models tend\nto answer questions (e.g., what color is the banana?) based on the\nhigh-frequency answers (e.g., yellow) ignoring image contents. Existing\napproaches tackle this problem by creating delicate models or introducing\nadditional visual annotations to reduce question dependency while strengthening\nimage dependency. However, they are still subject to the language prior problem\nsince the data biases have not been even alleviated. In this paper, we\nintroduce a self-supervised learning framework to solve this problem.\nConcretely, we first automatically generate labeled data to balance the biased\ndata, and propose a self-supervised auxiliary task to utilize the balanced data\nto assist the base VQA model to overcome language priors. Our method can\ncompensate for the data biases by generating balanced data without introducing\nexternal annotations. Experimental results show that our method can\nsignificantly outperform the state-of-the-art, improving the overall accuracy\nfrom 49.50% to 57.59% on the most commonly used benchmark VQA-CP v2. In other\nwords, we can increase the performance of annotation-based methods by 16%\nwithout using external annotations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 12:30:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhu", "Xi", ""], ["Mao", "Zhendong", ""], ["Liu", "Chunxiao", ""], ["Zhang", "Peng", ""], ["Wang", "Bin", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2012.13392", "submitter": "Chen Chen", "authors": "Ce Zheng and Wenhan Wu and Taojiannan Yang and Sijie Zhu and Chen Chen\n  and Ruixu Liu and Ju Shen and Nasser Kehtarnavaz and Mubarak Shah", "title": "Deep Learning-Based Human Pose Estimation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation aims to locate the human body parts and build human\nbody representation (e.g., body skeleton) from input data such as images and\nvideos. It has drawn increasing attention during the past decade and has been\nutilized in a wide range of applications including human-computer interaction,\nmotion analysis, augmented reality, and virtual reality. Although the recently\ndeveloped deep learning-based solutions have achieved high performance in human\npose estimation, there still remain challenges due to insufficient training\ndata, depth ambiguities, and occlusion. The goal of this survey paper is to\nprovide a comprehensive review of recent deep learning-based solutions for both\n2D and 3D pose estimation via a systematic analysis and comparison of these\nsolutions based on their input data and inference procedures. More than 240\nresearch papers since 2014 are covered in this survey. Furthermore, 2D and 3D\nhuman pose estimation datasets and evaluation metrics are included.\nQuantitative performance comparisons of the reviewed methods on popular\ndatasets are summarized and discussed. Finally, the challenges involved,\napplications, and future research directions are concluded. We also provide a\nregularly updated project page: \\url{https://github.com/zczcwh/DL-HPE}\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 18:49:06 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 21:50:33 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 19:24:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zheng", "Ce", ""], ["Wu", "Wenhan", ""], ["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Liu", "Ruixu", ""], ["Shen", "Ju", ""], ["Kehtarnavaz", "Nasser", ""], ["Shah", "Mubarak", ""]]}, {"id": "2012.13491", "submitter": "Xin Zhao", "authors": "Xin Zhao, Liang Zhao, Madhu Krishnan, Yixin Du, Shan Liu, Debargha\n  Mukherjee, Yaowu Xu, Adrian Grange", "title": "Study On Coding Tools Beyond Av1", "comments": "6 pages, 3 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Alliance for Open Media has recently initiated coding tool exploration\nactivities towards the next-generation video coding beyond AV1. With this\nregard, this paper presents a package of coding tools that have been\ninvestigated, implemented and tested on top of the codebase, known as libaom,\nwhich is used for the exploration of next-generation video compression tools.\nThe proposed tools cover several technical areas based on a traditional hybrid\nvideo coding structure, including block partitioning, prediction, transform and\nloop filtering. The proposed coding tools are integrated as a package, and a\ncombined coding gain over AV1 is demonstrated in this paper. Furthermore, to\nbetter understand the behavior of each tool, besides the combined coding gain,\nthe tool-on and tool-off tests are also simulated and reported for each\nindividual coding tool. Experimental results show that, compared to libaom, the\nproposed methods achieve an average 8.0% (up to 22.0%) overall BD-rate\nreduction for All Intra coding configuration a wide range of image and video\ncontent.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 02:38:47 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhao", "Xin", ""], ["Zhao", "Liang", ""], ["Krishnan", "Madhu", ""], ["Du", "Yixin", ""], ["Liu", "Shan", ""], ["Mukherjee", "Debargha", ""], ["Xu", "Yaowu", ""], ["Grange", "Adrian", ""]]}, {"id": "2012.13968", "submitter": "Zuhui Wang", "authors": "Zuhui Wang, Zhaozheng Yin, Young Anna Argyris", "title": "Detecting Medical Misinformation on Social Media Using Multimodal Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, outbreaks of vaccine-preventable diseases reached the highest number\nin the US since 1992. Medical misinformation, such as antivaccine content\npropagating through social media, is associated with increases in vaccine delay\nand refusal. Our overall goal is to develop an automatic detector for\nantivaccine messages to counteract the negative impact that antivaccine\nmessages have on the public health. Very few extant detection systems have\nconsidered multimodality of social media posts (images, texts, and hashtags),\nand instead focus on textual components, despite the rapid growth of\nphoto-sharing applications (e.g., Instagram). As a result, existing systems are\nnot sufficient for detecting antivaccine messages with heavy visual components\n(e.g., images) posted on these newer platforms. To solve this problem, we\npropose a deep learning network that leverages both visual and textual\ninformation. A new semantic- and task-level attention mechanism was created to\nhelp our model to focus on the essential contents of a post that signal\nantivaccine messages. The proposed model, which consists of three branches, can\ngenerate comprehensive fused features for predictions. Moreover, an ensemble\nmethod is proposed to further improve the final prediction accuracy. To\nevaluate the proposed model's performance, a real-world social media dataset\nthat consists of more than 30,000 samples was collected from Instagram between\nJanuary 2016 and October 2019. Our 30 experiment results demonstrate that the\nfinal network achieves above 97% testing accuracy and outperforms other\nrelevant models, demonstrating that it can detect a large amount of antivaccine\nmessages posted daily. The implementation code is available at\nhttps://github.com/wzhings/antivaccine_detection.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 16:03:32 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Zuhui", ""], ["Yin", "Zhaozheng", ""], ["Argyris", "Young Anna", ""]]}, {"id": "2012.14625", "submitter": "Todd Goodall", "authors": "Todd Goodall and Alan C. Bovik", "title": "The VIP Gallery for Video Processing Education", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital video pervades daily life. Mobile video, digital TV, and digital\ncinema are now ubiquitous, and as such, the field of Digital Video Processing\n(DVP) has experienced tremendous growth. Digital video systems also permeate\nscientific and engineering disciplines including but not limited to astronomy,\ncommunications, surveillance, entertainment, video coding, computer vision, and\nvision research. As a consequence, educational tools for DVP must cater to a\nlarge and diverse base of students. Towards enhancing DVP education we have\ncreated a carefully constructed gallery of educational tools that is designed\nto complement a comprehensive corpus of online lectures by providing examples\nof DVP on real-world content, along with a user-friendly interface that\norganizes numerous key DVP topics ranging from analog video, to human visual\nprocessing, to modern video codecs, etc. This demonstration gallery is\ncurrently being used effectively in the graduate class ``Digital Video'' at the\nUniversity of Texas at Austin. Students receive enhanced access to concepts\nthrough both learning theory from highly visual lectures and watching concrete\nexamples from the gallery, which captures the beauty of the underlying\nprinciples of modern video processing. To better understand the educational\nvalue of these tools, we conducted a pair of questionaire-based surveys to\nassess student background, expectations, and outcomes. The survey results\nsupport the teaching efficacy of this new didactic video toolset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 06:40:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Goodall", "Todd", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2012.15003", "submitter": "Zhijie Huang", "authors": "Zhijie Huang, Xiaopeng Guo, Mingyu Shang, Jie Gao and Jun Sun", "title": "An Efficient QP Variable Convolutional Neural Network Based In-loop\n  Filter for Intra Coding", "comments": "Accepted by DCC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a novel QP variable convolutional neural network based in-loop\nfilter is proposed for VVC intra coding. To avoid training and deploying\nmultiple networks, we develop an efficient QP attention module (QPAM) which can\ncapture compression noise levels for different QPs and emphasize meaningful\nfeatures along channel dimension. Then we embed QPAM into the residual block,\nand based on it, we design a network architecture that is equipped with\ncontrollability for different QPs. To make the proposed model focus more on\nexamples that have more compression artifacts or is hard to restore, a focal\nmean square error (MSE) loss function is employed to fine tune the network.\nExperimental results show that our approach achieves 4.03\\% BD-Rate saving on\naverage for all intra configuration, which is even better than QP-separate CNN\nmodels while having less model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 02:00:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Huang", "Zhijie", ""], ["Guo", "Xiaopeng", ""], ["Shang", "Mingyu", ""], ["Gao", "Jie", ""], ["Sun", "Jun", ""]]}, {"id": "2012.15067", "submitter": "Meng Wang", "authors": "Junru Li, Meng Wang, Li Zhang, Shiqi Wang, Kai Zhang, Shanshe Wang,\n  Siwei Ma and Wen Gao", "title": "Sub-sampled Cross-component Prediction for Emerging Video Coding\n  Standards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-component linear model (CCLM) prediction has been repeatedly proven to\nbe effective in reducing the inter-channel redundancies in video compression.\nEssentially speaking, the linear model is identically trained by employing\naccessible luma and chroma reference samples at both encoder and decoder,\nelevating the level of operational complexity due to the least square\nregression or max-min based model parameter derivation. In this paper, we\ninvestigate the capability of the linear model in the context of sub-sampled\nbased cross-component correlation mining, as a means of significantly releasing\nthe operation burden and facilitating the hardware and software design for both\nencoder and decoder. In particular, the sub-sampling ratios and positions are\nelaborately designed by exploiting the spatial correlation and the\ninter-channel correlation. Extensive experiments verify that the proposed\nmethod is characterized by its simplicity in operation and robustness in terms\nof rate-distortion performance, leading to the adoption by Versatile Video\nCoding (VVC) standard and the third generation of Audio Video Coding Standard\n(AVS3).\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 07:42:53 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Junru", ""], ["Wang", "Meng", ""], ["Zhang", "Li", ""], ["Wang", "Shiqi", ""], ["Zhang", "Kai", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2012.15635", "submitter": "Alan Smeaton", "authors": "Lorin Sweeney, Graham Healy, Alan F. Smeaton", "title": "Leveraging Audio Gestalt to Predict Media Memorability", "comments": "3 pages, 1 Figure, 2 Tables", "journal-ref": "MediaEval Multimedia Benchmark Workshop Working Notes, 14-15\n  December 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memorability determines what evanesces into emptiness, and what worms its way\ninto the deepest furrows of our minds. It is the key to curating more\nmeaningful media content as we wade through daily digital torrents. The\nPredicting Media Memorability task in MediaEval 2020 aims to address the\nquestion of media memorability by setting the task of automatically predicting\nvideo memorability. Our approach is a multimodal deep learning-based late\nfusion that combines visual, semantic, and auditory features. We used audio\ngestalt to estimate the influence of the audio modality on overall video\nmemorability, and accordingly inform which combination of features would best\npredict a given video's memorability scores.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 14:50:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sweeney", "Lorin", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2012.15641", "submitter": "Alan Smeaton", "authors": "Phuc H. Le-Khac and Ayush K. Rai and Graham Healy and Alan F. Smeaton\n  and Noel E. O'Connor", "title": "Investigating Memorability of Dynamic Media", "comments": "3 pages, 1 figure. 1 table", "journal-ref": "MediaEval Multimedia Benchmark Workshop Working Notes, 14-15\n  December 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Predicting Media Memorability task in MediaEval'20 has some challenging\naspects compared to previous years. In this paper we identify the high-dynamic\ncontent in videos and dataset of limited size as the core challenges for the\ntask, we propose directions to overcome some of these challenges and we present\nour initial result in these directions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:01:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Le-Khac", "Phuc H.", ""], ["Rai", "Ayush K.", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "2012.15650", "submitter": "Alan Smeaton", "authors": "Alba Garc\\'ia Seco De Herrera and Rukiye Savran Kiziltepe and Jon\n  Chamberlain and Mihai Gabriel Constantin and Claire-H\\'el\\`ene Demarty and\n  Faiyaz Doctor and Bogdan Ionescu and Alan F. Smeaton", "title": "Overview of MediaEval 2020 Predicting Media Memorability Task: What\n  Makes a Video Memorable?", "comments": "3 pages, 1 Figure", "journal-ref": "MediaEval Multimedia Benchmark Workshop Working Notes, 14-15\n  December 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the MediaEval 2020 \\textit{Predicting Media\nMemorability} task. After first being proposed at MediaEval 2018, the\nPredicting Media Memorability task is in its 3rd edition this year, as the\nprediction of short-term and long-term video memorability (VM) remains a\nchallenging task. In 2020, the format remained the same as in previous\neditions. This year the videos are a subset of the TRECVid 2019 Video-to-Text\ndataset, containing more action rich video content as compared with the 2019\ntask. In this paper a description of some aspects of this task is provided,\nincluding its main characteristics, a description of the collection, the ground\ntruth dataset, evaluation metrics and the requirements for participants' run\nsubmissions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:12:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["De Herrera", "Alba Garc\u00eda Seco", ""], ["Kiziltepe", "Rukiye Savran", ""], ["Chamberlain", "Jon", ""], ["Constantin", "Mihai Gabriel", ""], ["Demarty", "Claire-H\u00e9l\u00e8ne", ""], ["Doctor", "Faiyaz", ""], ["Ionescu", "Bogdan", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2012.15853", "submitter": "Wieslaw Kopec", "authors": "Anna Jaskulska, Kinga Skorupska, Barbara Karpowicz, Cezary Biele,\n  Jaros{\\l}aw Kowalski, Wies{\\l}aw Kope\\'c", "title": "Exploration of Voice User Interfaces for Older Adults - A Pilot Study to\n  Address Progressive Vision Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice User Interfaces (VUIs) owing to recent developments in Artificial\nIntelligence (AI) and Natural Language Processing (NLP), are becoming\nincreasingly intuitive and functional. They are especially promising for older\nadults, also with special needs, as VUIs remove some barriers related to access\nto Information and Communications Technology (ICT) solutions. In this pilot\nstudy we examine interdisciplinary opportunities in the area of VUIs as\nassistive technologies, based on an exploratory study with older adults, and a\nfollow-up in-depth pilot study with two participants regarding the needs of\npeople who are gradually losing their sight at a later age.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:58:32 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Karpowicz", "Barbara", ""], ["Biele", "Cezary", ""], ["Kowalski", "Jaros\u0142aw", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}]